{"created":"2024-05-16 17:59:52","title":"Probing Hidden Dimensions via Muon Lifetime Measurements","abstract":"In the context of Kaluza-Klein theories, the time dilation of charged particles in an external field depends on the charge in a specific way. Experimental tests are proposed to search for extra dimensions using this distinctive feature.","sentences":["In the context of Kaluza-Klein theories, the time dilation of charged particles in an external field depends on the charge in a specific way.","Experimental tests are proposed to search for extra dimensions using this distinctive feature."],"url":"http://arxiv.org/abs/2405.10321v1","category":"gr-qc"}
{"created":"2024-05-16 17:59:42","title":"Fast simulation mapping: from standard to modified gravity cosmologies using the bias assignment method","abstract":"We assess the effectiveness of a non-parametric bias model in generating mock halo catalogues for modified gravity (MG) cosmologies, relying on the distribution of dark matter from either MG or $\\Lambda$CDM. We aim to generate halo catalogues that effectively capture the distinct impact of MG, ensuring high accuracy in both two- and three-point statistics for comprehensive analysis of large-scale structures. As part of this study we aim at investigating the inclusion of MG into non-local bias to directly map the tracers onto $\\Lambda$CDM fields, which would save many computational costs. We employ the bias assignment method (BAM) to model halo distribution statistics by leveraging seven high-resolution COLA simulations of MG cosmologies. Taking into account cosmic-web dependencies when learning the bias relations, we design two experiments to map the MG effects: one utilising the consistent MG density fields and the other employing the benchmark $\\Lambda$CDM density field. BAM generates MG halo catalogues from both calibrations experiments excelling in summary statistics, achieving a $\\sim 1\\%$ accuracy in the power spectrum across a wide range of $k$-modes, with only minimal differences well below 10\\% at modes subject to cosmic variance, particularly below $k<0.07$ $h$Mpc$^{-1}$. The reduced bispectrum remains consistent with the reference catalogues within 10\\% for the studied configuration. Our results demonstrate that a non-linear and non-local bias description can model the effects of MG starting from a $\\Lambda$CDM dark matter field.","sentences":["We assess the effectiveness of a non-parametric bias model in generating mock halo catalogues for modified gravity (MG) cosmologies, relying on the distribution of dark matter from either MG or $\\Lambda$CDM.","We aim to generate halo catalogues that effectively capture the distinct impact of MG, ensuring high accuracy in both two- and three-point statistics for comprehensive analysis of large-scale structures.","As part of this study we aim at investigating the inclusion of MG into non-local bias to directly map the tracers onto $\\Lambda$CDM fields, which would save many computational costs.","We employ the bias assignment method (BAM) to model halo distribution statistics by leveraging seven high-resolution COLA simulations of MG cosmologies.","Taking into account cosmic-web dependencies when learning the bias relations, we design two experiments to map the MG effects: one utilising the consistent MG density fields and the other employing the benchmark $\\Lambda$CDM density field.","BAM generates MG halo catalogues from both calibrations experiments excelling in summary statistics, achieving a $\\sim 1\\%$ accuracy in the power spectrum across a wide range of $k$-modes, with only minimal differences well below 10\\% at modes subject to cosmic variance, particularly below $k<0.07$ $h$Mpc$^{-1}$. The reduced bispectrum remains consistent with the reference catalogues within 10\\% for the studied configuration.","Our results demonstrate that a non-linear and non-local bias description can model the effects of MG starting from a $\\Lambda$CDM dark matter field."],"url":"http://arxiv.org/abs/2405.10319v1","category":"astro-ph.CO"}
{"created":"2024-05-16 17:59:36","title":"Gauge theory of giant phonon magnetic moment in doped Dirac semimetals","abstract":"We present a quantum theory for phonon magnetic moment in doped Dirac semimetals. Our theory is based on an emergent gauge field approach to the electron-phonon coupling, applicable for both gapless and gapped systems. We find that the magnetic moment is directly proportional to the electrical Hall conductivity through the phonon Hall viscosity. Our theory is combined with the first-principles calculations, allowing us to quantitatively implement it to realistic materials. Magnetic moments are found to be on the order of Bohr magneton for certain phonon modes in graphene and $\\text{Cd}_3 \\text{As}_2$. Our results provide practical guidance for the dynamic generation of large magnetization in the topological quantum materials.","sentences":["We present a quantum theory for phonon magnetic moment in doped Dirac semimetals.","Our theory is based on an emergent gauge field approach to the electron-phonon coupling, applicable for both gapless and gapped systems.","We find that the magnetic moment is directly proportional to the electrical Hall conductivity through the phonon Hall viscosity.","Our theory is combined with the first-principles calculations, allowing us to quantitatively implement it to realistic materials.","Magnetic moments are found to be on the order of Bohr magneton for certain phonon modes in graphene and $\\text{Cd}_3 \\text{As}_2$.","Our results provide practical guidance for the dynamic generation of large magnetization in the topological quantum materials."],"url":"http://arxiv.org/abs/2405.10318v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 17:59:22","title":"Text-to-Vector Generation with Neural Path Representation","abstract":"Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.","sentences":["Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties.","However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task.","Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible.","However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints.","To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities.","By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs.","Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs.","In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process.","In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure.","We demonstrate the effectiveness of our method through extensive experiments and showcase various applications.","The project page is https://intchous.github.io/T2V-NPR."],"url":"http://arxiv.org/abs/2405.10317v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:21","title":"Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model","abstract":"Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning. However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset. On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts. To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting. For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples. For textual prompting, we leverage GPT-4V's visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts. Our method is out-of-the-box and does not require fine-tuning or optimization. It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner. Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively.","sentences":["Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning.","However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset.","On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts.","To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting.","For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples.","For textual prompting, we leverage GPT-4V's visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts.","Our method is out-of-the-box and does not require fine-tuning or optimization.","It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner.","Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.10316v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:07","title":"TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction","abstract":"Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots. The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps. Previous methods often require domain-specific knowledge a priori. We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world. The robots can then learn from humans to close various sim-to-real gaps. We propose TRANSIC, a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework. TRANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction. Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution. We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly. Through synergistic integration of policies learned in simulation and from humans, TRANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps. It displays attractive properties such as scaling with human effort. Videos and code are available at https://transic-robot.github.io/","sentences":["Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots.","The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps.","Previous methods often require domain-specific knowledge a priori.","We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world.","The robots can then learn from humans to close various sim-to-real gaps.","We propose TRANSIC, a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework.","TRANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction.","Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution.","We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly.","Through synergistic integration of policies learned in simulation and from humans, TRANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps.","It displays attractive properties such as scaling with human effort.","Videos and code are available at https://transic-robot.github.io/"],"url":"http://arxiv.org/abs/2405.10315v1","category":"cs.RO"}
{"created":"2024-05-16 17:59:05","title":"CAT3D: Create Anything in 3D with Multi-View Diffusion Models","abstract":"Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation. See our project page for results and interactive demos at https://cat3d.github.io .","sentences":["Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene.","We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model.","Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene.","These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time.","CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation.","See our project page for results and interactive demos at https://cat3d.github.io ."],"url":"http://arxiv.org/abs/2405.10314v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:02","title":"How Far Are We From AGI","abstract":"The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors. Yet, the escalating demands on AI have highlighted the limitations of AI's current offerings, catalyzing a movement towards Artificial General Intelligence (AGI). AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI's definitions, goals, and developmental trajectories. Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI.","sentences":["The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors.","Yet, the escalating demands on AI have highlighted the limitations of AI's current offerings, catalyzing a movement towards Artificial General Intelligence (AGI).","AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution.","While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI's definitions, goals, and developmental trajectories.","Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives.","We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions.","As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors.","Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI.","Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains.","In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI."],"url":"http://arxiv.org/abs/2405.10313v1","category":"cs.AI"}
{"created":"2024-05-16 17:58:45","title":"UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models","abstract":"Recently, Multi-Modal(MM) Large Language Models(LLMs) have unlocked many complex use-cases that require MM understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelity of MM-LLMs we introduce the model-agnostic UniRAG technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT4 and Gemini-Pro and smaller open-source models like Llava, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by MM retrievers like UniIR models.","sentences":["Recently, Multi-Modal(MM) Large Language Models(LLMs) have unlocked many complex use-cases that require MM understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities.","To further improve the output fidelity of MM-LLMs we introduce the model-agnostic UniRAG technique that adds relevant retrieved information to prompts as few-shot examples during inference.","Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT4 and Gemini-Pro and smaller open-source models like Llava, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by MM retrievers like UniIR models."],"url":"http://arxiv.org/abs/2405.10311v1","category":"cs.IR"}
{"created":"2024-05-16 17:58:44","title":"Stochastic Q-learning for Large Discrete Action Spaces","abstract":"In complex environments with large discrete action spaces, effective decision-making is critical in reinforcement learning (RL). Despite the widespread use of value-based RL approaches like Q-learning, they come with a computational burden, necessitating the maximization of a value function over all actions in each iteration. This burden becomes particularly challenging when addressing large-scale problems and using deep neural networks as function approximators. In this paper, we present stochastic value-based RL approaches which, in each iteration, as opposed to optimizing over the entire set of $n$ actions, only consider a variable stochastic set of a sublinear number of actions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic value-based RL methods include, among others, Stochastic Q-learning, StochDQN, and StochDDQN, all of which integrate this stochastic approach for both value-function updates and action selection. The theoretical convergence of Stochastic Q-learning is established, while an analysis of stochastic maximization is provided. Moreover, through empirical validation, we illustrate that the various proposed approaches outperform the baseline methods across diverse environments, including different control problems, achieving near-optimal average returns in significantly reduced time.","sentences":["In complex environments with large discrete action spaces, effective decision-making is critical in reinforcement learning (RL).","Despite the widespread use of value-based RL approaches like Q-learning, they come with a computational burden, necessitating the maximization of a value function over all actions in each iteration.","This burden becomes particularly challenging when addressing large-scale problems and using deep neural networks as function approximators.","In this paper, we present stochastic value-based RL approaches which, in each iteration, as opposed to optimizing over the entire set of $n$ actions, only consider a variable stochastic set of a sublinear number of actions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic value-based RL methods include, among others, Stochastic Q-learning, StochDQN, and StochDDQN, all of which integrate this stochastic approach for both value-function updates and action selection.","The theoretical convergence of Stochastic Q-learning is established, while an analysis of stochastic maximization is provided.","Moreover, through empirical validation, we illustrate that the various proposed approaches outperform the baseline methods across diverse environments, including different control problems, achieving near-optimal average returns in significantly reduced time."],"url":"http://arxiv.org/abs/2405.10310v1","category":"cs.LG"}
{"created":"2024-05-16 17:58:16","title":"On the lapse contour","abstract":"The gravitational path integral is usually implemented with a covariant action by analogy with other gauge field theories, but the gravitational case is different in important ways. A key difference is that the integrand has an essential singularity, which occurs at zero lapse where the spacetime metric degenerates. The lapse integration contour required to impose the local time reparametrization constraints must run from $-\\infty$ to $+\\infty$, yet must not pass through zero. This raises the question: what is the correct integration contour, and why? We study that question by starting with the reduced phase space path integral, which involves no essential singularity. We observe that if the momenta are to be integrated before the lapse, to obtain a configuration space path integral, the lapse contour should pass below the origin in the complex lapse plane. This contour is also consistent with the requirement that quantum field fluctuation amplitudes have the usual short distance vacuum form, and with obtaining the Bekenstein-Hawking horizon entropy from a Lorentzian path integral. We close with a discussion of related issues, including potential obstacles to deriving a nonperturbative covariant gravitational path integral.","sentences":["The gravitational path integral is usually implemented with a covariant action by analogy with other gauge field theories, but the gravitational case is different in important ways.","A key difference is that the integrand has an essential singularity, which occurs at zero lapse where the spacetime metric degenerates.","The lapse integration contour required to impose the local time reparametrization constraints must run from $-\\infty$ to $+\\infty$, yet must not pass through zero.","This raises the question: what is the correct integration contour, and why?","We study that question by starting with the reduced phase space path integral, which involves no essential singularity.","We observe that if the momenta are to be integrated before the lapse, to obtain a configuration space path integral, the lapse contour should pass below the origin in the complex lapse plane.","This contour is also consistent with the requirement that quantum field fluctuation amplitudes have the usual short distance vacuum form, and with obtaining the Bekenstein-Hawking horizon entropy from a Lorentzian path integral.","We close with a discussion of related issues, including potential obstacles to deriving a nonperturbative covariant gravitational path integral."],"url":"http://arxiv.org/abs/2405.10307v1","category":"hep-th"}
{"created":"2024-05-16 17:56:55","title":"4D Panoptic Scene Graph Generation","abstract":"We are living in a three-dimensional space while moving forward through a fourth dimension: time. To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce 4D Panoptic Scene Graph (PSG-4D), a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status information, and edges, which capture the temporal relations. To facilitate research in this new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of 1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding scene graphs via a relation component. Extensive experiments on the new dataset show that our method can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world application example to demonstrate how we can achieve dynamic scene understanding by integrating a large language model into our PSG-4D system.","sentences":["We are living in a three-dimensional space while moving forward through a fourth dimension: time.","To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce 4D Panoptic Scene Graph (PSG-4D), a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding.","Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status information, and edges, which capture the temporal relations.","To facilitate research in this new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of 1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs.","To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding scene graphs via a relation component.","Extensive experiments on the new dataset show that our method can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world application example to demonstrate how we can achieve dynamic scene understanding by integrating a large language model into our PSG-4D system."],"url":"http://arxiv.org/abs/2405.10305v1","category":"cs.CV"}
{"created":"2024-05-16 17:56:22","title":"Towards Unpolarized GPDs from Pseudo-Distributions","abstract":"We present an exploration of the unpolarized isovector proton generalized parton distributions (GPDs) $H^{u-d}(x, \\xi, t)$ and $E^{u-d}(x, \\xi, t)$ in the pseudo-distribution formalism using distillation. Taking advantage of the large kinematic coverage made possible by this approach, we present results on the moments of GPDs up to the order $x^3$ -- including their skewness dependence -- at a pion mass $m_\\pi = 358$ MeV and a lattice spacing $a = 0.094$ fm.","sentences":["We present an exploration of the unpolarized isovector proton generalized parton distributions (GPDs) $H^{u-d}(x, \\xi, t)$ and $E^{u-d}(x, \\xi, t)$ in the pseudo-distribution formalism using distillation.","Taking advantage of the large kinematic coverage made possible by this approach, we present results on the moments of GPDs up to the order $x^3$ -- including their skewness dependence -- at a pion mass $m_\\pi = 358$ MeV and a lattice spacing $a = 0.094$ fm."],"url":"http://arxiv.org/abs/2405.10304v1","category":"hep-lat"}
{"created":"2024-05-16 17:56:16","title":"Asymmetric Warm Dark Matter: from Cosmological Asymmetry to Chirality of Life","abstract":"We investigate a novel scenario involving asymmetric keV-range dark matter (DM) in the form of right-handed (sterile) neutrinos. Based on the Fermi-Dirac distribution, we demonstrate that asymmetric fermionic DM forms a Fermi degenerate gas, making it potentially colder than symmetric fermionic DM. This setup simultaneously accounts for the Universe's baryon asymmetry through tiny Yukawa interactions with Standard Model leptons and the Higgs field, and the homochirality of amino acids via decay into circularly polarized photons. This scenario can be investigated through soft X-ray searches conducted by current and upcoming space missions. The helical X-rays is a smoking-gun signal of our scenario. Additionally, we propose a new mechanism to suppress DM thermal production by introducing a light modulus, which may also benefit cosmology involving generic right-handed neutrinos with large mixing.","sentences":["We investigate a novel scenario involving asymmetric keV-range dark matter (DM) in the form of right-handed (sterile) neutrinos.","Based on the Fermi-Dirac distribution, we demonstrate that asymmetric fermionic DM forms a Fermi degenerate gas, making it potentially colder than symmetric fermionic DM.","This setup simultaneously accounts for the Universe's baryon asymmetry through tiny Yukawa interactions with Standard Model leptons and the Higgs field, and the homochirality of amino acids via decay into circularly polarized photons.","This scenario can be investigated through soft X-ray searches conducted by current and upcoming space missions.","The helical X-rays is a smoking-gun signal of our scenario.","Additionally, we propose a new mechanism to suppress DM thermal production by introducing a light modulus, which may also benefit cosmology involving generic right-handed neutrinos with large mixing."],"url":"http://arxiv.org/abs/2405.10303v1","category":"hep-ph"}
{"created":"2024-05-16 17:55:42","title":"Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift","abstract":"As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.","sentences":["As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts.","A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance.","The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution.","In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain.","Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation.","Our proposed methodologies are computationally efficient and easy to implement.","Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details.","This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals.","Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts."],"url":"http://arxiv.org/abs/2405.10302v1","category":"stat.ME"}
{"created":"2024-05-16 17:55:24","title":"Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees","abstract":"Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.","sentences":["Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values.","For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making.","This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion.","It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution.","Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor.","It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy.","Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data.","En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor."],"url":"http://arxiv.org/abs/2405.10301v1","category":"stat.ML"}
{"created":"2024-05-16 17:54:15","title":"Grounding DINO 1.5: Advance the \"Edge\" of Open-Set Object Detection","abstract":"This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the \"Edge\" of open-set object detection. The suite encompasses two models: Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment. The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding. The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset. Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection. Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios. Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API","sentences":["This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the \"Edge\" of open-set object detection.","The suite encompasses two models:","Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment.","The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding.","The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset.","Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection.","Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios.","Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API"],"url":"http://arxiv.org/abs/2405.10300v1","category":"cs.CV"}
{"created":"2024-05-16 17:53:32","title":"HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models","abstract":"The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency, energy consumption, GPU memory usage, and performance. There is a growing interest in establishing Pareto frontiers for different language model configurations to identify optimal models with specified hardware constraints. Notably, architectures that excel in latency on one device may not perform optimally on another. However, exhaustive training and evaluation of numerous architectures across diverse hardware configurations is computationally prohibitive. To this end, we propose HW-GPT-Bench, a hardware-aware language model surrogate benchmark, where we leverage weight-sharing techniques from Neural Architecture Search (NAS) to efficiently train a supernet proxy, encompassing language models of varying scales in a single model. We conduct profiling of these models across 13 devices, considering 5 hardware metrics and 3 distinct model scales. Finally, we showcase the usability of HW-GPT-Bench using 8 different multi-objective NAS algorithms and evaluate the quality of the resultant Pareto fronts. Through this benchmark, our objective is to propel and expedite research in the advancement of multi-objective methods for NAS and structural pruning in large language models.","sentences":["The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency, energy consumption, GPU memory usage, and performance.","There is a growing interest in establishing Pareto frontiers for different language model configurations to identify optimal models with specified hardware constraints.","Notably, architectures that excel in latency on one device may not perform optimally on another.","However, exhaustive training and evaluation of numerous architectures across diverse hardware configurations is computationally prohibitive.","To this end, we propose HW-GPT-Bench, a hardware-aware language model surrogate benchmark, where we leverage weight-sharing techniques from Neural Architecture Search (NAS) to efficiently train a supernet proxy, encompassing language models of varying scales in a single model.","We conduct profiling of these models across 13 devices, considering 5 hardware metrics and 3 distinct model scales.","Finally, we showcase the usability of HW-GPT-Bench using 8 different multi-objective NAS algorithms and evaluate the quality of the resultant Pareto fronts.","Through this benchmark, our objective is to propel and expedite research in the advancement of multi-objective methods for NAS and structural pruning in large language models."],"url":"http://arxiv.org/abs/2405.10299v1","category":"cs.LG"}
{"created":"2024-05-16 17:52:36","title":"Low-Degree Polynomials Are Good Extractors","abstract":"We prove that random low-degree polynomials (over $\\mathbb{F}_2$) are unbiased, in an extremely general sense. That is, we show that random low-degree polynomials are good randomness extractors for a wide class of distributions. Prior to our work, such results were only known for the small families of (1) uniform sources, (2) affine sources, and (3) local sources. We significantly generalize these results, and prove the following.   1. Low-degree polynomials extract from small families. We show that a random low-degree polynomial is a good low-error extractor for any small family of sources. In particular, we improve the positive result of Alrabiah, Chattopadhyay, Goodman, Li, and Ribeiro (ICALP 2022) for local sources, and give new results for polynomial sources and variety sources via a single unified approach.   2. Low-degree polynomials extract from sumset sources. We show that a random low-degree polynomial is a good extractor for sumset sources, which are the most general large family of sources (capturing independent sources, interleaved sources, small-space sources, and more). This extractor achieves polynomially small error, and its min-entropy requirement is tight up to a square.   Our results on sumset extractors imply new complexity separations for linear ROBPs, and the tools that go into its proof have further applications, as well. The two main tools we use are a new structural result on sumset-punctured Reed-Muller codes, paired with a novel type of reduction between randomness extractors. Using the first new tool, we strengthen and generalize the extractor impossibility results of Chattopadhyay, Goodman, and Gurumukhani (ITCS 2024). Using the second, we show the existence of sumset extractors for min-entropy $k=O(\\log(n/\\varepsilon))$, resolving an open problem of Chattopadhyay and Liao (STOC 2022).","sentences":["We prove that random low-degree polynomials (over $\\mathbb{F}_2$) are unbiased, in an extremely general sense.","That is, we show that random low-degree polynomials are good randomness extractors for a wide class of distributions.","Prior to our work, such results were only known for the small families of (1) uniform sources, (2) affine sources, and (3) local sources.","We significantly generalize these results, and prove the following.   ","1. Low-degree polynomials extract from small families.","We show that a random low-degree polynomial is a good low-error extractor for any small family of sources.","In particular, we improve the positive result of Alrabiah, Chattopadhyay, Goodman, Li, and Ribeiro (ICALP 2022) for local sources, and give new results for polynomial sources and variety sources via a single unified approach.   ","2. Low-degree polynomials extract from sumset sources.","We show that a random low-degree polynomial is a good extractor for sumset sources, which are the most general large family of sources (capturing independent sources, interleaved sources, small-space sources, and more).","This extractor achieves polynomially small error, and its min-entropy requirement is tight up to a square.   ","Our results on sumset extractors imply new complexity separations for linear ROBPs, and the tools that go into its proof have further applications, as well.","The two main tools we use are a new structural result on sumset-punctured Reed-Muller codes, paired with a novel type of reduction between randomness extractors.","Using the first new tool, we strengthen and generalize the extractor impossibility results of Chattopadhyay, Goodman, and Gurumukhani (ITCS 2024).","Using the second, we show the existence of sumset extractors for min-entropy $k=O(\\log(n/\\varepsilon))$, resolving an open problem of Chattopadhyay and Liao (STOC 2022)."],"url":"http://arxiv.org/abs/2405.10297v1","category":"cs.CC"}
{"created":"2024-05-16 17:52:26","title":"Verifying Unboundedness via Amalgamation","abstract":"Well-structured transition systems (WSTS) are an abstract family of systems that encompasses a vast landscape of infinite-state systems. By requiring a well-quasi-ordering (wqo) on the set of states, a WSTS enables generic algorithms for classic verification tasks such as coverability and termination. However, even for systems that are WSTS like vector addition systems (VAS), the framework is notoriously ill-equipped to analyse reachability (as opposed to coverability). Moreover, some important types of infinite-state systems fall out of WSTS' scope entirely, such as pushdown systems (PDS).   Inspired by recent algorithmic techniques on VAS, we propose an abstract notion of systems where the set of runs is equipped with a wqo and supports amalgamation of runs. We show that it subsumes a large class of infinite-state systems, including (reachability languages of) VAS and PDS, and even all systems from the abstract framework of valence systems, except for those already known to be Turing-complete.   Moreover, this abstract setting enables simple and general algorithmic solutions to unboundedness problems, which have received much attention in recent years. We present algorithms for the (i) simultaneous unboundedness problem (which implies computability of downward closures and decidability of separability by piecewise testable languages), (ii) computing priority downward closures, (iii) deciding whether a language is bounded, meaning included in $w_1^*\\cdots w_k^*$ for some words $w_1,\\ldots,w_k$, and (iv)~effective regularity of unary languages. This leads to either drastically simpler proofs or new decidability results for a rich variety of systems.","sentences":["Well-structured transition systems (WSTS) are an abstract family of systems that encompasses a vast landscape of infinite-state systems.","By requiring a well-quasi-ordering (wqo) on the set of states, a WSTS enables generic algorithms for classic verification tasks such as coverability and termination.","However, even for systems that are WSTS like vector addition systems (VAS), the framework is notoriously ill-equipped to analyse reachability (as opposed to coverability).","Moreover, some important types of infinite-state systems fall out of WSTS' scope entirely, such as pushdown systems (PDS).   ","Inspired by recent algorithmic techniques on VAS, we propose an abstract notion of systems where the set of runs is equipped with a wqo and supports amalgamation of runs.","We show that it subsumes a large class of infinite-state systems, including (reachability languages of) VAS and PDS, and even all systems from the abstract framework of valence systems, except for those already known to be Turing-complete.   ","Moreover, this abstract setting enables simple and general algorithmic solutions to unboundedness problems, which have received much attention in recent years.","We present algorithms for the (i) simultaneous unboundedness problem (which implies computability of downward closures and decidability of separability by piecewise testable languages), (ii) computing priority downward closures, (iii) deciding whether a language is bounded, meaning included in $w_1^*\\cdots w_k^*$ for some words $w_1,\\ldots,w_k$, and (iv)~effective regularity of unary languages.","This leads to either drastically simpler proofs or new decidability results for a rich variety of systems."],"url":"http://arxiv.org/abs/2405.10296v1","category":"cs.FL"}
{"created":"2024-05-16 17:52:12","title":"Societal Adaptation to Advanced AI","abstract":"Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse. However, this approach becomes less feasible as the number of developers of advanced AI grows, and impedes beneficial use-cases as well as harmful ones. In response, we urge a complementary approach: increasing societal adaptation to advanced AI, that is, reducing the expected negative impacts from a given level of diffusion of a given AI capability. We introduce a conceptual framework which helps identify adaptive interventions that avoid, defend against and remedy potentially harmful uses of AI systems, illustrated with examples in election manipulation, cyberterrorism, and loss of control to AI decision-makers. We discuss a three-step cycle that society can implement to adapt to AI. Increasing society's ability to implement this cycle builds its resilience to advanced AI. We conclude with concrete recommendations for governments, industry, and third-parties.","sentences":["Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse.","However, this approach becomes less feasible as the number of developers of advanced AI grows, and impedes beneficial use-cases as well as harmful ones.","In response, we urge a complementary approach: increasing societal adaptation to advanced AI, that is, reducing the expected negative impacts from a given level of diffusion of a given AI capability.","We introduce a conceptual framework which helps identify adaptive interventions that avoid, defend against and remedy potentially harmful uses of AI systems, illustrated with examples in election manipulation, cyberterrorism, and loss of control to AI decision-makers.","We discuss a three-step cycle that society can implement to adapt to AI.","Increasing society's ability to implement this cycle builds its resilience to advanced AI.","We conclude with concrete recommendations for governments, industry, and third-parties."],"url":"http://arxiv.org/abs/2405.10295v1","category":"cs.CY"}
{"created":"2024-05-16 17:52:09","title":"Corrections to adiabatic behavior for long paths","abstract":"The cost and the error of the adiabatic theorem for preparing the final eigenstate are discussed in terms of path length. Previous studies in terms of the norm of the Hamiltonian and its derivatives with the spectral gap are limited to describe the cost of adiabatic state preparation for large systems. We argue that total time is not a good measure for determining the computational difficulty of adiabatic quantum computation by developing a no-go theorem. From the result of time-periodic Hamiltonian cases, we suggest that there are proxies for computational cost which typically grow as path length increases when the error is kept fixed and small and consider possible conjectures on how general the behavior is.","sentences":["The cost and the error of the adiabatic theorem for preparing the final eigenstate are discussed in terms of path length.","Previous studies in terms of the norm of the Hamiltonian and its derivatives with the spectral gap are limited to describe the cost of adiabatic state preparation for large systems.","We argue that total time is not a good measure for determining the computational difficulty of adiabatic quantum computation by developing a no-go theorem.","From the result of time-periodic Hamiltonian cases, we suggest that there are proxies for computational cost which typically grow as path length increases when the error is kept fixed and small and consider possible conjectures on how general the behavior is."],"url":"http://arxiv.org/abs/2405.10294v1","category":"quant-ph"}
{"created":"2024-05-16 17:51:38","title":"Charged Black Holes from Interacting Vacuum","abstract":"In this paper charged black holes are obtained assuming that a Born-Infeld electrodynamics may arise from an interaction between the electromagnetic field and a vacuum component. In this context Cauchy horizons do not appear in the maximal analytical extension once an event horizon is formed so that the interior spacetime does not suffer from any sort of instabilities which are well known in the literature. On the contrary, the causal structure exhibits an event horizon -- encapsulating a spacelike singularity -- and a cosmological horizon. We show that the strong cosmic censorship is then restored for a wide range of the parameters including configurations in which the black hole charge is much larger than its mass. We also show that the black hole thus formed described by our solution exhibits an unstable photon sphere analogous to that of the Schwarzschild metric.","sentences":["In this paper charged black holes are obtained assuming that a Born-Infeld electrodynamics may arise from an interaction between the electromagnetic field and a vacuum component.","In this context Cauchy horizons do not appear in the maximal analytical extension once an event horizon is formed so that the interior spacetime does not suffer from any sort of instabilities which are well known in the literature.","On the contrary, the causal structure exhibits an event horizon -- encapsulating a spacelike singularity -- and a cosmological horizon.","We show that the strong cosmic censorship is then restored for a wide range of the parameters including configurations in which the black hole charge is much larger than its mass.","We also show that the black hole thus formed described by our solution exhibits an unstable photon sphere analogous to that of the Schwarzschild metric."],"url":"http://arxiv.org/abs/2405.10293v1","category":"gr-qc"}
{"created":"2024-05-16 17:50:19","title":"Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning","abstract":"Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.","sentences":["Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios.","However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments.","To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL).","Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action.","Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards.","Finally, our framework uses these task rewards to fine-tune the entire VLM with RL.","Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini.","Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method."],"url":"http://arxiv.org/abs/2405.10292v1","category":"cs.AI"}
{"created":"2024-05-16 17:50:05","title":"Interaction induced splitting of Dirac monopoles in the topological Thouless pumping of strongly interacting Bosons and SU($N$) Fermions","abstract":"Motivated by the observation of the breakdown of quantization for the Thouless pump in the presence of strong interaction by ETH [Walter et. al. Nat. Phys. 19, 1471 (2023), Viebahn et. al. arXiv:2308.03756], we study the interplay of strong interaction and topology in the (1+1)-dimensional interacting Rice-Mele model. We point out that the quantization of the interacting Thouless pump is dictated by the Chern number, i.e., the Dirac monopoles enclosed by the generalized Brillouin zone of the many-body wave function. By analyzing the change of location monopoles due to interaction, we predict the Thouless charge pump for strongly interacting Bose and SU($N$) Fermi gases in optical lattices and explain the ETH experiment.","sentences":["Motivated by the observation of the breakdown of quantization for the Thouless pump in the presence of strong interaction by ETH [Walter et.","al.","Nat.","Phys. 19, 1471 (2023), Viebahn et.","al. arXiv:2308.03756], we study the interplay of strong interaction and topology in the (1+1)-dimensional interacting Rice-Mele model.","We point out that the quantization of the interacting Thouless pump is dictated by the Chern number, i.e., the Dirac monopoles enclosed by the generalized Brillouin zone of the many-body wave function.","By analyzing the change of location monopoles due to interaction, we predict the Thouless charge pump for strongly interacting Bose and SU($N$)","Fermi gases in optical lattices and explain the ETH experiment."],"url":"http://arxiv.org/abs/2405.10291v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-16 17:48:21","title":"Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction","abstract":"Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.","sentences":["Facts extraction is pivotal for constructing knowledge graphs.","Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction.","In this paper, we specifically address the extraction of temporal facts from natural language text.","Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences.","To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts.","In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results.","To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs).","To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset.","Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets."],"url":"http://arxiv.org/abs/2405.10288v1","category":"cs.CL"}
{"created":"2024-05-16 17:47:49","title":"Exotic compact objects and light bosonic fields","abstract":"In this note, we discuss the effect of light, non-gauge, bosonic degrees of freedom on the exterior spacetime of an exotic compact object. We show that such fields generically introduce large deviations from spacetimes of vacuum General Relativity near and outside the surfaces of ultra-compact exotic objects unless one assumes they totally decouple from the standard model or new heavy fields. Hence, using solutions of vacuum General Relativity to model ultra-compact exotic objects and their perturbations relies implicitly on this assumption or on the absence of such fields.","sentences":["In this note, we discuss the effect of light, non-gauge, bosonic degrees of freedom on the exterior spacetime of an exotic compact object.","We show that such fields generically introduce large deviations from spacetimes of vacuum General Relativity near and outside the surfaces of ultra-compact exotic objects unless one assumes they totally decouple from the standard model or new heavy fields.","Hence, using solutions of vacuum General Relativity to model ultra-compact exotic objects and their perturbations relies implicitly on this assumption or on the absence of such fields."],"url":"http://arxiv.org/abs/2405.10287v1","category":"gr-qc"}
{"created":"2024-05-16 17:46:54","title":"FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models","abstract":"Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity. Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs. Finally, we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition ($\\sim +6\\%$ on average over 11 datasets) and image retrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO).","sentences":["Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized.","Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity.","Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs.","Finally, we propose training with sigmoid loss to address such a requirement.","We show very large gains over the current state-of-the-art for both image recognition ($\\sim +6\\%$ on average over 11 datasets) and image retrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO)."],"url":"http://arxiv.org/abs/2405.10286v1","category":"cs.CV"}
{"created":"2024-05-16 17:46:12","title":"Interacting chiral fermions on the lattice with matrix product operator norms","abstract":"We develop a formalism for simulating one-dimensional interacting chiral fermions on the lattice without breaking any local symmetries by defining a Fock space endowed with a semi-definite norm defined in terms of matrix product operators. This formalism can be understood as a second-quantized form of Stacey fermions, hence providing a possible solution for the fermion doubling problem and circumventing the Nielsen-Ninomiya theorem. We prove that the emerging theory is hermitian by virtue of the fact that it gives rise to a hermitian generalized eigenvalue problem and that it has local features as it can be simulated using tensor network methods similar to the ones used for simulating local quantum Hamiltonians. We also show that the scaling limit of the free model recovers the chiral fermion field. As a proof of principle, we consider a single Weyl fermion on a periodic ring with Hubbard-type nearest-neighbor interactions and construct a variational generalized DMRG code demonstrating that the ground states of the system for large system sizes can be determined efficiently.","sentences":["We develop a formalism for simulating one-dimensional interacting chiral fermions on the lattice without breaking any local symmetries by defining a Fock space endowed with a semi-definite norm defined in terms of matrix product operators.","This formalism can be understood as a second-quantized form of Stacey fermions, hence providing a possible solution for the fermion doubling problem and circumventing the Nielsen-Ninomiya theorem.","We prove that the emerging theory is hermitian by virtue of the fact that it gives rise to a hermitian generalized eigenvalue problem and that it has local features as it can be simulated using tensor network methods similar to the ones used for simulating local quantum Hamiltonians.","We also show that the scaling limit of the free model recovers the chiral fermion field.","As a proof of principle, we consider a single Weyl fermion on a periodic ring with Hubbard-type nearest-neighbor interactions and construct a variational generalized DMRG code demonstrating that the ground states of the system for large system sizes can be determined efficiently."],"url":"http://arxiv.org/abs/2405.10285v1","category":"hep-lat"}
{"created":"2024-05-16 17:41:09","title":"GKLS Vector Field Dynamics for Gaussian States","abstract":"We construct the vector field associated with the GKLS generator for systems described by Gaussian states. This vector field is defined on the dual space of the algebra of operators, restricted to operators quadratic in position and momentum. It is shown that the GKLS dynamics accepts a decomposition principle, that is, this vector field can be decomposed in three parts, a conservative Hamiltonian component, a gradient-like and a Choi-Krauss vector field. The last two terms are considered a \"perturbation\" associated with dissipation. Examples are presented for a harmonic oscillator with different dissipation terms.","sentences":["We construct the vector field associated with the GKLS generator for systems described by Gaussian states.","This vector field is defined on the dual space of the algebra of operators, restricted to operators quadratic in position and momentum.","It is shown that the GKLS dynamics accepts a decomposition principle, that is, this vector field can be decomposed in three parts, a conservative Hamiltonian component, a gradient-like and a Choi-Krauss vector field.","The last two terms are considered a \"perturbation\" associated with dissipation.","Examples are presented for a harmonic oscillator with different dissipation terms."],"url":"http://arxiv.org/abs/2405.10282v1","category":"quant-ph"}
{"created":"2024-05-16 17:36:03","title":"Photon emission from macroscopic currents","abstract":"Coherent states are a well-established tool of quantum optics to describe electromagnetic waves in terms of photons. However, they do not describe the near-field regime of radiation sources. Instead, we generically use classical solutions of Maxwell's equations to describe radiation in the near-field regime. The classical solutions provide linear relations between currents and emitted electromagnetic fields, whereas evolution of states at the quantum level proceeds through unitary time evolution operators involving photon operators. This begs questions how the classical radiation equations relate to unitary quantum evolution, and how we can describe macroscopic fields from antennas or magnetic coils in terms of elementary photons. The present paper answers both questions through the construction of generalized Glauber states for radiation emitters.","sentences":["Coherent states are a well-established tool of quantum optics to describe electromagnetic waves in terms of photons.","However, they do not describe the near-field regime of radiation sources.","Instead, we generically use classical solutions of Maxwell's equations to describe radiation in the near-field regime.","The classical solutions provide linear relations between currents and emitted electromagnetic fields, whereas evolution of states at the quantum level proceeds through unitary time evolution operators involving photon operators.","This begs questions how the classical radiation equations relate to unitary quantum evolution, and how we can describe macroscopic fields from antennas or magnetic coils in terms of elementary photons.","The present paper answers both questions through the construction of generalized Glauber states for radiation emitters."],"url":"http://arxiv.org/abs/2405.10279v1","category":"quant-ph"}
{"created":"2024-05-16 17:34:37","title":"Hilbert Functions and Low-Degree Randomness Extractors","abstract":"For $S\\subseteq \\mathbb{F}^n$, consider the linear space of restrictions of degree-$d$ polynomials to $S$. The Hilbert function of $S$, denoted $\\mathrm{h}_S(d,\\mathbb{F})$, is the dimension of this space. We obtain a tight lower bound on the smallest value of the Hilbert function of subsets $S$ of arbitrary finite grids in $\\mathbb{F}^n$ with a fixed size $|S|$. We achieve this by proving that this value coincides with a combinatorial quantity, namely the smallest number of low Hamming weight points in a down-closed set of size $|S|$.   Understanding the smallest values of Hilbert functions is closely related to the study of degree-$d$ closure of sets, a notion introduced by Nie and Wang (Journal of Combinatorial Theory, Series A, 2015). We use bounds on the Hilbert function to obtain a tight bound on the size of degree-$d$ closures of subsets of $\\mathbb{F}_q^n$, which answers a question posed by Doron, Ta-Shma, and Tell (Computational Complexity, 2022).   We use the bounds on the Hilbert function and degree-$d$ closure of sets to prove that a random low-degree polynomial is an extractor for samplable randomness sources. Most notably, we prove the existence of low-degree extractors and dispersers for sources generated by constant-degree polynomials and polynomial-size circuits. Until recently, even the existence of arbitrary deterministic extractors for such sources was not known.","sentences":["For $S\\subseteq \\mathbb{F}^n$, consider the linear space of restrictions of degree-$d$ polynomials to $S$. The Hilbert function of $S$, denoted $\\mathrm{h}_S(d,\\mathbb{F})$, is the dimension of this space.","We obtain a tight lower bound on the smallest value of the Hilbert function of subsets $S$ of arbitrary finite grids in $\\mathbb{F}^n$ with a fixed size $|S|$.","We achieve this by proving that this value coincides with a combinatorial quantity, namely the smallest number of low Hamming weight points in a down-closed set of size $|S|$.   Understanding the smallest values of Hilbert functions is closely related to the study of degree-$d$ closure of sets, a notion introduced by Nie and Wang (Journal of Combinatorial Theory, Series A, 2015).","We use bounds on the Hilbert function to obtain a tight bound on the size of degree-$d$ closures of subsets of $\\mathbb{F}_q^n$, which answers a question posed by Doron, Ta-Shma, and Tell (Computational Complexity, 2022).   ","We use the bounds on the Hilbert function and degree-$d$ closure of sets to prove that a random low-degree polynomial is an extractor for samplable randomness sources.","Most notably, we prove the existence of low-degree extractors and dispersers for sources generated by constant-degree polynomials and polynomial-size circuits.","Until recently, even the existence of arbitrary deterministic extractors for such sources was not known."],"url":"http://arxiv.org/abs/2405.10277v1","category":"cs.CC"}
{"created":"2024-05-16 17:29:37","title":"Faces that Speak: Jointly Synthesising Talking Face and Speech from Text","abstract":"The goal of this work is to simultaneously generate natural talking faces and speech outputs from text. We achieve this by integrating Talking Face Generation (TFG) and Text-to-Speech (TTS) systems into a unified framework. We address the main challenges of each task: (1) generating a range of head poses representative of real-world scenarios, and (2) ensuring voice consistency despite variations in facial motion for the same identity. To tackle these issues, we introduce a motion sampler based on conditional flow matching, which is capable of high-quality motion code generation in an efficient way. Moreover, we introduce a novel conditioning method for the TTS system, which utilises motion-removed features from the TFG model to yield uniform speech outputs. Our extensive experiments demonstrate that our method effectively creates natural-looking talking faces and speech that accurately match the input text. To our knowledge, this is the first effort to build a multimodal synthesis system that can generalise to unseen identities.","sentences":["The goal of this work is to simultaneously generate natural talking faces and speech outputs from text.","We achieve this by integrating Talking Face Generation (TFG) and Text-to-Speech (TTS) systems into a unified framework.","We address the main challenges of each task: (1) generating a range of head poses representative of real-world scenarios, and (2) ensuring voice consistency despite variations in facial motion for the same identity.","To tackle these issues, we introduce a motion sampler based on conditional flow matching, which is capable of high-quality motion code generation in an efficient way.","Moreover, we introduce a novel conditioning method for the TTS system, which utilises motion-removed features from the TFG model to yield uniform speech outputs.","Our extensive experiments demonstrate that our method effectively creates natural-looking talking faces and speech that accurately match the input text.","To our knowledge, this is the first effort to build a multimodal synthesis system that can generalise to unseen identities."],"url":"http://arxiv.org/abs/2405.10272v1","category":"cs.CV"}
{"created":"2024-05-16 17:27:41","title":"Automated Federated Learning via Informed Pruning","abstract":"Federated learning (FL) represents a pivotal shift in machine learning (ML) as it enables collaborative training of local ML models coordinated by a central aggregator, all without the need to exchange local data. However, its application on edge devices is hindered by limited computational capabilities and data communication challenges, compounded by the inherent complexity of Deep Learning (DL) models. Model pruning is identified as a key technique for compressing DL models on devices with limited resources. Nonetheless, conventional pruning techniques typically rely on manually crafted heuristics and demand human expertise to achieve a balance between model size, speed, and accuracy, often resulting in sub-optimal solutions.   In this study, we introduce an automated federated learning approach utilizing informed pruning, called AutoFLIP, which dynamically prunes and compresses DL models within both the local clients and the global server. It leverages a federated loss exploration phase to investigate model gradient behavior across diverse datasets and losses, providing insights into parameter significance. Our experiments showcase notable enhancements in scenarios with strong non-IID data, underscoring AutoFLIP's capacity to tackle computational constraints and achieve superior global convergence.","sentences":["Federated learning (FL) represents a pivotal shift in machine learning (ML) as it enables collaborative training of local ML models coordinated by a central aggregator, all without the need to exchange local data.","However, its application on edge devices is hindered by limited computational capabilities and data communication challenges, compounded by the inherent complexity of Deep Learning (DL) models.","Model pruning is identified as a key technique for compressing DL models on devices with limited resources.","Nonetheless, conventional pruning techniques typically rely on manually crafted heuristics and demand human expertise to achieve a balance between model size, speed, and accuracy, often resulting in sub-optimal solutions.   ","In this study, we introduce an automated federated learning approach utilizing informed pruning, called AutoFLIP, which dynamically prunes and compresses DL models within both the local clients and the global server.","It leverages a federated loss exploration phase to investigate model gradient behavior across diverse datasets and losses, providing insights into parameter significance.","Our experiments showcase notable enhancements in scenarios with strong non-IID data, underscoring AutoFLIP's capacity to tackle computational constraints and achieve superior global convergence."],"url":"http://arxiv.org/abs/2405.10271v1","category":"cs.LG"}
{"created":"2024-05-16 17:25:26","title":"Quadratic quasi-normal mode dependence on linear mode parity","abstract":"Quasi-normal modes (QNMs) uniquely describe the gravitational-wave ringdown of post-merger black holes. While the linear QNM regime has been extensively studied, recent work has highlighted the importance of second-perturbative-order, quadratic QNMs (QQNMs) arising from the nonlinear coupling of linear QNMs. Previous attempts to quantify the magnitude of these QQNMs have shown discrepant results. Using a new hyperboloidal framework, we resolve the discrepancy by showing that the QQNM/QNM ratio is a function not only of the black hole parameters but also of the ratio between even- and odd-parity linear QNMs: the ratio QQNM/QNM depends on what created the ringing black hole, but only through this ratio of even- to odd-parity linear perturbations.","sentences":["Quasi-normal modes (QNMs) uniquely describe the gravitational-wave ringdown of post-merger black holes.","While the linear QNM regime has been extensively studied, recent work has highlighted the importance of second-perturbative-order, quadratic QNMs (QQNMs) arising from the nonlinear coupling of linear QNMs.","Previous attempts to quantify the magnitude of these QQNMs have shown discrepant results.","Using a new hyperboloidal framework, we resolve the discrepancy by showing that the QQNM/QNM ratio is a function not only of the black hole parameters but also of the ratio between even- and odd-parity linear QNMs: the ratio QQNM/QNM depends on what created the ringing black hole, but only through this ratio of even- to odd-parity linear perturbations."],"url":"http://arxiv.org/abs/2405.10270v1","category":"gr-qc"}
{"created":"2024-05-16 17:19:58","title":"Sharpness-Aware Minimization in Genetic Programming","abstract":"Sharpness-Aware Minimization (SAM) was recently introduced as a regularization procedure for training deep neural networks. It simultaneously minimizes the fitness (or loss) function and the so-called fitness sharpness. The latter serves as a %connection between the geometry of the fitness landscape measure of the nonlinear behavior of a solution %and generalization and does so by finding solutions that lie in neighborhoods having uniformly similar loss values across all fitness cases. In this contribution, we adapt SAM for tree Genetic Programming (TGP) by exploring the semantic neighborhoods of solutions using two simple approaches By capitalizing upon perturbing input and output of program trees, sharpness can be estimated and used as a second optimization criterion during the evolution. To better understand the impact of this variant of SAM on TGP, we collect numerous indicators of the evolutionary process, including generalization ability, complexity, diversity, and a recently proposed genotype-phenotype mapping to study the amount of redundancy in trees. The experimental results demonstrate that using any of the two proposed SAM adaptations in TGP allows (i) a significant reduction of tree sizes in the population and (ii) a decrease in redundancy of the trees. When assessed on real-world benchmarks, the generalization ability of the elite solutions does not deteriorate.","sentences":["Sharpness-Aware Minimization (SAM) was recently introduced as a regularization procedure for training deep neural networks.","It simultaneously minimizes the fitness (or loss) function and the so-called fitness sharpness.","The latter serves as a %connection between the geometry of the fitness landscape measure of the nonlinear behavior of a solution %and generalization and does so by finding solutions that lie in neighborhoods having uniformly similar loss values across all fitness cases.","In this contribution, we adapt SAM for tree Genetic Programming (TGP) by exploring the semantic neighborhoods of solutions using two simple approaches By capitalizing upon perturbing input and output of program trees, sharpness can be estimated and used as a second optimization criterion during the evolution.","To better understand the impact of this variant of SAM on TGP, we collect numerous indicators of the evolutionary process, including generalization ability, complexity, diversity, and a recently proposed genotype-phenotype mapping to study the amount of redundancy in trees.","The experimental results demonstrate that using any of the two proposed SAM adaptations in TGP allows (i) a significant reduction of tree sizes in the population and (ii) a decrease in redundancy of the trees.","When assessed on real-world benchmarks, the generalization ability of the elite solutions does not deteriorate."],"url":"http://arxiv.org/abs/2405.10267v1","category":"cs.NE"}
{"created":"2024-05-16 17:15:39","title":"Architectures and random properties of symplectic quantum circuits","abstract":"Parametrized and random unitary (or orthogonal) $n$-qubit circuits play a central role in quantum information. As such, one could naturally assume that circuits implementing symplectic transformation would attract similar attention. However, this is not the case, as $\\mathbb{SP}(d/2)$ -- the group of $d\\times d$ unitary symplectic matrices -- has thus far been overlooked. In this work, we aim at starting to right this wrong. We begin by presenting a universal set of generators $\\mathcal{G}$ for the symplectic algebra $i\\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators acting on neighboring sites in a one-dimensional lattice. Here, we uncover two critical differences between such set, and equivalent ones for unitary and orthogonal circuits. Namely, we find that the operators in $\\mathcal{G}$ cannot generate arbitrary local symplectic unitaries and that they are not translationally invariant. We then review the Schur-Weyl duality between the symplectic group and the Brauer algebra, and use tools from Weingarten calculus to prove that Pauli measurements at the output of Haar random symplectic circuits can converge to Gaussian processes. As a by-product, such analysis provides us with concentration bounds for Pauli measurements in circuits that form $t$-designs over $\\mathbb{SP}(d/2)$. To finish, we present tensor-network tools to analyze shallow random symplectic circuits, and we use these to numerically show that computational-basis measurements anti-concentrate at logarithmic depth.","sentences":["Parametrized and random unitary (or orthogonal) $n$-qubit circuits play a central role in quantum information.","As such, one could naturally assume that circuits implementing symplectic transformation would attract similar attention.","However, this is not the case, as $\\mathbb{SP}(d/2)$ -- the group of $d\\times d$ unitary symplectic matrices -- has thus far been overlooked.","In this work, we aim at starting to right this wrong.","We begin by presenting a universal set of generators $\\mathcal{G}$ for the symplectic algebra $i\\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators acting on neighboring sites in a one-dimensional lattice.","Here, we uncover two critical differences between such set, and equivalent ones for unitary and orthogonal circuits.","Namely, we find that the operators in $\\mathcal{G}$ cannot generate arbitrary local symplectic unitaries and that they are not translationally invariant.","We then review the Schur-Weyl duality between the symplectic group and the Brauer algebra, and use tools from Weingarten calculus to prove that Pauli measurements at the output of Haar random symplectic circuits can converge to Gaussian processes.","As a by-product, such analysis provides us with concentration bounds for Pauli measurements in circuits that form $t$-designs over $\\mathbb{SP}(d/2)$. To finish, we present tensor-network tools to analyze shallow random symplectic circuits, and we use these to numerically show that computational-basis measurements anti-concentrate at logarithmic depth."],"url":"http://arxiv.org/abs/2405.10264v1","category":"quant-ph"}
{"created":"2024-05-16 17:13:55","title":"On Partially Unitary Learning","abstract":"The problem of an optimal mapping between Hilbert spaces $IN$ of $\\left|\\psi\\right\\rangle$ and $OUT$ of $\\left|\\phi\\right\\rangle$ based on a set of wavefunction measurements (within a phase) $\\psi_l \\to \\phi_l$, $l=1\\dots M$, is formulated as an optimization problem maximizing the total fidelity $\\sum_{l=1}^{M} \\omega^{(l)} \\left|\\langle\\phi_l|\\mathcal{U}|\\psi_l\\rangle\\right|^2$ subject to probability preservation constraints on $\\mathcal{U}$ (partial unitarity). Constructed operator $\\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; it is a partially unitary rectangular matrix of the dimension $\\dim(OUT) \\times \\dim(IN)$ transforming operators as $A^{OUT}=\\mathcal{U} A^{IN} \\mathcal{U}^{\\dagger}$. An iteration algorithm finding the global maximum of this optimization problem is developed and it's application to a number of problems is demonstrated. A software product implementing the algorithm is available from the authors.","sentences":["The problem of an optimal mapping between Hilbert spaces $IN$ of $\\left|\\psi\\right\\rangle$ and $OUT$ of $\\left|\\phi\\right\\rangle$ based on a set of wavefunction measurements (within a phase) $\\psi_l \\to \\phi_l$, $l=1\\dots M$, is formulated as an optimization problem maximizing the total fidelity $\\sum_{l=1}^{M} \\omega^{(l)} \\left|\\langle\\phi_l|\\mathcal{U}|\\psi_l\\rangle\\right|^2$ subject to probability preservation constraints on $\\mathcal{U}$ (partial unitarity).","Constructed operator $\\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; it is a partially unitary rectangular matrix of the dimension $\\dim(OUT) \\times \\dim(IN)$ transforming operators as $A^{OUT}=\\mathcal{U} A^{IN} \\mathcal{U}^{\\dagger}$.","An iteration algorithm finding the global maximum of this optimization problem is developed and it's application to a number of problems is demonstrated.","A software product implementing the algorithm is available from the authors."],"url":"http://arxiv.org/abs/2405.10263v1","category":"cs.LG"}
{"created":"2024-05-16 17:13:25","title":"Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN Learning Over-Fitted Features","abstract":"This paper investigates the dynamics of a deep neural network (DNN) learning interactions. Previous studies have discovered and mathematically proven that given each input sample, a well-trained DNN usually only encodes a small number of interactions (non-linear relationships) between input variables in the sample. A series of theorems have been derived to prove that we can consider the DNN's inference equivalent to using these interactions as primitive patterns for inference. In this paper, we discover the DNN learns interactions in two phases. The first phase mainly penalizes interactions of medium and high orders, and the second phase mainly learns interactions of gradually increasing orders. We can consider the two-phase phenomenon as the starting point of a DNN learning over-fitted features. Such a phenomenon has been widely shared by DNNs with various architectures trained for different tasks. Therefore, the discovery of the two-phase dynamics provides a detailed mechanism for how a DNN gradually learns different inference patterns (interactions). In particular, we have also verified the claim that high-order interactions have weaker generalization power than low-order interactions. Thus, the discovered two-phase dynamics also explains how the generalization power of a DNN changes during the training process.","sentences":["This paper investigates the dynamics of a deep neural network (DNN) learning interactions.","Previous studies have discovered and mathematically proven that given each input sample, a well-trained DNN usually only encodes a small number of interactions (non-linear relationships) between input variables in the sample.","A series of theorems have been derived to prove that we can consider the DNN's inference equivalent to using these interactions as primitive patterns for inference.","In this paper, we discover the DNN learns interactions in two phases.","The first phase mainly penalizes interactions of medium and high orders, and the second phase mainly learns interactions of gradually increasing orders.","We can consider the two-phase phenomenon as the starting point of a DNN learning over-fitted features.","Such a phenomenon has been widely shared by DNNs with various architectures trained for different tasks.","Therefore, the discovery of the two-phase dynamics provides a detailed mechanism for how a DNN gradually learns different inference patterns (interactions).","In particular, we have also verified the claim that high-order interactions have weaker generalization power than low-order interactions.","Thus, the discovered two-phase dynamics also explains how the generalization power of a DNN changes during the training process."],"url":"http://arxiv.org/abs/2405.10262v1","category":"cs.LG"}
{"created":"2024-05-16 17:12:18","title":"Keep It Private: Unsupervised Privatization of Online Text","abstract":"Authorship obfuscation techniques hold the promise of helping people protect their privacy in online communications by automatically rewriting text to hide the identity of the original author. However, obfuscation has been evaluated in narrow settings in the NLP literature and has primarily been addressed with superficial edit operations that can lead to unnatural outputs. In this work, we introduce an automatic text privatization framework that fine-tunes a large language model via reinforcement learning to produce rewrites that balance soundness, sense, and privacy. We evaluate it extensively on a large-scale test set of English Reddit posts by 68k authors composed of short-medium length texts. We study how the performance changes among evaluative conditions including authorial profile length and authorship detection strategy. Our method maintains high text quality according to both automated metrics and human evaluation, and successfully evades several automated authorship attacks.","sentences":["Authorship obfuscation techniques hold the promise of helping people protect their privacy in online communications by automatically rewriting text to hide the identity of the original author.","However, obfuscation has been evaluated in narrow settings in the NLP literature and has primarily been addressed with superficial edit operations that can lead to unnatural outputs.","In this work, we introduce an automatic text privatization framework that fine-tunes a large language model via reinforcement learning to produce rewrites that balance soundness, sense, and privacy.","We evaluate it extensively on a large-scale test set of English Reddit posts by 68k authors composed of short-medium length texts.","We study how the performance changes among evaluative conditions including authorial profile length and authorship detection strategy.","Our method maintains high text quality according to both automated metrics and human evaluation, and successfully evades several automated authorship attacks."],"url":"http://arxiv.org/abs/2405.10260v1","category":"cs.CL"}
{"created":"2024-05-16 17:12:00","title":"Energy-limited quantum dynamics","abstract":"We consider quantum systems with energy constraints. In general, quantum channels and continuous-time dynamics need not satisfy energy conservation. Physically meaningful channels, however, can only introduce a finite amount of energy to the system, and continuous-time dynamics may only increase the energy gradually over time. We systematically study such \"energy-limited\" channels and dynamics. For Markovian dynamics, energy-limitedness is equivalent to a single operator inequality in the Heisenberg picture. By tracking the output energy, we observe that the energy-constrained operator and diamond norms of Shirokov and Winter satisfy submultiplicativity estimates with respect to energy-limited channels. This makes for a powerful toolkit for quantitative analyses of dynamical problems in finite and infinite-dimensional systems. As an application, we derive state-dependent bounds for quantum speed limits and related problems that outperform the usual operator/diamond norm estimates, which have to account for fluctuations in high-energy states.","sentences":["We consider quantum systems with energy constraints.","In general, quantum channels and continuous-time dynamics need not satisfy energy conservation.","Physically meaningful channels, however, can only introduce a finite amount of energy to the system, and continuous-time dynamics may only increase the energy gradually over time.","We systematically study such \"energy-limited\" channels and dynamics.","For Markovian dynamics, energy-limitedness is equivalent to a single operator inequality in the Heisenberg picture.","By tracking the output energy, we observe that the energy-constrained operator and diamond norms of Shirokov and Winter satisfy submultiplicativity estimates with respect to energy-limited channels.","This makes for a powerful toolkit for quantitative analyses of dynamical problems in finite and infinite-dimensional systems.","As an application, we derive state-dependent bounds for quantum speed limits and related problems that outperform the usual operator/diamond norm estimates, which have to account for fluctuations in high-energy states."],"url":"http://arxiv.org/abs/2405.10259v1","category":"quant-ph"}
{"created":"2024-05-16 16:59:58","title":"When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models","abstract":"As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D.","sentences":["As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces.","This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data.","Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems.","Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs).","It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation.","The paper also includes a brief review of other methods that integrate 3D and language.","The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs.","Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world.","To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D."],"url":"http://arxiv.org/abs/2405.10255v1","category":"cs.CV"}
{"created":"2024-05-16 16:59:12","title":"PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology","abstract":"Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine. However, there is a mismatch between most clinical analysis, which is defined at the level of one or more whole slide images, and foundation models to date, which process the thousands of image tiles contained in a whole slide image separately. The requirement to train a network to aggregate information across a large number of tiles in multiple whole slide images limits these models' impact. In this work, we present a slide-level foundation model for H&E-stained histopathology, PRISM, that builds on Virchow tile embeddings and leverages clinical report text for pre-training. Using the tile embeddings, PRISM produces slide-level embeddings with the ability to generate clinical reports, resulting in several modes of use. Using text prompts, PRISM achieves zero-shot cancer detection and sub-typing performance approaching and surpassing that of a supervised aggregator model. Using the slide embeddings with linear classifiers, PRISM surpasses supervised aggregator models. Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yields label-efficient training for biomarker prediction, a task that typically suffers from low availability of training data; an aggregator initialized with PRISM and trained on as little as 10% of the training data can outperform a supervised baseline that uses all of the data.","sentences":["Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine.","However, there is a mismatch between most clinical analysis, which is defined at the level of one or more whole slide images, and foundation models to date, which process the thousands of image tiles contained in a whole slide image separately.","The requirement to train a network to aggregate information across a large number of tiles in multiple whole slide images limits these models' impact.","In this work, we present a slide-level foundation model for H&E-stained histopathology, PRISM, that builds on Virchow tile embeddings and leverages clinical report text for pre-training.","Using the tile embeddings, PRISM produces slide-level embeddings with the ability to generate clinical reports, resulting in several modes of use.","Using text prompts, PRISM achieves zero-shot cancer detection and sub-typing performance approaching and surpassing that of a supervised aggregator model.","Using the slide embeddings with linear classifiers, PRISM surpasses supervised aggregator models.","Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yields label-efficient training for biomarker prediction, a task that typically suffers from low availability of training data; an aggregator initialized with PRISM and trained on as little as 10% of the training data can outperform a supervised baseline that uses all of the data."],"url":"http://arxiv.org/abs/2405.10254v1","category":"eess.IV"}
{"created":"2024-05-16 16:56:54","title":"A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks","abstract":"Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance of LLMs in natural language generation (NLG) tasks, a pivotal criterion for determining model excellence. Thus, this paper conducts a comprehensive evaluation of well-known and high-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models, in the context of NLG tasks. We select English and Chinese datasets encompassing Dialogue Generation and Text Summarization. Moreover, we propose a common evaluation setting that incorporates input templates and post-processing strategies. Our study reports both automatic results, accompanied by a detailed analysis.","sentences":["Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation.","However, to the best of our knowledge, no work has specifically investigated the performance of LLMs in natural language generation (NLG) tasks, a pivotal criterion for determining model excellence.","Thus, this paper conducts a comprehensive evaluation of well-known and high-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models, in the context of NLG tasks.","We select English and Chinese datasets encompassing Dialogue Generation and Text Summarization.","Moreover, we propose a common evaluation setting that incorporates input templates and post-processing strategies.","Our study reports both automatic results, accompanied by a detailed analysis."],"url":"http://arxiv.org/abs/2405.10251v1","category":"cs.CL"}
{"created":"2024-05-16 16:55:06","title":"IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers","abstract":"Large language models (LLMs) have exhibited a strong promise in automatically generating executable code from natural language descriptions, particularly with interactive features that allow users to engage in the code-generation process by instructing the LLM with iterative feedback. However, existing interaction paradigms often assume that users have expert knowledge to debug source code and are not optimized for non-professional programmers' use. This raises challenges in making interactive code generation more accessible for individuals with varying levels of programming expertise. To tackle these challenges, we present IntelliExplain, which offers a novel human-LLM interaction paradigm to enhance non-professional programmers' experience by enabling them to interact with source code via natural language explanations. Users interact with IntelliExplain by providing natural language corrective feedback on errors they identify from the explanations. Feedback is used by the system to revise the code, until the user is satisfied with explanations by the system of the code. Our user study demonstrates that users with IntelliExplain achieve a significantly higher success rate 11.6% and 25.3% better than with vanilla GPT-3.5, while also requiring 39.0% and 15.6% less time in Text-to-SQL and Python code generation tasks, respectively.","sentences":["Large language models (LLMs) have exhibited a strong promise in automatically generating executable code from natural language descriptions, particularly with interactive features that allow users to engage in the code-generation process by instructing the LLM with iterative feedback.","However, existing interaction paradigms often assume that users have expert knowledge to debug source code and are not optimized for non-professional programmers' use.","This raises challenges in making interactive code generation more accessible for individuals with varying levels of programming expertise.","To tackle these challenges, we present IntelliExplain, which offers a novel human-LLM interaction paradigm to enhance non-professional programmers' experience by enabling them to interact with source code via natural language explanations.","Users interact with IntelliExplain by providing natural language corrective feedback on errors they identify from the explanations.","Feedback is used by the system to revise the code, until the user is satisfied with explanations by the system of the code.","Our user study demonstrates that users with IntelliExplain achieve a significantly higher success rate 11.6% and 25.3% better than with vanilla GPT-3.5, while also requiring 39.0% and 15.6% less time in Text-to-SQL and Python code generation tasks, respectively."],"url":"http://arxiv.org/abs/2405.10250v1","category":"cs.HC"}
{"created":"2024-05-16 16:49:20","title":"A Foundation Model for Brain Lesion Segmentation with Mixture of Modality Experts","abstract":"Brain lesion segmentation plays an essential role in neurological research and diagnosis. As brain lesions can be caused by various pathological alterations, different types of brain lesions tend to manifest with different characteristics on different imaging modalities. Due to this complexity, brain lesion segmentation methods are often developed in a task-specific manner. A specific segmentation model is developed for a particular lesion type and imaging modality. However, the use of task-specific models requires predetermination of the lesion type and imaging modality, which complicates their deployment in real-world scenarios. In this work, we propose a universal foundation model for 3D brain lesion segmentation, which can automatically segment different types of brain lesions for input data of various imaging modalities. We formulate a novel Mixture of Modality Experts (MoME) framework with multiple expert networks attending to different imaging modalities. A hierarchical gating network combines the expert predictions and fosters expertise collaboration. Furthermore, we introduce a curriculum learning strategy during training to avoid the degeneration of each expert network and preserve their specialization. We evaluated the proposed method on nine brain lesion datasets, encompassing five imaging modalities and eight lesion types. The results show that our model outperforms state-of-the-art universal models and provides promising generalization to unseen datasets.","sentences":["Brain lesion segmentation plays an essential role in neurological research and diagnosis.","As brain lesions can be caused by various pathological alterations, different types of brain lesions tend to manifest with different characteristics on different imaging modalities.","Due to this complexity, brain lesion segmentation methods are often developed in a task-specific manner.","A specific segmentation model is developed for a particular lesion type and imaging modality.","However, the use of task-specific models requires predetermination of the lesion type and imaging modality, which complicates their deployment in real-world scenarios.","In this work, we propose a universal foundation model for 3D brain lesion segmentation, which can automatically segment different types of brain lesions for input data of various imaging modalities.","We formulate a novel Mixture of Modality Experts (MoME) framework with multiple expert networks attending to different imaging modalities.","A hierarchical gating network combines the expert predictions and fosters expertise collaboration.","Furthermore, we introduce a curriculum learning strategy during training to avoid the degeneration of each expert network and preserve their specialization.","We evaluated the proposed method on nine brain lesion datasets, encompassing five imaging modalities and eight lesion types.","The results show that our model outperforms state-of-the-art universal models and provides promising generalization to unseen datasets."],"url":"http://arxiv.org/abs/2405.10246v1","category":"eess.IV"}
{"created":"2024-05-16 16:46:46","title":"DocuMint: Docstring Generation for Python using Small Language Models","abstract":"Effective communication, specifically through documentation, is the beating heart of collaboration among contributors in software development. Recent advancements in language models (LMs) have enabled the introduction of a new type of actor in that ecosystem: LM-powered assistants capable of code generation, optimization, and maintenance. Our study investigates the efficacy of small language models (SLMs) for generating high-quality docstrings by assessing accuracy, conciseness, and clarity, benchmarking performance quantitatively through mathematical formulas and qualitatively through human evaluation using Likert scale. Further, we introduce DocuMint, as a large-scale supervised fine-tuning dataset with 100,000 samples. In quantitative experiments, Llama 3 8B achieved the best performance across all metrics, with conciseness and clarity scores of 0.605 and 64.88, respectively. However, under human evaluation, CodeGemma 7B achieved the highest overall score with an average of 8.3 out of 10 across all metrics. Fine-tuning the CodeGemma 2B model using the DocuMint dataset led to significant improvements in performance across all metrics, with gains of up to 22.5% in conciseness. The fine-tuned model and the dataset can be found in HuggingFace and the code can be found in the repository.","sentences":["Effective communication, specifically through documentation, is the beating heart of collaboration among contributors in software development.","Recent advancements in language models (LMs) have enabled the introduction of a new type of actor in that ecosystem: LM-powered assistants capable of code generation, optimization, and maintenance.","Our study investigates the efficacy of small language models (SLMs) for generating high-quality docstrings by assessing accuracy, conciseness, and clarity, benchmarking performance quantitatively through mathematical formulas and qualitatively through human evaluation using Likert scale.","Further, we introduce DocuMint, as a large-scale supervised fine-tuning dataset with 100,000 samples.","In quantitative experiments, Llama 3 8B achieved the best performance across all metrics, with conciseness and clarity scores of 0.605 and 64.88, respectively.","However, under human evaluation, CodeGemma 7B achieved the highest overall score with an average of 8.3 out of 10 across all metrics.","Fine-tuning the CodeGemma 2B model using the DocuMint dataset led to significant improvements in performance across all metrics, with gains of up to 22.5% in conciseness.","The fine-tuned model and the dataset can be found in HuggingFace and the code can be found in the repository."],"url":"http://arxiv.org/abs/2405.10243v1","category":"cs.SE"}
{"created":"2024-05-16 16:39:03","title":"An introduction to map-making for CMB experiments","abstract":"The cosmic microwave background (CMB) anisotropies are a powerful probe of the early universe, and have largely contributed to establishing the current standard cosmological model. To extract the information encoded in those tiny variations, one must first compress the raw, time-domain data collected by a telescope into maps of the sky at the observed frequencies, in a procedure known as map-making. I provide a general introduction to this problem, and highlight a few specificities of the MAPPRAISER implementation.","sentences":["The cosmic microwave background (CMB) anisotropies are a powerful probe of the early universe, and have largely contributed to establishing the current standard cosmological model.","To extract the information encoded in those tiny variations, one must first compress the raw, time-domain data collected by a telescope into maps of the sky at the observed frequencies, in a procedure known as map-making.","I provide a general introduction to this problem, and highlight a few specificities of the MAPPRAISER implementation."],"url":"http://arxiv.org/abs/2405.10239v1","category":"astro-ph.CO"}
{"created":"2024-05-16 16:34:06","title":"Boone--Higman Embeddings for Contracting Self-Similar Groups","abstract":"We give a short proof that every contracting self-similar group embeds into a finitely presented simple group. In particular, any contracting self-similar group embeds into the corresponding R\\\"over--Nekrashevych group, and this in turn embeds into one of the twisted Brin--Thompson groups introduced by the first author and Matthew Zaremsky. The proof here is a simplification of a more general argument given by the authors, Collin Bleak, and Matthew Zaremsky for contracting rational similarity groups.","sentences":["We give a short proof that every contracting self-similar group embeds into a finitely presented simple group.","In particular, any contracting self-similar group embeds into the corresponding R\\\"over--Nekrashevych group, and this in turn embeds into one of the twisted Brin--Thompson groups introduced by the first author and Matthew Zaremsky.","The proof here is a simplification of a more general argument given by the authors, Collin Bleak, and Matthew Zaremsky for contracting rational similarity groups."],"url":"http://arxiv.org/abs/2405.10234v1","category":"math.GR"}
{"created":"2024-05-16 16:34:03","title":"iDRAMA-Scored-2024: A Dataset of the Scored Social Media Platform from 2020 to 2023","abstract":"Online web communities often face bans for violating platform policies, encouraging their migration to alternative platforms. This migration, however, can result in increased toxicity and unforeseen consequences on the new platform. In recent years, researchers have collected data from many alternative platforms, indicating coordinated efforts leading to offline events, conspiracy movements, hate speech propagation, and harassment. Thus, it becomes crucial to characterize and understand these alternative platforms. To advance research in this direction, we collect and release a large-scale dataset from Scored -- an alternative Reddit platform that sheltered banned fringe communities, for example, c/TheDonald (a prominent right-wing community) and c/GreatAwakening (a conspiratorial community). Over four years, we collected approximately 57M posts from Scored, with at least 58 communities identified as migrating from Reddit and over 950 communities created since the platform's inception. Furthermore, we provide sentence embeddings of all posts in our dataset, generated through a state-of-the-art model, to further advance the field in characterizing the discussions within these communities. We aim to provide these resources to facilitate their investigations without the need for extensive data collection and processing efforts.","sentences":["Online web communities often face bans for violating platform policies, encouraging their migration to alternative platforms.","This migration, however, can result in increased toxicity and unforeseen consequences on the new platform.","In recent years, researchers have collected data from many alternative platforms, indicating coordinated efforts leading to offline events, conspiracy movements, hate speech propagation, and harassment.","Thus, it becomes crucial to characterize and understand these alternative platforms.","To advance research in this direction, we collect and release a large-scale dataset from Scored -- an alternative Reddit platform that sheltered banned fringe communities, for example, c/TheDonald (a prominent right-wing community) and c/GreatAwakening (a conspiratorial community).","Over four years, we collected approximately 57M posts from Scored, with at least 58 communities identified as migrating from Reddit and over 950 communities created since the platform's inception.","Furthermore, we provide sentence embeddings of all posts in our dataset, generated through a state-of-the-art model, to further advance the field in characterizing the discussions within these communities.","We aim to provide these resources to facilitate their investigations without the need for extensive data collection and processing efforts."],"url":"http://arxiv.org/abs/2405.10233v1","category":"cs.SI"}
{"created":"2024-05-16 16:33:34","title":"Beyond Static Calibration: The Impact of User Preference Dynamics on Calibrated Recommendation","abstract":"Calibration in recommender systems is an important performance criterion that ensures consistency between the distribution of user preference categories and that of recommendations generated by the system. Standard methods for mitigating miscalibration typically assume that user preference profiles are static, and they measure calibration relative to the full history of user's interactions, including possibly outdated and stale preference categories. We conjecture that this approach can lead to recommendations that, while appearing calibrated, in fact, distort users' true preferences. In this paper, we conduct a preliminary investigation of recommendation calibration at a more granular level, taking into account evolving user preferences. By analyzing differently sized training time windows from the most recent interactions to the oldest, we identify the most relevant segment of user's preferences that optimizes the calibration metric. We perform an exploratory analysis with datasets from different domains with distinctive user-interaction characteristics. We demonstrate how the evolving nature of user preferences affects recommendation calibration, and how this effect is manifested differently depending on the characteristics of the data in a given domain. Datasets, codes, and more detailed experimental results are available at: https://github.com/nicolelin13/DynamicCalibrationUMAP.","sentences":["Calibration in recommender systems is an important performance criterion that ensures consistency between the distribution of user preference categories and that of recommendations generated by the system.","Standard methods for mitigating miscalibration typically assume that user preference profiles are static, and they measure calibration relative to the full history of user's interactions, including possibly outdated and stale preference categories.","We conjecture that this approach can lead to recommendations that, while appearing calibrated, in fact, distort users' true preferences.","In this paper, we conduct a preliminary investigation of recommendation calibration at a more granular level, taking into account evolving user preferences.","By analyzing differently sized training time windows from the most recent interactions to the oldest, we identify the most relevant segment of user's preferences that optimizes the calibration metric.","We perform an exploratory analysis with datasets from different domains with distinctive user-interaction characteristics.","We demonstrate how the evolving nature of user preferences affects recommendation calibration, and how this effect is manifested differently depending on the characteristics of the data in a given domain.","Datasets, codes, and more detailed experimental results are available at: https://github.com/nicolelin13/DynamicCalibrationUMAP."],"url":"http://arxiv.org/abs/2405.10232v1","category":"cs.IR"}
{"created":"2024-05-16 16:29:49","title":"Influencer Cartels","abstract":"Social media influencers account for a growing share of marketing worldwide. We demonstrate the existence of a novel form of market failure in this advertising market: influencer cartels, where groups of influencers collude to increase their advertising revenue by inflating their engagement. Our theoretical model shows that influencer cartels can improve consumer welfare if they expand social media engagement to the target audience, or reduce welfare if they divert engagement to less relevant audiences. We validate the model empirically using novel data on influencer cartels combined with machine learning tools, and derive policy implications for how to maximize consumer welfare.","sentences":["Social media influencers account for a growing share of marketing worldwide.","We demonstrate the existence of a novel form of market failure in this advertising market: influencer cartels, where groups of influencers collude to increase their advertising revenue by inflating their engagement.","Our theoretical model shows that influencer cartels can improve consumer welfare if they expand social media engagement to the target audience, or reduce welfare if they divert engagement to less relevant audiences.","We validate the model empirically using novel data on influencer cartels combined with machine learning tools, and derive policy implications for how to maximize consumer welfare."],"url":"http://arxiv.org/abs/2405.10231v1","category":"econ.GN"}
{"created":"2024-05-16 16:27:13","title":"The fermionic double smeared null energy condition","abstract":"Energy conditions are crucial for understanding why exotic phenomena such as traversable wormholes and closed timelike curves remain elusive. In this paper, we prove the Double Smeared Null Energy Condition (DSNEC) for the fermionic free theory in 4-dimensional flat Minkowski space-time, extending previous work on the same energy condition for the bosonic case [1][2] by adapting Fewster and Mistry's method [3] to the energy-momentum tensor $T_{++}$. A notable difference from previous works lies in the presence of the $\\gamma_0 \\gamma_+$ matrix in $T_{++}$, causing a loss of symmetry. This challenge is addressed by making use of its square-root matrix. We provide explicit analytic results for the massless case as well as numerical insights for the mass-dependence of the bound in the case of Gaussian smearing.","sentences":["Energy conditions are crucial for understanding why exotic phenomena such as traversable wormholes and closed timelike curves remain elusive.","In this paper, we prove the Double Smeared Null Energy Condition (DSNEC) for the fermionic free theory in 4-dimensional flat Minkowski space-time, extending previous work on the same energy condition for the bosonic case [1][2] by adapting Fewster and Mistry's method [3] to the energy-momentum tensor $T_{++}$. A notable difference from previous works lies in the presence of the $\\gamma_0 \\gamma_+$ matrix in $T_{++}$, causing a loss of symmetry.","This challenge is addressed by making use of its square-root matrix.","We provide explicit analytic results for the massless case as well as numerical insights for the mass-dependence of the bound in the case of Gaussian smearing."],"url":"http://arxiv.org/abs/2405.10228v1","category":"gr-qc"}
{"created":"2024-05-16 16:25:14","title":"Experimental Validation of Collision-Radiation Dataset for Molecular Hydrogen in Plasmas","abstract":"Quantitative spectroscopy of molecular hydrogen has generated substantial demand, leading to the accumulation of diverse elementary-process data encompassing radiative transitions, electron-impact transitions, predissociations, and quenching. However, their rates currently available are still sparse and there are inconsistencies among those proposed by different authors. In this study, we demonstrate an experimental validation of such molecular dataset by composing a collisional-radiative model (CRM) for molecular hydrogen and comparing experimentally-obtained vibronic populations across multiple levels. From the population kinetics of molecular hydrogen, the importance of each elementary process in various parameter space is studied. In low-density plasmas (electron density $n_\\mathrm{e} \\lesssim 10^{17}\\;\\mathrm{m^{-3}}$) the excitation rates from the ground states and radiative decay rates, both of which have been reported previously, determines the excited state population. The inconsistency in the excitation rates affects the population distribution the most significantly in this parameter space. On the other hand, in higher density plasmas ($n_\\mathrm{e} \\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$), the excitation rates \\textit{from} excited states become important, which have never been reported in the literature, and may need to be approximated in some way. In order to validate these molecular datasets and approximated rates, we carried out experimental observations for two different hydrogen plasmas; a low-density radio-frequency (RF) heated plasma ($n_\\mathrm{e}\\approx 10^{16}\\;\\mathrm{m^{-3}}$) and the Large Helical Device (LHD) divertor plasma ($n_\\mathrm{e}\\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$)... [continued]","sentences":["Quantitative spectroscopy of molecular hydrogen has generated substantial demand, leading to the accumulation of diverse elementary-process data encompassing radiative transitions, electron-impact transitions, predissociations, and quenching.","However, their rates currently available are still sparse and there are inconsistencies among those proposed by different authors.","In this study, we demonstrate an experimental validation of such molecular dataset by composing a collisional-radiative model (CRM) for molecular hydrogen and comparing experimentally-obtained vibronic populations across multiple levels.","From the population kinetics of molecular hydrogen, the importance of each elementary process in various parameter space is studied.","In low-density plasmas (electron density $n_\\mathrm{e} \\lesssim 10^{17}\\;\\mathrm{m^{-3}}$) the excitation rates from the ground states and radiative decay rates, both of which have been reported previously, determines the excited state population.","The inconsistency in the excitation rates affects the population distribution the most significantly in this parameter space.","On the other hand, in higher density plasmas ($n_\\mathrm{e} \\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$), the excitation rates \\textit{from} excited states become important, which have never been reported in the literature, and may need to be approximated in some way.","In order to validate these molecular datasets and approximated rates, we carried out experimental observations for two different hydrogen plasmas; a low-density radio-frequency (RF) heated plasma ($n_\\mathrm{e}\\approx 10^{16}\\;\\mathrm{m^{-3}}$) and the Large Helical Device (LHD) divertor plasma ($n_\\mathrm{e}\\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$)...","[continued]"],"url":"http://arxiv.org/abs/2405.10227v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 16:24:36","title":"Geometric phase amplification in a clock interferometer for enhanced metrology","abstract":"High-precision measurements are crucial for testing the fundamental laws of nature and for advancing the technological frontier. Clock interferometry, where particles with an internal clock are coherently split and recombined along two spatial paths, has sparked significant interest due to its fundamental implications, especially at the intersection of quantum mechanics and general relativity. Here, we demonstrate that a clock interferometer provides metrological improvement with respect to its technical-noise-limited counterpart employing a single internal quantum state. This enhancement around a critical working point can be interpreted as a geometric-phase-induced signal-to-noise ratio gain. In our experimental setup, we infer a precision enhancement of 8.8 decibels when measuring a small difference between external fields. We estimate that tens of decibels of precision enhancement could be attained for measurements with a higher atom flux. This opens the door to the development of a superior probe for fundamental physics as well as a high-performance sensor for various technological applications.","sentences":["High-precision measurements are crucial for testing the fundamental laws of nature and for advancing the technological frontier.","Clock interferometry, where particles with an internal clock are coherently split and recombined along two spatial paths, has sparked significant interest due to its fundamental implications, especially at the intersection of quantum mechanics and general relativity.","Here, we demonstrate that a clock interferometer provides metrological improvement with respect to its technical-noise-limited counterpart employing a single internal quantum state.","This enhancement around a critical working point can be interpreted as a geometric-phase-induced signal-to-noise ratio gain.","In our experimental setup, we infer a precision enhancement of 8.8 decibels when measuring a small difference between external fields.","We estimate that tens of decibels of precision enhancement could be attained for measurements with a higher atom flux.","This opens the door to the development of a superior probe for fundamental physics as well as a high-performance sensor for various technological applications."],"url":"http://arxiv.org/abs/2405.10226v1","category":"quant-ph"}
{"created":"2024-05-16 16:18:35","title":"GDPR: Is it worth it? Perceptions of workers who have experienced its implementation","abstract":"The General Data Protection Regulation (GDPR) remains the gold standard in privacy and security regulation. We investigate how the cost and effort required to implement GDPR is viewed by workers who have also experienced the regulations' benefits as citizens: is it worth it? In a multi-stage study, we survey N = 273 & 102 individuals who remained working in the same companies before, during, and after the implementation of GDPR. The survey finds that participants recognise their rights when prompted but know little about their regulator. They have observed concrete changes to data practices in their workplaces and appreciate the trade-offs. They take comfort that their personal data is handled as carefully as their employers' client data. The very people who comply with and execute the GDPR consider it to be positive for their company, positive for privacy and not a pointless, bureaucratic regulation. This is rare as it contradicts the conventional negative narrative about regulation. Policymakers may wish to build upon this public support while it lasts and consider early feedback from a similar dual professional-consumer group as the GDPR evolves.","sentences":["The General Data Protection Regulation (GDPR) remains the gold standard in privacy and security regulation.","We investigate how the cost and effort required to implement GDPR is viewed by workers who have also experienced the regulations' benefits as citizens: is it worth it?","In a multi-stage study, we survey N = 273 & 102 individuals who remained working in the same companies before, during, and after the implementation of GDPR.","The survey finds that participants recognise their rights when prompted but know little about their regulator.","They have observed concrete changes to data practices in their workplaces and appreciate the trade-offs.","They take comfort that their personal data is handled as carefully as their employers' client data.","The very people who comply with and execute the GDPR consider it to be positive for their company, positive for privacy and not a pointless, bureaucratic regulation.","This is rare as it contradicts the conventional negative narrative about regulation.","Policymakers may wish to build upon this public support while it lasts and consider early feedback from a similar dual professional-consumer group as the GDPR evolves."],"url":"http://arxiv.org/abs/2405.10225v1","category":"cs.CY"}
{"created":"2024-05-16 16:13:13","title":"Distance inequalities of generalized $k$-intersection bodies","abstract":"It was shown in [11] that for every symmetric star body $K \\subseteq \\mathbb R^n$ of volume $1$, every even continuous probability density $f$ on $K$ and $1 \\leq k \\leq n-1$, there exists a subspace $F \\subseteq \\mathbb R^n$ of codimension $k$ such that   \\[   \\int_{K \\cap F} f \\geq c^k (d_{\\rm ovr}(K, \\mathcal{BP}_k^n))^{-k}   \\]   where $d_{\\rm ovr}(K, \\mathcal{BP}_k^n)$ is the outer volume ratio distance from $K$ to the class of generalized $k$-intersection bodies, and $c>0$ is a universal constant.   The upper bound $d_{\\rm ovr}(K, \\mathcal{BP}_k^n) \\leq c' \\sqrt{n/k} \\left(\\log\\left(\\frac{en}k\\right)\\right)^{3/2}$ was established in [13] for every convex body $K$.   In this note we show that there exist a symmetric convex body $K$ of volume $1$ and an even continuous probability density $f$ supported on $K$ such that   \\[   \\int_{K \\cap F} f \\leq \\left( c \\sqrt{\\frac n{k \\log(n)} } \\right)^{-k}.   \\]   As a consequence we obtain a lower bound for $d_{\\rm ovr}(K, \\mathcal{BP}_k^n)$ with $K$ a convex body, complementing the upper bound in [13]. This is \\[c \\sqrt{n/k} (\\log(n))^{-1/2} \\leq \\sup_K d_{\\rm ovr}(K, \\mathcal{BP}_k^n) \\leq c' \\sqrt{n/k} \\left(\\log\\left(\\frac{en}k\\right)\\right)^{3/2}.\\]   The case $k=1$ was obtained previously in [5,6].","sentences":["It was shown in [11] that for every symmetric star body $K \\subseteq \\mathbb R^n$ of volume $1$, every even continuous probability density $f$ on $K$ and $1 \\leq k \\leq n-1$, there exists a subspace $F \\subseteq \\mathbb R^n$ of codimension $k$ such that   \\[   \\int_{K \\cap F} f \\geq c^k (d_{\\rm ovr}(K, \\mathcal{BP}_k^n))^{-k}   \\]   where $d_{\\rm ovr}(K, \\mathcal{BP}_k^n)$ is the outer volume ratio distance from $K$ to the class of generalized $k$-intersection bodies, and $c>0$ is a universal constant.   ","The upper bound $d_{\\rm ovr}(K, \\mathcal{BP}_k^n)","\\leq c' \\sqrt{n/k} \\left(\\log\\left(\\frac{en}k\\right)\\right)^{3/2}$ was established in [13] for every convex body $K$.   In this note we show that there exist a symmetric convex body $K$ of volume $1$ and an even continuous probability density $f$ supported on $K$ such that   \\[   \\int_{K \\cap F} f \\leq \\left( c \\sqrt{\\frac n{k \\log(n)} } \\right)^{-k}.   ","\\]   As a consequence we obtain a lower bound for $d_{\\rm ovr}(K, \\mathcal{BP}_k^n)$ with $K$ a convex body, complementing the upper bound in [13].","This is \\[c \\sqrt{n/k} (\\log(n))^{-1/2} \\leq \\sup_K d_{\\rm ovr}(K, \\mathcal{BP}_k^n)","\\leq c' \\sqrt{n/k} \\left(\\log\\left(\\frac{en}k\\right)\\right)^{3/2}.\\]   ","The case $k=1$ was obtained previously in [5,6]."],"url":"http://arxiv.org/abs/2405.10223v1","category":"math.MG"}
{"created":"2024-05-16 16:08:49","title":"ENADPool: The Edge-Node Attention-based Differentiable Pooling for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) are powerful tools for graph classification. One important operation for GNNs is the downsampling or pooling that can learn effective embeddings from the node representations. In this paper, we propose a new hierarchical pooling operation, namely the Edge-Node Attention-based Differentiable Pooling (ENADPool), for GNNs to learn effective graph representations. Unlike the classical hierarchical pooling operation that is based on the unclear node assignment and simply computes the averaged feature over the nodes of each cluster, the proposed ENADPool not only employs a hard clustering strategy to assign each node into an unique cluster, but also compress the node features as well as their edge connectivity strengths into the resulting hierarchical structure based on the attention mechanism after each pooling step. As a result, the proposed ENADPool simultaneously identifies the importance of different nodes within each separated cluster and edges between corresponding clusters, that significantly addresses the shortcomings of the uniform edge-node based structure information aggregation arising in the classical hierarchical pooling operation. Moreover, to mitigate the over-smoothing problem arising in existing GNNs, we propose a Multi-distance GNN (MD-GNN) model associated with the proposed ENADPool operation, allowing the nodes to actively and directly receive the feature information from neighbors at different random walk steps. Experiments demonstrate the effectiveness of the MD-GNN associated with the proposed ENADPool.","sentences":["Graph Neural Networks (GNNs) are powerful tools for graph classification.","One important operation for GNNs is the downsampling or pooling that can learn effective embeddings from the node representations.","In this paper, we propose a new hierarchical pooling operation, namely the Edge-Node Attention-based Differentiable Pooling (ENADPool), for GNNs to learn effective graph representations.","Unlike the classical hierarchical pooling operation that is based on the unclear node assignment and simply computes the averaged feature over the nodes of each cluster, the proposed ENADPool not only employs a hard clustering strategy to assign each node into an unique cluster, but also compress the node features as well as their edge connectivity strengths into the resulting hierarchical structure based on the attention mechanism after each pooling step.","As a result, the proposed ENADPool simultaneously identifies the importance of different nodes within each separated cluster and edges between corresponding clusters, that significantly addresses the shortcomings of the uniform edge-node based structure information aggregation arising in the classical hierarchical pooling operation.","Moreover, to mitigate the over-smoothing problem arising in existing GNNs, we propose a Multi-distance GNN (MD-GNN) model associated with the proposed ENADPool operation, allowing the nodes to actively and directly receive the feature information from neighbors at different random walk steps.","Experiments demonstrate the effectiveness of the MD-GNN associated with the proposed ENADPool."],"url":"http://arxiv.org/abs/2405.10218v1","category":"cs.LG"}
{"created":"2024-05-16 16:05:33","title":"Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting","abstract":"Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large pre-trained or foundational models across different modalities and tasks. However, its application to time series data, particularly within foundational models, remains underexplored. This paper examines the impact of LoRA on contemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos. We demonstrate LoRA's fine-tuning potential for forecasting the vital signs of sepsis patients in intensive care units (ICUs), emphasizing the models' adaptability to previously unseen, out-of-domain modalities. Integrating LoRA aims to enhance forecasting performance while reducing inefficiencies associated with fine-tuning large models on limited domain-specific data. Our experiments show that LoRA fine-tuning of time series foundational models significantly improves forecasting, achieving results comparable to state-of-the-art models trained from scratch on similar modalities. We conduct comprehensive ablation studies to demonstrate the trade-offs between the number of tunable parameters and forecasting performance and assess the impact of varying LoRA matrix ranks on model performance.","sentences":["Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large pre-trained or foundational models across different modalities and tasks.","However, its application to time series data, particularly within foundational models, remains underexplored.","This paper examines the impact of LoRA on contemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos.","We demonstrate LoRA's fine-tuning potential for forecasting the vital signs of sepsis patients in intensive care units (ICUs), emphasizing the models' adaptability to previously unseen, out-of-domain modalities.","Integrating LoRA aims to enhance forecasting performance while reducing inefficiencies associated with fine-tuning large models on limited domain-specific data.","Our experiments show that LoRA fine-tuning of time series foundational models significantly improves forecasting, achieving results comparable to state-of-the-art models trained from scratch on similar modalities.","We conduct comprehensive ablation studies to demonstrate the trade-offs between the number of tunable parameters and forecasting performance and assess the impact of varying LoRA matrix ranks on model performance."],"url":"http://arxiv.org/abs/2405.10216v1","category":"cs.LG"}
{"created":"2024-05-16 16:05:21","title":"SMLP: Symbolic Machine Learning Prover (User Manual)","abstract":"SMLP: Symbolic Machine Learning Prover an open source tool for exploration and optimization of systems represented by machine learning models. SMLP uses symbolic reasoning for ML model exploration and optimization under verification and stability constraints, based on SMT, constraint and NN solvers. In addition its exploration methods are guided by probabilistic and statistical methods. SMLP is a general purpose tool that requires only data suitable for ML modelling in the csv format (usually samples of the system's input/output). SMLP has been applied at Intel for analyzing and optimizing hardware designs at the analog level. Currently SMLP supports NNs, polynomial and tree models, and uses SMT solvers for reasoning and optimization at the backend, integration of specialized NN solvers is in progress.","sentences":["SMLP:","Symbolic Machine Learning Prover an open source tool for exploration and optimization of systems represented by machine learning models.","SMLP uses symbolic reasoning for ML model exploration and optimization under verification and stability constraints, based on SMT, constraint and NN solvers.","In addition its exploration methods are guided by probabilistic and statistical methods.","SMLP is a general purpose tool that requires only data suitable for ML modelling in the csv format (usually samples of the system's input/output).","SMLP has been applied at Intel for analyzing and optimizing hardware designs at the analog level.","Currently SMLP supports NNs, polynomial and tree models, and uses SMT solvers for reasoning and optimization at the backend, integration of specialized NN solvers is in progress."],"url":"http://arxiv.org/abs/2405.10215v1","category":"cs.LG"}
{"created":"2024-05-16 16:04:20","title":"A Design Trajectory Map of Human-AI Collaborative Reinforcement Learning Systems: Survey and Taxonomy","abstract":"Driven by the algorithmic advancements in reinforcement learning and the increasing number of implementations of human-AI collaboration, Collaborative Reinforcement Learning (CRL) has been receiving growing attention. Despite this recent upsurge, this area is still rarely systematically studied. In this paper, we provide an extensive survey, investigating CRL methods based on both interactive reinforcement learning algorithms and human-AI collaborative frameworks that were proposed in the past decade. We elucidate and discuss via synergistic analysis methods both the growth of the field and the state-of-the-art; we conceptualise the existing frameworks from the perspectives of design patterns, collaborative levels, parties and capabilities, and review interactive methods and algorithmic models. Specifically, we create a new Human-AI CRL Design Trajectory Map, as a systematic modelling tool for the selection of existing CRL frameworks, as well as a method of designing new CRL systems, and finally of improving future CRL designs. Furthermore, we elaborate generic Human-AI CRL challenges, providing the research community with a guide towards novel research directions. The aim of this paper is to empower researchers with a systematic framework for the design of efficient and 'natural' human-AI collaborative methods, making it possible to work on maximised realisation of humans' and AI's potentials.","sentences":["Driven by the algorithmic advancements in reinforcement learning and the increasing number of implementations of human-AI collaboration, Collaborative Reinforcement Learning (CRL) has been receiving growing attention.","Despite this recent upsurge, this area is still rarely systematically studied.","In this paper, we provide an extensive survey, investigating CRL methods based on both interactive reinforcement learning algorithms and human-AI collaborative frameworks that were proposed in the past decade.","We elucidate and discuss via synergistic analysis methods both the growth of the field and the state-of-the-art; we conceptualise the existing frameworks from the perspectives of design patterns, collaborative levels, parties and capabilities, and review interactive methods and algorithmic models.","Specifically, we create a new Human-AI CRL Design Trajectory Map, as a systematic modelling tool for the selection of existing CRL frameworks, as well as a method of designing new CRL systems, and finally of improving future CRL designs.","Furthermore, we elaborate generic Human-AI CRL challenges, providing the research community with a guide towards novel research directions.","The aim of this paper is to empower researchers with a systematic framework for the design of efficient and 'natural' human-AI collaborative methods, making it possible to work on maximised realisation of humans' and AI's potentials."],"url":"http://arxiv.org/abs/2405.10214v1","category":"cs.HC"}
{"created":"2024-05-16 16:00:47","title":"Building a Luganda Text-to-Speech Model From Crowdsourced Data","abstract":"Text-to-speech (TTS) development for African languages such as Luganda is still limited, primarily due to the scarcity of high-quality, single-speaker recordings essential for training TTS models. Prior work has focused on utilizing the Luganda Common Voice recordings of multiple speakers aged between 20-49. Although the generated speech is intelligible, it is still of lower quality than the model trained on studio-grade recordings. This is due to the insufficient data preprocessing methods applied to improve the quality of the Common Voice recordings. Furthermore, speech convergence is more difficult to achieve due to varying intonations, as well as background noise. In this paper, we show that the quality of Luganda TTS from Common Voice can improve by training on multiple speakers of close intonation in addition to further preprocessing of the training data. Specifically, we selected six female speakers with close intonation determined by subjectively listening and comparing their voice recordings. In addition to trimming out silent portions from the beginning and end of the recordings, we applied a pre-trained speech enhancement model to reduce background noise and enhance audio quality. We also utilized a pre-trained, non-intrusive, self-supervised Mean Opinion Score (MOS) estimation model to filter recordings with an estimated MOS over 3.5, indicating high perceived quality. Subjective MOS evaluations from nine native Luganda speakers demonstrate that our TTS model achieves a significantly better MOS of 3.55 compared to the reported 2.5 MOS of the existing model. Moreover, for a fair comparison, our model trained on six speakers outperforms models trained on a single-speaker (3.13 MOS) or two speakers (3.22 MOS). This showcases the effectiveness of compensating for the lack of data from one speaker with data from multiple speakers of close intonation to improve TTS quality.","sentences":["Text-to-speech (TTS) development for African languages such as Luganda is still limited, primarily due to the scarcity of high-quality, single-speaker recordings essential for training TTS models.","Prior work has focused on utilizing the Luganda Common Voice recordings of multiple speakers aged between 20-49.","Although the generated speech is intelligible, it is still of lower quality than the model trained on studio-grade recordings.","This is due to the insufficient data preprocessing methods applied to improve the quality of the Common Voice recordings.","Furthermore, speech convergence is more difficult to achieve due to varying intonations, as well as background noise.","In this paper, we show that the quality of Luganda TTS from Common Voice can improve by training on multiple speakers of close intonation in addition to further preprocessing of the training data.","Specifically, we selected six female speakers with close intonation determined by subjectively listening and comparing their voice recordings.","In addition to trimming out silent portions from the beginning and end of the recordings, we applied a pre-trained speech enhancement model to reduce background noise and enhance audio quality.","We also utilized a pre-trained, non-intrusive, self-supervised Mean Opinion Score (MOS) estimation model to filter recordings with an estimated MOS over 3.5, indicating high perceived quality.","Subjective MOS evaluations from nine native Luganda speakers demonstrate that our TTS model achieves a significantly better MOS of 3.55 compared to the reported 2.5 MOS of the existing model.","Moreover, for a fair comparison, our model trained on six speakers outperforms models trained on a single-speaker (3.13 MOS) or two speakers (3.22 MOS).","This showcases the effectiveness of compensating for the lack of data from one speaker with data from multiple speakers of close intonation to improve TTS quality."],"url":"http://arxiv.org/abs/2405.10211v1","category":"cs.SD"}
{"created":"2024-05-16 16:00:35","title":"GPT Store Mining and Analysis","abstract":"As a pivotal extension of the renowned ChatGPT, the GPT Store serves as a dynamic marketplace for various Generative Pre-trained Transformer (GPT) models, shaping the frontier of conversational AI. This paper presents an in-depth measurement study of the GPT Store, with a focus on the categorization of GPTs by topic, factors influencing GPT popularity, and the potential security risks. Our investigation starts with assessing the categorization of GPTs in the GPT Store, analyzing how they are organized by topics, and evaluating the effectiveness of the classification system. We then examine the factors that affect the popularity of specific GPTs, looking into user preferences, algorithmic influences, and market trends. Finally, the study delves into the security risks of the GPT Store, identifying potential threats and evaluating the robustness of existing security measures. This study offers a detailed overview of the GPT Store's current state, shedding light on its operational dynamics and user interaction patterns. Our findings aim to enhance understanding of the GPT ecosystem, providing valuable insights for future research, development, and policy-making in generative AI.","sentences":["As a pivotal extension of the renowned ChatGPT, the GPT Store serves as a dynamic marketplace for various Generative Pre-trained Transformer (GPT) models, shaping the frontier of conversational AI.","This paper presents an in-depth measurement study of the GPT Store, with a focus on the categorization of GPTs by topic, factors influencing GPT popularity, and the potential security risks.","Our investigation starts with assessing the categorization of GPTs in the GPT Store, analyzing how they are organized by topics, and evaluating the effectiveness of the classification system.","We then examine the factors that affect the popularity of specific GPTs, looking into user preferences, algorithmic influences, and market trends.","Finally, the study delves into the security risks of the GPT Store, identifying potential threats and evaluating the robustness of existing security measures.","This study offers a detailed overview of the GPT Store's current state, shedding light on its operational dynamics and user interaction patterns.","Our findings aim to enhance understanding of the GPT ecosystem, providing valuable insights for future research, development, and policy-making in generative AI."],"url":"http://arxiv.org/abs/2405.10210v1","category":"cs.LG"}
{"created":"2024-05-16 16:00:11","title":"Remarks on discrete subgroups with full limit sets in higher rank Lie groups","abstract":"We show that real semi-simple Lie groups of higher rank contain (infinitely generated) discrete subgroups with full limit sets in the corresponding Furstenberg boundaries. Additionally, we provide criteria under which discrete subgroups of $G = \\operatorname{SL}(3,\\mathbb{R})$ must have a full limit set in the Furstenberg boundary of $G$.   In the appendix, we show the the existence of Zariski-dense discrete subgroups $\\Gamma$ of $\\operatorname{SL}(n,\\mathbb{R})$, where $n\\ge 3$, such that the Jordan projection of some loxodromic element $\\gamma \\in\\Gamma$ lies on the boundary of the limit cone of $\\Gamma$.","sentences":["We show that real semi-simple Lie groups of higher rank contain (infinitely generated) discrete subgroups with full limit sets in the corresponding Furstenberg boundaries.","Additionally, we provide criteria under which discrete subgroups of $G = \\operatorname{SL}(3,\\mathbb{R})$ must have a full limit set in the Furstenberg boundary of $G$.   In the appendix, we show the the existence of Zariski-dense discrete subgroups $\\Gamma$ of $\\operatorname{SL}(n,\\mathbb{R})$, where $n\\ge 3$, such that the Jordan projection of some loxodromic element $\\gamma \\in\\Gamma$ lies on the boundary of the limit cone of $\\Gamma$."],"url":"http://arxiv.org/abs/2405.10209v1","category":"math.GT"}
{"created":"2024-05-16 15:59:29","title":"How To Save A World: The Go-Along Interview as Game Preservation Methodology in Wurm Online","abstract":"Massively multiplayer online (MMO) games boomed in the late 1990s to 2000s. In parallel, ethnographic studies of these communities emerged, generally involving participant observation and interviews. Several decades on, many MMOs have been reconfigured, remastered or are potentially no longer accessible at all, which presents challenges for their continued study and long-term preservation. In this paper we explore the \"go-along\" methodology, in which a researcher joins a participant on a walk through a familiar place and asks them questions, as a qualitative research method applicable for the study and preservation of games culture. Though the methodology has been introduced in digital media studies, to date it has had limited application in digital games, if at all. We report on a pilot study exploring applications of the go-along method to the sandbox MMO Wurm Online; a persistent, player-directed world with a rich history. We report on our motivations for the work, our analysis of the resulting interviews, and our reflections on both the use of go-alongs in digital games, as well as the unique and inspiring culture and community of this lesser-known game.","sentences":["Massively multiplayer online (MMO) games boomed in the late 1990s to 2000s.","In parallel, ethnographic studies of these communities emerged, generally involving participant observation and interviews.","Several decades on, many MMOs have been reconfigured, remastered or are potentially no longer accessible at all, which presents challenges for their continued study and long-term preservation.","In this paper we explore the \"go-along\" methodology, in which a researcher joins a participant on a walk through a familiar place and asks them questions, as a qualitative research method applicable for the study and preservation of games culture.","Though the methodology has been introduced in digital media studies, to date it has had limited application in digital games, if at all.","We report on a pilot study exploring applications of the go-along method to the sandbox MMO Wurm Online; a persistent, player-directed world with a rich history.","We report on our motivations for the work, our analysis of the resulting interviews, and our reflections on both the use of go-alongs in digital games, as well as the unique and inspiring culture and community of this lesser-known game."],"url":"http://arxiv.org/abs/2405.10208v1","category":"cs.HC"}
{"created":"2024-05-16 15:56:42","title":"On bicrossed product of fusion categories and exact factorizations","abstract":"We introduce the notion of a matched pair of fusion rings and fusion categories, generalizing the one for groups. Using this concept, we define the bicrossed product of fusion rings and fusion categories and we construct exact factorizations for them. This concept generalizes the bicrossed product, also known as external Zappa-Sz\\'ep product, of groups. We also show that every exact factorization of fusion rings can be presented as a bicrossed product. With this characterization, we describe the adjoint subcategory and universal grading group of an exact factorization of fusion categories. We give explicit fusion rules and associativity constraints for examples of fusion categories arising as a bicrossed product of combinations of Tambara-Yamagami categories and pointed fusion categories. These examples are new to the best of the knowledge of the authors.","sentences":["We introduce the notion of a matched pair of fusion rings and fusion categories, generalizing the one for groups.","Using this concept, we define the bicrossed product of fusion rings and fusion categories and we construct exact factorizations for them.","This concept generalizes the bicrossed product, also known as external Zappa-Sz\\'ep product, of groups.","We also show that every exact factorization of fusion rings can be presented as a bicrossed product.","With this characterization, we describe the adjoint subcategory and universal grading group of an exact factorization of fusion categories.","We give explicit fusion rules and associativity constraints for examples of fusion categories arising as a bicrossed product of combinations of Tambara-Yamagami categories and pointed fusion categories.","These examples are new to the best of the knowledge of the authors."],"url":"http://arxiv.org/abs/2405.10207v1","category":"math.QA"}
{"created":"2024-05-16 15:51:28","title":"\"The Death of Wikipedia?\" -- Exploring the Impact of ChatGPT on Wikipedia Engagement","abstract":"Wikipedia is one of the most popular websites in the world, serving as a major source of information and learning resource for millions of users worldwide. While motivations for its usage vary, prior research suggests shallow information gathering -- looking up facts and information or answering questions -- dominates over more in-depth usage. On the 22nd of November 2022, ChatGPT was released to the public and has quickly become a popular source of information, serving as an effective question-answering and knowledge gathering resource. Early indications have suggested that it may be drawing users away from traditional question answering services such as Stack Overflow, raising the question of how it may have impacted Wikipedia. In this paper, we explore Wikipedia user metrics across four areas: page views, unique visitor numbers, edit counts and editor numbers within twelve language instances of Wikipedia. We perform pairwise comparisons of these metrics before and after the release of ChatGPT and implement a panel regression model to observe and quantify longer-term trends. We find no evidence of a fall in engagement across any of the four metrics, instead observing that page views and visitor numbers increased in the period following ChatGPT's launch. However, we observe a lower increase in languages where ChatGPT was available than in languages where it was not, which may suggest ChatGPT's availability limited growth in those languages. Our results contribute to the understanding of how emerging generative AI tools are disrupting the Web ecosystem.","sentences":["Wikipedia is one of the most popular websites in the world, serving as a major source of information and learning resource for millions of users worldwide.","While motivations for its usage vary, prior research suggests shallow information gathering -- looking up facts and information or answering questions -- dominates over more in-depth usage.","On the 22nd of November 2022, ChatGPT was released to the public and has quickly become a popular source of information, serving as an effective question-answering and knowledge gathering resource.","Early indications have suggested that it may be drawing users away from traditional question answering services such as Stack Overflow, raising the question of how it may have impacted Wikipedia.","In this paper, we explore Wikipedia user metrics across four areas: page views, unique visitor numbers, edit counts and editor numbers within twelve language instances of Wikipedia.","We perform pairwise comparisons of these metrics before and after the release of ChatGPT and implement a panel regression model to observe and quantify longer-term trends.","We find no evidence of a fall in engagement across any of the four metrics, instead observing that page views and visitor numbers increased in the period following ChatGPT's launch.","However, we observe a lower increase in languages where ChatGPT was available than in languages where it was not, which may suggest ChatGPT's availability limited growth in those languages.","Our results contribute to the understanding of how emerging generative AI tools are disrupting the Web ecosystem."],"url":"http://arxiv.org/abs/2405.10205v1","category":"cs.HC"}
{"created":"2024-05-16 15:45:42","title":"Investigating cosmic histories with a stiff era through Gravitational Waves","abstract":"We investigate the potential of gravitational-wave background searches to constrain cosmic histories characterised by a stiff equation of state, preceded by a period of matter domination. Such a scenario leads to a characteristic peak in the primordial gravitational-wave spectrum originating from cosmological inflation. Assuming instant transitions between distinct epochs, which allows an analytical treatment of the gravitational-wave spectrum, we perform a Bayesian inference analysis to derive constraints from the first three observing runs of the LIGO-Virgo-KAGRA Collaboration. Additionally, we consider a smooth transition, employing an axion-like particle physics model, and highlight the difference with the instant transition approximation. We then forecast detection prospects for such a cosmic history through future gravitational-wave experiments.","sentences":["We investigate the potential of gravitational-wave background searches to constrain cosmic histories characterised by a stiff equation of state, preceded by a period of matter domination.","Such a scenario leads to a characteristic peak in the primordial gravitational-wave spectrum originating from cosmological inflation.","Assuming instant transitions between distinct epochs, which allows an analytical treatment of the gravitational-wave spectrum, we perform a Bayesian inference analysis to derive constraints from the first three observing runs of the LIGO-Virgo-KAGRA Collaboration.","Additionally, we consider a smooth transition, employing an axion-like particle physics model, and highlight the difference with the instant transition approximation.","We then forecast detection prospects for such a cosmic history through future gravitational-wave experiments."],"url":"http://arxiv.org/abs/2405.10201v1","category":"gr-qc"}
{"created":"2024-05-16 15:40:55","title":"Quantum criticality and Kibble-Zurek scaling in the Aubry-Andr\u00e9-Stark model","abstract":"We explore quantum criticality and Kibble-Zurek scaling (KZS) in the Aubry-Andre-Stark (AAS) model, where the Stark field of strength $\\varepsilon$ is added onto the one-dimensional quasiperiodic lattice. We perform scaling analysis and numerical calculations of the localization length, inverse participation ratio (IPR), and energy gap between the ground and first excited states to characterize critical properties of the delocalization-localization transition. Remarkably, our scaling analysis shows that, near the critical point, the localization length $\\xi$ scales with $\\varepsilon$ as $\\xi\\propto\\varepsilon^{-\\nu}$ with $\\nu\\approx0.3$ a new critical exponent for the AAS model, which is different from the counterparts for both the pure Aubry-Andre (AA) model and Stark model. The IPR $\\mathcal{I}$ scales as $\\mathcal{I}\\propto\\varepsilon^{s}$ with the critical exponent $s\\approx0.098$, which is also different from both two pure models. The energy gap $\\Delta E$ scales as $\\Delta E\\propto \\varepsilon^{\\nu z}$ with the same critical exponent $z\\approx2.374$ as that for the pure AA model. We further reveal hybrid scaling functions in the overlap between the critical regions of the Anderson and Stark localizations. Moreover, we investigate the driven dynamics of the localization transitions in the AAS model. By linearly changing the Stark (quasiperiodic) potential, we calculate the evolution of the localization length and the IPR, and study their dependence on the driving rate. We find that the driven dynamics from the ground state is well described by the KZS with the critical exponents obtained from the static scaling analysis. When both the Stark and quasiperiodic potentials are relevant, the KZS form includes the two scaling variables. This work extends our understanding of critical phenomena on localization transitions and generalizes the application of the KZS to hybrid models.","sentences":["We explore quantum criticality and Kibble-Zurek scaling (KZS) in the Aubry-Andre-Stark (AAS) model, where the Stark field of strength $\\varepsilon$ is added onto the one-dimensional quasiperiodic lattice.","We perform scaling analysis and numerical calculations of the localization length, inverse participation ratio (IPR), and energy gap between the ground and first excited states to characterize critical properties of the delocalization-localization transition.","Remarkably, our scaling analysis shows that, near the critical point, the localization length $\\xi$ scales with $\\varepsilon$ as $\\xi\\propto\\varepsilon^{-\\nu}$ with $\\nu\\approx0.3$ a new critical exponent for the AAS model, which is different from the counterparts for both the pure Aubry-Andre (AA) model and Stark model.","The IPR $\\mathcal{I}$ scales as $\\mathcal{I}\\propto\\varepsilon^{s}$ with the critical exponent $s\\approx0.098$, which is also different from both two pure models.","The energy gap $\\Delta E$ scales as $\\Delta E\\propto \\varepsilon^{\\nu z}$ with the same critical exponent $z\\approx2.374$ as that for the pure AA model.","We further reveal hybrid scaling functions in the overlap between the critical regions of the Anderson and Stark localizations.","Moreover, we investigate the driven dynamics of the localization transitions in the AAS model.","By linearly changing the Stark (quasiperiodic) potential, we calculate the evolution of the localization length and the IPR, and study their dependence on the driving rate.","We find that the driven dynamics from the ground state is well described by the KZS with the critical exponents obtained from the static scaling analysis.","When both the Stark and quasiperiodic potentials are relevant, the KZS form includes the two scaling variables.","This work extends our understanding of critical phenomena on localization transitions and generalizes the application of the KZS to hybrid models."],"url":"http://arxiv.org/abs/2405.10199v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-16 15:39:09","title":"Comprehensive Causal Machine Learning","abstract":"Uncovering causal effects at various levels of granularity provides substantial value to decision makers. Comprehensive machine learning approaches to causal effect estimation allow to use a single causal machine learning approach for estimation and inference of causal mean effects for all levels of granularity. Focusing on selection-on-observables, this paper compares three such approaches, the modified causal forest (mcf), the generalized random forest (grf), and double machine learning (dml). It also provides proven theoretical guarantees for the mcf and compares the theoretical properties of the approaches. The findings indicate that dml-based methods excel for average treatment effects at the population level (ATE) and group level (GATE) with few groups, when selection into treatment is not too strong. However, for finer causal heterogeneity, explicitly outcome-centred forest-based approaches are superior. The mcf has three additional benefits: (i) It is the most robust estimator in cases when dml-based approaches underperform because of substantial selectivity; (ii) it is the best estimator for GATEs when the number of groups gets larger; and (iii), it is the only estimator that is internally consistent, in the sense that low-dimensional causal ATEs and GATEs are obtained as aggregates of finer-grained causal parameters.","sentences":["Uncovering causal effects at various levels of granularity provides substantial value to decision makers.","Comprehensive machine learning approaches to causal effect estimation allow to use a single causal machine learning approach for estimation and inference of causal mean effects for all levels of granularity.","Focusing on selection-on-observables, this paper compares three such approaches, the modified causal forest (mcf), the generalized random forest (grf), and double machine learning (dml).","It also provides proven theoretical guarantees for the mcf and compares the theoretical properties of the approaches.","The findings indicate that dml-based methods excel for average treatment effects at the population level (ATE) and group level (GATE) with few groups, when selection into treatment is not too strong.","However, for finer causal heterogeneity, explicitly outcome-centred forest-based approaches are superior.","The mcf has three additional benefits: (i) It is the most robust estimator in cases when dml-based approaches underperform because of substantial selectivity; (ii) it is the best estimator for GATEs when the number of groups gets larger; and (iii), it is the only estimator that is internally consistent, in the sense that low-dimensional causal ATEs and GATEs are obtained as aggregates of finer-grained causal parameters."],"url":"http://arxiv.org/abs/2405.10198v1","category":"econ.EM"}
{"created":"2024-05-16 15:37:19","title":"The Lamperti transformation in the infinite-dimensional setting and the genealogies of self-similar Markov processes","abstract":"We propose a change in focus from the prevalent paradigm based on the branching property as a tool to analyze the structure of population models, to one based on the self-similarity property, which we also introduce for the first time in the setting of measure-valued processes. By extending the well-known Lamperti transformation for self-similar Markov processes to the Banach-valued case we are able to generalize celebrated results in population genetics that describe the frequency-process of measure-valued stable branching processes in terms of the subfamily of Beta-Fleming-Viot processes. In our work we describe the frequency process of populations whose total size evolves as any positive self-similar Markov process in terms of general $\\Lambda$-Fleming-Viot processes. Our results demonstrate the potential power of the self-similar perspective for the study of population models in which the reproduction dynamics of the individuals depend on the total population size, allowing for more complex and realistic models.","sentences":["We propose a change in focus from the prevalent paradigm based on the branching property as a tool to analyze the structure of population models, to one based on the self-similarity property, which we also introduce for the first time in the setting of measure-valued processes.","By extending the well-known Lamperti transformation for self-similar Markov processes to the Banach-valued case we are able to generalize celebrated results in population genetics that describe the frequency-process of measure-valued stable branching processes in terms of the subfamily of Beta-Fleming-Viot processes.","In our work we describe the frequency process of populations whose total size evolves as any positive self-similar Markov process in terms of general $\\Lambda$-Fleming-Viot processes.","Our results demonstrate the potential power of the self-similar perspective for the study of population models in which the reproduction dynamics of the individuals depend on the total population size, allowing for more complex and realistic models."],"url":"http://arxiv.org/abs/2405.10193v1","category":"math.PR"}
{"created":"2024-05-16 15:34:44","title":"Bounds on Dao numbers and applications to regular local rings","abstract":"The so-called Dao numbers are a sort of measure of the asymptotic behaviour of full properties of certain product ideals in a Noetherian local ring $R$ with infinite residue field and positive depth. In this paper, we answer a question of H. Dao on how to bound such numbers. The auxiliary tools range from Castelnuovo-Mumford regularity of appropriate graded structures to reduction numbers of the maximal ideal. In particular, we substantially improve previous results (and answer questions) by the authors. As an application, we provide new characterizations of when $R$ is regular; for instance, we show that this holds if and only if the maximal ideal of $R$ can be generated by a $d$-sequence (in the sense of Huneke) if and only if the third Dao number of any (minimal) reduction of the maximal ideal vanishes.","sentences":["The so-called Dao numbers are a sort of measure of the asymptotic behaviour of full properties of certain product ideals in a Noetherian local ring $R$ with infinite residue field and positive depth.","In this paper, we answer a question of H. Dao on how to bound such numbers.","The auxiliary tools range from Castelnuovo-Mumford regularity of appropriate graded structures to reduction numbers of the maximal ideal.","In particular, we substantially improve previous results (and answer questions) by the authors.","As an application, we provide new characterizations of when $R$ is regular; for instance, we show that this holds if and only if the maximal ideal of $R$ can be generated by a $d$-sequence (in the sense of Huneke) if and only if the third Dao number of any (minimal) reduction of the maximal ideal vanishes."],"url":"http://arxiv.org/abs/2405.10192v1","category":"math.AC"}
{"created":"2024-05-16 15:33:13","title":"On fusing matrices associated with conformal boundary conditions","abstract":"In the context of rational conformal field theories (RCFT) we look at the fusing matrices that arise when a topological defect is attached to a conformal boundary condition. We call such junctions open topological defects. One type of fusing matrices arises when two open defects fuse while another arises when an open defect passes through a boundary operator. We use the topological field theory approach to RCFTs based on Frobenius algebra objects in modular tensor categories to describe the general structure associated with such matrices and how to compute them from a given Frobenius algebra object and its representation theory. We illustrate the computational process on the rational free boson theories. Applications to boundary renormalisation group flows are briefly discussed.","sentences":["In the context of rational conformal field theories (RCFT) we look at the fusing matrices that arise when a topological defect is attached to a conformal boundary condition.","We call such junctions open topological defects.","One type of fusing matrices arises when two open defects fuse while another arises when an open defect passes through a boundary operator.","We use the topological field theory approach to RCFTs based on Frobenius algebra objects in modular tensor categories to describe the general structure associated with such matrices and how to compute them from a given Frobenius algebra object and its representation theory.","We illustrate the computational process on the rational free boson theories.","Applications to boundary renormalisation group flows are briefly discussed."],"url":"http://arxiv.org/abs/2405.10189v1","category":"hep-th"}
{"created":"2024-05-16 15:31:37","title":"Bridging Syntax and Semantics of Lean Expressions in E-Graphs","abstract":"Interactive theorem provers, like Isabelle/HOL, Coq and Lean, have expressive languages that allow the formalization of general mathematical objects and proofs. In this context, an important goal is to reduce the time and effort needed to prove theorems. A significant means of achieving this is by improving proof automation. We have implemented an early prototype of proof automation for equational reasoning in Lean by using equality saturation. To achieve this, we need to bridge the gap between Lean's expression semantics and the syntactically driven e-graphs in equality saturation. This involves handling bound variables, implicit typing, as well as Lean's definitional equality, which is more general than syntactic equality and involves notions like $\\alpha$-equivalence, $\\beta$-reduction, and $\\eta$-reduction. In this extended abstract, we highlight how we attempt to bridge this gap, and which challenges remain to be solved. Notably, while our techniques are partially unsound, the resulting proof automation remains sound by virtue of Lean's proof checking.","sentences":["Interactive theorem provers, like Isabelle/HOL, Coq and Lean, have expressive languages that allow the formalization of general mathematical objects and proofs.","In this context, an important goal is to reduce the time and effort needed to prove theorems.","A significant means of achieving this is by improving proof automation.","We have implemented an early prototype of proof automation for equational reasoning in Lean by using equality saturation.","To achieve this, we need to bridge the gap between Lean's expression semantics and the syntactically driven e-graphs in equality saturation.","This involves handling bound variables, implicit typing, as well as Lean's definitional equality, which is more general than syntactic equality and involves notions like $\\alpha$-equivalence, $\\beta$-reduction, and $\\eta$-reduction.","In this extended abstract, we highlight how we attempt to bridge this gap, and which challenges remain to be solved.","Notably, while our techniques are partially unsound, the resulting proof automation remains sound by virtue of Lean's proof checking."],"url":"http://arxiv.org/abs/2405.10188v1","category":"cs.SC"}
{"created":"2024-05-16 15:30:27","title":"Introducing Learning Rate Adaptation CMA-ES into Rigid 2D/3D Registration for Robotic Navigation in Spine Surgery","abstract":"The covariance matrix adaptive evolution strategy (CMA-ES) has been widely used in the field of 2D/3D registration in recent years. This optimization method exhibits exceptional robustness and usability for complex surgical scenarios. However, due to the inherent ill-posed nature of the 2D/3D registration task and the presence of numerous local minima in the landscape of similarity measures. Evolution strategies often require a larger population size in each generation in each generation to ensure the stability of registration and the globality and effectiveness of search, which makes the entire process computationally expensive. In this paper, we build a 2D/3D registration framework based on a learning rate adaptation CMA-ES manner. The framework employs a fixed and small population size, leading to minimized runtime and optimal utilization of computing resources. We conduct experimental comparisons between the proposed framework and other intensity-based baselines using a substantial volume of synthetic data. The results suggests that our method demonstrates superiority in both registration accuracy and running time. Code is available at github.com/m1nhengChen/CMAES-reg.","sentences":["The covariance matrix adaptive evolution strategy (CMA-ES) has been widely used in the field of 2D/3D registration in recent years.","This optimization method exhibits exceptional robustness and usability for complex surgical scenarios.","However, due to the inherent ill-posed nature of the 2D/3D registration task and the presence of numerous local minima in the landscape of similarity measures.","Evolution strategies often require a larger population size in each generation in each generation to ensure the stability of registration and the globality and effectiveness of search, which makes the entire process computationally expensive.","In this paper, we build a 2D/3D registration framework based on a learning rate adaptation CMA-ES manner.","The framework employs a fixed and small population size, leading to minimized runtime and optimal utilization of computing resources.","We conduct experimental comparisons between the proposed framework and other intensity-based baselines using a substantial volume of synthetic data.","The results suggests that our method demonstrates superiority in both registration accuracy and running time.","Code is available at github.com/m1nhengChen/CMAES-reg."],"url":"http://arxiv.org/abs/2405.10186v1","category":"eess.IV"}
{"created":"2024-05-16 15:30:18","title":"DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data","abstract":"Instance segmentation is data-hungry, and as model capacity increases, data scale becomes crucial for improving the accuracy. Most instance segmentation datasets today require costly manual annotation, limiting their data scale. Models trained on such data are prone to overfitting on the training set, especially for those rare categories. While recent works have delved into exploiting generative models to create synthetic datasets for data augmentation, these approaches do not efficiently harness the full potential of generative models.   To address these issues, we introduce a more efficient strategy to construct generative datasets for data augmentation, termed DiverGen. Firstly, we provide an explanation of the role of generative data from the perspective of distribution discrepancy. We investigate the impact of different data on the distribution learned by the model. We argue that generative data can expand the data distribution that the model can learn, thus mitigating overfitting. Additionally, we find that the diversity of generative data is crucial for improving model performance and enhance it through various strategies, including category diversity, prompt diversity, and generative model diversity. With these strategies, we can scale the data to millions while maintaining the trend of model performance improvement. On the LVIS dataset, DiverGen significantly outperforms the strong model X-Paste, achieving +1.1 box AP and +1.1 mask AP across all categories, and +1.9 box AP and +2.5 mask AP for rare categories.","sentences":["Instance segmentation is data-hungry, and as model capacity increases, data scale becomes crucial for improving the accuracy.","Most instance segmentation datasets today require costly manual annotation, limiting their data scale.","Models trained on such data are prone to overfitting on the training set, especially for those rare categories.","While recent works have delved into exploiting generative models to create synthetic datasets for data augmentation, these approaches do not efficiently harness the full potential of generative models.   ","To address these issues, we introduce a more efficient strategy to construct generative datasets for data augmentation, termed DiverGen.","Firstly, we provide an explanation of the role of generative data from the perspective of distribution discrepancy.","We investigate the impact of different data on the distribution learned by the model.","We argue that generative data can expand the data distribution that the model can learn, thus mitigating overfitting.","Additionally, we find that the diversity of generative data is crucial for improving model performance and enhance it through various strategies, including category diversity, prompt diversity, and generative model diversity.","With these strategies, we can scale the data to millions while maintaining the trend of model performance improvement.","On the LVIS dataset, DiverGen significantly outperforms the strong model X-Paste, achieving +1.1 box AP and +1.1 mask AP across all categories, and +1.9 box AP and +2.5 mask AP for rare categories."],"url":"http://arxiv.org/abs/2405.10185v1","category":"cs.CV"}
{"created":"2024-05-16 15:27:51","title":"A Guide to Tracking Phylogenies in Parallel and Distributed Agent-based Evolution Models","abstract":"Computer simulations are an important tool for studying the mechanics of biological evolution. In particular, in silico work with agent-based models provides an opportunity to collect high-quality records of ancestry relationships among simulated agents. Such phylogenies can provide insight into evolutionary dynamics within these simulations. Existing work generally tracks lineages directly, yielding an exact phylogenetic record of evolutionary history. However, direct tracking can be inefficient for large-scale, many-processor evolutionary simulations. An alternate approach to extracting phylogenetic information from simulation that scales more favorably is post hoc estimation, akin to how bioinformaticians build phylogenies by assessing genetic similarities between organisms. Recently introduced ``hereditary stratigraphy'' algorithms provide means for efficient inference of phylogenetic history from non-coding annotations on simulated organisms' genomes. A number of options exist in configuring hereditary stratigraphy methodology, but no work has yet tested how they impact reconstruction quality. To address this question, we surveyed reconstruction accuracy under alternate configurations across a matrix of evolutionary conditions varying in selection pressure, spatial structure, and ecological dynamics. We synthesize results from these experiments to suggest a prescriptive system of best practices for work with hereditary stratigraphy, ultimately guiding researchers in choosing appropriate instrumentation for large-scale simulation studies.","sentences":["Computer simulations are an important tool for studying the mechanics of biological evolution.","In particular, in silico work with agent-based models provides an opportunity to collect high-quality records of ancestry relationships among simulated agents.","Such phylogenies can provide insight into evolutionary dynamics within these simulations.","Existing work generally tracks lineages directly, yielding an exact phylogenetic record of evolutionary history.","However, direct tracking can be inefficient for large-scale, many-processor evolutionary simulations.","An alternate approach to extracting phylogenetic information from simulation that scales more favorably is post hoc estimation, akin to how bioinformaticians build phylogenies by assessing genetic similarities between organisms.","Recently introduced ``hereditary stratigraphy'' algorithms provide means for efficient inference of phylogenetic history from non-coding annotations on simulated organisms' genomes.","A number of options exist in configuring hereditary stratigraphy methodology, but no work has yet tested how they impact reconstruction quality.","To address this question, we surveyed reconstruction accuracy under alternate configurations across a matrix of evolutionary conditions varying in selection pressure, spatial structure, and ecological dynamics.","We synthesize results from these experiments to suggest a prescriptive system of best practices for work with hereditary stratigraphy, ultimately guiding researchers in choosing appropriate instrumentation for large-scale simulation studies."],"url":"http://arxiv.org/abs/2405.10183v1","category":"cs.NE"}
{"created":"2024-05-16 15:18:41","title":"Peering into the Wolf-Rayet phenomenon through [WO] and [WC] stars","abstract":"Spectroscopic observations have shown for decades that the Wolf-Rayet (WR) phenomenon is ubiquitous among stars with different initial masses. Although much effort to understand the winds from massive WR stars has been presented in the literature, not much has been done for such type of stars in the low-mass range. Here we present an attempt to understand the winds from [WR]-type stars using results from spectral analyses with the full non-LTE stellar atmosphere code PoWR. These results are put into context with the properties of massive WR stars. We found that WC+[WC] stars and WO+[WO] stars create independent sequences in the mass-loss rate ($\\dot{M}$) and modified wind momentum ($D_\\mathrm{mom}$) versus luminosity ($L$) diagrams. Our analysis indicates that even when the winds of WR and [WR] stars become optically thin, there is no breakdown of the general mass-loss trend, contrary to the observed ``weak wind phenomenon'' in OB stars. We report that all WR-type stars studied here broadly define single sequences in the wind efficiency ($\\eta$) versus transformed mass-loss rate ($\\dot{M}_\\mathrm{t}$), the $\\dot{M}_\\mathrm{t}$-$T_\\mathrm{eff}$ diagram, and the $(L, T_\\mathrm{eff}, \\dot{M})$ space, which suggest these to be fundamental properties of the WR phenomenon (regardless of the mass range), at least for WR-type stars of the O and C sequences. Our analytical estimations could drive computations of future stellar evolution models for WR-type stars.","sentences":["Spectroscopic observations have shown for decades that the Wolf-Rayet (WR) phenomenon is ubiquitous among stars with different initial masses.","Although much effort to understand the winds from massive WR stars has been presented in the literature, not much has been done for such type of stars in the low-mass range.","Here we present an attempt to understand the winds from [WR]-type stars using results from spectral analyses with the full non-LTE stellar atmosphere code PoWR.","These results are put into context with the properties of massive WR stars.","We found that WC+[WC] stars and WO+[WO] stars create independent sequences in the mass-loss rate ($\\dot{M}$) and modified wind momentum ($D_\\mathrm{mom}$) versus luminosity ($L$) diagrams.","Our analysis indicates that even when the winds of WR and [WR] stars become optically thin, there is no breakdown of the general mass-loss trend, contrary to the observed ``weak wind phenomenon'' in OB stars.","We report that all WR-type stars studied here broadly define single sequences in the wind efficiency ($\\eta$) versus transformed mass-loss rate ($\\dot{M}_\\mathrm{t}$), the $\\dot{M}_\\mathrm{t}$-$T_\\mathrm{eff}$ diagram, and the $(L, T_\\mathrm{eff}, \\dot{M})$ space, which suggest these to be fundamental properties of the WR phenomenon (regardless of the mass range), at least for WR-type stars of the O and C sequences.","Our analytical estimations could drive computations of future stellar evolution models for WR-type stars."],"url":"http://arxiv.org/abs/2405.10177v1","category":"astro-ph.SR"}
{"created":"2024-05-16 15:13:42","title":"Filling Missing Values Matters for Range Image-Based Point Cloud Segmentation","abstract":"Point cloud segmentation (PCS) plays an essential role in robot perception and navigation tasks. To efficiently understand large-scale outdoor point clouds, their range image representation is commonly adopted. This image-like representation is compact and structured, making range image-based PCS models practical. However, undesirable missing values in the range images damage the shapes and patterns of objects. This problem creates difficulty for the models in learning coherent and complete geometric information from the objects. Consequently, the PCS models only achieve inferior performance. Delving deeply into this issue, we find that the use of unreasonable projection approaches and deskewing scans mainly leads to unwanted missing values in the range images. Besides, almost all previous works fail to consider filling in the unexpected missing values in the PCS task. To alleviate this problem, we first propose a new projection method, namely scan unfolding++ (SU++), to avoid massive missing values in the generated range images. Then, we introduce a simple yet effective approach, namely range-dependent $K$-nearest neighbor interpolation ($K$NNI), to further fill in missing values. Finally, we introduce the Filling Missing Values Network (FMVNet) and Fast FMVNet. Extensive experimental results on SemanticKITTI, SemanticPOSS, and nuScenes datasets demonstrate that by employing the proposed SU++ and $K$NNI, existing range image-based PCS models consistently achieve better performance than the baseline models. Besides, both FMVNet and Fast FMVNet achieve state-of-the-art performance in terms of the speed-accuracy trade-off. The proposed methods can be applied to other range image-based tasks and practical applications.","sentences":["Point cloud segmentation (PCS) plays an essential role in robot perception and navigation tasks.","To efficiently understand large-scale outdoor point clouds, their range image representation is commonly adopted.","This image-like representation is compact and structured, making range image-based PCS models practical.","However, undesirable missing values in the range images damage the shapes and patterns of objects.","This problem creates difficulty for the models in learning coherent and complete geometric information from the objects.","Consequently, the PCS models only achieve inferior performance.","Delving deeply into this issue, we find that the use of unreasonable projection approaches and deskewing scans mainly leads to unwanted missing values in the range images.","Besides, almost all previous works fail to consider filling in the unexpected missing values in the PCS task.","To alleviate this problem, we first propose a new projection method, namely scan unfolding++ (SU++), to avoid massive missing values in the generated range images.","Then, we introduce a simple yet effective approach, namely range-dependent $K$-nearest neighbor interpolation ($K$NNI), to further fill in missing values.","Finally, we introduce the Filling Missing Values Network (FMVNet) and Fast FMVNet.","Extensive experimental results on SemanticKITTI, SemanticPOSS, and nuScenes datasets demonstrate that by employing the proposed SU++ and $K$NNI, existing range image-based PCS models consistently achieve better performance than the baseline models.","Besides, both FMVNet and Fast FMVNet achieve state-of-the-art performance in terms of the speed-accuracy trade-off.","The proposed methods can be applied to other range image-based tasks and practical applications."],"url":"http://arxiv.org/abs/2405.10175v1","category":"cs.CV"}
{"created":"2024-05-16 15:07:42","title":"Most likely configurations for fermion localization in a Braneworld-$f(Q,B_Q)$","abstract":"This study delves deeply into braneworld scenarios within modified gravity models, investigating their impact on particle localization and the structure of branes. Through a comprehensive blend of numerical analyses and theoretical inquiries, we unravel a nuanced correlation between deviations from standard General Relativity (GR) and the emergence of split branes. By employing probabilistic measurements, we pinpoint stable configurations that align with brane division intervals, thus challenging prevailing assumptions regarding the gravitational framework of our universe. Furthermore, our investigation extends to the localization of fermions within the brane, exposing intricate dynamics shaped by scalar field characteristics and modifications to gravitational models. By harnessing quantum information measurements, notably Shannon entropy, we discern heightened probabilities of fermion localization within the brane as gravitational models diverge from standard paradigms. This underscores the limitations of General Relativity in comprehensively describing the complexities inherent in our universe. Lastly, our exploration of massive fermions unveils their potential to breach the confines of the brane, hinting at promising avenues for future experimental endeavors aimed at probing the nature of extra dimensions and gravitational interactions. This suggests exciting prospects for advancing our understanding of fundamental physics beyond conventional boundaries.","sentences":["This study delves deeply into braneworld scenarios within modified gravity models, investigating their impact on particle localization and the structure of branes.","Through a comprehensive blend of numerical analyses and theoretical inquiries, we unravel a nuanced correlation between deviations from standard General Relativity (GR) and the emergence of split branes.","By employing probabilistic measurements, we pinpoint stable configurations that align with brane division intervals, thus challenging prevailing assumptions regarding the gravitational framework of our universe.","Furthermore, our investigation extends to the localization of fermions within the brane, exposing intricate dynamics shaped by scalar field characteristics and modifications to gravitational models.","By harnessing quantum information measurements, notably Shannon entropy, we discern heightened probabilities of fermion localization within the brane as gravitational models diverge from standard paradigms.","This underscores the limitations of General Relativity in comprehensively describing the complexities inherent in our universe.","Lastly, our exploration of massive fermions unveils their potential to breach the confines of the brane, hinting at promising avenues for future experimental endeavors aimed at probing the nature of extra dimensions and gravitational interactions.","This suggests exciting prospects for advancing our understanding of fundamental physics beyond conventional boundaries."],"url":"http://arxiv.org/abs/2405.10169v1","category":"gr-qc"}
{"created":"2024-05-16 14:54:28","title":"Torus knots and generalized Schr\u00f6der paths","abstract":"We relate invariants of torus knots to the counts of a class of lattice paths, which we call generalized Schr\\\"oder paths. We determine generating functions of such paths, located in a region determined by a type of a torus knot under consideration, and show that they encode colored HOMFLY-PT polynomials of this knot. The generators of uncolored HOMFLY-PT homology correspond to a basic set of such paths. Invoking the knots-quivers correspondence, we express generating functions of such paths as quiver generating series, and also relate them to quadruply-graded knot homology. Furthermore, we determine corresponding A-polynomials, which provide algebraic equations and recursion relations for generating functions of generalized Schr\\\"oder paths. The lattice paths of our interest explicitly enumerate BPS states associated to knots via brane constructions.","sentences":["We relate invariants of torus knots to the counts of a class of lattice paths, which we call generalized Schr\\\"oder paths.","We determine generating functions of such paths, located in a region determined by a type of a torus knot under consideration, and show that they encode colored HOMFLY-PT polynomials of this knot.","The generators of uncolored HOMFLY-PT homology correspond to a basic set of such paths.","Invoking the knots-quivers correspondence, we express generating functions of such paths as quiver generating series, and also relate them to quadruply-graded knot homology.","Furthermore, we determine corresponding A-polynomials, which provide algebraic equations and recursion relations for generating functions of generalized Schr\\\"oder paths.","The lattice paths of our interest explicitly enumerate BPS states associated to knots via brane constructions."],"url":"http://arxiv.org/abs/2405.10161v1","category":"hep-th"}
{"created":"2024-05-16 14:53:45","title":"PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning","abstract":"Remote sensing image-text retrieval constitutes a foundational aspect of remote sensing interpretation tasks, facilitating the alignment of vision and language representations. This paper introduces a prior instruction representation (PIR) learning paradigm that draws on prior knowledge to instruct adaptive learning of vision and text representations. Based on PIR, a domain-adapted remote sensing image-text retrieval framework PIR-ITR is designed to address semantic noise issues in vision-language understanding tasks. However, with massive additional data for pre-training the vision-language foundation model, remote sensing image-text retrieval is further developed into an open-domain retrieval task. Continuing with the above, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote sensing image-text retrieval, to address semantic noise in remote sensing vision-language representations and further improve open-domain retrieval performance. In vision representation, Vision Instruction Representation (VIR) based on Spatial-PAE utilizes the prior-guided knowledge of the remote sensing scene recognition by building a belief matrix to select key features for reducing the impact of semantic noise. In text representation, Language Cycle Attention (LCA) based on Temporal-PAE uses the previous time step to cyclically activate the current time step to enhance text representation capability. A cluster-wise Affiliation Loss (AL) is proposed to constrain the inter-classes and to reduce the semantic confusion zones in the common subspace. Comprehensive experiments demonstrate that PIR could enhance vision and text representations and outperform the state-of-the-art methods of closed-domain and open-domain retrieval on two benchmark datasets, RSICD and RSITMD.","sentences":["Remote sensing image-text retrieval constitutes a foundational aspect of remote sensing interpretation tasks, facilitating the alignment of vision and language representations.","This paper introduces a prior instruction representation (PIR) learning paradigm that draws on prior knowledge to instruct adaptive learning of vision and text representations.","Based on PIR, a domain-adapted remote sensing image-text retrieval framework PIR-ITR is designed to address semantic noise issues in vision-language understanding tasks.","However, with massive additional data for pre-training the vision-language foundation model, remote sensing image-text retrieval is further developed into an open-domain retrieval task.","Continuing with the above, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote sensing image-text retrieval, to address semantic noise in remote sensing vision-language representations and further improve open-domain retrieval performance.","In vision representation, Vision Instruction Representation (VIR) based on Spatial-PAE utilizes the prior-guided knowledge of the remote sensing scene recognition by building a belief matrix to select key features for reducing the impact of semantic noise.","In text representation, Language Cycle Attention (LCA) based on Temporal-PAE uses the previous time step to cyclically activate the current time step to enhance text representation capability.","A cluster-wise Affiliation Loss (AL) is proposed to constrain the inter-classes and to reduce the semantic confusion zones in the common subspace.","Comprehensive experiments demonstrate that PIR could enhance vision and text representations and outperform the state-of-the-art methods of closed-domain and open-domain retrieval on two benchmark datasets, RSICD and RSITMD."],"url":"http://arxiv.org/abs/2405.10160v1","category":"cs.CV"}
{"created":"2024-05-16 14:53:34","title":"Artin-Schreier towers of finite fields","abstract":"Given a prime number $p$, we consider the tower of finite fields $F_p=L_{-1}\\subset L_0\\subset L_1\\subset\\cdots$, where each step corresponds to an Artin-Schreier extension of degree $p$, so that for $i\\geq 0$, $L_{i}=L_{i-1}[c_{i}]$, where $c_i$ is a root of $X^p-X-a_{i-1}$ and $a_{i-1}=(c_{-1}\\cdots c_{i-1})^{p-1}$, with $c_{-1}=1$. We extend and strengthen to arbitrary primes prior work of Popovych for $p=2$ on the multiplicative order of the given generator $c_i$ for $L_i$ over $L_{i-1}$. In particular, for $i\\geq 0$, we show that $O(c_i)=O(a_i)$, except only when $p=2$ and $i=1$, and that $O(c_i)$ is equal to the product of the orders of $c_j$ modulo $L_{j-1}^\\times$, where $0\\leq j\\leq i$ if $p$ is odd, and $i\\geq 2$ and $1\\leq j\\leq i$ if $p=2$. We also show that for $i\\geq 0$, the $\\mathrm{Gal}(L_i/L_{i-1})$-conjugates of $a_i$ form a normal basis of $L_i$ over $L_{i-1}$. In addition, we obtain the minimal polynomial of $c_1$ over $F_p$ in explicit form.","sentences":["Given a prime number $p$, we consider the tower of finite fields $F_p=L_{-1}\\subset L_0\\subset L_1\\subset\\cdots$, where each step corresponds to an Artin-Schreier extension of degree $p$, so that for $i\\geq 0$, $L_{i}=L_{i-1}[c_{i}]$, where $c_i$ is a root of $X^p-X-a_{i-1}$ and $a_{i-1}=(c_{-1}\\cdots c_{i-1})^{p-1}$, with $c_{-1}=1$. We extend and strengthen to arbitrary primes prior work of Popovych for $p=2$ on the multiplicative order of the given generator $c_i$ for $L_i$ over $L_{i-1}$. In particular, for $i\\geq 0$, we show that $O(c_i)=O(a_i)$, except only when $p=2$ and $i=1$, and that $O(c_i)$ is equal to the product of the orders of $c_j$ modulo $L_{j-1}^\\times$, where $0\\leq j\\leq i$ if $p$ is odd, and $i\\geq 2$ and $1\\leq j\\leq i$ if $p=2$. We also show that for $i\\geq 0$, the $\\mathrm{Gal}(L_i/L_{i-1})$-conjugates of $a_i$ form a normal basis of $L_i$ over $L_{i-1}$. In addition, we obtain the minimal polynomial of $c_1$ over $F_p$ in explicit form."],"url":"http://arxiv.org/abs/2405.10159v1","category":"math.NT"}
{"created":"2024-05-16 14:50:06","title":"Incorporating ESO into Deep Koopman Operator Modelling for Control of Autonomous Vehicles","abstract":"Koopman operator theory is a kind of data-driven modelling approach that accurately captures the nonlinearities of mechatronic systems such as vehicles against physics-based methods. However, the infinite-dimensional Koopman operator is impossible to implement in real-world applications. To approximate the infinite-dimensional Koopman operator through collection dataset rather than manual trial and error, we adopt deep neural networks (DNNs) to extract basis functions by offline training and map the nonlinearities of vehicle planar dynamics into a linear form in the lifted space. Besides, the effects of the dimensions of basis functions on the model accuracy are explored. Further, the extended state observer (ESO) is introduced to online estimate the total disturbance in the lifted space and compensate for the modelling errors and residuals of the learned deep Koopman operator (DK) while also improving its generalization. Then, the proposed model is applied to predict vehicle states within prediction horizons and later formulates the constrained finite-time optimization problem of model predictive control (MPC), i.e., ESO-DKMPC. In terms of the trajectory tracking of autonomous vehicles, the ESO-DKMPC generates the wheel steering angle to govern lateral motions based on the decoupling control structure. The various conditions under the double-lane change scenarios are built on the CarSim/Simulink co-simulation platform, and extensive comparisons are conducted with the linear MPC (LMPC) and nonlinear MPC (NMPC) informed by the physics-based model. The results indicate that the proposed ESO-DKMPC has better tracking performance and moderate efficacy both within linear and nonlinear regions.","sentences":["Koopman operator theory is a kind of data-driven modelling approach that accurately captures the nonlinearities of mechatronic systems such as vehicles against physics-based methods.","However, the infinite-dimensional Koopman operator is impossible to implement in real-world applications.","To approximate the infinite-dimensional Koopman operator through collection dataset rather than manual trial and error, we adopt deep neural networks (DNNs) to extract basis functions by offline training and map the nonlinearities of vehicle planar dynamics into a linear form in the lifted space.","Besides, the effects of the dimensions of basis functions on the model accuracy are explored.","Further, the extended state observer (ESO) is introduced to online estimate the total disturbance in the lifted space and compensate for the modelling errors and residuals of the learned deep Koopman operator (DK) while also improving its generalization.","Then, the proposed model is applied to predict vehicle states within prediction horizons and later formulates the constrained finite-time optimization problem of model predictive control (MPC), i.e., ESO-DKMPC.","In terms of the trajectory tracking of autonomous vehicles, the ESO-DKMPC generates the wheel steering angle to govern lateral motions based on the decoupling control structure.","The various conditions under the double-lane change scenarios are built on the CarSim/Simulink co-simulation platform, and extensive comparisons are conducted with the linear MPC (LMPC) and nonlinear MPC (NMPC) informed by the physics-based model.","The results indicate that the proposed ESO-DKMPC has better tracking performance and moderate efficacy both within linear and nonlinear regions."],"url":"http://arxiv.org/abs/2405.10157v1","category":"eess.SY"}
{"created":"2024-05-16 14:46:34","title":"Relativistic EELS scattering cross-sections for microanalysis based on Dirac solutions","abstract":"The rich information of electron energy-loss spectroscopy (EELS) comes from the complex inelastic scattering process whereby fast electrons transfer energy and momentum to atoms, exciting bound electrons from their ground states to higher unoccupied states. To quantify EELS, the common practice is to compare the cross-sections integrated within an energy window or fit the observed spectrum with theoretical differential cross-sections calculated from a generalized oscillator strength (GOS) database with experimental parameters. The previous Hartree-Fock-based and DFT-based GOS are calculated from Schr\\\"odinger's solution of atomic orbitals, which does not include the full relativistic effects. Here, we attempt to go beyond the limitations of the Schr\\\"odinger solution in the GOS tabulation by including the full relativistic effects using the Dirac equation within the local density approximation, which is particularly important for core-shell electrons of heavy elements with strong spin-orbit coupling. This has been done for all elements in the periodic table (up to Z = 118) for all possible excitation edges using modern computing capabilities and parallelization algorithms. The relativistic effects of fast incoming electrons were included to calculate cross-sections that are specific to the acceleration voltage. We make these tabulated GOS available under an open-source license to the benefit of both academic users as well as allowing integration into commercial solutions.","sentences":["The rich information of electron energy-loss spectroscopy (EELS) comes from the complex inelastic scattering process whereby fast electrons transfer energy and momentum to atoms, exciting bound electrons from their ground states to higher unoccupied states.","To quantify EELS, the common practice is to compare the cross-sections integrated within an energy window or fit the observed spectrum with theoretical differential cross-sections calculated from a generalized oscillator strength (GOS) database with experimental parameters.","The previous Hartree-Fock-based and DFT-based GOS are calculated from Schr\\\"odinger's solution of atomic orbitals, which does not include the full relativistic effects.","Here, we attempt to go beyond the limitations of the Schr\\\"odinger solution in the GOS tabulation by including the full relativistic effects using the Dirac equation within the local density approximation, which is particularly important for core-shell electrons of heavy elements with strong spin-orbit coupling.","This has been done for all elements in the periodic table (up to Z = 118) for all possible excitation edges using modern computing capabilities and parallelization algorithms.","The relativistic effects of fast incoming electrons were included to calculate cross-sections that are specific to the acceleration voltage.","We make these tabulated GOS available under an open-source license to the benefit of both academic users as well as allowing integration into commercial solutions."],"url":"http://arxiv.org/abs/2405.10151v1","category":"physics.atom-ph"}
{"created":"2024-05-16 14:46:18","title":"Speaker Verification in Agent-Generated Conversations","abstract":"The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.","sentences":["The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks.","However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied.","To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker.","To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances.","We also develop and evaluate speaker verification models under experiment setups.","We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models.","Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics."],"url":"http://arxiv.org/abs/2405.10150v1","category":"cs.CL"}
{"created":"2024-05-16 14:45:20","title":"Delooping cyclic groups with lens spaces in homotopy type theory","abstract":"In the setting of homotopy type theory, each type can be interpreted as a space. Moreover, given an element of a type, i.e. a point in the corresponding space, one can define another type which encodes the space of loops based at this point. In particular, when the type we started with is a groupoid, this loop space is always a group. Conversely, to every group we can associate a type (more precisely, a pointed connected groupoid) whose loop space is this group: this operation is called delooping. The generic procedures for constructing such deloopings of groups (based on torsors, or on descriptions of Eilenberg-MacLane spaces as higher inductive types) are unfortunately equipped with elimination principles which do not directly allow eliminating to untruncated types, and are thus difficult to work with in practice. Here, we construct deloopings of the cyclic groups $\\mathbb{Z}_m$ which are cellular, and thus do not suffer from this shortcoming. In order to do so, we provide type-theoretic implementations of lens spaces, which constitute an important family of spaces in algebraic topology. Our definition is based on the computation of an iterative join of suitable maps from the circle to an arbitrary delooping of $\\mathbb{Z}_m$. In some sense, this work generalizes the construction of real projective spaces by Buchholtz and Rijke, which handles the case m=2, although the general setting requires more involved tools. Finally, we use this construction to also provide cellular descriptions of dihedral groups, and explain how we can hope to use those to compute the cohomology and higher actions of such groups.","sentences":["In the setting of homotopy type theory, each type can be interpreted as a space.","Moreover, given an element of a type, i.e. a point in the corresponding space, one can define another type which encodes the space of loops based at this point.","In particular, when the type we started with is a groupoid, this loop space is always a group.","Conversely, to every group we can associate a type (more precisely, a pointed connected groupoid) whose loop space is this group: this operation is called delooping.","The generic procedures for constructing such deloopings of groups (based on torsors, or on descriptions of Eilenberg-MacLane spaces as higher inductive types) are unfortunately equipped with elimination principles which do not directly allow eliminating to untruncated types, and are thus difficult to work with in practice.","Here, we construct deloopings of the cyclic groups $\\mathbb{Z}_m$ which are cellular, and thus do not suffer from this shortcoming.","In order to do so, we provide type-theoretic implementations of lens spaces, which constitute an important family of spaces in algebraic topology.","Our definition is based on the computation of an iterative join of suitable maps from the circle to an arbitrary delooping of $\\mathbb{Z}_m$. In some sense, this work generalizes the construction of real projective spaces by Buchholtz and Rijke, which handles the case m=2, although the general setting requires more involved tools.","Finally, we use this construction to also provide cellular descriptions of dihedral groups, and explain how we can hope to use those to compute the cohomology and higher actions of such groups."],"url":"http://arxiv.org/abs/2405.10149v1","category":"cs.LO"}
{"created":"2024-05-16 14:44:30","title":"Isomorphism of relative holomorphs and matrix similarity","abstract":"Let $V$ be a finite-dimensional vector space over the field with $p$ elements, where $p$ is a prime number. Given arbitrary $\\alpha,\\beta\\in \\mathrm{GL}(V)$, we consider the simidirect products $V\\rtimes\\langle \\alpha\\rangle$ and $V\\rtimes\\langle \\beta\\rangle$, and show that if $V\\rtimes\\langle \\alpha\\rangle$ and $V\\rtimes\\langle \\beta\\rangle$ are isomorphic, then $\\alpha$ must be similar to a power of $\\beta$ that generates the same subgroup as $\\beta$; that is, if $H$ and $K$ are cyclic subgroups of $\\mathrm{GL}(V)$ such that $V\\rtimes H\\cong V\\rtimes K$, then $H$ and $K$ must be conjugate subgroups of $\\mathrm{GL}(V)$. If we remove the cyclic condition, there exist examples of non-isomorphic, let alone non-conjugate, subgroups $H$ and $K$ of $\\mathrm{GL}(V)$ such that $V\\rtimes H\\cong V\\rtimes K$. Even if we require that non-cyclic subgroups $H$ and $K$ of $\\mathrm{GL}(V)$ be abelian, we may still have $V\\rtimes H\\cong V\\rtimes K$ with $H$ and $K$ non-conjugate in $\\mathrm{GL}(V)$, but in this case, $H$ and $K$ must at least be isomorphic. If we replace $V$ by a free module $U$ over ${\\mathbf Z}/p^m{\\mathbf Z}$ of finite rank, with $m>1$, it may happen that $U\\rtimes H\\cong U\\rtimes K$ for non-conjugate cyclic subgroups of $\\mathrm{GL}(U)$. If we completely abandon our requirements on $V$, a sufficient criterion is given for a finite group $G$ to admit non-conjugate cyclic subgroups $H$ and $K$ of $\\mathrm{Aut}(G)$ such that $G\\rtimes H\\cong G\\rtimes K$. This criterion is satisfied by many groups.","sentences":["Let $V$ be a finite-dimensional vector space over the field with $p$ elements, where $p$ is a prime number.","Given arbitrary $\\alpha,\\beta\\in \\mathrm{GL}(V)$, we consider the simidirect products $V\\rtimes\\langle \\alpha\\rangle$ and $V\\rtimes\\langle \\beta\\rangle$, and show that if $V\\rtimes\\langle \\alpha\\rangle$ and $V\\rtimes\\langle \\beta\\rangle$ are isomorphic, then $\\alpha$ must be similar to a power of $\\beta$ that generates the same subgroup as $\\beta$; that is, if $H$ and $K$ are cyclic subgroups of $\\mathrm{GL}(V)$ such that $V\\rtimes H\\cong V\\rtimes K$, then $H$ and $K$ must be conjugate subgroups of $\\mathrm{GL}(V)$. If we remove the cyclic condition, there exist examples of non-isomorphic, let alone non-conjugate, subgroups $H$ and $K$ of $\\mathrm{GL}(V)$ such that $V\\rtimes H\\cong V\\rtimes K$. Even if we require that non-cyclic subgroups $H$ and $K$ of $\\mathrm{GL}(V)$ be abelian, we may still have $V\\rtimes H\\cong V\\rtimes K$ with $H$ and $K$ non-conjugate in $\\mathrm{GL}(V)$, but in this case, $H$ and $K$ must at least be isomorphic.","If we replace $V$ by a free module $U$ over ${\\mathbf Z}/p^m{\\mathbf Z}$ of finite rank, with $m>1$, it may happen that $U\\rtimes H\\cong U\\rtimes K$ for non-conjugate cyclic subgroups of $\\mathrm{GL}(U)$. If we completely abandon our requirements on $V$, a sufficient criterion is given for a finite group $G$ to admit non-conjugate cyclic subgroups $H$ and $K$ of $\\mathrm{Aut}(G)$ such that $G\\rtimes H\\cong","G\\rtimes K$.","This criterion is satisfied by many groups."],"url":"http://arxiv.org/abs/2405.10147v1","category":"math.GR"}
{"created":"2024-05-16 14:41:41","title":"Single-ensemble multilevel Monte Carlo for discrete interacting-particle methods","abstract":"To solve problems in domains such as filtering, optimization, and posterior sampling, interacting-particle methods have recently received much attention. These parallelizable and often gradient-free algorithms use an ensemble of particles that evolve in time, based on a combination of well-chosen dynamics and interaction between the particles. For computationally expensive dynamics -- for example, dynamics that solve inverse problems with an expensive forward model -- the cost of attaining a high accuracy quickly becomes prohibitive. We exploit a hierarchy of approximations to this forward model and apply multilevel Monte Carlo (MLMC) techniques, improving the asymptotic cost-to-error relation. More specifically, we use MLMC at each time step to estimate the interaction term within a single, globally-coupled ensemble. This technique was proposed by Hoel et al. in the context of the ensemble Kalman filter; the goal of the present paper is to study its applicability to a general framework of interacting-particle methods. After extending the algorithm and its analysis to a broad set of methods with fixed numbers of time steps, we motivate the application of the method to the class of algorithms with an infinite time horizon, which includes popular methods such as ensemble Kalman algorithms for optimization and sampling. Numerical tests confirm the improved asymptotic scaling of the multilevel approach.","sentences":["To solve problems in domains such as filtering, optimization, and posterior sampling, interacting-particle methods have recently received much attention.","These parallelizable and often gradient-free algorithms use an ensemble of particles that evolve in time, based on a combination of well-chosen dynamics and interaction between the particles.","For computationally expensive dynamics -- for example, dynamics that solve inverse problems with an expensive forward model -- the cost of attaining a high accuracy quickly becomes prohibitive.","We exploit a hierarchy of approximations to this forward model and apply multilevel Monte Carlo (MLMC) techniques, improving the asymptotic cost-to-error relation.","More specifically, we use MLMC at each time step to estimate the interaction term within a single, globally-coupled ensemble.","This technique was proposed by Hoel et al.","in the context of the ensemble Kalman filter; the goal of the present paper is to study its applicability to a general framework of interacting-particle methods.","After extending the algorithm and its analysis to a broad set of methods with fixed numbers of time steps, we motivate the application of the method to the class of algorithms with an infinite time horizon, which includes popular methods such as ensemble Kalman algorithms for optimization and sampling.","Numerical tests confirm the improved asymptotic scaling of the multilevel approach."],"url":"http://arxiv.org/abs/2405.10146v1","category":"math.NA"}
{"created":"2024-05-16 14:40:48","title":"Deep Koopman Operator-Informed Safety Command Governor for Autonomous Vehicles","abstract":"Modeling of nonlinear behaviors with physical-based models poses challenges. However, Koopman operator maps the original nonlinear system into an infinite-dimensional linear space to achieve global linearization of the nonlinear system through input and output data, which derives an absolute equivalent linear representation of the original state space. Due to the impossibility of implementing the infinite-dimensional Koopman operator, finite-dimensional kernel functions are selected as an approximation. Given its flexible structure and high accuracy, deep learning is initially employed to extract kernel functions from data and acquire a linear evolution dynamic of the autonomous vehicle in the lifted space. Additionally, the control barrier function (CBF) converts the state constraints to the constraints on the input to render safety property. Then, in terms of the lateral stability of the in-wheel motor driven vehicle, the CBF conditions are incorporated with the learned deep Koopman model. Because of the linear fashion of the deep Koopman model, the quadratic programming problem is formulated to generate the applied driving torque with minimal perturbation to the original driving torque as a safety command governor. In the end, to validate the fidelity of the deep Koopman model compared to other mainstream approaches and demonstrate the lateral improvement achieved by the proposed safety command governor, data collection and safety testing scenarios are conducted on a hardware-in-the-loop platform.","sentences":["Modeling of nonlinear behaviors with physical-based models poses challenges.","However, Koopman operator maps the original nonlinear system into an infinite-dimensional linear space to achieve global linearization of the nonlinear system through input and output data, which derives an absolute equivalent linear representation of the original state space.","Due to the impossibility of implementing the infinite-dimensional Koopman operator, finite-dimensional kernel functions are selected as an approximation.","Given its flexible structure and high accuracy, deep learning is initially employed to extract kernel functions from data and acquire a linear evolution dynamic of the autonomous vehicle in the lifted space.","Additionally, the control barrier function (CBF) converts the state constraints to the constraints on the input to render safety property.","Then, in terms of the lateral stability of the in-wheel motor driven vehicle, the CBF conditions are incorporated with the learned deep Koopman model.","Because of the linear fashion of the deep Koopman model, the quadratic programming problem is formulated to generate the applied driving torque with minimal perturbation to the original driving torque as a safety command governor.","In the end, to validate the fidelity of the deep Koopman model compared to other mainstream approaches and demonstrate the lateral improvement achieved by the proposed safety command governor, data collection and safety testing scenarios are conducted on a hardware-in-the-loop platform."],"url":"http://arxiv.org/abs/2405.10145v1","category":"eess.SY"}
{"created":"2024-05-16 14:35:12","title":"GS-Planner: A Gaussian-Splatting-based Planning Framework for Active High-Fidelity Reconstruction","abstract":"Active reconstruction technique enables robots to autonomously collect scene data for full coverage, relieving users from tedious and time-consuming data capturing process. However, designed based on unsuitable scene representations, existing methods show unrealistic reconstruction results or the inability of online quality evaluation. Due to the recent advancements in explicit radiance field technology, online active high-fidelity reconstruction has become achievable. In this paper, we propose GS-Planner, a planning framework for active high-fidelity reconstruction using 3D Gaussian Splatting. With improvement on 3DGS to recognize unobserved regions, we evaluate the reconstruction quality and completeness of 3DGS map online to guide the robot. Then we design a sampling-based active reconstruction strategy to explore the unobserved areas and improve the reconstruction geometric and textural quality. To establish a complete robot active reconstruction system, we choose quadrotor as the robotic platform for its high agility. Then we devise a safety constraint with 3DGS to generate executable trajectories for quadrotor navigation in the 3DGS map. To validate the effectiveness of our method, we conduct extensive experiments and ablation studies in highly realistic simulation scenes.","sentences":["Active reconstruction technique enables robots to autonomously collect scene data for full coverage, relieving users from tedious and time-consuming data capturing process.","However, designed based on unsuitable scene representations, existing methods show unrealistic reconstruction results or the inability of online quality evaluation.","Due to the recent advancements in explicit radiance field technology, online active high-fidelity reconstruction has become achievable.","In this paper, we propose GS-Planner, a planning framework for active high-fidelity reconstruction using 3D Gaussian Splatting.","With improvement on 3DGS to recognize unobserved regions, we evaluate the reconstruction quality and completeness of 3DGS map online to guide the robot.","Then we design a sampling-based active reconstruction strategy to explore the unobserved areas and improve the reconstruction geometric and textural quality.","To establish a complete robot active reconstruction system, we choose quadrotor as the robotic platform for its high agility.","Then we devise a safety constraint with 3DGS to generate executable trajectories for quadrotor navigation in the 3DGS map.","To validate the effectiveness of our method, we conduct extensive experiments and ablation studies in highly realistic simulation scenes."],"url":"http://arxiv.org/abs/2405.10142v1","category":"cs.RO"}
{"created":"2024-05-16 14:31:30","title":"Self-supervised feature distillation and design of experiments for efficient training of micromechanical deep learning surrogates","abstract":"Machine learning surrogate emulators are needed in engineering design and optimization tasks to rapidly emulate computationally expensive physics-based models. In micromechanics problems the local full-field response variables are desired at microstructural length scales. While there has been a great deal of work on establishing architectures for these tasks there has been relatively little work on establishing microstructural experimental design strategies. This work demonstrates that intelligent selection of microstructural volume elements for subsequent physics simulations enables the establishment of more accurate surrogate models. There exist two key challenges towards establishing a suitable framework: (1) microstructural feature quantification and (2) establishment of a criteria which encourages construction of a diverse training data set. Three feature extraction strategies are used as well as three design criteria. A novel contrastive feature extraction approach is established for automated self-supervised extraction of microstructural summary statistics. Results indicate that for the problem considered up to a 8\\% improvement in surrogate performance may be achieved using the proposed design and training strategy. Trends indicate this approach may be even more beneficial when scaled towards larger problems. These results demonstrate that the selection of an efficient experimental design is an important consideration when establishing machine learning based surrogate models.","sentences":["Machine learning surrogate emulators are needed in engineering design and optimization tasks to rapidly emulate computationally expensive physics-based models.","In micromechanics problems the local full-field response variables are desired at microstructural length scales.","While there has been a great deal of work on establishing architectures for these tasks there has been relatively little work on establishing microstructural experimental design strategies.","This work demonstrates that intelligent selection of microstructural volume elements for subsequent physics simulations enables the establishment of more accurate surrogate models.","There exist two key challenges towards establishing a suitable framework: (1) microstructural feature quantification and (2) establishment of a criteria which encourages construction of a diverse training data set.","Three feature extraction strategies are used as well as three design criteria.","A novel contrastive feature extraction approach is established for automated self-supervised extraction of microstructural summary statistics.","Results indicate that for the problem considered up to a 8\\% improvement in surrogate performance may be achieved using the proposed design and training strategy.","Trends indicate this approach may be even more beneficial when scaled towards larger problems.","These results demonstrate that the selection of an efficient experimental design is an important consideration when establishing machine learning based surrogate models."],"url":"http://arxiv.org/abs/2405.10135v1","category":"cs.CE"}
{"created":"2024-05-16 14:31:15","title":"Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention","abstract":"In autonomous driving, accurately interpreting the movements of other road users and leveraging this knowledge to forecast future trajectories is crucial. This is typically achieved through the integration of map data and tracked trajectories of various agents. Numerous methodologies combine this information into a singular embedding for each agent, which is then utilized to predict future behavior. However, these approaches have a notable drawback in that they may lose exact location information during the encoding process. The encoding still includes general map information. However, the generation of valid and consistent trajectories is not guaranteed. This can cause the predicted trajectories to stray from the actual lanes. This paper introduces a new refinement module designed to project the predicted trajectories back onto the actual map, rectifying these discrepancies and leading towards more consistent predictions. This versatile module can be readily incorporated into a wide range of architectures. Additionally, we propose a novel scene encoder that handles all relations between agents and their environment in a single unified heterogeneous graph attention network. By analyzing the attention values on the different edges in this graph, we can gain unique insights into the neural network's inner workings leading towards a more explainable prediction.","sentences":["In autonomous driving, accurately interpreting the movements of other road users and leveraging this knowledge to forecast future trajectories is crucial.","This is typically achieved through the integration of map data and tracked trajectories of various agents.","Numerous methodologies combine this information into a singular embedding for each agent, which is then utilized to predict future behavior.","However, these approaches have a notable drawback in that they may lose exact location information during the encoding process.","The encoding still includes general map information.","However, the generation of valid and consistent trajectories is not guaranteed.","This can cause the predicted trajectories to stray from the actual lanes.","This paper introduces a new refinement module designed to project the predicted trajectories back onto the actual map, rectifying these discrepancies and leading towards more consistent predictions.","This versatile module can be readily incorporated into a wide range of architectures.","Additionally, we propose a novel scene encoder that handles all relations between agents and their environment in a single unified heterogeneous graph attention network.","By analyzing the attention values on the different edges in this graph, we can gain unique insights into the neural network's inner workings leading towards a more explainable prediction."],"url":"http://arxiv.org/abs/2405.10134v1","category":"cs.RO"}
{"created":"2024-05-16 14:28:01","title":"StyloAI: Distinguishing AI-Generated Content with Stylometric Analysis","abstract":"The emergence of large language models (LLMs) capable of generating realistic texts and images has sparked ethical concerns across various sectors. In response, researchers in academia and industry are actively exploring methods to distinguish AI-generated content from human-authored material. However, a crucial question remains: What are the unique characteristics of AI-generated text? Addressing this gap, this study proposes StyloAI, a data-driven model that uses 31 stylometric features to identify AI-generated texts by applying a Random Forest classifier on two multi-domain datasets. StyloAI achieves accuracy rates of 81% and 98% on the test set of the AuTextification dataset and the Education dataset, respectively. This approach surpasses the performance of existing state-of-the-art models and provides valuable insights into the differences between AI-generated and human-authored texts.","sentences":["The emergence of large language models (LLMs) capable of generating realistic texts and images has sparked ethical concerns across various sectors.","In response, researchers in academia and industry are actively exploring methods to distinguish AI-generated content from human-authored material.","However, a crucial question remains: What are the unique characteristics of AI-generated text?","Addressing this gap, this study proposes StyloAI, a data-driven model that uses 31 stylometric features to identify AI-generated texts by applying a Random Forest classifier on two multi-domain datasets.","StyloAI achieves accuracy rates of 81% and 98% on the test set of the AuTextification dataset and the Education dataset, respectively.","This approach surpasses the performance of existing state-of-the-art models and provides valuable insights into the differences between AI-generated and human-authored texts."],"url":"http://arxiv.org/abs/2405.10129v1","category":"cs.CL"}
{"created":"2024-05-16 14:27:32","title":"Red Teaming Language Models for Contradictory Dialogues","abstract":"Most language models currently available are prone to self-contradiction during dialogues. To mitigate this issue, this study explores a novel contradictory dialogue processing task that aims to detect and modify contradictory statements in a conversation. This task is inspired by research on context faithfulness and dialogue comprehension, which have demonstrated that the detection and understanding of contradictions often necessitate detailed explanations. We develop a dataset comprising contradictory dialogues, in which one side of the conversation contradicts itself. Each dialogue is accompanied by an explanatory label that highlights the location and details of the contradiction. With this dataset, we present a Red Teaming framework for contradictory dialogue processing. The framework detects and attempts to explain the dialogue, then modifies the existing contradictory content using the explanation. Our experiments demonstrate that the framework improves the ability to detect contradictory dialogues and provides valid explanations. Additionally, it showcases distinct capabilities for modifying such dialogues. Our study highlights the importance of the logical inconsistency problem in conversational AI.","sentences":["Most language models currently available are prone to self-contradiction during dialogues.","To mitigate this issue, this study explores a novel contradictory dialogue processing task that aims to detect and modify contradictory statements in a conversation.","This task is inspired by research on context faithfulness and dialogue comprehension, which have demonstrated that the detection and understanding of contradictions often necessitate detailed explanations.","We develop a dataset comprising contradictory dialogues, in which one side of the conversation contradicts itself.","Each dialogue is accompanied by an explanatory label that highlights the location and details of the contradiction.","With this dataset, we present a Red Teaming framework for contradictory dialogue processing.","The framework detects and attempts to explain the dialogue, then modifies the existing contradictory content using the explanation.","Our experiments demonstrate that the framework improves the ability to detect contradictory dialogues and provides valid explanations.","Additionally, it showcases distinct capabilities for modifying such dialogues.","Our study highlights the importance of the logical inconsistency problem in conversational AI."],"url":"http://arxiv.org/abs/2405.10128v1","category":"cs.CL"}
{"created":"2024-05-16 14:26:53","title":"Mergers of hairy black holes: Constraining topological couplings from entropy","abstract":"Hairy black-holes are a unique prediction of certain theories that extend General Relativity (GR) with a scalar field. The presence of scalar hair is reflected non-trivially in the entropy of the black hole along with any topological coupling that may be present in the action. Demanding that a system of two merging black holes obeys the global second law of thermodynamics imposes a bound on this topological coupling coefficient. In this work we study how this bound is pushed from its GR value by the presence of scalar hair by considering estimates of binary black-hole merger parameters through inference studies of mock gravitational-wave (GW) events. We find that the scalar charge may produce a statistically significant deviation of the change in entropy over the GR prediction. However, we also find the latter is susceptible to biases arising out of GW inferences which ends up being two orders of magnitude larger, therefore overwhelming any change induced by the scalar hair.","sentences":["Hairy black-holes are a unique prediction of certain theories that extend General Relativity (GR) with a scalar field.","The presence of scalar hair is reflected non-trivially in the entropy of the black hole along with any topological coupling that may be present in the action.","Demanding that a system of two merging black holes obeys the global second law of thermodynamics imposes a bound on this topological coupling coefficient.","In this work we study how this bound is pushed from its GR value by the presence of scalar hair by considering estimates of binary black-hole merger parameters through inference studies of mock gravitational-wave (GW) events.","We find that the scalar charge may produce a statistically significant deviation of the change in entropy over the GR prediction.","However, we also find the latter is susceptible to biases arising out of GW inferences which ends up being two orders of magnitude larger, therefore overwhelming any change induced by the scalar hair."],"url":"http://arxiv.org/abs/2405.10127v1","category":"gr-qc"}
{"created":"2024-05-16 14:22:20","title":"Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks","abstract":"Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains.","sentences":["Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps.","While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences.","The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence.","To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence.","In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step.","Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images.","Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method.","In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains."],"url":"http://arxiv.org/abs/2405.10122v1","category":"cs.CV"}
{"created":"2024-05-16 14:21:33","title":"Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation","abstract":"Integrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue generation capabilities. However, the effective incorporation of such knowledge in zero-resource scenarios remains a substantial challenge due to the scarcity of diverse, high-quality dialogue datasets. To address this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs for enriched dialogue generation in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises two main stages: knowledge distillation, using an Implicit Query Transformer to extract and encode visual implicit knowledge from image-text pairs into knowledge vectors; and knowledge integration, employing a novel Bidirectional Variational Information Fusion technique to seamlessly integrate these distilled vectors into LLMs. This enables the LLMs to generate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through implicit multimodal cues, effectively overcoming the limitations of zero-resource scenarios. Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms existing state-of-the-art models in generating high-quality dialogues. The code will be publicly available following acceptance.","sentences":["Integrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue generation capabilities.","However, the effective incorporation of such knowledge in zero-resource scenarios remains a substantial challenge due to the scarcity of diverse, high-quality dialogue datasets.","To address this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs for enriched dialogue generation in zero-resource contexts by leveraging implicit multimodal knowledge.","VIKDF comprises two main stages: knowledge distillation, using an Implicit Query Transformer to extract and encode visual implicit knowledge from image-text pairs into knowledge vectors; and knowledge integration, employing a novel Bidirectional Variational Information Fusion technique to seamlessly integrate these distilled vectors into LLMs.","This enables the LLMs to generate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through implicit multimodal cues, effectively overcoming the limitations of zero-resource scenarios.","Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms existing state-of-the-art models in generating high-quality dialogues.","The code will be publicly available following acceptance."],"url":"http://arxiv.org/abs/2405.10121v1","category":"cs.CL"}
{"created":"2024-05-16 14:09:43","title":"Enhancing Energy Efficiency in O-RAN Through Intelligent xApps Deployment","abstract":"The proliferation of 5G technology presents an unprecedented challenge in managing the energy consumption of densely deployed network infrastructures, particularly Base Stations (BSs), which account for the majority of power usage in mobile networks. The O-RAN architecture, with its emphasis on open and intelligent design, offers a promising framework to address the Energy Efficiency (EE) demands of modern telecommunication systems. This paper introduces two xApps designed for the O-RAN architecture to optimize power savings without compromising the Quality of Service (QoS). Utilizing a commercial RAN Intelligent Controller (RIC) simulator, we demonstrate the effectiveness of our proposed xApps through extensive simulations that reflect real-world operational conditions. Our results show a significant reduction in power consumption, achieving up to 50% power savings with a minimal number of User Equipments (UEs), by intelligently managing the operational state of Radio Cards (RCs), particularly through switching between active and sleep modes based on network resource block usage conditions.","sentences":["The proliferation of 5G technology presents an unprecedented challenge in managing the energy consumption of densely deployed network infrastructures, particularly Base Stations (BSs), which account for the majority of power usage in mobile networks.","The O-RAN architecture, with its emphasis on open and intelligent design, offers a promising framework to address the Energy Efficiency (EE) demands of modern telecommunication systems.","This paper introduces two xApps designed for the O-RAN architecture to optimize power savings without compromising the Quality of Service (QoS).","Utilizing a commercial RAN Intelligent Controller (RIC) simulator, we demonstrate the effectiveness of our proposed xApps through extensive simulations that reflect real-world operational conditions.","Our results show a significant reduction in power consumption, achieving up to 50% power savings with a minimal number of User Equipments (UEs), by intelligently managing the operational state of Radio Cards (RCs), particularly through switching between active and sleep modes based on network resource block usage conditions."],"url":"http://arxiv.org/abs/2405.10116v1","category":"eess.SY"}
{"created":"2024-05-16 14:06:31","title":"Parameter study for hot spot trajectories around Sgr$A*$","abstract":"Intense flaring events in the near-infrared and X-ray wavebands of our Galactic Center have been the subject of research for decades. In recent years, the GRAVITY instrument of the Very Large Telescope captured the motion and polarimetric signature of such a flare in close proximity to the supermassive black hole. This study aims to investigate a broad parameter space for hot spot motion in the vicinity of Sgr$A*$ and reproduce the observed flaring behavior. To this end, we have developed a General Relativistic Radiative Transfer code and conducted a parameter study including both planar and ejected hot spot configurations around supermassive black holes. Super-Keplerian orbital frequencies are favored by circular equatorial, cylindrical and parabolic models, whereas conical hot spot trajectories provide a better fit for orbital frequencies below the Keplerian value. Additionally, a distant observer cannot effectively differentiate between Schwarzschild and Kerr black holes, as well as face-on orbits at different observation angles.","sentences":["Intense flaring events in the near-infrared and X-ray wavebands of our Galactic Center have been the subject of research for decades.","In recent years, the GRAVITY instrument of the Very Large Telescope captured the motion and polarimetric signature of such a flare in close proximity to the supermassive black hole.","This study aims to investigate a broad parameter space for hot spot motion in the vicinity of Sgr$A*$ and reproduce the observed flaring behavior.","To this end, we have developed a General Relativistic Radiative Transfer code and conducted a parameter study including both planar and ejected hot spot configurations around supermassive black holes.","Super-Keplerian orbital frequencies are favored by circular equatorial, cylindrical and parabolic models, whereas conical hot spot trajectories provide a better fit for orbital frequencies below the Keplerian value.","Additionally, a distant observer cannot effectively differentiate between Schwarzschild and Kerr black holes, as well as face-on orbits at different observation angles."],"url":"http://arxiv.org/abs/2405.10115v1","category":"astro-ph.HE"}
{"created":"2024-05-16 14:01:48","title":"Quantum Field Theory in Curved Spacetime Approach to the Backreaction of Dynamical Casimir Effect","abstract":"In this thesis, we investigate the dynamical Casimir effect, the creation of particles from vacuum by dynamical boundary conditions or dynamical background, and its backreaction to the motion of the boundary. The backreaction of particle creation to the boundary motion is studied using quantum field theory in curved spacetime technique, in 1+1 dimension and 3+1 dimension. The relevant quantities in these quantum field processes are carefully analyzed, including regularization of the UV and IR divergent of vacuum energy, and estimation of classical backreaction effects like radiation pressure. We recovered the qualitative result of backreaction in 1+1 dimensions. In the 3+1 dimension, we find that the backreaction tends to slow down the system to suppress the further particle creation, similar to the case of cosmological particle creation.","sentences":["In this thesis, we investigate the dynamical Casimir effect, the creation of particles from vacuum by dynamical boundary conditions or dynamical background, and its backreaction to the motion of the boundary.","The backreaction of particle creation to the boundary motion is studied using quantum field theory in curved spacetime technique, in 1+1 dimension and 3+1 dimension.","The relevant quantities in these quantum field processes are carefully analyzed, including regularization of the UV and IR divergent of vacuum energy, and estimation of classical backreaction effects like radiation pressure.","We recovered the qualitative result of backreaction in 1+1 dimensions.","In the 3+1 dimension, we find that the backreaction tends to slow down the system to suppress the further particle creation, similar to the case of cosmological particle creation."],"url":"http://arxiv.org/abs/2405.10108v1","category":"hep-th"}
{"created":"2024-05-16 14:00:55","title":"Advancing Set-Conditional Set Generation: Graph Diffusion for Fast Simulation of Reconstructed Particles","abstract":"The computational intensity of detailed detector simulations poses a significant bottleneck in generating simulated data for collider experiments. This challenge inspires the continued development of fast simulation techniques based on machine learning to serve as efficient surrogate models. In our approach, a network generates a set of reconstructed objects conditioned on input particle sets. Building on the success of a slot-attention-based model, we present a new architecture utilizing diffusion, showcasing an enhanced performance in the context of single jets.","sentences":["The computational intensity of detailed detector simulations poses a significant bottleneck in generating simulated data for collider experiments.","This challenge inspires the continued development of fast simulation techniques based on machine learning to serve as efficient surrogate models.","In our approach, a network generates a set of reconstructed objects conditioned on input particle sets.","Building on the success of a slot-attention-based model, we present a new architecture utilizing diffusion, showcasing an enhanced performance in the context of single jets."],"url":"http://arxiv.org/abs/2405.10106v1","category":"hep-ex"}
{"created":"2024-05-16 14:00:28","title":"On-chip integrated metasystem for spin-dependent multi-channel colour holography","abstract":"On-chip integrated metasurface driven by in-plane guided waves is of great interests in various light field manipulation applications such as colorful augmented reality and holographic display. However, it remains a challenge to design colorful multichannel holography by a single on-chip metasurface. Here we present metasurfaces integrated on top of guided-wave photonic slab that achieves multi-channel colorful holographic light display. An end-to-end scheme is used to inverse design the metasurface for projecting off-chip preset multiple patterns. Particular examples are presented for customized patterns that were encoded into the metasurface with a single-cell meta-atom, working simultaneously at RGB color channels and for several different diffractive distance, with polarization dependence. Holographic images are generated at 18 independent channels with such a single-cell metasurface. The proposed design scheme is easy to implement and the resulting device is viable to fabrication, promising a plenty of applications in nanophotonics.","sentences":["On-chip integrated metasurface driven by in-plane guided waves is of great interests in various light field manipulation applications such as colorful augmented reality and holographic display.","However, it remains a challenge to design colorful multichannel holography by a single on-chip metasurface.","Here we present metasurfaces integrated on top of guided-wave photonic slab that achieves multi-channel colorful holographic light display.","An end-to-end scheme is used to inverse design the metasurface for projecting off-chip preset multiple patterns.","Particular examples are presented for customized patterns that were encoded into the metasurface with a single-cell meta-atom, working simultaneously at RGB color channels and for several different diffractive distance, with polarization dependence.","Holographic images are generated at 18 independent channels with such a single-cell metasurface.","The proposed design scheme is easy to implement and the resulting device is viable to fabrication, promising a plenty of applications in nanophotonics."],"url":"http://arxiv.org/abs/2405.10104v1","category":"physics.optics"}
{"created":"2024-05-16 13:57:46","title":"One-step Pulsed Laser Deposition of Metal oxynitride/Carbon Composites for Supercapacitor Application","abstract":"Advanced material composite of nanocarbons and metal-based materials provides a synergistic effect to obtain excellent electrochemical charge-storage performance and other properties. Herein, 3D porous carbon-metal oxynitride nanocomposites with tunable carbon/metal and oxygen/nitrogen ratio are synthesized uniquely by simultaneous ablation from two different targets by single-step pulsed laser deposition at room temperature. Co ablation of titanium and vanadium nitride targets together with graphite allowed us to synthesize carbon-metal oxynitride porous nanocomposite and exploit them as a binder-free thin film supercapacitor electrode in aqueous electrolyte. We show that the elemental composition ratio and hence the structural properties can be tuned by selecting target configuration and by manipulating the ablation position. We investigate how this tuning capability impacts their charge-storage performances. We anticipate the utilization of as-synthesized various composites in a single PLD production run as next-generation active materials for flexible energy storage and optoelectronic applications.","sentences":["Advanced material composite of nanocarbons and metal-based materials provides a synergistic effect to obtain excellent electrochemical charge-storage performance and other properties.","Herein, 3D porous carbon-metal oxynitride nanocomposites with tunable carbon/metal and oxygen/nitrogen ratio are synthesized uniquely by simultaneous ablation from two different targets by single-step pulsed laser deposition at room temperature.","Co ablation of titanium and vanadium nitride targets together with graphite allowed us to synthesize carbon-metal oxynitride porous nanocomposite and exploit them as a binder-free thin film supercapacitor electrode in aqueous electrolyte.","We show that the elemental composition ratio and hence the structural properties can be tuned by selecting target configuration and by manipulating the ablation position.","We investigate how this tuning capability impacts their charge-storage performances.","We anticipate the utilization of as-synthesized various composites in a single PLD production run as next-generation active materials for flexible energy storage and optoelectronic applications."],"url":"http://arxiv.org/abs/2405.10103v1","category":"physics.app-ph"}
{"created":"2024-05-16 13:55:53","title":"A novel Reservoir Architecture for Periodic Time Series Prediction","abstract":"This paper introduces a novel approach to predicting periodic time series using reservoir computing. The model is tailored to deliver precise forecasts of rhythms, a crucial aspect for tasks such as generating musical rhythm. Leveraging reservoir computing, our proposed method is ultimately oriented towards predicting human perception of rhythm. Our network accurately predicts rhythmic signals within the human frequency perception range. The model architecture incorporates primary and intermediate neurons tasked with capturing and transmitting rhythmic information. Two parameter matrices, denoted as c and k, regulate the reservoir's overall dynamics. We propose a loss function to adapt c post-training and introduce a dynamic selection (DS) mechanism that adjusts $k$ to focus on areas with outstanding contributions. Experimental results on a diverse test set showcase accurate predictions, further improved through real-time tuning of the reservoir via c and k. Comparative assessments highlight its superior performance compared to conventional models.","sentences":["This paper introduces a novel approach to predicting periodic time series using reservoir computing.","The model is tailored to deliver precise forecasts of rhythms, a crucial aspect for tasks such as generating musical rhythm.","Leveraging reservoir computing, our proposed method is ultimately oriented towards predicting human perception of rhythm.","Our network accurately predicts rhythmic signals within the human frequency perception range.","The model architecture incorporates primary and intermediate neurons tasked with capturing and transmitting rhythmic information.","Two parameter matrices, denoted as c and k, regulate the reservoir's overall dynamics.","We propose a loss function to adapt c post-training and introduce a dynamic selection (DS) mechanism that adjusts $k$ to focus on areas with outstanding contributions.","Experimental results on a diverse test set showcase accurate predictions, further improved through real-time tuning of the reservoir via c and k. Comparative assessments highlight its superior performance compared to conventional models."],"url":"http://arxiv.org/abs/2405.10102v1","category":"cs.NE"}
{"created":"2024-05-16 13:55:23","title":"Dimensionally reducing the Classical Regge Growth (CRG) conjecture","abstract":"We explore the Classical Regge Growth conjecture (CRG) in the 4d effective field theory that results from compactifying $D$-dimensional General Relativity on a compact, Ricci-flat manifold. While the higher dimensional description is given in terms of pure Einstein gravity and the conjecture is automatically satisfied, it imposes several non-trivial constraints in the 4d spectrum. Namely, there must be either none or an infinite number of massive spin-2 modes, and the mass ratio between consecutive KK spin-2 replicas is bounded by the 4d coupling constants.","sentences":["We explore the Classical Regge Growth conjecture (CRG) in the 4d effective field theory that results from compactifying $D$-dimensional General Relativity on a compact, Ricci-flat manifold.","While the higher dimensional description is given in terms of pure Einstein gravity and the conjecture is automatically satisfied, it imposes several non-trivial constraints in the 4d spectrum.","Namely, there must be either none or an infinite number of massive spin-2 modes, and the mass ratio between consecutive KK spin-2 replicas is bounded by the 4d coupling constants."],"url":"http://arxiv.org/abs/2405.10100v1","category":"hep-th"}
{"created":"2024-05-16 13:54:53","title":"Compositional Value Iteration with Pareto Caching","abstract":"The de-facto standard approach in MDP verification is based on value iteration (VI). We propose compositional VI, a framework for model checking compositional MDPs, that addresses efficiency while maintaining soundness. Concretely, compositional MDPs naturally arise from the combination of individual components, and their structure can be expressed using, e.g., string diagrams. Towards efficiency, we observe that compositional VI repeatedly verifies individual components. We propose a technique called Pareto caching that allows to reuse verification results, even for previously unseen queries. Towards soundness, we present two stopping criteria: one generalizes the optimistic value iteration paradigm and the other uses Pareto caches in conjunction with recent baseline algorithms. Our experimental evaluations shows the promise of the novel algorithm and its variations, and identifies challenges for future work.","sentences":["The de-facto standard approach in MDP verification is based on value iteration (VI).","We propose compositional VI, a framework for model checking compositional MDPs, that addresses efficiency while maintaining soundness.","Concretely, compositional MDPs naturally arise from the combination of individual components, and their structure can be expressed using, e.g., string diagrams.","Towards efficiency, we observe that compositional VI repeatedly verifies individual components.","We propose a technique called Pareto caching that allows to reuse verification results, even for previously unseen queries.","Towards soundness, we present two stopping criteria: one generalizes the optimistic value iteration paradigm and the other uses Pareto caches in conjunction with recent baseline algorithms.","Our experimental evaluations shows the promise of the novel algorithm and its variations, and identifies challenges for future work."],"url":"http://arxiv.org/abs/2405.10099v1","category":"cs.LO"}
{"created":"2024-05-16 13:54:37","title":"When Large Language Model Meets Optimization","abstract":"Optimization algorithms and large language models (LLMs) enhance decision-making in dynamic environments by integrating artificial intelligence with traditional techniques. LLMs, with extensive domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization, while optimization algorithms refine LLM architectures and output quality. This synergy offers novel approaches for advancing general AI, addressing both the computational challenges of complex problems and the application of LLMs in practical scenarios. This review outlines the progress and potential of combining LLMs with optimization algorithms, providing insights for future research directions.","sentences":["Optimization algorithms and large language models (LLMs) enhance decision-making in dynamic environments by integrating artificial intelligence with traditional techniques.","LLMs, with extensive domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization, while optimization algorithms refine LLM architectures and output quality.","This synergy offers novel approaches for advancing general AI, addressing both the computational challenges of complex problems and the application of LLMs in practical scenarios.","This review outlines the progress and potential of combining LLMs with optimization algorithms, providing insights for future research directions."],"url":"http://arxiv.org/abs/2405.10098v1","category":"cs.NE"}
{"created":"2024-05-16 13:54:04","title":"Connecting disclinations by ridges","abstract":"We consider a thin elastic sheet with a finite number of disclinations in a variational framework in the F\\\"oppl-von K\\'arm\\'an approximation. Under the non-physical assumption that the out-of-plane displacement is a convex function, we prove that minimizers display ridges between the disclinations. We prove the associated energy scaling law with upper and lower bounds that match up to logarithmic factors in the thickness of the sheet. One of the key estimates in the proof that we consider of independent interest is a generalization of the monotonicity property of the Monge-Amp\\`ere measure.","sentences":["We consider a thin elastic sheet with a finite number of disclinations in a variational framework in the F\\\"oppl-von K\\'arm\\'an approximation.","Under the non-physical assumption that the out-of-plane displacement is a convex function, we prove that minimizers display ridges between the disclinations.","We prove the associated energy scaling law with upper and lower bounds that match up to logarithmic factors in the thickness of the sheet.","One of the key estimates in the proof that we consider of independent interest is a generalization of the monotonicity property of the Monge-Amp\\`ere measure."],"url":"http://arxiv.org/abs/2405.10097v1","category":"math.AP"}
{"created":"2024-05-16 13:47:25","title":"Decidability of Quasi-Dense Modal Logics","abstract":"The decidability of axiomatic extensions of the modal logic K with modal reduction principles, i.e. axioms of the form $\\Diamond^{k} p \\rightarrow \\Diamond^{n} p$, has remained a long-standing open problem. In this paper, we make significant progress toward solving this problem and show that decidability holds for a large subclass of these logics, namely, for 'quasi-dense logics.' Such logics are extensions of K with with modal reduction axioms such that $0 < k < n$ (dubbed 'quasi-density axioms'). To prove decidability, we define novel proof systems for quasi-dense logics consisting of disjunctive existential rules, which are first-order formulae typically used to specify ontologies in the context of database theory. We show that such proof systems can be used to generate proofs and models of modal formulae, and provide an intricate model-theoretic argument showing that such generated models can be encoded as finite objects called 'templates.' By enumerating templates of bound size, we obtain an EXPSPACE decision procedure as a consequence.","sentences":["The decidability of axiomatic extensions of the modal logic K with modal reduction principles, i.e. axioms of the form $\\Diamond^{k} p \\rightarrow \\Diamond^{n} p$, has remained a long-standing open problem.","In this paper, we make significant progress toward solving this problem and show that decidability holds for a large subclass of these logics, namely, for 'quasi-dense logics.'","Such logics are extensions of K with with modal reduction axioms such that $0 < k < n$ (dubbed 'quasi-density axioms').","To prove decidability, we define novel proof systems for quasi-dense logics consisting of disjunctive existential rules, which are first-order formulae typically used to specify ontologies in the context of database theory.","We show that such proof systems can be used to generate proofs and models of modal formulae, and provide an intricate model-theoretic argument showing that such generated models can be encoded as finite objects called 'templates.'","By enumerating templates of bound size, we obtain an EXPSPACE decision procedure as a consequence."],"url":"http://arxiv.org/abs/2405.10094v1","category":"cs.LO"}
{"created":"2024-05-16 13:44:56","title":"LaT-PFN: A Joint Embedding Predictive Architecture for In-context Time-series Forecasting","abstract":"We introduce LatentTimePFN (LaT-PFN), a foundational Time Series model with a strong embedding space that enables zero-shot forecasting. To achieve this, we perform in-context learning in latent space utilizing a novel integration of the Prior-data Fitted Networks (PFN) and Joint Embedding Predictive Architecture (JEPA) frameworks. We leverage the JEPA framework to create a prediction-optimized latent representation of the underlying stochastic process that generates time series and combines it with contextual learning, using a PFN. Furthermore, we improve on preceding works by utilizing related time series as a context and introducing an abstract time axis. This drastically reduces training time and increases the versatility of the model by allowing any time granularity and forecast horizon. We show that this results in superior zero-shot predictions compared to established baselines. We also demonstrate our latent space produces informative embeddings of both individual time steps and fixed-length summaries of entire series. Finally, we observe the emergence of multi-step patch embeddings without explicit training, suggesting the model actively learns discrete tokens that encode local structures in the data, analogous to vision transformers.","sentences":["We introduce LatentTimePFN (LaT-PFN), a foundational Time Series model with a strong embedding space that enables zero-shot forecasting.","To achieve this, we perform in-context learning in latent space utilizing a novel integration of the Prior-data Fitted Networks (PFN) and","Joint Embedding Predictive Architecture (JEPA) frameworks.","We leverage the JEPA framework to create a prediction-optimized latent representation of the underlying stochastic process that generates time series and combines it with contextual learning, using a PFN.","Furthermore, we improve on preceding works by utilizing related time series as a context and introducing an abstract time axis.","This drastically reduces training time and increases the versatility of the model by allowing any time granularity and forecast horizon.","We show that this results in superior zero-shot predictions compared to established baselines.","We also demonstrate our latent space produces informative embeddings of both individual time steps and fixed-length summaries of entire series.","Finally, we observe the emergence of multi-step patch embeddings without explicit training, suggesting the model actively learns discrete tokens that encode local structures in the data, analogous to vision transformers."],"url":"http://arxiv.org/abs/2405.10093v1","category":"cs.LG"}
{"created":"2024-05-16 13:43:04","title":"A motivic integral identity for $(-1)$-shifted symplectic stacks","abstract":"We prove a motivic integral identity relating the motivic Behrend function of a $(-1)$-shifted symplectic stack to that of its stack of graded points. This generalizes analogous identities for moduli stacks of objects in a $3$-Calabi$\\unicode{x2013}$Yau category obtained by Kontsevich$\\unicode{x2013}$Soibelman and Joyce$\\unicode{x2013}$Song, which are crucial in proving wall-crossing formulae for Donaldson$\\unicode{x2013}$Thomas invariants. We expect our identity to be useful in generalizing motivic Donaldson$\\unicode{x2013}$Thomas theory to general $(-1)$-shifted symplectic stacks.","sentences":["We prove a motivic integral identity relating the motivic Behrend function of a $(-1)$-shifted symplectic stack to that of its stack of graded points.","This generalizes analogous identities for moduli stacks of objects in a $3$-Calabi$\\unicode{x2013}$Yau category obtained by Kontsevich$\\unicode{x2013}$Soibelman and Joyce$\\unicode{x2013}$Song, which are crucial in proving wall-crossing formulae for Donaldson$\\unicode{x2013}$Thomas invariants.","We expect our identity to be useful in generalizing motivic Donaldson$\\unicode{x2013}$Thomas theory to general $(-1)$-shifted symplectic stacks."],"url":"http://arxiv.org/abs/2405.10092v1","category":"math.AG"}
{"created":"2024-05-16 13:28:10","title":"Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation","abstract":"The Learning-to-match (LTM) framework proves to be an effective inverse optimal transport approach for learning the underlying ground metric between two sources of data, facilitating subsequent matching. However, the conventional LTM framework faces scalability challenges, necessitating the use of the entire dataset each time the parameters of the ground metric are updated. In adapting LTM to the deep learning context, we introduce the mini-batch Learning-to-match (m-LTM) framework for audio-text retrieval problems. This framework leverages mini-batch subsampling and Mahalanobis-enhanced family of ground metrics. Moreover, to cope with misaligned training data in practice, we propose a variant using partial optimal transport to mitigate the harm of misaligned data pairs in training data. We conduct extensive experiments on audio-text matching problems using three datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our proposed method is capable of learning rich and expressive joint embedding space, which achieves SOTA performance. Beyond this, the proposed m-LTM framework is able to close the modality gap across audio and text embedding, which surpasses both triplet and contrastive loss in the zero-shot sound event detection task on the ESC-50 dataset. Notably, our strategy of employing partial optimal transport with m-LTM demonstrates greater noise tolerance than contrastive loss, especially under varying noise ratios in training data on the AudioCaps dataset. Our code is available at https://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval","sentences":["The Learning-to-match (LTM) framework proves to be an effective inverse optimal transport approach for learning the underlying ground metric between two sources of data, facilitating subsequent matching.","However, the conventional LTM framework faces scalability challenges, necessitating the use of the entire dataset each time the parameters of the ground metric are updated.","In adapting LTM to the deep learning context, we introduce the mini-batch Learning-to-match (m-LTM) framework for audio-text retrieval problems.","This framework leverages mini-batch subsampling and Mahalanobis-enhanced family of ground metrics.","Moreover, to cope with misaligned training data in practice, we propose a variant using partial optimal transport to mitigate the harm of misaligned data pairs in training data.","We conduct extensive experiments on audio-text matching problems using three datasets: AudioCaps, Clotho, and ESC-50.","Results demonstrate that our proposed method is capable of learning rich and expressive joint embedding space, which achieves SOTA performance.","Beyond this, the proposed m-LTM framework is able to close the modality gap across audio and text embedding, which surpasses both triplet and contrastive loss in the zero-shot sound event detection task on the ESC-50 dataset.","Notably, our strategy of employing partial optimal transport with m-LTM demonstrates greater noise tolerance than contrastive loss, especially under varying noise ratios in training data on the AudioCaps dataset.","Our code is available at https://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval"],"url":"http://arxiv.org/abs/2405.10084v1","category":"eess.AS"}
{"created":"2024-05-16 13:25:36","title":"An Integrated Framework for Multi-Granular Explanation of Video Summarization","abstract":"In this paper, we propose an integrated framework for multi-granular explanation of video summarization. This framework integrates methods for producing explanations both at the fragment level (indicating which video fragments influenced the most the decisions of the summarizer) and the more fine-grained visual object level (highlighting which visual objects were the most influential for the summarizer). To build this framework, we extend our previous work on this field, by investigating the use of a model-agnostic, perturbation-based approach for fragment-level explanation of the video summarization results, and introducing a new method that combines the results of video panoptic segmentation with an adaptation of a perturbation-based explanation approach to produce object-level explanations. The performance of the developed framework is evaluated using a state-of-the-art summarization method and two datasets for benchmarking video summarization. The findings of the conducted quantitative and qualitative evaluations demonstrate the ability of our framework to spot the most and least influential fragments and visual objects of the video for the summarizer, and to provide a comprehensive set of visual-based explanations about the output of the summarization process.","sentences":["In this paper, we propose an integrated framework for multi-granular explanation of video summarization.","This framework integrates methods for producing explanations both at the fragment level (indicating which video fragments influenced the most the decisions of the summarizer) and the more fine-grained visual object level (highlighting which visual objects were the most influential for the summarizer).","To build this framework, we extend our previous work on this field, by investigating the use of a model-agnostic, perturbation-based approach for fragment-level explanation of the video summarization results, and introducing a new method that combines the results of video panoptic segmentation with an adaptation of a perturbation-based explanation approach to produce object-level explanations.","The performance of the developed framework is evaluated using a state-of-the-art summarization method and two datasets for benchmarking video summarization.","The findings of the conducted quantitative and qualitative evaluations demonstrate the ability of our framework to spot the most and least influential fragments and visual objects of the video for the summarizer, and to provide a comprehensive set of visual-based explanations about the output of the summarization process."],"url":"http://arxiv.org/abs/2405.10082v1","category":"cs.CV"}
{"created":"2024-05-16 13:24:41","title":"General relativistic magnetohydodynamics simulations for binary neutron star mergers","abstract":"Binary neutron star mergers used to be the most promising candidate for gravitational waves for ground-based gravitational wave detectors, such as advanced LIGO and advanced VIRGO. This was proved by the detection of gravitational waves from a binary neutron star merger in 2017. Numerical modeling is pivotal in predicting and interpreting binary neutron star mergers. This chapter reviews the progress of fully general relativistic magnetized binary neutron star merger simulations. From 2008 to 2024, about forty numerical relativity simulations of magnetized binary neutron star mergers were conducted with a different level of sophistication. This chapter aims to comprehensively view the magnetohydrodynamics effect in binary neutron star mergers by reviewing all the related works.","sentences":["Binary neutron star mergers used to be the most promising candidate for gravitational waves for ground-based gravitational wave detectors, such as advanced LIGO and advanced VIRGO.","This was proved by the detection of gravitational waves from a binary neutron star merger in 2017.","Numerical modeling is pivotal in predicting and interpreting binary neutron star mergers.","This chapter reviews the progress of fully general relativistic magnetized binary neutron star merger simulations.","From 2008 to 2024, about forty numerical relativity simulations of magnetized binary neutron star mergers were conducted with a different level of sophistication.","This chapter aims to comprehensively view the magnetohydrodynamics effect in binary neutron star mergers by reviewing all the related works."],"url":"http://arxiv.org/abs/2405.10081v1","category":"astro-ph.HE"}
{"created":"2024-05-16 13:20:01","title":"Spurious reconstruction from brain activity","abstract":"Advances in brain decoding, particularly visual image reconstruction, have sparked discussions about the societal implications and ethical considerations of neurotechnology. As these methods aim to recover visual experiences from brain activity and achieve prediction beyond training samples (zero-shot prediction), it is crucial to assess their capabilities and limitations to inform public expectations and regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (NSD) and text-to-image diffusion models, reveals limitations in their generalizability. We found decreased performance when applying these methods to a different dataset designed to prevent category overlaps between training and test sets. UMAP visualization of the text features with NSD images showed limited diversity of semantic and visual clusters, with overlap between training and test sets. Formal analysis and simulations demonstrated that clustered training samples can lead to \"output dimension collapse,\" restricting predictable output feature dimensions. Diversifying the training set improved generalizability. However, text features alone are insufficient for mapping to the visual space. We argue that recent photo-like reconstructions may primarily be a blend of classification into trained categories and generation of inauthentic images through text-to-image diffusion (hallucination). Diverse datasets and compositional representations spanning the image space are essential for genuine zero-shot prediction. Interdisciplinary discussions grounded in understanding the current capabilities and limitations, as well as ethical considerations, of the technology are crucial for its responsible development.","sentences":["Advances in brain decoding, particularly visual image reconstruction, have sparked discussions about the societal implications and ethical considerations of neurotechnology.","As these methods aim to recover visual experiences from brain activity and achieve prediction beyond training samples (zero-shot prediction), it is crucial to assess their capabilities and limitations to inform public expectations and regulations.","Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (NSD) and text-to-image diffusion models, reveals limitations in their generalizability.","We found decreased performance when applying these methods to a different dataset designed to prevent category overlaps between training and test sets.","UMAP visualization of the text features with NSD images showed limited diversity of semantic and visual clusters, with overlap between training and test sets.","Formal analysis and simulations demonstrated that clustered training samples can lead to \"output dimension collapse,\" restricting predictable output feature dimensions.","Diversifying the training set improved generalizability.","However, text features alone are insufficient for mapping to the visual space.","We argue that recent photo-like reconstructions may primarily be a blend of classification into trained categories and generation of inauthentic images through text-to-image diffusion (hallucination).","Diverse datasets and compositional representations spanning the image space are essential for genuine zero-shot prediction.","Interdisciplinary discussions grounded in understanding the current capabilities and limitations, as well as ethical considerations, of the technology are crucial for its responsible development."],"url":"http://arxiv.org/abs/2405.10078v1","category":"q-bio.NC"}
{"created":"2024-05-16 13:16:48","title":"Towards Real-Time Urban Physics Simulations with Digital Twins","abstract":"Urban populations continue to grow, highlighting the critical need to safeguard civilians against potential disruptions, such as dangerous gas contaminant dispersion. The digital twin (DT) framework offers promise in analyzing and predicting such events. This study presents a computational framework for modelling airborne contaminant dispersion in built environments. Leveraging automatic generation of computational domains and solution processes, the proposed framework solves the underlying physical model equations with the finite element method (FEM) for numerical solutions. Model order reduction (MOR) methods are investigated to enhance computational efficiency without compromising accuracy. The study outlines the automatic model generation process, the details of the employed model, and the future perspectives for the realization of a DT. Throughout this research, the aim is to develop a reliable predictive model combining physics and data in a hybrid DT to provide informed real-time support within evacuation scenarios.","sentences":["Urban populations continue to grow, highlighting the critical need to safeguard civilians against potential disruptions, such as dangerous gas contaminant dispersion.","The digital twin (DT) framework offers promise in analyzing and predicting such events.","This study presents a computational framework for modelling airborne contaminant dispersion in built environments.","Leveraging automatic generation of computational domains and solution processes, the proposed framework solves the underlying physical model equations with the finite element method (FEM) for numerical solutions.","Model order reduction (MOR) methods are investigated to enhance computational efficiency without compromising accuracy.","The study outlines the automatic model generation process, the details of the employed model, and the future perspectives for the realization of a DT.","Throughout this research, the aim is to develop a reliable predictive model combining physics and data in a hybrid DT to provide informed real-time support within evacuation scenarios."],"url":"http://arxiv.org/abs/2405.10077v1","category":"cs.CE"}
{"created":"2024-05-16 13:16:31","title":"Travelling Waves and Exponential Nonlinearities in the Zeldovich-Frank-Kamenetskii Equation","abstract":"We prove the existence of a family of travelling wave solutions in a variant of the $\\textit{Zeldovich-Frank-Kamenetskii (ZFK) equation}$, a reaction-diffusion equation which models the propagation of planar laminar premixed flames in combustion theory. Our results are valid in an asymptotic regime which corresponds to a reaction with high activation energy, and provide a rigorous and geometrically informative counterpart to formal asymptotic results that have been obtained for similar problems using $\\textit{high activation energy asymptotics}$. We also go beyond the existing results by (i) proving smoothness of the minimum wave speed function $\\overline c(\\epsilon)$, where $0< \\epsilon \\ll 1$ is the small parameter, and (ii) providing an asymptotic series for a flat slow manifold which plays a role in the construction of travelling wave solutions for non-minimal wave speeds $c > \\overline c(\\epsilon)$. The analysis is complicated by the presence of an exponential nonlinearity which leads to two different scaling regimes as $\\epsilon \\to 0$, which we refer to herein as the $\\textit{convective-diffusive}$ and $\\textit{diffusive-reactive}$ zones. The main idea of the proof is to use the geometric blow-up method to identify and characterise a $(c,\\epsilon)$-family of heteroclinic orbits which traverse both of these regimes, and correspond to travelling waves in the original ZFK equation. More generally, our analysis contributes to a growing number of studies which demonstrate the utility of geometric blow-up approaches to the study dynamical systems with singular exponential nonlinearities.","sentences":["We prove the existence of a family of travelling wave solutions in a variant of the $\\textit{Zeldovich-Frank-Kamenetskii (ZFK) equation}$, a reaction-diffusion equation which models the propagation of planar laminar premixed flames in combustion theory.","Our results are valid in an asymptotic regime which corresponds to a reaction with high activation energy, and provide a rigorous and geometrically informative counterpart to formal asymptotic results that have been obtained for similar problems using $\\textit{high activation energy asymptotics}$.","We also go beyond the existing results by (i) proving smoothness of the minimum wave speed function $\\overline c(\\epsilon)$, where $0<","\\epsilon \\ll 1$ is the small parameter, and (ii) providing an asymptotic series for a flat slow manifold which plays a role in the construction of travelling wave solutions for non-minimal wave speeds $c >","\\overline c(\\epsilon)$. The analysis is complicated by the presence of an exponential nonlinearity which leads to two different scaling regimes as $\\epsilon \\to 0$, which we refer to herein as the $\\textit{convective-diffusive}$ and $\\textit{diffusive-reactive}$ zones.","The main idea of the proof is to use the geometric blow-up method to identify and characterise a $(c,\\epsilon)$-family of heteroclinic orbits which traverse both of these regimes, and correspond to travelling waves in the original ZFK equation.","More generally, our analysis contributes to a growing number of studies which demonstrate the utility of geometric blow-up approaches to the study dynamical systems with singular exponential nonlinearities."],"url":"http://arxiv.org/abs/2405.10076v1","category":"math.DS"}
{"created":"2024-05-16 13:14:43","title":"HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical Phase Recognition","abstract":"Natural language could play an important role in developing generalist surgical models by providing a broad source of supervision from raw texts. This flexible form of supervision can enable the model's transferability across datasets and tasks as natural language can be used to reference learned visual concepts or describe new ones. In this work, we present HecVL, a novel hierarchical video-language pretraining approach for building a generalist surgical model. Specifically, we construct a hierarchical video-text paired dataset by pairing the surgical lecture video with three hierarchical levels of texts: at clip-level, atomic actions using transcribed audio texts; at phase-level, conceptual text summaries; and at video-level, overall abstract text of the surgical procedure. Then, we propose a novel fine-to-coarse contrastive learning framework that learns separate embedding spaces for the three video-text hierarchies using a single model. By disentangling embedding spaces of different hierarchical levels, the learned multi-modal representations encode short-term and long-term surgical concepts in the same model. Thanks to the injected textual semantics, we demonstrate that the HecVL approach can enable zero-shot surgical phase recognition without any human annotation. Furthermore, we show that the same HecVL model for surgical phase recognition can be transferred across different surgical procedures and medical centers.","sentences":["Natural language could play an important role in developing generalist surgical models by providing a broad source of supervision from raw texts.","This flexible form of supervision can enable the model's transferability across datasets and tasks as natural language can be used to reference learned visual concepts or describe new ones.","In this work, we present HecVL, a novel hierarchical video-language pretraining approach for building a generalist surgical model.","Specifically, we construct a hierarchical video-text paired dataset by pairing the surgical lecture video with three hierarchical levels of texts: at clip-level, atomic actions using transcribed audio texts; at phase-level, conceptual text summaries; and at video-level, overall abstract text of the surgical procedure.","Then, we propose a novel fine-to-coarse contrastive learning framework that learns separate embedding spaces for the three video-text hierarchies using a single model.","By disentangling embedding spaces of different hierarchical levels, the learned multi-modal representations encode short-term and long-term surgical concepts in the same model.","Thanks to the injected textual semantics, we demonstrate that the HecVL approach can enable zero-shot surgical phase recognition without any human annotation.","Furthermore, we show that the same HecVL model for surgical phase recognition can be transferred across different surgical procedures and medical centers."],"url":"http://arxiv.org/abs/2405.10075v1","category":"cs.CV"}
{"created":"2024-05-16 13:11:26","title":"Planning and Optimizing Transit Lines","abstract":"For all line-based transit systems like bus, metro and tram, the routes of the lines and the frequencies at which they are operated are determining for the operational performance of the system. However, as transit line planning happens early in the planning process, it is not straightforward to predict the effects of line planning decisions on relevant performance indicators. This challenge has in more than 40 years of research on transit line planning let to many different models.   In this chapter, we concentrate on models for transit line planning including transit line planning under uncertainty. We pay particular attention to the interplay of passenger routes, frequency and capacity, and specify three different levels of aggregation at which these can be modeled.   Transit line planning has been studied in different communities under different names.The problem can be decomposed into the components line generation, line selection, and frequency setting. We include publications that regard one of these individual steps as well as publications that combine two or all of them. We do not restrict to models build with a certain solution approach in mind, but do have a focus on models expressed in the language of mathematical programming.","sentences":["For all line-based transit systems like bus, metro and tram, the routes of the lines and the frequencies at which they are operated are determining for the operational performance of the system.","However, as transit line planning happens early in the planning process, it is not straightforward to predict the effects of line planning decisions on relevant performance indicators.","This challenge has in more than 40 years of research on transit line planning let to many different models.   ","In this chapter, we concentrate on models for transit line planning including transit line planning under uncertainty.","We pay particular attention to the interplay of passenger routes, frequency and capacity, and specify three different levels of aggregation at which these can be modeled.   ","Transit line planning has been studied in different communities under different names.","The problem can be decomposed into the components line generation, line selection, and frequency setting.","We include publications that regard one of these individual steps as well as publications that combine two or all of them.","We do not restrict to models build with a certain solution approach in mind, but do have a focus on models expressed in the language of mathematical programming."],"url":"http://arxiv.org/abs/2405.10074v1","category":"math.OC"}
{"created":"2024-05-16 13:04:40","title":"HeMiTo-dynamics: a characterisation of mammalian prion toxicity using non-dimensionalisation, linear stability and perturbation analyses","abstract":"Prion-like proteins play crucial parts in biological processes in organisms ranging from yeast to humans. For instance, many neurodegenerative diseases are believed to be caused by the production of prion-like proteins in neural tissue. As such, understanding the dynamics of prion-like protein production is a vital step toward treating neurodegenerative disease. Mathematical models of prion-like protein dynamics show great promise as a tool for predicting disease trajectories and devising better treatment strategies for prion-related diseases. Herein, we investigate a generic model for prion-like dynamics consisting of a class of non-linear ordinary differential equations (ODEs), establishing constraints through a linear stability analysis that enforce the expected properties of mammalian prion-like toxicity. Furthermore, we identify that prion toxicity evolves through three distinct phases for which we provide analytical descriptions using perturbation analyses. Specifically, prion-toxicity is initially characterised by the healthy phase, where the dynamics are dominated by the healthy form of prions, thereafter the system enters the mixed phase, where both healthy and toxic prions interact, and lastly, the system enters the toxic phase, where toxic prions dominate, and we refer to these phases as HeMiTo-dynamics. These findings hold the potential to aid researchers in developing precise mathematical models for prion-like dynamics, enabling them to better understand underlying mechanisms and devise effective treatments for prion-related diseases.","sentences":["Prion-like proteins play crucial parts in biological processes in organisms ranging from yeast to humans.","For instance, many neurodegenerative diseases are believed to be caused by the production of prion-like proteins in neural tissue.","As such, understanding the dynamics of prion-like protein production is a vital step toward treating neurodegenerative disease.","Mathematical models of prion-like protein dynamics show great promise as a tool for predicting disease trajectories and devising better treatment strategies for prion-related diseases.","Herein, we investigate a generic model for prion-like dynamics consisting of a class of non-linear ordinary differential equations (ODEs), establishing constraints through a linear stability analysis that enforce the expected properties of mammalian prion-like toxicity.","Furthermore, we identify that prion toxicity evolves through three distinct phases for which we provide analytical descriptions using perturbation analyses.","Specifically, prion-toxicity is initially characterised by the healthy phase, where the dynamics are dominated by the healthy form of prions, thereafter the system enters the mixed phase, where both healthy and toxic prions interact, and lastly, the system enters the toxic phase, where toxic prions dominate, and we refer to these phases as HeMiTo-dynamics.","These findings hold the potential to aid researchers in developing precise mathematical models for prion-like dynamics, enabling them to better understand underlying mechanisms and devise effective treatments for prion-related diseases."],"url":"http://arxiv.org/abs/2405.10070v1","category":"q-bio.QM"}
{"created":"2024-05-16 12:54:22","title":"Arc coordinates for maximal representations","abstract":"We generalize arc coordinates for maximal representations from a hyperbolic surface with boundary into $\\text{PSp}(4,\\mathbb{R})$, focusing on the case where the surface is a pair of pants. We introduce geometric parameters within the space of right-angled hexagons in the Siegel space $\\mathcal{X}$. These parameters enable the visualization of a right-angled hexagon as a polygonal chain inside the hyperbolic plane $\\mathbb{H}^{2}$. We explore the geometric properties of reflections in $\\mathcal{X}$ and introduce the notion of maximal representation of the reflection group $W_{3}=\\mathbb{Z}/2\\mathbb{Z}*\\mathbb{Z}/2\\mathbb{Z}*\\mathbb{Z}/2\\mathbb{Z}$. We parametrize maximal representations from $W_{3}$ into $\\text{PSp}^{\\pm}(4,\\mathbb{R})$, this induces a natural parametrization of a subset of maximal and Shilov hyperbolic representations into $\\text{PSp}(4,\\mathbb{R})$.","sentences":["We generalize arc coordinates for maximal representations from a hyperbolic surface with boundary into $\\text{PSp}(4,\\mathbb{R})$, focusing on the case where the surface is a pair of pants.","We introduce geometric parameters within the space of right-angled hexagons in the Siegel space $\\mathcal{X}$. These parameters enable the visualization of a right-angled hexagon as a polygonal chain inside the hyperbolic plane $","\\mathbb{H}^{2}$. We explore the geometric properties of reflections in $\\mathcal{X}$ and introduce the notion of maximal representation of the reflection group","$W_{3}=\\mathbb{Z}/2\\mathbb{Z}*\\mathbb{Z}/2\\mathbb{Z}*\\mathbb{Z}/2\\mathbb{Z}$. We parametrize maximal representations from $W_{3}$ into $\\text{PSp}^{\\pm}(4,\\mathbb{R})$, this induces a natural parametrization of a subset of maximal and Shilov hyperbolic representations into $\\text{PSp}(4,\\mathbb{R})$."],"url":"http://arxiv.org/abs/2405.10065v1","category":"math.GT"}
{"created":"2024-05-16 12:50:51","title":"Warped CFT duals of the Pleba\u0144ski-Demia\u0144ski family of solutions","abstract":"In this paper, we analyze the symmetry properties of the complete family of type D spacetimes generalized form the Pleba\\'{n}ski-Demia\\'{n}ski solution in four dimensions holographically in terms of a warped CFT. The generalized Pleba\\'{n}ski-Demia\\'{n}ski solutions are black hole-like spacetimes characterized by seven physical parameters. Most of the black holes in four dimensions are included within this family. Generically consider a solution with horizon in this family, we figure out the possible warped conformal symmetry attached to the horizon. The horizon can be either extremal or non-extremal. In the extremal case, the near horizon region can be mapped to an infinite spacetime with geometry given by a warped and twist product of AdS$_2$ and S$^2$. The new boundary conditions for AdS$_2$ as well as their higher dimensional uplifts are applied here to manifest the asymptotic symmetry as the warped conformal symmetry. In the non-extremal case, the global warped conformal symmetry is singled out by analyzing the scalar wave equation with constant frequency. The local warped conformal symmetries are represented by the charge algebra associated to the vector fields which preserve the scalar wave equation as well as its frequency. In defining the variation of the covariant charges, a proper counterterm is introduced for consistency conditions which is supposed to be suitable for all the solutions within the family. As a consistency check, the horizon entropy is reproduced by the entropy formula of the warped CFT by using its modular covariance and the central terms derived in the bulk spacetimes.","sentences":["In this paper, we analyze the symmetry properties of the complete family of type D spacetimes generalized form the Pleba\\'{n}ski-Demia\\'{n}ski solution in four dimensions holographically in terms of a warped CFT.","The generalized Pleba\\'{n}ski-Demia\\'{n}ski solutions are black hole-like spacetimes characterized by seven physical parameters.","Most of the black holes in four dimensions are included within this family.","Generically consider a solution with horizon in this family, we figure out the possible warped conformal symmetry attached to the horizon.","The horizon can be either extremal or non-extremal.","In the extremal case, the near horizon region can be mapped to an infinite spacetime with geometry given by a warped and twist product of AdS$_2$ and S$^2$. The new boundary conditions for AdS$_2$ as well as their higher dimensional uplifts are applied here to manifest the asymptotic symmetry as the warped conformal symmetry.","In the non-extremal case, the global warped conformal symmetry is singled out by analyzing the scalar wave equation with constant frequency.","The local warped conformal symmetries are represented by the charge algebra associated to the vector fields which preserve the scalar wave equation as well as its frequency.","In defining the variation of the covariant charges, a proper counterterm is introduced for consistency conditions which is supposed to be suitable for all the solutions within the family.","As a consistency check, the horizon entropy is reproduced by the entropy formula of the warped CFT by using its modular covariance and the central terms derived in the bulk spacetimes."],"url":"http://arxiv.org/abs/2405.10061v1","category":"hep-th"}
{"created":"2024-05-16 12:42:36","title":"A finite-sample generalization bound for stable LPV systems","abstract":"One of the main theoretical challenges in learning dynamical systems from data is providing upper bounds on the generalization error, that is, the difference between the expected prediction error and the empirical prediction error measured on some finite sample. In machine learning, a popular class of such bounds are the so-called Probably Approximately Correct (PAC) bounds. In this paper, we derive a PAC bound for stable continuous-time linear parameter-varying (LPV) systems. Our bound depends on the H2 norm of the chosen class of the LPV systems, but does not depend on the time interval for which the signals are considered.","sentences":["One of the main theoretical challenges in learning dynamical systems from data is providing upper bounds on the generalization error, that is, the difference between the expected prediction error and the empirical prediction error measured on some finite sample.","In machine learning, a popular class of such bounds are the so-called Probably Approximately Correct (PAC) bounds.","In this paper, we derive a PAC bound for stable continuous-time linear parameter-varying (LPV) systems.","Our bound depends on the H2 norm of the chosen class of the LPV systems, but does not depend on the time interval for which the signals are considered."],"url":"http://arxiv.org/abs/2405.10054v1","category":"cs.LG"}
{"created":"2024-05-16 12:42:06","title":"SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection","abstract":"Open-vocabulary object detection (OvOD) has transformed detection into a language-guided task, empowering users to freely define their class vocabularies of interest during inference. However, our initial investigation indicates that existing OvOD detectors exhibit significant variability when dealing with vocabularies across various semantic granularities, posing a concern for real-world deployment. To this end, we introduce Semantic Hierarchy Nexus (SHiNe), a novel classifier that uses semantic knowledge from class hierarchies. It runs offline in three steps: i) it retrieves relevant super-/sub-categories from a hierarchy for each target class; ii) it integrates these categories into hierarchy-aware sentences; iii) it fuses these sentence embeddings to generate the nexus classifier vector. Our evaluation on various detection benchmarks demonstrates that SHiNe enhances robustness across diverse vocabulary granularities, achieving up to +31.9% mAP50 with ground truth hierarchies, while retaining improvements using hierarchies generated by large language models. Moreover, when applied to open-vocabulary classification on ImageNet-1k, SHiNe improves the CLIP zero-shot baseline by +2.8% accuracy. SHiNe is training-free and can be seamlessly integrated with any off-the-shelf OvOD detector, without incurring additional computational overhead during inference. The code is open source.","sentences":["Open-vocabulary object detection (OvOD) has transformed detection into a language-guided task, empowering users to freely define their class vocabularies of interest during inference.","However, our initial investigation indicates that existing OvOD detectors exhibit significant variability when dealing with vocabularies across various semantic granularities, posing a concern for real-world deployment.","To this end, we introduce Semantic Hierarchy Nexus (SHiNe), a novel classifier that uses semantic knowledge from class hierarchies.","It runs offline in three steps: i) it retrieves relevant super-/sub-categories from a hierarchy for each target class; ii) it integrates these categories into hierarchy-aware sentences; iii) it fuses these sentence embeddings to generate the nexus classifier vector.","Our evaluation on various detection benchmarks demonstrates that SHiNe enhances robustness across diverse vocabulary granularities, achieving up to +31.9% mAP50 with ground truth hierarchies, while retaining improvements using hierarchies generated by large language models.","Moreover, when applied to open-vocabulary classification on ImageNet-1k, SHiNe improves the CLIP zero-shot baseline by +2.8% accuracy.","SHiNe is training-free and can be seamlessly integrated with any off-the-shelf OvOD detector, without incurring additional computational overhead during inference.","The code is open source."],"url":"http://arxiv.org/abs/2405.10053v1","category":"cs.CV"}
{"created":"2024-05-16 12:40:01","title":"MarkLLM: An Open-Source Toolkit for LLM Watermarking","abstract":"LLM watermarking, which embeds imperceptible yet algorithmically detectable signals in model outputs to identify LLM-generated text, has become crucial in mitigating the potential misuse of large language models. However, the abundance of LLM watermarking algorithms, their intricate mechanisms, and the complex evaluation procedures and perspectives pose challenges for researchers and the community to easily experiment with, understand, and assess the latest advancements. To address these issues, we introduce MarkLLM, an open-source toolkit for LLM watermarking. MarkLLM offers a unified and extensible framework for implementing LLM watermarking algorithms, while providing user-friendly interfaces to ensure ease of access. Furthermore, it enhances understanding by supporting automatic visualization of the underlying mechanisms of these algorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools spanning three perspectives, along with two types of automated evaluation pipelines. Through MarkLLM, we aim to support researchers while improving the comprehension and involvement of the general public in LLM watermarking technology, fostering consensus and driving further advancements in research and application. Our code is available at https://github.com/THU-BPM/MarkLLM.","sentences":["LLM watermarking, which embeds imperceptible yet algorithmically detectable signals in model outputs to identify LLM-generated text, has become crucial in mitigating the potential misuse of large language models.","However, the abundance of LLM watermarking algorithms, their intricate mechanisms, and the complex evaluation procedures and perspectives pose challenges for researchers and the community to easily experiment with, understand, and assess the latest advancements.","To address these issues, we introduce MarkLLM, an open-source toolkit for LLM watermarking.","MarkLLM offers a unified and extensible framework for implementing LLM watermarking algorithms, while providing user-friendly interfaces to ensure ease of access.","Furthermore, it enhances understanding by supporting automatic visualization of the underlying mechanisms of these algorithms.","For evaluation, MarkLLM offers a comprehensive suite of 12 tools spanning three perspectives, along with two types of automated evaluation pipelines.","Through MarkLLM, we aim to support researchers while improving the comprehension and involvement of the general public in LLM watermarking technology, fostering consensus and driving further advancements in research and application.","Our code is available at https://github.com/THU-BPM/MarkLLM."],"url":"http://arxiv.org/abs/2405.10051v1","category":"cs.CR"}
{"created":"2024-05-16 12:29:12","title":"Global Benchmark Database","abstract":"This paper presents Global Benchmark Database (GBD), a comprehensive suite of tools for provisioning and sustainably maintaining benchmark instances and their metadata. The availability of benchmark metadata is essential for many tasks in empirical research, e.g., for the data-driven compilation of benchmarks, the domain-specific analysis of runtime experiments, or the instance-specific selection of solvers. In this paper, we introduce the data model of GBD as well as its interfaces and provide examples of how to interact with them. We also demonstrate the integration of custom data sources and explain how to extend GBD with additional problem domains, instance formats and feature extractors.","sentences":["This paper presents Global Benchmark Database (GBD), a comprehensive suite of tools for provisioning and sustainably maintaining benchmark instances and their metadata.","The availability of benchmark metadata is essential for many tasks in empirical research, e.g., for the data-driven compilation of benchmarks, the domain-specific analysis of runtime experiments, or the instance-specific selection of solvers.","In this paper, we introduce the data model of GBD as well as its interfaces and provide examples of how to interact with them.","We also demonstrate the integration of custom data sources and explain how to extend GBD with additional problem domains, instance formats and feature extractors."],"url":"http://arxiv.org/abs/2405.10045v1","category":"cs.DB"}
{"created":"2024-05-16 12:28:45","title":"Nuclearity of Hypergraph C*-Algebras","abstract":"We partially characterize nuclearity for the recently introduced class of hypergraph C*-algebras using a tailor-made hypergraph minor relation. The latter is generated by certain operations on hypergraphs which resemble the moves on directed graphs used by Eilers, Restorff, Ruiz and S{\\o}rensen to classify unital graph C*-algebras. In particular, we obtain a new proof of the fact that every graph C*-algebra associated to a finite graph is nuclear.","sentences":["We partially characterize nuclearity for the recently introduced class of hypergraph C*-algebras using a tailor-made hypergraph minor relation.","The latter is generated by certain operations on hypergraphs which resemble the moves on directed graphs used by Eilers, Restorff, Ruiz and S{\\o}rensen to classify unital graph C*-algebras.","In particular, we obtain a new proof of the fact that every graph C*-algebra associated to a finite graph is nuclear."],"url":"http://arxiv.org/abs/2405.10044v1","category":"math.OA"}
{"created":"2024-05-16 12:22:41","title":"SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation","abstract":"Large language models (LLMs) are versatile and can address many tasks, but for computational efficiency, it is often desirable to distill their capabilities into smaller student models. One way to do this for classification tasks is via dataset synthesis, which can be accomplished by generating examples of each label from the LLM. Prior approaches to synthesis use few-shot prompting, which relies on the LLM's parametric knowledge to generate usable examples. However, this leads to issues of repetition, bias towards popular entities, and stylistic differences from human text. In this work, we propose Synthesize by Retrieval and Refinement (SynthesizRR), which uses retrieval augmentation to introduce variety into the dataset synthesis process: as retrieved passages vary, the LLM is \"seeded\" with different content to generate its examples. We empirically study the synthesis of six datasets, covering topic classification, sentiment analysis, tone detection, and humor, requiring complex synthesis strategies. We find SynthesizRR greatly improves lexical and semantic diversity, similarity to human-written text, and distillation performance, when compared to standard 32-shot prompting and six baseline approaches.","sentences":["Large language models (LLMs) are versatile and can address many tasks, but for computational efficiency, it is often desirable to distill their capabilities into smaller student models.","One way to do this for classification tasks is via dataset synthesis, which can be accomplished by generating examples of each label from the LLM.","Prior approaches to synthesis use few-shot prompting, which relies on the LLM's parametric knowledge to generate usable examples.","However, this leads to issues of repetition, bias towards popular entities, and stylistic differences from human text.","In this work, we propose Synthesize by Retrieval and Refinement (SynthesizRR), which uses retrieval augmentation to introduce variety into the dataset synthesis process: as retrieved passages vary, the LLM is \"seeded\" with different content to generate its examples.","We empirically study the synthesis of six datasets, covering topic classification, sentiment analysis, tone detection, and humor, requiring complex synthesis strategies.","We find SynthesizRR greatly improves lexical and semantic diversity, similarity to human-written text, and distillation performance, when compared to standard 32-shot prompting and six baseline approaches."],"url":"http://arxiv.org/abs/2405.10040v1","category":"cs.CL"}
{"created":"2024-05-16 12:17:33","title":"Silicon nitride integrated photonics from visible to mid-infrared spectra","abstract":"Recently, silicon nitride (Si3N4) photonic integrated circuits (PICs) are of a great interest due to their extremely low waveguides losses. The number of Si3N4 integrated photonics platform applications is constantly growing including the Internet of Things (IoT), artificial intelligence (AI), light detection and ranging (LiDAR) devices, hybrid neuromorphic and quantum computing. Their heterogeneous integration with a III-V platform leads to a new advanced large scale PICs with thousands of elements. Here, we review key trends in Si3N4 integrated circuits technology and fill an information gap in the field of state-of-the-art photonic devices operating from visible to mid-infrared spectra. A comprehensive overview of Si3N4 integrared circtuis microfabrication process details (deposition, lithography, etching, etc.) is introduced. Finally, we point out the limits and challenges of silicon nitride photonics performance in an ultrawide range providing routes and prospects for their future scaling and optimization.","sentences":["Recently, silicon nitride (Si3N4) photonic integrated circuits (PICs) are of a great interest due to their extremely low waveguides losses.","The number of Si3N4 integrated photonics platform applications is constantly growing including the Internet of Things (IoT), artificial intelligence (AI), light detection and ranging (LiDAR) devices, hybrid neuromorphic and quantum computing.","Their heterogeneous integration with a III-V platform leads to a new advanced large scale PICs with thousands of elements.","Here, we review key trends in Si3N4 integrated circuits technology and fill an information gap in the field of state-of-the-art photonic devices operating from visible to mid-infrared spectra.","A comprehensive overview of Si3N4 integrared circtuis microfabrication process details (deposition, lithography, etching, etc.) is introduced.","Finally, we point out the limits and challenges of silicon nitride photonics performance in an ultrawide range providing routes and prospects for their future scaling and optimization."],"url":"http://arxiv.org/abs/2405.10038v1","category":"physics.optics"}
{"created":"2024-05-16 12:15:18","title":"Tight scaling of key rate for differential-phase-shift quantum key distribution","abstract":"The performance of quantum key distribution (QKD) protocols is evaluated based on the ease of implementation and key generation rate. Among major protocols, the differential-phase-shift (DPS) protocol has the advantage of simple implementation using a train of coherent pulses and a passive detection unit. Unfortunately, however, its key rate is known to be at least proportional to $\\eta^2$ with respect to channel transmission $\\eta\\to0$. If one can only prove the rate proportional to $\\eta^2$ and cannot improve the analysis beyond that, then the DPS protocol will be deemed inferior to other major protocols, such as the decoy BB84 protocol. In this paper, we consider a type of DPS protocol in which the phase of each emitted block comprising $n$ pulses is randomized and significantly improve the analysis of its key rate. Specifically, we reveal that the key rate is proportional to $\\eta^{1+\\frac{1}{n-2}}$ and this rate is tight. This implies that the DPS protocol can achieve a key rate proportional to $\\eta$ for a large number of $n$, which is the same scaling as the decoy BB84 protocol. Our result suggests that the DPS protocol can achieve a combination of both advantages of ease of implementation and a high key generation rate.","sentences":["The performance of quantum key distribution (QKD) protocols is evaluated based on the ease of implementation and key generation rate.","Among major protocols, the differential-phase-shift (DPS) protocol has the advantage of simple implementation using a train of coherent pulses and a passive detection unit.","Unfortunately, however, its key rate is known to be at least proportional to $\\eta^2$ with respect to channel transmission $\\eta\\to0$. If one can only prove the rate proportional to $\\eta^2$ and cannot improve the analysis beyond that, then the DPS protocol will be deemed inferior to other major protocols, such as the decoy BB84 protocol.","In this paper, we consider a type of DPS protocol in which the phase of each emitted block comprising $n$ pulses is randomized and significantly improve the analysis of its key rate.","Specifically, we reveal that the key rate is proportional to $\\eta^{1+\\frac{1}{n-2}}$ and this rate is tight.","This implies that the DPS protocol can achieve a key rate proportional to $\\eta$ for a large number of $n$, which is the same scaling as the decoy BB84 protocol.","Our result suggests that the DPS protocol can achieve a combination of both advantages of ease of implementation and a high key generation rate."],"url":"http://arxiv.org/abs/2405.10033v1","category":"quant-ph"}
{"created":"2024-05-16 12:12:45","title":"Constraining the nonstandard propagating gravitational waves in the cosmological background with GWTC-3","abstract":"The detection of gravitational waves (GWs) has opened a new window to test the fundamental nature of gravity. We present constraints on the nonstandard propagation of GWs using the spectral siren method applied to binary black hole (BBH) mergers from the third Gravitational-Wave Transient Catalog (GWTC-3). The spectral siren method exploits the redshift distribution of BBHs to probe the cosmic expansion history and break degeneracies between cosmology and modified gravity effects. We focus on the friction term $\\nu$ in the nonstandard GW propagation equation, which characterizes the running of the Planck mass. Assuming the standard $\\Lambda$CDM cosmology, we find $\\nu = 0.5^{+3.5}_{-2.6}$ (median and $90\\%$ credible interval), improving upon previous constraints from the bright siren event GW170817 by an order of magnitude. This improvement is due to the higher redshifts of BBHs in GWTC-3, reaching up to $z \\sim 1$. Our result suggests that the propagation of GWs is consistent with the predictions of general relativity, placing limits on modified gravity theories that predict a time-varying Planck mass. As the sensitivity of GW detectors improves, the spectral siren method will provide a powerful tool for testing gravity on cosmological scales and probing the physics of the early Universe.","sentences":["The detection of gravitational waves (GWs) has opened a new window to test the fundamental nature of gravity.","We present constraints on the nonstandard propagation of GWs using the spectral siren method applied to binary black hole (BBH) mergers from the third Gravitational-Wave Transient Catalog (GWTC-3).","The spectral siren method exploits the redshift distribution of BBHs to probe the cosmic expansion history and break degeneracies between cosmology and modified gravity effects.","We focus on the friction term $\\nu$ in the nonstandard GW propagation equation, which characterizes the running of the Planck mass.","Assuming the standard $\\Lambda$CDM cosmology, we find $\\nu = 0.5^{+3.5}_{-2.6}$ (median and $90\\%$ credible interval), improving upon previous constraints from the bright siren event GW170817 by an order of magnitude.","This improvement is due to the higher redshifts of BBHs in GWTC-3, reaching up to $z \\sim 1$.","Our result suggests that the propagation of GWs is consistent with the predictions of general relativity, placing limits on modified gravity theories that predict a time-varying Planck mass.","As the sensitivity of GW detectors improves, the spectral siren method will provide a powerful tool for testing gravity on cosmological scales and probing the physics of the early Universe."],"url":"http://arxiv.org/abs/2405.10031v1","category":"astro-ph.CO"}
{"created":"2024-05-16 12:11:59","title":"AsCL: An Asymmetry-sensitive Contrastive Learning Method for Image-Text Retrieval with Cross-Modal Fusion","abstract":"The image-text retrieval task aims to retrieve relevant information from a given image or text. The main challenge is to unify multimodal representation and distinguish fine-grained differences across modalities, thereby finding similar contents and filtering irrelevant contents. However, existing methods mainly focus on unified semantic representation and concept alignment for multi-modalities, while the fine-grained differences across modalities have rarely been studied before, making it difficult to solve the information asymmetry problem. In this paper, we propose a novel asymmetry-sensitive contrastive learning method. By generating corresponding positive and negative samples for different asymmetry types, our method can simultaneously ensure fine-grained semantic differentiation and unified semantic representation between multi-modalities. Additionally, a hierarchical cross-modal fusion method is proposed, which integrates global and local-level features through a multimodal attention mechanism to achieve concept alignment. Extensive experiments performed on MSCOCO and Flickr30K, demonstrate the effectiveness and superiority of our proposed method.","sentences":["The image-text retrieval task aims to retrieve relevant information from a given image or text.","The main challenge is to unify multimodal representation and distinguish fine-grained differences across modalities, thereby finding similar contents and filtering irrelevant contents.","However, existing methods mainly focus on unified semantic representation and concept alignment for multi-modalities, while the fine-grained differences across modalities have rarely been studied before, making it difficult to solve the information asymmetry problem.","In this paper, we propose a novel asymmetry-sensitive contrastive learning method.","By generating corresponding positive and negative samples for different asymmetry types, our method can simultaneously ensure fine-grained semantic differentiation and unified semantic representation between multi-modalities.","Additionally, a hierarchical cross-modal fusion method is proposed, which integrates global and local-level features through a multimodal attention mechanism to achieve concept alignment.","Extensive experiments performed on MSCOCO and Flickr30K, demonstrate the effectiveness and superiority of our proposed method."],"url":"http://arxiv.org/abs/2405.10029v1","category":"cs.MM"}
{"created":"2024-05-16 12:11:09","title":"The Real Price of Bandit Information in Multiclass Classification","abstract":"We revisit the classical problem of multiclass classification with bandit feedback (Kakade, Shalev-Shwartz and Tewari, 2008), where each input classifies to one of $K$ possible labels and feedback is restricted to whether the predicted label is correct or not. Our primary inquiry is with regard to the dependency on the number of labels $K$, and whether $T$-step regret bounds in this setting can be improved beyond the $\\smash{\\sqrt{KT}}$ dependence exhibited by existing algorithms. Our main contribution is in showing that the minimax regret of bandit multiclass is in fact more nuanced, and is of the form $\\smash{\\widetilde{\\Theta}\\left(\\min \\left\\{|\\mathcal{H}| + \\sqrt{T}, \\sqrt{KT \\log |{\\mathcal{H}|}} \\right\\} \\right) }$, where $\\mathcal{H}$ is the underlying (finite) hypothesis class. In particular, we present a new bandit classification algorithm that guarantees regret $\\smash{\\widetilde{O}(|\\mathcal{H}|+\\sqrt{T})}$, improving over classical algorithms for moderately-sized hypothesis classes, and give a matching lower bound establishing tightness of the upper bounds (up to log-factors) in all parameter regimes.","sentences":["We revisit the classical problem of multiclass classification with bandit feedback (Kakade, Shalev-Shwartz and Tewari, 2008), where each input classifies to one of $K$ possible labels and feedback is restricted to whether the predicted label is correct or not.","Our primary inquiry is with regard to the dependency on the number of labels $K$, and whether $T$-step regret bounds in this setting can be improved beyond the $\\smash{\\sqrt{KT}}$ dependence exhibited by existing algorithms.","Our main contribution is in showing that the minimax regret of bandit multiclass is in fact more nuanced, and is of the form $\\smash{\\widetilde{\\Theta}\\left(\\min \\left\\{|\\mathcal{H}| + \\sqrt{T}, \\sqrt{KT \\log |{\\mathcal{H}|}} \\right\\} \\right) }$, where $\\mathcal{H}$ is the underlying (finite) hypothesis class.","In particular, we present a new bandit classification algorithm that guarantees regret $\\smash{\\widetilde{O}(|\\mathcal{H}|+\\sqrt{T})}$, improving over classical algorithms for moderately-sized hypothesis classes, and give a matching lower bound establishing tightness of the upper bounds (up to log-factors) in all parameter regimes."],"url":"http://arxiv.org/abs/2405.10027v1","category":"cs.LG"}
{"created":"2024-05-16 12:05:45","title":"Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models","abstract":"Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which aims to predict the ground-truth transcription from the decoded N-best hypotheses. Thanks to the strong language generation ability of LLMs and rich information in the N-best list, GER shows great effectiveness in enhancing ASR results. However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection. In this paper, we propose ClozeGER, a new paradigm for ASR generative error correction. First, we introduce a multimodal LLM (i.e., SpeechGPT) to receive source speech as extra input to improve the fidelity of correction output. Then, we reformat GER as a cloze test with logits calibration to remove the input information redundancy and simplify GER with clear instructions. Experiments show that ClozeGER achieves a new breakthrough over vanilla GER on 9 popular ASR datasets.","sentences":["Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which aims to predict the ground-truth transcription from the decoded N-best hypotheses.","Thanks to the strong language generation ability of LLMs and rich information in the N-best list, GER shows great effectiveness in enhancing ASR results.","However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection.","In this paper, we propose ClozeGER, a new paradigm for ASR generative error correction.","First, we introduce a multimodal LLM (i.e., SpeechGPT) to receive source speech as extra input to improve the fidelity of correction output.","Then, we reformat GER as a cloze test with logits calibration to remove the input information redundancy and simplify GER with clear instructions.","Experiments show that ClozeGER achieves a new breakthrough over vanilla GER on 9 popular ASR datasets."],"url":"http://arxiv.org/abs/2405.10025v1","category":"cs.CL"}
{"created":"2024-05-16 12:01:26","title":"Anomalous radial acceleration of galaxies and clusters supports hyperconical modified gravity","abstract":"General relativity (GR) is the most successful theory of gravity, with great observational support at local scales. However, to keep GR valid at over cosmic scales, some phenomena (such as the flat galaxy rotation curves and the cosmic acceleration) require the assumption of exotic dark matter. The radial acceleration relation (RAR) indicates a tight correlation between dynamical mass and baryonic mass in galaxies and galaxy clusters. This suggests that the observations could be better explained by modified gravity theories without exotic matter. Modified Newtonian Dynamics (MOND) is an alternative theory for explaining some cases of flat galaxy rotation curves by using a new fundamental constant acceleration $a_0$, the so-called Milgromian parameter. However, this non-relativistic model is too rigid (with insufficient parameters) to fit the large diversity of observational phenomena. In contrast, a relativistic MOND-like gravity naturally emerges from the hyperconical model, which derives a fictitious acceleration compatible with observations. This study analyses the compatibility of the hyperconical model with respect to RAR observations of 10 galaxy clusters obtained from HIFLUGCS and 60 high-quality SPARC galaxy rotation curves. The results show that a general relation can be fitted to most cases with only one or two parameters, with an acceptable chi-square and $p$-value. These findings suggest a possible way to complete the proposed modification of GR on a cosmic scale.","sentences":["General relativity (GR) is the most successful theory of gravity, with great observational support at local scales.","However, to keep GR valid at over cosmic scales, some phenomena (such as the flat galaxy rotation curves and the cosmic acceleration) require the assumption of exotic dark matter.","The radial acceleration relation (RAR) indicates a tight correlation between dynamical mass and baryonic mass in galaxies and galaxy clusters.","This suggests that the observations could be better explained by modified gravity theories without exotic matter.","Modified Newtonian Dynamics (MOND) is an alternative theory for explaining some cases of flat galaxy rotation curves by using a new fundamental constant acceleration $a_0$, the so-called Milgromian parameter.","However, this non-relativistic model is too rigid (with insufficient parameters) to fit the large diversity of observational phenomena.","In contrast, a relativistic MOND-like gravity naturally emerges from the hyperconical model, which derives a fictitious acceleration compatible with observations.","This study analyses the compatibility of the hyperconical model with respect to RAR observations of 10 galaxy clusters obtained from HIFLUGCS and 60 high-quality SPARC galaxy rotation curves.","The results show that a general relation can be fitted to most cases with only one or two parameters, with an acceptable chi-square and $p$-value.","These findings suggest a possible way to complete the proposed modification of GR on a cosmic scale."],"url":"http://arxiv.org/abs/2405.10019v1","category":"astro-ph.CO"}
{"created":"2024-05-16 11:59:17","title":"Toward double copy on arbitrary backgrounds","abstract":"Double copy relates scattering amplitudes in a web of gravitational and gauge theories. Although it has seen great success when applied to amplitudes in vacuum, far less is known about double copy in arbitrary gravitational and gauge backgrounds. Focussing on the simplest pair production amplitudes of scalar QCD in a background gauge field, we construct, at next-to-leading order in perturbation theory, a double copy map to particle production in general metrics (and associated axio-dilatons) constructed from the gauge background. We connect our results to convolutional and classical double copy and, turning to examples, identify a class of gauge fields which generate FRW spacetimes via double copy. For this case we are able to conjecture the all-orders form of the double copy map.","sentences":["Double copy relates scattering amplitudes in a web of gravitational and gauge theories.","Although it has seen great success when applied to amplitudes in vacuum, far less is known about double copy in arbitrary gravitational and gauge backgrounds.","Focussing on the simplest pair production amplitudes of scalar QCD in a background gauge field, we construct, at next-to-leading order in perturbation theory, a double copy map to particle production in general metrics (and associated axio-dilatons) constructed from the gauge background.","We connect our results to convolutional and classical double copy and, turning to examples, identify a class of gauge fields which generate FRW spacetimes via double copy.","For this case we are able to conjecture the all-orders form of the double copy map."],"url":"http://arxiv.org/abs/2405.10016v1","category":"hep-th"}
{"created":"2024-05-16 11:58:52","title":"Frequency-Domain Refinement with Multiscale Diffusion for Super Resolution","abstract":"The performance of single image super-resolution depends heavily on how to generate and complement high-frequency details to low-resolution images. Recently, diffusion-based models exhibit great potential in generating high-quality images for super-resolution tasks. However, existing models encounter difficulties in directly predicting high-frequency information of wide bandwidth by solely utilizing the high-resolution ground truth as the target for all sampling timesteps. To tackle this problem and achieve higher-quality super-resolution, we propose a novel Frequency Domain-guided multiscale Diffusion model (FDDiff), which decomposes the high-frequency information complementing process into finer-grained steps. In particular, a wavelet packet-based frequency complement chain is developed to provide multiscale intermediate targets with increasing bandwidth for reverse diffusion process. Then FDDiff guides reverse diffusion process to progressively complement the missing high-frequency details over timesteps. Moreover, we design a multiscale frequency refinement network to predict the required high-frequency components at multiple scales within one unified network. Comprehensive evaluations on popular benchmarks are conducted, and demonstrate that FDDiff outperforms prior generative methods with higher-fidelity super-resolution results.","sentences":["The performance of single image super-resolution depends heavily on how to generate and complement high-frequency details to low-resolution images.","Recently, diffusion-based models exhibit great potential in generating high-quality images for super-resolution tasks.","However, existing models encounter difficulties in directly predicting high-frequency information of wide bandwidth by solely utilizing the high-resolution ground truth as the target for all sampling timesteps.","To tackle this problem and achieve higher-quality super-resolution, we propose a novel Frequency Domain-guided multiscale Diffusion model (FDDiff), which decomposes the high-frequency information complementing process into finer-grained steps.","In particular, a wavelet packet-based frequency complement chain is developed to provide multiscale intermediate targets with increasing bandwidth for reverse diffusion process.","Then FDDiff guides reverse diffusion process to progressively complement the missing high-frequency details over timesteps.","Moreover, we design a multiscale frequency refinement network to predict the required high-frequency components at multiple scales within one unified network.","Comprehensive evaluations on popular benchmarks are conducted, and demonstrate that FDDiff outperforms prior generative methods with higher-fidelity super-resolution results."],"url":"http://arxiv.org/abs/2405.10014v1","category":"cs.CV"}
{"created":"2024-05-16 11:58:48","title":"Charged rotating wormholes: charge without charge","abstract":"We present a family of charged rotating wormhole solutions to the Einstein-Maxwell equations, supported by anisotropic matter fields. We first revisit the charged static cases and analyze the conditions for the solution to represent a wormhole geometry. The rotating geometry is obtained by applying the Newman-Janis algorithm to the static geometry. We show the solutions to Maxwell equations in detail. We believe that our wormhole geometry offers a geometric realization corresponding to the concept of 'charge without charge'.","sentences":["We present a family of charged rotating wormhole solutions to the Einstein-Maxwell equations, supported by anisotropic matter fields.","We first revisit the charged static cases and analyze the conditions for the solution to represent a wormhole geometry.","The rotating geometry is obtained by applying the Newman-Janis algorithm to the static geometry.","We show the solutions to Maxwell equations in detail.","We believe that our wormhole geometry offers a geometric realization corresponding to the concept of 'charge without charge'."],"url":"http://arxiv.org/abs/2405.10013v1","category":"gr-qc"}
{"created":"2024-05-16 11:50:38","title":"Tetrahedron equation and Schur functions","abstract":"The tetrahedron equation introduced by Zamolodchikov is a three-dimensional generalization of the Yang-Baxter equation. Several types of solutions to the tetrahedron equation that have connections to quantum groups can be viewed as $q$-oscillator valued vertex models with matrix elements of the $L$-operators given by generators of the $q$-oscillator algebra acting on the Fock space. Using one of the $q=0$-oscillator valued vertex models introduced by Bazhanov-Sergeev, we introduce a family of partition functions that admits an explicit algebraic presentation using Schur functions. Our construction is based on the three-dimensional realization of the Zamolodchikov-Faddeev algebra provided by Kuniba-Okado-Maruyama. Furthermore, we investigate an inhomogeneous generalization of the three-dimensional lattice model. We show that the inhomogeneous analog of (a certain subclass of) partition functions can be expressed as loop elementary symmetric functions.","sentences":["The tetrahedron equation introduced by Zamolodchikov is a three-dimensional generalization of the Yang-Baxter equation.","Several types of solutions to the tetrahedron equation that have connections to quantum groups can be viewed as $q$-oscillator valued vertex models with matrix elements of the $L$-operators given by generators of the $q$-oscillator algebra acting on the Fock space.","Using one of the $q=0$-oscillator valued vertex models introduced by Bazhanov-Sergeev, we introduce a family of partition functions that admits an explicit algebraic presentation using Schur functions.","Our construction is based on the three-dimensional realization of the Zamolodchikov-Faddeev algebra provided by Kuniba-Okado-Maruyama.","Furthermore, we investigate an inhomogeneous generalization of the three-dimensional lattice model.","We show that the inhomogeneous analog of (a certain subclass of) partition functions can be expressed as loop elementary symmetric functions."],"url":"http://arxiv.org/abs/2405.10011v1","category":"math-ph"}
{"created":"2024-05-16 11:49:08","title":"Solving the enigma: Deriving optimal explanations of deep networks","abstract":"The accelerated progress of artificial intelligence (AI) has popularized deep learning models across domains, yet their inherent opacity poses challenges, notably in critical fields like healthcare, medicine and the geosciences. Explainable AI (XAI) has emerged to shed light on these \"black box\" models, helping decipher their decision making process. Nevertheless, different XAI methods yield highly different explanations. This inter-method variability increases uncertainty and lowers trust in deep networks' predictions. In this study, for the first time, we propose a novel framework designed to enhance the explainability of deep networks, by maximizing both the accuracy and the comprehensibility of the explanations. Our framework integrates various explanations from established XAI methods and employs a non-linear \"explanation optimizer\" to construct a unique and optimal explanation. Through experiments on multi-class and binary classification tasks in 2D object and 3D neuroscience imaging, we validate the efficacy of our approach. Our explanation optimizer achieved superior faithfulness scores, averaging 155% and 63% higher than the best performing XAI method in the 3D and 2D applications, respectively. Additionally, our approach yielded lower complexity, increasing comprehensibility. Our results suggest that optimal explanations based on specific criteria are derivable and address the issue of inter-method variability in the current XAI literature.","sentences":["The accelerated progress of artificial intelligence (AI) has popularized deep learning models across domains, yet their inherent opacity poses challenges, notably in critical fields like healthcare, medicine and the geosciences.","Explainable AI (XAI) has emerged to shed light on these \"black box\" models, helping decipher their decision making process.","Nevertheless, different XAI methods yield highly different explanations.","This inter-method variability increases uncertainty and lowers trust in deep networks' predictions.","In this study, for the first time, we propose a novel framework designed to enhance the explainability of deep networks, by maximizing both the accuracy and the comprehensibility of the explanations.","Our framework integrates various explanations from established XAI methods and employs a non-linear \"explanation optimizer\" to construct a unique and optimal explanation.","Through experiments on multi-class and binary classification tasks in 2D object and 3D neuroscience imaging, we validate the efficacy of our approach.","Our explanation optimizer achieved superior faithfulness scores, averaging 155% and 63% higher than the best performing XAI method in the 3D and 2D applications, respectively.","Additionally, our approach yielded lower complexity, increasing comprehensibility.","Our results suggest that optimal explanations based on specific criteria are derivable and address the issue of inter-method variability in the current XAI literature."],"url":"http://arxiv.org/abs/2405.10008v1","category":"cs.CV"}
{"created":"2024-05-16 11:44:35","title":"ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal Image Dataset","abstract":"Automated medical image analysis systems often require large amounts of training data with high quality labels, which are difficult and time consuming to generate. This paper introduces Radiology Object in COntext version 2 (ROCOv2), a multimodal dataset consisting of radiological images and associated medical concepts and captions extracted from the PMC Open Access subset. It is an updated version of the ROCO dataset published in 2018, and adds 35,705 new images added to PMC since 2018. It further provides manually curated concepts for imaging modalities with additional anatomical and directional concepts for X-rays. The dataset consists of 79,789 images and has been used, with minor modifications, in the concept detection and caption prediction tasks of ImageCLEFmedical Caption 2023. The dataset is suitable for training image annotation models based on image-caption pairs, or for multi-label image classification using Unified Medical Language System (UMLS) concepts provided with each image. In addition, it can serve for pre-training of medical domain models, and evaluation of deep learning models for multi-task learning.","sentences":["Automated medical image analysis systems often require large amounts of training data with high quality labels, which are difficult and time consuming to generate.","This paper introduces Radiology Object in COntext version 2 (ROCOv2), a multimodal dataset consisting of radiological images and associated medical concepts and captions extracted from the PMC Open Access subset.","It is an updated version of the ROCO dataset published in 2018, and adds 35,705 new images added to PMC since 2018.","It further provides manually curated concepts for imaging modalities with additional anatomical and directional concepts for X-rays.","The dataset consists of 79,789 images and has been used, with minor modifications, in the concept detection and caption prediction tasks of ImageCLEFmedical Caption 2023.","The dataset is suitable for training image annotation models based on image-caption pairs, or for multi-label image classification using Unified Medical Language System (UMLS) concepts provided with each image.","In addition, it can serve for pre-training of medical domain models, and evaluation of deep learning models for multi-task learning."],"url":"http://arxiv.org/abs/2405.10004v1","category":"eess.IV"}
{"created":"2024-05-16 11:35:06","title":"On Detecting Low-pass Graph Signals under Partial Observations","abstract":"The application of graph signal processing (GSP) on partially observed graph signals with missing nodes has gained attention recently. This is because processing data from large graphs are difficult, if not impossible due to the lack of availability of full observations. Many prior works have been developed using the assumption that the generated graph signals are smooth or low pass filtered. This paper treats a blind graph filter detection problem under this context. We propose a detector that certifies whether the partially observed graph signals are low pass filtered, without requiring the graph topology knowledge. As an example application, our detector leads to a pre-screening method to filter out non low pass signals and thus robustify the prior GSP algorithms. We also bound the sample complexity of our detector in terms of the class of filters, number of observed nodes, etc. Numerical experiments verify the efficacy of our method.","sentences":["The application of graph signal processing (GSP) on partially observed graph signals with missing nodes has gained attention recently.","This is because processing data from large graphs are difficult, if not impossible due to the lack of availability of full observations.","Many prior works have been developed using the assumption that the generated graph signals are smooth or low pass filtered.","This paper treats a blind graph filter detection problem under this context.","We propose a detector that certifies whether the partially observed graph signals are low pass filtered, without requiring the graph topology knowledge.","As an example application, our detector leads to a pre-screening method to filter out non low pass signals and thus robustify the prior GSP algorithms.","We also bound the sample complexity of our detector in terms of the class of filters, number of observed nodes, etc.","Numerical experiments verify the efficacy of our method."],"url":"http://arxiv.org/abs/2405.10001v1","category":"eess.SP"}
{"created":"2024-05-16 11:33:49","title":"Reward Centering","abstract":"We show that discounted methods for solving continuing reinforcement learning problems can perform significantly better if they center their rewards by subtracting out the rewards' empirical average. The improvement is substantial at commonly used discount factors and increases further as the discount factor approaches one. In addition, we show that if a problem's rewards are shifted by a constant, then standard methods perform much worse, whereas methods with reward centering are unaffected. Estimating the average reward is straightforward in the on-policy setting; we propose a slightly more sophisticated method for the off-policy setting. Reward centering is a general idea, so we expect almost every reinforcement-learning algorithm to benefit by the addition of reward centering.","sentences":["We show that discounted methods for solving continuing reinforcement learning problems can perform significantly better if they center their rewards by subtracting out the rewards' empirical average.","The improvement is substantial at commonly used discount factors and increases further as the discount factor approaches one.","In addition, we show that if a problem's rewards are shifted by a constant, then standard methods perform much worse, whereas methods with reward centering are unaffected.","Estimating the average reward is straightforward in the on-policy setting; we propose a slightly more sophisticated method for the off-policy setting.","Reward centering is a general idea, so we expect almost every reinforcement-learning algorithm to benefit by the addition of reward centering."],"url":"http://arxiv.org/abs/2405.09999v1","category":"cs.LG"}
{"created":"2024-05-16 11:30:18","title":"Partial bases and homological stability of $\\operatorname{GL}_{n}(R)$ revisited","abstract":"Let $R$ be a unital ring satisfying the invariant basis number property, that every stably free $R$-module is free, and that the complex of partial bases of every finite rank free module is Cohen--Macaulay. This class of rings includes every ring of stable rank $1$ (e.g. any local, semi-local or Artinian ring), every Euclidean domain, and every Dedekind domain $\\mathcal{O}_S$ of arithmetic type where $|S| > 1$ and $S$ contains at least one non-complex place. Extending recent work of Galatius--Kupers--Randal-Williams and Kupers--Miller--Patzt, we prove that the sequence of general linear groups $\\operatorname{GL}_n(R)$ satisfies slope-$1$ homological stability with $\\mathbb{Z}[1/2]$-coefficients.","sentences":["Let $R$ be a unital ring satisfying the invariant basis number property, that every stably free $R$-module is free, and that the complex of partial bases of every finite rank free module is Cohen--Macaulay.","This class of rings includes every ring of stable rank $1$ (e.g. any local, semi-local or Artinian ring), every Euclidean domain, and every Dedekind domain $\\mathcal{O}_S$ of arithmetic type where $|S| > 1$ and $S$ contains at least one non-complex place.","Extending recent work of Galatius--Kupers--Randal-Williams and Kupers--Miller--Patzt, we prove that the sequence of general linear groups $\\operatorname{GL}_n(R)$ satisfies slope-$1$ homological stability with $\\mathbb{Z}[1/2]$-coefficients."],"url":"http://arxiv.org/abs/2405.09998v1","category":"math.AT"}
{"created":"2024-05-16 11:30:08","title":"Generative Design through Quality-Diversity Data Synthesis and Language Models","abstract":"Two fundamental challenges face generative models in engineering applications: the acquisition of high-performing, diverse datasets, and the adherence to precise constraints in generated designs. We propose a novel approach combining optimization, constraint satisfaction, and language models to tackle these challenges in architectural design. Our method uses Quality-Diversity (QD) to generate a diverse, high-performing dataset. We then fine-tune a language model with this dataset to generate high-level designs. These designs are then refined into detailed, constraint-compliant layouts using the Wave Function Collapse algorithm. Our system demonstrates reliable adherence to textual guidance, enabling the generation of layouts with targeted architectural and performance features. Crucially, our results indicate that data synthesized through the evolutionary search of QD not only improves overall model performance but is essential for the model's ability to closely adhere to textual guidance. This improvement underscores the pivotal role evolutionary computation can play in creating the datasets key to training generative models for design. Web article at https://tilegpt.github.io","sentences":["Two fundamental challenges face generative models in engineering applications: the acquisition of high-performing, diverse datasets, and the adherence to precise constraints in generated designs.","We propose a novel approach combining optimization, constraint satisfaction, and language models to tackle these challenges in architectural design.","Our method uses Quality-Diversity (QD) to generate a diverse, high-performing dataset.","We then fine-tune a language model with this dataset to generate high-level designs.","These designs are then refined into detailed, constraint-compliant layouts using the Wave Function Collapse algorithm.","Our system demonstrates reliable adherence to textual guidance, enabling the generation of layouts with targeted architectural and performance features.","Crucially, our results indicate that data synthesized through the evolutionary search of QD not only improves overall model performance but is essential for the model's ability to closely adhere to textual guidance.","This improvement underscores the pivotal role evolutionary computation can play in creating the datasets key to training generative models for design.","Web article at https://tilegpt.github.io"],"url":"http://arxiv.org/abs/2405.09997v1","category":"cs.NE"}
{"created":"2024-05-16 11:27:22","title":"Semantic Communication via Rate Distortion Perception Bottleneck","abstract":"With the advancement of Artificial Intelligence (AI) technology, next-generation wireless communication network is facing unprecedented challenge. Semantic communication has become a novel solution to address such challenges, with enhancing the efficiency of bandwidth utilization by transmitting meaningful information and filtering out superfluous data. Unfortunately, recent studies have shown that classical Shannon information theory primarily focuses on the bit-level distortion, which cannot adequately address the perceptual quality issues of data reconstruction at the receiver end. In this work, we consider the impact of semantic-level distortion on semantic communication. We develop an image inference network based on the Information Bottleneck (IB) framework and concurrently establish an image reconstruction network. This network is designed to achieve joint optimization of perception and bit-level distortion, as well as image inference, associated with compressing information. To maintain consistency with the principles of IB for handling high-dimensional data, we employ variational approximation methods to simplify the optimization problem. Finally, we confirm the existence of the rate distortion perception tradeoff within IB framework through experimental analysis conducted on the MNIST dataset.","sentences":["With the advancement of Artificial Intelligence (AI) technology, next-generation wireless communication network is facing unprecedented challenge.","Semantic communication has become a novel solution to address such challenges, with enhancing the efficiency of bandwidth utilization by transmitting meaningful information and filtering out superfluous data.","Unfortunately, recent studies have shown that classical Shannon information theory primarily focuses on the bit-level distortion, which cannot adequately address the perceptual quality issues of data reconstruction at the receiver end.","In this work, we consider the impact of semantic-level distortion on semantic communication.","We develop an image inference network based on the Information Bottleneck (IB) framework and concurrently establish an image reconstruction network.","This network is designed to achieve joint optimization of perception and bit-level distortion, as well as image inference, associated with compressing information.","To maintain consistency with the principles of IB for handling high-dimensional data, we employ variational approximation methods to simplify the optimization problem.","Finally, we confirm the existence of the rate distortion perception tradeoff within IB framework through experimental analysis conducted on the MNIST dataset."],"url":"http://arxiv.org/abs/2405.09995v1","category":"eess.SP"}
{"created":"2024-05-16 11:21:02","title":"Histopathology Foundation Models Enable Accurate Ovarian Cancer Subtype Classification","abstract":"Large pretrained transformers are increasingly being developed as generalised foundation models which can underpin powerful task-specific artificial intelligence models. Histopathology foundation models show promise across many tasks, but analyses have been limited by arbitrary hyperparameters that were not tuned to the specific task/dataset. We report the most rigorous single-task validation conducted to date of a histopathology foundation model, and the first performed in ovarian cancer subtyping. Attention-based multiple instance learning classifiers were compared using vision transformer and ResNet features generated through varied preprocessing and pretraining procedures. The training set consisted of 1864 whole slide images from 434 ovarian carcinoma cases at Leeds Hospitals. Five-class classification performance was evaluated through five-fold cross-validation, and these cross-validation models were ensembled for evaluation on a hold-out test set and an external set from the Transcanadian study. Reporting followed the TRIPOD+AI checklist. The vision transformer-based histopathology foundation model, UNI, performed best in every evaluation, with five-class balanced accuracies of 88% and 93% in hold-out internal and external testing, compared to the best ResNet model scores of 68% and 81%, respectively. Normalisations and augmentations aided the generalisability of ResNet-based models, but these still did not match the performance of UNI, which gave the best external performance in any ovarian cancer subtyping study to date. Histopathology foundation models offer a clear benefit to subtyping, improving classification performance to a degree where clinical utility is tangible, albeit with an increased computational burden. Such models could provide a second opinion in challenging cases and may improve the accuracy, objectivity, and efficiency of pathological diagnoses overall.","sentences":["Large pretrained transformers are increasingly being developed as generalised foundation models which can underpin powerful task-specific artificial intelligence models.","Histopathology foundation models show promise across many tasks, but analyses have been limited by arbitrary hyperparameters that were not tuned to the specific task/dataset.","We report the most rigorous single-task validation conducted to date of a histopathology foundation model, and the first performed in ovarian cancer subtyping.","Attention-based multiple instance learning classifiers were compared using vision transformer and ResNet features generated through varied preprocessing and pretraining procedures.","The training set consisted of 1864 whole slide images from 434 ovarian carcinoma cases at Leeds Hospitals.","Five-class classification performance was evaluated through five-fold cross-validation, and these cross-validation models were ensembled for evaluation on a hold-out test set and an external set from the Transcanadian study.","Reporting followed the TRIPOD+AI checklist.","The vision transformer-based histopathology foundation model, UNI, performed best in every evaluation, with five-class balanced accuracies of 88% and 93% in hold-out internal and external testing, compared to the best ResNet model scores of 68% and 81%, respectively.","Normalisations and augmentations aided the generalisability of ResNet-based models, but these still did not match the performance of UNI, which gave the best external performance in any ovarian cancer subtyping study to date.","Histopathology foundation models offer a clear benefit to subtyping, improving classification performance to a degree where clinical utility is tangible, albeit with an increased computational burden.","Such models could provide a second opinion in challenging cases and may improve the accuracy, objectivity, and efficiency of pathological diagnoses overall."],"url":"http://arxiv.org/abs/2405.09990v1","category":"eess.IV"}
{"created":"2024-05-16 11:05:41","title":"VirtualModel: Generating Object-ID-retentive Human-object Interaction Image by Diffusion Model for E-commerce Marketing","abstract":"Due to the significant advances in large-scale text-to-image generation by diffusion model (DM), controllable human image generation has been attracting much attention recently. Existing works, such as Controlnet [36], T2I-adapter [20] and HumanSD [10] have demonstrated good abilities in generating human images based on pose conditions, they still fail to meet the requirements of real e-commerce scenarios. These include (1) the interaction between the shown product and human should be considered, (2) human parts like face/hand/arm/foot and the interaction between human model and product should be hyper-realistic, and (3) the identity of the product shown in advertising should be exactly consistent with the product itself. To this end, in this paper, we first define a new human image generation task for e-commerce marketing, i.e., Object-ID-retentive Human-object Interaction image Generation (OHG), and then propose a VirtualModel framework to generate human images for product shown, which supports displays of any categories of products and any types of human-object interaction. As shown in Figure 1, VirtualModel not only outperforms other methods in terms of accurate pose control and image quality but also allows for the display of user-specified product objects by maintaining the product-ID consistency and enhancing the plausibility of human-object interaction. Codes and data will be released.","sentences":["Due to the significant advances in large-scale text-to-image generation by diffusion model (DM), controllable human image generation has been attracting much attention recently.","Existing works, such as Controlnet","[36], T2I-adapter [20] and HumanSD","[10] have demonstrated good abilities in generating human images based on pose conditions, they still fail to meet the requirements of real e-commerce scenarios.","These include (1) the interaction between the shown product and human should be considered, (2) human parts like face/hand/arm/foot and the interaction between human model and product should be hyper-realistic, and (3) the identity of the product shown in advertising should be exactly consistent with the product itself.","To this end, in this paper, we first define a new human image generation task for e-commerce marketing, i.e., Object-ID-retentive Human-object Interaction image Generation (OHG), and then propose a VirtualModel framework to generate human images for product shown, which supports displays of any categories of products and any types of human-object interaction.","As shown in Figure 1, VirtualModel not only outperforms other methods in terms of accurate pose control and image quality but also allows for the display of user-specified product objects by maintaining the product-ID consistency and enhancing the plausibility of human-object interaction.","Codes and data will be released."],"url":"http://arxiv.org/abs/2405.09985v1","category":"cs.CV"}
{"created":"2024-05-16 11:04:04","title":"Mode of sustainable economic development","abstract":"To implement the previously formulated principles of sustainable economic development, all non-negative solutions of the linear system of equations and inequalities, which are satisfied by the vector of real consumption, are completely described. It is established that the vector of real consumption with the minimum level of excess supply is determined by the solution of some quadratic programming problem. The necessary and sufficient conditions are established under which the economic system, described by the \"input-output\" production model, functions in the mode of sustainable development. A complete description of the equilibrium states for which markets are partially cleared in the economy model of production \"input-output\" is given, on the basis that all solutions of system of linear equations and inequalities are completely described. The existence of a family of taxation vectors in the \"input-output\" model of production, under which the economic system is able to function in the mode of sustainable development, is proved. Restrictions were found for the vector of taxation in the economic system, under which the economic system is able to function in the mode of sustainable development. The axioms of the aggregated description of the economy is proposed.","sentences":["To implement the previously formulated principles of sustainable economic development, all non-negative solutions of the linear system of equations and inequalities, which are satisfied by the vector of real consumption, are completely described.","It is established that the vector of real consumption with the minimum level of excess supply is determined by the solution of some quadratic programming problem.","The necessary and sufficient conditions are established under which the economic system, described by the \"input-output\" production model, functions in the mode of sustainable development.","A complete description of the equilibrium states for which markets are partially cleared in the economy model of production \"input-output\" is given, on the basis that all solutions of system of linear equations and inequalities are completely described.","The existence of a family of taxation vectors in the \"input-output\" model of production, under which the economic system is able to function in the mode of sustainable development, is proved.","Restrictions were found for the vector of taxation in the economic system, under which the economic system is able to function in the mode of sustainable development.","The axioms of the aggregated description of the economy is proposed."],"url":"http://arxiv.org/abs/2405.09984v1","category":"econ.GN"}
{"created":"2024-05-16 10:54:26","title":"Adversarial Robustness for Visual Grounding of Multimodal Large Language Models","abstract":"Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs. To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows. Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object. Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box. In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image. Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs. Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs.","sentences":["Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities.","However, the adversarial robustness of visual grounding remains unexplored in MLLMs.","To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows.","Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object.","Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box.","In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image.","Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs.","Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs."],"url":"http://arxiv.org/abs/2405.09981v1","category":"cs.CV"}
{"created":"2024-05-16 10:53:31","title":"FinTextQA: A Dataset for Long-form Financial Question Answering","abstract":"Accurate evaluation of financial question answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold.","sentences":["Accurate evaluation of financial question answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts.","However, current financial QA datasets lack scope diversity and question complexity.","This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance.","FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.","Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator.","A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions.","The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold."],"url":"http://arxiv.org/abs/2405.09980v1","category":"cs.CL"}
{"created":"2024-05-16 10:45:04","title":"Pedestrian evacuations with imitation of cooperative behavior","abstract":"We analyze the dynamics of room evacuation for mixed populations that include both competitive and cooperative individuals through numerical simulations using the social force model. Cooperative agents represent well-trained individuals who know how to behave in order to reduce risks within high-density crowds. We consider that competitive agents can imitate cooperative behavior when they are in close proximity to cooperators. We study the effects of the imitation of cooperative behavior on the duration and safety of evacuations, analyzing evacuation time and other quantities of interest for varying parameters such as the proportions of mixing, the aspect ratio of the room, and the parameters characterizing individual behaviors. Our main findings reveal that the addition of a relatively small number of cooperative agents into a crowd can reduce evacuation time and the density near the exit door, making the evacuation faster and safer despite an increase in the total number of agents. In particular, for long spaces such as corridors, a small number of added cooperative agents can significantly facilitate the evacuation process. We compare our results with those of systems without imitation and also study the general role of cooperation, providing further analysis for homogeneous populations. Our main conclusions emphasize the potential relevance of training people how to behave in high-density crowds","sentences":["We analyze the dynamics of room evacuation for mixed populations that include both competitive and cooperative individuals through numerical simulations using the social force model.","Cooperative agents represent well-trained individuals who know how to behave in order to reduce risks within high-density crowds.","We consider that competitive agents can imitate cooperative behavior when they are in close proximity to cooperators.","We study the effects of the imitation of cooperative behavior on the duration and safety of evacuations, analyzing evacuation time and other quantities of interest for varying parameters such as the proportions of mixing, the aspect ratio of the room, and the parameters characterizing individual behaviors.","Our main findings reveal that the addition of a relatively small number of cooperative agents into a crowd can reduce evacuation time and the density near the exit door, making the evacuation faster and safer despite an increase in the total number of agents.","In particular, for long spaces such as corridors, a small number of added cooperative agents can significantly facilitate the evacuation process.","We compare our results with those of systems without imitation and also study the general role of cooperation, providing further analysis for homogeneous populations.","Our main conclusions emphasize the potential relevance of training people how to behave in high-density crowds"],"url":"http://arxiv.org/abs/2405.09978v1","category":"physics.soc-ph"}
{"created":"2024-05-16 10:41:37","title":"Generalized Conditional Displacement","abstract":"Conditional displacement with a qubit ancilla is a critical component in continuous-variable error correction protocols. We present the generalized conditional displacement operator, conditioned on a qudit ancilla, showing how it enhances error-correction with Gottesman-Kitaev-Preskill (GKP) codes and exploring potential implementations.","sentences":["Conditional displacement with a qubit ancilla is a critical component in continuous-variable error correction protocols.","We present the generalized conditional displacement operator, conditioned on a qudit ancilla, showing how it enhances error-correction with Gottesman-Kitaev-Preskill (GKP) codes and exploring potential implementations."],"url":"http://arxiv.org/abs/2405.09977v1","category":"quant-ph"}
{"created":"2024-05-16 10:32:39","title":"Predicting Solar Heat Production to Optimize Renewable Energy Usage","abstract":"Utilizing solar energy to meet space heating and domestic hot water demand is very efficient (in terms of environmental footprint as well as cost), but in order to ensure that user demand is entirely covered throughout the year needs to be complemented with auxiliary heating systems, typically boilers and heat pumps. Naturally, the optimal control of such a system depends on an accurate prediction of solar thermal production.   Experimental testing and physics-based numerical models are used to find a collector's performance curve - the mapping from solar radiation and other external conditions to heat production - but this curve changes over time once the collector is exposed to outdoor conditions. In order to deploy advanced control strategies in small domestic installations, we present an approach that uses machine learning to automatically construct and continuously adapt a model that predicts heat production. Our design is driven by the need to (a) construct and adapt models using supervision that can be extracted from low-cost instrumentation, avoiding extreme accuracy and reliability requirements; and (b) at inference time, use inputs that are typically provided in publicly available weather forecasts.   Recent developments in attention-based machine learning, as well as careful adaptation of the training setup to the specifics of the task, have allowed us to design a machine learning-based solution that covers our requirements. We present positive empirical results for the predictive accuracy of our solution, and discuss the impact of these results on the end-to-end system.","sentences":["Utilizing solar energy to meet space heating and domestic hot water demand is very efficient (in terms of environmental footprint as well as cost), but in order to ensure that user demand is entirely covered throughout the year needs to be complemented with auxiliary heating systems, typically boilers and heat pumps.","Naturally, the optimal control of such a system depends on an accurate prediction of solar thermal production.   ","Experimental testing and physics-based numerical models are used to find a collector's performance curve - the mapping from solar radiation and other external conditions to heat production - but this curve changes over time once the collector is exposed to outdoor conditions.","In order to deploy advanced control strategies in small domestic installations, we present an approach that uses machine learning to automatically construct and continuously adapt a model that predicts heat production.","Our design is driven by the need to (a) construct and adapt models using supervision that can be extracted from low-cost instrumentation, avoiding extreme accuracy and reliability requirements; and (b) at inference time, use inputs that are typically provided in publicly available weather forecasts.   ","Recent developments in attention-based machine learning, as well as careful adaptation of the training setup to the specifics of the task, have allowed us to design a machine learning-based solution that covers our requirements.","We present positive empirical results for the predictive accuracy of our solution, and discuss the impact of these results on the end-to-end system."],"url":"http://arxiv.org/abs/2405.09972v1","category":"cs.LG"}
{"created":"2024-05-16 10:31:25","title":"Cut Elimination of Intuitionistic Tense Logic","abstract":"In this paper, we use a new method to prove cut-elimination of intuitionistic tense logic. This method focuses on splitting the contraction rule and cut rules. Further general theories and applications of this method shall be developed in the future.","sentences":["In this paper, we use a new method to prove cut-elimination of intuitionistic tense logic.","This method focuses on splitting the contraction rule and cut rules.","Further general theories and applications of this method shall be developed in the future."],"url":"http://arxiv.org/abs/2405.09970v1","category":"math.LO"}
{"created":"2024-05-16 10:30:18","title":"New insights on null and timelike warped symmetric spacetime splittings","abstract":"We explore in detail the 2+2 and 1+1+2 formalism in spherically symmetric spacetimes, spanning from deducing the dynamical equations to relating them to the well-known generalised Painlev\\'e-Gullstrand (GPG) coordinate system. The evolution equations are the Raychaudhuri equations for null rays, including those also known as cross-focusing equations whose derivation, to the best of our knowledge, we present for the first time. We physically interpret the scalars that arise in this scenario, namely the flow 2-expansion $\\Theta_{n}$, the flow acceleration $\\mathcal{A}$, and the radial extrinsic curvature $\\mathcal{B}$. We derive a coordinate independent formula for the redshift which shows that $\\mathcal{B}$ is the sole source for the redshift in spherically symmetric spacetimes. We also establish the correspondence between the 1+1+2 scalars and the 1+3 splitting scalars, expansion and shear. We further make a comparison with the Newman-Penrose formalism, in order to clarify the context where each formalism is more useful, and finally, we extend our results to planar and hyperbolic symmetric warped spacetimes as well, in particular, the relationship between $\\mathcal{B}$ and the redshift.","sentences":["We explore in detail the 2+2 and 1+1+2 formalism in spherically symmetric spacetimes, spanning from deducing the dynamical equations to relating them to the well-known generalised Painlev\\'e-Gullstrand (GPG) coordinate system.","The evolution equations are the Raychaudhuri equations for null rays, including those also known as cross-focusing equations whose derivation, to the best of our knowledge, we present for the first time.","We physically interpret the scalars that arise in this scenario, namely the flow 2-expansion $\\Theta_{n}$, the flow acceleration $\\mathcal{A}$, and the radial extrinsic curvature $\\mathcal{B}$. We derive a coordinate independent formula for the redshift which shows that $\\mathcal{B}$ is the sole source for the redshift in spherically symmetric spacetimes.","We also establish the correspondence between the 1+1+2 scalars and the 1+3 splitting scalars, expansion and shear.","We further make a comparison with the Newman-Penrose formalism, in order to clarify the context where each formalism is more useful, and finally, we extend our results to planar and hyperbolic symmetric warped spacetimes as well, in particular, the relationship between $\\mathcal{B}$ and the redshift."],"url":"http://arxiv.org/abs/2405.09968v1","category":"gr-qc"}
{"created":"2024-05-16 10:28:31","title":"Unveiling stellar aurorae: Simulating auroral emission lines in hot stars induced by high-energy irradiation","abstract":"Auroral emission lines result from the interaction between magnetic field and stellar wind, offering valuable insights into physical properties and processes occurring within magnetospheres of celestial bodies. While extensively studied in planetary and exoplanetary atmospheres, in ultra-cool dwarfs, and as radio emission from early-type stars, the presence of specific auroral emission lines in hot star spectra remains unexplored. In this study, we utilized TLUSTY code to simulate the auroral lines, while modelling the effect of the interaction between stellar wind and magnetosphere through X-ray irradiation. Utilizing high-resolution synthetic spectra generated from model atmospheres, we identified potential candidate lines indicative of auroral emission, which were absent in non-irradiated spectra. Emission lines in synthetic spectra were present primarily in the infrared domain. The most prominent line generated by irradiation was He ii 69458 A, which appeared in all our model atmospheres with effective temperatures ranging from 15 kK to 30 kK. We also calculated the minimum irradiation required to detect emission in this most prominent line. The presence of emission lines was interpreted by considering changes in the population of different excited states of given atoms. Besides the appearance of infrared emission lines, high-energy irradiation causes infrared excess. To complement our simulations, we also searched for auroral lines in Far Ultraviolet Spectroscopic Explorer (FUSE) observations, which are deposited in the Multimission Archive at Space Telescope (MAST) catalogue. The comparison of observed spectra with synthetic spectra did not identify any possible candidate emission lines in FUSE spectra.","sentences":["Auroral emission lines result from the interaction between magnetic field and stellar wind, offering valuable insights into physical properties and processes occurring within magnetospheres of celestial bodies.","While extensively studied in planetary and exoplanetary atmospheres, in ultra-cool dwarfs, and as radio emission from early-type stars, the presence of specific auroral emission lines in hot star spectra remains unexplored.","In this study, we utilized TLUSTY code to simulate the auroral lines, while modelling the effect of the interaction between stellar wind and magnetosphere through X-ray irradiation.","Utilizing high-resolution synthetic spectra generated from model atmospheres, we identified potential candidate lines indicative of auroral emission, which were absent in non-irradiated spectra.","Emission lines in synthetic spectra were present primarily in the infrared domain.","The most prominent line generated by irradiation was He ii 69458 A, which appeared in all our model atmospheres with effective temperatures ranging from 15 kK to 30 kK. We also calculated the minimum irradiation required to detect emission in this most prominent line.","The presence of emission lines was interpreted by considering changes in the population of different excited states of given atoms.","Besides the appearance of infrared emission lines, high-energy irradiation causes infrared excess.","To complement our simulations, we also searched for auroral lines in Far Ultraviolet Spectroscopic Explorer (FUSE) observations, which are deposited in the Multimission Archive at Space Telescope (MAST) catalogue.","The comparison of observed spectra with synthetic spectra did not identify any possible candidate emission lines in FUSE spectra."],"url":"http://arxiv.org/abs/2405.09967v1","category":"astro-ph.SR"}
{"created":"2024-05-16 10:23:02","title":"Tempered Fractional Hawkes Process and Its Generalization","abstract":"Hawkes process (HP) is a point process with a conditionally dependent intensity function. This paper defines the tempered fractional Hawkes process (TFHP) by time-changing the HP with an inverse tempered stable subordinator. We obtained results that generalize the fractional Hawkes process defined in Hainaut (2020) to a tempered version which has \\textit{semi-heavy tailed} decay. We derive the mean, the variance, covariance and the governing fractional difference-differential equations of the TFHP. Additionally, we introduce the generalized fractional Hawkes process (GFHP) by time-changing the HP with the inverse L\\'evy subordinator. This definition encompasses all potential (inverse L\\'evy) time changes as specific instances. We also explore the distributional characteristics and the governing difference-differential equation of the one-dimensional distribution for the GFHP.","sentences":["Hawkes process (HP) is a point process with a conditionally dependent intensity function.","This paper defines the tempered fractional Hawkes process (TFHP) by time-changing the HP with an inverse tempered stable subordinator.","We obtained results that generalize the fractional Hawkes process defined in Hainaut (2020) to a tempered version which has \\textit{semi-heavy tailed} decay.","We derive the mean, the variance, covariance and the governing fractional difference-differential equations of the TFHP.","Additionally, we introduce the generalized fractional Hawkes process (GFHP) by time-changing the HP with the inverse L\\'evy subordinator.","This definition encompasses all potential (inverse L\\'evy) time changes as specific instances.","We also explore the distributional characteristics and the governing difference-differential equation of the one-dimensional distribution for the GFHP."],"url":"http://arxiv.org/abs/2405.09966v1","category":"math.PR"}
{"created":"2024-05-16 10:21:03","title":"Leveraging Large Language Models for Automated Web-Form-Test Generation: An Empirical Study","abstract":"The testing of web forms is an essential activity for ensuring the quality of web applications, which mainly involves evaluating the interactions between users and forms. Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have great potential for contextual text generation. OpenAI's GPT LLMs have been receiving a lot of attention in software testing, however, they may fail to be applied in practice because of information security concerns. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. According to the experimental results, different LLMs can achieve different testing effectiveness. Notably, the GPT-4, GLM-4, and Baichuan2 LLMs can generate better web-form tests than the others. Compared with GPT-4, other LLMs find it difficult to generate appropriate tests for web forms, resulting in decreased successfully-submitted rates (SSRs, measured by the proportions of the LLMs-generated web-form tests that can be successfully inserted into the web forms and submitted) ranging from 9.10% to 74.15%. Nevertheless, some LLMs achieve higher SSRs than GPT-3.5, indicating a better ability to generate appropriate tests for web forms. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Finally, we offer some insights for using LLMs to guide automated web-form testing.","sentences":["The testing of web forms is an essential activity for ensuring the quality of web applications, which mainly involves evaluating the interactions between users and forms.","Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests.","Large Language Models (LLMs) have great potential for contextual text generation.","OpenAI's GPT LLMs have been receiving a lot of attention in software testing, however, they may fail to be applied in practice because of information security concerns.","To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation.","To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications.","According to the experimental results, different LLMs can achieve different testing effectiveness.","Notably, the GPT-4, GLM-4, and Baichuan2 LLMs can generate better web-form tests than the others.","Compared with GPT-4, other LLMs find it difficult to generate appropriate tests for web forms, resulting in decreased successfully-submitted rates (SSRs, measured by the proportions of the LLMs-generated web-form tests that can be successfully inserted into the web forms and submitted) ranging from 9.10% to 74.15%.","Nevertheless, some LLMs achieve higher SSRs than GPT-3.5, indicating a better ability to generate appropriate tests for web forms.","Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated.","Finally, we offer some insights for using LLMs to guide automated web-form testing."],"url":"http://arxiv.org/abs/2405.09965v1","category":"cs.SE"}
{"created":"2024-05-16 10:15:16","title":"KPNDepth: Depth Estimation of Lane Images under Complex Rainy Environment","abstract":"With the development of deep neural network generative models in recent years, significant progress has been made in the research of depth estimation in lane scenes. However, current research achievements are mainly focused on clear daytime scenarios. In complex rainy environments, the influence of rain streaks and local fog effects often leads to erroneous increases in the overall depth estimation values in images. Moreover, these natural factors can introduce disturbances to the accurate prediction of depth boundaries in images. In this paper, we investigate lane depth estimation in complex rainy environments. Based on the concept of convolutional kernel prediction, we propose a dual-layer pixel-wise convolutional kernel prediction network trained on offline data. By predicting two sets of independent convolutional kernels for the target image, we restore the depth information loss caused by complex environmental factors and address the issue of rain streak artifacts generated by a single convolutional kernel set. Furthermore, considering the lack of real rainy lane data currently available, we introduce an image synthesis algorithm, RCFLane, which comprehensively considers the darkening of the environment due to rainfall and local fog effects. We create a synthetic dataset containing 820 experimental images, which we refer to as RainKITTI, on the commonly used depth estimation dataset KITTI. Extensive experiments demonstrate that our proposed depth estimation framework achieves favorable results in highly complex lane rainy environments.","sentences":["With the development of deep neural network generative models in recent years, significant progress has been made in the research of depth estimation in lane scenes.","However, current research achievements are mainly focused on clear daytime scenarios.","In complex rainy environments, the influence of rain streaks and local fog effects often leads to erroneous increases in the overall depth estimation values in images.","Moreover, these natural factors can introduce disturbances to the accurate prediction of depth boundaries in images.","In this paper, we investigate lane depth estimation in complex rainy environments.","Based on the concept of convolutional kernel prediction, we propose a dual-layer pixel-wise convolutional kernel prediction network trained on offline data.","By predicting two sets of independent convolutional kernels for the target image, we restore the depth information loss caused by complex environmental factors and address the issue of rain streak artifacts generated by a single convolutional kernel set.","Furthermore, considering the lack of real rainy lane data currently available, we introduce an image synthesis algorithm, RCFLane, which comprehensively considers the darkening of the environment due to rainfall and local fog effects.","We create a synthetic dataset containing 820 experimental images, which we refer to as RainKITTI, on the commonly used depth estimation dataset KITTI.","Extensive experiments demonstrate that our proposed depth estimation framework achieves favorable results in highly complex lane rainy environments."],"url":"http://arxiv.org/abs/2405.09964v1","category":"cs.CV"}
{"created":"2024-05-16 10:07:30","title":"Patient-Specific Real-Time Segmentation in Trackerless Brain Ultrasound","abstract":"Intraoperative ultrasound (iUS) imaging has the potential to improve surgical outcomes in brain surgery. However, its interpretation is challenging, even for expert neurosurgeons. In this work, we designed the first patient-specific framework that performs brain tumor segmentation in trackerless iUS. To disambiguate ultrasound imaging and adapt to the neurosurgeon's surgical objective, a patient-specific real-time network is trained using synthetic ultrasound data generated by simulating virtual iUS sweep acquisitions in pre-operative MR data. Extensive experiments performed in real ultrasound data demonstrate the effectiveness of the proposed approach, allowing for adapting to the surgeon's definition of surgical targets and outperforming non-patient-specific models, neurosurgeon experts, and high-end tracking systems. Our code is available at: \\url{https://github.com/ReubenDo/MHVAE-Seg}.","sentences":["Intraoperative ultrasound (iUS) imaging has the potential to improve surgical outcomes in brain surgery.","However, its interpretation is challenging, even for expert neurosurgeons.","In this work, we designed the first patient-specific framework that performs brain tumor segmentation in trackerless iUS.","To disambiguate ultrasound imaging and adapt to the neurosurgeon's surgical objective, a patient-specific real-time network is trained using synthetic ultrasound data generated by simulating virtual iUS sweep acquisitions in pre-operative MR data.","Extensive experiments performed in real ultrasound data demonstrate the effectiveness of the proposed approach, allowing for adapting to the surgeon's definition of surgical targets and outperforming non-patient-specific models, neurosurgeon experts, and high-end tracking systems.","Our code is available at: \\url{https://github.com/ReubenDo/MHVAE-Seg}."],"url":"http://arxiv.org/abs/2405.09959v1","category":"eess.IV"}
{"created":"2024-05-16 09:55:25","title":"Low-rank Tree Tensor Network Operators for Long-Range Pairwise Interactions","abstract":"Compactly representing and efficently applying linear operators are fundamental ingredients in tensor network methods for simulating quantum many-body problems and solving high-dimensional problems in scientific computing. In this work, we study such representations for tree tensor networks, the so called tree tensor network operators (TTNOs), paying particular attention to Hamiltonian operators that involve long-range pairwise interactions between particles. Generalizing the work by Lin, Tong, and others on matrix product operators, we establish a direct connection between the hierarchical low-rank structure of the interaction matrix and the TTNO property. This connection allows us to arrive at very compact TTNO representations by compressing the interaction matrix into a hierarchically semi-separable matrix. Numerical experiments for different quantum spin systems validate our results and highlight the potential advantages of TTNOs over matrix product operators.","sentences":["Compactly representing and efficently applying linear operators are fundamental ingredients in tensor network methods for simulating quantum many-body problems and solving high-dimensional problems in scientific computing.","In this work, we study such representations for tree tensor networks, the so called tree tensor network operators (TTNOs), paying particular attention to Hamiltonian operators that involve long-range pairwise interactions between particles.","Generalizing the work by Lin, Tong, and others on matrix product operators, we establish a direct connection between the hierarchical low-rank structure of the interaction matrix and the TTNO property.","This connection allows us to arrive at very compact TTNO representations by compressing the interaction matrix into a hierarchically semi-separable matrix.","Numerical experiments for different quantum spin systems validate our results and highlight the potential advantages of TTNOs over matrix product operators."],"url":"http://arxiv.org/abs/2405.09952v1","category":"math.NA"}
{"created":"2024-05-16 09:55:15","title":"A Review of Multiple Access Techniques for Intelligent Reflecting Surface-Assisted Systems","abstract":"Intelligent Reflecting Surface (IRS) is envisioned to be a technical enabler for the sixth-generation (6G) wireless system. Its potential lies in delivering high performance while maintaining both power efficiency and cost-effectiveness. Previous studies have primarily focused on point-to-point IRS communications involving a single user. Nevertheless, a practical system must serve multiple users simultaneously. The unique characteristics of IRS, such as non-frequency-selective reflection and the necessity for joint active/passive beamforming, create obstacles to the use of conventional multiple access (MA) techniques. This motivates us to review various MA techniques to make clear their functionalities in the presence of IRS. Through this paper, our aim is to provide researchers with a comprehensive understanding of challenges and available solutions, offering insights to foster their design of efficient multiple access for IRS-aided systems.","sentences":["Intelligent Reflecting Surface (IRS) is envisioned to be a technical enabler for the sixth-generation (6G) wireless system.","Its potential lies in delivering high performance while maintaining both power efficiency and cost-effectiveness.","Previous studies have primarily focused on point-to-point IRS communications involving a single user.","Nevertheless, a practical system must serve multiple users simultaneously.","The unique characteristics of IRS, such as non-frequency-selective reflection and the necessity for joint active/passive beamforming, create obstacles to the use of conventional multiple access (MA) techniques.","This motivates us to review various MA techniques to make clear their functionalities in the presence of IRS.","Through this paper, our aim is to provide researchers with a comprehensive understanding of challenges and available solutions, offering insights to foster their design of efficient multiple access for IRS-aided systems."],"url":"http://arxiv.org/abs/2405.09951v1","category":"cs.IT"}
{"created":"2024-05-16 09:53:29","title":"Homogenization of the Dirac operator with position-dependent mass","abstract":"We address the homogenization of the two-dimensional Dirac operator with position-dependent mass. The mass is piecewise constant and supported on small pairwise disjoint inclusions evenly distributed along an $\\varepsilon$-periodic square lattice. Under rather general assumptions on geometry of these inclusions we prove that the corresponding family of Dirac operators converges as $\\varepsilon\\to 0$ in the norm resolvent sense to the Dirac operator with a constant effective mass provided the masses in the inclusions are adjusted to the scaling of the geometry. We also estimate the speed of this convergence in terms of the scaling rates.","sentences":["We address the homogenization of the two-dimensional Dirac operator with position-dependent mass.","The mass is piecewise constant and supported on small pairwise disjoint inclusions evenly distributed along an $\\varepsilon$-periodic square lattice.","Under rather general assumptions on geometry of these inclusions we prove that the corresponding family of Dirac operators converges as $\\varepsilon\\to 0$ in the norm resolvent sense to the Dirac operator with a constant effective mass provided the masses in the inclusions are adjusted to the scaling of the geometry.","We also estimate the speed of this convergence in terms of the scaling rates."],"url":"http://arxiv.org/abs/2405.09949v1","category":"math.AP"}
{"created":"2024-05-16 09:52:21","title":"Mitigating Text Toxicity with Counterfactual Generation","abstract":"Toxicity mitigation consists in rephrasing text in order to remove offensive or harmful meaning. Neural natural language processing (NLP) models have been widely used to target and mitigate textual toxicity. However, existing methods fail to detoxify text while preserving the initial non-toxic meaning at the same time. In this work, we propose to apply counterfactual generation methods from the eXplainable AI (XAI) field to target and mitigate textual toxicity. In particular, we perform text detoxification by applying local feature importance and counterfactual generation methods to a toxicity classifier distinguishing between toxic and non-toxic texts. We carry out text detoxification through counterfactual generation on three datasets and compare our approach to three competitors. Automatic and human evaluations show that recently developed NLP counterfactual generators can mitigate toxicity accurately while better preserving the meaning of the initial text as compared to classical detoxification methods. Finally, we take a step back from using automated detoxification tools, and discuss how to manage the polysemous nature of toxicity and the risk of malicious use of detoxification tools. This work is the first to bridge the gap between counterfactual generation and text detoxification and paves the way towards more practical application of XAI methods.","sentences":["Toxicity mitigation consists in rephrasing text in order to remove offensive or harmful meaning.","Neural natural language processing (NLP) models have been widely used to target and mitigate textual toxicity.","However, existing methods fail to detoxify text while preserving the initial non-toxic meaning at the same time.","In this work, we propose to apply counterfactual generation methods from the eXplainable AI (XAI) field to target and mitigate textual toxicity.","In particular, we perform text detoxification by applying local feature importance and counterfactual generation methods to a toxicity classifier distinguishing between toxic and non-toxic texts.","We carry out text detoxification through counterfactual generation on three datasets and compare our approach to three competitors.","Automatic and human evaluations show that recently developed NLP counterfactual generators can mitigate toxicity accurately while better preserving the meaning of the initial text as compared to classical detoxification methods.","Finally, we take a step back from using automated detoxification tools, and discuss how to manage the polysemous nature of toxicity and the risk of malicious use of detoxification tools.","This work is the first to bridge the gap between counterfactual generation and text detoxification and paves the way towards more practical application of XAI methods."],"url":"http://arxiv.org/abs/2405.09948v1","category":"cs.CL"}
{"created":"2024-05-16 09:51:49","title":"A Nonabelian Hodge Correspondence for Principal Bundles in Positive Characteristic","abstract":"In this paper, we prove a nonabelian Hodge correspondence for principal bundles on a smooth variety $X$ in positive characteristic, which generalizes the Ogus-Vologodsky correspondence for vector bundles. Then we extend the correspondence to logahoric torsors over a log pair $(X,D)$, where $D$ a reduced normal crossing divisor in $X$. As an intermediate step, we prove a correspondence between principal bundles on root stacks $\\mathscr{X}$ and parahoric torsors on $(X,D)$, which generalizes the correspondence on curves given by Balaji--Seshadri to higher dimensional case.","sentences":["In this paper, we prove a nonabelian Hodge correspondence for principal bundles on a smooth variety $X$ in positive characteristic, which generalizes the Ogus-Vologodsky correspondence for vector bundles.","Then we extend the correspondence to logahoric torsors over a log pair $(X,D)$, where $D$ a reduced normal crossing divisor in $X$. As an intermediate step, we prove a correspondence between principal bundles on root stacks $\\mathscr{X}$ and parahoric torsors on $(X,D)$, which generalizes the correspondence on curves given by Balaji--Seshadri to higher dimensional case."],"url":"http://arxiv.org/abs/2405.09947v1","category":"math.AG"}
{"created":"2024-05-16 09:51:41","title":"On the logical structure of some maximality and well-foundedness principles equivalent to choice principles","abstract":"We study the logical structure of Teichm{\\\"u}ller-Tukey lemma, a maximality principle equivalent to the axiom of choice and show that it corresponds to the generalisation to arbitrary cardinals of update induction, a well-foundedness principle from constructive mathematics classically equivalent to the axiom of dependent choice.From there, we state general forms of maximality and well-foundedness principles equivalent to the axiom of choice, including a variant of Zorn's lemma. A comparison with the general class of choice and bar induction principles given by Brede and the first author is initiated.","sentences":["We study the logical structure of Teichm{\\\"u}ller-Tukey lemma, a maximality principle equivalent to the axiom of choice and show that it corresponds to the generalisation to arbitrary cardinals of update induction, a well-foundedness principle from constructive mathematics classically equivalent to the axiom of dependent choice.","From there, we state general forms of maximality and well-foundedness principles equivalent to the axiom of choice, including a variant of Zorn's lemma.","A comparison with the general class of choice and bar induction principles given by Brede and the first author is initiated."],"url":"http://arxiv.org/abs/2405.09946v1","category":"cs.LO"}
{"created":"2024-05-16 09:50:42","title":"Near-horizon chaos beyond Einstein gravity","abstract":"We investigate chaos in the dynamics of outgoing massless particles near the horizon of static spherically symmetric (SSS) black holes in two well-motivated models of $f(R)$ gravity. In both these models, we probe chaos in the particle trajectories (under suitable harmonic confinement) in the vicinity of the black hole horizons, for a set of initial conditions. The particle trajectories, associated Poincar$\\acute{e}$ sections, and Lyapunov exponents clearly illustrate the role played by the black hole horizon in the growth of chaos within a specific energy range. We demonstrate how this energy range is controlled by the parameters of the modified gravity theory under consideration. The growth of chaos in such a classical setting is known to respect a surface gravity bound arising from universal aspects of particle dynamics close to the black hole horizon [K. Hashimoto and N. Tanahashi, Phys. Rev. D 95, 024007 (2017)], analogous to the quantum MSS bound [J. Maldacena, S.H. Shenker and D. Stanford, JHEP 08 (2016) 106]. Interestingly, both models studied in our work respect the bound, in contrast to some of the other models of $f(R)$ gravity in the existing literature. The work serves as a motivation to use chaos as an additional tool to probe Einstein gravity in the strong gravity regime in the vicinity of black hole horizons.","sentences":["We investigate chaos in the dynamics of outgoing massless particles near the horizon of static spherically symmetric (SSS) black holes in two well-motivated models of $f(R)$ gravity.","In both these models, we probe chaos in the particle trajectories (under suitable harmonic confinement) in the vicinity of the black hole horizons, for a set of initial conditions.","The particle trajectories, associated Poincar$\\acute{e}$ sections, and Lyapunov exponents clearly illustrate the role played by the black hole horizon in the growth of chaos within a specific energy range.","We demonstrate how this energy range is controlled by the parameters of the modified gravity theory under consideration.","The growth of chaos in such a classical setting is known to respect a surface gravity bound arising from universal aspects of particle dynamics close to the black hole horizon","[K. Hashimoto and N. Tanahashi, Phys.","Rev. D 95, 024007 (2017)], analogous to the quantum MSS bound [J. Maldacena, S.H. Shenker and D. Stanford, JHEP 08 (2016) 106].","Interestingly, both models studied in our work respect the bound, in contrast to some of the other models of $f(R)$ gravity in the existing literature.","The work serves as a motivation to use chaos as an additional tool to probe Einstein gravity in the strong gravity regime in the vicinity of black hole horizons."],"url":"http://arxiv.org/abs/2405.09945v1","category":"gr-qc"}
{"created":"2024-05-16 09:42:37","title":"SciQAG: A Framework for Auto-Generated Scientific Question Answering Dataset with Fine-grained Evaluation","abstract":"The use of question-answer (QA) pairs for training and evaluating large language models (LLMs) has attracted considerable attention. Yet few available QA datasets are based on knowledge from the scientific literature. Here we bridge this gap by presenting Automatic Generation of Scientific Question Answers (SciQAG), a framework for automatic generation and evaluation of scientific QA pairs sourced from published scientific literature. We fine-tune an open-source LLM to generate \\num{960000} scientific QA pairs from full-text scientific papers and propose a five-dimensional metric to evaluate the quality of the generated QA pairs. We show via LLM-based evaluation that the generated QA pairs consistently achieve an average score of 2.5 out of 3 across five dimensions, indicating that our framework can distill key knowledge from papers into high-quality QA pairs at scale. We make the dataset, models, and evaluation codes publicly available.","sentences":["The use of question-answer (QA) pairs for training and evaluating large language models (LLMs) has attracted considerable attention.","Yet few available QA datasets are based on knowledge from the scientific literature.","Here we bridge this gap by presenting Automatic Generation of Scientific Question Answers (SciQAG), a framework for automatic generation and evaluation of scientific QA pairs sourced from published scientific literature.","We fine-tune an open-source LLM to generate \\num{960000} scientific QA pairs from full-text scientific papers and propose a five-dimensional metric to evaluate the quality of the generated QA pairs.","We show via LLM-based evaluation that the generated QA pairs consistently achieve an average score of 2.5 out of 3 across five dimensions, indicating that our framework can distill key knowledge from papers into high-quality QA pairs at scale.","We make the dataset, models, and evaluation codes publicly available."],"url":"http://arxiv.org/abs/2405.09939v1","category":"cs.CL"}
{"created":"2024-05-16 09:42:19","title":"Synthetic RAW data generator for ESA HARMONY mission","abstract":"In this paper, we introduce HEEPS/MARE, the end-to-end simulator developed for the SAR oceanographic products of ESA Earth Explorer 10 mission, Harmony, expected to launch in Decembre 2029. Harmony is primarily dedicated to the observation of small-scale motion and deformation fields of the Earth surface (oceans, glaciers and ice sheets, solid Earth), thanks to passive SAR/ATI receivers carried by two companion satellites for Sentinel-1. The paper focuses on the raw data generator designed to efficiently simulate large, heterogeneous, moving oceanic areas and produce the acquired SAR/ATI bistatic IQ signals. The heterogeneous sea-surface model, bistatic scattering model, multi-GPU implementation and achieved performance are emphasized. Finally, sample results are presented, to illustrate the ability of Harmony to map wind and surface current vectors at kilometric scale.","sentences":["In this paper, we introduce HEEPS/MARE, the end-to-end simulator developed for the SAR oceanographic products of ESA Earth Explorer 10 mission, Harmony, expected to launch in Decembre 2029.","Harmony is primarily dedicated to the observation of small-scale motion and deformation fields of the Earth surface (oceans, glaciers and ice sheets, solid Earth), thanks to passive SAR/ATI receivers carried by two companion satellites for Sentinel-1.","The paper focuses on the raw data generator designed to efficiently simulate large, heterogeneous, moving oceanic areas and produce the acquired SAR/ATI bistatic IQ signals.","The heterogeneous sea-surface model, bistatic scattering model, multi-GPU implementation and achieved performance are emphasized.","Finally, sample results are presented, to illustrate the ability of Harmony to map wind and surface current vectors at kilometric scale."],"url":"http://arxiv.org/abs/2405.09938v1","category":"physics.geo-ph"}
{"created":"2024-05-16 09:41:12","title":"DEBATE: Devil's Advocate-Based Assessment and Text Evaluation","abstract":"As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important. Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks. However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance. This is because there exist biases in LLM agent's responses, including preferences for certain text structure or content. In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil's Advocate. Within the framework, one agent is instructed to criticize other agents' arguments, potentially resolving the bias in LLM agent's answers. DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat. We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators.","sentences":["As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important.","Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks.","However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance.","This is because there exist biases in LLM agent's responses, including preferences for certain text structure or content.","In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil's Advocate.","Within the framework, one agent is instructed to criticize other agents' arguments, potentially resolving the bias in LLM agent's answers.","DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat.","We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators."],"url":"http://arxiv.org/abs/2405.09935v1","category":"cs.CL"}
{"created":"2024-05-16 09:37:57","title":"Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fr\u00e9chet Domain Distance","abstract":"Multiple-instance learning (MIL) is an attractive approach for digital pathology applications as it reduces the costs related to data collection and labelling. However, it is not clear how sensitive MIL is to clinically realistic domain shifts, i.e., differences in data distribution that could negatively affect performance, and if already existing metrics for detecting domain shifts work well with these algorithms. We trained an attention-based MIL algorithm to classify whether a whole-slide image of a lymph node contains breast tumour metastases. The algorithm was evaluated on data from a hospital in a different country and various subsets of this data that correspond to different levels of domain shift. Our contributions include showing that MIL for digital pathology is affected by clinically realistic differences in data, evaluating which features from a MIL model are most suitable for detecting changes in performance, and proposing an unsupervised metric named Fr\\'echet Domain Distance (FDD) for quantification of domain shifts. Shift measure performance was evaluated through the mean Pearson correlation to change in classification performance, where FDD achieved 0.70 on 10-fold cross-validation models. The baselines included Deep ensemble, Difference of Confidence, and Representation shift which resulted in 0.45, -0.29, and 0.56 mean Pearson correlation, respectively. FDD could be a valuable tool for care providers and vendors who need to verify if a MIL system is likely to perform reliably when implemented at a new site, without requiring any additional annotations from pathologists.","sentences":["Multiple-instance learning (MIL) is an attractive approach for digital pathology applications as it reduces the costs related to data collection and labelling.","However, it is not clear how sensitive MIL is to clinically realistic domain shifts, i.e., differences in data distribution that could negatively affect performance, and if already existing metrics for detecting domain shifts work well with these algorithms.","We trained an attention-based MIL algorithm to classify whether a whole-slide image of a lymph node contains breast tumour metastases.","The algorithm was evaluated on data from a hospital in a different country and various subsets of this data that correspond to different levels of domain shift.","Our contributions include showing that MIL for digital pathology is affected by clinically realistic differences in data, evaluating which features from a MIL model are most suitable for detecting changes in performance, and proposing an unsupervised metric named Fr\\'echet Domain Distance (FDD) for quantification of domain shifts.","Shift measure performance was evaluated through the mean Pearson correlation to change in classification performance, where FDD achieved 0.70 on 10-fold cross-validation models.","The baselines included Deep ensemble, Difference of Confidence, and Representation shift which resulted in 0.45, -0.29, and 0.56 mean Pearson correlation, respectively.","FDD could be a valuable tool for care providers and vendors who need to verify if a MIL system is likely to perform reliably when implemented at a new site, without requiring any additional annotations from pathologists."],"url":"http://arxiv.org/abs/2405.09934v1","category":"cs.CV"}
{"created":"2024-05-16 09:37:54","title":"MiniMaxAD: A Lightweight Autoencoder for Feature-Rich Anomaly Detection","abstract":"Previous unsupervised anomaly detection (UAD) methods often struggle with significant intra-class diversity; i.e., a class in a dataset contains multiple subclasses, which we categorize as Feature-Rich Anomaly Detection Datasets (FRADs). This is evident in applications such as unified setting and unmanned supermarket scenarios. To address this challenge, we developed MiniMaxAD: a lightweight autoencoder designed to efficiently compress and memorize extensive information from normal images. Our model utilizes a large kernel convolutional network equipped with a Global Response Normalization (GRN) unit and employs a multi-scale feature reconstruction strategy. The GRN unit significantly increases the upper limit of the network's capacity, while the large kernel convolution facilitates the extraction of highly abstract patterns, leading to compact normal feature modeling. Additionally, we introduce an Adaptive Contraction Loss (ADCLoss), tailored to FRADs to overcome the limitations of global cosine distance loss. MiniMaxAD was comprehensively tested across six challenging UAD benchmarks, achieving state-of-the-art results in four and highly competitive outcomes in the remaining two. Notably, our model achieved a detection AUROC of up to 97.0\\% in ViSA under the unified setting. Moreover, it not only achieved state-of-the-art performance in unmanned supermarket tasks but also exhibited an inference speed 37 times faster than the previous best method, demonstrating its effectiveness in complex UAD tasks.","sentences":["Previous unsupervised anomaly detection (UAD) methods often struggle with significant intra-class diversity; i.e., a class in a dataset contains multiple subclasses, which we categorize as Feature-Rich Anomaly Detection Datasets (FRADs).","This is evident in applications such as unified setting and unmanned supermarket scenarios.","To address this challenge, we developed MiniMaxAD: a lightweight autoencoder designed to efficiently compress and memorize extensive information from normal images.","Our model utilizes a large kernel convolutional network equipped with a Global Response Normalization (GRN) unit and employs a multi-scale feature reconstruction strategy.","The GRN unit significantly increases the upper limit of the network's capacity, while the large kernel convolution facilitates the extraction of highly abstract patterns, leading to compact normal feature modeling.","Additionally, we introduce an Adaptive Contraction Loss (ADCLoss), tailored to FRADs to overcome the limitations of global cosine distance loss.","MiniMaxAD was comprehensively tested across six challenging UAD benchmarks, achieving state-of-the-art results in four and highly competitive outcomes in the remaining two.","Notably, our model achieved a detection AUROC of up to 97.0\\% in ViSA under the unified setting.","Moreover, it not only achieved state-of-the-art performance in unmanned supermarket tasks but also exhibited an inference speed 37 times faster than the previous best method, demonstrating its effectiveness in complex UAD tasks."],"url":"http://arxiv.org/abs/2405.09933v1","category":"cs.CV"}
{"created":"2024-05-16 09:34:57","title":"Learning from Observer Gaze:Zero-Shot Attention Prediction Oriented by Human-Object Interaction Recognition","abstract":"Most existing attention prediction research focuses on salient instances like humans and objects. However, the more complex interaction-oriented attention, arising from the comprehension of interactions between instances by human observers, remains largely unexplored. This is equally crucial for advancing human-machine interaction and human-centered artificial intelligence. To bridge this gap, we first collect a novel gaze fixation dataset named IG, comprising 530,000 fixation points across 740 diverse interaction categories, capturing visual attention during human observers cognitive processes of interactions. Subsequently, we introduce the zero-shot interaction-oriented attention prediction task ZeroIA, which challenges models to predict visual cues for interactions not encountered during training. Thirdly, we present the Interactive Attention model IA, designed to emulate human observers cognitive processes to tackle the ZeroIA problem. Extensive experiments demonstrate that the proposed IA outperforms other state-of-the-art approaches in both ZeroIA and fully supervised settings. Lastly, we endeavor to apply interaction-oriented attention to the interaction recognition task itself. Further experimental results demonstrate the promising potential to enhance the performance and interpretability of existing state-of-the-art HOI models by incorporating real human attention data from IG and attention labels generated by IA.","sentences":["Most existing attention prediction research focuses on salient instances like humans and objects.","However, the more complex interaction-oriented attention, arising from the comprehension of interactions between instances by human observers, remains largely unexplored.","This is equally crucial for advancing human-machine interaction and human-centered artificial intelligence.","To bridge this gap, we first collect a novel gaze fixation dataset named IG, comprising 530,000 fixation points across 740 diverse interaction categories, capturing visual attention during human observers cognitive processes of interactions.","Subsequently, we introduce the zero-shot interaction-oriented attention prediction task ZeroIA, which challenges models to predict visual cues for interactions not encountered during training.","Thirdly, we present the Interactive Attention model IA, designed to emulate human observers cognitive processes to tackle the ZeroIA problem.","Extensive experiments demonstrate that the proposed IA outperforms other state-of-the-art approaches in both ZeroIA and fully supervised settings.","Lastly, we endeavor to apply interaction-oriented attention to the interaction recognition task itself.","Further experimental results demonstrate the promising potential to enhance the performance and interpretability of existing state-of-the-art HOI models by incorporating real human attention data from IG and attention labels generated by IA."],"url":"http://arxiv.org/abs/2405.09931v1","category":"cs.CV"}
{"created":"2024-05-16 09:34:49","title":"Continuous operators from spaces of Lipschitz functions","abstract":"We study the existence of continuous (linear) operators from the Banach spaces $\\mathrm{Lip}_0(M)$ of Lipschitz functions on infinite metric spaces $M$ vanishing at a distinguished point and from their predual spaces $\\mathcal{F}(M)$ onto certain Banach spaces, including $C(K)$-spaces and the spaces $c_0$ and $\\ell_1$. For pairs of spaces $\\mathrm{Lip}_0(M)$ and $C(K)$ we prove that if they are endowed with topologies weaker than the norm topology, then usually no continuous (linear or not) surjection exists between those spaces. We show that, given a Banach space $E$, there exists a continuous operator from a Lipschitz-free space $\\mathcal{F}(M)$ onto $E$ if and only if $\\mathcal{F}(M)$ contains a subset homeomorphic to $E$ if and only if $d(M)\\ge d(E)$. We obtain a new characterization of the Schur property for spaces $\\mathcal{F}(M)$: a space $\\mathcal{F}(M)$ has the Schur property if and only if for every discrete metric space $N$ with cardinality $d(M)$ the spaces $\\mathcal{F}(M)$ and $\\mathcal{F}(N)$ are weakly sequentially homeomorphic. It is also showed that if a metric space $M$ contains a bilipschitz copy of the unit sphere $S_{c_0}$ of the space $c_0$, then $\\mathrm{Lip}_0(M)$ admits a continuous operator onto $\\ell_1$ and hence onto $c_0$. We provide several conditions for a space $M$ implying that $\\mathrm{Lip}_0(M)$ is not a Grothendieck space.","sentences":["We study the existence of continuous (linear) operators from the Banach spaces $\\mathrm{Lip}_0(M)$ of Lipschitz functions on infinite metric spaces $M$ vanishing at a distinguished point and from their predual spaces $\\mathcal{F}(M)$ onto certain Banach spaces, including $C(K)$-spaces and the spaces $c_0$ and $\\ell_1$. For pairs of spaces $\\mathrm{Lip}_0(M)$ and $C(K)$ we prove that if they are endowed with topologies weaker than the norm topology, then usually no continuous (linear or not) surjection exists between those spaces.","We show that, given a Banach space $E$, there exists a continuous operator from a Lipschitz-free space $\\mathcal{F}(M)$ onto $E$ if and only if $\\mathcal{F}(M)$ contains a subset homeomorphic to $E$ if and","only if $d(M)\\ge d(E)$. We obtain a new characterization of the Schur property for spaces $\\mathcal{F}(M)$: a space $\\mathcal{F}(M)$ has the Schur property if and only if for every discrete metric space $N$ with cardinality $d(M)$ the spaces $\\mathcal{F}(M)$ and $\\mathcal{F}(N)$ are weakly sequentially homeomorphic.","It is also showed that if a metric space $M$ contains a bilipschitz copy of the unit sphere $S_{c_0}$ of the space $c_0$, then $\\mathrm{Lip}_0(M)$ admits a continuous operator onto $\\ell_1$ and hence onto $c_0$. We provide several conditions for a space $M$ implying that $\\mathrm{Lip}_0(M)$ is not a Grothendieck space."],"url":"http://arxiv.org/abs/2405.09930v1","category":"math.FA"}
{"created":"2024-05-16 09:33:53","title":"Unified Modeling and Performance Comparison for Cellular and Cell-Free Massive MIMO","abstract":"Cell-free massive multi-input multi-output (MIMO) has recently gained a lot of attention due to its high potential in sixth-generation (6G) wireless systems. The goal of this paper is to first present a unified modeling for massive MIMO, encompassing both cellular and cell-free architectures with a variable number of antennas per access point. We derive signal transmission models and achievable spectral efficiency in both the downlink and uplink using zero-forcing and maximal-ratio schemes. We also provide performance comparisons in terms of per-user and sum spectral efficiency.","sentences":["Cell-free massive multi-input multi-output (MIMO) has recently gained a lot of attention due to its high potential in sixth-generation (6G) wireless systems.","The goal of this paper is to first present a unified modeling for massive MIMO, encompassing both cellular and cell-free architectures with a variable number of antennas per access point.","We derive signal transmission models and achievable spectral efficiency in both the downlink and uplink using zero-forcing and maximal-ratio schemes.","We also provide performance comparisons in terms of per-user and sum spectral efficiency."],"url":"http://arxiv.org/abs/2405.09928v1","category":"cs.IT"}
{"created":"2024-05-16 09:33:28","title":"Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy","abstract":"This work focuses on addressing two major challenges in the context of large-scale nonconvex Bi-Level Optimization (BLO) problems, which are increasingly applied in machine learning due to their ability to model nested structures. These challenges involve ensuring computational efficiency and providing theoretical guarantees. While recent advances in scalable BLO algorithms have primarily relied on lower-level convexity simplification, our work specifically tackles large-scale BLO problems involving nonconvexity in both the upper and lower levels. We simultaneously address computational and theoretical challenges by introducing an innovative single-loop gradient-based algorithm, utilizing the Moreau envelope-based reformulation, and providing non-asymptotic convergence analysis for general nonconvex BLO problems. Notably, our algorithm relies solely on first-order gradient information, enhancing its practicality and efficiency, especially for large-scale BLO learning tasks. We validate our approach's effectiveness through experiments on various synthetic problems, two typical hyper-parameter learning tasks, and a real-world neural architecture search application, collectively demonstrating its superior performance.","sentences":["This work focuses on addressing two major challenges in the context of large-scale nonconvex Bi-Level Optimization (BLO) problems, which are increasingly applied in machine learning due to their ability to model nested structures.","These challenges involve ensuring computational efficiency and providing theoretical guarantees.","While recent advances in scalable BLO algorithms have primarily relied on lower-level convexity simplification, our work specifically tackles large-scale BLO problems involving nonconvexity in both the upper and lower levels.","We simultaneously address computational and theoretical challenges by introducing an innovative single-loop gradient-based algorithm, utilizing the Moreau envelope-based reformulation, and providing non-asymptotic convergence analysis for general nonconvex BLO problems.","Notably, our algorithm relies solely on first-order gradient information, enhancing its practicality and efficiency, especially for large-scale BLO learning tasks.","We validate our approach's effectiveness through experiments on various synthetic problems, two typical hyper-parameter learning tasks, and a real-world neural architecture search application, collectively demonstrating its superior performance."],"url":"http://arxiv.org/abs/2405.09927v1","category":"math.OC"}
{"created":"2024-05-16 09:26:41","title":"ACES: A Teleoperated Robotic Solution to Pipe Inspection from the Inside","abstract":"This paper presents the definition of a teleoperated robotic system for non-destructive corrosion inspection of Steel Cylinder Concrete Pipes (SCCP) from the inside. A general description of in-pipe environment and a state of the art of in-pipe navigation solutions are exposed, with a zoom on the characteristics of the SCCP case of interest (pipe dimensions, curves, slopes, humidity, payload, etc.). Then, two specific steel corrosion measurement techniques are described. In order to operate them, several possible architectures of inspection system (mobile platform combined with a robotic inspection manipulator) are presented, depending if the mobile platform is self-centred or not and regarding the robotic manipulator type, namely a basic cylindrical manipulator, a self centred one, or a force-controlled 6 degrees of freedom (DoF) robotic arm. A suitable mechanical architecture is then selected according to SCCP inspection needs. This includes relevant interfaces between the robot, the corrosion measurement Non Destructive Testing (NDT) device and the pipe. Finally, possible future adaptation of the chosen solution are exposed.","sentences":["This paper presents the definition of a teleoperated robotic system for non-destructive corrosion inspection of Steel Cylinder Concrete Pipes (SCCP) from the inside.","A general description of in-pipe environment and a state of the art of in-pipe navigation solutions are exposed, with a zoom on the characteristics of the SCCP case of interest (pipe dimensions, curves, slopes, humidity, payload, etc.).","Then, two specific steel corrosion measurement techniques are described.","In order to operate them, several possible architectures of inspection system (mobile platform combined with a robotic inspection manipulator) are presented, depending if the mobile platform is self-centred or not and regarding the robotic manipulator type, namely a basic cylindrical manipulator, a self centred one, or a force-controlled 6 degrees of freedom (DoF) robotic arm.","A suitable mechanical architecture is then selected according to SCCP inspection needs.","This includes relevant interfaces between the robot, the corrosion measurement Non Destructive Testing (NDT) device and the pipe.","Finally, possible future adaptation of the chosen solution are exposed."],"url":"http://arxiv.org/abs/2405.09925v1","category":"cs.RO"}
{"created":"2024-05-16 09:26:19","title":"Infrared Adversarial Car Stickers","abstract":"Infrared physical adversarial examples are of great significance for studying the security of infrared AI systems that are widely used in our lives such as autonomous driving. Previous infrared physical attacks mainly focused on 2D infrared pedestrian detection which may not fully manifest its destructiveness to AI systems. In this work, we propose a physical attack method against infrared detectors based on 3D modeling, which is applied to a real car. The goal is to design a set of infrared adversarial stickers to make cars invisible to infrared detectors at various viewing angles, distances, and scenes. We build a 3D infrared car model with real infrared characteristics and propose an infrared adversarial pattern generation method based on 3D mesh shadow. We propose a 3D control points-based mesh smoothing algorithm and use a set of smoothness loss functions to enhance the smoothness of adversarial meshes and facilitate the sticker implementation. Besides, We designed the aluminum stickers and conducted physical experiments on two real Mercedes-Benz A200L cars. Our adversarial stickers hid the cars from Faster RCNN, an object detector, at various viewing angles, distances, and scenes. The attack success rate (ASR) was 91.49% for real cars. In comparison, the ASRs of random stickers and no sticker were only 6.21% and 0.66%, respectively. In addition, the ASRs of the designed stickers against six unseen object detectors such as YOLOv3 and Deformable DETR were between 73.35%-95.80%, showing good transferability of the attack performance across detectors.","sentences":["Infrared physical adversarial examples are of great significance for studying the security of infrared AI systems that are widely used in our lives such as autonomous driving.","Previous infrared physical attacks mainly focused on 2D infrared pedestrian detection which may not fully manifest its destructiveness to AI systems.","In this work, we propose a physical attack method against infrared detectors based on 3D modeling, which is applied to a real car.","The goal is to design a set of infrared adversarial stickers to make cars invisible to infrared detectors at various viewing angles, distances, and scenes.","We build a 3D infrared car model with real infrared characteristics and propose an infrared adversarial pattern generation method based on 3D mesh shadow.","We propose a 3D control points-based mesh smoothing algorithm and use a set of smoothness loss functions to enhance the smoothness of adversarial meshes and facilitate the sticker implementation.","Besides, We designed the aluminum stickers and conducted physical experiments on two real Mercedes-Benz A200L cars.","Our adversarial stickers hid the cars from Faster RCNN, an object detector, at various viewing angles, distances, and scenes.","The attack success rate (ASR) was 91.49% for real cars.","In comparison, the ASRs of random stickers and no sticker were only 6.21% and 0.66%, respectively.","In addition, the ASRs of the designed stickers against six unseen object detectors such as YOLOv3 and Deformable DETR were between 73.35%-95.80%, showing good transferability of the attack performance across detectors."],"url":"http://arxiv.org/abs/2405.09924v1","category":"cs.CV"}
{"created":"2024-05-16 09:26:13","title":"NTIRE 2024 Restore Any Image Model (RAIM) in the Wild Challenge","abstract":"In this paper, we review the NTIRE 2024 challenge on Restore Any Image Model (RAIM) in the Wild. The RAIM challenge constructed a benchmark for image restoration in the wild, including real-world images with/without reference ground truth in various scenarios from real applications. The participants were required to restore the real-captured images from complex and unknown degradation, where generative perceptual quality and fidelity are desired in the restoration result. The challenge consisted of two tasks. Task one employed real referenced data pairs, where quantitative evaluation is available. Task two used unpaired images, and a comprehensive user study was conducted. The challenge attracted more than 200 registrations, where 39 of them submitted results with more than 400 submissions. Top-ranked methods improved the state-of-the-art restoration performance and obtained unanimous recognition from all 18 judges. The proposed datasets are available at https://drive.google.com/file/d/1DqbxUoiUqkAIkExu3jZAqoElr_nu1IXb/view?usp=sharing and the homepage of this challenge is at https://codalab.lisn.upsaclay.fr/competitions/17632.","sentences":["In this paper, we review the NTIRE 2024 challenge on Restore Any Image Model (RAIM) in the Wild.","The RAIM challenge constructed a benchmark for image restoration in the wild, including real-world images with/without reference ground truth in various scenarios from real applications.","The participants were required to restore the real-captured images from complex and unknown degradation, where generative perceptual quality and fidelity are desired in the restoration result.","The challenge consisted of two tasks.","Task one employed real referenced data pairs, where quantitative evaluation is available.","Task two used unpaired images, and a comprehensive user study was conducted.","The challenge attracted more than 200 registrations, where 39 of them submitted results with more than 400 submissions.","Top-ranked methods improved the state-of-the-art restoration performance and obtained unanimous recognition from all 18 judges.","The proposed datasets are available at https://drive.google.com/file/d/1DqbxUoiUqkAIkExu3jZAqoElr_nu1IXb/view?usp=sharing and the homepage of this challenge is at https://codalab.lisn.upsaclay.fr/competitions/17632."],"url":"http://arxiv.org/abs/2405.09923v1","category":"cs.CV"}
{"created":"2024-05-16 09:25:45","title":"Cross-sensor self-supervised training and alignment for remote sensing","abstract":"Large-scale \"foundation models\" have gained traction as a way to leverage the vast amounts of unlabeled remote sensing data collected every day. However, due to the multiplicity of Earth Observation satellites, these models should learn \"sensor agnostic\" representations, that generalize across sensor characteristics with minimal fine-tuning. This is complicated by data availability, as low-resolution imagery, such as Sentinel-2 and Landsat-8 data, are available in large amounts, while very high-resolution aerial or satellite data is less common. To tackle these challenges, we introduce cross-sensor self-supervised training and alignment for remote sensing (X-STARS). We design a self-supervised training loss, the Multi-Sensor Alignment Dense loss (MSAD), to align representations across sensors, even with vastly different resolutions. Our X-STARS can be applied to train models from scratch, or to adapt large models pretrained on e.g low-resolution EO data to new high-resolution sensors, in a continual pretraining framework. We collect and release MSC-France, a new multi-sensor dataset, on which we train our X-STARS models, then evaluated on seven downstream classification and segmentation tasks. We demonstrate that X-STARS outperforms the state-of-the-art by a significant margin with less data across various conditions of data availability and resolutions.","sentences":["Large-scale \"foundation models\" have gained traction as a way to leverage the vast amounts of unlabeled remote sensing data collected every day.","However, due to the multiplicity of Earth Observation satellites, these models should learn \"sensor agnostic\" representations, that generalize across sensor characteristics with minimal fine-tuning.","This is complicated by data availability, as low-resolution imagery, such as Sentinel-2 and Landsat-8 data, are available in large amounts, while very high-resolution aerial or satellite data is less common.","To tackle these challenges, we introduce cross-sensor self-supervised training and alignment for remote sensing (X-STARS).","We design a self-supervised training loss, the Multi-Sensor Alignment Dense loss (MSAD), to align representations across sensors, even with vastly different resolutions.","Our X-STARS can be applied to train models from scratch, or to adapt large models pretrained on e.g low-resolution EO data to new high-resolution sensors, in a continual pretraining framework.","We collect and release MSC-France, a new multi-sensor dataset, on which we train our X-STARS models, then evaluated on seven downstream classification and segmentation tasks.","We demonstrate that X-STARS outperforms the state-of-the-art by a significant margin with less data across various conditions of data availability and resolutions."],"url":"http://arxiv.org/abs/2405.09922v1","category":"cs.CV"}
{"created":"2024-05-16 09:19:08","title":"Dynamic online matching with budget refills","abstract":"Inspired by sequential budgeted allocation problems, we study the online matching problem with budget refills. In this context, we consider an online bipartite graph G=(U,V,E), where the nodes in $V$ are discovered sequentially and nodes in $U$ are known beforehand. Each $u\\in U$ is endowed with a budget $b_{u,t}\\in \\mathbb{N}$ that dynamically evolves over time. Unlike the canonical setting, in many applications, the budget can be refilled from time to time, which leads to a much richer dynamic that we consider here. Intuitively, adding extra budgets in $U$ seems to ease the matching task, and our results support this intuition. In fact, for the stochastic framework considered where we studied the matching size built by Greedy algorithm on an Erd\\H{o}s-R\\'eyni random graph, we showed that the matching size generated by Greedy converges with high probability to a solution of an explicit system of ODE. Moreover, under specific conditions, the competitive ratio (performance measure of the algorithm) can even tend to 1. For the adversarial part, where the graph considered is deterministic and the algorithm used is Balance, the $b$-matching bound holds when the refills are scarce. However, when refills are regular, our results suggest a potential improvement in algorithm performance. In both cases, Balance algorithm manages to reach the performance of the upper bound on the adversarial graphs considered.","sentences":["Inspired by sequential budgeted allocation problems, we study the online matching problem with budget refills.","In this context, we consider an online bipartite graph G=(U,V,E), where the nodes in $V$ are discovered sequentially and nodes in $U$ are known beforehand.","Each $u\\in U$ is endowed with a budget $b_{u,t}\\in \\mathbb{N}$ that dynamically evolves over time.","Unlike the canonical setting, in many applications, the budget can be refilled from time to time, which leads to a much richer dynamic that we consider here.","Intuitively, adding extra budgets in $U$ seems to ease the matching task, and our results support this intuition.","In fact, for the stochastic framework considered where we studied the matching size built by Greedy algorithm on an Erd\\H{o}s-R\\'eyni random graph, we showed that the matching size generated by Greedy converges with high probability to a solution of an explicit system of ODE.","Moreover, under specific conditions, the competitive ratio (performance measure of the algorithm) can even tend to 1.","For the adversarial part, where the graph considered is deterministic and the algorithm used is Balance, the $b$-matching bound holds when the refills are scarce.","However, when refills are regular, our results suggest a potential improvement in algorithm performance.","In both cases, Balance algorithm manages to reach the performance of the upper bound on the adversarial graphs considered."],"url":"http://arxiv.org/abs/2405.09920v1","category":"cs.DS"}
{"created":"2024-05-16 09:15:08","title":"Conundrums for continuous Lebesgue measure-preserving interval maps","abstract":"We consider continuous maps of the interval which preserve the Lebesgue measure. Except for the identity map or $1 - \\id$ all such maps have topological entropy at least $\\log2/2$ and generically they have infinite topological entropy. In this article we show that thegeneric map has zero measure-theoretic entropy. This implies that there are dramatic differences in the topological versus metric behavior both for injectivity as well as for the structure of thelevel sets of generic maps.","sentences":["We consider continuous maps of the interval which preserve the Lebesgue measure.","Except for the identity map or $1 - \\id$ all such maps have topological entropy at least $\\log2/2$ and generically they have infinite topological entropy.","In this article we show that thegeneric map has zero measure-theoretic entropy.","This implies that there are dramatic differences in the topological versus metric behavior both for injectivity as well as for the structure of thelevel sets of generic maps."],"url":"http://arxiv.org/abs/2405.09917v1","category":"math.DS"}
{"created":"2024-05-16 09:02:09","title":"Driver at 10 MJ and 1 shot per 30 minutes for inertial confinement fusion at high gain: efficient, compact, low laser-plasma instabilities, multi-color, low-cost, applicable to multiple fusion schemes","abstract":"The ignition at the National Ignition Facility (NIF) set off a global wave of research on the inertial fusion energy (IFE). However, IFE requires a necessary target gain G of 30-100, while it is hard to achieve the fusions at such high gain with the energy, configuration, and technical route of the NIF. We will present a conceptual design for the next generation laser driver of 10 MJ, 2~3 PW at the laser wavelength of 0.353 micrometer (or 0.353 micrometer, then the energy and power can be higher), and 1 shot per 30 minutes, which is efficient, compact, low-cost, low laser-plasma instabilities, applicable to multiple laser fusion schemes, and aiming for G > 30.","sentences":["The ignition at the National Ignition Facility (NIF) set off a global wave of research on the inertial fusion energy (IFE).","However, IFE requires a necessary target gain G of 30-100, while it is hard to achieve the fusions at such high gain with the energy, configuration, and technical route of the NIF.","We will present a conceptual design for the next generation laser driver of 10 MJ, 2~3 PW at the laser wavelength of 0.353 micrometer (or 0.353 micrometer, then the energy and power can be higher), and 1 shot per 30 minutes, which is efficient, compact, low-cost, low laser-plasma instabilities, applicable to multiple laser fusion schemes, and aiming for G > 30."],"url":"http://arxiv.org/abs/2405.09912v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 08:57:34","title":"A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations","abstract":"As telecommunication systems evolve to meet increasing demands, integrating deep neural networks (DNNs) has shown promise in enhancing performance. However, the trade-off between accuracy and flexibility remains challenging when replacing traditional receivers with DNNs. This paper introduces a novel probabilistic framework that allows a single DNN demapper to demap multiple QAM and APSK constellations simultaneously. We also demonstrate that our framework allows exploiting hierarchical relationships in families of constellations. The consequence is that we need fewer neural network outputs to encode the same function without an increase in Bit Error Rate (BER). Our simulation results confirm that our approach approaches the optimal demodulation error bound under an Additive White Gaussian Noise (AWGN) channel for multiple constellations. Thereby, we address multiple important issues in making DNNs flexible enough for practical use as receivers.","sentences":["As telecommunication systems evolve to meet increasing demands, integrating deep neural networks (DNNs) has shown promise in enhancing performance.","However, the trade-off between accuracy and flexibility remains challenging when replacing traditional receivers with DNNs.","This paper introduces a novel probabilistic framework that allows a single DNN demapper to demap multiple QAM and APSK constellations simultaneously.","We also demonstrate that our framework allows exploiting hierarchical relationships in families of constellations.","The consequence is that we need fewer neural network outputs to encode the same function without an increase in Bit Error Rate (BER).","Our simulation results confirm that our approach approaches the optimal demodulation error bound under an Additive White Gaussian Noise (AWGN) channel for multiple constellations.","Thereby, we address multiple important issues in making DNNs flexible enough for practical use as receivers."],"url":"http://arxiv.org/abs/2405.09909v1","category":"cs.LG"}
{"created":"2024-05-16 08:54:24","title":"Process-based Inference for Spatial Energetics Using Bayesian Predictive Stacking","abstract":"Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of ``spatial energetics'' in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from ``actigraph'' units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles.","sentences":["Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities.","Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of ``spatial energetics'' in environmental health sciences.","We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device.","We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from ``actigraph'' units as the subject traverses a path or trajectory in their daily routine.","Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models.","This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo.","We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous.","We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles."],"url":"http://arxiv.org/abs/2405.09906v1","category":"stat.ME"}
{"created":"2024-05-16 08:53:23","title":"Cell-Free Terahertz Massive MIMO: A Novel Paradigm Beyond Ultra-Massive MIMO","abstract":"Terahertz (THz) frequencies have recently garnered considerable attention due to their potential to offer abundant spectral resources for communication, as well as distinct advantages in sensing, positioning, and imaging. Nevertheless, practical implementation encounters challenges stemming from the limited distances of signal transmission, primarily due to notable propagation, absorption, and blockage losses. To address this issue, the current strategy involves employing ultra-massive multi-input multi-output (UMMIMO) to generate high beamforming gains, thereby extending the transmission range. This paper introduces an alternative solution through the utilization of cell-free massive MIMO (CFmMIMO) architecture, wherein the closest access point is actively chosen to reduce the distance, rather than relying solely on a substantial number of antennas. We compare these two techniques through simulations and the numerical results justify that CFmMIMO is superior to UMMIMO in both spectral and energy efficiency at THz frequencies.","sentences":["Terahertz (THz) frequencies have recently garnered considerable attention due to their potential to offer abundant spectral resources for communication, as well as distinct advantages in sensing, positioning, and imaging.","Nevertheless, practical implementation encounters challenges stemming from the limited distances of signal transmission, primarily due to notable propagation, absorption, and blockage losses.","To address this issue, the current strategy involves employing ultra-massive multi-input multi-output (UMMIMO) to generate high beamforming gains, thereby extending the transmission range.","This paper introduces an alternative solution through the utilization of cell-free massive MIMO (CFmMIMO) architecture, wherein the closest access point is actively chosen to reduce the distance, rather than relying solely on a substantial number of antennas.","We compare these two techniques through simulations and the numerical results justify that CFmMIMO is superior to UMMIMO in both spectral and energy efficiency at THz frequencies."],"url":"http://arxiv.org/abs/2405.09905v1","category":"cs.IT"}
{"created":"2024-05-16 08:49:50","title":"Federated Learning for Misbehaviour Detection with Variational Autoencoders and Gaussian Mixture Models","abstract":"Federated Learning (FL) has become an attractive approach to collaboratively train Machine Learning (ML) models while data sources' privacy is still preserved. However, most of existing FL approaches are based on supervised techniques, which could require resource-intensive activities and human intervention to obtain labelled datasets. Furthermore, in the scope of cyberattack detection, such techniques are not able to identify previously unknown threats. In this direction, this work proposes a novel unsupervised FL approach for the identification of potential misbehavior in vehicular environments. We leverage the computing capabilities of public cloud services for model aggregation purposes, and also as a central repository of misbehavior events, enabling cross-vehicle learning and collective defense strategies. Our solution integrates the use of Gaussian Mixture Models (GMM) and Variational Autoencoders (VAE) on the VeReMi dataset in a federated environment, where each vehicle is intended to train only with its own data. Furthermore, we use Restricted Boltzmann Machines (RBM) for pre-training purposes, and Fedplus as aggregation function to enhance model's convergence. Our approach provides better performance (more than 80 percent) compared to recent proposals, which are usually based on supervised techniques and artificial divisions of the VeReMi dataset.","sentences":["Federated Learning (FL) has become an attractive approach to collaboratively train Machine Learning (ML) models while data sources' privacy is still preserved.","However, most of existing FL approaches are based on supervised techniques, which could require resource-intensive activities and human intervention to obtain labelled datasets.","Furthermore, in the scope of cyberattack detection, such techniques are not able to identify previously unknown threats.","In this direction, this work proposes a novel unsupervised FL approach for the identification of potential misbehavior in vehicular environments.","We leverage the computing capabilities of public cloud services for model aggregation purposes, and also as a central repository of misbehavior events, enabling cross-vehicle learning and collective defense strategies.","Our solution integrates the use of Gaussian Mixture Models (GMM) and Variational Autoencoders (VAE) on the VeReMi dataset in a federated environment, where each vehicle is intended to train only with its own data.","Furthermore, we use Restricted Boltzmann Machines (RBM) for pre-training purposes, and Fedplus as aggregation function to enhance model's convergence.","Our approach provides better performance (more than 80 percent) compared to recent proposals, which are usually based on supervised techniques and artificial divisions of the VeReMi dataset."],"url":"http://arxiv.org/abs/2405.09903v1","category":"cs.LG"}
{"created":"2024-05-16 08:49:05","title":"Unveiling the Potential: Harnessing Deep Metric Learning to Circumvent Video Streaming Encryption","abstract":"Encryption on the internet with the shift to HTTPS has been an important step to improve the privacy of internet users. However, there is an increasing body of work about extracting information from encrypted internet traffic without having to decrypt it. Such attacks bypass security guarantees assumed to be given by HTTPS and thus need to be understood. Prior works showed that the variable bitrates of video streams are sufficient to identify which video someone is watching. These works generally have to make trade-offs in aspects such as accuracy, scalability, robustness, etc. These trade-offs complicate the practical use of these attacks. To that end, we propose a deep metric learning framework based on the triplet loss method. Through this framework, we achieve robust, generalisable, scalable and transferable encrypted video stream detection. First, the triplet loss is better able to deal with video streams not seen during training. Second, our approach can accurately classify videos not seen during training. Third, we show that our method scales well to a dataset of over 1000 videos. Finally, we show that a model trained on video streams over Chrome can also classify streams over Firefox. Our results suggest that this side-channel attack is more broadly applicable than originally thought. We provide our code alongside a diverse and up-to-date dataset for future research.","sentences":["Encryption on the internet with the shift to HTTPS has been an important step to improve the privacy of internet users.","However, there is an increasing body of work about extracting information from encrypted internet traffic without having to decrypt it.","Such attacks bypass security guarantees assumed to be given by HTTPS and thus need to be understood.","Prior works showed that the variable bitrates of video streams are sufficient to identify which video someone is watching.","These works generally have to make trade-offs in aspects such as accuracy, scalability, robustness, etc.","These trade-offs complicate the practical use of these attacks.","To that end, we propose a deep metric learning framework based on the triplet loss method.","Through this framework, we achieve robust, generalisable, scalable and transferable encrypted video stream detection.","First, the triplet loss is better able to deal with video streams not seen during training.","Second, our approach can accurately classify videos not seen during training.","Third, we show that our method scales well to a dataset of over 1000 videos.","Finally, we show that a model trained on video streams over Chrome can also classify streams over Firefox.","Our results suggest that this side-channel attack is more broadly applicable than originally thought.","We provide our code alongside a diverse and up-to-date dataset for future research."],"url":"http://arxiv.org/abs/2405.09902v1","category":"cs.CV"}
{"created":"2024-05-16 08:48:23","title":"Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models","abstract":"Recent deep music generation studies have put much emphasis on long-term generation with structures. However, we are yet to see high-quality, well-structured whole-song generation. In this paper, we make the first attempt to model a full music piece under the realization of compositional hierarchy. With a focus on symbolic representations of pop songs, we define a hierarchical language, in which each level of hierarchy focuses on the semantics and context dependency at a certain music scope. The high-level languages reveal whole-song form, phrase, and cadence, whereas the low-level languages focus on notes, chords, and their local patterns. A cascaded diffusion model is trained to model the hierarchical language, where each level is conditioned on its upper levels. Experiments and analysis show that our model is capable of generating full-piece music with recognizable global verse-chorus structure and cadences, and the music quality is higher than the baselines. Additionally, we show that the proposed model is controllable in a flexible way. By sampling from the interpretable hierarchical languages or adjusting pre-trained external representations, users can control the music flow via various features such as phrase harmonic structures, rhythmic patterns, and accompaniment texture.","sentences":["Recent deep music generation studies have put much emphasis on long-term generation with structures.","However, we are yet to see high-quality, well-structured whole-song generation.","In this paper, we make the first attempt to model a full music piece under the realization of compositional hierarchy.","With a focus on symbolic representations of pop songs, we define a hierarchical language, in which each level of hierarchy focuses on the semantics and context dependency at a certain music scope.","The high-level languages reveal whole-song form, phrase, and cadence, whereas the low-level languages focus on notes, chords, and their local patterns.","A cascaded diffusion model is trained to model the hierarchical language, where each level is conditioned on its upper levels.","Experiments and analysis show that our model is capable of generating full-piece music with recognizable global verse-chorus structure and cadences, and the music quality is higher than the baselines.","Additionally, we show that the proposed model is controllable in a flexible way.","By sampling from the interpretable hierarchical languages or adjusting pre-trained external representations, users can control the music flow via various features such as phrase harmonic structures, rhythmic patterns, and accompaniment texture."],"url":"http://arxiv.org/abs/2405.09901v1","category":"cs.SD"}
{"created":"2024-05-16 08:43:35","title":"Quantum Metrology with Higher-order Exceptional Points in Atom-cavity Magnonics","abstract":"Exceptional points (EPs), early arising from non-Hermitian physics, significantly amplify the system's response to minor perturbations, and act as a useful concept to enhance measurement in metrology. In particular, such a metrological enhancement grows dramatically with the EP's order. However, the Langevin noises intrinsically existing in the non-Hermitian systems diminish this enhancement. In this study, we propose a protocol for quantum metrology with the construction of higher-order EPs (HOEPs) in atom-cavity system through Hermitian magnon-photon interaction. The construction of HOEPs utilizes the atom-cavity non-Hermitian-like dynamical behavior but avoids the external Langevin noises via the Hermitian interaction. A general analysis is exhibited for the construction of arbitrary $n$-th order EP (EPn). As a demonstration of the superiority of these HOEPs in quantum metrology, we work out an EP3/4-based atomic sensor with sensitivity being orders of magnitude higher than that achievable in an EP2-based one. We further unveil the mechanism behind the sensitivity enhancement from HOEPs. The experimental establishment for this proposal is suggested with potential candidates. This EP-based atomic sensor, taking advantage of the atom-light interface, offers new insight into quantum metrology with HOEPs.","sentences":["Exceptional points (EPs), early arising from non-Hermitian physics, significantly amplify the system's response to minor perturbations, and act as a useful concept to enhance measurement in metrology.","In particular, such a metrological enhancement grows dramatically with the EP's order.","However, the Langevin noises intrinsically existing in the non-Hermitian systems diminish this enhancement.","In this study, we propose a protocol for quantum metrology with the construction of higher-order EPs (HOEPs) in atom-cavity system through Hermitian magnon-photon interaction.","The construction of HOEPs utilizes the atom-cavity non-Hermitian-like dynamical behavior but avoids the external Langevin noises via the Hermitian interaction.","A general analysis is exhibited for the construction of arbitrary $n$-th order EP (EPn).","As a demonstration of the superiority of these HOEPs in quantum metrology, we work out an EP3/4-based atomic sensor with sensitivity being orders of magnitude higher than that achievable in an EP2-based one.","We further unveil the mechanism behind the sensitivity enhancement from HOEPs.","The experimental establishment for this proposal is suggested with potential candidates.","This EP-based atomic sensor, taking advantage of the atom-light interface, offers new insight into quantum metrology with HOEPs."],"url":"http://arxiv.org/abs/2405.09899v1","category":"quant-ph"}
{"created":"2024-05-16 08:40:05","title":"Towards Informatics-Driven Design of Nuclear Waste Forms","abstract":"Informatics-driven approaches, such as machine learning and sequential experimental design, have shown the potential to drastically impact next-generation materials discovery and design. In this perspective, we present a few guiding principles for applying informatics-based methods towards the design of novel nuclear waste forms. We advocate for adopting a system design approach, and describe the effective usage of data-driven methods in every stage of such a design process. We demonstrate how this approach can optimally leverage physics-based simulations, machine learning surrogates, and experimental synthesis and characterization, within a feedback-driven closed-loop sequential learning framework. We discuss the importance of incorporating domain knowledge into the representation of materials, the construction and curation of datasets, the development of predictive property models, and the design and execution of experiments. We illustrate the application of this approach by successfully designing and validating Na- and Nd-containing phosphate-based ceramic waste forms. Finally, we discuss open challenges in such informatics-driven workflows and present an outlook for their widespread application for the cleanup of nuclear wastes.","sentences":["Informatics-driven approaches, such as machine learning and sequential experimental design, have shown the potential to drastically impact next-generation materials discovery and design.","In this perspective, we present a few guiding principles for applying informatics-based methods towards the design of novel nuclear waste forms.","We advocate for adopting a system design approach, and describe the effective usage of data-driven methods in every stage of such a design process.","We demonstrate how this approach can optimally leverage physics-based simulations, machine learning surrogates, and experimental synthesis and characterization, within a feedback-driven closed-loop sequential learning framework.","We discuss the importance of incorporating domain knowledge into the representation of materials, the construction and curation of datasets, the development of predictive property models, and the design and execution of experiments.","We illustrate the application of this approach by successfully designing and validating Na- and Nd-containing phosphate-based ceramic waste forms.","Finally, we discuss open challenges in such informatics-driven workflows and present an outlook for their widespread application for the cleanup of nuclear wastes."],"url":"http://arxiv.org/abs/2405.09897v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 08:28:25","title":"Quantum Automorphism Groups of Hypergraphs","abstract":"We introduce a quantum automorphism group for hypergraphs, which turns out to generalize the quantum automorphism group of Bichon for classical graphs. Further, we show that our quantum automorphism group acts on hypergraph C*-algebras as recently defined. In particular, this action generalizes the one on graph C*-algebras by Schmidt-Weber in 2018.","sentences":["We introduce a quantum automorphism group for hypergraphs, which turns out to generalize the quantum automorphism group of Bichon for classical graphs.","Further, we show that our quantum automorphism group acts on hypergraph C*-algebras as recently defined.","In particular, this action generalizes the one on graph C*-algebras by Schmidt-Weber in 2018."],"url":"http://arxiv.org/abs/2405.09894v1","category":"math.OA"}
{"created":"2024-05-16 08:19:11","title":"\"Hunt Takes Hare\": Theming Games Through Game-Word Vector Translation","abstract":"A game's theme is an important part of its design -- it conveys narrative information, rhetorical messages, helps the player intuit strategies, aids in tutorialisation and more. Thematic elements of games are notoriously difficult for AI systems to understand and manipulate, however, and often rely on large amounts of hand-written interpretations and knowledge. In this paper we present a technique which connects game embeddings, a recent method for modelling game dynamics from log data, and word embeddings, which models semantic information about language. We explain two different approaches for using game embeddings in this way, and show evidence that game embeddings enhance the linguistic translations of game concepts from one theme to another, opening up exciting new possibilities for reasoning about the thematic elements of games in the future.","sentences":["A game's theme is an important part of its design -- it conveys narrative information, rhetorical messages, helps the player intuit strategies, aids in tutorialisation and more.","Thematic elements of games are notoriously difficult for AI systems to understand and manipulate, however, and often rely on large amounts of hand-written interpretations and knowledge.","In this paper we present a technique which connects game embeddings, a recent method for modelling game dynamics from log data, and word embeddings, which models semantic information about language.","We explain two different approaches for using game embeddings in this way, and show evidence that game embeddings enhance the linguistic translations of game concepts from one theme to another, opening up exciting new possibilities for reasoning about the thematic elements of games in the future."],"url":"http://arxiv.org/abs/2405.09893v1","category":"cs.AI"}
{"created":"2024-05-16 08:12:44","title":"The time fractional order derivative for multi-class AR model","abstract":"In this paper, a multi-class Aw-Rascle \\textrm{(AR)} model with time fractional order derivative is presented. The conservative form of the proposed model is considered for the natural extension and generalization of equations involved. The fractional order derivative involved in the model equations is computed by applying the Caputo fractional derivative definition. An explicit difference scheme is obtained through finite difference method of discretization. The scheme is shown to be consistent, conditionally stable and convergent. Numerical flux is computed by original Roe decomposition and an entropy condition applied to the Roe decomposition. From numerical results, the effect of fractional-order derivative of time, on the traffic flow of vehicle classes is determined. Results obtained from the proposed model remain within limits therefore, they are realistic.","sentences":["In this paper, a multi-class Aw-Rascle \\textrm{(AR)} model with time fractional order derivative is presented.","The conservative form of the proposed model is considered for the natural extension and generalization of equations involved.","The fractional order derivative involved in the model equations is computed by applying the Caputo fractional derivative definition.","An explicit difference scheme is obtained through finite difference method of discretization.","The scheme is shown to be consistent, conditionally stable and convergent.","Numerical flux is computed by original Roe decomposition and an entropy condition applied to the Roe decomposition.","From numerical results, the effect of fractional-order derivative of time, on the traffic flow of vehicle classes is determined.","Results obtained from the proposed model remain within limits therefore, they are realistic."],"url":"http://arxiv.org/abs/2405.09888v1","category":"math.AP"}
{"created":"2024-05-16 08:07:25","title":"MTLComb: multi-task learning combining regression and classification tasks for joint feature selection","abstract":"Multi-task learning (MTL) is a learning paradigm that enables the simultaneous training of multiple communicating algorithms. Although MTL has been successfully applied to ether regression or classification tasks alone, incorporating mixed types of tasks into a unified MTL framework remains challenging, primarily due to variations in the magnitudes of losses associated with different tasks. This challenge, particularly evident in MTL applications with joint feature selection, often results in biased selections. To overcome this obstacle, we propose a provable loss weighting scheme that analytically determines the optimal weights for balancing regression and classification tasks. This scheme significantly mitigates the otherwise biased feature selection. Building upon this scheme, we introduce MTLComb, an MTL algorithm and software package encompassing optimization procedures, training protocols, and hyperparameter estimation procedures. MTLComb is designed for learning shared predictors among tasks of mixed types. To showcase the efficacy of MTLComb, we conduct tests on both simulated data and biomedical studies pertaining to sepsis and schizophrenia.","sentences":["Multi-task learning (MTL) is a learning paradigm that enables the simultaneous training of multiple communicating algorithms.","Although MTL has been successfully applied to ether regression or classification tasks alone, incorporating mixed types of tasks into a unified MTL framework remains challenging, primarily due to variations in the magnitudes of losses associated with different tasks.","This challenge, particularly evident in MTL applications with joint feature selection, often results in biased selections.","To overcome this obstacle, we propose a provable loss weighting scheme that analytically determines the optimal weights for balancing regression and classification tasks.","This scheme significantly mitigates the otherwise biased feature selection.","Building upon this scheme, we introduce MTLComb, an MTL algorithm and software package encompassing optimization procedures, training protocols, and hyperparameter estimation procedures.","MTLComb is designed for learning shared predictors among tasks of mixed types.","To showcase the efficacy of MTLComb, we conduct tests on both simulated data and biomedical studies pertaining to sepsis and schizophrenia."],"url":"http://arxiv.org/abs/2405.09886v1","category":"cs.LG"}
{"created":"2024-05-16 08:05:36","title":"DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection","abstract":"With the rapid development of face recognition (FR) systems, the privacy of face images on social media is facing severe challenges due to the abuse of unauthorized FR systems. Some studies utilize adversarial attack techniques to defend against malicious FR systems by generating adversarial examples. However, the generated adversarial examples, i.e., the protected face images, tend to suffer from subpar visual quality and low transferability. In this paper, we propose a novel face protection approach, dubbed DiffAM, which leverages the powerful generative ability of diffusion models to generate high-quality protected face images with adversarial makeup transferred from reference images. To be specific, we first introduce a makeup removal module to generate non-makeup images utilizing a fine-tuned diffusion model with guidance of textual prompts in CLIP space. As the inverse process of makeup transfer, makeup removal can make it easier to establish the deterministic relationship between makeup domain and non-makeup domain regardless of elaborate text prompts. Then, with this relationship, a CLIP-based makeup loss along with an ensemble attack strategy is introduced to jointly guide the direction of adversarial makeup domain, achieving the generation of protected face images with natural-looking makeup and high black-box transferability. Extensive experiments demonstrate that DiffAM achieves higher visual quality and attack success rates with a gain of 12.98% under black-box setting compared with the state of the arts. The code will be available at https://github.com/HansSunY/DiffAM.","sentences":["With the rapid development of face recognition (FR) systems, the privacy of face images on social media is facing severe challenges due to the abuse of unauthorized FR systems.","Some studies utilize adversarial attack techniques to defend against malicious FR systems by generating adversarial examples.","However, the generated adversarial examples, i.e., the protected face images, tend to suffer from subpar visual quality and low transferability.","In this paper, we propose a novel face protection approach, dubbed DiffAM, which leverages the powerful generative ability of diffusion models to generate high-quality protected face images with adversarial makeup transferred from reference images.","To be specific, we first introduce a makeup removal module to generate non-makeup images utilizing a fine-tuned diffusion model with guidance of textual prompts in CLIP space.","As the inverse process of makeup transfer, makeup removal can make it easier to establish the deterministic relationship between makeup domain and non-makeup domain regardless of elaborate text prompts.","Then, with this relationship, a CLIP-based makeup loss along with an ensemble attack strategy is introduced to jointly guide the direction of adversarial makeup domain, achieving the generation of protected face images with natural-looking makeup and high black-box transferability.","Extensive experiments demonstrate that DiffAM achieves higher visual quality and attack success rates with a gain of 12.98% under black-box setting compared with the state of the arts.","The code will be available at https://github.com/HansSunY/DiffAM."],"url":"http://arxiv.org/abs/2405.09882v1","category":"cs.CV"}
{"created":"2024-05-16 08:05:15","title":"Scalable Timing Coordination of Bell State Analyzers in Quantum Networks","abstract":"The optical Bell State Analyzer (BSA) plays a key role in the optical generation of entanglement in quantum networks. The optical BSA is effective in controlling the timing of arriving photons to achieve interference. It is unclear whether timing synchronization is possible even in multi-hop and complex large-scale networks, and if so, how efficient it is. We investigate the scalability of BSA synchronization mechanisms over multiple hops for quantum networks both with and without memory in each node. We first focus on the exchange of entanglement between two network nodes via a BSA, especially effective methods of optical path coordination in achieving the simultaneous arrival of photons at the BSA. In optical memoryless quantum networks, including repeater graph state networks, we see that the quantum optical path coordination works well, though some possible timing coordination mechanisms have effects that cascade to adjacent links and beyond, some of which was not going to work well of timing coordination. We also discuss the effect of quantum memory, given that end-to-end extension of entangled states through multi-node entanglement exchange is essential for the practical application of quantum networks. Finally, cycles of all-optical links in the network topology are shown to may not be to synchronize, this property should be taken into account when considering synchronization in large networks.","sentences":["The optical Bell State Analyzer (BSA) plays a key role in the optical generation of entanglement in quantum networks.","The optical BSA is effective in controlling the timing of arriving photons to achieve interference.","It is unclear whether timing synchronization is possible even in multi-hop and complex large-scale networks, and if so, how efficient it is.","We investigate the scalability of BSA synchronization mechanisms over multiple hops for quantum networks both with and without memory in each node.","We first focus on the exchange of entanglement between two network nodes via a BSA, especially effective methods of optical path coordination in achieving the simultaneous arrival of photons at the BSA.","In optical memoryless quantum networks, including repeater graph state networks, we see that the quantum optical path coordination works well, though some possible timing coordination mechanisms have effects that cascade to adjacent links and beyond, some of which was not going to work well of timing coordination.","We also discuss the effect of quantum memory, given that end-to-end extension of entangled states through multi-node entanglement exchange is essential for the practical application of quantum networks.","Finally, cycles of all-optical links in the network topology are shown to may not be to synchronize, this property should be taken into account when considering synchronization in large networks."],"url":"http://arxiv.org/abs/2405.09881v1","category":"quant-ph"}
{"created":"2024-05-16 08:00:55","title":"Generative Unlearning for Any Identity","abstract":"Recent advances in generative models trained on large-scale datasets have made it possible to synthesize high-quality samples across various domains. Moreover, the emergence of strong inversion networks enables not only a reconstruction of real-world images but also the modification of attributes through various editing methods. However, in certain domains related to privacy issues, e.g., human faces, advanced generative models along with strong inversion methods can lead to potential misuses. In this paper, we propose an essential yet under-explored task called generative identity unlearning, which steers the model not to generate an image of a specific identity. In the generative identity unlearning, we target the following objectives: (i) preventing the generation of images with a certain identity, and (ii) preserving the overall quality of the generative model. To satisfy these goals, we propose a novel framework, Generative Unlearning for Any Identity (GUIDE), which prevents the reconstruction of a specific identity by unlearning the generator with only a single image. GUIDE consists of two parts: (i) finding a target point for optimization that un-identifies the source latent code and (ii) novel loss functions that facilitate the unlearning procedure while less affecting the learned distribution. Our extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in the generative machine unlearning task. The code is available at https://github.com/KHU-AGI/GUIDE.","sentences":["Recent advances in generative models trained on large-scale datasets have made it possible to synthesize high-quality samples across various domains.","Moreover, the emergence of strong inversion networks enables not only a reconstruction of real-world images but also the modification of attributes through various editing methods.","However, in certain domains related to privacy issues, e.g., human faces, advanced generative models along with strong inversion methods can lead to potential misuses.","In this paper, we propose an essential yet under-explored task called generative identity unlearning, which steers the model not to generate an image of a specific identity.","In the generative identity unlearning, we target the following objectives: (i) preventing the generation of images with a certain identity, and (ii) preserving the overall quality of the generative model.","To satisfy these goals, we propose a novel framework, Generative Unlearning for Any Identity (GUIDE), which prevents the reconstruction of a specific identity by unlearning the generator with only a single image.","GUIDE consists of two parts: (i) finding a target point for optimization that un-identifies the source latent code and (ii) novel loss functions that facilitate the unlearning procedure while less affecting the learned distribution.","Our extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in the generative machine unlearning task.","The code is available at https://github.com/KHU-AGI/GUIDE."],"url":"http://arxiv.org/abs/2405.09879v1","category":"cs.CV"}
{"created":"2024-05-16 17:19:06","title":"A Tale of Two Languages: Large-Vocabulary Continuous Sign Language Recognition from Spoken Language Supervision","abstract":"In this work, our goals are two fold: large-vocabulary continuous sign language recognition (CSLR), and sign language retrieval. To this end, we introduce a multi-task Transformer model, CSLR2, that is able to ingest a signing sequence and output in a joint embedding space between signed language and spoken language text. To enable CSLR evaluation in the large-vocabulary setting, we introduce new dataset annotations that have been manually collected. These provide continuous sign-level annotations for six hours of test videos, and will be made publicly available. We demonstrate that by a careful choice of loss functions, training the model for both the CSLR and retrieval tasks is mutually beneficial in terms of performance -- retrieval improves CSLR performance by providing context, while CSLR improves retrieval with more fine-grained supervision. We further show the benefits of leveraging weak and noisy supervision from large-vocabulary datasets such as BOBSL, namely sign-level pseudo-labels, and English subtitles. Our model significantly outperforms the previous state of the art on both tasks.","sentences":["In this work, our goals are two fold: large-vocabulary continuous sign language recognition (CSLR), and sign language retrieval.","To this end, we introduce a multi-task Transformer model, CSLR2, that is able to ingest a signing sequence and output in a joint embedding space between signed language and spoken language text.","To enable CSLR evaluation in the large-vocabulary setting, we introduce new dataset annotations that have been manually collected.","These provide continuous sign-level annotations for six hours of test videos, and will be made publicly available.","We demonstrate that by a careful choice of loss functions, training the model for both the CSLR and retrieval tasks is mutually beneficial in terms of performance -- retrieval improves CSLR performance by providing context, while CSLR improves retrieval with more fine-grained supervision.","We further show the benefits of leveraging weak and noisy supervision from large-vocabulary datasets such as BOBSL, namely sign-level pseudo-labels, and English subtitles.","Our model significantly outperforms the previous state of the art on both tasks."],"url":"http://arxiv.org/abs/2405.10266v1","category":"cs.CV"}
{"created":"2024-05-16 16:36:16","title":"Novel Data Models for Inter-operable LCA Frameworks","abstract":"Life cycle assessment (LCA) plays a critical role in assessing the environmental impacts of a product, technology, or service throughout its entire life cycle. Nonetheless, many existing LCA tools and methods lack adequate metadata management, which can hinder their further development and wide adoption. In the example of LCA for clean energy technologies, metadata helps monitor data and the environment that holds the integrity of the energy assets and sustainability of the materials sources across their entire value chains. Ontologizing metadata, i.e. a common vocabulary and language to connect multiple data sources, as well as implementing AI-aware data management, can have long-lasting, positive, and accelerating effects along with collecting and utilizing quality data from different sources and across the entire data lifecycle. The integration of ontologies in life cycle assessments has garnered significant attention in recent years. We synthesized the existing literature on ontologies for LCAs, providing insights into this interdisciplinary field's evolution, current state, and future directions. We also proposed the framework for a suitable data model and the workflow thereof to warrant the alignment with existing ontologies, practical frameworks, and industry standards.","sentences":["Life cycle assessment (LCA) plays a critical role in assessing the environmental impacts of a product, technology, or service throughout its entire life cycle.","Nonetheless, many existing LCA tools and methods lack adequate metadata management, which can hinder their further development and wide adoption.","In the example of LCA for clean energy technologies, metadata helps monitor data and the environment that holds the integrity of the energy assets and sustainability of the materials sources across their entire value chains.","Ontologizing metadata, i.e. a common vocabulary and language to connect multiple data sources, as well as implementing AI-aware data management, can have long-lasting, positive, and accelerating effects along with collecting and utilizing quality data from different sources and across the entire data lifecycle.","The integration of ontologies in life cycle assessments has garnered significant attention in recent years.","We synthesized the existing literature on ontologies for LCAs, providing insights into this interdisciplinary field's evolution, current state, and future directions.","We also proposed the framework for a suitable data model and the workflow thereof to warrant the alignment with existing ontologies, practical frameworks, and industry standards."],"url":"http://arxiv.org/abs/2405.10235v1","category":"cs.DB"}
{"created":"2024-05-16 16:11:16","title":"Kramers nodal line in the charge density wave state of YTe$_3$ and the influence of twin domains","abstract":"Recent studies have focused on the relationship between charge density wave (CDW) collective electronic ground states and nontrivial topological states. Using angle-resolved photoemission and density functional theory, we establish that YTe$_3$ is a CDW-induced Kramers nodal line (KNL) metal, a newly proposed topological state of matter. YTe$_3$ is a non-magnetic quasi-2D chalcogenide with a CDW wave vector ($q_{\\rm cdw}$) of 0.2907c$^*$. Scanning tunneling microscopy and low energy electron diffraction revealed two orthogonal CDW domains, each with a unidirectional CDW and similar YTe$_3$. The effective band structure (EBS) computations, using DFT-calculated folded bands, show excellent agreement with ARPES because a realistic x-ray crystal structure and twin domains are considered in the calculations. The Fermi surface and ARPES intensity plots show weak shadow bands displaced by $q_{\\rm cdw}$ from the main bands. These are linked to CDW modulation, as the EBS calculation confirms. Bilayer split main and shadow bands suggest the existence of crossings, according to theory and experiment. DFT bands, including spin-orbit coupling, indicate a nodal line along the $\\Sigma$ line from multiple band crossings perpendicular to the KNL. Additionally, doubly degenerate bands are only found along the KNL at all energies, with some bands dispersing through the Fermi level.","sentences":["Recent studies have focused on the relationship between charge density wave (CDW) collective electronic ground states and nontrivial topological states.","Using angle-resolved photoemission and density functional theory, we establish that YTe$_3$ is a CDW-induced Kramers nodal line (KNL) metal, a newly proposed topological state of matter.","YTe$_3$ is a non-magnetic quasi-2D chalcogenide with a CDW wave vector ($q_{\\rm cdw}$) of 0.2907c$^*$. Scanning tunneling microscopy and low energy electron diffraction revealed two orthogonal CDW domains, each with a unidirectional CDW and similar YTe$_3$. The effective band structure (EBS) computations, using DFT-calculated folded bands, show excellent agreement with ARPES because a realistic x-ray crystal structure and twin domains are considered in the calculations.","The Fermi surface and ARPES intensity plots show weak shadow bands displaced by $q_{\\rm cdw}$ from the main bands.","These are linked to CDW modulation, as the EBS calculation confirms.","Bilayer split main and shadow bands suggest the existence of crossings, according to theory and experiment.","DFT bands, including spin-orbit coupling, indicate a nodal line along the $\\Sigma$ line from multiple band crossings perpendicular to the KNL.","Additionally, doubly degenerate bands are only found along the KNL at all energies, with some bands dispersing through the Fermi level."],"url":"http://arxiv.org/abs/2405.10222v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 16:11:00","title":"Scalarisation-based risk concepts for robust multi-objective optimisation","abstract":"Robust optimisation is a well-established framework for optimising functions in the presence of uncertainty. The inherent goal of this problem is to identify a collection of inputs whose outputs are both desirable for the decision maker, whilst also being robust to the underlying uncertainties in the problem. In this work, we study the multi-objective extension of this problem from a computational standpoint. We identify that the majority of all robust multi-objective algorithms rely on two key operations: robustification and scalarisation. Robustification refers to the strategy that is used to marginalise over the uncertainty in the problem. Whilst scalarisation refers to the procedure that is used to encode the relative importance of each objective. As these operations are not necessarily commutative, the order that they are performed in has an impact on the resulting solutions that are identified and the final decisions that are made. This work aims to give an exposition on the philosophical differences between these two operations and highlight when one should opt for one ordering over the other. As part of our analysis, we showcase how many existing risk concepts can be easily integrated into the specification and solution of a robust multi-objective optimisation problem. Besides this, we also demonstrate how one can principally define the notion of a robust Pareto front and a robust performance metric based on our robustify and scalarise methodology. To illustrate the efficacy of these new ideas, we present two insightful numerical case studies which are based on real-world data sets.","sentences":["Robust optimisation is a well-established framework for optimising functions in the presence of uncertainty.","The inherent goal of this problem is to identify a collection of inputs whose outputs are both desirable for the decision maker, whilst also being robust to the underlying uncertainties in the problem.","In this work, we study the multi-objective extension of this problem from a computational standpoint.","We identify that the majority of all robust multi-objective algorithms rely on two key operations: robustification and scalarisation.","Robustification refers to the strategy that is used to marginalise over the uncertainty in the problem.","Whilst scalarisation refers to the procedure that is used to encode the relative importance of each objective.","As these operations are not necessarily commutative, the order that they are performed in has an impact on the resulting solutions that are identified and the final decisions that are made.","This work aims to give an exposition on the philosophical differences between these two operations and highlight when one should opt for one ordering over the other.","As part of our analysis, we showcase how many existing risk concepts can be easily integrated into the specification and solution of a robust multi-objective optimisation problem.","Besides this, we also demonstrate how one can principally define the notion of a robust Pareto front and a robust performance metric based on our robustify and scalarise methodology.","To illustrate the efficacy of these new ideas, we present two insightful numerical case studies which are based on real-world data sets."],"url":"http://arxiv.org/abs/2405.10221v1","category":"math.OC"}
{"created":"2024-05-16 16:02:42","title":"Words as Trigger Points in Social Media Discussions","abstract":"Trigger points are a concept introduced by Mau, Lux, and Westheuser (2023) to study qualitative focus group interviews and understand polarisation in Germany. When people communicate, trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals react affectively to such triggers and show strong and negative emotional responses. In this paper, we introduce the first systematic study of the large-scale effect of individual words as trigger points by analysing a large amount of social media posts. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million posts from subreddits related to a set of words identified as trigger points in UK politics. We find that such trigger words affect user engagement and have noticeable consequences on animosity in online discussions. We share empirical evidence of trigger words causing animosity, and how they provide incentives for hate speech, adversarial debates, and disagreements. Our work is the first to introduce trigger points to computational studies of online communication. Our findings are relevant to researchers interested in online harms and who examine how citizens debate politics and society in light of affective polarisation.","sentences":["Trigger points are a concept introduced by Mau, Lux, and Westheuser (2023) to study qualitative focus group interviews and understand polarisation in Germany.","When people communicate, trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned.","In the original studies, individuals react affectively to such triggers and show strong and negative emotional responses.","In this paper, we introduce the first systematic study of the large-scale effect of individual words as trigger points by analysing a large amount of social media posts.","We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million posts from subreddits related to a set of words identified as trigger points in UK politics.","We find that such trigger words affect user engagement and have noticeable consequences on animosity in online discussions.","We share empirical evidence of trigger words causing animosity, and how they provide incentives for hate speech, adversarial debates, and disagreements.","Our work is the first to introduce trigger points to computational studies of online communication.","Our findings are relevant to researchers interested in online harms and who examine how citizens debate politics and society in light of affective polarisation."],"url":"http://arxiv.org/abs/2405.10213v1","category":"cs.SI"}
{"created":"2024-05-16 15:51:54","title":"A Participatory Budgeting based Truthful Budget-Limited Incentive Mechanism for Time-Constrained Tasks in Crowdsensing Systems","abstract":"Crowdsensing, also known as participatory sensing, is a method of data collection that involves gathering information from a large number of common people (or individuals), often using mobile devices or other personal technologies. This paper considers the set-up with multiple task requesters and several task executors in a strategic setting. Each task requester has multiple heterogeneous tasks and an estimated budget for the tasks. In our proposed model, the Government has a publicly known fund (or budget) and is limited. Due to limited funds, it may not be possible for the platform to offer the funds to all the available task requesters. For that purpose, in the first tier, the voting by the city dwellers over the task requesters is carried out to decide on the subset of task requesters receiving the Government fund. In the second tier, each task of the task requesters has start and finish times. Based on that, firstly, the tasks are distributed to distinct slots. In each slot, we have multiple task executors for executing the floated tasks. Each task executor reports a cost (private) for completing the floated task(s). Given the above-discussed set-up, the objectives of the second tier are: (1) to schedule each task of the task requesters in the available slots in a non-conflicting manner and (2) to select a set of executors for the available tasks in such a way that the total incentive given to the task executors should be at most the budget for the tasks. For the discussed scenario, a truthful incentive based mechanism is designed that also takes care of budget criteria. Theoretical analysis is done, and it shows that the proposed mechanism is computationally efficient, truthful, budget-feasible, and individually rational. The simulation is carried out, and the efficacy of the designed mechanism is compared with the state-of-the-art mechanisms.","sentences":["Crowdsensing, also known as participatory sensing, is a method of data collection that involves gathering information from a large number of common people (or individuals), often using mobile devices or other personal technologies.","This paper considers the set-up with multiple task requesters and several task executors in a strategic setting.","Each task requester has multiple heterogeneous tasks and an estimated budget for the tasks.","In our proposed model, the Government has a publicly known fund (or budget) and is limited.","Due to limited funds, it may not be possible for the platform to offer the funds to all the available task requesters.","For that purpose, in the first tier, the voting by the city dwellers over the task requesters is carried out to decide on the subset of task requesters receiving the Government fund.","In the second tier, each task of the task requesters has start and finish times.","Based on that, firstly, the tasks are distributed to distinct slots.","In each slot, we have multiple task executors for executing the floated tasks.","Each task executor reports a cost (private) for completing the floated task(s).","Given the above-discussed set-up, the objectives of the second tier are: (1) to schedule each task of the task requesters in the available slots in a non-conflicting manner and (2) to select a set of executors for the available tasks in such a way that the total incentive given to the task executors should be at most the budget for the tasks.","For the discussed scenario, a truthful incentive based mechanism is designed that also takes care of budget criteria.","Theoretical analysis is done, and it shows that the proposed mechanism is computationally efficient, truthful, budget-feasible, and individually rational.","The simulation is carried out, and the efficacy of the designed mechanism is compared with the state-of-the-art mechanisms."],"url":"http://arxiv.org/abs/2405.10206v1","category":"cs.GT"}
{"created":"2024-05-16 15:21:47","title":"Conditions on the continuity of the Hausdorff measure","abstract":"Let $b_k$ be strictly decreasing sequence of real numbers such that $b_0 = 1$ and $f_k$ be decreasing, linear functions such that $f_k(b_k) = 1$ and $f_k(b_{k-1}) = 0$, $k = 1, 2, \\dots$. We define iterated function system (IFS) $S_n$ by limiting the collection of functions $f_k$ to first n, meaning $S_n = \\{f_k \\}_{k=1}^n$. Let $J_n$ denote the limit set of $S_n$. We show that if $S_n$ fulfills the following two conditions: (1)~$\\lim\\limits_{n \\to \\infty} \\left(1-h_n\\right) \\ln{n} = 0 $ where $h_n$ is the Hausdorff dimension of $J_n$, and (2)~$\\sup \\limits_{k\\in \\mathbb{N}} \\left \\{\\frac{b_k-b_{k+1}}{b_{k+1}} \\right \\} < \\infty $, then $\\lim\\limits_{n\\to \\infty} H_{h_n}(J_n) = 1 = H_1(J)$, where $h_n$ is the Hausdorff dimension of $J_n$ and $H_{h_n}$ is the corresponding Hausdorff measure. We also show examples of families of IFSes fulfilling those properties.","sentences":["Let $b_k$ be strictly decreasing sequence of real numbers such that $b_0 = 1$ and $f_k$ be decreasing, linear functions such that $f_k(b_k) = 1$ and $f_k(b_{k-1})","= 0$, $k = 1, 2, \\dots$.","We define iterated function system (IFS) $S_n$ by limiting the collection of functions $f_k$ to first n, meaning $S_n = \\{f_k \\}_{k=1}^n$. Let $J_n$ denote the limit set of $S_n$. We show that if $S_n$ fulfills the following two conditions: (1)~$\\lim\\limits_{n \\to \\infty} \\left(1-h_n\\right) \\ln{n} = 0 $ where $h_n$ is the Hausdorff dimension of $J_n$, and (2)~$\\sup \\limits_{k\\in \\mathbb{N}} \\left \\{\\frac{b_k-b_{k+1}}{b_{k+1}} \\right \\} <","\\infty $, then $\\lim\\limits_{n\\to \\infty} H_{h_n}(J_n) = 1 = H_1(J)$, where $h_n$ is the Hausdorff dimension of $J_n$ and $H_{h_n}$ is the corresponding Hausdorff measure.","We also show examples of families of IFSes fulfilling those properties."],"url":"http://arxiv.org/abs/2405.10179v1","category":"math.DS"}
{"created":"2024-05-16 15:02:24","title":"LFED: A Literary Fiction Evaluation Dataset for Large Language Models","abstract":"The rapid evolution of large language models (LLMs) has ushered in the need for comprehensive assessments of their performance across various dimensions. In this paper, we propose LFED, a Literary Fiction Evaluation Dataset, which aims to evaluate the capability of LLMs on the long fiction comprehension and reasoning. We collect 95 literary fictions that are either originally written in Chinese or translated into Chinese, covering a wide range of topics across several centuries. We define a question taxonomy with 8 question categories to guide the creation of 1,304 questions. Additionally, we conduct an in-depth analysis to ascertain how specific attributes of literary fictions (e.g., novel types, character numbers, the year of publication) impact LLM performance in evaluations. Through a series of experiments with various state-of-the-art LLMs, we demonstrate that these models face considerable challenges in effectively addressing questions related to literary fictions, with ChatGPT reaching only 57.08% under the zero-shot setting. The dataset will be publicly available at https://github.com/tjunlp-lab/LFED.git","sentences":["The rapid evolution of large language models (LLMs) has ushered in the need for comprehensive assessments of their performance across various dimensions.","In this paper, we propose LFED, a Literary Fiction Evaluation Dataset, which aims to evaluate the capability of LLMs on the long fiction comprehension and reasoning.","We collect 95 literary fictions that are either originally written in Chinese or translated into Chinese, covering a wide range of topics across several centuries.","We define a question taxonomy with 8 question categories to guide the creation of 1,304 questions.","Additionally, we conduct an in-depth analysis to ascertain how specific attributes of literary fictions (e.g., novel types, character numbers, the year of publication) impact LLM performance in evaluations.","Through a series of experiments with various state-of-the-art LLMs, we demonstrate that these models face considerable challenges in effectively addressing questions related to literary fictions, with ChatGPT reaching only 57.08% under the zero-shot setting.","The dataset will be publicly available at https://github.com/tjunlp-lab/LFED.git"],"url":"http://arxiv.org/abs/2405.10166v1","category":"cs.CL"}
{"created":"2024-05-16 15:01:57","title":"Recent results from MAGIC","abstract":"MAGIC is a system of two 17-m diameter Imaging Atmospheric Cherenkov Telescopes, located at an altitude of 2200 m in Roque de los Muchachos on the Canary island of La Palma, exploring the gamma-ray sky above a few tens of GeV and up to tens of TeV. This system provides a low energy threshold and a fast automated response to transient phenomena. In this contribution, some selected results of MAGIC, which has been collecting data for more than 20 years, are reviewed. Special attention is given to multiwavelength and multimessenger astronomy, such as GRB 201216C, the farthest ground-based detection of a very-high-energy gamma-ray bursts, as well as the RS Ophiuchi nova. The scientific program also includes measuring the cosmic-ray electron positron spectrum, estimating the size of stars using intensity interferometry, studying gravitational lensing and searching for dark matter in spheroidal galaxies. Finally, a glimpse into the future is given by presenting the performance of the joint observations with the first Large-Sized Telescope from the Cherenkov Telescope Array and MAGIC.","sentences":["MAGIC is a system of two 17-m diameter Imaging Atmospheric Cherenkov Telescopes, located at an altitude of 2200 m in Roque de los Muchachos on the Canary island of La Palma, exploring the gamma-ray sky above a few tens of GeV and up to tens of TeV. This system provides a low energy threshold and a fast automated response to transient phenomena.","In this contribution, some selected results of MAGIC, which has been collecting data for more than 20 years, are reviewed.","Special attention is given to multiwavelength and multimessenger astronomy, such as GRB 201216C, the farthest ground-based detection of a very-high-energy gamma-ray bursts, as well as the RS Ophiuchi nova.","The scientific program also includes measuring the cosmic-ray electron positron spectrum, estimating the size of stars using intensity interferometry, studying gravitational lensing and searching for dark matter in spheroidal galaxies.","Finally, a glimpse into the future is given by presenting the performance of the joint observations with the first Large-Sized Telescope from the Cherenkov Telescope Array and MAGIC."],"url":"http://arxiv.org/abs/2405.10165v1","category":"astro-ph.HE"}
{"created":"2024-05-16 14:52:42","title":"Mechanics and morphology of proliferating cell collectives with self-inhibiting growth","abstract":"We study the dynamics of proliferating cell collectives whose microscopic constituents' growth is inhibited by macroscopic growth-induced stress. Discrete particle simulations of a growing collective show the emergence of concentric-ring patterns in cell size whose spatio-temporal structure is closely tied to the individual cell's stress response. Motivated by these observations, we derive a multiscale continuum theory whose parameters map directly to the discrete model. Analytical solutions of this theory show the concentric patterns arise from anisotropically accumulated resistance to growth over many cell cycles. This work shows how purely mechanical processes can affect the internal patterning and morphology of cell collectives, and provides a concise theoretical framework for connecting the micro- to macroscopic dynamics of proliferating matter.","sentences":["We study the dynamics of proliferating cell collectives whose microscopic constituents' growth is inhibited by macroscopic growth-induced stress.","Discrete particle simulations of a growing collective show the emergence of concentric-ring patterns in cell size whose spatio-temporal structure is closely tied to the individual cell's stress response.","Motivated by these observations, we derive a multiscale continuum theory whose parameters map directly to the discrete model.","Analytical solutions of this theory show the concentric patterns arise from anisotropically accumulated resistance to growth over many cell cycles.","This work shows how purely mechanical processes can affect the internal patterning and morphology of cell collectives, and provides a concise theoretical framework for connecting the micro- to macroscopic dynamics of proliferating matter."],"url":"http://arxiv.org/abs/2405.10158v1","category":"physics.bio-ph"}
{"created":"2024-05-16 14:48:24","title":"Firefighters' Perceptions on Collaboration and Interaction with Autonomous Drones: Results of a Field Trial","abstract":"Applications of drones in emergency response, like firefighting, have been promoted in the past decade. As the autonomy of drones continues to improve, the ways in which they are integrated into firefighting teams and their impact on crews are changing. This demands more understanding of how firefighters perceive and interact with autonomous drones. This paper presents a drone-based system for emergency operations with which firefighters can interact through sound, lights, and a graphical user interface. We use interviews with stakeholders collected in two field trials to explore their perceptions of the interaction and collaboration with drones. Our result shows that firefighters perceived visual interaction as adequate. However, for audio instructions and interfaces, information overload emerges as an essential problem. The potential impact of drones on current work configurations may involve shifting the position of humans closer to supervisory decision-makers and changing the training structure and content.","sentences":["Applications of drones in emergency response, like firefighting, have been promoted in the past decade.","As the autonomy of drones continues to improve, the ways in which they are integrated into firefighting teams and their impact on crews are changing.","This demands more understanding of how firefighters perceive and interact with autonomous drones.","This paper presents a drone-based system for emergency operations with which firefighters can interact through sound, lights, and a graphical user interface.","We use interviews with stakeholders collected in two field trials to explore their perceptions of the interaction and collaboration with drones.","Our result shows that firefighters perceived visual interaction as adequate.","However, for audio instructions and interfaces, information overload emerges as an essential problem.","The potential impact of drones on current work configurations may involve shifting the position of humans closer to supervisory decision-makers and changing the training structure and content."],"url":"http://arxiv.org/abs/2405.10153v1","category":"cs.HC"}
{"created":"2024-05-16 14:33:39","title":"PL-MTEB: Polish Massive Text Embedding Benchmark","abstract":"In this paper, we introduce the Polish Massive Text Embedding Benchmark (PL-MTEB), a comprehensive benchmark for text embeddings in Polish. The PL-MTEB consists of 28 diverse NLP tasks from 5 task types. We adapted the tasks based on previously used datasets by the Polish NLP community. In addition, we created a new PLSC (Polish Library of Science Corpus) dataset consisting of titles and abstracts of scientific publications in Polish, which was used as the basis for two novel clustering tasks. We evaluated 15 publicly available models for text embedding, including Polish and multilingual ones, and collected detailed results for individual tasks and aggregated results for each task type and the entire benchmark. PL-MTEB comes with open-source code at https://github.com/rafalposwiata/pl-mteb.","sentences":["In this paper, we introduce the Polish Massive Text Embedding Benchmark (PL-MTEB), a comprehensive benchmark for text embeddings in Polish.","The PL-MTEB consists of 28 diverse NLP tasks from 5 task types.","We adapted the tasks based on previously used datasets by the Polish NLP community.","In addition, we created a new PLSC (Polish Library of Science Corpus) dataset consisting of titles and abstracts of scientific publications in Polish, which was used as the basis for two novel clustering tasks.","We evaluated 15 publicly available models for text embedding, including Polish and multilingual ones, and collected detailed results for individual tasks and aggregated results for each task type and the entire benchmark.","PL-MTEB comes with open-source code at https://github.com/rafalposwiata/pl-mteb."],"url":"http://arxiv.org/abs/2405.10138v1","category":"cs.CL"}
{"created":"2024-05-16 14:02:42","title":"In-situ optical vector analysis based on integrated lithium niobate single-sideband modulators","abstract":"Optical vector analysis (OVA) is an enabling technology for comprehensively characterizing both amplitude and phase responses of optical devices or systems. Conventional OVA technologies are mostly based on discrete optoelectronic components, leading to unsatisfactory system sizes, complexity, and stability. They also encounter challenges in revealing the on-chip characteristics of integrated photonic devices, which are often overwhelmed by the substantial coupling loss and extra spectral response at chip facets. In this work, we demonstrate a miniaturized OVA system for integrated photonics devices based on broadband single sideband (SSB) modulators on a thin-film lithium niobate (LN) platform. The OVA could provide a direct probe of both amplitude and phase responses of photonic devices with kHz-level resolution and tens of terahertz measurement bandwidth. We perform in-situ characterizations of single and coupled microring resonators fabricated on the same chip as the OVA, unfolding their intrinsic loss and coupling states unambiguously. Furthermore, we achieve the direct measurement of collective phase dynamics and density of states of the Bloch modes in a synthetic frequency crystal, by in-situ OVA of a dynamically modulated microring resonator. Our in-situ OVA system provides a compact, high-precision, and broadband solution for characterizing future integrated photonic devices and circuits, with potential applications ranging from optical communications, biosensing, neuromorphic computing, to quantum information processing.","sentences":["Optical vector analysis (OVA) is an enabling technology for comprehensively characterizing both amplitude and phase responses of optical devices or systems.","Conventional OVA technologies are mostly based on discrete optoelectronic components, leading to unsatisfactory system sizes, complexity, and stability.","They also encounter challenges in revealing the on-chip characteristics of integrated photonic devices, which are often overwhelmed by the substantial coupling loss and extra spectral response at chip facets.","In this work, we demonstrate a miniaturized OVA system for integrated photonics devices based on broadband single sideband (SSB) modulators on a thin-film lithium niobate (LN) platform.","The OVA could provide a direct probe of both amplitude and phase responses of photonic devices with kHz-level resolution and tens of terahertz measurement bandwidth.","We perform in-situ characterizations of single and coupled microring resonators fabricated on the same chip as the OVA, unfolding their intrinsic loss and coupling states unambiguously.","Furthermore, we achieve the direct measurement of collective phase dynamics and density of states of the Bloch modes in a synthetic frequency crystal, by in-situ OVA of a dynamically modulated microring resonator.","Our in-situ OVA system provides a compact, high-precision, and broadband solution for characterizing future integrated photonic devices and circuits, with potential applications ranging from optical communications, biosensing, neuromorphic computing, to quantum information processing."],"url":"http://arxiv.org/abs/2405.10109v1","category":"physics.optics"}
{"created":"2024-05-16 14:00:50","title":"Geometry of the fixed points loci and discretization of Springer fibers in classical types","abstract":"Consider a simple algebraic group $G$ of classical type and its Lie algebra $\\mathfrak{g}$. Let $(e,h,f) \\subset \\mathfrak{g}$ be an $\\mathfrak{sl}_2$-triple and $Q_e= C_G(e,h,f)$. The torus $T_e$ that comes from the $\\mathfrak{sl}_2$-triple acts on the Springer fiber $\\mathcal{B}_e$. Let $\\mathcal{B}_e^{gr}$ denote the fixed point loci of $\\mathcal{B}_e$ under this torus action. Our main geometric result is that when the partition of $e$ has up to $4$ rows, the derived category $D^b(\\mathcal{B}_e^{gr})$ admits a complete exceptional collection that is compatible with the $Q_e$-action. The objects in this collection give us a finite set $Y_e$ that is naturally equipped with a $Q_e$-centrally extended structure. We prove that the set $Y_e$ constructed in this way coincides with a finite set that has appeared in various contexts in representation theory. For example, a direct summand $J_c$ of the asymptotic Hecke algebra is isomorphic to $K_0(Sh^{Q_e}(Y_e\\times Y_e)$. The left cells in the two-sided cell $c$ corresponding to the adjoint orbit of $e$ are in bijection with the $Q_e$-orbits in $Y_e$. Our main numerical result is an algorithm to compute the multiplicities of the $Q_e$-centrally extended orbits that appear in $Y_e$.","sentences":["Consider a simple algebraic group $G$ of classical type and its Lie algebra $\\mathfrak{g}$. Let $(e,h,f) \\subset \\mathfrak{g}$ be an $\\mathfrak{sl}_2$-triple and $Q_e= C_G(e,h,f)$.","The torus $T_e$ that comes from the $\\mathfrak{sl}_2$-triple acts on the Springer fiber $\\mathcal{B}_e$. Let $\\mathcal{B}_e^{gr}$ denote the fixed point loci of $\\mathcal{B}_e$ under this torus action.","Our main geometric result is that when the partition of $e$ has up to $4$ rows, the derived category $D^b(\\mathcal{B}_e^{gr})$ admits a complete exceptional collection that is compatible with the $Q_e$-action.","The objects in this collection give us a finite set $Y_e$ that is naturally equipped with a $Q_e$-centrally extended structure.","We prove that the set $Y_e$ constructed in this way coincides with a finite set that has appeared in various contexts in representation theory.","For example, a direct summand $J_c$ of the asymptotic Hecke algebra is isomorphic to $K_0(Sh^{Q_e}(Y_e\\times Y_e)$.","The left cells in the two-sided cell $c$ corresponding to the adjoint orbit of $e$ are in bijection with the $Q_e$-orbits in $Y_e$. Our main numerical result is an algorithm to compute the multiplicities of the $Q_e$-centrally extended orbits that appear in $Y_e$."],"url":"http://arxiv.org/abs/2405.10105v1","category":"math.RT"}
{"created":"2024-05-16 12:56:32","title":"Sparse and Orthogonal Low-rank Collective Matrix Factorization (solrCMF): Efficient data integration in flexible layouts","abstract":"Interest in unsupervised methods for joint analysis of heterogeneous data sources has risen in recent years. Low-rank latent factor models have proven to be an effective tool for data integration and have been extended to a large number of data source layouts. Of particular interest is the separation of variation present in data sources into shared and individual subspaces. In addition, interpretability of estimated latent factors is crucial to further understanding.   We present sparse and orthogonal low-rank Collective Matrix Factorization (solrCMF) to estimate low-rank latent factor models for flexible data layouts. These encompass traditional multi-view (one group, multiple data types) and multi-grid (multiple groups, multiple data types) layouts, as well as augmented layouts, which allow the inclusion of side information between data types or groups. In addition, solrCMF allows tensor-like layouts (repeated layers), estimates interpretable factors, and determines variation structure among factors and data sources.   Using a penalized optimization approach, we automatically separate variability into the globally and partially shared as well as individual components and estimate sparse representations of factors. To further increase interpretability of factors, we enforce orthogonality between them. Estimation is performed efficiently in a recent multi-block ADMM framework which we adapted to support embedded manifold constraints.   The performance of solrCMF is demonstrated in simulation studies and compares favorably to existing methods.","sentences":["Interest in unsupervised methods for joint analysis of heterogeneous data sources has risen in recent years.","Low-rank latent factor models have proven to be an effective tool for data integration and have been extended to a large number of data source layouts.","Of particular interest is the separation of variation present in data sources into shared and individual subspaces.","In addition, interpretability of estimated latent factors is crucial to further understanding.   ","We present sparse and orthogonal low-rank Collective Matrix Factorization (solrCMF) to estimate low-rank latent factor models for flexible data layouts.","These encompass traditional multi-view (one group, multiple data types) and multi-grid (multiple groups, multiple data types) layouts, as well as augmented layouts, which allow the inclusion of side information between data types or groups.","In addition, solrCMF allows tensor-like layouts (repeated layers), estimates interpretable factors, and determines variation structure among factors and data sources.   ","Using a penalized optimization approach, we automatically separate variability into the globally and partially shared as well as individual components and estimate sparse representations of factors.","To further increase interpretability of factors, we enforce orthogonality between them.","Estimation is performed efficiently in a recent multi-block ADMM framework which we adapted to support embedded manifold constraints.   ","The performance of solrCMF is demonstrated in simulation studies and compares favorably to existing methods."],"url":"http://arxiv.org/abs/2405.10067v1","category":"stat.ME"}
{"created":"2024-05-16 12:50:15","title":"Typing Requirement Model as Coroutines","abstract":"Model-Driven Engineering (MDE) is a technique that aims to boost productivity in software development and ensure the safety of critical systems. Central to MDE is the refinement of high-level requirement models into executable code. Given that requirement models form the foundation of the entire development process, ensuring their correctness is crucial. RM2PT is a widely used MDE platform that employs the REModel language for requirement modeling. REModel contains contract sections and other sections including a UML sequence diagram. This paper contributes a coroutine-based type system that represents pre- and post-conditions in the contract sections in a requirement model as the receiving and yielding parts of coroutines, respectively. The type system is capable of composing coroutine types, so that users can view functions as a whole system and check their collective behavior. By doing so, our type system ensures that the contracts defined in it are executed as outlined in the accompanied sequence diagram. We assessed our approach using four case studies provided by RM2PT, validating the accuracy of the models.","sentences":["Model-Driven Engineering (MDE) is a technique that aims to boost productivity in software development and ensure the safety of critical systems.","Central to MDE is the refinement of high-level requirement models into executable code.","Given that requirement models form the foundation of the entire development process, ensuring their correctness is crucial.","RM2PT is a widely used MDE platform that employs the REModel language for requirement modeling.","REModel contains contract sections and other sections including a UML sequence diagram.","This paper contributes a coroutine-based type system that represents pre- and post-conditions in the contract sections in a requirement model as the receiving and yielding parts of coroutines, respectively.","The type system is capable of composing coroutine types, so that users can view functions as a whole system and check their collective behavior.","By doing so, our type system ensures that the contracts defined in it are executed as outlined in the accompanied sequence diagram.","We assessed our approach using four case studies provided by RM2PT, validating the accuracy of the models."],"url":"http://arxiv.org/abs/2405.10060v1","category":"cs.SE"}
{"created":"2024-05-16 12:25:39","title":"Crash Landing onto \"you\": Untethered Soft Aerial Robots for Safe Environmental Interaction, Sensing, and Perching","abstract":"There are various desired capabilities to create aerial forest-traversing robots capable of monitoring both biological and abiotic data. The features range from multi-functionality, robustness, and adaptability. These robots have to weather turbulent winds and various obstacles such as forest flora and wildlife thus amplifying the complexity of operating in such uncertain environments. The key for successful data collection is the flexibility to intermittently move from tree-to-tree, in order to perch at vantage locations for elongated time. This effort to perch not only reduces the disturbance caused by multi-rotor systems during data collection, but also allows the system to rest and recharge for longer outdoor missions. Current systems feature the addition of perching modules that increase the aerial robots' weight and reduce the drone's overall endurance. Thus in our work, the key questions currently studied are: \"How do we develop a single robot capable of metamorphosing its body for multi-modal flight and dynamic perching?\", \"How do we detect and land on perchable objects robustly and dynamically?\", and \"What important spatial-temporal data is important for us to collect?\"","sentences":["There are various desired capabilities to create aerial forest-traversing robots capable of monitoring both biological and abiotic data.","The features range from multi-functionality, robustness, and adaptability.","These robots have to weather turbulent winds and various obstacles such as forest flora and wildlife thus amplifying the complexity of operating in such uncertain environments.","The key for successful data collection is the flexibility to intermittently move from tree-to-tree, in order to perch at vantage locations for elongated time.","This effort to perch not only reduces the disturbance caused by multi-rotor systems during data collection, but also allows the system to rest and recharge for longer outdoor missions.","Current systems feature the addition of perching modules that increase the aerial robots' weight and reduce the drone's overall endurance.","Thus in our work, the key questions currently studied are: \"How do we develop a single robot capable of metamorphosing its body for multi-modal flight and dynamic perching?\", \"How do we detect and land on perchable objects robustly and dynamically?\", and \"What important spatial-temporal data is important for us to collect?\""],"url":"http://arxiv.org/abs/2405.10043v1","category":"cs.RO"}
{"created":"2024-05-16 12:23:48","title":"Revealing Hierarchical Structure of Leaf Venations in Plant Science via Label-Efficient Segmentation: Dataset and Method","abstract":"Hierarchical leaf vein segmentation is a crucial but under-explored task in agricultural sciences, where analysis of the hierarchical structure of plant leaf venation can contribute to plant breeding. While current segmentation techniques rely on data-driven models, there is no publicly available dataset specifically designed for hierarchical leaf vein segmentation. To address this gap, we introduce the HierArchical Leaf Vein Segmentation (HALVS) dataset, the first public hierarchical leaf vein segmentation dataset. HALVS comprises 5,057 real-scanned high-resolution leaf images collected from three plant species: soybean, sweet cherry, and London planetree. It also includes human-annotated ground truth for three orders of leaf veins, with a total labeling effort of 83.8 person-days. Based on HALVS, we further develop a label-efficient learning paradigm that leverages partial label information, i.e. missing annotations for tertiary veins. Empirical studies are performed on HALVS, revealing new observations, challenges, and research directions on leaf vein segmentation.","sentences":["Hierarchical leaf vein segmentation is a crucial but under-explored task in agricultural sciences, where analysis of the hierarchical structure of plant leaf venation can contribute to plant breeding.","While current segmentation techniques rely on data-driven models, there is no publicly available dataset specifically designed for hierarchical leaf vein segmentation.","To address this gap, we introduce the HierArchical Leaf Vein Segmentation (HALVS) dataset, the first public hierarchical leaf vein segmentation dataset.","HALVS comprises 5,057 real-scanned high-resolution leaf images collected from three plant species: soybean, sweet cherry, and London planetree.","It also includes human-annotated ground truth for three orders of leaf veins, with a total labeling effort of 83.8 person-days.","Based on HALVS, we further develop a label-efficient learning paradigm that leverages partial label information, i.e. missing annotations for tertiary veins.","Empirical studies are performed on HALVS, revealing new observations, challenges, and research directions on leaf vein segmentation."],"url":"http://arxiv.org/abs/2405.10041v1","category":"cs.CV"}
{"created":"2024-05-16 12:16:07","title":"Large-scale Data Integration using Matrix Denoising and Geometric Factor Matching","abstract":"Unsupervised integrative analysis of multiple data sources has become common place and scalable algorithms are necessary to accommodate ever increasing availability of data. Only few currently methods have estimation speed as their focus, and those that do are only applicable to restricted data layouts such as different data types measured on the same observation units. We introduce a novel point of view on low-rank matrix integration phrased as a graph estimation problem which allows development of a method, large-scale Collective Matrix Factorization (lsCMF), which is able to integrate data in flexible layouts in a speedy fashion. It utilizes a matrix denoising framework for rank estimation and geometric properties of singular vectors to efficiently integrate data. The quick estimation speed of lsCMF while retaining good estimation of data structure is then demonstrated in simulation studies.","sentences":["Unsupervised integrative analysis of multiple data sources has become common place and scalable algorithms are necessary to accommodate ever increasing availability of data.","Only few currently methods have estimation speed as their focus, and those that do are only applicable to restricted data layouts such as different data types measured on the same observation units.","We introduce a novel point of view on low-rank matrix integration phrased as a graph estimation problem which allows development of a method, large-scale Collective Matrix Factorization (lsCMF), which is able to integrate data in flexible layouts in a speedy fashion.","It utilizes a matrix denoising framework for rank estimation and geometric properties of singular vectors to efficiently integrate data.","The quick estimation speed of lsCMF while retaining good estimation of data structure is then demonstrated in simulation studies."],"url":"http://arxiv.org/abs/2405.10036v1","category":"stat.ME"}
{"created":"2024-05-16 12:11:14","title":"Accelerator beam phase space tomography using machine learning to account for variations in beamline components","abstract":"We describe a technique for reconstruction of the four-dimensional transverse phase space of a beam in an accelerator beamline, taking into account the presence of unknown errors on the strengths of magnets used in the data collection. Use of machine learning allows rapid reconstruction of the phase-space distribution while at the same time providing estimates of the magnet errors. The technique is demonstrated using experimental data from CLARA, an accelerator test facility at Daresbury Laboratory.","sentences":["We describe a technique for reconstruction of the four-dimensional transverse phase space of a beam in an accelerator beamline, taking into account the presence of unknown errors on the strengths of magnets used in the data collection.","Use of machine learning allows rapid reconstruction of the phase-space distribution while at the same time providing estimates of the magnet errors.","The technique is demonstrated using experimental data from CLARA, an accelerator test facility at Daresbury Laboratory."],"url":"http://arxiv.org/abs/2405.10028v1","category":"physics.acc-ph"}
{"created":"2024-05-16 12:04:55","title":"$\u0394\\text{-}{\\rm OPE}$: Off-Policy Estimation with Pairs of Policies","abstract":"The off-policy paradigm casts recommendation as a counterfactual decision-making task, allowing practitioners to unbiasedly estimate online metrics using offline data. This leads to effective evaluation metrics, as well as learning procedures that directly optimise online success. Nevertheless, the high variance that comes with unbiasedness is typically the crux that complicates practical applications. An important insight is that the difference between policy values can often be estimated with significantly reduced variance, if said policies have positive covariance. This allows us to formulate a pairwise off-policy estimation task: $\\Delta\\text{-}{\\rm OPE}$.   $\\Delta\\text{-}{\\rm OPE}$ subsumes the common use-case of estimating improvements of a learnt policy over a production policy, using data collected by a stochastic logging policy. We introduce $\\Delta\\text{-}{\\rm OPE}$ methods based on the widely used Inverse Propensity Scoring estimator and its extensions. Moreover, we characterise a variance-optimal additive control variate that further enhances efficiency. Simulated, offline, and online experiments show that our methods significantly improve performance for both evaluation and learning tasks.","sentences":["The off-policy paradigm casts recommendation as a counterfactual decision-making task, allowing practitioners to unbiasedly estimate online metrics using offline data.","This leads to effective evaluation metrics, as well as learning procedures that directly optimise online success.","Nevertheless, the high variance that comes with unbiasedness is typically the crux that complicates practical applications.","An important insight is that the difference between policy values can often be estimated with significantly reduced variance, if said policies have positive covariance.","This allows us to formulate a pairwise off-policy estimation task: $\\Delta\\text{-}{\\rm OPE}$.   $\\Delta\\text{-}{\\rm OPE}$ subsumes the common use-case of estimating improvements of a learnt policy over a production policy, using data collected by a stochastic logging policy.","We introduce $\\Delta\\text{-}{\\rm OPE}$ methods based on the widely used Inverse Propensity Scoring estimator and its extensions.","Moreover, we characterise a variance-optimal additive control variate that further enhances efficiency.","Simulated, offline, and online experiments show that our methods significantly improve performance for both evaluation and learning tasks."],"url":"http://arxiv.org/abs/2405.10024v1","category":"cs.LG"}
{"created":"2024-05-16 12:02:02","title":"Natural Language Can Help Bridge the Sim2Real Gap","abstract":"The main challenge in learning image-conditioned robotic policies is acquiring a visual representation conducive to low-level control. Due to the high dimensionality of the image space, learning a good visual representation requires a considerable amount of visual data. However, when learning in the real world, data is expensive. Sim2Real is a promising paradigm for overcoming data scarcity in the real-world target domain by using a simulator to collect large amounts of cheap data closely related to the target task. However, it is difficult to transfer an image-conditioned policy from sim to real when the domains are very visually dissimilar. To bridge the sim2real visual gap, we propose using natural language descriptions of images as a unifying signal across domains that captures the underlying task-relevant semantics. Our key insight is that if two image observations from different domains are labeled with similar language, the policy should predict similar action distributions for both images. We demonstrate that training the image encoder to predict the language description or the distance between descriptions of a sim or real image serves as a useful, data-efficient pretraining step that helps learn a domain-invariant image representation. We can then use this image encoder as the backbone of an IL policy trained simultaneously on a large amount of simulated and a handful of real demonstrations. Our approach outperforms widely used prior sim2real methods and strong vision-language pretraining baselines like CLIP and R3M by 25 to 40%.","sentences":["The main challenge in learning image-conditioned robotic policies is acquiring a visual representation conducive to low-level control.","Due to the high dimensionality of the image space, learning a good visual representation requires a considerable amount of visual data.","However, when learning in the real world, data is expensive.","Sim2Real is a promising paradigm for overcoming data scarcity in the real-world target domain by using a simulator to collect large amounts of cheap data closely related to the target task.","However, it is difficult to transfer an image-conditioned policy from sim to real when the domains are very visually dissimilar.","To bridge the sim2real visual gap, we propose using natural language descriptions of images as a unifying signal across domains that captures the underlying task-relevant semantics.","Our key insight is that if two image observations from different domains are labeled with similar language, the policy should predict similar action distributions for both images.","We demonstrate that training the image encoder to predict the language description or the distance between descriptions of a sim or real image serves as a useful, data-efficient pretraining step that helps learn a domain-invariant image representation.","We can then use this image encoder as the backbone of an IL policy trained simultaneously on a large amount of simulated and a handful of real demonstrations.","Our approach outperforms widely used prior sim2real methods and strong vision-language pretraining baselines like CLIP and R3M by 25 to 40%."],"url":"http://arxiv.org/abs/2405.10020v1","category":"cs.RO"}
{"created":"2024-05-16 11:28:01","title":"Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance","abstract":"Real driving-video dehazing poses a significant challenge due to the inherent difficulty in acquiring precisely aligned hazy/clear video pairs for effective model training, especially in dynamic driving scenarios with unpredictable weather conditions. In this paper, we propose a pioneering approach that addresses this challenge through a nonaligned regularization strategy. Our core concept involves identifying clear frames that closely match hazy frames, serving as references to supervise a video dehazing network. Our approach comprises two key components: reference matching and video dehazing. Firstly, we introduce a non-aligned reference frame matching module, leveraging an adaptive sliding window to match high-quality reference frames from clear videos. Video dehazing incorporates flow-guided cosine attention sampler and deformable cosine attention fusion modules to enhance spatial multiframe alignment and fuse their improved information. To validate our approach, we collect a GoProHazy dataset captured effortlessly with GoPro cameras in diverse rural and urban road environments. Extensive experiments demonstrate the superiority of the proposed method over current state-of-the-art methods in the challenging task of real driving-video dehazing. Project page.","sentences":["Real driving-video dehazing poses a significant challenge due to the inherent difficulty in acquiring precisely aligned hazy/clear video pairs for effective model training, especially in dynamic driving scenarios with unpredictable weather conditions.","In this paper, we propose a pioneering approach that addresses this challenge through a nonaligned regularization strategy.","Our core concept involves identifying clear frames that closely match hazy frames, serving as references to supervise a video dehazing network.","Our approach comprises two key components: reference matching and video dehazing.","Firstly, we introduce a non-aligned reference frame matching module, leveraging an adaptive sliding window to match high-quality reference frames from clear videos.","Video dehazing incorporates flow-guided cosine attention sampler and deformable cosine attention fusion modules to enhance spatial multiframe alignment and fuse their improved information.","To validate our approach, we collect a GoProHazy dataset captured effortlessly with GoPro cameras in diverse rural and urban road environments.","Extensive experiments demonstrate the superiority of the proposed method over current state-of-the-art methods in the challenging task of real driving-video dehazing.","Project page."],"url":"http://arxiv.org/abs/2405.09996v1","category":"cs.CV"}
{"created":"2024-05-16 11:01:09","title":"Zero-Shot Hierarchical Classification on the Common Procurement Vocabulary Taxonomy","abstract":"Classifying public tenders is a useful task for both companies that are invited to participate and for inspecting fraudulent activities. To facilitate the task for both participants and public administrations, the European Union presented a common taxonomy (\\textit{Common Procurement Vocabulary}, CPV) which is mandatory for tenders of certain importance; however, the contracts in which a CPV label is mandatory are the minority compared to all the Public Administrations activities. Classifying over a real-world taxonomy introduces some difficulties that can not be ignored. First of all, some fine-grained classes have an insufficient (if any) number of observations in the training set, while other classes are far more frequent (even thousands of times) than the average. To overcome those difficulties, we present a zero-shot approach, based on a pre-trained language model that relies only on label description and respects the label taxonomy. To train our proposed model, we used industrial data, which comes from \\url{contrattipubblici.org}, a service by \\href{https://spaziodati.eu}{SpazioDati s.r.l}. that collects public contracts stipulated in Italy in the last 25 years. Results show that the proposed model achieves better performance in classifying low-frequent classes compared to three different baselines, and is also able to predict never-seen classes.","sentences":["Classifying public tenders is a useful task for both companies that are invited to participate and for inspecting fraudulent activities.","To facilitate the task for both participants and public administrations, the European Union presented a common taxonomy (\\textit{Common Procurement Vocabulary}, CPV) which is mandatory for tenders of certain importance; however, the contracts in which a CPV label is mandatory are the minority compared to all the Public Administrations activities.","Classifying over a real-world taxonomy introduces some difficulties that can not be ignored.","First of all, some fine-grained classes have an insufficient (if any) number of observations in the training set, while other classes are far more frequent (even thousands of times) than the average.","To overcome those difficulties, we present a zero-shot approach, based on a pre-trained language model that relies only on label description and respects the label taxonomy.","To train our proposed model, we used industrial data, which comes from \\url{contrattipubblici.org}, a service by \\href{https://spaziodati.eu}{SpazioDati s.r.l}.","that collects public contracts stipulated in Italy in the last 25 years.","Results show that the proposed model achieves better performance in classifying low-frequent classes compared to three different baselines, and is also able to predict never-seen classes."],"url":"http://arxiv.org/abs/2405.09983v1","category":"cs.LG"}
{"created":"2024-05-16 10:39:05","title":"Tau and low multiplicity physics at Belle and Belle II","abstract":"The Belle and Belle II experiments have collected a 1.4 ab$^{-1}$ sample of $e^+e^-$ collision data at centre-of-mass energies near the $\\Upsilon (nS)$ resonances, of which 424 fb$^{-1}$ were collected at Belle II in Run1 (2019--2022). We present a measurement of the lepton-flavour universality between electrons and muons, the search for the lepton-flavour violation decay $\\tau^{-} \\to \\mu^{-}\\mu^{+}\\mu^{-}$, and a measurement of the $e^+e^- \\to \\pi^+\\pi^-\\pi^0$ cross section in the energy range from 0.62--3.5 GeV using an initial-state radiation technique, all performed with Belle II data.","sentences":["The Belle and Belle II experiments have collected a 1.4 ab$^{-1}$ sample of $e^+e^-$ collision data at centre-of-mass energies near the $\\Upsilon (nS)$ resonances, of which 424 fb$^{-1}$ were collected at Belle II in Run1 (2019--2022).","We present a measurement of the lepton-flavour universality between electrons and muons, the search for the lepton-flavour violation decay $\\tau^{-} \\to \\mu^{-}\\mu^{+}\\mu^{-}$, and a measurement of the $e^+e^- \\to \\pi^+\\pi^-\\pi^0$ cross section in the energy range from 0.62--3.5 GeV using an initial-state radiation technique, all performed with Belle II data."],"url":"http://arxiv.org/abs/2405.09974v1","category":"hep-ex"}
{"created":"2024-05-16 10:07:59","title":"A Unified Deep Transfer Learning Model for Accurate IoT Localization in Diverse Environments","abstract":"Internet of Things (IoT) is an ever-evolving technological paradigm that is reshaping industries and societies globally. Real-time data collection, analysis, and decision-making facilitated by localization solutions form the foundation for location-based services, enabling them to support critical functions within diverse IoT ecosystems. However, most existing works on localization focus on single environment, resulting in the development of multiple models to support multiple environments. In the context of smart cities, these raise costs and complexity due to the dynamicity of such environments. To address these challenges, this paper presents a unified indoor-outdoor localization solution that leverages transfer learning (TL) schemes to build a single deep learning model. The model accurately predicts the localization of IoT devices in diverse environments. The performance evaluation shows that by adopting an encoder-based TL scheme, we can improve the baseline model by about 17.18% in indoor environments and 9.79% in outdoor environments.","sentences":["Internet of Things (IoT) is an ever-evolving technological paradigm that is reshaping industries and societies globally.","Real-time data collection, analysis, and decision-making facilitated by localization solutions form the foundation for location-based services, enabling them to support critical functions within diverse IoT ecosystems.","However, most existing works on localization focus on single environment, resulting in the development of multiple models to support multiple environments.","In the context of smart cities, these raise costs and complexity due to the dynamicity of such environments.","To address these challenges, this paper presents a unified indoor-outdoor localization solution that leverages transfer learning (TL) schemes to build a single deep learning model.","The model accurately predicts the localization of IoT devices in diverse environments.","The performance evaluation shows that by adopting an encoder-based TL scheme, we can improve the baseline model by about 17.18% in indoor environments and 9.79% in outdoor environments."],"url":"http://arxiv.org/abs/2405.09960v1","category":"eess.SP"}
{"created":"2024-05-16 08:14:58","title":"Flow and Equation of State of nuclear matter at $\\mathbf{E_{\\mathrm{kin}}}$/A=0.25-1.5 GeV with the SMASH transport approach","abstract":"We present a comparison of directed and elliptic flow data by the FOPI collaboration in Au-Au, Xe-CsI, and Ni-Ni collisions at beam kinetic energies from 0.25 to 1.5 GeV per nucleon to simulations using the SMASH hadronic transport model. The Equation of State is parameterized as a function of nuclear density and momentum dependent potentials are newly introduced in SMASH. With a statistical analysis, we show that the collective flow data at lower energies is in best agreement with a soft momentum dependent potential, while the elliptic flow at higher energies requires a harder momentum dependent Equation of State.","sentences":["We present a comparison of directed and elliptic flow data by the FOPI collaboration in Au-Au, Xe-CsI, and Ni-Ni collisions at beam kinetic energies from 0.25 to 1.5 GeV per nucleon to simulations using the SMASH hadronic transport model.","The Equation of State is parameterized as a function of nuclear density and momentum dependent potentials are newly introduced in SMASH.","With a statistical analysis, we show that the collective flow data at lower energies is in best agreement with a soft momentum dependent potential, while the elliptic flow at higher energies requires a harder momentum dependent Equation of State."],"url":"http://arxiv.org/abs/2405.09889v1","category":"nucl-th"}
{"created":"2024-05-16 08:07:24","title":"Symmetry breaking and non-ergodicity in a driven-dissipative ensemble of multi-level atoms in a cavity","abstract":"Dissipative light-matter systems can display emergent collective behavior. Here, we report a $\\mathbb{Z}_2$-symmetry-breaking phase transition in a system of multi-level $^{87}$Rb atoms strongly coupled to a weakly driven two-mode optical cavity. In the symmetry-broken phase, non-ergodic dynamics manifests in the emergence of multiple stationary states with disjoint basins of attraction. This feature enables the amplification of a small atomic population imbalance into a characteristic macroscopic cavity transmission signal. Our experiment does not only showcase strongly dissipative atom-cavity systems as platforms for probing non-trivial collective many-body phenomena, but also highlights their potential for hosting technological applications in the context of sensing, density classification, and pattern retrieval dynamics within associative memories.","sentences":["Dissipative light-matter systems can display emergent collective behavior.","Here, we report a $\\mathbb{Z}_2$-symmetry-breaking phase transition in a system of multi-level $^{87}$Rb atoms strongly coupled to a weakly driven two-mode optical cavity.","In the symmetry-broken phase, non-ergodic dynamics manifests in the emergence of multiple stationary states with disjoint basins of attraction.","This feature enables the amplification of a small atomic population imbalance into a characteristic macroscopic cavity transmission signal.","Our experiment does not only showcase strongly dissipative atom-cavity systems as platforms for probing non-trivial collective many-body phenomena, but also highlights their potential for hosting technological applications in the context of sensing, density classification, and pattern retrieval dynamics within associative memories."],"url":"http://arxiv.org/abs/2405.09885v1","category":"quant-ph"}
{"created":"2024-05-16 08:06:52","title":"RoScenes: A Large-scale Multi-view 3D Dataset for Roadside Perception","abstract":"We introduce RoScenes, the largest multi-view roadside perception dataset, which aims to shed light on the development of vision-centric Bird's Eye View (BEV) approaches for more challenging traffic scenes. The highlights of RoScenes include significantly large perception area, full scene coverage and crowded traffic. More specifically, our dataset achieves surprising 21.13M 3D annotations within 64,000 $m^2$. To relieve the expensive costs of roadside 3D labeling, we present a novel BEV-to-3D joint annotation pipeline to efficiently collect such a large volume of data. After that, we organize a comprehensive study for current BEV methods on RoScenes in terms of effectiveness and efficiency. Tested methods suffer from the vast perception area and variation of sensor layout across scenes, resulting in performance levels falling below expectations. To this end, we propose RoBEV that incorporates feature-guided position embedding for effective 2D-3D feature assignment. With its help, our method outperforms state-of-the-art by a large margin without extra computational overhead on validation set. Our dataset and devkit will be made available at \\url{https://github.com/xiaosu-zhu/RoScenes}.","sentences":["We introduce RoScenes, the largest multi-view roadside perception dataset, which aims to shed light on the development of vision-centric Bird's Eye View (BEV) approaches for more challenging traffic scenes.","The highlights of RoScenes include significantly large perception area, full scene coverage and crowded traffic.","More specifically, our dataset achieves surprising 21.13M 3D annotations within 64,000 $m^2$. To relieve the expensive costs of roadside 3D labeling, we present a novel BEV-to-3D joint annotation pipeline to efficiently collect such a large volume of data.","After that, we organize a comprehensive study for current BEV methods on RoScenes in terms of effectiveness and efficiency.","Tested methods suffer from the vast perception area and variation of sensor layout across scenes, resulting in performance levels falling below expectations.","To this end, we propose RoBEV that incorporates feature-guided position embedding for effective 2D-3D feature assignment.","With its help, our method outperforms state-of-the-art by a large margin without extra computational overhead on validation set.","Our dataset and devkit will be made available at \\url{https://github.com/xiaosu-zhu/RoScenes}."],"url":"http://arxiv.org/abs/2405.09883v1","category":"cs.CV"}
{"created":"2024-05-16 07:57:31","title":"Hyperplane Arrangements and Fixed Points in Iterated PWL Neural Networks","abstract":"We leverage the framework of hyperplane arrangements to analyze potential regions of (stable) fixed points. We provide an upper bound on the number of fixed points for multi-layer neural networks equipped with piecewise linear (PWL) activation functions with arbitrary many linear pieces. The theoretical optimality of the exponential growth in the number of layers of the latter bound is shown. Specifically, we also derive a sharper upper bound on the number of stable fixed points for one-hidden-layer networks with hard tanh activation.","sentences":["We leverage the framework of hyperplane arrangements to analyze potential regions of (stable) fixed points.","We provide an upper bound on the number of fixed points for multi-layer neural networks equipped with piecewise linear (PWL) activation functions with arbitrary many linear pieces.","The theoretical optimality of the exponential growth in the number of layers of the latter bound is shown.","Specifically, we also derive a sharper upper bound on the number of stable fixed points for one-hidden-layer networks with hard tanh activation."],"url":"http://arxiv.org/abs/2405.09878v1","category":"cs.LG"}
{"created":"2024-05-16 07:53:07","title":"Risk Management for Medical Devices via the Riskman Ontology & Shapes","abstract":"We introduce the Riskman ontology & shapes for representing and analysing information about risk management for medical devices. Risk management is concerned with taking necessary precautions so a medical device does not cause harms for users or the environment. To date, risk management documentation is submitted to notified bodies (for certification) in the form of semi-structured natural language text. We propose to use classes from the Riskman ontology to logically model risk management documentation and to use the included SHACL constraints to check for syntactic completeness and conformity to relevant standards. In particular, the ontology is modelled after ISO 14971 and the recently published VDE Spec 90025. Our proposed methodology has the potential to save many person-hours for both manufacturers (when creating risk management documentation) as well as notified bodies (when assessing submitted applications for certification), and thus offers considerable benefits for healthcare and, by extension, society as a whole.","sentences":["We introduce the Riskman ontology & shapes for representing and analysing information about risk management for medical devices.","Risk management is concerned with taking necessary precautions so a medical device does not cause harms for users or the environment.","To date, risk management documentation is submitted to notified bodies (for certification) in the form of semi-structured natural language text.","We propose to use classes from the Riskman ontology to logically model risk management documentation and to use the included SHACL constraints to check for syntactic completeness and conformity to relevant standards.","In particular, the ontology is modelled after ISO 14971 and the recently published VDE Spec 90025.","Our proposed methodology has the potential to save many person-hours for both manufacturers (when creating risk management documentation) as well as notified bodies (when assessing submitted applications for certification), and thus offers considerable benefits for healthcare and, by extension, society as a whole."],"url":"http://arxiv.org/abs/2405.09875v1","category":"cs.AI"}
{"created":"2024-05-16 07:41:54","title":"Box-Free Model Watermarks Are Prone to Black-Box Removal Attacks","abstract":"Box-free model watermarking is an emerging technique to safeguard the intellectual property of deep learning models, particularly those for low-level image processing tasks. Existing works have verified and improved its effectiveness in several aspects. However, in this paper, we reveal that box-free model watermarking is prone to removal attacks, even under the real-world threat model such that the protected model and the watermark extractor are in black boxes. Under this setting, we carry out three studies. 1) We develop an extractor-gradient-guided (EGG) remover and show its effectiveness when the extractor uses ReLU activation only. 2) More generally, for an unknown extractor, we leverage adversarial attacks and design the EGG remover based on the estimated gradients. 3) Under the most stringent condition that the extractor is inaccessible, we design a transferable remover based on a set of private proxy models. In all cases, the proposed removers can successfully remove embedded watermarks while preserving the quality of the processed images, and we also demonstrate that the EGG remover can even replace the watermarks. Extensive experimental results verify the effectiveness and generalizability of the proposed attacks, revealing the vulnerabilities of the existing box-free methods and calling for further research.","sentences":["Box-free model watermarking is an emerging technique to safeguard the intellectual property of deep learning models, particularly those for low-level image processing tasks.","Existing works have verified and improved its effectiveness in several aspects.","However, in this paper, we reveal that box-free model watermarking is prone to removal attacks, even under the real-world threat model such that the protected model and the watermark extractor are in black boxes.","Under this setting, we carry out three studies.","1) We develop an extractor-gradient-guided (EGG) remover and show its effectiveness when the extractor uses ReLU activation only.","2) More generally, for an unknown extractor, we leverage adversarial attacks and design the EGG remover based on the estimated gradients.","3) Under the most stringent condition that the extractor is inaccessible, we design a transferable remover based on a set of private proxy models.","In all cases, the proposed removers can successfully remove embedded watermarks while preserving the quality of the processed images, and we also demonstrate that the EGG remover can even replace the watermarks.","Extensive experimental results verify the effectiveness and generalizability of the proposed attacks, revealing the vulnerabilities of the existing box-free methods and calling for further research."],"url":"http://arxiv.org/abs/2405.09863v1","category":"cs.CV"}
{"created":"2024-05-16 07:25:10","title":"IGOT: Information Gain Optimized Tokenizer on Domain Adaptive Pretraining","abstract":"Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have demonstrated strong capabilities in various fields of natural language generation. However, there are still many problems when using LLM in specialized domain-specific fields. When using generative AI to process downstream tasks, a common approach is to add new knowledge (e.g., private domain knowledge, cutting-edge information) to a pretrained model through continued training or fine-tuning. However, whether there is a universal paradigm for domain adaptation training is still an open question. In this article, we proposed Information Gain Optimized Tokenizer (IGOT), which analyzes the special token set of downstream tasks, constructs a new subset using heuristic function $\\phi$ with the special token and its information gain, to build new domain-specific tokenizer, and continues pretraining on the downstream task data. We explored the many positive effects of this method's customized tokenizer on domain-adaptive pretraining and verified this method can perform better than the ordinary method of just collecting data and fine-tuning. Based on our experiment, the continued pretraining process of IGOT with LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and 5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even reach a 31.5\\% of training time saving, making porting general generative AI to specific domains more effective than before. In domain-specific tasks, supervised $IGOT_\\tau$ shows great performance on reducing both the convergence radius and convergence point during keep pretraining.","sentences":["Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have demonstrated strong capabilities in various fields of natural language generation.","However, there are still many problems when using LLM in specialized domain-specific fields.","When using generative AI to process downstream tasks, a common approach is to add new knowledge (e.g., private domain knowledge, cutting-edge information) to a pretrained model through continued training or fine-tuning.","However, whether there is a universal paradigm for domain adaptation training is still an open question.","In this article, we proposed Information Gain Optimized Tokenizer (IGOT), which analyzes the special token set of downstream tasks, constructs a new subset using heuristic function $\\phi$ with the special token and its information gain, to build new domain-specific tokenizer, and continues pretraining on the downstream task data.","We explored the many positive effects of this method's customized tokenizer on domain-adaptive pretraining and verified this method can perform better than the ordinary method of just collecting data and fine-tuning.","Based on our experiment, the continued pretraining process of IGOT with LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and 5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even reach a 31.5\\% of training time saving, making porting general generative AI to specific domains more effective than before.","In domain-specific tasks, supervised $IGOT_\\tau$ shows great performance on reducing both the convergence radius and convergence point during keep pretraining."],"url":"http://arxiv.org/abs/2405.09857v1","category":"cs.CL"}
{"created":"2024-05-16 06:55:11","title":"Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling","abstract":"Chain of thought (CoT) has proven useful for problems requiring complex reasoning. Many of these problems are both textual and multimodal. Given the inputs in different modalities, a model generates a rationale and then uses it to answer a question. Because of the hallucination issue, the generated soft negative rationales with high textual quality but illogical semantics do not always help improve answer accuracy. This study proposes a rationale generation method using soft negative sampling (SNSE-CoT) to mitigate hallucinations in multimodal CoT. Five methods were applied to generate soft negative samples that shared highly similar text but had different semantics from the original. Bidirectional margin loss (BML) was applied to introduce them into the traditional contrastive learning framework that involves only positive and negative samples. Extensive experiments on the ScienceQA dataset demonstrated the effectiveness of the proposed method. Code and data are released at https://github.com/zgMin/SNSE-CoT.","sentences":["Chain of thought (CoT) has proven useful for problems requiring complex reasoning.","Many of these problems are both textual and multimodal.","Given the inputs in different modalities, a model generates a rationale and then uses it to answer a question.","Because of the hallucination issue, the generated soft negative rationales with high textual quality but illogical semantics do not always help improve answer accuracy.","This study proposes a rationale generation method using soft negative sampling (SNSE-CoT) to mitigate hallucinations in multimodal CoT.","Five methods were applied to generate soft negative samples that shared highly similar text but had different semantics from the original.","Bidirectional margin loss (BML) was applied to introduce them into the traditional contrastive learning framework that involves only positive and negative samples.","Extensive experiments on the ScienceQA dataset demonstrated the effectiveness of the proposed method.","Code and data are released at https://github.com/zgMin/SNSE-CoT."],"url":"http://arxiv.org/abs/2405.09848v1","category":"cs.CL"}
{"created":"2024-05-16 06:43:56","title":"Organizational Selection of Innovation","abstract":"Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules -- including averaging agents' project scores as well as counting their approval votes -- especially when organizations have tight budgets and can select only a few project alternatives out of many.","sentences":["Budgetary constraints force organizations to pursue only a subset of possible innovation projects.","Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent.","This raises the question of how to most effectively aggregate their collective nous.","Our model of organizational portfolio selection provides some first answers.","We show that portfolio performance can vary widely.","Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them.","In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation.","In particular, letting agents rank projects often outperforms alternative aggregation rules -- including averaging agents' project scores as well as counting their approval votes -- especially when organizations have tight budgets and can select only a few project alternatives out of many."],"url":"http://arxiv.org/abs/2405.09843v1","category":"econ.TH"}
{"created":"2024-05-16 06:09:22","title":"Mixed-Integer Linear Optimization for Cardinality-Constrained Random Forests","abstract":"Random forests are among the most famous algorithms for solving classification problems, in particular for large-scale data sets. Considering a set of labeled points and several decision trees, the method takes the majority vote to classify a new given point. In some scenarios, however, labels are only accessible for a proper subset of the given points. Moreover, this subset can be non-representative, e.g., due to collection bias. Semi-supervised learning considers the setting of labeled and unlabeled data and often improves the reliability of the results. In addition, it can be possible to obtain additional information about class sizes from undisclosed sources. We propose a mixed-integer linear optimization model for computing a semi-supervised random forest that covers the setting of labeled and unlabeled data points as well as the overall number of points in each class for a binary classification. Since the solution time rapidly grows as the number of variables increases, we present some problem-tailored preprocessing techniques and an intuitive branching rule. Our numerical results show that our approach leads to a better accuracy and a better Matthews correlation coefficient for biased samples compared to random forests by majority vote, even if only few labeled points are available.","sentences":["Random forests are among the most famous algorithms for solving classification problems, in particular for large-scale data sets.","Considering a set of labeled points and several decision trees, the method takes the majority vote to classify a new given point.","In some scenarios, however, labels are only accessible for a proper subset of the given points.","Moreover, this subset can be non-representative, e.g., due to collection bias.","Semi-supervised learning considers the setting of labeled and unlabeled data and often improves the reliability of the results.","In addition, it can be possible to obtain additional information about class sizes from undisclosed sources.","We propose a mixed-integer linear optimization model for computing a semi-supervised random forest that covers the setting of labeled and unlabeled data points as well as the overall number of points in each class for a binary classification.","Since the solution time rapidly grows as the number of variables increases, we present some problem-tailored preprocessing techniques and an intuitive branching rule.","Our numerical results show that our approach leads to a better accuracy and a better Matthews correlation coefficient for biased samples compared to random forests by majority vote, even if only few labeled points are available."],"url":"http://arxiv.org/abs/2405.09832v1","category":"math.OC"}
{"created":"2024-05-16 05:37:50","title":"Evaluating Algorithmic Bias in Models for Predicting Academic Performance of Filipino Students","abstract":"Algorithmic bias is a major issue in machine learning models in educational contexts. However, it has not yet been studied thoroughly in Asian learning contexts, and only limited work has considered algorithmic bias based on regional (sub-national) background. As a step towards addressing this gap, this paper examines the population of 5,986 students at a large university in the Philippines, investigating algorithmic bias based on students' regional background. The university used the Canvas learning management system (LMS) in its online courses across a broad range of domains. Over the period of three semesters, we collected 48.7 million log records of the students' activity in Canvas. We used these logs to train binary classification models that predict student grades from the LMS activity. The best-performing model reached AUC of 0.75 and weighted F1-score of 0.79. Subsequently, we examined the data for bias based on students' region. Evaluation using three metrics: AUC, weighted F1-score, and MADD showed consistent results across all demographic groups. Thus, no unfairness was observed against a particular student group in the grade predictions.","sentences":["Algorithmic bias is a major issue in machine learning models in educational contexts.","However, it has not yet been studied thoroughly in Asian learning contexts, and only limited work has considered algorithmic bias based on regional (sub-national) background.","As a step towards addressing this gap, this paper examines the population of 5,986 students at a large university in the Philippines, investigating algorithmic bias based on students' regional background.","The university used the Canvas learning management system (LMS) in its online courses across a broad range of domains.","Over the period of three semesters, we collected 48.7 million log records of the students' activity in Canvas.","We used these logs to train binary classification models that predict student grades from the LMS activity.","The best-performing model reached AUC of 0.75 and weighted F1-score of 0.79.","Subsequently, we examined the data for bias based on students' region.","Evaluation using three metrics: AUC, weighted F1-score, and MADD showed consistent results across all demographic groups.","Thus, no unfairness was observed against a particular student group in the grade predictions."],"url":"http://arxiv.org/abs/2405.09821v1","category":"cs.LG"}
{"created":"2024-05-16 05:09:01","title":"Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis","abstract":"In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin.","sentences":["In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence.","Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion.","The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics.","To address this challenge, we develop a generative retrieval framework based on a large language model.","This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech.","To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements.","We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech.","Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation.","Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples.","User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin."],"url":"http://arxiv.org/abs/2405.09814v1","category":"cs.GR"}
{"created":"2024-05-16 04:43:23","title":"Trajecctory-Based Individualized Treatment Rules","abstract":"A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.","sentences":["A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics.","Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time.","Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data.","Information available from the longitudinal nature of the data can be especially useful in mental health studies.","Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects.","This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications.","Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure.","The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules.","This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes.","This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches.","The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information."],"url":"http://arxiv.org/abs/2405.09810v1","category":"stat.ME"}
{"created":"2024-05-16 04:28:44","title":"MediSyn: Text-Guided Diffusion Models for Broad Medical 2D and 3D Image Synthesis","abstract":"Diffusion models have recently gained significant traction due to their ability to generate high-fidelity and diverse images and videos conditioned on text prompts. In medicine, this application promises to address the critical challenge of data scarcity, a consequence of barriers in data sharing, stringent patient privacy regulations, and disparities in patient population and demographics. By generating realistic and varying medical 2D and 3D images, these models offer a rich, privacy-respecting resource for algorithmic training and research. To this end, we introduce MediSyn, a pair of instruction-tuned text-guided latent diffusion models with the ability to generate high-fidelity and diverse medical 2D and 3D images across specialties and modalities. Through established metrics, we show significant improvement in broad medical image and video synthesis guided by text prompts.","sentences":["Diffusion models have recently gained significant traction due to their ability to generate high-fidelity and diverse images and videos conditioned on text prompts.","In medicine, this application promises to address the critical challenge of data scarcity, a consequence of barriers in data sharing, stringent patient privacy regulations, and disparities in patient population and demographics.","By generating realistic and varying medical 2D and 3D images, these models offer a rich, privacy-respecting resource for algorithmic training and research.","To this end, we introduce MediSyn, a pair of instruction-tuned text-guided latent diffusion models with the ability to generate high-fidelity and diverse medical 2D and 3D images across specialties and modalities.","Through established metrics, we show significant improvement in broad medical image and video synthesis guided by text prompts."],"url":"http://arxiv.org/abs/2405.09806v1","category":"cs.CV"}
{"created":"2024-05-16 04:25:53","title":"SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data","abstract":"Traditional security mechanisms isolate resources from users who should not access them. We reflect the compositional nature of such security mechanisms back into the structure of LLMs to build a provably secure LLM; that we term SecureLLM. Other approaches to LLM safety attempt to protect against bad actors or bad outcomes, but can only do so to an extent making them inappropriate for sensitive data. SecureLLM blends access security with fine-tuning methods. Each data silo has associated with it a separate fine-tuning and a user has access only to the collection of fine-tunings that they have permission for. The model must then perform on compositional tasks at the intersection of those data silos with the combination of those individual fine-tunings. While applicable to any task like document QA or making API calls, in this work we concern ourselves with models that learn the layouts of new SQL databases to provide natural-language-to-SQL translation capabilities. Existing fine-tuning composition methods fail in this challenging environment, as they are not well-equipped for handling compositional tasks. Compositionality remains a challenge for LLMs. We contribute both a difficult new compositional natural-language-to-SQL translation task and a new perspective on LLM security that allows models to be deployed to secure environments today.","sentences":["Traditional security mechanisms isolate resources from users who should not access them.","We reflect the compositional nature of such security mechanisms back into the structure of LLMs to build a provably secure LLM; that we term SecureLLM.","Other approaches to LLM safety attempt to protect against bad actors or bad outcomes, but can only do so to an extent making them inappropriate for sensitive data.","SecureLLM blends access security with fine-tuning methods.","Each data silo has associated with it a separate fine-tuning and a user has access only to the collection of fine-tunings that they have permission for.","The model must then perform on compositional tasks at the intersection of those data silos with the combination of those individual fine-tunings.","While applicable to any task like document QA or making API calls, in this work we concern ourselves with models that learn the layouts of new SQL databases to provide natural-language-to-SQL translation capabilities.","Existing fine-tuning composition methods fail in this challenging environment, as they are not well-equipped for handling compositional tasks.","Compositionality remains a challenge for LLMs.","We contribute both a difficult new compositional natural-language-to-SQL translation task and a new perspective on LLM security that allows models to be deployed to secure environments today."],"url":"http://arxiv.org/abs/2405.09805v1","category":"cs.CL"}
{"created":"2024-05-16 04:21:09","title":"Analysis and Predictive Modeling of Solar Coronal Holes Using Computer Vision and LSTM Networks","abstract":"In the era of space exploration, coronal holes on the sun play a significant role due to their impact on satellites and aircraft through their open magnetic fields and increased solar wind emissions. This study employs computer vision techniques to detect coronal hole regions and estimate their sizes using imagery from the Solar Dynamics Observatory (SDO). Additionally, we utilize deep learning methods, specifically Long Short-Term Memory (LSTM) networks, to analyze trends in the area of coronal holes and predict their areas across various solar regions over a span of seven days. By examining time series data, we aim to identify patterns in coronal hole behavior and understand their potential effects on space weather. This research enhances our ability to anticipate and prepare for space weather events that could affect Earth's technological systems.","sentences":["In the era of space exploration, coronal holes on the sun play a significant role due to their impact on satellites and aircraft through their open magnetic fields and increased solar wind emissions.","This study employs computer vision techniques to detect coronal hole regions and estimate their sizes using imagery from the Solar Dynamics Observatory (SDO).","Additionally, we utilize deep learning methods, specifically Long Short-Term Memory (LSTM) networks, to analyze trends in the area of coronal holes and predict their areas across various solar regions over a span of seven days.","By examining time series data, we aim to identify patterns in coronal hole behavior and understand their potential effects on space weather.","This research enhances our ability to anticipate and prepare for space weather events that could affect Earth's technological systems."],"url":"http://arxiv.org/abs/2405.09802v1","category":"astro-ph.SR"}
{"created":"2024-05-16 04:02:43","title":"Many-Shot In-Context Learning in Multimodal Foundation Models","abstract":"Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to almost 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, we also explore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .","sentences":["Large language models are well-known to be effective at few-shot in-context learning (ICL).","Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples.","In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL.","We benchmark GPT-4o and Gemini 1.5 Pro across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification).","We observe that many-shot ICL, including up to almost 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples)","ICL across all of the datasets.","Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets.","Given the high inference costs associated with the long prompts required for many-shot ICL, we also explore the impact of batching multiple queries in a single API call.","We show that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency.","Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples.","We find that while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets.","Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains.","Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL ."],"url":"http://arxiv.org/abs/2405.09798v1","category":"cs.LG"}
{"created":"2024-05-16 03:52:00","title":"Human-AI Safety: A Descendant of Generative AI and Control Systems Safety","abstract":"Generative artificial intelligence (AI) is interacting with people at an unprecedented scale, offering new avenues for immense positive impact, but also raising widespread concerns around the potential for individual and societal harm. Today, the predominant paradigm for human-AI safety focuses on fine-tuning the generative model's outputs to better agree with human-provided examples or feedback. In reality, however, the consequences of an AI model's outputs cannot be determined in an isolated context: they are tightly entangled with the responses and behavior of human users over time. In this position paper, we argue that meaningful safety assurances for these AI technologies can only be achieved by reasoning about how the feedback loop formed by the AI's outputs and human behavior may drive the interaction towards different outcomes. To this end, we envision a high-value window of opportunity to bridge the rapidly growing capabilities of generative AI and the dynamical safety frameworks from control theory, laying a new foundation for human-centered AI safety in the coming decades.","sentences":["Generative artificial intelligence (AI) is interacting with people at an unprecedented scale, offering new avenues for immense positive impact, but also raising widespread concerns around the potential for individual and societal harm.","Today, the predominant paradigm for human-AI safety focuses on fine-tuning the generative model's outputs to better agree with human-provided examples or feedback.","In reality, however, the consequences of an AI model's outputs cannot be determined in an isolated context: they are tightly entangled with the responses and behavior of human users over time.","In this position paper, we argue that meaningful safety assurances for these AI technologies can only be achieved by reasoning about how the feedback loop formed by the AI's outputs and human behavior may drive the interaction towards different outcomes.","To this end, we envision a high-value window of opportunity to bridge the rapidly growing capabilities of generative AI and the dynamical safety frameworks from control theory, laying a new foundation for human-centered AI safety in the coming decades."],"url":"http://arxiv.org/abs/2405.09794v1","category":"cs.AI"}
{"created":"2024-05-16 03:24:05","title":"Synthesizing Proteins on the Graphics Card. Protein Folding and the Limits of Critical AI Studies","abstract":"This paper investigates the application of the transformer architecture in protein folding, as exemplified by DeepMind's AlphaFold project, and its implications for the understanding of large language models as models of language. The prevailing discourse often assumes a ready-made analogy between proteins -- encoded as sequences of amino acids -- and natural language -- encoded as sequences of discrete symbols. Instead of assuming as given the linguistic structure of proteins, we critically evaluate this analogy to assess the kind of knowledge-making afforded by the transformer architecture. We first trace the analogy's emergence and historical development, carving out the influence of structural linguistics on structural biology beginning in the mid-20th century. We then examine three often overlooked pre-processing steps essential to the transformer architecture, including subword tokenization, word embedding, and positional encoding, to demonstrate its regime of representation based on continuous, high-dimensional vector spaces, which departs from the discrete, semantically demarcated symbols of language. The successful deployment of transformers in protein folding, we argue, discloses what we consider a non-linguistic approach to token processing intrinsic to the architecture. We contend that through this non-linguistic processing, the transformer architecture carves out unique epistemological territory and produces a new class of knowledge, distinct from established domains. We contend that our search for intelligent machines has to begin with the shape, rather than the place, of intelligence. Consequently, the emerging field of critical AI studies should take methodological inspiration from the history of science in its quest to conceptualize the contributions of artificial intelligence to knowledge-making, within and beyond the domain-specific sciences.","sentences":["This paper investigates the application of the transformer architecture in protein folding, as exemplified by DeepMind's AlphaFold project, and its implications for the understanding of large language models as models of language.","The prevailing discourse often assumes a ready-made analogy between proteins -- encoded as sequences of amino acids -- and natural language -- encoded as sequences of discrete symbols.","Instead of assuming as given the linguistic structure of proteins, we critically evaluate this analogy to assess the kind of knowledge-making afforded by the transformer architecture.","We first trace the analogy's emergence and historical development, carving out the influence of structural linguistics on structural biology beginning in the mid-20th century.","We then examine three often overlooked pre-processing steps essential to the transformer architecture, including subword tokenization, word embedding, and positional encoding, to demonstrate its regime of representation based on continuous, high-dimensional vector spaces, which departs from the discrete, semantically demarcated symbols of language.","The successful deployment of transformers in protein folding, we argue, discloses what we consider a non-linguistic approach to token processing intrinsic to the architecture.","We contend that through this non-linguistic processing, the transformer architecture carves out unique epistemological territory and produces a new class of knowledge, distinct from established domains.","We contend that our search for intelligent machines has to begin with the shape, rather than the place, of intelligence.","Consequently, the emerging field of critical AI studies should take methodological inspiration from the history of science in its quest to conceptualize the contributions of artificial intelligence to knowledge-making, within and beyond the domain-specific sciences."],"url":"http://arxiv.org/abs/2405.09788v1","category":"cs.CY"}
{"created":"2024-05-16 03:04:33","title":"Online bipartite matching with imperfect advice","abstract":"We study the problem of online unweighted bipartite matching with $n$ offline vertices and $n$ online vertices where one wishes to be competitive against the optimal offline algorithm. While the classic RANKING algorithm of Karp et al. [1990] provably attains competitive ratio of $1-1/e > 1/2$, we show that no learning-augmented method can be both 1-consistent and strictly better than $1/2$-robust under the adversarial arrival model. Meanwhile, under the random arrival model, we show how one can utilize methods from distribution testing to design an algorithm that takes in external advice about the online vertices and provably achieves competitive ratio interpolating between any ratio attainable by advice-free methods and the optimal ratio of 1, depending on the advice quality.","sentences":["We study the problem of online unweighted bipartite matching with $n$ offline vertices and $n$ online vertices where one wishes to be competitive against the optimal offline algorithm.","While the classic RANKING algorithm of Karp et al.","[1990] provably attains competitive ratio of $1-1/e > 1/2$, we show that no learning-augmented method can be both 1-consistent and strictly better than $1/2$-robust under the adversarial arrival model.","Meanwhile, under the random arrival model, we show how one can utilize methods from distribution testing to design an algorithm that takes in external advice about the online vertices and provably achieves competitive ratio interpolating between any ratio attainable by advice-free methods and the optimal ratio of 1, depending on the advice quality."],"url":"http://arxiv.org/abs/2405.09784v1","category":"cs.LG"}
{"created":"2024-05-16 03:04:10","title":"LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery","abstract":"Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities. However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery. Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis. Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations. We introduce Scientific Generative Agent (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters. We conduct extensive experiments to demonstrate our framework's efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis.","sentences":["Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities.","However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery.","Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis.","Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations.","We introduce Scientific Generative Agent (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters.","We conduct extensive experiments to demonstrate our framework's efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis."],"url":"http://arxiv.org/abs/2405.09783v1","category":"cs.LG"}
{"created":"2024-05-16 03:00:41","title":"An Independent Implementation of Quantum Machine Learning Algorithms in Qiskit for Genomic Data","abstract":"In this paper, we explore the power of Quantum Machine Learning as we extend, implement and evaluate algorithms like Quantum Support Vector Classifier (QSVC), Pegasos-QSVC, Variational Quantum Circuits (VQC), and Quantum Neural Networks (QNN) in Qiskit with diverse feature mapping techniques for genomic sequence classification.","sentences":["In this paper, we explore the power of Quantum Machine Learning as we extend, implement and evaluate algorithms like Quantum Support Vector Classifier (QSVC), Pegasos-QSVC, Variational Quantum Circuits (VQC), and Quantum Neural Networks (QNN) in Qiskit with diverse feature mapping techniques for genomic sequence classification."],"url":"http://arxiv.org/abs/2405.09781v1","category":"cs.LG"}
{"created":"2024-05-16 03:00:08","title":"EFEAR-4D: Ego-Velocity Filtering for Efficient and Accurate 4D radar Odometry","abstract":"Odometry is a crucial component for successfully implementing autonomous navigation, relying on sensors such as cameras, LiDARs and IMUs. However, these sensors may encounter challenges in extreme weather conditions, such as snowfall and fog. The emergence of FMCW radar technology offers the potential for robust perception in adverse conditions. As the latest generation of FWCW radars, the 4D mmWave radar provides point cloud with range, azimuth, elevation, and Doppler velocity information, despite inherent sparsity and noises in the point cloud. In this paper, we propose EFEAR-4D, an accurate, highly efficient, and learning-free method for large-scale 4D radar odometry estimation. EFEAR-4D exploits Doppler velocity information delicately for robust ego-velocity estimation, resulting in a highly accurate prior guess. EFEAR-4D maintains robustness against point-cloud sparsity and noises across diverse environments through dynamic object removal and effective region-wise feature extraction. Extensive experiments on two publicly available 4D radar datasets demonstrate state-of-the-art reliability and localization accuracy of EFEAR-4D under various conditions. Furthermore, we have collected a dataset following the same route but varying installation heights of the 4D radar, emphasizing the significant impact of radar height on point cloud quality - a crucial consideration for real-world deployments. Our algorithm and dataset will be available soon at https://github.com/CLASS-Lab/EFEAR-4D.","sentences":["Odometry is a crucial component for successfully implementing autonomous navigation, relying on sensors such as cameras, LiDARs and IMUs.","However, these sensors may encounter challenges in extreme weather conditions, such as snowfall and fog.","The emergence of FMCW radar technology offers the potential for robust perception in adverse conditions.","As the latest generation of FWCW radars, the 4D mmWave radar provides point cloud with range, azimuth, elevation, and Doppler velocity information, despite inherent sparsity and noises in the point cloud.","In this paper, we propose EFEAR-4D, an accurate, highly efficient, and learning-free method for large-scale 4D radar odometry estimation.","EFEAR-4D exploits Doppler velocity information delicately for robust ego-velocity estimation, resulting in a highly accurate prior guess.","EFEAR-4D maintains robustness against point-cloud sparsity and noises across diverse environments through dynamic object removal and effective region-wise feature extraction.","Extensive experiments on two publicly available 4D radar datasets demonstrate state-of-the-art reliability and localization accuracy of EFEAR-4D under various conditions.","Furthermore, we have collected a dataset following the same route but varying installation heights of the 4D radar, emphasizing the significant impact of radar height on point cloud quality - a crucial consideration for real-world deployments.","Our algorithm and dataset will be available soon at https://github.com/CLASS-Lab/EFEAR-4D."],"url":"http://arxiv.org/abs/2405.09780v1","category":"cs.RO"}
{"created":"2024-05-16 02:21:13","title":"Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3)","abstract":"With the rapid development of natural language processing (NLP) technology, large-scale pre-trained language models such as GPT-3 have become a popular research object in NLP field. This paper aims to explore sentiment analysis optimization techniques based on large pre-trained language models such as GPT-3 to improve model performance and effect and further promote the development of natural language processing (NLP). By introducing the importance of sentiment analysis and the limitations of traditional methods, GPT-3 and Fine-tuning techniques are introduced in this paper, and their applications in sentiment analysis are explained in detail. The experimental results show that the Fine-tuning technique can optimize GPT-3 model and obtain good performance in sentiment analysis task. This study provides an important reference for future sentiment analysis using large-scale language models.","sentences":["With the rapid development of natural language processing (NLP) technology, large-scale pre-trained language models such as GPT-3 have become a popular research object in NLP field.","This paper aims to explore sentiment analysis optimization techniques based on large pre-trained language models such as GPT-3 to improve model performance and effect and further promote the development of natural language processing (NLP).","By introducing the importance of sentiment analysis and the limitations of traditional methods, GPT-3 and Fine-tuning techniques are introduced in this paper, and their applications in sentiment analysis are explained in detail.","The experimental results show that the Fine-tuning technique can optimize GPT-3 model and obtain good performance in sentiment analysis task.","This study provides an important reference for future sentiment analysis using large-scale language models."],"url":"http://arxiv.org/abs/2405.09770v1","category":"cs.CL"}
{"created":"2024-05-16 02:18:41","title":"Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model","abstract":"Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.","sentences":["Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis.","These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware.","While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination.","As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations.","In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests.","We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour.","Our results highlight the model's strength in generating varied prosody and spontaneous outputs.","It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS.","However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS.","Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness.","Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis."],"url":"http://arxiv.org/abs/2405.09768v1","category":"eess.AS"}
{"created":"2024-05-16 02:11:03","title":"Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space","abstract":"We present HyperSum, an extractive summarization framework that captures both the efficiency of traditional lexical summarization and the accuracy of contemporary neural approaches. HyperSum exploits the pseudo-orthogonality that emerges when randomly initializing vectors at extremely high dimensions (\"blessing of dimensionality\") to construct representative and efficient sentence embeddings. Simply clustering the obtained embeddings and extracting their medoids yields competitive summaries. HyperSum often outperforms state-of-the-art summarizers -- in terms of both summary accuracy and faithfulness -- while being 10 to 100 times faster. We open-source HyperSum as a strong baseline for unsupervised extractive summarization.","sentences":["We present HyperSum, an extractive summarization framework that captures both the efficiency of traditional lexical summarization and the accuracy of contemporary neural approaches.","HyperSum exploits the pseudo-orthogonality that emerges when randomly initializing vectors at extremely high dimensions (\"blessing of dimensionality\") to construct representative and efficient sentence embeddings.","Simply clustering the obtained embeddings and extracting their medoids yields competitive summaries.","HyperSum often outperforms state-of-the-art summarizers -- in terms of both summary accuracy and faithfulness -- while being 10 to 100 times faster.","We open-source HyperSum as a strong baseline for unsupervised extractive summarization."],"url":"http://arxiv.org/abs/2405.09765v1","category":"cs.CL"}
{"created":"2024-05-16 02:10:30","title":"Fusion Intelligence: Confluence of Natural and Artificial Intelligence for Enhanced Problem-Solving Efficiency","abstract":"This paper introduces Fusion Intelligence (FI), a bio-inspired intelligent system, where the innate sensing, intelligence and unique actuation abilities of biological organisms such as bees and ants are integrated with the computational power of Artificial Intelligence (AI). This interdisciplinary field seeks to create systems that are not only smart but also adaptive and responsive in ways that mimic the nature. As FI evolves, it holds the promise of revolutionizing the way we approach complex problems, leveraging the best of both biological and digital worlds to create solutions that are more effective, sustainable, and harmonious with the environment. We demonstrate FI's potential to enhance agricultural IoT system performance through a simulated case study on improving insect pollination efficacy (entomophily).","sentences":["This paper introduces Fusion Intelligence (FI), a bio-inspired intelligent system, where the innate sensing, intelligence and unique actuation abilities of biological organisms such as bees and ants are integrated with the computational power of Artificial Intelligence (AI).","This interdisciplinary field seeks to create systems that are not only smart but also adaptive and responsive in ways that mimic the nature.","As FI evolves, it holds the promise of revolutionizing the way we approach complex problems, leveraging the best of both biological and digital worlds to create solutions that are more effective, sustainable, and harmonious with the environment.","We demonstrate FI's potential to enhance agricultural IoT system performance through a simulated case study on improving insect pollination efficacy (entomophily)."],"url":"http://arxiv.org/abs/2405.09763v1","category":"cs.AI"}
{"created":"2024-05-16 01:26:38","title":"Stacked Intelligent Metasurfaces for Holographic MIMO Aided Cell-Free Networks","abstract":"Large-scale multiple-input and multiple-output (MIMO) systems are capable of achieving high date rate. However, given the high hardware cost and excessive power consumption of massive MIMO systems, as a remedy, intelligent metasurfaces have been designed for efficient holographic MIMO (HMIMO) systems. In this paper, we propose a HMIMO architecture based on stacked intelligent metasurfaces (SIM) for the uplink of cell-free systems, where the SIM is employed at the access points (APs) for improving the spectral- and energy-efficiency. Specifically, we conceive distributed beamforming for SIM-assisted cell-free networks, where both the SIM coefficients and the local receiver combiner vectors of each AP are optimized based on the local channel state information (CSI) for the local detection of each user equipment (UE) information. Afterward, the central processing unit (CPU) fuses the local detections gleaned from all APs to detect the aggregate multi-user signal. Specifically, to design the SIM coefficients and the combining vectors of the APs, a low-complexity layer-by-layer iterative optimization algorithm is proposed for maximizing the equivalent gain of the channel spanning from the UEs to the APs. At the CPU, the weight vector used for combining the local detections from all APs is designed based on the minimum mean square error (MMSE) criterion, where the hardware impairments (HWIs) are also taken into consideration based on their statistics. The simulation results show that the SIM-based HMIMO outperforms the conventional single-layer HMIMO in terms of the achievable rate. We demonstrate that both the HWI of the radio frequency (RF) chains at the APs and the UEs limit the achievable rate in the high signal-to-noise-ratio (SNR) region.","sentences":["Large-scale multiple-input and multiple-output (MIMO) systems are capable of achieving high date rate.","However, given the high hardware cost and excessive power consumption of massive MIMO systems, as a remedy, intelligent metasurfaces have been designed for efficient holographic MIMO (HMIMO) systems.","In this paper, we propose a HMIMO architecture based on stacked intelligent metasurfaces (SIM) for the uplink of cell-free systems, where the SIM is employed at the access points (APs) for improving the spectral- and energy-efficiency.","Specifically, we conceive distributed beamforming for SIM-assisted cell-free networks, where both the SIM coefficients and the local receiver combiner vectors of each AP are optimized based on the local channel state information (CSI) for the local detection of each user equipment (UE) information.","Afterward, the central processing unit (CPU) fuses the local detections gleaned from all APs to detect the aggregate multi-user signal.","Specifically, to design the SIM coefficients and the combining vectors of the APs, a low-complexity layer-by-layer iterative optimization algorithm is proposed for maximizing the equivalent gain of the channel spanning from the UEs to the APs.","At the CPU, the weight vector used for combining the local detections from all APs is designed based on the minimum mean square error (MMSE) criterion, where the hardware impairments (HWIs) are also taken into consideration based on their statistics.","The simulation results show that the SIM-based HMIMO outperforms the conventional single-layer HMIMO in terms of the achievable rate.","We demonstrate that both the HWI of the radio frequency (RF) chains at the APs and the UEs limit the achievable rate in the high signal-to-noise-ratio (SNR) region."],"url":"http://arxiv.org/abs/2405.09753v1","category":"cs.IT"}
{"created":"2024-05-16 01:02:09","title":"Many Hands Make Light Work: Task-Oriented Dialogue System with Module-Based Mixture-of-Experts","abstract":"Task-oriented dialogue systems are broadly used in virtual assistants and other automated services, providing interfaces between users and machines to facilitate specific tasks. Nowadays, task-oriented dialogue systems have greatly benefited from pre-trained language models (PLMs). However, their task-solving performance is constrained by the inherent capacities of PLMs, and scaling these models is expensive and complex as the model size becomes larger. To address these challenges, we propose Soft Mixture-of-Expert Task-Oriented Dialogue system (SMETOD) which leverages an ensemble of Mixture-of-Experts (MoEs) to excel at subproblems and generate specialized outputs for task-oriented dialogues. SMETOD also scales up a task-oriented dialogue system with simplicity and flexibility while maintaining inference efficiency. We extensively evaluate our model on three benchmark functionalities: intent prediction, dialogue state tracking, and dialogue response generation. Experimental results demonstrate that SMETOD achieves state-of-the-art performance on most evaluated metrics. Moreover, comparisons against existing strong baselines show that SMETOD has a great advantage in the cost of inference and correctness in problem-solving.","sentences":["Task-oriented dialogue systems are broadly used in virtual assistants and other automated services, providing interfaces between users and machines to facilitate specific tasks.","Nowadays, task-oriented dialogue systems have greatly benefited from pre-trained language models (PLMs).","However, their task-solving performance is constrained by the inherent capacities of PLMs, and scaling these models is expensive and complex as the model size becomes larger.","To address these challenges, we propose Soft Mixture-of-Expert Task-Oriented Dialogue system (SMETOD) which leverages an ensemble of Mixture-of-Experts (MoEs) to excel at subproblems and generate specialized outputs for task-oriented dialogues.","SMETOD also scales up a task-oriented dialogue system with simplicity and flexibility while maintaining inference efficiency.","We extensively evaluate our model on three benchmark functionalities: intent prediction, dialogue state tracking, and dialogue response generation.","Experimental results demonstrate that SMETOD achieves state-of-the-art performance on most evaluated metrics.","Moreover, comparisons against existing strong baselines show that SMETOD has a great advantage in the cost of inference and correctness in problem-solving."],"url":"http://arxiv.org/abs/2405.09744v1","category":"cs.CL"}
{"created":"2024-05-15 23:39:02","title":"Optimizing Curved EM Skins for Opportunistic Relaying in Vehicular Networks","abstract":"Electromagnetic skins (EMSs) are recognized for enhancing communication performance, spanning from coverage to capacity. While much of the scientific literature focuses on reconfigurable intelligent surfaces that dynamically adjust phase configurations over time, this study takes a different approach by considering low-cost static passive curved EMS (CEMS)s. These are pre-configured during manufacturing to conform to the shape of irregular surfaces, e.g., car doors, effectively transforming them into anomalous mirrors. This design allows vehicles to serve as opportunistic passive relays, mitigating blockage issues in vehicular networks. This paper delves into a novel design method for the phase profile of CEMS based on coarse a-priori distributions of incident and reflection angles onto the surface, influenced by vehicular traffic patterns. A penalty-based method is employed to optimize both the average spectral efficiency (SE) and average coverage probability, and it is compared against a lower-complexity and physically intuitive modular architecture, utilizing a codebook-based discrete optimization technique. Numerical results demonstrate that properly designed CEMS lead to a remarkable improvements in average SE and coverage probability, namely when the direct path is blocked.","sentences":["Electromagnetic skins (EMSs) are recognized for enhancing communication performance, spanning from coverage to capacity.","While much of the scientific literature focuses on reconfigurable intelligent surfaces that dynamically adjust phase configurations over time, this study takes a different approach by considering low-cost static passive curved EMS (CEMS)s.","These are pre-configured during manufacturing to conform to the shape of irregular surfaces, e.g., car doors, effectively transforming them into anomalous mirrors.","This design allows vehicles to serve as opportunistic passive relays, mitigating blockage issues in vehicular networks.","This paper delves into a novel design method for the phase profile of CEMS based on coarse a-priori distributions of incident and reflection angles onto the surface, influenced by vehicular traffic patterns.","A penalty-based method is employed to optimize both the average spectral efficiency (SE) and average coverage probability, and it is compared against a lower-complexity and physically intuitive modular architecture, utilizing a codebook-based discrete optimization technique.","Numerical results demonstrate that properly designed CEMS lead to a remarkable improvements in average SE and coverage probability, namely when the direct path is blocked."],"url":"http://arxiv.org/abs/2405.09730v1","category":"eess.SP"}
{"created":"2024-05-15 22:28:23","title":"Spectral Editing of Activations for Large Language Model Alignment","abstract":"Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as inference and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.","sentences":["Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content.","Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods.","We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated).","We also extend our method to non-linear editing using feature functions.","We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families.","The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as inference and data efficiency.","We also show that SEA editing only has a limited negative impact on other model capabilities."],"url":"http://arxiv.org/abs/2405.09719v1","category":"cs.CL"}
{"created":"2024-05-15 21:55:31","title":"SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge","abstract":"Learning commonsense reasoning from visual contexts and scenes in real-world is a crucial step toward advanced artificial intelligence. However, existing video reasoning benchmarks are still inadequate since they were mainly designed for factual or situated reasoning and rarely involve broader knowledge in the real world. Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge. We propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving. To create such a dataset, we propose an automatic and scalable generation method to generate question-answer pairs, knowledge graphs, and rationales by instructing the combinations of LLMs and MLLMs. Concretely, we first extract observable situated entities, relations, and processes from videos for situated knowledge and then extend to open-world knowledge beyond the visible content. The task generation is facilitated through multiple dialogues as iterations and subsequently corrected and refined by our designed self-promptings and demonstrations. With a corpus of both explicit situated facts and implicit commonsense, we generate associated question-answer pairs and reasoning processes, finally followed by manual reviews for quality assurance. We evaluated recent mainstream large vision-language models on the benchmark and found several insightful conclusions. For more information, please refer to our benchmark at www.bobbywu.com/SOKBench.","sentences":["Learning commonsense reasoning from visual contexts and scenes in real-world is a crucial step toward advanced artificial intelligence.","However, existing video reasoning benchmarks are still inadequate since they were mainly designed for factual or situated reasoning and rarely involve broader knowledge in the real world.","Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge.","We propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K situations with instance-level annotations depicted in the videos.","The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving.","To create such a dataset, we propose an automatic and scalable generation method to generate question-answer pairs, knowledge graphs, and rationales by instructing the combinations of LLMs and MLLMs.","Concretely, we first extract observable situated entities, relations, and processes from videos for situated knowledge and then extend to open-world knowledge beyond the visible content.","The task generation is facilitated through multiple dialogues as iterations and subsequently corrected and refined by our designed self-promptings and demonstrations.","With a corpus of both explicit situated facts and implicit commonsense, we generate associated question-answer pairs and reasoning processes, finally followed by manual reviews for quality assurance.","We evaluated recent mainstream large vision-language models on the benchmark and found several insightful conclusions.","For more information, please refer to our benchmark at www.bobbywu.com/SOKBench."],"url":"http://arxiv.org/abs/2405.09713v1","category":"cs.CV"}
{"created":"2024-05-15 21:53:54","title":"STAR: A Benchmark for Situated Reasoning in Real-World Videos","abstract":"Reasoning in the real world is not divorced from situations. How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence. This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR Benchmark). This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical. The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships). Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning. Questions and answers are procedurally generated. The answering logic of each question is represented by a functional program based on a situation hyper-graph. We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task. We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark.","sentences":["Reasoning in the real world is not divorced from situations.","How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence.","This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR Benchmark).","This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical.","The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility.","We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships).","Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning.","Questions and answers are procedurally generated.","The answering logic of each question is represented by a functional program based on a situation hyper-graph.","We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task.","We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark."],"url":"http://arxiv.org/abs/2405.09711v1","category":"cs.AI"}
{"created":"2024-05-15 21:28:55","title":"No More Mumbles: Enhancing Robot Intelligibility through Speech Adaptation","abstract":"Spoken language interaction is at the heart of interpersonal communication, and people flexibly adapt their speech to different individuals and environments. It is surprising that robots, and by extension other digital devices, are not equipped to adapt their speech and instead rely on fixed speech parameters, which often hinder comprehension by the user. We conducted a speech comprehension study involving 39 participants who were exposed to different environmental and contextual conditions. During the experiment, the robot articulated words using different vocal parameters, and the participants were tasked with both recognising the spoken words and rating their subjective impression of the robot's speech. The experiment's primary outcome shows that spaces with good acoustic quality positively correlate with intelligibility and user experience. However, increasing the distance between the user and the robot exacerbated the user experience, while distracting background sounds significantly reduced speech recognition accuracy and user satisfaction. We next built an adaptive voice for the robot. For this, the robot needs to know how difficult it is for a user to understand spoken language in a particular setting. We present a prediction model that rates how annoying the ambient acoustic environment is and, consequentially, how hard it is to understand someone in this setting. Then, we develop a convolutional neural network model to adapt the robot's speech parameters to different users and spaces, while taking into account the influence of ambient acoustics on intelligibility. Finally, we present an evaluation with 27 users, demonstrating superior intelligibility and user experience with adaptive voice parameters compared to fixed voice.","sentences":["Spoken language interaction is at the heart of interpersonal communication, and people flexibly adapt their speech to different individuals and environments.","It is surprising that robots, and by extension other digital devices, are not equipped to adapt their speech and instead rely on fixed speech parameters, which often hinder comprehension by the user.","We conducted a speech comprehension study involving 39 participants who were exposed to different environmental and contextual conditions.","During the experiment, the robot articulated words using different vocal parameters, and the participants were tasked with both recognising the spoken words and rating their subjective impression of the robot's speech.","The experiment's primary outcome shows that spaces with good acoustic quality positively correlate with intelligibility and user experience.","However, increasing the distance between the user and the robot exacerbated the user experience, while distracting background sounds significantly reduced speech recognition accuracy and user satisfaction.","We next built an adaptive voice for the robot.","For this, the robot needs to know how difficult it is for a user to understand spoken language in a particular setting.","We present a prediction model that rates how annoying the ambient acoustic environment is and, consequentially, how hard it is to understand someone in this setting.","Then, we develop a convolutional neural network model to adapt the robot's speech parameters to different users and spaces, while taking into account the influence of ambient acoustics on intelligibility.","Finally, we present an evaluation with 27 users, demonstrating superior intelligibility and user experience with adaptive voice parameters compared to fixed voice."],"url":"http://arxiv.org/abs/2405.09708v1","category":"cs.RO"}
{"created":"2024-05-15 21:06:23","title":"The Realization of a Gas Puff Imaging System on the Wendelstein 7-X Stellarator","abstract":"A system for studying the spatio-temporal dynamics of fluctuations in the boundary of the W7-X plasma using the Gas-Puff Imaging (GPI) technique has been designed, constructed, installed, and operated. This GPI system addresses a number of challenges specific to long-pulse superconducting devices like W7-X, including the long distance between the plasma and the vacuum vessel wall, the long distance between the plasma and diagnostic ports, the range of last closed flux surface locations for different magnetic configurations in W7-X, and management of heat loads on the system's plasma-facing components. The system features a pair of \"converging-diverging\" nozzles for partially collimating the gas puffed locally $\\approx$135 mm radially outboard of the plasma boundary, a pop-up turning mirror for viewing the gas puff emission from the side (also acting as a shutter for the re-entrant vacuum window), and a high-throughput optical system that collects visible emission resulting from the interaction between the puffed gas and the plasma and directs it along a water-cooled re-entrant tube directly onto the 8 x 16 pixel detector array of the fast camera. The DEGAS 2 neutrals code was used to simulate the H$_\\alpha$ (656 nm) and the HeI (587 nm) line emission expected from well-characterized gas-puffs of H$_2$ and He and excited within typical edge plasma profiles in W7-X, thereby predicting line brightnesses used to reduce the risks associated with system sensitivity and placement of the field of view. Operation of GPI on W7-X shows excellent signal to noise ratios (>100) over the field of view for minimally perturbing gas puffs. The GPI system provides detailed measurements of the 2-dimensional (radial and poloidal) dynamics of plasma fluctuations in the W7-X edge, scrape-off layer, and in and around the magnetic islands that make up the island divertor configuration employed on W7-X.","sentences":["A system for studying the spatio-temporal dynamics of fluctuations in the boundary of the W7-X plasma using the Gas-Puff Imaging (GPI) technique has been designed, constructed, installed, and operated.","This GPI system addresses a number of challenges specific to long-pulse superconducting devices like W7-X, including the long distance between the plasma and the vacuum vessel wall, the long distance between the plasma and diagnostic ports, the range of last closed flux surface locations for different magnetic configurations in W7-X, and management of heat loads on the system's plasma-facing components.","The system features a pair of \"converging-diverging\" nozzles for partially collimating the gas puffed locally $\\approx$135 mm radially outboard of the plasma boundary, a pop-up turning mirror for viewing the gas puff emission from the side (also acting as a shutter for the re-entrant vacuum window), and a high-throughput optical system that collects visible emission resulting from the interaction between the puffed gas and the plasma and directs it along a water-cooled re-entrant tube directly onto the 8 x 16 pixel detector array of the fast camera.","The DEGAS 2 neutrals code was used to simulate the H$_\\alpha$ (656 nm) and the HeI (587 nm) line emission expected from well-characterized gas-puffs of H$_2$ and He and excited within typical edge plasma profiles in W7-X, thereby predicting line brightnesses used to reduce the risks associated with system sensitivity and placement of the field of view.","Operation of GPI on W7-X shows excellent signal to noise ratios (>100) over the field of view for minimally perturbing gas puffs.","The GPI system provides detailed measurements of the 2-dimensional (radial and poloidal) dynamics of plasma fluctuations in the W7-X edge, scrape-off layer, and in and around the magnetic islands that make up the island divertor configuration employed on W7-X."],"url":"http://arxiv.org/abs/2405.09705v1","category":"physics.plasm-ph"}
{"created":"2024-05-15 20:41:46","title":"Modeling User Preferences via Brain-Computer Interfacing","abstract":"Present Brain-Computer Interfacing (BCI) technology allows inference and detection of cognitive and affective states, but fairly little has been done to study scenarios in which such information can facilitate new applications that rely on modeling human cognition. One state that can be quantified from various physiological signals is attention. Estimates of human attention can be used to reveal preferences and novel dimensions of user experience. Previous approaches have tackled these incredibly challenging tasks using a variety of behavioral signals, from dwell-time to click-through data, and computational models of visual correspondence to these behavioral signals. However, behavioral signals are only rough estimations of the real underlying attention and affective preferences of the users. Indeed, users may attend to some content simply because it is salient, but not because it is really interesting, or simply because it is outrageous. With this paper, we put forward a research agenda and example work using BCI to infer users' preferences, their attentional correlates towards visual content, and their associations with affective experience. Subsequently, we link these to relevant applications, such as information retrieval, personalized steering of generative models, and crowdsourcing population estimates of affective experiences.","sentences":["Present Brain-Computer Interfacing (BCI) technology allows inference and detection of cognitive and affective states, but fairly little has been done to study scenarios in which such information can facilitate new applications that rely on modeling human cognition.","One state that can be quantified from various physiological signals is attention.","Estimates of human attention can be used to reveal preferences and novel dimensions of user experience.","Previous approaches have tackled these incredibly challenging tasks using a variety of behavioral signals, from dwell-time to click-through data, and computational models of visual correspondence to these behavioral signals.","However, behavioral signals are only rough estimations of the real underlying attention and affective preferences of the users.","Indeed, users may attend to some content simply because it is salient, but not because it is really interesting, or simply because it is outrageous.","With this paper, we put forward a research agenda and example work using BCI to infer users' preferences, their attentional correlates towards visual content, and their associations with affective experience.","Subsequently, we link these to relevant applications, such as information retrieval, personalized steering of generative models, and crowdsourcing population estimates of affective experiences."],"url":"http://arxiv.org/abs/2405.09691v1","category":"cs.HC"}
{"created":"2024-05-15 20:37:48","title":"Generalized Holographic Reduced Representations","abstract":"Deep learning has achieved remarkable success in recent years. Central to its success is its ability to learn representations that preserve task-relevant structure. However, massive energy, compute, and data costs are required to learn general representations. This paper explores Hyperdimensional Computing (HDC), a computationally and data-efficient brain-inspired alternative. HDC acts as a bridge between connectionist and symbolic approaches to artificial intelligence (AI), allowing explicit specification of representational structure as in symbolic approaches while retaining the flexibility of connectionist approaches. However, HDC's simplicity poses challenges for encoding complex compositional structures, especially in its binding operation. To address this, we propose Generalized Holographic Reduced Representations (GHRR), an extension of Fourier Holographic Reduced Representations (FHRR), a specific HDC implementation. GHRR introduces a flexible, non-commutative binding operation, enabling improved encoding of complex data structures while preserving HDC's desirable properties of robustness and transparency. In this work, we introduce the GHRR framework, prove its theoretical properties and its adherence to HDC properties, explore its kernel and binding characteristics, and perform empirical experiments showcasing its flexible non-commutativity, enhanced decoding accuracy for compositional structures, and improved memorization capacity compared to FHRR.","sentences":["Deep learning has achieved remarkable success in recent years.","Central to its success is its ability to learn representations that preserve task-relevant structure.","However, massive energy, compute, and data costs are required to learn general representations.","This paper explores Hyperdimensional Computing (HDC), a computationally and data-efficient brain-inspired alternative.","HDC acts as a bridge between connectionist and symbolic approaches to artificial intelligence (AI), allowing explicit specification of representational structure as in symbolic approaches while retaining the flexibility of connectionist approaches.","However, HDC's simplicity poses challenges for encoding complex compositional structures, especially in its binding operation.","To address this, we propose Generalized Holographic Reduced Representations (GHRR), an extension of Fourier Holographic Reduced Representations (FHRR), a specific HDC implementation.","GHRR introduces a flexible, non-commutative binding operation, enabling improved encoding of complex data structures while preserving HDC's desirable properties of robustness and transparency.","In this work, we introduce the GHRR framework, prove its theoretical properties and its adherence to HDC properties, explore its kernel and binding characteristics, and perform empirical experiments showcasing its flexible non-commutativity, enhanced decoding accuracy for compositional structures, and improved memorization capacity compared to FHRR."],"url":"http://arxiv.org/abs/2405.09689v1","category":"cs.LG"}
{"created":"2024-05-15 20:27:56","title":"From Local to Global Order: A Theory of Neural Synaptic Balance","abstract":"We develop a theory of neural synaptic balance and how it can emerge or be enforced in neural networks. For a given additive cost function $R$ (regularizer), a neuron is said to be in balance if the total cost of its input weights is equal to the total cost of its output weights. The basic example is provided by feedforward networks of ReLU units trained with $L_2$ regularizers, which exhibit balance after proper training. The theory explains this phenomenon and extends it in several directions. The first direction is the extension to bilinear and other activation functions. The second direction is the extension to more general regularizers, including all $L_p$ ($p>0$) regularizers. The third direction is the extension to non-layered architectures, recurrent architectures, convolutional architectures, as well as architectures with mixed activation functions. The theory is based on two local neuronal operations: scaling which is commutative, and balancing which is not commutative. Finally, and most importantly, given any initial set of weights, when local balancing operations are applied to each neuron in a stochastic manner, global order always emerges through the convergence of the stochastic balancing algorithm to the same unique set of balanced weights. The reason for this convergence is the existence of an underlying strictly convex optimization problem where the relevant variables are constrained to a linear, only architecture-dependent, manifold. The theory is corroborated through various simulations carried out on benchmark data sets. Scaling and balancing operations are entirely local and thus physically plausible in biological and neuromorphic networks.","sentences":["We develop a theory of neural synaptic balance and how it can emerge or be enforced in neural networks.","For a given additive cost function $R$ (regularizer), a neuron is said to be in balance if the total cost of its input weights is equal to the total cost of its output weights.","The basic example is provided by feedforward networks of ReLU units trained with $L_2$ regularizers, which exhibit balance after proper training.","The theory explains this phenomenon and extends it in several directions.","The first direction is the extension to bilinear and other activation functions.","The second direction is the extension to more general regularizers, including all $L_p$ ($p>0$) regularizers.","The third direction is the extension to non-layered architectures, recurrent architectures, convolutional architectures, as well as architectures with mixed activation functions.","The theory is based on two local neuronal operations: scaling which is commutative, and balancing which is not commutative.","Finally, and most importantly, given any initial set of weights, when local balancing operations are applied to each neuron in a stochastic manner, global order always emerges through the convergence of the stochastic balancing algorithm to the same unique set of balanced weights.","The reason for this convergence is the existence of an underlying strictly convex optimization problem where the relevant variables are constrained to a linear, only architecture-dependent, manifold.","The theory is corroborated through various simulations carried out on benchmark data sets.","Scaling and balancing operations are entirely local and thus physically plausible in biological and neuromorphic networks."],"url":"http://arxiv.org/abs/2405.09688v1","category":"cs.NE"}
{"created":"2024-05-15 19:44:54","title":"Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation","abstract":"The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policy-makers are therefore tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency legislation of Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study on generative AI harms we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy on mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.","sentences":["The rapid advancement of AI technologies yields numerous future impacts on individuals and society.","Policy-makers are therefore tasked to react quickly and establish policies that mitigate those impacts.","However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI.","In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts.","We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts.","We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency legislation of Article 50 of the EU AI Act.","We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations.","We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security.","Through this case study on generative AI harms we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy on mitigating various negative impacts.","We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies."],"url":"http://arxiv.org/abs/2405.09679v1","category":"cs.CL"}
{"created":"2024-05-15 19:27:45","title":"LoRA Learns Less and Forgets Less","abstract":"Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\approx$100K prompt-response pairs) and continued pretraining ($\\approx$10B unstructured tokens) data regimes. Our results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model's performance on tasks outside the target domain. We show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. We show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.","sentences":["Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models.","LoRA saves memory by training only low rank perturbations to selected weight matrices.","In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics.","We consider both the instruction finetuning ($\\approx$100K prompt-response pairs) and continued pretraining ($\\approx$10B unstructured tokens) data regimes.","Our results show that, in most settings, LoRA substantially underperforms full finetuning.","Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model's performance on tasks outside the target domain.","We show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations.","We show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps.","We conclude by proposing best practices for finetuning with LoRA."],"url":"http://arxiv.org/abs/2405.09673v1","category":"cs.LG"}
{"created":"2024-05-15 18:48:57","title":"Detecting Continuous Integration Skip : A Reinforcement Learning-based Approach","abstract":"The software industry is experiencing a surge in the adoption of Continuous Integration (CI) practices, both in commercial and open-source environments. CI practices facilitate the seamless integration of code changes by employing automated building and testing processes. Some frameworks, such as Travis CI and GitHub Actions have significantly contributed to simplifying and enhancing the CI process, rendering it more accessible and efficient for development teams. Despite the availability these CI tools , developers continue to encounter difficulties in accurately flagging commits as either suitable for CI execution or as candidates for skipping especially for large projects with many dependencies. Inaccurate flagging of commits can lead to resource-intensive test and build processes, as even minor commits may inadvertently trigger the Continuous Integration process. The problem of detecting CI-skip commits, can be modeled as binary classification task where we decide to either build a commit or to skip it. This study proposes a novel solution that leverages Deep Reinforcement Learning techniques to construct an optimal Decision Tree classifier that addresses the imbalanced nature of the data. We evaluate our solution by running a within and a cross project validation benchmark on diverse range of Open-Source projects hosted on GitHub which showcased superior results when compared with existing state-of-the-art methods.","sentences":["The software industry is experiencing a surge in the adoption of Continuous Integration (CI) practices, both in commercial and open-source environments.","CI practices facilitate the seamless integration of code changes by employing automated building and testing processes.","Some frameworks, such as Travis CI and GitHub Actions have significantly contributed to simplifying and enhancing the CI process, rendering it more accessible and efficient for development teams.","Despite the availability these CI tools , developers continue to encounter difficulties in accurately flagging commits as either suitable for CI execution or as candidates for skipping especially for large projects with many dependencies.","Inaccurate flagging of commits can lead to resource-intensive test and build processes, as even minor commits may inadvertently trigger the Continuous Integration process.","The problem of detecting CI-skip commits, can be modeled as binary classification task where we decide to either build a commit or to skip it.","This study proposes a novel solution that leverages Deep Reinforcement Learning techniques to construct an optimal Decision Tree classifier that addresses the imbalanced nature of the data.","We evaluate our solution by running a within and a cross project validation benchmark on diverse range of Open-Source projects hosted on GitHub which showcased superior results when compared with existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.09657v1","category":"cs.SE"}
{"created":"2024-05-15 18:27:15","title":"Challenges and opportunities for digital twins in precision medicine: a complex systems perspective","abstract":"The adoption of digital twins (DTs) in precision medicine is increasingly viable, propelled by extensive data collection and advancements in artificial intelligence (AI), alongside traditional biomedical methodologies. However, the reliance on black-box predictive models, which utilize large datasets, presents limitations that could impede the broader application of DTs in clinical settings. We argue that hypothesis-driven generative models, particularly multiscale modeling, are essential for boosting the clinical accuracy and relevance of DTs, thereby making a significant impact on healthcare innovation. This paper explores the transformative potential of DTs in healthcare, emphasizing their capability to simulate complex, interdependent biological processes across multiple scales. By integrating generative models with extensive datasets, we propose a scenario-based modeling approach that enables the exploration of diverse therapeutic strategies, thus supporting dynamic clinical decision-making. This method not only leverages advancements in data science and big data for improving disease treatment and prevention but also incorporates insights from complex systems and network science, quantitative biology, and digital medicine, promising substantial advancements in patient care.","sentences":["The adoption of digital twins (DTs) in precision medicine is increasingly viable, propelled by extensive data collection and advancements in artificial intelligence (AI), alongside traditional biomedical methodologies.","However, the reliance on black-box predictive models, which utilize large datasets, presents limitations that could impede the broader application of DTs in clinical settings.","We argue that hypothesis-driven generative models, particularly multiscale modeling, are essential for boosting the clinical accuracy and relevance of DTs, thereby making a significant impact on healthcare innovation.","This paper explores the transformative potential of DTs in healthcare, emphasizing their capability to simulate complex, interdependent biological processes across multiple scales.","By integrating generative models with extensive datasets, we propose a scenario-based modeling approach that enables the exploration of diverse therapeutic strategies, thus supporting dynamic clinical decision-making.","This method not only leverages advancements in data science and big data for improving disease treatment and prevention but also incorporates insights from complex systems and network science, quantitative biology, and digital medicine, promising substantial advancements in patient care."],"url":"http://arxiv.org/abs/2405.09649v1","category":"physics.bio-ph"}
{"created":"2024-05-15 18:26:55","title":"BSQ Conserved Charges in Relativistic Viscous Hydrodynamics solved with Smoothed Particle Hydrodynamics","abstract":"Conservation laws play a crucial role in the modeling of heavy-ion collisions, including the those for charges such as baryon number (B), strangeness (S), and electric charge (Q). In this study, we present a new 2+1 relativistic viscous hydrodynamic code called CCAKE which uses the Smoothed Particle Hydrodynamics (SPH) formalism to locally conserve BSQ charges, together with an extended description of the multi-dimensional equation of state (EoS) obtained from lattice Quantum Chromodynamics. Initial conditions for CCAKE are supplied by the ICCING model, which samples gluon splittings into quark anti-quark pairs to generate the initial BSQ charge distributions. We study correlations between the BSQ charges and find that local BSQ fluctuations remain finite during the evolution, with corresponding chemical potentials of ($\\sim100$--$200 \\,\\rm MeV$) at freeze-out. We find that our framework produces reasonable multiplicities of identified particles and that ICCING has no significant effect on the collective flow of all charged particles nor of identified particles when only one particle of interest is considered. However, we show specifically for Pb+Pb collisions at the LHC $\\sqrt{s_{NN}}=5.02$ TeV that ICCING does have an effect on collective flow of identified particles if two particles of interest are considered.","sentences":["Conservation laws play a crucial role in the modeling of heavy-ion collisions, including the those for charges such as baryon number (B), strangeness (S), and electric charge (Q).","In this study, we present a new 2+1 relativistic viscous hydrodynamic code called CCAKE which uses the Smoothed Particle Hydrodynamics (SPH) formalism to locally conserve BSQ charges, together with an extended description of the multi-dimensional equation of state (EoS) obtained from lattice Quantum Chromodynamics.","Initial conditions for CCAKE are supplied by the ICCING model, which samples gluon splittings into quark anti-quark pairs to generate the initial BSQ charge distributions.","We study correlations between the BSQ charges and find that local BSQ fluctuations remain finite during the evolution, with corresponding chemical potentials of ($\\sim100$--$200 \\,\\rm MeV$) at freeze-out.","We find that our framework produces reasonable multiplicities of identified particles and that ICCING has no significant effect on the collective flow of all charged particles nor of identified particles when only one particle of interest is considered.","However, we show specifically for Pb+Pb collisions at the LHC $\\sqrt{s_{NN}}=5.02$ TeV that ICCING does have an effect on collective flow of identified particles if two particles of interest are considered."],"url":"http://arxiv.org/abs/2405.09648v1","category":"nucl-th"}
{"created":"2024-05-15 18:06:08","title":"Evolving motility of active droplets is captured by a self-repelling random walk model","abstract":"Swimming droplets are a class of active particles whose motility changes as a function of time due to shrinkage and self-avoidance of their trail. Here we combine experiments and theory to show that our non-Markovian droplet (NMD) model, akin to a true self-avoiding walk [1], quantitatively captures droplet motion. We thus estimate the effective temperature arising from hydrodynamic flows and the coupling strength of the propulsion force as a function of fuel concentration. This framework explains a broad range of phenomena, including memory effects, solute-mediated interactions, droplet hovering above the surface, and enhanced collective diffusion.","sentences":["Swimming droplets are a class of active particles whose motility changes as a function of time due to shrinkage and self-avoidance of their trail.","Here we combine experiments and theory to show that our non-Markovian droplet (NMD) model, akin to a true self-avoiding walk [1], quantitatively captures droplet motion.","We thus estimate the effective temperature arising from hydrodynamic flows and the coupling strength of the propulsion force as a function of fuel concentration.","This framework explains a broad range of phenomena, including memory effects, solute-mediated interactions, droplet hovering above the surface, and enhanced collective diffusion."],"url":"http://arxiv.org/abs/2405.09636v1","category":"cond-mat.soft"}
{"created":"2024-05-15 18:00:07","title":"Fundamental Tests of P and CP Symmetries Using Octet Baryons at the $J/\u03c8$ Threshold","abstract":"We investigate tests of the parity P and the combined parity and charge-conjugate CP symmetries from differential angular distributions of $J/\\psi$ decaying into the lowest-lying baryon pairs at BESIII and the next-generation super tau-charm facilities (STCFs). Large corrections from $Z$ and $W$ exchange induced parity violating effects are found for $J/\\psi$ decays with large logarithms resummed up to $\\mathcal{O}(\\alpha_s)$. The parity-violating asymmetries on the production and the decay sides of $J/\\psi$ are both found to be of $\\mathcal{O}(10^{-4})$, thus barely observable with the 10 billion $J/\\psi$ events currently collected at BESIII. Nevertheless, these asymmetries utilizing the current BESIII data already permit a determination of the weak mixing angle with an absolute uncertainty $\\delta s_w^2\\approx0.08$, corresponding to the first determination of $s_w^2$ at the $J/\\psi$ threshold. While limited by statistics currently, STCFs are estimated to improve this precision by a factor of $\\sim\\,20$ to $\\delta s_w^2\\approx0.004$ within one year based on luminosity rescaling. We also obtain the 95% confidence level upper bounds on the electric dipole moments of the octet baryons, which are of $\\mathcal{O}(10^{-18})\\,e{\\rm\\,cm}$ for BESIII and $\\mathcal{O}(10^{-19})\\,e{\\rm\\,cm}$ for STCFs. These bounds are improved by two to three orders of magnitude in comparison with the only existing one on $\\Lambda$ from Fermilab. The method discussed in this work also paves a way for a first and direct measurement of the $\\Xi$ and $\\Sigma$ electric dipole moments.","sentences":["We investigate tests of the parity P and the combined parity and charge-conjugate CP symmetries from differential angular distributions of $J/\\psi$ decaying into the lowest-lying baryon pairs at BESIII and the next-generation super tau-charm facilities (STCFs).","Large corrections from $Z$ and $W$ exchange induced parity violating effects are found for $J/\\psi$ decays with large logarithms resummed up to $\\mathcal{O}(\\alpha_s)$. The parity-violating asymmetries on the production and the decay sides of $J/\\psi$ are both found to be of $\\mathcal{O}(10^{-4})$, thus barely observable with the 10 billion $J/\\psi$ events currently collected at BESIII.","Nevertheless, these asymmetries utilizing the current BESIII data already permit a determination of the weak mixing angle with an absolute uncertainty $\\delta s_w^2\\approx0.08$, corresponding to the first determination of $s_w^2$ at the $J/\\psi$ threshold.","While limited by statistics currently, STCFs are estimated to improve this precision by a factor of $\\sim\\,20$ to $\\delta s_w^2\\approx0.004$ within one year based on luminosity rescaling.","We also obtain the 95% confidence level upper bounds on the electric dipole moments of the octet baryons, which are of $\\mathcal{O}(10^{-18})\\,e{\\rm\\,cm}$ for BESIII and $\\mathcal{O}(10^{-19})\\,e{\\rm\\,cm}$ for STCFs.","These bounds are improved by two to three orders of magnitude in comparison with the only existing one on $\\Lambda$ from Fermilab.","The method discussed in this work also paves a way for a first and direct measurement of the $\\Xi$ and $\\Sigma$ electric dipole moments."],"url":"http://arxiv.org/abs/2405.09625v1","category":"hep-ph"}
{"created":"2024-05-15 18:00:06","title":"Holevo Cram\u00e9r-Rao bound: How close can we get without entangling measurements?","abstract":"In multi-parameter quantum metrology, the resource of entanglement can lead to an increase in efficiency of the estimation process. Entanglement can be used in the state preparation stage, or the measurement stage, or both, to harness this advantage; here we focus on the role of entangling measurements. Specifically, entangling or collective measurements over multiple identical copies of a probe state are known to be superior to measuring each probe individually, but the extent of this improvement is an open problem. It is also known that such entangling measurements, though resource-intensive, are required to attain the ultimate limits in multi-parameter quantum metrology and quantum information processing tasks. In this work we investigate the maximum precision improvement that collective quantum measurements can offer over individual measurements for estimating parameters of qudit states, calling this the 'collective quantum enhancement'. We show that, whereas the maximum enhancement can, in principle, be a factor of $n$ for estimating $n$ parameters, this bound is not tight for large $n$. Instead, our results prove an enhancement linear in dimension of the qudit is possible using collective measurements and lead us to conjecture that this is the maximum collective quantum enhancement in any local estimation scenario.","sentences":["In multi-parameter quantum metrology, the resource of entanglement can lead to an increase in efficiency of the estimation process.","Entanglement can be used in the state preparation stage, or the measurement stage, or both, to harness this advantage; here we focus on the role of entangling measurements.","Specifically, entangling or collective measurements over multiple identical copies of a probe state are known to be superior to measuring each probe individually, but the extent of this improvement is an open problem.","It is also known that such entangling measurements, though resource-intensive, are required to attain the ultimate limits in multi-parameter quantum metrology and quantum information processing tasks.","In this work we investigate the maximum precision improvement that collective quantum measurements can offer over individual measurements for estimating parameters of qudit states, calling this the 'collective quantum enhancement'.","We show that, whereas the maximum enhancement can, in principle, be a factor of $n$ for estimating $n$ parameters, this bound is not tight for large $n$. Instead, our results prove an enhancement linear in dimension of the qudit is possible using collective measurements and lead us to conjecture that this is the maximum collective quantum enhancement in any local estimation scenario."],"url":"http://arxiv.org/abs/2405.09622v1","category":"quant-ph"}
{"created":"2024-05-15 17:19:42","title":"Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models","abstract":"The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a framework for evaluating world modeling in language models by testing their ability to use knowledge of a concept to match a target text with a plausible/implausible context. EWOK targets specific concepts from multiple knowledge domains known to be vital for world modeling in humans. Domains range from social interactions (help/hinder) to spatial relations (left/right). Both, contexts and targets are minimal pairs. Objects, agents, and locations in the items can be flexibly filled in enabling easy generation of multiple controlled datasets. We then introduce EWOK-CORE-1.0, a dataset of 4,374 items covering 11 world knowledge domains. We evaluate 20 openweights large language models (1.3B--70B parameters) across a battery of evaluation paradigms along with a human norming study comprising 12,480 measurements. The overall performance of all tested models is worse than human performance, with results varying drastically across domains. These data highlight simple cases where even large models fail and present rich avenues for targeted research on LLM world modeling capabilities.","sentences":["The ability to build and leverage world models is essential for a general-purpose AI agent.","Testing such capabilities is hard, in part because the building blocks of world models are ill-defined.","We present Elements of World Knowledge (EWOK), a framework for evaluating world modeling in language models by testing their ability to use knowledge of a concept to match a target text with a plausible/implausible context.","EWOK targets specific concepts from multiple knowledge domains known to be vital for world modeling in humans.","Domains range from social interactions (help/hinder) to spatial relations (left/right).","Both, contexts and targets are minimal pairs.","Objects, agents, and locations in the items can be flexibly filled in enabling easy generation of multiple controlled datasets.","We then introduce EWOK-CORE-1.0, a dataset of 4,374 items covering 11 world knowledge domains.","We evaluate 20 openweights large language models (1.3B--70B parameters) across a battery of evaluation paradigms along with a human norming study comprising 12,480 measurements.","The overall performance of all tested models is worse than human performance, with results varying drastically across domains.","These data highlight simple cases where even large models fail and present rich avenues for targeted research on LLM world modeling capabilities."],"url":"http://arxiv.org/abs/2405.09605v1","category":"cs.CL"}
{"created":"2024-05-15 15:17:52","title":"Improving Label Error Detection and Elimination with Uncertainty Quantification","abstract":"Identifying and handling label errors can significantly enhance the accuracy of supervised machine learning models. Recent approaches for identifying label errors demonstrate that a low self-confidence of models with respect to a certain label represents a good indicator of an erroneous label. However, latest work has built on softmax probabilities to measure self-confidence. In this paper, we argue that -- as softmax probabilities do not reflect a model's predictive uncertainty accurately -- label error detection requires more sophisticated measures of model uncertainty. Therefore, we develop a range of novel, model-agnostic algorithms for Uncertainty Quantification-Based Label Error Detection (UQ-LED), which combine the techniques of confident learning (CL), Monte Carlo Dropout (MCD), model uncertainty measures (e.g., entropy), and ensemble learning to enhance label error detection. We comprehensively evaluate our algorithms on four image classification benchmark datasets in two stages. In the first stage, we demonstrate that our UQ-LED algorithms outperform state-of-the-art confident learning in identifying label errors. In the second stage, we show that removing all identified errors from the training data based on our approach results in higher accuracies than training on all available labeled data. Importantly, besides our contributions to the detection of label errors, we particularly propose a novel approach to generate realistic, class-dependent label errors synthetically. Overall, our study demonstrates that selectively cleaning datasets with UQ-LED algorithms leads to more accurate classifications than using larger, noisier datasets.","sentences":["Identifying and handling label errors can significantly enhance the accuracy of supervised machine learning models.","Recent approaches for identifying label errors demonstrate that a low self-confidence of models with respect to a certain label represents a good indicator of an erroneous label.","However, latest work has built on softmax probabilities to measure self-confidence.","In this paper, we argue that -- as softmax probabilities do not reflect a model's predictive uncertainty accurately -- label error detection requires more sophisticated measures of model uncertainty.","Therefore, we develop a range of novel, model-agnostic algorithms for Uncertainty Quantification-Based Label Error Detection (UQ-LED), which combine the techniques of confident learning (CL), Monte Carlo Dropout (MCD), model uncertainty measures (e.g., entropy), and ensemble learning to enhance label error detection.","We comprehensively evaluate our algorithms on four image classification benchmark datasets in two stages.","In the first stage, we demonstrate that our UQ-LED algorithms outperform state-of-the-art confident learning in identifying label errors.","In the second stage, we show that removing all identified errors from the training data based on our approach results in higher accuracies than training on all available labeled data.","Importantly, besides our contributions to the detection of label errors, we particularly propose a novel approach to generate realistic, class-dependent label errors synthetically.","Overall, our study demonstrates that selectively cleaning datasets with UQ-LED algorithms leads to more accurate classifications than using larger, noisier datasets."],"url":"http://arxiv.org/abs/2405.09602v1","category":"cs.LG"}
{"created":"2024-05-15 14:14:34","title":"Aggregate Representation Measure for Predictive Model Reusability","abstract":"In this paper, we propose a predictive quantifier to estimate the retraining cost of a trained model in distribution shifts. The proposed Aggregated Representation Measure (ARM) quantifies the change in the model's representation from the old to new data distribution. It provides, before actually retraining the model, a single concise index of resources - epochs, energy, and carbon emissions - required for the retraining. This enables reuse of a model with a much lower cost than training a new model from scratch. The experimental results indicate that ARM reasonably predicts retraining costs for varying noise intensities and enables comparisons among multiple model architectures to determine the most cost-effective and sustainable option.","sentences":["In this paper, we propose a predictive quantifier to estimate the retraining cost of a trained model in distribution shifts.","The proposed Aggregated Representation Measure (ARM) quantifies the change in the model's representation from the old to new data distribution.","It provides, before actually retraining the model, a single concise index of resources - epochs, energy, and carbon emissions - required for the retraining.","This enables reuse of a model with a much lower cost than training a new model from scratch.","The experimental results indicate that ARM reasonably predicts retraining costs for varying noise intensities and enables comparisons among multiple model architectures to determine the most cost-effective and sustainable option."],"url":"http://arxiv.org/abs/2405.09600v1","category":"cs.LG"}
{"created":"2024-05-15 14:06:28","title":"Properties that allow or prohibit transferability of adversarial attacks among quantized networks","abstract":"Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Further, these adversarial examples are found to be transferable from the source network in which they are crafted to a black-box target network. As the trend of using deep learning on embedded devices grows, it becomes relevant to study the transferability properties of adversarial examples among compressed networks. In this paper, we consider quantization as a network compression technique and evaluate the performance of transfer-based attacks when the source and target networks are quantized at different bitwidths. We explore how algorithm specific properties affect transferability by considering various adversarial example generation algorithms. Furthermore, we examine transferability in a more realistic scenario where the source and target networks may differ in bitwidth and other model-related properties like capacity and architecture. We find that although quantization reduces transferability, certain attack types demonstrate an ability to enhance it. Additionally, the average transferability of adversarial examples among quantized versions of a network can be used to estimate the transferability to quantized target networks with varying capacity and architecture.","sentences":["Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples.","Further, these adversarial examples are found to be transferable from the source network in which they are crafted to a black-box target network.","As the trend of using deep learning on embedded devices grows, it becomes relevant to study the transferability properties of adversarial examples among compressed networks.","In this paper, we consider quantization as a network compression technique and evaluate the performance of transfer-based attacks when the source and target networks are quantized at different bitwidths.","We explore how algorithm specific properties affect transferability by considering various adversarial example generation algorithms.","Furthermore, we examine transferability in a more realistic scenario where the source and target networks may differ in bitwidth and other model-related properties like capacity and architecture.","We find that although quantization reduces transferability, certain attack types demonstrate an ability to enhance it.","Additionally, the average transferability of adversarial examples among quantized versions of a network can be used to estimate the transferability to quantized target networks with varying capacity and architecture."],"url":"http://arxiv.org/abs/2405.09598v1","category":"cs.LG"}
{"created":"2024-05-15 13:50:23","title":"When AI Eats Itself: On the Caveats of Data Pollution in the Era of Generative AI","abstract":"Generative artificial intelligence (AI) technologies and large models are producing realistic outputs across various domains, such as images, text, speech, and music. Creating these advanced generative models requires significant resources, particularly large and high-quality datasets. To minimize training expenses, many algorithm developers use data created by the models themselves as a cost-effective training solution. However, not all synthetic data effectively improve model performance, necessitating a strategic balance in the use of real versus synthetic data to optimize outcomes.   Currently, the previously well-controlled integration of real and synthetic data is becoming uncontrollable. The widespread and unregulated dissemination of synthetic data online leads to the contamination of datasets traditionally compiled through web scraping, now mixed with unlabeled synthetic data. This trend portends a future where generative AI systems may increasingly rely blindly on consuming self-generated data, raising concerns about model performance and ethical issues. What will happen if generative AI continuously consumes itself without discernment? What measures can we take to mitigate the potential adverse effects?   There is a significant gap in the scientific literature regarding the impact of synthetic data use in generative AI, particularly in terms of the fusion of multimodal information. To address this research gap, this review investigates the consequences of integrating synthetic data blindly on training generative AI on both image and text modalities and explores strategies to mitigate these effects. The goal is to offer a comprehensive view of synthetic data's role, advocating for a balanced approach to its use and exploring practices that promote the sustainable development of generative AI technologies in the era of large models.","sentences":["Generative artificial intelligence (AI) technologies and large models are producing realistic outputs across various domains, such as images, text, speech, and music.","Creating these advanced generative models requires significant resources, particularly large and high-quality datasets.","To minimize training expenses, many algorithm developers use data created by the models themselves as a cost-effective training solution.","However, not all synthetic data effectively improve model performance, necessitating a strategic balance in the use of real versus synthetic data to optimize outcomes.   ","Currently, the previously well-controlled integration of real and synthetic data is becoming uncontrollable.","The widespread and unregulated dissemination of synthetic data online leads to the contamination of datasets traditionally compiled through web scraping, now mixed with unlabeled synthetic data.","This trend portends a future where generative AI systems may increasingly rely blindly on consuming self-generated data, raising concerns about model performance and ethical issues.","What will happen if generative AI continuously consumes itself without discernment?","What measures can we take to mitigate the potential adverse effects?   ","There is a significant gap in the scientific literature regarding the impact of synthetic data use in generative AI, particularly in terms of the fusion of multimodal information.","To address this research gap, this review investigates the consequences of integrating synthetic data blindly on training generative AI on both image and text modalities and explores strategies to mitigate these effects.","The goal is to offer a comprehensive view of synthetic data's role, advocating for a balanced approach to its use and exploring practices that promote the sustainable development of generative AI technologies in the era of large models."],"url":"http://arxiv.org/abs/2405.09597v1","category":"cs.LG"}
{"created":"2024-05-15 13:43:07","title":"Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)","abstract":"The prediction of ship trajectories is a growing field of study in artificial intelligence. Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series. This study proposes a viable alternative for predicting these trajectories using only GNSS positions. It considers this spatio-temporal problem as a natural language processing problem. The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index. Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index. The method is compared with a classical Kalman filter, widely used in the maritime domain, and introduces the Fr\\'echet distance as the main evaluation metric. We show that it is possible to predict ship trajectories quite precisely up to 8 hours with 30 minutes of context. We demonstrate that this alternative works well enough to predict trajectories worldwide.","sentences":["The prediction of ship trajectories is a growing field of study in artificial intelligence.","Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series.","This study proposes a viable alternative for predicting these trajectories using only GNSS positions.","It considers this spatio-temporal problem as a natural language processing problem.","The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index.","Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index.","The method is compared with a classical Kalman filter, widely used in the maritime domain, and introduces the Fr\\'echet distance as the main evaluation metric.","We show that it is possible to predict ship trajectories quite precisely up to 8 hours with 30 minutes of context.","We demonstrate that this alternative works well enough to predict trajectories worldwide."],"url":"http://arxiv.org/abs/2405.09596v1","category":"cs.LG"}
{"created":"2024-05-15 13:32:45","title":"Simplicity within biological complexity","abstract":"Heterogeneous, interconnected, systems-level, molecular data have become increasingly available and key in precision medicine. We need to utilize them to better stratify patients into risk groups, discover new biomarkers and targets, repurpose known and discover new drugs to personalize medical treatment. Existing methodologies are limited and a paradigm shift is needed to achieve quantitative and qualitative breakthroughs. In this perspective paper, we survey the literature and argue for the development of a comprehensive, general framework for embedding of multi-scale molecular network data that would enable their explainable exploitation in precision medicine in linear time. Network embedding methods map nodes to points in low-dimensional space, so that proximity in the learned space reflects the network's topology-function relationships. They have recently achieved unprecedented performance on hard problems of utilizing few omic data in various biomedical applications. However, research thus far has been limited to special variants of the problems and data, with the performance depending on the underlying topology-function network biology hypotheses, the biomedical applications and evaluation metrics. The availability of multi-omic data, modern graph embedding paradigms and compute power call for a creation and training of efficient, explainable and controllable models, having no potentially dangerous, unexpected behaviour, that make a qualitative breakthrough. We propose to develop a general, comprehensive embedding framework for multi-omic network data, from models to efficient and scalable software implementation, and to apply it to biomedical informatics. It will lead to a paradigm shift in computational and biomedical understanding of data and diseases that will open up ways to solving some of the major bottlenecks in precision medicine and other domains.","sentences":["Heterogeneous, interconnected, systems-level, molecular data have become increasingly available and key in precision medicine.","We need to utilize them to better stratify patients into risk groups, discover new biomarkers and targets, repurpose known and discover new drugs to personalize medical treatment.","Existing methodologies are limited and a paradigm shift is needed to achieve quantitative and qualitative breakthroughs.","In this perspective paper, we survey the literature and argue for the development of a comprehensive, general framework for embedding of multi-scale molecular network data that would enable their explainable exploitation in precision medicine in linear time.","Network embedding methods map nodes to points in low-dimensional space, so that proximity in the learned space reflects the network's topology-function relationships.","They have recently achieved unprecedented performance on hard problems of utilizing few omic data in various biomedical applications.","However, research thus far has been limited to special variants of the problems and data, with the performance depending on the underlying topology-function network biology hypotheses, the biomedical applications and evaluation metrics.","The availability of multi-omic data, modern graph embedding paradigms and compute power call for a creation and training of efficient, explainable and controllable models, having no potentially dangerous, unexpected behaviour, that make a qualitative breakthrough.","We propose to develop a general, comprehensive embedding framework for multi-omic network data, from models to efficient and scalable software implementation, and to apply it to biomedical informatics.","It will lead to a paradigm shift in computational and biomedical understanding of data and diseases that will open up ways to solving some of the major bottlenecks in precision medicine and other domains."],"url":"http://arxiv.org/abs/2405.09595v1","category":"q-bio.OT"}
{"created":"2024-05-15 12:22:48","title":"SQL-to-Schema Enhances Schema Linking in Text-to-SQL","abstract":"In sophisticated existing Text-to-SQL methods exhibit errors in various proportions, including schema-linking errors (incorrect columns, tables, or extra columns), join errors, nested errors, and group-by errors. Consequently, there is a critical need to filter out unnecessary tables and columns, directing the language models attention to relevant tables and columns with schema-linking, to reduce errors during SQL generation. Previous approaches have involved sorting tables and columns based on their relevance to the question, selecting the top-ranked ones for sorting, or directly identifying the necessary tables and columns for SQL generation. However, these methods face challenges such as lengthy model training times, high consumption of expensive GPT-4 tokens in few-shot prompts, or suboptimal performance in schema linking. Therefore, we propose an inventive schema linking method in two steps: Firstly, generate an initial SQL query by utilizing the complete database schema. Subsequently, extract tables and columns from the initial SQL query to create a concise schema. Using CodeLlama-34B, when comparing the schemas obtained by mainstream methods with ours for SQL generation, our schema performs optimally. Leveraging GPT4, our SQL generation method achieved results that are comparable to mainstream Text-to-SQL methods on the Spider dataset.","sentences":["In sophisticated existing Text-to-SQL methods exhibit errors in various proportions, including schema-linking errors (incorrect columns, tables, or extra columns), join errors, nested errors, and group-by errors.","Consequently, there is a critical need to filter out unnecessary tables and columns, directing the language models attention to relevant tables and columns with schema-linking, to reduce errors during SQL generation.","Previous approaches have involved sorting tables and columns based on their relevance to the question, selecting the top-ranked ones for sorting, or directly identifying the necessary tables and columns for SQL generation.","However, these methods face challenges such as lengthy model training times, high consumption of expensive GPT-4 tokens in few-shot prompts, or suboptimal performance in schema linking.","Therefore, we propose an inventive schema linking method in two steps: Firstly, generate an initial SQL query by utilizing the complete database schema.","Subsequently, extract tables and columns from the initial SQL query to create a concise schema.","Using CodeLlama-34B, when comparing the schemas obtained by mainstream methods with ours for SQL generation, our schema performs optimally.","Leveraging GPT4, our SQL generation method achieved results that are comparable to mainstream Text-to-SQL methods on the Spider dataset."],"url":"http://arxiv.org/abs/2405.09593v1","category":"cs.DB"}
{"created":"2024-05-15 12:07:43","title":"A Survey of Generative Techniques for Spatial-Temporal Data Mining","abstract":"This paper focuses on the integration of generative techniques into spatial-temporal data mining, considering the significant growth and diverse nature of spatial-temporal data. With the advancements in RNNs, CNNs, and other non-generative techniques, researchers have explored their application in capturing temporal and spatial dependencies within spatial-temporal data. However, the emergence of generative techniques such as LLMs, SSL, Seq2Seq and diffusion models has opened up new possibilities for enhancing spatial-temporal data mining further. The paper provides a comprehensive analysis of generative technique-based spatial-temporal methods and introduces a standardized framework specifically designed for the spatial-temporal data mining pipeline. By offering a detailed review and a novel taxonomy of spatial-temporal methodology utilizing generative techniques, the paper enables a deeper understanding of the various techniques employed in this field. Furthermore, the paper highlights promising future research directions, urging researchers to delve deeper into spatial-temporal data mining. It emphasizes the need to explore untapped opportunities and push the boundaries of knowledge to unlock new insights and improve the effectiveness and efficiency of spatial-temporal data mining. By integrating generative techniques and providing a standardized framework, the paper contributes to advancing the field and encourages researchers to explore the vast potential of generative techniques in spatial-temporal data mining.","sentences":["This paper focuses on the integration of generative techniques into spatial-temporal data mining, considering the significant growth and diverse nature of spatial-temporal data.","With the advancements in RNNs, CNNs, and other non-generative techniques, researchers have explored their application in capturing temporal and spatial dependencies within spatial-temporal data.","However, the emergence of generative techniques such as LLMs, SSL, Seq2Seq and diffusion models has opened up new possibilities for enhancing spatial-temporal data mining further.","The paper provides a comprehensive analysis of generative technique-based spatial-temporal methods and introduces a standardized framework specifically designed for the spatial-temporal data mining pipeline.","By offering a detailed review and a novel taxonomy of spatial-temporal methodology utilizing generative techniques, the paper enables a deeper understanding of the various techniques employed in this field.","Furthermore, the paper highlights promising future research directions, urging researchers to delve deeper into spatial-temporal data mining.","It emphasizes the need to explore untapped opportunities and push the boundaries of knowledge to unlock new insights and improve the effectiveness and efficiency of spatial-temporal data mining.","By integrating generative techniques and providing a standardized framework, the paper contributes to advancing the field and encourages researchers to explore the vast potential of generative techniques in spatial-temporal data mining."],"url":"http://arxiv.org/abs/2405.09592v1","category":"cs.LG"}
{"created":"2024-05-15 11:58:08","title":"A Comprehensive Survey on Data Augmentation","abstract":"Data augmentation is a series of techniques that generate high-quality artificial data by manipulating existing data samples. By leveraging data augmentation techniques, AI models can achieve significantly improved applicability in tasks involving scarce or imbalanced datasets, thereby substantially enhancing AI models' generalization capabilities. Existing literature surveys only focus on a certain type of specific modality data, and categorize these methods from modality-specific and operation-centric perspectives, which lacks a consistent summary of data augmentation methods across multiple modalities and limits the comprehension of how existing data samples serve the data augmentation process. To bridge this gap, we propose a more enlightening taxonomy that encompasses data augmentation techniques for different common data modalities. Specifically, from a data-centric perspective, this survey proposes a modality-independent taxonomy by investigating how to take advantage of the intrinsic relationship between data samples, including single-wise, pair-wise, and population-wise sample data augmentation methods. Additionally, we categorize data augmentation methods across five data modalities through a unified inductive approach.","sentences":["Data augmentation is a series of techniques that generate high-quality artificial data by manipulating existing data samples.","By leveraging data augmentation techniques, AI models can achieve significantly improved applicability in tasks involving scarce or imbalanced datasets, thereby substantially enhancing AI models' generalization capabilities.","Existing literature surveys only focus on a certain type of specific modality data, and categorize these methods from modality-specific and operation-centric perspectives, which lacks a consistent summary of data augmentation methods across multiple modalities and limits the comprehension of how existing data samples serve the data augmentation process.","To bridge this gap, we propose a more enlightening taxonomy that encompasses data augmentation techniques for different common data modalities.","Specifically, from a data-centric perspective, this survey proposes a modality-independent taxonomy by investigating how to take advantage of the intrinsic relationship between data samples, including single-wise, pair-wise, and population-wise sample data augmentation methods.","Additionally, we categorize data augmentation methods across five data modalities through a unified inductive approach."],"url":"http://arxiv.org/abs/2405.09591v1","category":"cs.LG"}
{"created":"2024-05-15 10:16:25","title":"Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Review","abstract":"The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.","sentences":["The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks.","However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications.","The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount.","This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities.","By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners.","Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area."],"url":"http://arxiv.org/abs/2405.09589v1","category":"cs.LG"}
{"created":"2024-05-15 09:26:24","title":"Training Deep Learning Models with Hybrid Datasets for Robust Automatic Target Detection on real SAR images","abstract":"In this work, we propose to tackle several challenges hindering the development of Automatic Target Detection (ATD) algorithms for ground targets in SAR images. To address the lack of representative training data, we propose a Deep Learning approach to train ATD models with synthetic target signatures produced with the MOCEM simulator. We define an incrustation pipeline to incorporate synthetic targets into real backgrounds. Using this hybrid dataset, we train ATD models specifically tailored to bridge the domain gap between synthetic and real data. Our approach notably relies on massive physics-based data augmentation techniques and Adversarial Training of two deep-learning detection architectures. We then test these models on several datasets, including (1) patchworks of real SAR images, (2) images with the incrustation of real targets in real backgrounds, and (3) images with the incrustation of synthetic background objects in real backgrounds. Results show that the produced hybrid datasets are exempt from image overlay bias. Our approach can reach up to 90% of Average Precision on real data while exclusively using synthetic targets for training.","sentences":["In this work, we propose to tackle several challenges hindering the development of Automatic Target Detection (ATD) algorithms for ground targets in SAR images.","To address the lack of representative training data, we propose a Deep Learning approach to train ATD models with synthetic target signatures produced with the MOCEM simulator.","We define an incrustation pipeline to incorporate synthetic targets into real backgrounds.","Using this hybrid dataset, we train ATD models specifically tailored to bridge the domain gap between synthetic and real data.","Our approach notably relies on massive physics-based data augmentation techniques and Adversarial Training of two deep-learning detection architectures.","We then test these models on several datasets, including (1) patchworks of real SAR images, (2) images with the incrustation of real targets in real backgrounds, and (3) images with the incrustation of synthetic background objects in real backgrounds.","Results show that the produced hybrid datasets are exempt from image overlay bias.","Our approach can reach up to 90% of Average Precision on real data while exclusively using synthetic targets for training."],"url":"http://arxiv.org/abs/2405.09588v1","category":"cs.CV"}
{"created":"2024-05-16 17:58:39","title":"Hydrodynamic Edge Modes and Fragile Surface States of Symmetry Protected Integer Quantum Hall Effect of Bosons","abstract":"We adapt the fluid description of Fractional Quantum Hall (FQH) states, as seen in (arXiv:2203.06516), to model a system of interacting two-component bosons. This system represents the simplest physical realization of an interacting bosonic Symmetry-Protected Topological (SPT) phase, also known as the integer quantum Hall effect (IQHE) of bosons. In particular, we demonstrate how the fluid dynamical boundary conditions of no-penetration and no-stress at a hard wall naturally give rise to the two counter-propagating boundary modes expected in these SPT phases. Moreover, we identify energy-conserving hydro boundary conditions that can either create a gap in these edge modes or completely isolate the edge states from the bulk, as described in (Physical Review X 14, 011057 (2024)), where they are termed fragile surface states. These fragile surface states are typically absent in K-matrix edge theories and require bulk dynamics to manifest. By leveraging insights from hydrodynamical boundary dynamics, we can further elucidate the intricate surface properties of SPTs beyond the usual topological quantum field theory based approaches.","sentences":["We adapt the fluid description of Fractional Quantum Hall (FQH) states, as seen in (arXiv:2203.06516), to model a system of interacting two-component bosons.","This system represents the simplest physical realization of an interacting bosonic Symmetry-Protected Topological (SPT) phase, also known as the integer quantum Hall effect (IQHE) of bosons.","In particular, we demonstrate how the fluid dynamical boundary conditions of no-penetration and no-stress at a hard wall naturally give rise to the two counter-propagating boundary modes expected in these SPT phases.","Moreover, we identify energy-conserving hydro boundary conditions that can either create a gap in these edge modes or completely isolate the edge states from the bulk, as described in (Physical Review X 14, 011057 (2024)), where they are termed fragile surface states.","These fragile surface states are typically absent in K-matrix edge theories and require bulk dynamics to manifest.","By leveraging insights from hydrodynamical boundary dynamics, we can further elucidate the intricate surface properties of SPTs beyond the usual topological quantum field theory based approaches."],"url":"http://arxiv.org/abs/2405.10309v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 17:58:19","title":"Efficient Implementation of an Abstract Domain of Quantified First-Order Formulas","abstract":"This paper lays a practical foundation for using abstract interpretation with an abstract domain that consists of sets of quantified first-order logic formulas. This abstract domain seems infeasible at first sight due to the complexity of the formulas involved and the enormous size of sets of formulas (abstract elements). We introduce an efficient representation of abstract elements, which eliminates redundancies based on a novel syntactic subsumption relation that under-approximates semantic entailment. We develop algorithms and data-structures to efficiently compute the join of an abstract element with the abstraction of a concrete state, operating on the representation of abstract elements. To demonstrate feasibility of the domain, we use our data structures and algorithms to implement a symbolic abstraction algorithm that computes the least fixpoint of the best abstract transformer of a transition system, which corresponds to the strongest inductive invariant. We succeed at finding, for example, the least fixpoint for Paxos (which in our representation has 1,438 formulas with forall-exists-forall quantification) in time comparable to state-of-the-art property-directed approaches.","sentences":["This paper lays a practical foundation for using abstract interpretation with an abstract domain that consists of sets of quantified first-order logic formulas.","This abstract domain seems infeasible at first sight due to the complexity of the formulas involved and the enormous size of sets of formulas (abstract elements).","We introduce an efficient representation of abstract elements, which eliminates redundancies based on a novel syntactic subsumption relation that under-approximates semantic entailment.","We develop algorithms and data-structures to efficiently compute the join of an abstract element with the abstraction of a concrete state, operating on the representation of abstract elements.","To demonstrate feasibility of the domain, we use our data structures and algorithms to implement a symbolic abstraction algorithm that computes the least fixpoint of the best abstract transformer of a transition system, which corresponds to the strongest inductive invariant.","We succeed at finding, for example, the least fixpoint for Paxos (which in our representation has 1,438 formulas with forall-exists-forall quantification) in time comparable to state-of-the-art property-directed approaches."],"url":"http://arxiv.org/abs/2405.10308v1","category":"cs.LO"}
{"created":"2024-05-16 17:50:04","title":"On Sample Selection for Continual Learning: a Video Streaming Case Study","abstract":"Machine learning (ML) is a powerful tool to model the complexity of communication networks. As networks evolve, we cannot only train once and deploy. Retraining models, known as continual learning, is necessary. Yet, to date, there is no established methodology to answer the key questions: With which samples to retrain? When should we retrain?   We address these questions with the sample selection system Memento, which maintains a training set with the \"most useful\" samples to maximize sample space coverage. Memento particularly benefits rare patterns -- the notoriously long \"tail\" in networking -- and allows assessing rationally when retraining may help, i.e., when the coverage changes.   We deployed Memento on Puffer, the live-TV streaming project, and achieved a 14% reduction of stall time, 3.5x the improvement of random sample selection. Finally, Memento does not depend on a specific model architecture; it is likely to yield benefits in other ML-based networking applications.","sentences":["Machine learning (ML) is a powerful tool to model the complexity of communication networks.","As networks evolve, we cannot only train once and deploy.","Retraining models, known as continual learning, is necessary.","Yet, to date, there is no established methodology to answer the key questions: With which samples to retrain?","When should we retrain?   ","We address these questions with the sample selection system Memento, which maintains a training set with the \"most useful\" samples to maximize sample space coverage.","Memento particularly benefits rare patterns -- the notoriously long \"tail\" in networking -- and allows assessing rationally when retraining may help, i.e., when the coverage changes.   ","We deployed Memento on Puffer, the live-TV streaming project, and achieved a 14% reduction of stall time, 3.5x the improvement of random sample selection.","Finally, Memento does not depend on a specific model architecture; it is likely to yield benefits in other ML-based networking applications."],"url":"http://arxiv.org/abs/2405.10290v1","category":"cs.NI"}
{"created":"2024-05-16 17:37:01","title":"Evaluation of a Multi-Molecule Molecular Communication Testbed Based on Spectral Sensing","abstract":"This work presents a novel flow-based molecular communication (MC) testbed using spectral sensing and ink concentration estimation to enable real-time multi-molecule (MUMO) transmission. MUMO communication opens up crucial opportunities for increased throughput as well as implementing more complex coding, modulation, and resource allocation strategies for MC testbeds. A concentration estimator using non-invasive spectral sensing at the receiver is proposed based on a simple absorption model. We conduct in-depth channel impulse response (CIR) measurements and a preliminary communication performance evaluation. Additionally, a simple analytical model is used to check the consistency of the CIRs. The results indicate that by utilizing MUMO transmission, on-off-keying, and a simple difference detector, the testbed can achieve up to 3 bits per second for near-error-free communication, which is on par with comparable testbeds that utilize more sophisticated coding or detection methods. Our platform lays the ground for implementing MUMO communication and evaluating various physical layer and networking techniques based on multiple molecule types in future MC testbeds in real time.","sentences":["This work presents a novel flow-based molecular communication (MC) testbed using spectral sensing and ink concentration estimation to enable real-time multi-molecule (MUMO) transmission.","MUMO communication opens up crucial opportunities for increased throughput as well as implementing more complex coding, modulation, and resource allocation strategies for MC testbeds.","A concentration estimator using non-invasive spectral sensing at the receiver is proposed based on a simple absorption model.","We conduct in-depth channel impulse response (CIR) measurements and a preliminary communication performance evaluation.","Additionally, a simple analytical model is used to check the consistency of the CIRs.","The results indicate that by utilizing MUMO transmission, on-off-keying, and a simple difference detector, the testbed can achieve up to 3 bits per second for near-error-free communication, which is on par with comparable testbeds that utilize more sophisticated coding or detection methods.","Our platform lays the ground for implementing MUMO communication and evaluating various physical layer and networking techniques based on multiple molecule types in future MC testbeds in real time."],"url":"http://arxiv.org/abs/2405.10280v1","category":"cs.ET"}
{"created":"2024-05-16 17:35:28","title":"Locating the critical point for the hadron to quark-gluon plasma phase transition from finite-size scaling of proton cumulants in heavy-ion collisions","abstract":"We perform a finite-size scaling analysis of net-proton number cumulants in Au+Au collisions at center-of-mass energies between $\\sqrt{s_{\\rm{NN}}} = 2.4$ GeV and 54.4 GeV to search for evidence of a critical point in the QCD phase diagram. In our analysis, we use both susceptibility and Binder cumulants which we extract from the second and fourth moments of the net-proton number distributions. We take measurements in different rapidity bin widths, corresponding to different subvolumes of the system, as probes of different length scales. We use model simulations to verify the applicability of this approach, then apply it to data and find evidence for a critical point near the baryon chemical potential of $\\mu_{B} \\approx 625$ MeV and temperature of $T \\approx 140$ MeV. The Binder cumulants, also analyzed in varying rapidity bin widths, provide complementary evidence for a critical point in a similar region. This is the first analysis of experimental data to locate the critical point in a range consistent with theoretical predictions.","sentences":["We perform a finite-size scaling analysis of net-proton number cumulants in Au+Au collisions at center-of-mass energies between $\\sqrt{s_{\\rm{NN}}} = 2.4$ GeV and 54.4 GeV to search for evidence of a critical point in the QCD phase diagram.","In our analysis, we use both susceptibility and Binder cumulants which we extract from the second and fourth moments of the net-proton number distributions.","We take measurements in different rapidity bin widths, corresponding to different subvolumes of the system, as probes of different length scales.","We use model simulations to verify the applicability of this approach, then apply it to data and find evidence for a critical point near the baryon chemical potential of $\\mu_{B} \\approx 625$ MeV and temperature of $T \\approx 140$ MeV.","The Binder cumulants, also analyzed in varying rapidity bin widths, provide complementary evidence for a critical point in a similar region.","This is the first analysis of experimental data to locate the critical point in a range consistent with theoretical predictions."],"url":"http://arxiv.org/abs/2405.10278v1","category":"nucl-th"}
{"created":"2024-05-16 17:29:43","title":"A note on the equivalence of Gromov boundary and metric boundary","abstract":"In this paper, we introduce the concept of quasihyperbolically visible spaces. As a tool, we study the connection between the Gromov boundary and the metric boundary.","sentences":["In this paper, we introduce the concept of quasihyperbolically visible spaces.","As a tool, we study the connection between the Gromov boundary and the metric boundary."],"url":"http://arxiv.org/abs/2405.10273v1","category":"math.MG"}
{"created":"2024-05-16 17:25:07","title":"Direct magnetic imaging of fractional Chern insulators in twisted MoTe$_2$ with a superconducting sensor","abstract":"In the absence of time reversal symmetry, orbital magnetization provides a sensitive probe of topology and interactions, with particularly rich phenomenology in Chern insulators where topological edge states carry large equilibrium currents. Here, we use a nanoscale superconducting sensor to map the magnetic fringe fields in twisted bilayers of MoTe$_2$, where transport and optical sensing experiments have revealed the formation of fractional Chern insulator (FCI) states at zero magnetic field. At a temperature of 1.6K, we observe oscillations in the local magnetic field associated with fillings $\\nu=-1,-2/3,-3/5,-4/7$ and $-5/9$ of the first moir\\'e hole band, consistent with the formation of FCIs at these fillings. By quantitatively reconstructing the magnetization, we determine the local thermodynamic gaps of the most robust FCI state at $\\nu=-2/3$, finding $^{-2/3}\\Delta$ as large as 7 meV. Spatial mapping of the charge density- and displacement field-tuned magnetic phase diagram further allows us to characterize sample disorder, which we find to be dominated by both inhomogeneity in the effective unit cell area as well as inhomogeneity in the band edge offset and bound dipole moment. Our results highlight both the challenges posed by structural disorder in the study of twisted homobilayer moir\\'e systems and the opportunities afforded by the remarkably robust nature of the underlying correlated topological states.","sentences":["In the absence of time reversal symmetry, orbital magnetization provides a sensitive probe of topology and interactions, with particularly rich phenomenology in Chern insulators where topological edge states carry large equilibrium currents.","Here, we use a nanoscale superconducting sensor to map the magnetic fringe fields in twisted bilayers of MoTe$_2$, where transport and optical sensing experiments have revealed the formation of fractional Chern insulator (FCI) states at zero magnetic field.","At a temperature of 1.6K, we observe oscillations in the local magnetic field associated with fillings $\\nu=-1,-2/3,-3/5,-4/7$ and $-5/9$ of the first moir\\'e hole band, consistent with the formation of FCIs at these fillings.","By quantitatively reconstructing the magnetization, we determine the local thermodynamic gaps of the most robust FCI state at $\\nu=-2/3$, finding $^{-2/3}\\Delta$ as large as 7 meV. Spatial mapping of the charge density- and displacement field-tuned magnetic phase diagram further allows us to characterize sample disorder, which we find to be dominated by both inhomogeneity in the effective unit cell area as well as inhomogeneity in the band edge offset and bound dipole moment.","Our results highlight both the challenges posed by structural disorder in the study of twisted homobilayer moir\\'e systems and the opportunities afforded by the remarkably robust nature of the underlying correlated topological states."],"url":"http://arxiv.org/abs/2405.10269v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 17:05:03","title":"The 3He(\\vec n,p)3H parity-conserving asymmetry","abstract":"Recently, the n$^3$He collaboration reported a measurement of the parity-violating (PV) proton directional asymmetry $A_{\\mathrm {PV}} = (1.55\\pm 0.97~\\mathrm {(st\\ at)} \\pm 0.24~\\mathrm {(sys)})\\times 10^{-8}$ in the capture reaction of ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H at meV incident neutron energies. The result increased the limited inventory of precisely measured and calculable PV observables in few-body systems required to further understand the structure of hadronic weak interaction. In this letter, we report the experimental and theoretical investigation of a parity conserving (PC) asymmetry $A_{\\mathrm {PC}}$ in the same reaction (the first ever measured PC observable at meV neutron energies). As a result of S- and P-wave mixing in the reaction, the $A_{\\mathrm {PC}}$ is inversely proportional to the neutron wavelength $\\lambda$. The experimental value is $(\\lambda\\times A_{\\mathrm {PC}})\\equiv\\beta= (-1.97 \\pm 0.28~\\mathrm{(stat)}\\pm 0.12~\\mathrm{(sys)}) \\times 10^{-6}$ Amstrongs. We present results for a theoretical analysis of this reaction by solving the four-body scattering problem within the hyperspherical harmonic method. We find that in the ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H reaction, $A_{\\mathrm {PC}}$ depends critically on the energy and width of the close $0^-$ resonant state of ${}^4$He, resulting in a large sensitivity to the spin-orbit components of the nucleon-nucleon force and even to the three-nucleon force. The analysis of the accurately measured $A_{\\mathrm {PC}}$ and $A_{\\mathrm {PV}}$ using the same few-body theoretical models gives essential information needed to interpret the PV asymmetry in the ${}^3$He$(\\vec {n}, {\\mathrm p}){}^3$H reaction.","sentences":["Recently, the n$^3$He collaboration reported a measurement of the parity-violating (PV) proton directional asymmetry $A_{\\mathrm {PV}} = (1.55\\pm 0.97~\\mathrm {(st\\ at)} \\pm 0.24~\\mathrm {(sys)})\\times 10^{-8}$ in the capture reaction of ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H at meV incident neutron energies.","The result increased the limited inventory of precisely measured and calculable PV observables in few-body systems required to further understand the structure of hadronic weak interaction.","In this letter, we report the experimental and theoretical investigation of a parity conserving (PC) asymmetry $A_{\\mathrm {PC}}$ in the same reaction (the first ever measured PC observable at meV neutron energies).","As a result of S- and P-wave mixing in the reaction, the $A_{\\mathrm {PC}}$ is inversely proportional to the neutron wavelength $\\lambda$.","The experimental value is $(\\lambda\\times A_{\\mathrm {PC}})\\equiv\\beta= (-1.97 \\pm 0.28~\\mathrm{(stat)}\\pm 0.12~\\mathrm{(sys)})","\\times 10^{-6}$","Amstrongs.","We present results for a theoretical analysis of this reaction by solving the four-body scattering problem within the hyperspherical harmonic method.","We find that in the ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H reaction, $A_{\\mathrm {PC}}$ depends critically on the energy and width of the close $0^-$ resonant state of ${}^4$He, resulting in a large sensitivity to the spin-orbit components of the nucleon-nucleon force and even to the three-nucleon force.","The analysis of the accurately measured $A_{\\mathrm {PC}}$ and $A_{\\mathrm {PV}}$ using the same few-body theoretical models gives essential information needed to interpret the PV asymmetry in the ${}^3$He$(\\vec {n}, {\\mathrm p}){}^3$H reaction."],"url":"http://arxiv.org/abs/2405.10258v1","category":"nucl-ex"}
{"created":"2024-05-16 16:58:14","title":"Adaptive Quotient Filters","abstract":"Adaptive filters, such as telescoping and adaptive cuckoo filters, update their representation upon detecting a false positive to avoid repeating the same error in the future. Adaptive filters require an auxiliary structure, typically much larger than the main filter and often residing on slow storage, to facilitate adaptation. However, existing adaptive filters are not practical and have seen no adoption in real-world systems due to two main reasons. Firstly, they offer weak adaptivity guarantees, meaning that fixing a new false positive can cause a previously fixed false positive to come back. Secondly, the sub-optimal design of the auxiliary structure results in adaptivity overheads so substantial that they can actually diminish the overall system performance compared to a traditional filter.   In this paper, we design and implement AdaptiveQF, the first practical adaptive filter with minimal adaptivity overhead and strong adaptivity guarantees, which means that the performance and false-positive guarantees continue to hold even for adversarial workloads. The AdaptiveQF is based on the state-of-the-art quotient filter design and preserves all the critical features of the quotient filter such as cache efficiency and mergeability. Furthermore, we employ a new auxiliary structure design which results in considerably low adaptivity overhead and makes the AdaptiveQF practical in real systems.","sentences":["Adaptive filters, such as telescoping and adaptive cuckoo filters, update their representation upon detecting a false positive to avoid repeating the same error in the future.","Adaptive filters require an auxiliary structure, typically much larger than the main filter and often residing on slow storage, to facilitate adaptation.","However, existing adaptive filters are not practical and have seen no adoption in real-world systems due to two main reasons.","Firstly, they offer weak adaptivity guarantees, meaning that fixing a new false positive can cause a previously fixed false positive to come back.","Secondly, the sub-optimal design of the auxiliary structure results in adaptivity overheads so substantial that they can actually diminish the overall system performance compared to a traditional filter.   ","In this paper, we design and implement AdaptiveQF, the first practical adaptive filter with minimal adaptivity overhead and strong adaptivity guarantees, which means that the performance and false-positive guarantees continue to hold even for adversarial workloads.","The AdaptiveQF is based on the state-of-the-art quotient filter design and preserves all the critical features of the quotient filter such as cache efficiency and mergeability.","Furthermore, we employ a new auxiliary structure design which results in considerably low adaptivity overhead and makes the AdaptiveQF practical in real systems."],"url":"http://arxiv.org/abs/2405.10253v1","category":"cs.DS"}
{"created":"2024-05-16 16:57:05","title":"Bass Note Spectra of Binary Forms","abstract":"We show that the spectrum of every $\\mathbb{R}-$isotropic homogeneous binary form $P$ of degree $n\\geq3$ is an interval of the form $[0,M_P],$ where $M_P$ is some positive constant. This completes the discussion around a conjecture of Mordell from 1940 (disproved by Davenport) regarding the existence of spectral gaps for binary cubic forms and further settles Mahler's program for binary forms of every degree.","sentences":["We show that the spectrum of every $\\mathbb{R}-$isotropic homogeneous binary form $P$ of degree $n\\geq3$ is an interval of the form $[0,M_P],$ where $M_P$ is some positive constant.","This completes the discussion around a conjecture of Mordell from 1940 (disproved by Davenport) regarding the existence of spectral gaps for binary cubic forms and further settles Mahler's program for binary forms of every degree."],"url":"http://arxiv.org/abs/2405.10252v1","category":"math.NT"}
{"created":"2024-05-16 16:46:27","title":"Quantum State Learning Implies Circuit Lower Bounds","abstract":"We establish connections between state tomography, pseudorandomness, quantum state synthesis, and circuit lower bounds. In particular, let $\\mathfrak{C}$ be a family of non-uniform quantum circuits of polynomial size and suppose that there exists an algorithm that, given copies of $|\\psi \\rangle$, distinguishes whether $|\\psi \\rangle$ is produced by $\\mathfrak{C}$ or is Haar random, promised one of these is the case. For arbitrary fixed constant $c$, we show that if the algorithm uses at most $O(2^{n^c})$ time and $2^{n^{0.99}}$ samples then $\\mathsf{stateBQE} \\not\\subset \\mathsf{state}\\mathfrak{C}$. Here $\\mathsf{stateBQE} := \\mathsf{stateBQTIME}[2^{O(n)}]$ and $\\mathsf{state}\\mathfrak{C}$ are state synthesis complexity classes as introduced by Rosenthal and Yuen (ITCS 2022), which capture problems with classical inputs but quantum output. Note that efficient tomography implies a similarly efficient distinguishing algorithm against Haar random states, even for nearly exponential-time algorithms. Because every state produced by a polynomial-size circuit can be learned with $2^{O(n)}$ samples and time, or $O(n^{\\omega(1)})$ samples and $2^{O(n^{\\omega(1)})}$ time, we show that even slightly non-trivial quantum state tomography algorithms would lead to new statements about quantum state synthesis. Finally, a slight modification of our proof shows that distinguishing algorithms for quantum states can imply circuit lower bounds for decision problems as well. This help sheds light on why time-efficient tomography algorithms for non-uniform quantum circuit classes has only had limited and partial progress. Our work parallels results by Arunachalam et al. (FOCS 2021) that revealed a similar connection between quantum learning of Boolean functions and circuit lower bounds for classical circuit classes, but modified for the purposes of state tomography and state synthesis.","sentences":["We establish connections between state tomography, pseudorandomness, quantum state synthesis, and circuit lower bounds.","In particular, let $\\mathfrak{C}$ be a family of non-uniform quantum circuits of polynomial size and suppose that there exists an algorithm that, given copies of $|\\psi \\rangle$, distinguishes whether $|\\psi \\rangle$ is produced by $\\mathfrak{C}$ or is Haar random, promised one of these is the case.","For arbitrary fixed constant $c$, we show that if the algorithm uses at most $O(2^{n^c})$ time and $2^{n^{0.99}}$ samples then $\\mathsf{stateBQE} \\not\\subset \\mathsf{state}\\mathfrak{C}$. Here $\\mathsf{stateBQE} := \\mathsf{stateBQTIME}[2^{O(n)}]$ and $\\mathsf{state}\\mathfrak{C}$ are state synthesis complexity classes as introduced by Rosenthal and Yuen (ITCS 2022), which capture problems with classical inputs but quantum output.","Note that efficient tomography implies a similarly efficient distinguishing algorithm against Haar random states, even for nearly exponential-time algorithms.","Because every state produced by a polynomial-size circuit can be learned with $2^{O(n)}$ samples and time, or $O(n^{\\omega(1)})$ samples and $2^{O(n^{\\omega(1)})}$ time, we show that even slightly non-trivial quantum state tomography algorithms would lead to new statements about quantum state synthesis.","Finally, a slight modification of our proof shows that distinguishing algorithms for quantum states can imply circuit lower bounds for decision problems as well.","This help sheds light on why time-efficient tomography algorithms for non-uniform quantum circuit classes has only had limited and partial progress.","Our work parallels results by Arunachalam et al.","(FOCS 2021) that revealed a similar connection between quantum learning of Boolean functions and circuit lower bounds for classical circuit classes, but modified for the purposes of state tomography and state synthesis."],"url":"http://arxiv.org/abs/2405.10242v1","category":"quant-ph"}
{"created":"2024-05-16 16:38:56","title":"Rounding Large Independent Sets on Expanders","abstract":"We develop a new approach for approximating large independent sets when the input graph is a one-sided spectral expander - that is, the uniform random walk matrix of the graph has the second eigenvalue bounded away from 1. Consequently, we obtain a polynomial time algorithm to find linear-sized independent sets in one-sided expanders that are almost $3$-colorable or are promised to contain an independent set of size $(1/2-\\epsilon)n$. Our second result above can be refined to require only a weaker vertex expansion property with an efficient certificate. Somewhat surprisingly, we observe that the analogous task of finding a linear-sized independent set in almost $4$-colorable one-sided expanders (even when the second eigenvalue is $o_n(1)$) is NP-hard, assuming the Unique Games Conjecture.   All prior algorithms that beat the worst-case guarantees for this problem rely on bottom eigenspace enumeration techniques (following the classical spectral methods of Alon and Kahale) and require two-sided expansion, meaning a bounded number of negative eigenvalues of magnitude $\\Omega(1)$. Such techniques naturally extend to almost $k$-colorable graphs for any constant $k$, in contrast to analogous guarantees on one-sided expanders, which are Unique Games-hard to achieve for $k \\geq 4$.   Our rounding builds on the method of simulating multiple samples from a pseudodistribution introduced by Barak et. al. for rounding Unique Games instances. The key to our analysis is a new clustering property of large independent sets in expanding graphs - every large independent set has a larger-than-expected intersection with some member of a small list - and its formalization in the low-degree sum-of-squares proof system.","sentences":["We develop a new approach for approximating large independent sets when the input graph is a one-sided spectral expander - that is, the uniform random walk matrix of the graph has the second eigenvalue bounded away from 1.","Consequently, we obtain a polynomial time algorithm to find linear-sized independent sets in one-sided expanders that are almost $3$-colorable or are promised to contain an independent set of size $(1/2-\\epsilon)n$. Our second result above can be refined to require only a weaker vertex expansion property with an efficient certificate.","Somewhat surprisingly, we observe that the analogous task of finding a linear-sized independent set in almost $4$-colorable one-sided expanders (even when the second eigenvalue is $o_n(1)$) is NP-hard, assuming the Unique Games Conjecture.   ","All prior algorithms that beat the worst-case guarantees for this problem rely on bottom eigenspace enumeration techniques (following the classical spectral methods of Alon and Kahale) and require two-sided expansion, meaning a bounded number of negative eigenvalues of magnitude $\\Omega(1)$. Such techniques naturally extend to almost $k$-colorable graphs for any constant $k$, in contrast to analogous guarantees on one-sided expanders, which are Unique Games-hard to achieve for $k \\geq 4$.   ","Our rounding builds on the method of simulating multiple samples from a pseudodistribution introduced by Barak et.","al. for rounding Unique Games instances.","The key to our analysis is a new clustering property of large independent sets in expanding graphs - every large independent set has a larger-than-expected intersection with some member of a small list - and its formalization in the low-degree sum-of-squares proof system."],"url":"http://arxiv.org/abs/2405.10238v1","category":"cs.DS"}
{"created":"2024-05-16 16:37:23","title":"A systematic path to non-Markovian dynamics II: Probabilistic response of nonlinear multidimensional systems to Gaussian colored noise excitation","abstract":"The probabilistic characterization of non-Markovian responses to nonlinear dynamical systems under colored excitation is an important issue, arising in many applications. Extending the Fokker-Planck-Kolmogorov equation, governing the first-order response probability density function (pdf), to this case is a complicated task calling for special treatment. In this work, a new pdf-evolution equation is derived for the response of nonlinear dynamical systems under additive colored Gaussian noise. The derivation is based on the Stochastic Liouville equation (SLE), transformed, by means of an extended version of the Novikov-Furutsu theorem, to an exact yet non-closed equation, involving averages over the history of the functional derivatives of the non-Markovian response with respect to the excitation. The latter are calculated exactly by means of the state-transition matrix of variational, time-varying systems. Subsequently, an approximation scheme is implemented, relying on a decomposition of the state-transition matrix in its instantaneous mean value and its fluctuation around it. By a current-time approximation to the latter, we obtain our final equation, in which the effect of the instantaneous mean value of the response is maintained, rendering it nonlinear and non-local in time. Numerical results for the response pdf are provided for a bistable Duffing oscillator, under Gaussian excitation. The pdfs obtained from the solution of the novel equation and a simpler small correlation time (SCT) pdf-evolution equation are compared to Monde Carlo (MC) simulations. The novel equation outperforms the SCT equation as the excitation correlation time increases, keeping good agreement with the MC simulations.","sentences":["The probabilistic characterization of non-Markovian responses to nonlinear dynamical systems under colored excitation is an important issue, arising in many applications.","Extending the Fokker-Planck-Kolmogorov equation, governing the first-order response probability density function (pdf), to this case is a complicated task calling for special treatment.","In this work, a new pdf-evolution equation is derived for the response of nonlinear dynamical systems under additive colored Gaussian noise.","The derivation is based on the Stochastic Liouville equation (SLE), transformed, by means of an extended version of the Novikov-Furutsu theorem, to an exact yet non-closed equation, involving averages over the history of the functional derivatives of the non-Markovian response with respect to the excitation.","The latter are calculated exactly by means of the state-transition matrix of variational, time-varying systems.","Subsequently, an approximation scheme is implemented, relying on a decomposition of the state-transition matrix in its instantaneous mean value and its fluctuation around it.","By a current-time approximation to the latter, we obtain our final equation, in which the effect of the instantaneous mean value of the response is maintained, rendering it nonlinear and non-local in time.","Numerical results for the response pdf are provided for a bistable Duffing oscillator, under Gaussian excitation.","The pdfs obtained from the solution of the novel equation and a simpler small correlation time (SCT) pdf-evolution equation are compared to Monde Carlo (MC) simulations.","The novel equation outperforms the SCT equation as the excitation correlation time increases, keeping good agreement with the MC simulations."],"url":"http://arxiv.org/abs/2405.10236v1","category":"math-ph"}
{"created":"2024-05-16 15:38:59","title":"Forte: A Suite of Advanced Multireference Quantum Chemistry Methods","abstract":"Forte is an open-source library specialized in multireference electronic structure theories for molecular systems and the rapid prototyping of new methods. This paper gives an overview of the capabilities of Forte, its software architecture, and examples of applications enabled by the methods it implements.","sentences":["Forte is an open-source library specialized in multireference electronic structure theories for molecular systems and the rapid prototyping of new methods.","This paper gives an overview of the capabilities of Forte, its software architecture, and examples of applications enabled by the methods it implements."],"url":"http://arxiv.org/abs/2405.10197v1","category":"physics.chem-ph"}
{"created":"2024-05-16 15:38:55","title":"Searching for the QCD critical endpoint using multi-point Pad\u00e9 approximations","abstract":"Using the multi-point Pad\\'e approach, we locate Lee-Yang edge singularities of the QCD pressure in the complex baryon chemical potential plane. These singularities are extracted from singularities in the net baryon-number density calculated in $N_f=2+1$ lattice QCD at physical quark mass and purely imaginary chemical potential. Taking an appropriate scaling ansatz in the vicinity of the conjectured QCD critical endpoint, we extrapolate the singularities on $N_\\tau=6$ lattices to pure real baryon chemical potential to estimate the position of the critical endpoint (CEP). We find $T^{\\rm CEP}=105^{+8}_{-18}$~ MeV and $\\mu_B^{\\rm CEP} = 422^{+80}_{-35}$~ MeV, which compares well with recent estimates in the literature. For the slope of the transition line at the critical point we find $-0.16(24)$.","sentences":["Using the multi-point Pad\\'e approach, we locate Lee-Yang edge singularities of the QCD pressure in the complex baryon chemical potential plane.","These singularities are extracted from singularities in the net baryon-number density calculated in $N_f=2+1$ lattice QCD at physical quark mass and purely imaginary chemical potential.","Taking an appropriate scaling ansatz in the vicinity of the conjectured QCD critical endpoint, we extrapolate the singularities on $N_\\tau=6$ lattices to pure real baryon chemical potential to estimate the position of the critical endpoint (CEP).","We find $T^{\\rm CEP}=105^{+8}_{-18}$~ MeV and $\\mu_B^{\\rm CEP} = 422^{+80}_{-35}$~ MeV, which compares well with recent estimates in the literature.","For the slope of the transition line at the critical point we find $-0.16(24)$."],"url":"http://arxiv.org/abs/2405.10196v1","category":"hep-lat"}
{"created":"2024-05-16 15:38:19","title":"Formation pathways of the compact stellar systems","abstract":"The formation pathways of compact stellar systems (CSSs) are still under debate. We utilize the \\NH\\ simulation to investigate the origins of such objects in the field environment. We identified 55 CSS candidates in the simulation whose properties are similar to those of the observed ultra-compact dwarfs and compact ellipticals. All but two most massive objects (compact elliptical candidates) are a result of a short starburst. Sixteen are formed by tidal stripping, while the other 39 are intrinsically compact from their birth. The stripped objects originate from dwarf-like galaxies with a dark halo, but most of their dark matter is stripped through their orbital motion around a more massive neighbor galaxy. The 39 intrinsically compact systems are further divided into ``associated'' or ``isolated'' groups, depending on whether they were born near a massive dark halo or not. The isolated intrinsic compact objects (7) are born in a dark halo and their stellar properties are older and metal-poor compared to the associated counterparts (32). The stripped compact objects occupy a distinct region in the age-metallicity plane from the intrinsic compact objects. The associated intrinsic compact objects in our sample have never had a dark halo; they are the surviving star clumps of a massive galaxy.","sentences":["The formation pathways of compact stellar systems (CSSs) are still under debate.","We utilize the \\NH\\ simulation to investigate the origins of such objects in the field environment.","We identified 55 CSS candidates in the simulation whose properties are similar to those of the observed ultra-compact dwarfs and compact ellipticals.","All but two most massive objects (compact elliptical candidates) are a result of a short starburst.","Sixteen are formed by tidal stripping, while the other 39 are intrinsically compact from their birth.","The stripped objects originate from dwarf-like galaxies with a dark halo, but most of their dark matter is stripped through their orbital motion around a more massive neighbor galaxy.","The 39 intrinsically compact systems are further divided into ``associated'' or ``isolated'' groups, depending on whether they were born near a massive dark halo or not.","The isolated intrinsic compact objects (7) are born in a dark halo and their stellar properties are older and metal-poor compared to the associated counterparts (32).","The stripped compact objects occupy a distinct region in the age-metallicity plane from the intrinsic compact objects.","The associated intrinsic compact objects in our sample have never had a dark halo; they are the surviving star clumps of a massive galaxy."],"url":"http://arxiv.org/abs/2405.10195v1","category":"astro-ph.GA"}
{"created":"2024-05-16 15:31:28","title":"Influence Maximization in Hypergraphs using Multi-Objective Evolutionary Algorithms","abstract":"The Influence Maximization (IM) problem is a well-known NP-hard combinatorial problem over graphs whose goal is to find the set of nodes in a network that spreads influence at most. Among the various methods for solving the IM problem, evolutionary algorithms (EAs) have been shown to be particularly effective. While the literature on the topic is particularly ample, only a few attempts have been made at solving the IM problem over higher-order networks, namely extensions of standard graphs that can capture interactions that involve more than two nodes. Hypergraphs are a valuable tool for modeling complex interaction networks in various domains; however, they require rethinking of several graph-based problems, including IM. In this work, we propose a multi-objective EA for the IM problem over hypergraphs that leverages smart initialization and hypergraph-aware mutation. While the existing methods rely on greedy or heuristic methods, to our best knowledge this is the first attempt at applying EAs to this problem. Our results over nine real-world datasets and three propagation models, compared with five baseline algorithms, reveal that our method achieves in most cases state-of-the-art results in terms of hypervolume and solution diversity.","sentences":["The Influence Maximization (IM) problem is a well-known NP-hard combinatorial problem over graphs whose goal is to find the set of nodes in a network that spreads influence at most.","Among the various methods for solving the IM problem, evolutionary algorithms (EAs) have been shown to be particularly effective.","While the literature on the topic is particularly ample, only a few attempts have been made at solving the IM problem over higher-order networks, namely extensions of standard graphs that can capture interactions that involve more than two nodes.","Hypergraphs are a valuable tool for modeling complex interaction networks in various domains; however, they require rethinking of several graph-based problems, including IM.","In this work, we propose a multi-objective EA for the IM problem over hypergraphs that leverages smart initialization and hypergraph-aware mutation.","While the existing methods rely on greedy or heuristic methods, to our best knowledge this is the first attempt at applying EAs to this problem.","Our results over nine real-world datasets and three propagation models, compared with five baseline algorithms, reveal that our method achieves in most cases state-of-the-art results in terms of hypervolume and solution diversity."],"url":"http://arxiv.org/abs/2405.10187v1","category":"cs.SI"}
{"created":"2024-05-16 15:28:10","title":"Analysis of singularly perturbed stochastic chemical reaction networks motivated by applications to epigenetic cell memory","abstract":"Epigenetic cell memory, the inheritance of gene expression patterns across subsequent cell divisions, is a critical property of multi-cellular organisms. In recent work [10], a subset of the authors observed in a simulation study how the stochastic dynamics and time-scale differences between establishment and erasure processes in chromatin modifications (such as histone modifications and DNA methylation) can have a critical effect on epigenetic cell memory. In this paper, we provide a mathematical framework to rigorously validate and extend beyond these computational findings. Viewing our stochastic model of a chromatin modification circuit as a singularly perturbed, finite state, continuous time Markov chain, we extend beyond existing theory in order to characterize the leading coefficients in the series expansions of stationary distributions and mean first passage times. In particular, we characterize the limiting stationary distribution in terms of a reduced Markov chain, provide an algorithm to determine the orders of the poles of mean first passage times, and determine how changing erasure rates affects system behavior. The theoretical tools developed in this paper not only allow us to set a rigorous mathematical basis for the computational findings of our prior work, highlighting the effect of chromatin modification dynamics on epigenetic cell memory, but they can also be applied to other singularly perturbed Markov chains beyond the applications in this paper, especially those associated with chemical reaction networks.","sentences":["Epigenetic cell memory, the inheritance of gene expression patterns across subsequent cell divisions, is a critical property of multi-cellular organisms.","In recent work [10], a subset of the authors observed in a simulation study how the stochastic dynamics and time-scale differences between establishment and erasure processes in chromatin modifications (such as histone modifications and DNA methylation) can have a critical effect on epigenetic cell memory.","In this paper, we provide a mathematical framework to rigorously validate and extend beyond these computational findings.","Viewing our stochastic model of a chromatin modification circuit as a singularly perturbed, finite state, continuous time Markov chain, we extend beyond existing theory in order to characterize the leading coefficients in the series expansions of stationary distributions and mean first passage times.","In particular, we characterize the limiting stationary distribution in terms of a reduced Markov chain, provide an algorithm to determine the orders of the poles of mean first passage times, and determine how changing erasure rates affects system behavior.","The theoretical tools developed in this paper not only allow us to set a rigorous mathematical basis for the computational findings of our prior work, highlighting the effect of chromatin modification dynamics on epigenetic cell memory, but they can also be applied to other singularly perturbed Markov chains beyond the applications in this paper, especially those associated with chemical reaction networks."],"url":"http://arxiv.org/abs/2405.10184v1","category":"math.PR"}
{"created":"2024-05-16 15:23:10","title":"Phase behavior of metastable water from large-scale simulations of a quantitative accurate model: The liquid-liquid critical point","abstract":"Water's unique anomalies are vital in various applications and biological processes, yet the molecular mechanisms behind these anomalies remain debated, particularly in the metastable liquid phase under supercooling and stretching conditions. Experimental challenges in these conditions have led to simulations suggesting a liquid-liquid phase transition between low-density and high-density water phases, culminating in a liquid-liquid critical point (LLCP). However, these simulations are limited by computational expense, small system sizes, and reliability of water models. Using the FS model, we improve accuracy in predicting water's density and response functions across a broad range of temperatures and pressures. The FS model avoid by design first-order phase transitions towards crystalline phases, allowing thorough exploration of the metastable phase diagram. We employ advanced numerical techniques to bypass dynamical slowing down and perform finite-size scaling on systems significantly larger than those used in previous analyses. Our study extrapolates thermodynamic behavior in the infinite-system limit, accurately demonstrating the existence of the LLCP in the 3D Ising universality class at TC = 186 +/- 4 K and PC = 174 +/- 14 MPa, following a liquid-liquid phase separation below 200 MPa. These predictions align with recent experimental data and more sophisticated models, highlighting that hydrogen bond cooperativity governs the LLCP and the origin of water anomalies. Moreover, we observe that the hydrogen bond network exhibits substantial cooperative fluctuations at scales larger than 10 nm, even at temperatures relevant to biopreservation. These findings have significant implications for fields such as nanotechnology and biophysics, offering new insights into water's behavior under varied conditions.","sentences":["Water's unique anomalies are vital in various applications and biological processes, yet the molecular mechanisms behind these anomalies remain debated, particularly in the metastable liquid phase under supercooling and stretching conditions.","Experimental challenges in these conditions have led to simulations suggesting a liquid-liquid phase transition between low-density and high-density water phases, culminating in a liquid-liquid critical point (LLCP).","However, these simulations are limited by computational expense, small system sizes, and reliability of water models.","Using the FS model, we improve accuracy in predicting water's density and response functions across a broad range of temperatures and pressures.","The FS model avoid by design first-order phase transitions towards crystalline phases, allowing thorough exploration of the metastable phase diagram.","We employ advanced numerical techniques to bypass dynamical slowing down and perform finite-size scaling on systems significantly larger than those used in previous analyses.","Our study extrapolates thermodynamic behavior in the infinite-system limit, accurately demonstrating the existence of the LLCP in the 3D Ising universality class at TC = 186 +/- 4 K and PC = 174 +/- 14 MPa, following a liquid-liquid phase separation below 200 MPa.","These predictions align with recent experimental data and more sophisticated models, highlighting that hydrogen bond cooperativity governs the LLCP and the origin of water anomalies.","Moreover, we observe that the hydrogen bond network exhibits substantial cooperative fluctuations at scales larger than 10 nm, even at temperatures relevant to biopreservation.","These findings have significant implications for fields such as nanotechnology and biophysics, offering new insights into water's behavior under varied conditions."],"url":"http://arxiv.org/abs/2405.10181v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-16 15:16:33","title":"Topological, multi-mode amplification induced by non-reciprocal, long-range dissipative couplings","abstract":"Non-reciprocal couplings or drivings are known to induce steady-state, directional, amplification in driven-dissipative bosonic lattices. This amplification phenomena has been recently linked to the existence of a non-zero topological invariant defined with the system's dynamical matrix, and thus, it depends critically on the couplings' structure. In this work, we demonstrate the emergence of unconventional, non-reciprocal, long-range dissipative couplings induced by the interaction of the bosonic chain with a chiral, multi-mode channel, and then study their impact on topological amplification phenomena. We show that these couplings can lead to topological invariant values greater than one which induce topological, multi-mode amplification and metastability behaviour not predicted in other setups. Besides, we also show how these couplings can also stabilize topological amplifying phases in the presence of local parametric drivings. Finally, we conclude by showing how such phenomena can be naturally obtained in two-dimensional topological insulators hosting multiple edge modes.","sentences":["Non-reciprocal couplings or drivings are known to induce steady-state, directional, amplification in driven-dissipative bosonic lattices.","This amplification phenomena has been recently linked to the existence of a non-zero topological invariant defined with the system's dynamical matrix, and thus, it depends critically on the couplings' structure.","In this work, we demonstrate the emergence of unconventional, non-reciprocal, long-range dissipative couplings induced by the interaction of the bosonic chain with a chiral, multi-mode channel, and then study their impact on topological amplification phenomena.","We show that these couplings can lead to topological invariant values greater than one which induce topological, multi-mode amplification and metastability behaviour not predicted in other setups.","Besides, we also show how these couplings can also stabilize topological amplifying phases in the presence of local parametric drivings.","Finally, we conclude by showing how such phenomena can be naturally obtained in two-dimensional topological insulators hosting multiple edge modes."],"url":"http://arxiv.org/abs/2405.10176v1","category":"quant-ph"}
{"created":"2024-05-16 15:09:28","title":"Asymmetric Transmission through Polarization-Controlled Riemann Surface Evolution Path","abstract":"Exceptional points (EPs) are degeneracies at which two or more eigenvalues and eigenstates of a non-Hermtian system coalesce. Dynamically encircling EPs leads to chiral mode switching, where the final state of the system depends on the encircling handedness. In conventional schemes, the encircling direction is the same for different polarization states of the same mode, resulting in the same chiral transmission. Here, by mapping evolution path onto a double couple L-shaped silicon waveguide, we demonstrate opposite evolution directions for modes with different polarizations, thus achieving opposite asymmetric transmission results. Our work shows that the polarization state of the input mode can also serve as a variable parameter influencing the evolution path on the Riemann surfaces.","sentences":["Exceptional points (EPs) are degeneracies at which two or more eigenvalues and eigenstates of a non-Hermtian system coalesce.","Dynamically encircling EPs leads to chiral mode switching, where the final state of the system depends on the encircling handedness.","In conventional schemes, the encircling direction is the same for different polarization states of the same mode, resulting in the same chiral transmission.","Here, by mapping evolution path onto a double couple L-shaped silicon waveguide, we demonstrate opposite evolution directions for modes with different polarizations, thus achieving opposite asymmetric transmission results.","Our work shows that the polarization state of the input mode can also serve as a variable parameter influencing the evolution path on the Riemann surfaces."],"url":"http://arxiv.org/abs/2405.10173v1","category":"physics.optics"}
{"created":"2024-05-16 15:08:09","title":"Fixed points of maps and nontrivial weak solutions to a class of nonlinear strongly coupled elliptic systems","abstract":"Local indices at isolated fixed points of a differentiable compact nonlinear map $T$ on Banach spaces will be discussed. These results are applied to establish the existence of nontrivial solutions. As an example, the existence of nontrivial weak solutions to a class of nonlinear strongly coupled elliptic systems of two equations will also be studied. A bifurcation phenomenon due to cross-diffusion is reported.","sentences":["Local indices at isolated fixed points of a differentiable compact nonlinear map $T$ on Banach spaces will be discussed.","These results are applied to establish the existence of nontrivial solutions.","As an example, the existence of nontrivial weak solutions to a class of nonlinear strongly coupled elliptic systems of two equations will also be studied.","A bifurcation phenomenon due to cross-diffusion is reported."],"url":"http://arxiv.org/abs/2405.10171v1","category":"math.AP"}
{"created":"2024-05-16 15:07:56","title":"A Mess of Memory System Benchmarking, Simulation and Application Profiling","abstract":"The Memory stress (Mess) framework provides a unified view of the memory system benchmarking, simulation and application profiling. The Mess benchmark provides a holistic and detailed memory system characterization. It is based on hundreds of measurements that are represented as a family of bandwidth--latency curves. The benchmark increases the coverage of all the previous tools and leads to new findings in the behavior of the actual and simulated memory systems. We deploy the Mess benchmark to characterize Intel, AMD, IBM, Fujitsu, Amazon and NVIDIA servers with DDR4, DDR5, HBM2 and HBM2E memory. The Mess memory simulator uses bandwidth--latency concept for the memory performance simulation. We integrate Mess with widely-used CPUs simulators enabling modeling of all high-end memory technologies. The Mess simulator is fast, easy to integrate and it closely matches the actual system performance. By design, it enables a quick adoption of new memory technologies in hardware simulators. Finally, the Mess application profiling positions the application in the bandwidth--latency space of the target memory system. This information can be correlated with other application runtime activities and the source code, leading to a better overall understanding of the application's behavior. The current Mess benchmark release covers all major CPU and GPU ISAs, x86, ARM, Power, RISC-V, and NVIDIA's PTX. We also release as open source the ZSim, gem5 and OpenPiton Metro-MPI integrated with the Mess memory simulator for DDR4, DDR5, Optane, HBM2, HBM2E and CXL memory expanders. The Mess application profiling is already integrated into a suite of production HPC performance analysis tools.","sentences":["The Memory stress (Mess) framework provides a unified view of the memory system benchmarking, simulation and application profiling.","The Mess benchmark provides a holistic and detailed memory system characterization.","It is based on hundreds of measurements that are represented as a family of bandwidth--latency curves.","The benchmark increases the coverage of all the previous tools and leads to new findings in the behavior of the actual and simulated memory systems.","We deploy the Mess benchmark to characterize Intel, AMD, IBM, Fujitsu, Amazon and NVIDIA servers with DDR4, DDR5, HBM2 and HBM2E memory.","The Mess memory simulator uses bandwidth--latency concept for the memory performance simulation.","We integrate Mess with widely-used CPUs simulators enabling modeling of all high-end memory technologies.","The Mess simulator is fast, easy to integrate and it closely matches the actual system performance.","By design, it enables a quick adoption of new memory technologies in hardware simulators.","Finally, the Mess application profiling positions the application in the bandwidth--latency space of the target memory system.","This information can be correlated with other application runtime activities and the source code, leading to a better overall understanding of the application's behavior.","The current Mess benchmark release covers all major CPU and GPU ISAs, x86, ARM, Power, RISC-V, and NVIDIA's PTX.","We also release as open source the ZSim, gem5 and OpenPiton Metro-MPI integrated with the Mess memory simulator for DDR4, DDR5, Optane, HBM2, HBM2E and CXL memory expanders.","The Mess application profiling is already integrated into a suite of production HPC performance analysis tools."],"url":"http://arxiv.org/abs/2405.10170v1","category":"cs.AR"}
{"created":"2024-05-16 15:03:40","title":"Near Uniform Triangle Sampling Over Adjacency List Graph Streams","abstract":"Triangle counting and sampling are two fundamental problems for streaming algorithms. Arguably, designing sampling algorithms is more challenging than their counting variants. It may be noted that triangle counting has received far greater attention in the literature than the sampling variant. In this work, we consider the problem of approximately sampling triangles in different models of streaming with the focus being on the adjacency list model.   In this problem, the edges of a graph $G$ will arrive over a data stream. The goal is to design efficient streaming algorithms that can sample and output a triangle from a distribution, over the triangles in $G$, that is close to the uniform distribution over the triangles in $G$. The distance between distributions is measured in terms of $\\ell_1$-distance. The main technical contribution of this paper is to design algorithms for this triangle sampling problem in the adjacency list model with the space complexities matching their counting variants. For the sake of completeness, we also show results on the vertex and edge arrival models.","sentences":["Triangle counting and sampling are two fundamental problems for streaming algorithms.","Arguably, designing sampling algorithms is more challenging than their counting variants.","It may be noted that triangle counting has received far greater attention in the literature than the sampling variant.","In this work, we consider the problem of approximately sampling triangles in different models of streaming with the focus being on the adjacency list model.   ","In this problem, the edges of a graph $G$ will arrive over a data stream.","The goal is to design efficient streaming algorithms that can sample and output a triangle from a distribution, over the triangles in $G$, that is close to the uniform distribution over the triangles in $G$. The distance between distributions is measured in terms of $\\ell_1$-distance.","The main technical contribution of this paper is to design algorithms for this triangle sampling problem in the adjacency list model with the space complexities matching their counting variants.","For the sake of completeness, we also show results on the vertex and edge arrival models."],"url":"http://arxiv.org/abs/2405.10167v1","category":"cs.DS"}
{"created":"2024-05-16 14:38:32","title":"Note on existence of physical measures for systems with mixed central behavior","abstract":"We show the existence of physical measures for $C^{\\infty}$ smooth instances of certain partially hyperbolic dynamics, both continuous and discrete, exhibiting mixed behavior (positive and negative Lyapunov exponents) along the central non-uniformly hyperbolic multidimensional invariant direction, as a consequence of assuming the existence of certain types of ''regular points'' on positive volume subsets. This includes the $C^3$ robust class of multidimensional non-hyperbolic attractors obtained by Viana, and the $C^1$ robust classes of $3$-sectionally hyperbolic wild strange attractors presented by Shilnikov and Turaev.","sentences":["We show the existence of physical measures for $C^{\\infty}$ smooth instances of certain partially hyperbolic dynamics, both continuous and discrete, exhibiting mixed behavior (positive and negative Lyapunov exponents) along the central non-uniformly hyperbolic multidimensional invariant direction, as a consequence of assuming the existence of certain types of ''regular points'' on positive volume subsets.","This includes the $C^3$ robust class of multidimensional non-hyperbolic attractors obtained by Viana, and the $C^1$ robust classes of $3$-sectionally hyperbolic wild strange attractors presented by Shilnikov and Turaev."],"url":"http://arxiv.org/abs/2405.10144v1","category":"math.DS"}
{"created":"2024-05-16 14:34:44","title":"Libra: Building Decoupled Vision System on Large Language Models","abstract":"In this work, we introduce Libra, a prototype model with a decoupled vision system on a large language model (LLM). The decoupled vision system decouples inner-modal modeling and cross-modal interaction, yielding unique visual information modeling and effective cross-modal comprehension. Libra is trained through discrete auto-regressive modeling on both vision and language inputs. Specifically, we incorporate a routed visual expert with a cross-modal bridge module into a pretrained LLM to route the vision and language flows during attention computing to enable different attention patterns in inner-modal modeling and cross-modal interaction scenarios. Experimental results demonstrate that the dedicated design of Libra achieves a strong MLLM baseline that rivals existing works in the image-to-text scenario with merely 50 million training data, providing a new perspective for future multimodal foundation models. Code is available at https://github.com/YifanXu74/Libra.","sentences":["In this work, we introduce Libra, a prototype model with a decoupled vision system on a large language model (LLM).","The decoupled vision system decouples inner-modal modeling and cross-modal interaction, yielding unique visual information modeling and effective cross-modal comprehension.","Libra is trained through discrete auto-regressive modeling on both vision and language inputs.","Specifically, we incorporate a routed visual expert with a cross-modal bridge module into a pretrained LLM to route the vision and language flows during attention computing to enable different attention patterns in inner-modal modeling and cross-modal interaction scenarios.","Experimental results demonstrate that the dedicated design of Libra achieves a strong MLLM baseline that rivals existing works in the image-to-text scenario with merely 50 million training data, providing a new perspective for future multimodal foundation models.","Code is available at https://github.com/YifanXu74/Libra."],"url":"http://arxiv.org/abs/2405.10140v1","category":"cs.CV"}
{"created":"2024-05-16 14:29:56","title":"Cooperative Visual-LiDAR Extrinsic Calibration Technology for Intersection Vehicle-Infrastructure: A review","abstract":"In the typical urban intersection scenario, both vehicles and infrastructures are equipped with visual and LiDAR sensors. By successfully integrating the data from vehicle-side and road monitoring devices, a more comprehensive and accurate environmental perception and information acquisition can be achieved. The Calibration of sensors, as an essential component of autonomous driving technology, has consistently drawn significant attention. Particularly in scenarios involving multiple sensors collaboratively perceiving and addressing localization challenges, the requirement for inter-sensor calibration becomes crucial. Recent years have witnessed the emergence of the concept of multi-end cooperation, where infrastructure captures and transmits surrounding environment information to vehicles, bolstering their perception capabilities while mitigating costs. However, this also poses technical complexities, underscoring the pressing need for diverse end calibration. Camera and LiDAR, the bedrock sensors in autonomous driving, exhibit expansive applicability. This paper comprehensively examines and analyzes the calibration of multi-end camera-LiDAR setups from vehicle, roadside, and vehicle-road cooperation perspectives, outlining their relevant applications and profound significance. Concluding with a summary, we present our future-oriented ideas and hypotheses.","sentences":["In the typical urban intersection scenario, both vehicles and infrastructures are equipped with visual and LiDAR sensors.","By successfully integrating the data from vehicle-side and road monitoring devices, a more comprehensive and accurate environmental perception and information acquisition can be achieved.","The Calibration of sensors, as an essential component of autonomous driving technology, has consistently drawn significant attention.","Particularly in scenarios involving multiple sensors collaboratively perceiving and addressing localization challenges, the requirement for inter-sensor calibration becomes crucial.","Recent years have witnessed the emergence of the concept of multi-end cooperation, where infrastructure captures and transmits surrounding environment information to vehicles, bolstering their perception capabilities while mitigating costs.","However, this also poses technical complexities, underscoring the pressing need for diverse end calibration.","Camera and LiDAR, the bedrock sensors in autonomous driving, exhibit expansive applicability.","This paper comprehensively examines and analyzes the calibration of multi-end camera-LiDAR setups from vehicle, roadside, and vehicle-road cooperation perspectives, outlining their relevant applications and profound significance.","Concluding with a summary, we present our future-oriented ideas and hypotheses."],"url":"http://arxiv.org/abs/2405.10132v1","category":"cs.CV"}
{"created":"2024-05-16 14:22:54","title":"Smoothing Linear Codes by R\u00e9nyi Divergence and Applications to Security Reduction","abstract":"The concept of the smoothing parameter plays a crucial role in both lattice-based and code-based cryptography, primarily due to its effectiveness in achieving nearly uniform distributions through the addition of noise. Recent research by Pathegama and Barg has determined the optimal smoothing bound for random codes under R\\'enyi Divergence for any order $\\alpha \\in (1, \\infty)$ \\cite{pathegama2024r}. Considering the inherent complexity of encoding/decoding algorithms in random codes, our research introduces enhanced structural elements into these coding schemes. Specifically, this paper presents a novel derivation of the smoothing bound for random linear codes, maintaining the same order of R\\'enyi Divergence and achieving optimality for any $\\alpha\\in (1,\\infty)$. We extend this framework under KL Divergence by transitioning from random linear codes to random self-dual codes, and subsequently to random quasi-cyclic codes, incorporating progressively more structures. As an application, we derive an average-case to average-case reduction from the Learning Parity with Noise (LPN) problem to the average-case decoding problem. This reduction aligns with the parameter regime in \\cite{debris2022worst}, but uniquely employs R\\'enyi divergence and directly considers Bernoulli noise, instead of combining ball noise and Bernoulli noise.","sentences":["The concept of the smoothing parameter plays a crucial role in both lattice-based and code-based cryptography, primarily due to its effectiveness in achieving nearly uniform distributions through the addition of noise.","Recent research by Pathegama and Barg has determined the optimal smoothing bound for random codes under R\\'enyi Divergence for any order $\\alpha \\in (1, \\infty)$ \\cite{pathegama2024r}.","Considering the inherent complexity of encoding/decoding algorithms in random codes, our research introduces enhanced structural elements into these coding schemes.","Specifically, this paper presents a novel derivation of the smoothing bound for random linear codes, maintaining the same order of R\\'enyi Divergence and achieving optimality for any $\\alpha\\in (1,\\infty)$. We extend this framework under KL Divergence by transitioning from random linear codes to random self-dual codes, and subsequently to random quasi-cyclic codes, incorporating progressively more structures.","As an application, we derive an average-case to average-case reduction from the Learning Parity with Noise (LPN) problem to the average-case decoding problem.","This reduction aligns with the parameter regime in \\cite{debris2022worst}, but uniquely employs R\\'enyi divergence and directly considers Bernoulli noise, instead of combining ball noise and Bernoulli noise."],"url":"http://arxiv.org/abs/2405.10124v1","category":"cs.IT"}
{"created":"2024-05-16 14:19:13","title":"Imaging Spinor Bose Gasses Using Off-Axis Holography","abstract":"We introduce a novel, non-invasive imaging technique based on spin-dependent off-axis holography (SOAH) for spin-1 Bose-Einstein condensates (BECs). Utilizing a dual reference beam strategy, this method records two orthogonal circular polarization components of a single probe beam. The circular birefringence of spin-polarized atoms induces differing complex phase shifts in the polarization components of the light, which are reconstructed from the interference patterns captured on camera. Our approach enables spin- and density-resolved imaging of both phase and amplitude information \\emph{in-situ} on a sub-millisecond time scale with minimal disturbance to the condensate. We explore the technique's efficacy under various background static fields, demonstrating its sensitivity to the quantization axis of the atoms and confirming its effectiveness.","sentences":["We introduce a novel, non-invasive imaging technique based on spin-dependent off-axis holography (SOAH) for spin-1 Bose-Einstein condensates (BECs).","Utilizing a dual reference beam strategy, this method records two orthogonal circular polarization components of a single probe beam.","The circular birefringence of spin-polarized atoms induces differing complex phase shifts in the polarization components of the light, which are reconstructed from the interference patterns captured on camera.","Our approach enables spin- and density-resolved imaging of both phase and amplitude information \\emph{in-situ} on a sub-millisecond time scale with minimal disturbance to the condensate.","We explore the technique's efficacy under various background static fields, demonstrating its sensitivity to the quantization axis of the atoms and confirming its effectiveness."],"url":"http://arxiv.org/abs/2405.10120v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-16 14:15:44","title":"Applications of Quantum Machine Learning for Quantitative Finance","abstract":"Machine learning and quantum machine learning (QML) have gained significant importance, as they offer powerful tools for tackling complex computational problems across various domains. This work gives an extensive overview of QML uses in quantitative finance, an important discipline in the financial industry. We examine the connection between quantum computing and machine learning in financial applications, spanning a range of use cases including fraud detection, underwriting, Value at Risk, stock market prediction, portfolio optimization, and option pricing by overviewing the corpus of literature concerning various financial subdomains.","sentences":["Machine learning and quantum machine learning (QML) have gained significant importance, as they offer powerful tools for tackling complex computational problems across various domains.","This work gives an extensive overview of QML uses in quantitative finance, an important discipline in the financial industry.","We examine the connection between quantum computing and machine learning in financial applications, spanning a range of use cases including fraud detection, underwriting, Value at Risk, stock market prediction, portfolio optimization, and option pricing by overviewing the corpus of literature concerning various financial subdomains."],"url":"http://arxiv.org/abs/2405.10119v1","category":"quant-ph"}
{"created":"2024-05-16 14:11:00","title":"On the coupled Maxwell-Bloch system of equations with non-decaying fields at infinity","abstract":"We study an initial-boundary-value problem (IBVP) for a system of coupled Maxwell-Bloch equations (CMBE) that model two colors or polarizations of light resonantly interacting with a degenerate, two-level, active optical medium with an excited state and a pair of degenerate ground states. We assume that the electromagnetic field approaches non-vanishing plane waves in the far past and future. This type of interaction has been found to underlie nonlinear optical phenomena including electromagnetically induced transparency, slow light, stopped light, and quantum memory. Under the assumptions of unidirectional, lossless propagation of slowly-modulated plane waves, the resulting CMBE become completely integrable in the sense of possessing a Lax Pair. In this paper, we formulate an inverse scattering transform (IST) corresponding to these CMBE and their Lax pair, allowing for the spectral line of the atomic transitions in the active medium to have a finite width. The scattering problem for this Lax pair is the same as for the Manakov system. The main advancement in this IST for CMBE is calculating the nontrivial spatial propagation of the spectral data and determining the state of the optical medium in the distant future from that in the distant past, which is needed for the complete formulation of the IBVP. The Riemann-Hilbert problem is used to extract the spatio-temporal dependence of the solution from the evolving spectral data. We further derive and analyze several types of solitons and determine their velocity and stability, as well as find dark states of the medium which fail to interact with a given pulse.","sentences":["We study an initial-boundary-value problem (IBVP) for a system of coupled Maxwell-Bloch equations (CMBE) that model two colors or polarizations of light resonantly interacting with a degenerate, two-level, active optical medium with an excited state and a pair of degenerate ground states.","We assume that the electromagnetic field approaches non-vanishing plane waves in the far past and future.","This type of interaction has been found to underlie nonlinear optical phenomena including electromagnetically induced transparency, slow light, stopped light, and quantum memory.","Under the assumptions of unidirectional, lossless propagation of slowly-modulated plane waves, the resulting CMBE become completely integrable in the sense of possessing a Lax Pair.","In this paper, we formulate an inverse scattering transform (IST) corresponding to these CMBE and their Lax pair, allowing for the spectral line of the atomic transitions in the active medium to have a finite width.","The scattering problem for this Lax pair is the same as for the Manakov system.","The main advancement in this IST for CMBE is calculating the nontrivial spatial propagation of the spectral data and determining the state of the optical medium in the distant future from that in the distant past, which is needed for the complete formulation of the IBVP.","The Riemann-Hilbert problem is used to extract the spatio-temporal dependence of the solution from the evolving spectral data.","We further derive and analyze several types of solitons and determine their velocity and stability, as well as find dark states of the medium which fail to interact with a given pulse."],"url":"http://arxiv.org/abs/2405.10117v1","category":"nlin.SI"}
{"created":"2024-05-16 13:50:46","title":"The Effect of Quantization in Federated Learning: A R\u00e9nyi Differential Privacy Perspective","abstract":"Federated Learning (FL) is an emerging paradigm that holds great promise for privacy-preserving machine learning using distributed data. To enhance privacy, FL can be combined with Differential Privacy (DP), which involves adding Gaussian noise to the model weights. However, FL faces a significant challenge in terms of large communication overhead when transmitting these model weights. To address this issue, quantization is commonly employed. Nevertheless, the presence of quantized Gaussian noise introduces complexities in understanding privacy protection. This research paper investigates the impact of quantization on privacy in FL systems. We examine the privacy guarantees of quantized Gaussian mechanisms using R\\'enyi Differential Privacy (RDP). By deriving the privacy budget of quantized Gaussian mechanisms, we demonstrate that lower quantization bit levels provide improved privacy protection. To validate our theoretical findings, we employ Membership Inference Attacks (MIA), which gauge the accuracy of privacy leakage. The numerical results align with our theoretical analysis, confirming that quantization can indeed enhance privacy protection. This study not only enhances our understanding of the correlation between privacy and communication in FL but also underscores the advantages of quantization in preserving privacy.","sentences":["Federated Learning (FL) is an emerging paradigm that holds great promise for privacy-preserving machine learning using distributed data.","To enhance privacy, FL can be combined with Differential Privacy (DP), which involves adding Gaussian noise to the model weights.","However, FL faces a significant challenge in terms of large communication overhead when transmitting these model weights.","To address this issue, quantization is commonly employed.","Nevertheless, the presence of quantized Gaussian noise introduces complexities in understanding privacy protection.","This research paper investigates the impact of quantization on privacy in FL systems.","We examine the privacy guarantees of quantized Gaussian mechanisms using R\\'enyi Differential Privacy (RDP).","By deriving the privacy budget of quantized Gaussian mechanisms, we demonstrate that lower quantization bit levels provide improved privacy protection.","To validate our theoretical findings, we employ Membership Inference Attacks (MIA), which gauge the accuracy of privacy leakage.","The numerical results align with our theoretical analysis, confirming that quantization can indeed enhance privacy protection.","This study not only enhances our understanding of the correlation between privacy and communication in FL but also underscores the advantages of quantization in preserving privacy."],"url":"http://arxiv.org/abs/2405.10096v1","category":"cs.LG"}
{"created":"2024-05-16 13:26:16","title":"Two person non-zero-sum linear-quadratic differential game with Markovian jumps in infinite horizon","abstract":"This paper investigates an inhomogeneous non-zero-sum linear-quadratic (LQ, for short) differential game problem whose state process and cost functional are regulated by a Markov chain. Under the $L^2$ stabilizability framework, we first provide a sufficient condition to ensure the $L^2$-integrability of the state process and study a class of linear backward stochastic differential equation (BSDE, for short) in infinite horizon. Then, we seriously discuss the LQ problem and show that the closed-loop optimal control is characterized by the solutions to coupled algebra Riccati equations (CAREs, for short) with some stabilizing conditions and a linear BSDE. Based on those results, we further analyze the non-zero-sum stochastic differential game problem and give the closed-loop Nash equilibrium through the solution to a system of two cross-coupled CAREs and two cross-coupled BSDEs. Finally, some related numerical","sentences":["This paper investigates an inhomogeneous non-zero-sum linear-quadratic (LQ, for short) differential game problem whose state process and cost functional are regulated by a Markov chain.","Under the $L^2$ stabilizability framework, we first provide a sufficient condition to ensure the $L^2$-integrability of the state process and study a class of linear backward stochastic differential equation (BSDE, for short) in infinite horizon.","Then, we seriously discuss the LQ problem and show that the closed-loop optimal control is characterized by the solutions to coupled algebra Riccati equations (CAREs, for short) with some stabilizing conditions and a linear BSDE.","Based on those results, we further analyze the non-zero-sum stochastic differential game problem and give the closed-loop Nash equilibrium through the solution to a system of two cross-coupled CAREs and two cross-coupled BSDEs.","Finally, some related numerical"],"url":"http://arxiv.org/abs/2405.10083v1","category":"math.OC"}
{"created":"2024-05-16 13:22:04","title":"Emergence of moir\u00e9 superlattice potential in graphene by twisted-hBN layers","abstract":"Moir\\'e superlattices formed in stacks of two or more 2D crystals with similar lattice structures have recently become excellent platforms to reveal new physics in low-dimensional systems. They are, however, highly sensitive to the angle and lattice constant differences between the associated crystals, limiting the range of the material choice and the possible moir\\'e patterns for a given 2D crystal. Here, we present a novel approach to realize an atomically flat substrate with a periodic moir\\'e pattern that can induce the moir\\'e potential on the material on top by van der Waals (vdW) interactions, without suffering from the lattice and angle mismatch. By constructing a twisted hBN (thBN) moir\\'e substrate at an angle of about 1$^\\circ$, we show that the graphene on top, aligned around 15$^\\circ$ with the neighboring hBN layers, exhibits typical transport properties under a hexagonal moir\\'e potential, including multiple satellite Dirac points (DPs), Hofstadter butterfly effect, and Brown-Zak oscillations. All features point to the existence of the moir\\'e potential in graphene formed by thBN with $\\sim$1$^\\circ$ twist angle. Further statistical study shows that the twist from a parallel interface between the hBN layers is critical to induce the moir\\'e potential. Our study demonstrates that the thBN moir\\'e substrate can be used to investigate moir\\'e physics in arbitrary materials without being constrained by their lattice constants.","sentences":["Moir\\'e superlattices formed in stacks of two or more 2D crystals with similar lattice structures have recently become excellent platforms to reveal new physics in low-dimensional systems.","They are, however, highly sensitive to the angle and lattice constant differences between the associated crystals, limiting the range of the material choice and the possible moir\\'e patterns for a given 2D crystal.","Here, we present a novel approach to realize an atomically flat substrate with a periodic moir\\'e pattern that can induce the moir\\'e potential on the material on top by van der Waals (vdW) interactions, without suffering from the lattice and angle mismatch.","By constructing a twisted hBN (thBN) moir\\'e substrate at an angle of about 1$^\\circ$, we show that the graphene on top, aligned around 15$^\\circ$ with the neighboring hBN layers, exhibits typical transport properties under a hexagonal moir\\'e potential, including multiple satellite Dirac points (DPs), Hofstadter butterfly effect, and Brown-Zak oscillations.","All features point to the existence of the moir\\'e potential in graphene formed by thBN with $\\sim$1$^\\circ$ twist angle.","Further statistical study shows that the twist from a parallel interface between the hBN layers is critical to induce the moir\\'e potential.","Our study demonstrates that the thBN moir\\'e substrate can be used to investigate moir\\'e physics in arbitrary materials without being constrained by their lattice constants."],"url":"http://arxiv.org/abs/2405.10079v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 13:06:35","title":"Simplicial lists in operad theory I","abstract":"We define a category $\\mathsf{List}$ whose objects are sets and morphisms are mappings which assign to an element in the domain an ordered sequence (list) of elements in the codomain. We introduce and study a category of simplicial objects $\\mathsf{slist}$ whose objects are functors $\\Delta^{op} \\to \\mathsf{List}$, which we call simplicial lists, and morphisms are natural transformations which have functions as components. We demonstrate that $\\mathsf{sList}$ supports the combinatorics of (non-symmetric) operads by constructing a fully-faithful nerve functor $N^l : \\mathsf{Operad} \\to \\mathsf{sList}$ from the category of operads. This leads to a reasonable model for the theory of non-symmetric $\\infty$-operads.   We also demonstrate that $\\mathsf{sList}$ has the structure of a presheaf category. In particular, we study a subcategory $\\mathsf{sList}_{\\text{op}}$ of operadic simplicial lists, in which the nerve functor takes values. The latter category is also a presheaf category over a base whose objects may be interpreted as levelled trees. We construct a coherent nerve functor which outputs an $\\infty$-operad for each operad enriched in Kan complexes. We also define homology groups of simplicial lists and study first properties.","sentences":["We define a category $\\mathsf{List}$ whose objects are sets and morphisms are mappings which assign to an element in the domain an ordered sequence (list) of elements in the codomain.","We introduce and study a category of simplicial objects $\\mathsf{slist}$ whose objects are functors $\\Delta^{op} \\to \\mathsf{List}$, which we call simplicial lists, and morphisms are natural transformations which have functions as components.","We demonstrate that $\\mathsf{sList}$ supports the combinatorics of (non-symmetric) operads by constructing a fully-faithful nerve functor $N^l : \\mathsf{Operad} \\to \\mathsf{sList}$ from the category of operads.","This leads to a reasonable model for the theory of non-symmetric $\\infty$-operads.   ","We also demonstrate that $\\mathsf{sList}$ has the structure of a presheaf category.","In particular, we study a subcategory $\\mathsf{sList}_{\\text{op}}$ of operadic simplicial lists, in which the nerve functor takes values.","The latter category is also a presheaf category over a base whose objects may be interpreted as levelled trees.","We construct a coherent nerve functor which outputs an $\\infty$-operad for each operad enriched in Kan complexes.","We also define homology groups of simplicial lists and study first properties."],"url":"http://arxiv.org/abs/2405.10072v1","category":"math.AT"}
{"created":"2024-05-16 13:05:18","title":"Speckle Interferometry at SOAR in 2023","abstract":"Results of the speckle-interferometry observations at the 4.1 m Southern Astrophysical Research Telescope (SOAR) obtained during 2023 are presented: 1913 measurements of 1533 resolved pairs or subsystems (median separation 0.16\") and non-resolutions of 552 targets; 42 pairs are resolved here for the first time. This work continues our long-term effort to monitor orbital motion in close binaries and hierarchical systems. A large number (147) of orbits have been determined for the first time or updated using these measurements. Complementarity of this program with the Gaia mission is highlighted.","sentences":["Results of the speckle-interferometry observations at the 4.1 m Southern Astrophysical Research Telescope (SOAR) obtained during 2023 are presented: 1913 measurements of 1533 resolved pairs or subsystems (median separation 0.16\") and non-resolutions of 552 targets; 42 pairs are resolved here for the first time.","This work continues our long-term effort to monitor orbital motion in close binaries and hierarchical systems.","A large number (147) of orbits have been determined for the first time or updated using these measurements.","Complementarity of this program with the Gaia mission is highlighted."],"url":"http://arxiv.org/abs/2405.10071v1","category":"astro-ph.SR"}
{"created":"2024-05-16 12:54:17","title":"Meta results on data-driven control of nonlinear systems","abstract":"This note aims to provide a systematic understanding of direct data-driven control, enriching the existing literature not by adding another isolated result, but rather by offering a comprehensive, versatile, and unifying framework that sets the stage for future explorations and applications in this domain. To this end, we formulate the nonlinear design problem from a high-level perspective as a set of desired controlled systems and propose systematic procedures to synthesize data-driven control algorithms that meet the design requirements specified in the desired set. Various examples are presented to demonstrate the comprehensiveness and adaptability of the proposed approach.","sentences":["This note aims to provide a systematic understanding of direct data-driven control, enriching the existing literature not by adding another isolated result, but rather by offering a comprehensive, versatile, and unifying framework that sets the stage for future explorations and applications in this domain.","To this end, we formulate the nonlinear design problem from a high-level perspective as a set of desired controlled systems and propose systematic procedures to synthesize data-driven control algorithms that meet the design requirements specified in the desired set.","Various examples are presented to demonstrate the comprehensiveness and adaptability of the proposed approach."],"url":"http://arxiv.org/abs/2405.10064v1","category":"eess.SY"}
{"created":"2024-05-16 12:53:49","title":"Low-latency Symbol-Synchronous Communication for Multi-hop Sensor Networks","abstract":"Wireless sensor networks (WSNs) have received great interest due to their scalability, energy efficiency, and low-cost deployment. By utilizing multi-hop communication, WSNs can cover a wide area using low transmission power without the need for any communication infrastructure. Traditionally, WSNs rely on store-and-forward routing protocols and Time Division Multiple Access (TDMA)-based schedules that avoid interference between different wireless nodes. However, emerging challenging scenarios, such as the industrial Internet of Things (IoT) and robotic swarms, impose strict latency and reliability requirements, which traditional approaches cannot fulfill. In this paper, we propose a novel symbol-synchronous transmission design that provides reliable low-latency communication with a reasonable data rate on classical sub-6GHz RF frequency bands (e.g., the 2.4GHz ISM band). Instead of avoiding overlapping transmissions, the proposed scheme benefits from concurrent transmissions. Using simulation in MATLAB, we prove that the proposed design allows achieving a wire-like delay of 5ms for a 512-bit packet over multiple hops with only a 0.3% latency increase per extra hop and a low bit error rate (BER) of 0.04%. Compared to similar state-of-the-art approaches it can achieve a significantly higher data rate of 100kbps, which is expected to increase further with future improvements of the system.","sentences":["Wireless sensor networks (WSNs) have received great interest due to their scalability, energy efficiency, and low-cost deployment.","By utilizing multi-hop communication, WSNs can cover a wide area using low transmission power without the need for any communication infrastructure.","Traditionally, WSNs rely on store-and-forward routing protocols and Time Division Multiple Access (TDMA)-based schedules that avoid interference between different wireless nodes.","However, emerging challenging scenarios, such as the industrial Internet of Things (IoT) and robotic swarms, impose strict latency and reliability requirements, which traditional approaches cannot fulfill.","In this paper, we propose a novel symbol-synchronous transmission design that provides reliable low-latency communication with a reasonable data rate on classical sub-6GHz RF frequency bands (e.g., the 2.4GHz ISM band).","Instead of avoiding overlapping transmissions, the proposed scheme benefits from concurrent transmissions.","Using simulation in MATLAB, we prove that the proposed design allows achieving a wire-like delay of 5ms for a 512-bit packet over multiple hops with only a 0.3% latency increase per extra hop and a low bit error rate (BER) of 0.04%.","Compared to similar state-of-the-art approaches it can achieve a significantly higher data rate of 100kbps, which is expected to increase further with future improvements of the system."],"url":"http://arxiv.org/abs/2405.10063v1","category":"cs.NI"}
{"created":"2024-05-16 12:52:47","title":"Phenomenology of many-body localization in bond-disordered spin chains","abstract":"Many-body localization (MBL) hinders the thermalization of quantum many-body systems in the presence of strong disorder. In this work, we study the MBL regime in bond-disordered spin-1/2 XXZ spin chain, finding the multimodal distribution of entanglement entropy in eigenstates, sub-Poissonian level statistics, and revealing a relation between operators and initial states required for examining the breakdown of thermalization in the time evolution of the system. We employ a real space renormalization group scheme to identify these phenomenological features of the MBL regime that extend beyond the standard picture of local integrals of motion relevant for systems with disorder coupled to on-site operators. Our results pave the way for experimental probing of MBL in bond-disordered spin chains.","sentences":["Many-body localization (MBL) hinders the thermalization of quantum many-body systems in the presence of strong disorder.","In this work, we study the MBL regime in bond-disordered spin-1/2 XXZ spin chain, finding the multimodal distribution of entanglement entropy in eigenstates, sub-Poissonian level statistics, and revealing a relation between operators and initial states required for examining the breakdown of thermalization in the time evolution of the system.","We employ a real space renormalization group scheme to identify these phenomenological features of the MBL regime that extend beyond the standard picture of local integrals of motion relevant for systems with disorder coupled to on-site operators.","Our results pave the way for experimental probing of MBL in bond-disordered spin chains."],"url":"http://arxiv.org/abs/2405.10062v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-16 12:47:55","title":"Distributed Coloring in the SLEEPING Model","abstract":"In distributed network computing, a variant of the LOCAL model has been recently introduced, referred to as the SLEEPING model. In this model, nodes have the ability to decide on which round they are awake, and on which round they are sleeping. Two (adjacent) nodes can exchange messages in a round only if both of them are awake in that round. The SLEEPING model captures the ability of nodes to save energy when they are sleeping. In this framework, a major question is the following: is it possible to design algorithms that are energy efficient, i.e., where each node is awake for a few number of rounds only, without losing too much on the time efficiency, i.e., on the total number of rounds? This paper answers positively to this question, for one of the most fundamental problems in distributed network computing, namely $(\\Delta+1)$-coloring networks of maximum degree $\\Delta$. We provide a randomized algorithm with average awake-complexity constant, maximum awake-complexity $O(\\log\\log n)$ in $n$-node networks, and round-complexity $poly\\!\\log n$.","sentences":["In distributed network computing, a variant of the LOCAL model has been recently introduced, referred to as the SLEEPING model.","In this model, nodes have the ability to decide on which round they are awake, and on which round they are sleeping.","Two (adjacent) nodes can exchange messages in a round only if both of them are awake in that round.","The SLEEPING model captures the ability of nodes to save energy when they are sleeping.","In this framework, a major question is the following: is it possible to design algorithms that are energy efficient, i.e., where each node is awake for a few number of rounds only, without losing too much on the time efficiency, i.e., on the total number of rounds?","This paper answers positively to this question, for one of the most fundamental problems in distributed network computing, namely $(\\Delta+1)$-coloring networks of maximum degree $\\Delta$. We provide a randomized algorithm with average awake-complexity constant, maximum awake-complexity $O(\\log\\log n)$ in $n$-node networks, and round-complexity $poly\\!\\log n$."],"url":"http://arxiv.org/abs/2405.10058v1","category":"cs.DC"}
{"created":"2024-05-16 12:46:56","title":"AMECOS: A Modular Event-based Framework for Concurrent Object Specification","abstract":"In this work, we introduce a modular framework for specifying distributed systems that we call AMECOS. Specifically, our framework departs from the traditional use of sequential specification, which presents limitations both on the specification expressiveness and implementation efficiency of inherently concurrent objects, as documented by Casta{\\~n}eda, Rajsbaum and Raynal in CACM 2023. Our framework focuses on the interface between the various system components specified as concurrent objects. Interactions are described with sequences of object events. This provides a modular way of specifying distributed systems and separates legality (object semantics) from other issues, such as consistency. We demonstrate the usability of our framework by (i) specifying various well-known concurrent objects, such as shared memory, asynchronous message-passing, and reliable broadcast, (ii) providing hierarchies of ordering semantics (namely, consistency hierarchy, memory hierarchy, and reliable broadcast hierarchy), and (iii) presenting novel axiomatic proofs of the impossibility of the well-known Consensus and wait-free Set Agreement problems.","sentences":["In this work, we introduce a modular framework for specifying distributed systems that we call AMECOS.","Specifically, our framework departs from the traditional use of sequential specification, which presents limitations both on the specification expressiveness and implementation efficiency of inherently concurrent objects, as documented by Casta{\\~n}eda, Rajsbaum and Raynal in CACM 2023.","Our framework focuses on the interface between the various system components specified as concurrent objects.","Interactions are described with sequences of object events.","This provides a modular way of specifying distributed systems and separates legality (object semantics) from other issues, such as consistency.","We demonstrate the usability of our framework by (i) specifying various well-known concurrent objects, such as shared memory, asynchronous message-passing, and reliable broadcast, (ii) providing hierarchies of ordering semantics (namely, consistency hierarchy, memory hierarchy, and reliable broadcast hierarchy), and (iii) presenting novel axiomatic proofs of the impossibility of the well-known Consensus and wait-free Set Agreement problems."],"url":"http://arxiv.org/abs/2405.10057v1","category":"cs.DC"}
{"created":"2024-05-16 12:34:47","title":"Distribution of Test Statistic for Euclidean Distance Matrices","abstract":"Methods for global navigation satellite system fault detection using Euclidean Distance Matrices have been presented recently in the literature. Published methods define a test statistic in terms of eigenvalues of a certain matrix, but the distribution of the test statistic was not known, which presented a barrier to practical implementation. This document was a personal correspondence from Beatty to Derek Knowles. It includes a brief derivation of the distribution of the test statistic and a representative case showing that the theoretical distribution closely matches a simulated empirical distribution.","sentences":["Methods for global navigation satellite system fault detection using Euclidean Distance Matrices have been presented recently in the literature.","Published methods define a test statistic in terms of eigenvalues of a certain matrix, but the distribution of the test statistic was not known, which presented a barrier to practical implementation.","This document was a personal correspondence from Beatty to Derek Knowles.","It includes a brief derivation of the distribution of the test statistic and a representative case showing that the theoretical distribution closely matches a simulated empirical distribution."],"url":"http://arxiv.org/abs/2405.10049v1","category":"cs.RO"}
{"created":"2024-05-16 12:34:07","title":"Cohomologie de syst\u00e8mes locaux $p$-adiques sur les rev\u00eatements du demi-plan de Drinfeld","abstract":"Colmez, Dospinescu and Niziol have shown that the only $p$-adic representations of $\\rm{Gal}(\\bar{\\mathbb{Q}}_p/\\mathbb{Q}_p)$ appearing in the $p$-adic \\'etale cohomology of the coverings of Drinfeld's half-plane are the $2$-dimensional cuspidal representations (i.e. potentially semi-stable, whose associated Weil-Deligne representation is irreducible) with Hodge-Tate weights $0$ and $1$ and their multiplicities are given by the $p$-adic Langlands correspondence. We generalise this result to arbitrary weights, by considering the $p$-adic \\'etale cohomology with coefficients in the symmetric powers of the universal local system on Drinfeld's tower. A novelty is the appearance of potentially semistable $2$-dimensional non-cristabelian representations, with expected multiplicity. The key point is that the local systems we consider turn out to be particularly simple: they are \"isotrivial opers\" on a curve. We develop a recipe to compute the pro\\'etale cohomology of such a local system using the Hyodo-Kato cohomology of the curve and the de Rham complex of the flat filtered bundle associated to the local system.","sentences":["Colmez, Dospinescu and Niziol have shown that the only $p$-adic representations of $\\rm{Gal}(\\bar{\\mathbb{Q}}_p/\\mathbb{Q}_p)$ appearing in the $p$-adic \\'etale cohomology of the coverings of Drinfeld's half-plane are the $2$-dimensional cuspidal representations (i.e. potentially semi-stable, whose associated Weil-Deligne representation is irreducible) with Hodge-Tate weights $0$ and $1$ and their multiplicities are given by the $p$-adic Langlands correspondence.","We generalise this result to arbitrary weights, by considering the $p$-adic \\'etale cohomology with coefficients in the symmetric powers of the universal local system on Drinfeld's tower.","A novelty is the appearance of potentially semistable $2$-dimensional non-cristabelian representations, with expected multiplicity.","The key point is that the local systems we consider turn out to be particularly simple: they are \"isotrivial opers\" on a curve.","We develop a recipe to compute the pro\\'etale cohomology of such a local system using the Hyodo-Kato cohomology of the curve and the de Rham complex of the flat filtered bundle associated to the local system."],"url":"http://arxiv.org/abs/2405.10048v1","category":"math.NT"}
{"created":"2024-05-16 12:23:51","title":"Optimizing Search and Rescue UAV Connectivity in Challenging Terrain through Multi Q-Learning","abstract":"Using Unmanned Aerial Vehicles (UAVs) in Search and rescue operations (SAR) to navigate challenging terrain while maintaining reliable communication with the cellular network is a promising approach. This paper suggests a novel technique employing a reinforcement learning multi Q-learning algorithm to optimize UAV connectivity in such scenarios. We introduce a Strategic Planning Agent for efficient path planning and collision awareness and a Real-time Adaptive Agent to maintain optimal connection with the cellular base station. The agents trained in a simulated environment using multi Q-learning, encouraging them to learn from experience and adjust their decision-making to diverse terrain complexities and communication scenarios. Evaluation results reveal the significance of the approach, highlighting successful navigation in environments with varying obstacle densities and the ability to perform optimal connectivity using different frequency bands. This work paves the way for enhanced UAV autonomy and enhanced communication reliability in search and rescue operations.","sentences":["Using Unmanned Aerial Vehicles (UAVs) in Search and rescue operations (SAR) to navigate challenging terrain while maintaining reliable communication with the cellular network is a promising approach.","This paper suggests a novel technique employing a reinforcement learning multi Q-learning algorithm to optimize UAV connectivity in such scenarios.","We introduce a Strategic Planning Agent for efficient path planning and collision awareness and a Real-time Adaptive Agent to maintain optimal connection with the cellular base station.","The agents trained in a simulated environment using multi Q-learning, encouraging them to learn from experience and adjust their decision-making to diverse terrain complexities and communication scenarios.","Evaluation results reveal the significance of the approach, highlighting successful navigation in environments with varying obstacle densities and the ability to perform optimal connectivity using different frequency bands.","This work paves the way for enhanced UAV autonomy and enhanced communication reliability in search and rescue operations."],"url":"http://arxiv.org/abs/2405.10042v1","category":"cs.RO"}
{"created":"2024-05-16 12:16:25","title":"Bilateral Event Mining and Complementary for Event Stream Super-Resolution","abstract":"Event Stream Super-Resolution (ESR) aims to address the challenge of insufficient spatial resolution in event streams, which holds great significance for the application of event cameras in complex scenarios. Previous works for ESR often process positive and negative events in a mixed paradigm. This paradigm limits their ability to effectively model the unique characteristics of each event and mutually refine each other by considering their correlations. In this paper, we propose a bilateral event mining and complementary network (BMCNet) to fully leverage the potential of each event and capture the shared information to complement each other simultaneously. Specifically, we resort to a two-stream network to accomplish comprehensive mining of each type of events individually. To facilitate the exchange of information between two streams, we propose a bilateral information exchange (BIE) module. This module is layer-wisely embedded between two streams, enabling the effective propagation of hierarchical global information while alleviating the impact of invalid information brought by inherent characteristics of events. The experimental results demonstrate that our approach outperforms the previous state-of-the-art methods in ESR, achieving performance improvements of over 11\\% on both real and synthetic datasets. Moreover, our method significantly enhances the performance of event-based downstream tasks such as object recognition and video reconstruction. Our code is available at https://github.com/Lqm26/BMCNet-ESR.","sentences":["Event Stream Super-Resolution (ESR) aims to address the challenge of insufficient spatial resolution in event streams, which holds great significance for the application of event cameras in complex scenarios.","Previous works for ESR often process positive and negative events in a mixed paradigm.","This paradigm limits their ability to effectively model the unique characteristics of each event and mutually refine each other by considering their correlations.","In this paper, we propose a bilateral event mining and complementary network (BMCNet) to fully leverage the potential of each event and capture the shared information to complement each other simultaneously.","Specifically, we resort to a two-stream network to accomplish comprehensive mining of each type of events individually.","To facilitate the exchange of information between two streams, we propose a bilateral information exchange (BIE) module.","This module is layer-wisely embedded between two streams, enabling the effective propagation of hierarchical global information while alleviating the impact of invalid information brought by inherent characteristics of events.","The experimental results demonstrate that our approach outperforms the previous state-of-the-art methods in ESR, achieving performance improvements of over 11\\% on both real and synthetic datasets.","Moreover, our method significantly enhances the performance of event-based downstream tasks such as object recognition and video reconstruction.","Our code is available at https://github.com/Lqm26/BMCNet-ESR."],"url":"http://arxiv.org/abs/2405.10037v1","category":"cs.CV"}
{"created":"2024-05-16 12:15:26","title":"An extended Su-Schrieffer-Heeger model with time-reversal symmetry protection","abstract":"In this work, we theoretically study a modified Su-Schrieffer-Heeger (SSH) model in which each unit cell consists of three sites. Unlike existing extensions of the SSH model which are made by enlarging the periodicity of the (nearest-neighbor) hopping amplitudes, our modification is obtained by replacing the Pauli matrices in the system's Hamiltonian by their higher dimensional counterparts. This in turn leads to the presence of next-nearest neighbor hopping terms and the emergence of different symmetries than those of other extended SSH models. Moreover, the system supports a number of edge states that are protected by the time-reversal symmetry rather than the usual chiral symmetry. Finally, our system could be potentially realized in various experimental platforms including superconducting circuits as well as acoustic/optical waveguide arrays.","sentences":["In this work, we theoretically study a modified Su-Schrieffer-Heeger (SSH) model in which each unit cell consists of three sites.","Unlike existing extensions of the SSH model which are made by enlarging the periodicity of the (nearest-neighbor) hopping amplitudes, our modification is obtained by replacing the Pauli matrices in the system's Hamiltonian by their higher dimensional counterparts.","This in turn leads to the presence of next-nearest neighbor hopping terms and the emergence of different symmetries than those of other extended SSH models.","Moreover, the system supports a number of edge states that are protected by the time-reversal symmetry rather than the usual chiral symmetry.","Finally, our system could be potentially realized in various experimental platforms including superconducting circuits as well as acoustic/optical waveguide arrays."],"url":"http://arxiv.org/abs/2405.10034v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 12:15:02","title":"On the use of complex GTOs for the evaluation of radial integrals involving oscillating functions","abstract":"We study two classes of radial integrals involving a product of bound and continuum one-electron states. Using a representation of the continuum part with an expansion on complex Gaussian Type Orbitals, such integrals can be performed analytically. We investigate the reliability of this scheme for low-energy physical parameters. This study serves as a premise in view of potential applications in molecular scattering processes.","sentences":["We study two classes of radial integrals involving a product of bound and continuum one-electron states.","Using a representation of the continuum part with an expansion on complex Gaussian Type Orbitals, such integrals can be performed analytically.","We investigate the reliability of this scheme for low-energy physical parameters.","This study serves as a premise in view of potential applications in molecular scattering processes."],"url":"http://arxiv.org/abs/2405.10032v1","category":"physics.chem-ph"}
{"created":"2024-05-16 12:12:07","title":"RSDehamba: Lightweight Vision Mamba for Remote Sensing Satellite Image Dehazing","abstract":"Remote sensing image dehazing (RSID) aims to remove nonuniform and physically irregular haze factors for high-quality image restoration. The emergence of CNNs and Transformers has taken extraordinary strides in the RSID arena. However, these methods often struggle to demonstrate the balance of adequate long-range dependency modeling and maintaining computational efficiency. To this end, we propose the first lightweight network on the mamba-based model called RSDhamba in the field of RSID. Greatly inspired by the recent rise of Selective State Space Model (SSM) for its superior performance in modeling linear complexity and remote dependencies, our designed RSDehamba integrates the SSM framework into the U-Net architecture. Specifically, we propose the Vision Dehamba Block (VDB) as the core component of the overall network, which utilizes the linear complexity of SSM to achieve the capability of global context encoding. Simultaneously, the Direction-aware Scan Module (DSM) is designed to dynamically aggregate feature exchanges over different directional domains to effectively enhance the flexibility of sensing the spatially varying distribution of haze. In this way, our RSDhamba fully demonstrates the superiority of spatial distance capture dependencies and channel information exchange for better extraction of haze features. Extensive experimental results on widely used benchmarks validate the surpassing performance of our RSDehamba against existing state-of-the-art methods.","sentences":["Remote sensing image dehazing (RSID) aims to remove nonuniform and physically irregular haze factors for high-quality image restoration.","The emergence of CNNs and Transformers has taken extraordinary strides in the RSID arena.","However, these methods often struggle to demonstrate the balance of adequate long-range dependency modeling and maintaining computational efficiency.","To this end, we propose the first lightweight network on the mamba-based model called RSDhamba in the field of RSID.","Greatly inspired by the recent rise of Selective State Space Model (SSM) for its superior performance in modeling linear complexity and remote dependencies, our designed RSDehamba integrates the SSM framework into the U-Net architecture.","Specifically, we propose the Vision Dehamba Block (VDB) as the core component of the overall network, which utilizes the linear complexity of SSM to achieve the capability of global context encoding.","Simultaneously, the Direction-aware Scan Module (DSM) is designed to dynamically aggregate feature exchanges over different directional domains to effectively enhance the flexibility of sensing the spatially varying distribution of haze.","In this way, our RSDhamba fully demonstrates the superiority of spatial distance capture dependencies and channel information exchange for better extraction of haze features.","Extensive experimental results on widely used benchmarks validate the surpassing performance of our RSDehamba against existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.10030v1","category":"cs.CV"}
{"created":"2024-05-16 12:04:21","title":"On the selective formation of cubic tetrastack crystals from tetravalent patchy particles","abstract":"Achieving the formation of target open crystalline lattices from colloidal particles is of paramount importance for their potential application in photonics. Examples of such desired structures are the diamond, tetrastack, and pyrochlore lattices. Here, we demonstrate that the self-assembly of tetravalent patchy particles results in the selective formation of cubic tetrastack crystals, both in the bulk and in the systems subjected to external fields exerted by the solid substrate. It is demonstrated that the presence of an external field allows for the formation of well-defined single crystals with a low density of defects. Moreover, depending on the strength of the applied external field, the mechanism of epitaxial growth changes. For weakly attractive external fields, the crystallization occurs in a similar manner as in the bulk, since the fluid does not wet the substrate. Nonetheless, the formed crystal is considerably better ordered than the crystals formed in bulk, since the surface induces the ordering in the first layer. On the other hand, it is demonstrated that the formation of well-ordered cubic tetrastack crystals is considerably enhanced by the increase in external field strength, and the formation of the thick crystalline film occurs via a series of layering transitions.","sentences":["Achieving the formation of target open crystalline lattices from colloidal particles is of paramount importance for their potential application in photonics.","Examples of such desired structures are the diamond, tetrastack, and pyrochlore lattices.","Here, we demonstrate that the self-assembly of tetravalent patchy particles results in the selective formation of cubic tetrastack crystals, both in the bulk and in the systems subjected to external fields exerted by the solid substrate.","It is demonstrated that the presence of an external field allows for the formation of well-defined single crystals with a low density of defects.","Moreover, depending on the strength of the applied external field, the mechanism of epitaxial growth changes.","For weakly attractive external fields, the crystallization occurs in a similar manner as in the bulk, since the fluid does not wet the substrate.","Nonetheless, the formed crystal is considerably better ordered than the crystals formed in bulk, since the surface induces the ordering in the first layer.","On the other hand, it is demonstrated that the formation of well-ordered cubic tetrastack crystals is considerably enhanced by the increase in external field strength, and the formation of the thick crystalline film occurs via a series of layering transitions."],"url":"http://arxiv.org/abs/2405.10023v1","category":"cond-mat.soft"}
{"created":"2024-05-16 12:00:39","title":"Data-Efficient Low-Complexity Acoustic Scene Classification in the DCASE 2024 Challenge","abstract":"This article describes the Data-Efficient Low-Complexity Acoustic Scene Classification Task in the DCASE 2024 Challenge and the corresponding baseline system. The task setup is a continuation of previous editions (2022 and 2023), which focused on recording device mismatches and low-complexity constraints. This year's edition introduces an additional real-world problem: participants must develop data-efficient systems for five scenarios, which progressively limit the available training data. The provided baseline system is based on an efficient, factorized CNN architecture constructed from inverted residual blocks and uses Freq-MixStyle to tackle the device mismatch problem. The baseline system's accuracy ranges from 42.40% on the smallest to 56.99% on the largest training set.","sentences":["This article describes the Data-Efficient Low-Complexity Acoustic Scene Classification Task in the DCASE 2024 Challenge and the corresponding baseline system.","The task setup is a continuation of previous editions (2022 and 2023), which focused on recording device mismatches and low-complexity constraints.","This year's edition introduces an additional real-world problem: participants must develop data-efficient systems for five scenarios, which progressively limit the available training data.","The provided baseline system is based on an efficient, factorized CNN architecture constructed from inverted residual blocks and uses Freq-MixStyle to tackle the device mismatch problem.","The baseline system's accuracy ranges from 42.40% on the smallest to 56.99% on the largest training set."],"url":"http://arxiv.org/abs/2405.10018v1","category":"eess.AS"}
{"created":"2024-05-16 11:59:45","title":"Mechanism for the Broadened Linewidth in Antiferromagnetic Resonance","abstract":"The linewidth of antiferromagnetic resonance (AFMR) is found to be significantly broader than that of ferromagnetic resonance (FMR), even when the intrinsic Gilbert damping parameter is the same for both systems. We investigate the origin of this enhanced damping rate in AFMR by studying a bipartite magnet model. Through analytical calculations and numerical simulations, we present three perspectives on understanding this linewidth broadening in AFMR: i) The non-dissipative Heisenberg exchange interaction develops a damping-like component in the presence of Gilbert damping, ii) The transverse component of the exchange coupling reduces the AFMR frequency, thereby increasing the damping rate, and iii) The antiferromagnetic eigenmode exhibits characteristics of a two-mode squeezed state, which is inherently linked to an enhanced damping rate. Our findings provide a comprehensive understanding of the complex dynamics governing magnetic dissipation in antiferromagnet and offer insights into the experimentally observed broadened linewidths in AFMR spectra.","sentences":["The linewidth of antiferromagnetic resonance (AFMR) is found to be significantly broader than that of ferromagnetic resonance (FMR), even when the intrinsic Gilbert damping parameter is the same for both systems.","We investigate the origin of this enhanced damping rate in AFMR by studying a bipartite magnet model.","Through analytical calculations and numerical simulations, we present three perspectives on understanding this linewidth broadening in AFMR: i)","The non-dissipative Heisenberg exchange interaction develops a damping-like component in the presence of Gilbert damping, ii) The transverse component of the exchange coupling reduces the AFMR frequency, thereby increasing the damping rate, and iii)","The antiferromagnetic eigenmode exhibits characteristics of a two-mode squeezed state, which is inherently linked to an enhanced damping rate.","Our findings provide a comprehensive understanding of the complex dynamics governing magnetic dissipation in antiferromagnet and offer insights into the experimentally observed broadened linewidths in AFMR spectra."],"url":"http://arxiv.org/abs/2405.10017v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 11:59:15","title":"Non-Hermitian Topology in Hermitian Topological Matter","abstract":"Non-Hermiticity leads to distinctive topological phenomena absent in Hermitian systems. However, connection between such intrinsic non-Hermitian topology and Hermitian topology has remained largely elusive. Here, considering the bulk and boundary as an environment and system, we demonstrate that anomalous boundary states in Hermitian topological insulators exhibit non-Hermitian topology. We study the self-energy capturing the particle exchange between the bulk and boundary, and demonstrate that it detects Hermitian topology in the bulk and induces non-Hermitian topology at the boundary. As an illustrative example, we show the non-Hermitian topology and concomitant skin effect inherently embedded within chiral edge states of Chern insulators. We also find the emergence of hinge states within effective non-Hermitian Hamiltonians at surfaces of three-dimensional topological insulators. Furthermore, we comprehensively classify our correspondence across all the tenfold symmetry classes of topological insulators and superconductors. Our work uncovers a hidden connection between Hermitian and non-Hermitian topology, and provides an approach to identifying non-Hermitian topology in quantum matter.","sentences":["Non-Hermiticity leads to distinctive topological phenomena absent in Hermitian systems.","However, connection between such intrinsic non-Hermitian topology and Hermitian topology has remained largely elusive.","Here, considering the bulk and boundary as an environment and system, we demonstrate that anomalous boundary states in Hermitian topological insulators exhibit non-Hermitian topology.","We study the self-energy capturing the particle exchange between the bulk and boundary, and demonstrate that it detects Hermitian topology in the bulk and induces non-Hermitian topology at the boundary.","As an illustrative example, we show the non-Hermitian topology and concomitant skin effect inherently embedded within chiral edge states of Chern insulators.","We also find the emergence of hinge states within effective non-Hermitian Hamiltonians at surfaces of three-dimensional topological insulators.","Furthermore, we comprehensively classify our correspondence across all the tenfold symmetry classes of topological insulators and superconductors.","Our work uncovers a hidden connection between Hermitian and non-Hermitian topology, and provides an approach to identifying non-Hermitian topology in quantum matter."],"url":"http://arxiv.org/abs/2405.10015v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 11:50:09","title":"Introducing advanced hybrid coupling: Non-discriminatory coalescence of flow-based and net transfer capacity calculation regions","abstract":"Flow-based market coupling is substantially altering the computation of cross-zonal capacities for the trade of electricity in the vast majority of European markets. The main benefit of the flow-based method is improved accuracy by better representing the impact of cross-zonal trade on the power flows in transmission grids. Some borders, adjacent to flow-based capacity regions, are represented through net transfer capacities during market coupling. Under the current standard hybrid coupling, the utilization of grid elements in the flow-based regions due to the predicted trade across such borders is not available for trades between flow-based zones. The flow-based representation is not limited to the given capacity calculation region, but can be extended to also model the impact of trade with other regions. This so-called advanced hybrid coupling replaces the priority inherently given to trade across net transfer capacity-coupled borders by introducing virtual bidding zones. These map the effect of non-flow-based borders on line capacities in the flow-based regions, enabling the market coupling optimization to prioritize trade between flow-based bidding zones and trade across non-flow-based borders. This paper explains the mechanism of advanced hybrid coupling and how it is modeled mathematically. Based on a test network, a case study shows to what extent and why advanced hybrid coupling leads to welfare gains during market coupling and lower congestion management costs in the flow-based region.","sentences":["Flow-based market coupling is substantially altering the computation of cross-zonal capacities for the trade of electricity in the vast majority of European markets.","The main benefit of the flow-based method is improved accuracy by better representing the impact of cross-zonal trade on the power flows in transmission grids.","Some borders, adjacent to flow-based capacity regions, are represented through net transfer capacities during market coupling.","Under the current standard hybrid coupling, the utilization of grid elements in the flow-based regions due to the predicted trade across such borders is not available for trades between flow-based zones.","The flow-based representation is not limited to the given capacity calculation region, but can be extended to also model the impact of trade with other regions.","This so-called advanced hybrid coupling replaces the priority inherently given to trade across net transfer capacity-coupled borders by introducing virtual bidding zones.","These map the effect of non-flow-based borders on line capacities in the flow-based regions, enabling the market coupling optimization to prioritize trade between flow-based bidding zones and trade across non-flow-based borders.","This paper explains the mechanism of advanced hybrid coupling and how it is modeled mathematically.","Based on a test network, a case study shows to what extent and why advanced hybrid coupling leads to welfare gains during market coupling and lower congestion management costs in the flow-based region."],"url":"http://arxiv.org/abs/2405.10010v1","category":"eess.SY"}
{"created":"2024-05-16 11:46:39","title":"Machine Learning-Based Path Loss Modeling with Simplified Features","abstract":"Propagation modeling is a crucial tool for successful wireless deployments and spectrum planning with the demand for high modeling accuracy continuing to grow. Recognizing that detailed knowledge of the physical environment (terrain and clutter) is essential, we propose a novel approach that uses environmental information for predictions. Instead of relying on complex, detail-intensive models, we explore the use of simplified scalar features involving the total obstruction depth along the direct path from transmitter to receiver. Obstacle depth offers a streamlined, yet surprisingly accurate, method for predicting wireless signal propagation, providing a practical solution for efficient and effective wireless network planning.","sentences":["Propagation modeling is a crucial tool for successful wireless deployments and spectrum planning with the demand for high modeling accuracy continuing to grow.","Recognizing that detailed knowledge of the physical environment (terrain and clutter) is essential, we propose a novel approach that uses environmental information for predictions.","Instead of relying on complex, detail-intensive models, we explore the use of simplified scalar features involving the total obstruction depth along the direct path from transmitter to receiver.","Obstacle depth offers a streamlined, yet surprisingly accurate, method for predicting wireless signal propagation, providing a practical solution for efficient and effective wireless network planning."],"url":"http://arxiv.org/abs/2405.10006v1","category":"cs.LG"}
{"created":"2024-05-16 11:39:44","title":"Bogomolov-Gieseker inequality for log terminal K\u00e4hler threefolds","abstract":"In this article we are concerned with two main themes revolving around compact K\\\"ahler spaces with log terminal singularities. The first one is the orbifold version of the Bogomolov-Gieseker inequality for stable $\\mathbb Q$-sheaves on threefolds while the second one has to do with the construction of canonical metrics on such spaces and their properties.","sentences":["In this article we are concerned with two main themes revolving around compact K\\\"ahler spaces with log terminal singularities.","The first one is the orbifold version of the Bogomolov-Gieseker inequality for stable $\\mathbb Q$-sheaves on threefolds while the second one has to do with the construction of canonical metrics on such spaces and their properties."],"url":"http://arxiv.org/abs/2405.10003v1","category":"math.AG"}
{"created":"2024-05-16 11:39:03","title":"Rapid stabilization and finite time stabilization of the bilinear Schr\u00f6dinger equation","abstract":"We propose a method to establish the rapid stabilization of the bilinear Schr\\\"odinger control system and its linearized system, and the finite time stabilization of the linearized system using the Grammian operators. The analysis of the rapid stabilization involves a new quantity (variable) which is inspired by the adjoint state in the optimal control theory and is proposed in our recent work on control systems associated with strongly continuous group. The analysis of the finite time stabilization follows the strategy introduced by Coron and Nguyen in the study of the finite time stabilization of the heat equation and incorporate a new ingredient involving the estimate of the cost of controls of the linearized system in small time derived in this paper.","sentences":["We propose a method to establish the rapid stabilization of the bilinear Schr\\\"odinger control system and its linearized system, and the finite time stabilization of the linearized system using the Grammian operators.","The analysis of the rapid stabilization involves a new quantity (variable) which is inspired by the adjoint state in the optimal control theory and is proposed in our recent work on control systems associated with strongly continuous group.","The analysis of the finite time stabilization follows the strategy introduced by Coron and Nguyen in the study of the finite time stabilization of the heat equation and incorporate a new ingredient involving the estimate of the cost of controls of the linearized system in small time derived in this paper."],"url":"http://arxiv.org/abs/2405.10002v1","category":"math.OC"}
{"created":"2024-05-16 11:34:34","title":"Lack of differentiability of semigroups associated to delayed abstract thermoelastic systems","abstract":"In this paper, we consider the associated semigroups to some abstract thermoelastic systems (in particular the {\\alpha}-\\b{eta} system), with a partial delay on the coupled system. We will prove that the corresponding semigroups (in appropriate Hilbert spaces) are not differentiable.","sentences":["In this paper, we consider the associated semigroups to some abstract thermoelastic systems (in particular the {\\alpha}-\\b{eta} system), with a partial delay on the coupled system.","We will prove that the corresponding semigroups (in appropriate Hilbert spaces) are not differentiable."],"url":"http://arxiv.org/abs/2405.10000v1","category":"math.AP"}
{"created":"2024-05-16 11:24:10","title":"Convergence of kinetic Langevin samplers for non-convex potentials","abstract":"We study three kinetic Langevin samplers including the Euler discretization, the BU and the UBU splitting scheme. We provide contraction results in $L^1$-Wasserstein distance for non-convex potentials. These results are based on a carefully tailored distance function and an appropriate coupling construction. Additionally, the error in the $L^1$-Wasserstein distance between the true target measure and the invariant measure of the discretization scheme is bounded. To get an $\\varepsilon$-accuracy in $L^1$-Wasserstein distance, we show complexity guarantees of order $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ for the Euler scheme and $\\mathcal{O}(d^{1/4}/\\sqrt{\\varepsilon})$ for the UBU scheme under appropriate regularity assumptions on the target measure. The results are applicable to interacting particle systems and provide bounds for sampling probability measures of mean-field type.","sentences":["We study three kinetic Langevin samplers including the Euler discretization, the BU and the UBU splitting scheme.","We provide contraction results in $L^1$-Wasserstein distance for non-convex potentials.","These results are based on a carefully tailored distance function and an appropriate coupling construction.","Additionally, the error in the $L^1$-Wasserstein distance between the true target measure and the invariant measure of the discretization scheme is bounded.","To get an $\\varepsilon$-accuracy in $L^1$-Wasserstein distance, we show complexity guarantees of order $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ for the Euler scheme and $\\mathcal{O}(d^{1/4}/\\sqrt{\\varepsilon})$ for the UBU scheme under appropriate regularity assumptions on the target measure.","The results are applicable to interacting particle systems and provide bounds for sampling probability measures of mean-field type."],"url":"http://arxiv.org/abs/2405.09992v1","category":"math.PR"}
{"created":"2024-05-16 11:22:15","title":"A characterization of complex Hadamard matrices appearing in families of MUB triplets","abstract":"It is shown that a normalized complex Hadamard matrix of order $6$ having three distinct columns, each containing at least one $-1$ entry necessarily belongs to the transposed Fourier family, or to the family of $2$-circulant complex Hadamard matrices. The proofs rely on solving polynomial system of equations by Gr\\\"obner basis techniques, and make use of a structure theorem concerning regular Hadamard matrices. As a consequence, members of these two families can be easily recognized in practice. In particular, one can identify complex Hadamard matrices appearing in known triplets of pairwise mutually unbiased bases in dimension $6$.","sentences":["It is shown that a normalized complex Hadamard matrix of order $6$ having three distinct columns, each containing at least one $-1$ entry necessarily belongs to the transposed Fourier family, or to the family of $2$-circulant complex Hadamard matrices.","The proofs rely on solving polynomial system of equations by Gr\\\"obner basis techniques, and make use of a structure theorem concerning regular Hadamard matrices.","As a consequence, members of these two families can be easily recognized in practice.","In particular, one can identify complex Hadamard matrices appearing in known triplets of pairwise mutually unbiased bases in dimension $6$."],"url":"http://arxiv.org/abs/2405.09991v1","category":"math.CO"}
{"created":"2024-05-16 11:11:13","title":"Integral action feedback design for conservative abstract systems in the presence of input nonlinearities","abstract":"In this article, we present a stabilization feedback law with integral action for conservative abstract linear systems subjected to actuator nonlinearity. Based on the designed control law, we first prove the well-posedness and global asymptotic stability of the origin of the closed-loop system by constructing a weak Lyapunov functional. Secondly, as an illustration, we apply the results to a wave equation coupled with an ordinary differential equation (ODE) at the boundary. Finally, we give the simulation results to illustrate the effectiveness of our method.","sentences":["In this article, we present a stabilization feedback law with integral action for conservative abstract linear systems subjected to actuator nonlinearity.","Based on the designed control law, we first prove the well-posedness and global asymptotic stability of the origin of the closed-loop system by constructing a weak Lyapunov functional.","Secondly, as an illustration, we apply the results to a wave equation coupled with an ordinary differential equation (ODE) at the boundary.","Finally, we give the simulation results to illustrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.09986v1","category":"math.OC"}
{"created":"2024-05-16 11:00:27","title":"Dynamical behavior and optimal control of a stochastic SAIRS epidemic model with two saturated incidences","abstract":"Stochastic models are widely used to investigate the spread of epidemics in a complex environment. This paper extends a deterministic SAIRS epidemic model to a stochastic case with limited patient capacity and exposure. We first study the dynamical properties of the model under certain conditions, including persistence, extinction, and ergodic. Then, we introduce vaccination and isolation into the model as control variables. The optimal control strategies are obtained based on the Pontryagin minimum principle. Finally, numerical simulations are given to illustrate our theoretical results.","sentences":["Stochastic models are widely used to investigate the spread of epidemics in a complex environment.","This paper extends a deterministic SAIRS epidemic model to a stochastic case with limited patient capacity and exposure.","We first study the dynamical properties of the model under certain conditions, including persistence, extinction, and ergodic.","Then, we introduce vaccination and isolation into the model as control variables.","The optimal control strategies are obtained based on the Pontryagin minimum principle.","Finally, numerical simulations are given to illustrate our theoretical results."],"url":"http://arxiv.org/abs/2405.09982v1","category":"math.PR"}
{"created":"2024-05-16 10:53:08","title":"Harmonic and Interharmonic Detection in Power Systems Based on Fractal-Optimized Variational Mode Decomposition","abstract":"The proposed method introduces a parameter determination approach based on the minimum Fractal box dimension (FBD) of Variational Mode Decomposition (VMD) components, aiming to address the issue of manual determination of VMD decomposition layers in advance. Initially, VMD is applied to the original power signal, and the layer number for VMD decomposition is determined by selecting the K value associated with the smallest fractal box dimension among its components. Subsequently, several Intrinsic Mode Functions (IMFs) are obtained as fundamental, harmonic, and interharmonic signals representing different aspects of the power system. Furthermore, Hilbert transform(HT) is employed to extract instantaneous amplitude and frequency information from these harmonic signals. Experimental evaluation using simulation data and real-world power system data demonstrates that compared to Empirical Mode Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD), our proposed method achieves more accurate identification and effective extraction of harmonic signals.","sentences":["The proposed method introduces a parameter determination approach based on the minimum Fractal box dimension (FBD) of Variational Mode Decomposition (VMD) components, aiming to address the issue of manual determination of VMD decomposition layers in advance.","Initially, VMD is applied to the original power signal, and the layer number for VMD decomposition is determined by selecting the K value associated with the smallest fractal box dimension among its components.","Subsequently, several Intrinsic Mode Functions (IMFs) are obtained as fundamental, harmonic, and interharmonic signals representing different aspects of the power system.","Furthermore, Hilbert transform(HT) is employed to extract instantaneous amplitude and frequency information from these harmonic signals.","Experimental evaluation using simulation data and real-world power system data demonstrates that compared to Empirical Mode Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD), our proposed method achieves more accurate identification and effective extraction of harmonic signals."],"url":"http://arxiv.org/abs/2405.09979v1","category":"eess.SP"}
{"created":"2024-05-16 10:37:40","title":"Adaptive Ensemble Control for Stochastic Systems with Mixed Asymmetric Laplace Noises","abstract":"This paper presents an adaptive ensemble control for stochastic systems subject to asymmetric noises and outliers. Asymmetric noises skew system observations, and outliers with large amplitude deteriorate the observations even further. Such disturbances induce poor system estimation and degraded stochastic system control. In this work, we model the asymmetric noises and outliers by mixed asymmetric Laplace distributions (ALDs), and propose an optimal control for stochastic systems with mixed ALD noises. Particularly, we segregate the system disturbed by mixed ALD noises into subsystems, each of which is subject to a specific ALD noise. For each subsystem, we design an iterative quantile filter (IQF) to estimate the system parameters using system observations. With the estimated parameters by IQF, we derive the certainty equivalence (CE) control law for each subsystem. Then we use the Bayesian approach to ensemble the subsystem CE controllers, with each of the controllers weighted by their posterior probability. We finalize our control law as the weighted sum of the control signals by the sub-system CE controllers. To demonstrate our approach, we conduct numerical simulations and Monte Carlo analyses. The results show improved tracking performance by our approach for skew noises and its robustness to outliers, compared with single ALD based and RLS-based control policy.","sentences":["This paper presents an adaptive ensemble control for stochastic systems subject to asymmetric noises and outliers.","Asymmetric noises skew system observations, and outliers with large amplitude deteriorate the observations even further.","Such disturbances induce poor system estimation and degraded stochastic system control.","In this work, we model the asymmetric noises and outliers by mixed asymmetric Laplace distributions (ALDs), and propose an optimal control for stochastic systems with mixed ALD noises.","Particularly, we segregate the system disturbed by mixed ALD noises into subsystems, each of which is subject to a specific ALD noise.","For each subsystem, we design an iterative quantile filter (IQF) to estimate the system parameters using system observations.","With the estimated parameters by IQF, we derive the certainty equivalence (CE) control law for each subsystem.","Then we use the Bayesian approach to ensemble the subsystem CE controllers, with each of the controllers weighted by their posterior probability.","We finalize our control law as the weighted sum of the control signals by the sub-system CE controllers.","To demonstrate our approach, we conduct numerical simulations and Monte Carlo analyses.","The results show improved tracking performance by our approach for skew noises and its robustness to outliers, compared with single ALD based and RLS-based control policy."],"url":"http://arxiv.org/abs/2405.09973v1","category":"math.OC"}
{"created":"2024-05-16 10:30:30","title":"The van Est homomorphism for strict Lie 2-groups","abstract":"We construct a van Est map for strict Lie 2-groups from the Bott-Shulman-Stasheff double complex of the strict Lie 2-group to the Weil algebra of its associated strict Lie 2-algebra. We show that, under appropriate connectedness assumptions, this map induces isomorphisms in cohomology. As an application, we differentiate the Segal 2-form on the loop group.","sentences":["We construct a van Est map for strict Lie 2-groups from the Bott-Shulman-Stasheff double complex of the strict Lie 2-group to the Weil algebra of its associated strict Lie 2-algebra.","We show that, under appropriate connectedness assumptions, this map induces isomorphisms in cohomology.","As an application, we differentiate the Segal 2-form on the loop group."],"url":"http://arxiv.org/abs/2405.09969v1","category":"math.DG"}
{"created":"2024-05-16 10:03:01","title":"Data-Assimilated Crystal Growth Simulation for Multiple Crystalline Phases","abstract":"To determine crystal structures from an X-ray diffraction (XRD) pattern containing multiple unknown phases, a data-assimilated crystal growth (DACG) simulation method has been developed. The XRD penalty function selectively stabilizes the structures in the experimental data, promoting their grain growth during simulated annealing. Since the XRD pattern is calculated as the Fourier transform of the pair distribution function, the DACG simulation can be performed without prior determination of the lattice parameters. We applied it to C (graphite and diamond) and SiO$_2$ (low-quartz and low-cristobalite) systems, demonstrating that the DACG simulation successfully reproduced multiple crystal structures.","sentences":["To determine crystal structures from an X-ray diffraction (XRD) pattern containing multiple unknown phases, a data-assimilated crystal growth (DACG) simulation method has been developed.","The XRD penalty function selectively stabilizes the structures in the experimental data, promoting their grain growth during simulated annealing.","Since the XRD pattern is calculated as the Fourier transform of the pair distribution function, the DACG simulation can be performed without prior determination of the lattice parameters.","We applied it to C (graphite and diamond) and SiO$_2$ (low-quartz and low-cristobalite) systems, demonstrating that the DACG simulation successfully reproduced multiple crystal structures."],"url":"http://arxiv.org/abs/2405.09956v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 09:57:09","title":"Quantization of Cantor-Like Set on the Real Projective Line","abstract":"In this article, an iterated function system (IFS) is considered on the real projective line $\\mathbb{RP}^1$ so that the attractor is a Cantor-like set. Hausdorff dimension of this attractor is estimated. The existence of a probability measure associated with this IFS on $\\mathbb{RP}^1$ is also demonstrated. It is shown that the $n$-th quantization error of order $r$ for the push-forward measure is a constant multiple of the $n$-th quantization error of order $r$ of the original measure. Finally, an upper bound for the $n$-th quantization error of order $2$ for this measure is provided.","sentences":["In this article, an iterated function system (IFS) is considered on the real projective line $\\mathbb{RP}^1$ so that the attractor is a Cantor-like set.","Hausdorff dimension of this attractor is estimated.","The existence of a probability measure associated with this IFS on $\\mathbb{RP}^1$ is also demonstrated.","It is shown that the $n$-th quantization error of order $r$ for the push-forward measure is a constant multiple of the $n$-th quantization error of order $r$ of the original measure.","Finally, an upper bound for the $n$-th quantization error of order $2$ for this measure is provided."],"url":"http://arxiv.org/abs/2405.09954v1","category":"math.DS"}
{"created":"2024-05-16 09:54:47","title":"Ergodicity of some stochastic Fokker-Planck equations with additive common noise","abstract":"In this paper we consider stochastic Fokker-Planck Partial Differential Equations (PDEs), obtained as the mean-field limit of weakly interacting particle systems subjected to both independent (or idiosyncratic) and common Brownian noises. We provide sufficient conditions under which the deterministic counterpart of the Fokker-Planck equation, which corresponds to particle systems that are just subjected to independent noises, has several invariant measures, but for which the stochastic version admits a unique invariant measure under the presence of the additive common noise. The very difficulty comes from the fact that the common noise is just of finite dimension while the state variable, which should be seen as the conditional marginal law of the system given the common noise, lives in a space of infinite dimension. In this context, our result holds true if, in addition to standard confining properties, the mean field interaction term forces the system to be attracted by its conditional mean given the common noise and the intensity of the idiosyncratic noise is small.","sentences":["In this paper we consider stochastic Fokker-Planck Partial Differential Equations (PDEs), obtained as the mean-field limit of weakly interacting particle systems subjected to both independent (or idiosyncratic) and common Brownian noises.","We provide sufficient conditions under which the deterministic counterpart of the Fokker-Planck equation, which corresponds to particle systems that are just subjected to independent noises, has several invariant measures, but for which the stochastic version admits a unique invariant measure under the presence of the additive common noise.","The very difficulty comes from the fact that the common noise is just of finite dimension while the state variable, which should be seen as the conditional marginal law of the system given the common noise, lives in a space of infinite dimension.","In this context, our result holds true if, in addition to standard confining properties, the mean field interaction term forces the system to be attracted by its conditional mean given the common noise and the intensity of the idiosyncratic noise is small."],"url":"http://arxiv.org/abs/2405.09950v1","category":"math.PR"}
{"created":"2024-05-16 09:44:00","title":"FPDIoU Loss: A Loss Function for Efficient Bounding Box Regression of Rotated Object Detection","abstract":"Bounding box regression is one of the important steps of object detection. However, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. Most of the existing loss functions for rotated object detection calculate the difference between two bounding boxes only focus on the deviation of area or each points distance (e.g., $\\mathcal{L}_{Smooth-\\ell 1}$, $\\mathcal{L}_{RotatedIoU}$ and $\\mathcal{L}_{PIoU}$). The calculation process of some loss functions is extremely complex (e.g. $\\mathcal{L}_{KFIoU}$). In order to improve the efficiency and accuracy of bounding box regression for rotated object detection, we proposed a novel metric for arbitrary shapes comparison based on minimum points distance, which takes most of the factors from existing loss functions for rotated object detection into account, i.e., the overlap or nonoverlapping area, the central points distance and the rotation angle. We also proposed a loss function called $\\mathcal{L}_{FPDIoU}$ based on four points distance for accurate bounding box regression focusing on faster and high quality anchor boxes. In the experiments, $FPDIoU$ loss has been applied to state-of-the-art rotated object detection (e.g., RTMDET, H2RBox) models training with three popular benchmarks of rotated object detection including DOTA, DIOR, HRSC2016 and two benchmarks of arbitrary orientation scene text detection including ICDAR 2017 RRC-MLT and ICDAR 2019 RRC-MLT, which achieves better performance than existing loss functions.","sentences":["Bounding box regression is one of the important steps of object detection.","However, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training.","Most of the existing loss functions for rotated object detection calculate the difference between two bounding boxes only focus on the deviation of area or each points distance (e.g., $\\mathcal{L}_{Smooth-\\ell 1}$, $\\mathcal{L}_{RotatedIoU}$ and $\\mathcal{L}_{PIoU}$).","The calculation process of some loss functions is extremely complex (e.g. $\\mathcal{L}_{KFIoU}$).","In order to improve the efficiency and accuracy of bounding box regression for rotated object detection, we proposed a novel metric for arbitrary shapes comparison based on minimum points distance, which takes most of the factors from existing loss functions for rotated object detection into account, i.e., the overlap or nonoverlapping area, the central points distance and the rotation angle.","We also proposed a loss function called $\\mathcal{L}_{FPDIoU}$ based on four points distance for accurate bounding box regression focusing on faster and high quality anchor boxes.","In the experiments, $FPDIoU$ loss has been applied to state-of-the-art rotated object detection (e.g., RTMDET, H2RBox) models training with three popular benchmarks of rotated object detection including DOTA, DIOR, HRSC2016 and two benchmarks of arbitrary orientation scene text detection including ICDAR 2017 RRC-MLT and ICDAR 2019 RRC-MLT, which achieves better performance than existing loss functions."],"url":"http://arxiv.org/abs/2405.09942v1","category":"cs.CV"}
{"created":"2024-05-16 09:41:34","title":"Fujita-Kato solutions and optimal time decay for the Vlasov-Navier-Stokes system in the whole space","abstract":"We are concerned with the construction of global-in-time strong solutions for the incompressible Vlasov-Navier-Stokes systemin the whole three-dimensional space. One of our goals is to establish that small initial velocities with critical Sobolev regularity and sufficiently well localized initial kinetic distribution functions give rise to global and unique solutions. This constitutes an extension of the celebrated result for the incompressibleNavier-Stokes equations (NS) that has been established in 1964 by Fujita and Kato. If in addition the initial velocity is integrable, we establish that the total energy of the system decays to 0 with the optimal rate t^{-3/2}, like for the weak solutions of (NS). Our results partly rely on the use of a higher order energy functional that controls the regularity $H^1$ of the velocityand seems to have been first introduced by Li, Shou and Zhang in the contextof nonhomogeneous Vlasov-Navier-Stokes system. In the small data case, we show that this energy functional decays with the rate t^{-5/2}.","sentences":["We are concerned with the construction of global-in-time strong solutions for the incompressible Vlasov-Navier-Stokes systemin the whole three-dimensional space.","One of our goals is to establish that small initial velocities with critical Sobolev regularity and sufficiently well localized initial kinetic distribution functions give rise to global and unique solutions.","This constitutes an extension of the celebrated result for the incompressibleNavier-Stokes equations (NS) that has been established in 1964 by Fujita and Kato.","If in addition the initial velocity is integrable, we establish that the total energy of the system decays to 0 with the optimal rate t^{-3/2}, like for the weak solutions of (NS).","Our results partly rely on the use of a higher order energy functional that controls the regularity $H^1$ of the velocityand seems to have been first introduced by Li, Shou and Zhang in the contextof nonhomogeneous Vlasov-Navier-Stokes system.","In the small data case, we show that this energy functional decays with the rate t^{-5/2}."],"url":"http://arxiv.org/abs/2405.09937v1","category":"math.AP"}
{"created":"2024-05-16 09:41:27","title":"Collaborative planning of integrated hydrogen energy chain multi-energy systems: A review","abstract":"Most planning of the traditional hydrogen energy supply chain (HSC) focuses on the storage and transportation links between production and consumption ends. It ignores the energy flows and interactions between each link, making it unsuitable for energy system planning analysis. Therefore, we propose the concept of a hydrogen energy chain (HEC) based on the HSC, which emphasizes the interactions between different types of energy flows in the production, compression, storage, transportation, and application links of hydrogen. The HEC plays a crucial role in mitigating fluctuations of renewable energy and facilitating the optimal allocation of heterogeneous energy sources across time and space. Effective collaborative planning models that consider HEC are essential for the optimal configuration of multi-energy systems (MESs), which guarantees high-efficiency operation and the economic and environmental friendliness of the system. This paper presents a systematic review of recent articles on collaborative planning of integrated hydrogen energy chain multi-energy systems (HEC-MESs). First, we introduce the basic framework of HEC-MES, focusing on the current research status of the production, compression, storage, transportation, and application links in HEC. Furthermore, we review technology types of hydrogen energy for planning and summarize the typical forms of HEC in MESs. Then, the following sections outline the models and methods for collaborative planning of HEC-MES. They include detailed analyses of covered sector types, spatial and temporal scopes of planning, uncertainties, model formulations, and solution methods. Finally, the paper concludes by summarizing the research gaps identified in current articles and outlining directions for future research.","sentences":["Most planning of the traditional hydrogen energy supply chain (HSC) focuses on the storage and transportation links between production and consumption ends.","It ignores the energy flows and interactions between each link, making it unsuitable for energy system planning analysis.","Therefore, we propose the concept of a hydrogen energy chain (HEC) based on the HSC, which emphasizes the interactions between different types of energy flows in the production, compression, storage, transportation, and application links of hydrogen.","The HEC plays a crucial role in mitigating fluctuations of renewable energy and facilitating the optimal allocation of heterogeneous energy sources across time and space.","Effective collaborative planning models that consider HEC are essential for the optimal configuration of multi-energy systems (MESs), which guarantees high-efficiency operation and the economic and environmental friendliness of the system.","This paper presents a systematic review of recent articles on collaborative planning of integrated hydrogen energy chain multi-energy systems (HEC-MESs).","First, we introduce the basic framework of HEC-MES, focusing on the current research status of the production, compression, storage, transportation, and application links in HEC.","Furthermore, we review technology types of hydrogen energy for planning and summarize the typical forms of HEC in MESs.","Then, the following sections outline the models and methods for collaborative planning of HEC-MES.","They include detailed analyses of covered sector types, spatial and temporal scopes of planning, uncertainties, model formulations, and solution methods.","Finally, the paper concludes by summarizing the research gaps identified in current articles and outlining directions for future research."],"url":"http://arxiv.org/abs/2405.09936v1","category":"eess.SY"}
{"created":"2024-05-16 09:24:24","title":"Cyclicity in Besov-Dirichlet spaces from the Corona Theorem","abstract":"Tolokonnikov's Corona Theorem is used to obtain two results on cyclicity in Besov-Dirichlet spaces.","sentences":["Tolokonnikov's Corona Theorem is used to obtain two results on cyclicity in Besov-Dirichlet spaces."],"url":"http://arxiv.org/abs/2405.09921v1","category":"math.CA"}
{"created":"2024-05-16 09:13:54","title":"DIMSIM -- Device Integrity Monitoring through iSIM Applets and Distributed Ledger Technology","abstract":"In the context of industrial environment, devices, such as robots and drones, are vulnerable to malicious activities such device tampering (e.g., hardware and software changes). The problem becomes even worse in a multi-stakeholder environment where multiple players contribute to an ecosystem.   In such scenarios, particularly, when devices are deployed in remote settings, ensuring device integrity so that all stakeholders can trust them is challenging. Existing methods, often depend on additional hardware like the Trusted Platform Module (TPM) which may not be universally provided by all vendors. In this study, we introduce a distributed ledger technology-oriented architecture to monitor the remote devices' integrity using eUICC technology, a feature commonly found in industrial devices for cellular connectivity. We propose that using secure applets in eUICC, devices' integrity can be monitored and managed without installing any additional hardware.   To this end, we present an end-to-end architecture to monitor device integrity thereby enabling all the stakeholders in the system to trust the devices. Additionally, we leverage the properties of immutable databases to provide robustness and efficiently to our model. In our primary evaluations, we measure the overhead caused by hashing our proposed data packets and performance of integrating an immutable database into our system. Our results show that performing hashing on our data packets takes order of microseconds, while reading and writing to an immutable database also requires only milliseconds.","sentences":["In the context of industrial environment, devices, such as robots and drones, are vulnerable to malicious activities such device tampering (e.g., hardware and software changes).","The problem becomes even worse in a multi-stakeholder environment where multiple players contribute to an ecosystem.   ","In such scenarios, particularly, when devices are deployed in remote settings, ensuring device integrity so that all stakeholders can trust them is challenging.","Existing methods, often depend on additional hardware like the Trusted Platform Module (TPM) which may not be universally provided by all vendors.","In this study, we introduce a distributed ledger technology-oriented architecture to monitor the remote devices' integrity using eUICC technology, a feature commonly found in industrial devices for cellular connectivity.","We propose that using secure applets in eUICC, devices' integrity can be monitored and managed without installing any additional hardware.   ","To this end, we present an end-to-end architecture to monitor device integrity thereby enabling all the stakeholders in the system to trust the devices.","Additionally, we leverage the properties of immutable databases to provide robustness and efficiently to our model.","In our primary evaluations, we measure the overhead caused by hashing our proposed data packets and performance of integrating an immutable database into our system.","Our results show that performing hashing on our data packets takes order of microseconds, while reading and writing to an immutable database also requires only milliseconds."],"url":"http://arxiv.org/abs/2405.09916v1","category":"cs.CR"}
{"created":"2024-05-16 09:11:13","title":"Sparse Regression Codes for Non-Coherent SIMO channels","abstract":"We study the sparse regression codes over flat-fading channels with multiple receive antennas. We consider a practical scenario where the channel state information is not available at the transmitter and the receiver. In this setting, we study the maximum likelihood (ML) detector for SPARC, which has a prohibitively high search complexity. We propose a novel practical decoder, named maximum likelihood matching pursuit (MLMP), which incorporates a greedy search mechanism along with the ML metric. We also introduce a parallel search mechanism for MLMP. Comparing with the existing block-orthogonal matching pursuit based decoders, we show that MLMP has significant gains in the block error rate (BLER) performance. We also show that the proposed approach has significant gains over polar codes employing pilot-aided channel estimation.","sentences":["We study the sparse regression codes over flat-fading channels with multiple receive antennas.","We consider a practical scenario where the channel state information is not available at the transmitter and the receiver.","In this setting, we study the maximum likelihood (ML) detector for SPARC, which has a prohibitively high search complexity.","We propose a novel practical decoder, named maximum likelihood matching pursuit (MLMP), which incorporates a greedy search mechanism along with the ML metric.","We also introduce a parallel search mechanism for MLMP.","Comparing with the existing block-orthogonal matching pursuit based decoders, we show that MLMP has significant gains in the block error rate (BLER) performance.","We also show that the proposed approach has significant gains over polar codes employing pilot-aided channel estimation."],"url":"http://arxiv.org/abs/2405.09915v1","category":"eess.SP"}
{"created":"2024-05-16 09:08:24","title":"Distributed Joint User Activity Detection, Channel Estimation, and Data Detection via Expectation Propagation in Cell-Free Massive MIMO","abstract":"We consider the uplink of a grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) system. We propose an algorithm for distributed joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP) called JACD-EP. We develop the algorithm by factorizing the a posteriori probability (APP) of activities, channels, and transmitted data, then, mapping functions and variables onto a factor graph, and finally, performing a message passing on the resulting factor graph. If users with the same pilot sequence are sufficiently distant from each other, the JACD-EP algorithm is able to mitigate the effects of pilot contamination which naturally occurs in grant-free systems due to the large number of potential users and limited signaling resources. Furthermore, it outperforms state-of-the-art algorithms for JACD in GF-CF-MaMIMO systems.","sentences":["We consider the uplink of a grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) system.","We propose an algorithm for distributed joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP) called JACD-EP.","We develop the algorithm by factorizing the a posteriori probability (APP) of activities, channels, and transmitted data, then, mapping functions and variables onto a factor graph, and finally, performing a message passing on the resulting factor graph.","If users with the same pilot sequence are sufficiently distant from each other, the JACD-EP algorithm is able to mitigate the effects of pilot contamination which naturally occurs in grant-free systems due to the large number of potential users and limited signaling resources.","Furthermore, it outperforms state-of-the-art algorithms for JACD in GF-CF-MaMIMO systems."],"url":"http://arxiv.org/abs/2405.09914v1","category":"cs.IT"}
{"created":"2024-05-16 08:56:56","title":"On a compressible fluid-structure interaction problem with slip boundary conditions","abstract":"We study a system describing the compressible barotropic fluids interacting with (visco) elastic solid shell/plate. In particular, the elastic structure is part of the moving boundary of the fluid, and the Navier-slip type boundary condition is taken into account. Depending on the reference geometry (flat or not), we show the existence of weak solutions to the coupled system provided the adiabatic exponent satisfies $\\gamma > \\frac{12}{7}$ without damping and $\\gamma > \\frac{3}{2}$ with structure damping, utilizing the domain extension and regularization approximation. Moreover, via a modified relative entropy method in time-dependent domains, we prove the weak-strong uniqueness property of weak solutions. Finally, we give a rigorous justification of the incompressible inviscid limit of the compressible fluid-structure interaction problem with a flat reference geometry, in the regime of low Mach number, high Reynolds number, and well-prepared initial data. As a byproduct, by low Mach number we also derive the incompressible limit with reduced assumptions on the regularity of the structure but with a stronger assumption on the exponent of $\\gamma$.","sentences":["We study a system describing the compressible barotropic fluids interacting with (visco) elastic solid shell/plate.","In particular, the elastic structure is part of the moving boundary of the fluid, and the Navier-slip type boundary condition is taken into account.","Depending on the reference geometry (flat or not), we show the existence of weak solutions to the coupled system provided the adiabatic exponent satisfies $\\gamma > \\frac{12}{7}$ without damping and $\\gamma > \\frac{3}{2}$ with structure damping, utilizing the domain extension and regularization approximation.","Moreover, via a modified relative entropy method in time-dependent domains, we prove the weak-strong uniqueness property of weak solutions.","Finally, we give a rigorous justification of the incompressible inviscid limit of the compressible fluid-structure interaction problem with a flat reference geometry, in the regime of low Mach number, high Reynolds number, and well-prepared initial data.","As a byproduct, by low Mach number we also derive the incompressible limit with reduced assumptions on the regularity of the structure but with a stronger assumption on the exponent of $\\gamma$."],"url":"http://arxiv.org/abs/2405.09908v1","category":"math.AP"}
{"created":"2024-05-16 08:56:09","title":"End-to-end Optimization of Optical Communication Systems based on Directly Modulated Lasers","abstract":"The use of directly modulated lasers (DMLs) is attractive in low-power, cost-constrained short-reach optical links. However, their limited modulation bandwidth can induce waveform distortion, undermining their data throughput. Traditional distortion mitigation techniques have relied mainly on the separate training of transmitter-side pre-distortion and receiver-side equalization. This approach overlooks the potential gains obtained by simultaneous optimization of transmitter (constellation and pulse shaping) and receiver (equalization and symbol demapping). Moreover, in the context of DML operation, the choice of laser-driving configuration parameters such as the bias current and peak-to-peak modulation current has a significant impact on system performance. We propose a novel end-to-end optimization approach for DML systems, incorporating the learning of bias and peak-to-peak modulation current to the optimization of constellation points, pulse shaping and equalization. The simulation of the DML dynamics is based on the use of the laser rate equations at symbol rates between 15 and 25 Gbaud. The resulting output sequences from the rate equations are used to build a differentiable data-driven model, simplifying the calculation of gradients needed for end-to-end optimization. The proposed end-to-end approach is compared to 3 additional benchmark approaches: the uncompensated system without equalization, a receiver-side finite impulse response equalization approach and an end-to-end approach with learnable pulse shape and nonlinear Volterra equalization but fixed bias and peak-to-peak modulation current. The numerical simulations on the four approaches show that the joint optimization of bias, peak-to-peak current, constellation points, pulse shaping and equalization outperforms all other approaches throughout the tested symbol rates.","sentences":["The use of directly modulated lasers (DMLs) is attractive in low-power, cost-constrained short-reach optical links.","However, their limited modulation bandwidth can induce waveform distortion, undermining their data throughput.","Traditional distortion mitigation techniques have relied mainly on the separate training of transmitter-side pre-distortion and receiver-side equalization.","This approach overlooks the potential gains obtained by simultaneous optimization of transmitter (constellation and pulse shaping) and receiver (equalization and symbol demapping).","Moreover, in the context of DML operation, the choice of laser-driving configuration parameters such as the bias current and peak-to-peak modulation current has a significant impact on system performance.","We propose a novel end-to-end optimization approach for DML systems, incorporating the learning of bias and peak-to-peak modulation current to the optimization of constellation points, pulse shaping and equalization.","The simulation of the DML dynamics is based on the use of the laser rate equations at symbol rates between 15 and 25 Gbaud.","The resulting output sequences from the rate equations are used to build a differentiable data-driven model, simplifying the calculation of gradients needed for end-to-end optimization.","The proposed end-to-end approach is compared to 3 additional benchmark approaches: the uncompensated system without equalization, a receiver-side finite impulse response equalization approach and an end-to-end approach with learnable pulse shape and nonlinear Volterra equalization but fixed bias and peak-to-peak modulation current.","The numerical simulations on the four approaches show that the joint optimization of bias, peak-to-peak current, constellation points, pulse shaping and equalization outperforms all other approaches throughout the tested symbol rates."],"url":"http://arxiv.org/abs/2405.09907v1","category":"eess.SP"}
{"created":"2024-05-16 08:52:07","title":"The youngest of hot jupiters in action: episodic accretion outbursts in Gaia20eae","abstract":"Recent imaging observations with ALMA and other telescopes found widespread signatures of planet presence in protoplanetary discs at tens of au separations from their host stars. Here we point out that the presence of very massive planets at 0.1 au sized orbits can be deduced for protostars accreting gas at very high rates, when their discs display powerful Thermal Instability bursts. Earlier work showed that a massive planet modifies the nature of this instability, with outbursts triggered at the outer edge of the deep gap opened by the planet. We present simulations of this effect, finding two types of TI outbursts: downstream and upstream of the planet, which may or may not be causally connected. We apply our model to the outburst in Gaia20eae. We find that the agreement between the data and our disc thermal instability model is improved if there is a planet of 6 Jupiter masses orbiting the star at 0.062 au separation. Gaia20eae thus becomes the second episodically erupting star, after FU Ori, where the presence of a massive planet is strongly suspected. Future observations of similar systems will constrain the mode and the frequency of planet formation in such an early epoch.","sentences":["Recent imaging observations with ALMA and other telescopes found widespread signatures of planet presence in protoplanetary discs at tens of au separations from their host stars.","Here we point out that the presence of very massive planets at 0.1 au sized orbits can be deduced for protostars accreting gas at very high rates, when their discs display powerful Thermal Instability bursts.","Earlier work showed that a massive planet modifies the nature of this instability, with outbursts triggered at the outer edge of the deep gap opened by the planet.","We present simulations of this effect, finding two types of TI outbursts:","downstream and upstream of the planet, which may or may not be causally connected.","We apply our model to the outburst in Gaia20eae.","We find that the agreement between the data and our disc thermal instability model is improved if there is a planet of 6 Jupiter masses orbiting the star at 0.062 au separation.","Gaia20eae thus becomes the second episodically erupting star, after FU Ori, where the presence of a massive planet is strongly suspected.","Future observations of similar systems will constrain the mode and the frequency of planet formation in such an early epoch."],"url":"http://arxiv.org/abs/2405.09904v1","category":"astro-ph.EP"}
{"created":"2024-05-16 08:31:44","title":"Measuring the Fitness-for-Purpose of Requirements: An initial Model of Activities and Attributes","abstract":"Requirements engineering aims to fulfill a purpose, i.e., inform subsequent software development activities about stakeholders' needs and constraints that must be met by the system under development. The quality of requirements artifacts and processes is determined by how fit for this purpose they are, i.e., how they impact activities affected by them. However, research on requirements quality lacks a comprehensive overview of these activities and how to measure them. In this paper, we specify the research endeavor addressing this gap and propose an initial model of requirements-affected activities and their attributes. We construct a model from three distinct data sources, including both literature and empirical data. The results yield an initial model containing 24 activities and 16 attributes quantifying these activities. Our long-term goal is to develop evidence-based decision support on how to optimize the fitness for purpose of the RE phase to best support the subsequent, affected software development process. We do so by measuring the effect that requirements artifacts and processes have on the attributes of these activities. With the contribution at hand, we invite the research community to critically discuss our research roadmap and support the further evolution of the model.","sentences":["Requirements engineering aims to fulfill a purpose, i.e., inform subsequent software development activities about stakeholders' needs and constraints that must be met by the system under development.","The quality of requirements artifacts and processes is determined by how fit for this purpose they are, i.e., how they impact activities affected by them.","However, research on requirements quality lacks a comprehensive overview of these activities and how to measure them.","In this paper, we specify the research endeavor addressing this gap and propose an initial model of requirements-affected activities and their attributes.","We construct a model from three distinct data sources, including both literature and empirical data.","The results yield an initial model containing 24 activities and 16 attributes quantifying these activities.","Our long-term goal is to develop evidence-based decision support on how to optimize the fitness for purpose of the RE phase to best support the subsequent, affected software development process.","We do so by measuring the effect that requirements artifacts and processes have on the attributes of these activities.","With the contribution at hand, we invite the research community to critically discuss our research roadmap and support the further evolution of the model."],"url":"http://arxiv.org/abs/2405.09895v1","category":"cs.SE"}
{"created":"2024-05-16 08:16:19","title":"Balancing Similarity and Complementarity for Federated Learning","abstract":"In mobile and IoT systems, Federated Learning (FL) is increasingly important for effectively using data while maintaining user privacy. One key challenge in FL is managing statistical heterogeneity, such as non-i.i.d. data, arising from numerous clients and diverse data sources. This requires strategic cooperation, often with clients having similar characteristics. However, we are interested in a fundamental question: does achieving optimal cooperation necessarily entail cooperating with the most similar clients? Typically, significant model performance improvements are often realized not by partnering with the most similar models, but through leveraging complementary data. Our theoretical and empirical analyses suggest that optimal cooperation is achieved by enhancing complementarity in feature distribution while restricting the disparity in the correlation between features and targets. Accordingly, we introduce a novel framework, \\texttt{FedSaC}, which balances similarity and complementarity in FL cooperation. Our framework aims to approximate an optimal cooperation network for each client by optimizing a weighted sum of model similarity and feature complementarity. The strength of \\texttt{FedSaC} lies in its adaptability to various levels of data heterogeneity and multimodal scenarios. Our comprehensive unimodal and multimodal experiments demonstrate that \\texttt{FedSaC} markedly surpasses other state-of-the-art FL methods.","sentences":["In mobile and IoT systems, Federated Learning (FL) is increasingly important for effectively using data while maintaining user privacy.","One key challenge in FL is managing statistical heterogeneity, such as non-i.i.d. data, arising from numerous clients and diverse data sources.","This requires strategic cooperation, often with clients having similar characteristics.","However, we are interested in a fundamental question: does achieving optimal cooperation necessarily entail cooperating with the most similar clients?","Typically, significant model performance improvements are often realized not by partnering with the most similar models, but through leveraging complementary data.","Our theoretical and empirical analyses suggest that optimal cooperation is achieved by enhancing complementarity in feature distribution while restricting the disparity in the correlation between features and targets.","Accordingly, we introduce a novel framework, \\texttt{FedSaC}, which balances similarity and complementarity in FL cooperation.","Our framework aims to approximate an optimal cooperation network for each client by optimizing a weighted sum of model similarity and feature complementarity.","The strength of \\texttt{FedSaC} lies in its adaptability to various levels of data heterogeneity and multimodal scenarios.","Our comprehensive unimodal and multimodal experiments demonstrate that \\texttt{FedSaC} markedly surpasses other state-of-the-art FL methods."],"url":"http://arxiv.org/abs/2405.09892v1","category":"cs.LG"}
{"created":"2024-05-16 08:15:11","title":"Fe-FeH Eutectic Melting Curve and the Estimates of Earth's Core Temperature and Composition","abstract":"Fe and FeH form a binary eutectic system above ~40 GPa. Here we performed melting experiments in a laser-heated diamond-anvil cell (DAC) and obtained the Fe-FeH eutectic melting curve between 52 and 175 GPa. Its extrapolation shows the eutectic temperature to be 4700 K at the inner core boundary (ICB), which is lower than that in Fe-FeSi but is higher than those in the Fe-S, Fe-O, and Fe-C systems. In addition, its dT/dP slope is comparable to those of the melting curves of Fe and FeH endmembers, suggesting that the eutectic liquid composition changes little with increasing pressure and is about FeH0.6 at the ICB pressure. We also estimated the effect of each light element on depressing the liquidus temperature at 330 GPa based on a combination of binary eutectic temperature and composition and found that the effect is large for C and S, moderate for H and O, and small for Si when considering the amount of each element that reduces a certain percentage of a liquid iron density. Furthermore, we searched for a set of possible outer core liquid composition and ICB temperature (the liquidus temperature of the former at 330 GPa should match the latter), which explains the outer core density deficit that depends on core temperature. The results demonstrate that relatively low core temperatures, lower than the solidus temperature of a pyrolitic lowermost mantle at the core-mantle boundary (CMB), are possible when the core is poor in Si.","sentences":["Fe and FeH form a binary eutectic system above ~40 GPa.","Here we performed melting experiments in a laser-heated diamond-anvil cell (DAC) and obtained the Fe-FeH eutectic melting curve between 52 and 175 GPa.","Its extrapolation shows the eutectic temperature to be 4700 K at the inner core boundary (ICB), which is lower than that in Fe-FeSi but is higher than those in the Fe-S, Fe-O, and Fe-C systems.","In addition, its dT/dP slope is comparable to those of the melting curves of Fe and FeH endmembers, suggesting that the eutectic liquid composition changes little with increasing pressure and is about FeH0.6 at the ICB pressure.","We also estimated the effect of each light element on depressing the liquidus temperature at 330 GPa based on a combination of binary eutectic temperature and composition and found that the effect is large for C and S, moderate for H and O, and small for Si when considering the amount of each element that reduces a certain percentage of a liquid iron density.","Furthermore, we searched for a set of possible outer core liquid composition and ICB temperature (the liquidus temperature of the former at 330 GPa should match the latter), which explains the outer core density deficit that depends on core temperature.","The results demonstrate that relatively low core temperatures, lower than the solidus temperature of a pyrolitic lowermost mantle at the core-mantle boundary (CMB), are possible when the core is poor in Si."],"url":"http://arxiv.org/abs/2405.09890v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 08:10:05","title":"Quantization-based LHS for dependent inputs : application to sensitivity analysis of environmental models","abstract":"Numerical modeling is essential for comprehending intricate physical phenomena in different domains. To handle complexity, sensitivity analysis, particularly screening, is crucial for identifying influential input parameters. Kernel-based methods, such as the Hilbert Schmidt Independence Criterion (HSIC), are valuable for analyzing dependencies between inputs and outputs. Moreover, due to the computational expense of such models, metamodels (or surrogate models) are often unavoidable. Implementing metamodels and HSIC requires data from the original model, which leads to the need for space-filling designs. While existing methods like Latin Hypercube Sampling (LHS) are effective for independent variables, incorporating dependence is challenging. This paper introduces a novel LHS variant, Quantization-based LHS, which leverages Voronoi vector quantization to address correlated inputs. The method ensures comprehensive coverage of stratified variables, enhancing distribution across marginals. The paper outlines expectation estimators based on Quantization-based LHS in various dependency settings, demonstrating their unbiasedness. The method is applied on several models of growing complexities, first on simple examples to illustrate the theory, then on more complex environmental hydrological models, when the dependence is known or not, and with more and more interactive processes and factors. The last application is on the digital twin of a French vineyard catchment (Beaujolais region) to design a vegetative filter strip and reduce water, sediment and pesticide transfers from the fields to the river. Quantization-based LHS is used to compute HSIC measures and independence tests, demonstrating its usefulness, especially in the context of complex models.","sentences":["Numerical modeling is essential for comprehending intricate physical phenomena in different domains.","To handle complexity, sensitivity analysis, particularly screening, is crucial for identifying influential input parameters.","Kernel-based methods, such as the Hilbert Schmidt Independence Criterion (HSIC), are valuable for analyzing dependencies between inputs and outputs.","Moreover, due to the computational expense of such models, metamodels (or surrogate models) are often unavoidable.","Implementing metamodels and HSIC requires data from the original model, which leads to the need for space-filling designs.","While existing methods like Latin Hypercube Sampling (LHS) are effective for independent variables, incorporating dependence is challenging.","This paper introduces a novel LHS variant, Quantization-based LHS, which leverages Voronoi vector quantization to address correlated inputs.","The method ensures comprehensive coverage of stratified variables, enhancing distribution across marginals.","The paper outlines expectation estimators based on Quantization-based LHS in various dependency settings, demonstrating their unbiasedness.","The method is applied on several models of growing complexities, first on simple examples to illustrate the theory, then on more complex environmental hydrological models, when the dependence is known or not, and with more and more interactive processes and factors.","The last application is on the digital twin of a French vineyard catchment (Beaujolais region) to design a vegetative filter strip and reduce water, sediment and pesticide transfers from the fields to the river.","Quantization-based LHS is used to compute HSIC measures and independence tests, demonstrating its usefulness, especially in the context of complex models."],"url":"http://arxiv.org/abs/2405.09887v1","category":"stat.ME"}
{"created":"2024-05-16 07:56:26","title":"Electron delocalization in a 2D Mott insulator","abstract":"The prominent role of electron-electron interactions in two-dimensional (2D) materials versus three-dimensional (3D) ones is at the origin of the great variety of fermionic correlated states reported in the literature. In this respect, artificial van der Waals heterostructures comprising single layers of highly correlated insulators allow one to explore the effect of the subtle interlayer interaction in the way electrons correlate. In this work, we study the temperature dependence of the electronic properties of a van der Waals heterostructure composed of a single-layer Mott insulator lying on a metallic substrate by performing quasi-particle interference (QPI) maps. We show the emergence of a Fermi contour in the 2D Mott insulator at temperatures below 11K, which we attribute to the delocalization of the Mott electrons associated with the formation of a quantum coherent Kondo lattice. This Kondo lattice introduces a new periodicity in the system, so that the resulting Fermi surface encompasses both the substrate conduction electrons and the now delocalized correlated electrons from the 2D Mott insulator. Density Functional Theory calculations allow us to pinpoint the scattering vectors responsible for the experimentally observed quasi-particle interference maps, thus providing a complete picture of the delocalization of highly correlated electrons in a 2D Mott insulator.","sentences":["The prominent role of electron-electron interactions in two-dimensional (2D) materials versus three-dimensional (3D) ones is at the origin of the great variety of fermionic correlated states reported in the literature.","In this respect, artificial van der Waals heterostructures comprising single layers of highly correlated insulators allow one to explore the effect of the subtle interlayer interaction in the way electrons correlate.","In this work, we study the temperature dependence of the electronic properties of a van der Waals heterostructure composed of a single-layer Mott insulator lying on a metallic substrate by performing quasi-particle interference (QPI) maps.","We show the emergence of a Fermi contour in the 2D Mott insulator at temperatures below 11K, which we attribute to the delocalization of the Mott electrons associated with the formation of a quantum coherent Kondo lattice.","This Kondo lattice introduces a new periodicity in the system, so that the resulting Fermi surface encompasses both the substrate conduction electrons and the now delocalized correlated electrons from the 2D Mott insulator.","Density Functional Theory calculations allow us to pinpoint the scattering vectors responsible for the experimentally observed quasi-particle interference maps, thus providing a complete picture of the delocalization of highly correlated electrons in a 2D Mott insulator."],"url":"http://arxiv.org/abs/2405.09877v1","category":"cond-mat.str-el"}
{"created":"2024-05-16 07:43:31","title":"Haro 5-2: A New Pre-Main Sequence Quadruple Stellar System","abstract":"We have discovered that the Halpha emission line star Haro 5-2, located in the 3-6 Myr old Ori OB1b association, is a young quadruple system. The system has a 2+2 configuration with an outer separation of 2.6 arcseconds and with resolved subarcsecond inner binary components. The brightest component, Aa, dominates the A-binary, it is a weakline T Tauri star with spectral type M2.5pm1. The two stars of the B component are equally bright at J, but the Bb star is much redder. Optical spectroscopy of the combined B pair indicates a rich emission line spectrum with a M3pm1 spectral type. The spectrum is highly variable and switches back and forth between a classical and a weakline T Tauri star. In the near-infrared, the spectrum shows Paschen beta and Brackett gamma in emission, indicative of active accretion. A significant mid-infrared excess reveals the presence of circumstellar or circumbinary material in the system. Most multiple systems are likely formed during the protostellar phase, involving flybys of neighboring stars followed by an in-spiraling phase driven by accretion from circumbinary material and leading to compact sub-systems. However, Haro 5-2 stands out among young 2+2 quadruples as the two inner binaries are unusually wide relative to the separation of the A and B pair, allowing future studies of the individual components. Assuming the components are coeval, the system could potentially allow stringent tests of PMS evolutionary models.","sentences":["We have discovered that the Halpha emission line star Haro 5-2, located in the 3-6 Myr old Ori OB1b association, is a young quadruple system.","The system has a 2+2 configuration with an outer separation of 2.6 arcseconds and with resolved subarcsecond inner binary components.","The brightest component, Aa, dominates the A-binary, it is a weakline T Tauri star with spectral type M2.5pm1.","The two stars of the B component are equally bright at J, but the Bb star is much redder.","Optical spectroscopy of the combined B pair indicates a rich emission line spectrum with a M3pm1 spectral type.","The spectrum is highly variable and switches back and forth between a classical and a weakline T Tauri star.","In the near-infrared, the spectrum shows Paschen beta and Brackett gamma in emission, indicative of active accretion.","A significant mid-infrared excess reveals the presence of circumstellar or circumbinary material in the system.","Most multiple systems are likely formed during the protostellar phase, involving flybys of neighboring stars followed by an in-spiraling phase driven by accretion from circumbinary material and leading to compact sub-systems.","However, Haro 5-2 stands out among young 2+2 quadruples as the two inner binaries are unusually wide relative to the separation of the A and B pair, allowing future studies of the individual components.","Assuming the components are coeval, the system could potentially allow stringent tests of PMS evolutionary models."],"url":"http://arxiv.org/abs/2405.09867v1","category":"astro-ph.SR"}
{"created":"2024-05-16 07:43:15","title":"Rethinking Multi-User Semantic Communications with Deep Generative Models","abstract":"In recent years, novel communication strategies have emerged to face the challenges that the increased number of connected devices and the higher quality of transmitted information are posing. Among them, semantic communication obtained promising results especially when combined with state-of-the-art deep generative models, such as large language or diffusion models, able to regenerate content from extremely compressed semantic information. However, most of these approaches focus on single-user scenarios processing the received content at the receiver on top of conventional communication systems. In this paper, we propose to go beyond these methods by developing a novel generative semantic communication framework tailored for multi-user scenarios. This system assigns the channel to users knowing that the lost information can be filled in with a diffusion model at the receivers. Under this innovative perspective, OFDMA systems should not aim to transmit the largest part of information, but solely the bits necessary to the generative model to semantically regenerate the missing ones. The thorough experimental evaluation shows the capabilities of the novel diffusion model and the effectiveness of the proposed framework, leading towards a GenAI-based next generation of communications.","sentences":["In recent years, novel communication strategies have emerged to face the challenges that the increased number of connected devices and the higher quality of transmitted information are posing.","Among them, semantic communication obtained promising results especially when combined with state-of-the-art deep generative models, such as large language or diffusion models, able to regenerate content from extremely compressed semantic information.","However, most of these approaches focus on single-user scenarios processing the received content at the receiver on top of conventional communication systems.","In this paper, we propose to go beyond these methods by developing a novel generative semantic communication framework tailored for multi-user scenarios.","This system assigns the channel to users knowing that the lost information can be filled in with a diffusion model at the receivers.","Under this innovative perspective, OFDMA systems should not aim to transmit the largest part of information, but solely the bits necessary to the generative model to semantically regenerate the missing ones.","The thorough experimental evaluation shows the capabilities of the novel diffusion model and the effectiveness of the proposed framework, leading towards a GenAI-based next generation of communications."],"url":"http://arxiv.org/abs/2405.09866v1","category":"eess.SP"}
{"created":"2024-05-16 07:06:08","title":"Chiral symmetry breaking in the pseudo-quantum electrodynamics with non-Abelian four-fermion interactions","abstract":"In the context of 2+1 dimensional Dirac materials, we consider electromagnetic interactions alongside a type of spin-dependent Hubbard interaction. The former is described by PQED theory, while the latter corresponds to an effective theory represented by the $SU(N_c)$ Thirring model. Employing Hubbard-Stratonovich transformation and large N expansion in the model yields a non-local $SU(N_c)$ Yang-Mills action. Subsequently, we solve Schwinger-Dyson equations to obtain the self-energy function of the fermion propagator, from which we determine the critical fermion flavor number $N^c_f$ and critical fine structure constant $\\alpha_c$ indicative of chiral symmetry breaking. Our findings suggest that as the non-Abelian color number $N_c$ increases, the minimum value of the critical fermion flavor number monotonically increases, while the maximum value of the critical fine structure constant decreases accordingly, rendering the system more susceptible to chiral symmetry breaking.","sentences":["In the context of 2+1 dimensional Dirac materials, we consider electromagnetic interactions alongside a type of spin-dependent Hubbard interaction.","The former is described by PQED theory, while the latter corresponds to an effective theory represented by the $SU(N_c)$ Thirring model.","Employing Hubbard-Stratonovich transformation and large N expansion in the model yields a non-local $SU(N_c)$ Yang-Mills action.","Subsequently, we solve Schwinger-Dyson equations to obtain the self-energy function of the fermion propagator, from which we determine the critical fermion flavor number $N^c_f$ and critical fine structure constant $\\alpha_c$ indicative of chiral symmetry breaking.","Our findings suggest that as the non-Abelian color number $N_c$ increases, the minimum value of the critical fermion flavor number monotonically increases, while the maximum value of the critical fine structure constant decreases accordingly, rendering the system more susceptible to chiral symmetry breaking."],"url":"http://arxiv.org/abs/2405.09853v1","category":"hep-th"}
{"created":"2024-05-16 07:01:23","title":"Adaptive tracking MPC for nonlinear systems via online linear system identification","abstract":"This paper presents an adaptive tracking model predictive control (MPC) scheme to control unknown nonlinear systems based on an adaptively estimated linear model. The model is determined based on linear system identification using a moving window of past measurements, and it serves as a local approximation of the underlying nonlinear dynamics. We prove that the presented scheme ensures practical exponential stability of the (unknown) optimal reachable equilibrium for a given output setpoint. Finally, we apply the proposed scheme in simulation and compare it to an alternative direct data-driven MPC scheme based on the Fundamental Lemma.","sentences":["This paper presents an adaptive tracking model predictive control (MPC) scheme to control unknown nonlinear systems based on an adaptively estimated linear model.","The model is determined based on linear system identification using a moving window of past measurements, and it serves as a local approximation of the underlying nonlinear dynamics.","We prove that the presented scheme ensures practical exponential stability of the (unknown) optimal reachable equilibrium for a given output setpoint.","Finally, we apply the proposed scheme in simulation and compare it to an alternative direct data-driven MPC scheme based on the Fundamental Lemma."],"url":"http://arxiv.org/abs/2405.09852v1","category":"eess.SY"}
{"created":"2024-05-16 07:00:42","title":"Thermally activated particle motion in biased correlated Gaussian disorder potentials","abstract":"Thermally activated particle motion in disorder potentials is controlled by the large-$\\Delta V$ tail of the distribution of height $\\Delta V$ of the potential barriers created by the disorder. We employ the optimal fluctuation method to evaluate this tail for correlated quenched Gaussian potentials in one dimension in the presence of a small bias of the potential. We focus on the mean escape time (MET) of overdamped particles averaged over the disorder. We show that the bias leads to a strong (exponential) reduction of the MET in the direction along the bias. The reduction depends both on the bias, and on detailed properties of the covariance of the disorder, such as its derivatives and asymptotic behavior at large distances. We verify our theoretical predictions, as well as earlier predictions for zero bias, by performing large-deviation simulations of the potential disorder. The simulations employ correlated random potential sampling based on the circulant embedding method and the Wang-Landau algorithm, which enable us to probe probability densities smaller than $10^{-1200}$.","sentences":["Thermally activated particle motion in disorder potentials is controlled by the large-$\\Delta V$ tail of the distribution of height $\\Delta V$ of the potential barriers created by the disorder.","We employ the optimal fluctuation method to evaluate this tail for correlated quenched Gaussian potentials in one dimension in the presence of a small bias of the potential.","We focus on the mean escape time (MET) of overdamped particles averaged over the disorder.","We show that the bias leads to a strong (exponential) reduction of the MET in the direction along the bias.","The reduction depends both on the bias, and on detailed properties of the covariance of the disorder, such as its derivatives and asymptotic behavior at large distances.","We verify our theoretical predictions, as well as earlier predictions for zero bias, by performing large-deviation simulations of the potential disorder.","The simulations employ correlated random potential sampling based on the circulant embedding method and the Wang-Landau algorithm, which enable us to probe probability densities smaller than $10^{-1200}$."],"url":"http://arxiv.org/abs/2405.09850v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-16 06:47:10","title":"Nanomechanically induced transparency in $\\mathcal{PT}$-symmetric optical cavities","abstract":"In this paper, we analytically present the phenomena of nanomechanically induced transparency (NMIT) and transmission rate in a parity-time-symmetric ($\\mathcal{PT}$-symmetric) opto-nanomechanical system (ONMS) where a levitated dielectric nanospheres is trapped near the antinodes closest to right mirror of passive cavity which further coupled to an active cavity via hoping factor. We find that the phenomenon of NMIT may be generated from the output probe field in the presence of an effective opto-nanomechanical coupling between the cavity field and the nanosphere, whose steady-state position is influenced by the Coulomb interaction between the cavity mirror and the nanosphere. In addition, the width and height of the transparency window can be controlled through the effective optomechanical coupling, which is readily adjusted by altering changing the nanosphere's radius and the Coulomb interaction. One of the most interesting result is the transition NMIT behavior in $\\mathcal{PT}$-symmetric and broken $\\mathcal{PT}$-symmetric regime. We show that the presence of nanosphere in the passive cavity enhances the width and transmission rate of NMIT window in passive-passive regime and in passive-active regime, a notable decrease of sideband amplification has been observed. These results show that our scheme may find some potential applications for optical signal processing an and quantum information processing.","sentences":["In this paper, we analytically present the phenomena of nanomechanically induced transparency (NMIT) and transmission rate in a parity-time-symmetric ($\\mathcal{PT}$-symmetric) opto-nanomechanical system (ONMS) where a levitated dielectric nanospheres is trapped near the antinodes closest to right mirror of passive cavity which further coupled to an active cavity via hoping factor.","We find that the phenomenon of NMIT may be generated from the output probe field in the presence of an effective opto-nanomechanical coupling between the cavity field and the nanosphere, whose steady-state position is influenced by the Coulomb interaction between the cavity mirror and the nanosphere.","In addition, the width and height of the transparency window can be controlled through the effective optomechanical coupling, which is readily adjusted by altering changing the nanosphere's radius and the Coulomb interaction.","One of the most interesting result is the transition NMIT behavior in $\\mathcal{PT}$-symmetric and broken $\\mathcal{PT}$-symmetric regime.","We show that the presence of nanosphere in the passive cavity enhances the width and transmission rate of NMIT window in passive-passive regime and in passive-active regime, a notable decrease of sideband amplification has been observed.","These results show that our scheme may find some potential applications for optical signal processing an and quantum information processing."],"url":"http://arxiv.org/abs/2405.09845v1","category":"quant-ph"}
{"created":"2024-05-16 06:43:03","title":"Why Superconducting Ta Qubits Have Fewer Tunneling Two-Level Systems at the Air-Oxide Interface Than Nb Qubits","abstract":"Superconducting qubits are a key contender for quantum computing elements, but they often face challenges like noise and decoherence from two-level systems (TLS). Tantalum (Ta) qubits are notable for their long T$_1$ coherence times nearing milliseconds, mainly due to fewer TLS, though the cause was unclear. Our research explored this by analyzing the air-oxide interface with density functional theory, particularly comparing Nb oxide (Nb$_2$O$_5$) and Ta oxide (Ta$_2$O$_5$). We discovered that Ta$_2$O$_5$ forms a smoother surface with fewer dangling O atoms and TLS than Nb$_2$O$_5$. The greater atomic mass of Ta also lowers the TLS tunnel splittings below the qubit's operating frequency. Furthermore, using external electric fields or SO$_2$ passivation can significantly reduce TLS on Nb surfaces, potentially improving their coherence times.","sentences":["Superconducting qubits are a key contender for quantum computing elements, but they often face challenges like noise and decoherence from two-level systems (TLS).","Tantalum (Ta) qubits are notable for their long T$_1$ coherence times nearing milliseconds, mainly due to fewer TLS, though the cause was unclear.","Our research explored this by analyzing the air-oxide interface with density functional theory, particularly comparing Nb oxide (Nb$_2$O$_5$) and Ta oxide (Ta$_2$O$_5$).","We discovered that Ta$_2$O$_5$ forms a smoother surface with fewer dangling O atoms and TLS than Nb$_2$O$_5$. The greater atomic mass of Ta also lowers the TLS tunnel splittings below the qubit's operating frequency.","Furthermore, using external electric fields or SO$_2$ passivation can significantly reduce TLS on Nb surfaces, potentially improving their coherence times."],"url":"http://arxiv.org/abs/2405.09842v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 06:35:42","title":"Advances in Robust Federated Learning: Heterogeneity Considerations","abstract":"In the field of heterogeneous federated learning (FL), the key challenge is to efficiently and collaboratively train models across multiple clients with different data distributions, model structures, task objectives, computational capabilities, and communication resources. This diversity leads to significant heterogeneity, which increases the complexity of model training. In this paper, we first outline the basic concepts of heterogeneous federated learning and summarize the research challenges in federated learning in terms of five aspects: data, model, task, device, and communication. In addition, we explore how existing state-of-the-art approaches cope with the heterogeneity of federated learning, and categorize and review these approaches at three different levels: data-level, model-level, and architecture-level. Subsequently, the paper extensively discusses privacy-preserving strategies in heterogeneous federated learning environments. Finally, the paper discusses current open issues and directions for future research, aiming to promote the further development of heterogeneous federated learning.","sentences":["In the field of heterogeneous federated learning (FL), the key challenge is to efficiently and collaboratively train models across multiple clients with different data distributions, model structures, task objectives, computational capabilities, and communication resources.","This diversity leads to significant heterogeneity, which increases the complexity of model training.","In this paper, we first outline the basic concepts of heterogeneous federated learning and summarize the research challenges in federated learning in terms of five aspects: data, model, task, device, and communication.","In addition, we explore how existing state-of-the-art approaches cope with the heterogeneity of federated learning, and categorize and review these approaches at three different levels: data-level, model-level, and architecture-level.","Subsequently, the paper extensively discusses privacy-preserving strategies in heterogeneous federated learning environments.","Finally, the paper discusses current open issues and directions for future research, aiming to promote the further development of heterogeneous federated learning."],"url":"http://arxiv.org/abs/2405.09839v1","category":"cs.LG"}
{"created":"2024-05-16 06:31:02","title":"Unsupervised Work Behavior Pattern Extraction Based on Hierarchical Probabilistic Model","abstract":"Evolving consumer demands and market trends have led to businesses increasingly embracing a production approach that prioritizes flexibility and customization. Consequently, factory workers must engage in tasks that are more complex than before. Thus, productivity depends on each worker's skills in assembling products. Therefore, analyzing the behavior of a worker is crucial for work improvement. However, manual analysis is time consuming and does not provide quick and accurate feedback. Machine learning have been attempted to automate the analyses; however, most of these methods need several labels for training. To this end, we extend the Gaussian process hidden semi-Markov model (GP-HSMM), to enable the rapid and automated analysis of worker behavior without pre-training. The model does not require labeled data and can automatically and accurately segment continuous motions into motion classes. The proposed model is a probabilistic model that hierarchically connects GP-HSMM and HSMM, enabling the extraction of behavioral patterns with different granularities. Furthermore, it mutually infers the parameters between the GP-HSMM and HSMM, resulting in accurate motion pattern extraction. We applied the proposed method to motion data in which workers assembled products at an actual production site. The accuracy of behavior pattern extraction was evaluated using normalized Levenshtein distance (NLD). The smaller the value of NLD, the more accurate is the pattern extraction. The NLD of motion patterns captured by GP-HSMM and HSMM layers in our proposed method was 0.50 and 0.33, respectively, which are the smallest compared to that of the baseline methods.","sentences":["Evolving consumer demands and market trends have led to businesses increasingly embracing a production approach that prioritizes flexibility and customization.","Consequently, factory workers must engage in tasks that are more complex than before.","Thus, productivity depends on each worker's skills in assembling products.","Therefore, analyzing the behavior of a worker is crucial for work improvement.","However, manual analysis is time consuming and does not provide quick and accurate feedback.","Machine learning have been attempted to automate the analyses; however, most of these methods need several labels for training.","To this end, we extend the Gaussian process hidden semi-Markov model (GP-HSMM), to enable the rapid and automated analysis of worker behavior without pre-training.","The model does not require labeled data and can automatically and accurately segment continuous motions into motion classes.","The proposed model is a probabilistic model that hierarchically connects GP-HSMM and HSMM, enabling the extraction of behavioral patterns with different granularities.","Furthermore, it mutually infers the parameters between the GP-HSMM and HSMM, resulting in accurate motion pattern extraction.","We applied the proposed method to motion data in which workers assembled products at an actual production site.","The accuracy of behavior pattern extraction was evaluated using normalized Levenshtein distance (NLD).","The smaller the value of NLD, the more accurate is the pattern extraction.","The NLD of motion patterns captured by GP-HSMM and HSMM layers in our proposed method was 0.50 and 0.33, respectively, which are the smallest compared to that of the baseline methods."],"url":"http://arxiv.org/abs/2405.09838v1","category":"cs.LG"}
{"created":"2024-05-16 06:16:52","title":"Topological Floquet engineering of a three-band optical lattice with dual-mode resonant driving","abstract":"We present a Floquet framework for controlling topological features of a one-dimensional optical lattice system with dual-mode resonant driving, in which both the amplitude and phase of the lattice potential are modulated simultaneously. We investigate a three-band model consisting of the three lowest orbitals and elucidate the formation of a cross-linked two-leg ladder through an indirect interband coupling via an off-resonant band. We numerically demonstrate the emergence of topologically nontrivial bands within the driven system, and a topological charge pumping phenomenon with cyclic parameter changes in the dual-mode resonant driving. Finally, we show that the band topology in the driven three-band system is protected by parity-time reversal symmetry.","sentences":["We present a Floquet framework for controlling topological features of a one-dimensional optical lattice system with dual-mode resonant driving, in which both the amplitude and phase of the lattice potential are modulated simultaneously.","We investigate a three-band model consisting of the three lowest orbitals and elucidate the formation of a cross-linked two-leg ladder through an indirect interband coupling via an off-resonant band.","We numerically demonstrate the emergence of topologically nontrivial bands within the driven system, and a topological charge pumping phenomenon with cyclic parameter changes in the dual-mode resonant driving.","Finally, we show that the band topology in the driven three-band system is protected by parity-time reversal symmetry."],"url":"http://arxiv.org/abs/2405.09834v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-16 06:06:04","title":"Quantum Systems from Random Probabilistic Automata","abstract":"Probabilistic cellular automata with deterministic updating are quantum systems. We employ the quantum formalism for an investigation of random probabilistic cellular automata, which start with a probability distribution over initial configurations. The properties of the deterministic updating are randomly distributed over space and time. We are interested in a possible continuum limit for a very large number of cells. As an example we consider bits with two colors, moving to the left or right on a linear chain. At randomly distributed scattering points, they change direction and color. A numerical simulation reveals the typical features of quantum systems. We find particular initial probability distributions which reemerge periodically after a certain number of time steps, as produced by the periodic evolution of energy eigenstates in quantum mechanics. Using a description in terms of wave functions allows to introduce statistical observables for momentum and energy. They characterize the probabilistic information without taking definite values for a given bit configuration, with a conceptual status similar to temperature in classical statistical thermal equilibrium. Conservation of energy and momentum are essential ingredients for the understanding of the evolution of our stochastic probabilistic automata. This evolution resembles in some aspects a single Dirac fermion in two dimensions with a random potential.","sentences":["Probabilistic cellular automata with deterministic updating are quantum systems.","We employ the quantum formalism for an investigation of random probabilistic cellular automata, which start with a probability distribution over initial configurations.","The properties of the deterministic updating are randomly distributed over space and time.","We are interested in a possible continuum limit for a very large number of cells.","As an example we consider bits with two colors, moving to the left or right on a linear chain.","At randomly distributed scattering points, they change direction and color.","A numerical simulation reveals the typical features of quantum systems.","We find particular initial probability distributions which reemerge periodically after a certain number of time steps, as produced by the periodic evolution of energy eigenstates in quantum mechanics.","Using a description in terms of wave functions allows to introduce statistical observables for momentum and energy.","They characterize the probabilistic information without taking definite values for a given bit configuration, with a conceptual status similar to temperature in classical statistical thermal equilibrium.","Conservation of energy and momentum are essential ingredients for the understanding of the evolution of our stochastic probabilistic automata.","This evolution resembles in some aspects a single Dirac fermion in two dimensions with a random potential."],"url":"http://arxiv.org/abs/2405.09829v1","category":"quant-ph"}
{"created":"2024-05-16 05:54:19","title":"Lie symmetry analysis of (2+1)-dimensional time fractional Kadomtsev-Petviashvili equation","abstract":"In this paper, Lie symmetry analysis method is applied to the (2+1)-dimensional time fractional Kadomtsev-Petviashvili (KP) equation with the mixed derivative of Riemann-Liouville time-fractional derivative and integer-order $x$-derivative. We obtained all the Lie symmetries admitted by the KP equation and used them to reduce the (2+1)-dimensional fractional partial differential equation with Riemann-Liouville fractional derivative to some (1+1)-dimensional fractional partial differential equations with Erd\\'{e}lyi-Kober fractional derivative or Riemann-Liouville fractional derivative, thereby getting some exact solutions of the reduced equations. In addition, the new conservation theorem and the generalization of Noether operators are developed to construct the conservation laws for the equation studied.","sentences":["In this paper, Lie symmetry analysis method is applied to the (2+1)-dimensional time fractional Kadomtsev-Petviashvili (KP) equation with the mixed derivative of Riemann-Liouville time-fractional derivative and integer-order $x$-derivative.","We obtained all the Lie symmetries admitted by the KP equation and used them to reduce the (2+1)-dimensional fractional partial differential equation with Riemann-Liouville fractional derivative to some (1+1)-dimensional fractional partial differential equations with Erd\\'{e}lyi-Kober fractional derivative or Riemann-Liouville fractional derivative, thereby getting some exact solutions of the reduced equations.","In addition, the new conservation theorem and the generalization of Noether operators are developed to construct the conservation laws for the equation studied."],"url":"http://arxiv.org/abs/2405.09826v1","category":"nlin.SI"}
{"created":"2024-05-16 05:50:53","title":"A Sample of Compact Object Candidates in Single-lined Spectroscopic Binaries from LAMOST Medium Resolution Survey","abstract":"The stellar spectra from LAMOST Medium Resolution Survey can be used to search for compact objects in binaries. The LAMOST DR10 catalog includes > 980, 000 targets with multiple medium resolution spectra. We select the targets with large or rapid radial velocity variation, and obtained an input-sample of 1822 sources. We use light curves and spectra to identify and exclude eclipsing binaries and double-lined spectroscopic binaries in the input-sample. We finally derive a catalog of 89 candidates with well-folded radial velocity, which are all single-lined spectroscopic binaries, indicating an unseen companion residing in each system. The mass function of each system can be well constrained based on the radial velocity curve. In our sample, 26 sources have mass function higher than 0.1 $M_{\\odot}$, among which 18 sources have ellipsoidal type light curves. In our opinion, compact objects are likely existent in all these 26 binaries, which are worth follow-up identification.","sentences":["The stellar spectra from LAMOST Medium Resolution Survey can be used to search for compact objects in binaries.","The LAMOST DR10 catalog includes > 980, 000 targets with multiple medium resolution spectra.","We select the targets with large or rapid radial velocity variation, and obtained an input-sample of 1822 sources.","We use light curves and spectra to identify and exclude eclipsing binaries and double-lined spectroscopic binaries in the input-sample.","We finally derive a catalog of 89 candidates with well-folded radial velocity, which are all single-lined spectroscopic binaries, indicating an unseen companion residing in each system.","The mass function of each system can be well constrained based on the radial velocity curve.","In our sample, 26 sources have mass function higher than 0.1 $M_{\\odot}$, among which 18 sources have ellipsoidal type light curves.","In our opinion, compact objects are likely existent in all these 26 binaries, which are worth follow-up identification."],"url":"http://arxiv.org/abs/2405.09825v1","category":"astro-ph.SR"}
{"created":"2024-05-16 05:36:28","title":"Automating the Training and Deployment of Models in MLOps by Integrating Systems with Machine Learning","abstract":"This article introduces the importance of machine learning in real-world applications and explores the rise of MLOps (Machine Learning Operations) and its importance for solving challenges such as model deployment and performance monitoring. By reviewing the evolution of MLOps and its relationship to traditional software development methods, the paper proposes ways to integrate the system into machine learning to solve the problems faced by existing MLOps and improve productivity. This paper focuses on the importance of automated model training, and the method to ensure the transparency and repeatability of the training process through version control system. In addition, the challenges of integrating machine learning components into traditional CI/CD pipelines are discussed, and solutions such as versioning environments and containerization are proposed. Finally, the paper emphasizes the importance of continuous monitoring and feedback loops after model deployment to maintain model performance and reliability. Using case studies and best practices from Netflix, the article presents key strategies and lessons learned for successful implementation of MLOps practices, providing valuable references for other organizations to build and optimize their own MLOps practices.","sentences":["This article introduces the importance of machine learning in real-world applications and explores the rise of MLOps (Machine Learning Operations) and its importance for solving challenges such as model deployment and performance monitoring.","By reviewing the evolution of MLOps and its relationship to traditional software development methods, the paper proposes ways to integrate the system into machine learning to solve the problems faced by existing MLOps and improve productivity.","This paper focuses on the importance of automated model training, and the method to ensure the transparency and repeatability of the training process through version control system.","In addition, the challenges of integrating machine learning components into traditional CI/CD pipelines are discussed, and solutions such as versioning environments and containerization are proposed.","Finally, the paper emphasizes the importance of continuous monitoring and feedback loops after model deployment to maintain model performance and reliability.","Using case studies and best practices from Netflix, the article presents key strategies and lessons learned for successful implementation of MLOps practices, providing valuable references for other organizations to build and optimize their own MLOps practices."],"url":"http://arxiv.org/abs/2405.09819v1","category":"cs.SE"}
{"created":"2024-05-16 05:20:47","title":"Active Learning with Fully Bayesian Neural Networks for Discontinuous and Nonstationary Data","abstract":"Active learning optimizes the exploration of large parameter spaces by strategically selecting which experiments or simulations to conduct, thus reducing resource consumption and potentially accelerating scientific discovery. A key component of this approach is a probabilistic surrogate model, typically a Gaussian Process (GP), which approximates an unknown functional relationship between control parameters and a target property. However, conventional GPs often struggle when applied to systems with discontinuities and non-stationarities, prompting the exploration of alternative models. This limitation becomes particularly relevant in physical science problems, which are often characterized by abrupt transitions between different system states and rapid changes in physical property behavior. Fully Bayesian Neural Networks (FBNNs) serve as a promising substitute, treating all neural network weights probabilistically and leveraging advanced Markov Chain Monte Carlo techniques for direct sampling from the posterior distribution. This approach enables FBNNs to provide reliable predictive distributions, crucial for making informed decisions under uncertainty in the active learning setting. Although traditionally considered too computationally expensive for 'big data' applications, many physical sciences problems involve small amounts of data in relatively low-dimensional parameter spaces. Here, we assess the suitability and performance of FBNNs with the No-U-Turn Sampler for active learning tasks in the 'small data' regime, highlighting their potential to enhance predictive accuracy and reliability on test functions relevant to problems in physical sciences.","sentences":["Active learning optimizes the exploration of large parameter spaces by strategically selecting which experiments or simulations to conduct, thus reducing resource consumption and potentially accelerating scientific discovery.","A key component of this approach is a probabilistic surrogate model, typically a Gaussian Process (GP), which approximates an unknown functional relationship between control parameters and a target property.","However, conventional GPs often struggle when applied to systems with discontinuities and non-stationarities, prompting the exploration of alternative models.","This limitation becomes particularly relevant in physical science problems, which are often characterized by abrupt transitions between different system states and rapid changes in physical property behavior.","Fully Bayesian Neural Networks (FBNNs) serve as a promising substitute, treating all neural network weights probabilistically and leveraging advanced Markov Chain Monte Carlo techniques for direct sampling from the posterior distribution.","This approach enables FBNNs to provide reliable predictive distributions, crucial for making informed decisions under uncertainty in the active learning setting.","Although traditionally considered too computationally expensive for 'big data' applications, many physical sciences problems involve small amounts of data in relatively low-dimensional parameter spaces.","Here, we assess the suitability and performance of FBNNs with the No-U-Turn Sampler for active learning tasks in the 'small data' regime, highlighting their potential to enhance predictive accuracy and reliability on test functions relevant to problems in physical sciences."],"url":"http://arxiv.org/abs/2405.09817v1","category":"cs.LG"}
{"created":"2024-05-16 05:00:01","title":"Revisiting the exclusion principle in epidemiology at its ultimate limit","abstract":"The competitive exclusion principle in epidemiology implies that when competing strains of a pathogen provide complete protection for each other, the strain with the largest reproduction number outcompetes the other strains and drives them to extinction. The introduction of various trade-off mechanisms may facilitate the coexistence of competing strains, especially when their respective basic reproduction numbers are close so that the competition between the strains is weak. Yet, one may expect that a substantial competitive advantage of one of the strains will eventually outbalance trade-off mechanisms driving less competitive strains to extinction. The literature, however, lacks a rigorous validation of this statement. In this work, we challenge the validity of the exclusion principle at an ultimate limit in which one strain has a vast competitive advantage over the other strains. We show that when one strain is significantly more transmissible than the others, and under broad conditions, an epidemic system with two strains has a stable endemic equilibrium in which both strains coexist with comparable prevalence. Thus, the competitive exclusion principle does not unconditionally hold beyond the established case of complete immunity.","sentences":["The competitive exclusion principle in epidemiology implies that when competing strains of a pathogen provide complete protection for each other, the strain with the largest reproduction number outcompetes the other strains and drives them to extinction.","The introduction of various trade-off mechanisms may facilitate the coexistence of competing strains, especially when their respective basic reproduction numbers are close so that the competition between the strains is weak.","Yet, one may expect that a substantial competitive advantage of one of the strains will eventually outbalance trade-off mechanisms driving less competitive strains to extinction.","The literature, however, lacks a rigorous validation of this statement.","In this work, we challenge the validity of the exclusion principle at an ultimate limit in which one strain has a vast competitive advantage over the other strains.","We show that when one strain is significantly more transmissible than the others, and under broad conditions, an epidemic system with two strains has a stable endemic equilibrium in which both strains coexist with comparable prevalence.","Thus, the competitive exclusion principle does not unconditionally hold beyond the established case of complete immunity."],"url":"http://arxiv.org/abs/2405.09813v1","category":"q-bio.PE"}
{"created":"2024-05-16 04:54:41","title":"Mean-field and cumulant approaches to modelling organic polariton physics","abstract":"In this thesis we develop methods for many-body open quantum systems and apply them to systems of organic polaritons. The methods employ a mean-field approach to reduce the dimensionality of large-scale problems. Initially assuming the absence of correlations in the many-body state, this approach is built upon in two ways.   First, we show how the mean-field approximation can be combined with matrix product operator methods to efficiently simulate the non-Markovian dynamics of a many-body system with strong coupling to multiple environments. We apply this method to calculate the threshold and photoluminescence for a realistic model of an organic laser.   Second, we extend the mean-field description by systematically including higher-order correlations via cumulant expansions of the Heisenberg equations of motion. We investigate the validity and convergence properties of these expansions, both with respect to expansion order and system size, for many-body systems with many-to-one network structures. We then show how the cumulant expansions may be used to calculate spatially resolved dynamics of organic polaritons. This enables a study of organic polariton transport in which we observe reversible conversion to dark exciton states and sub-group-velocity propagation.   The methods established in this work offer versatile tools for analysing large, many-body open quantum systems and investigating finite-size effects. Their application reveals the intricate dynamics of organic polaritons resulting from the interplay of strong light-matter coupling and vibrational effects.","sentences":["In this thesis we develop methods for many-body open quantum systems and apply them to systems of organic polaritons.","The methods employ a mean-field approach to reduce the dimensionality of large-scale problems.","Initially assuming the absence of correlations in the many-body state, this approach is built upon in two ways.   ","First, we show how the mean-field approximation can be combined with matrix product operator methods to efficiently simulate the non-Markovian dynamics of a many-body system with strong coupling to multiple environments.","We apply this method to calculate the threshold and photoluminescence for a realistic model of an organic laser.   ","Second, we extend the mean-field description by systematically including higher-order correlations via cumulant expansions of the Heisenberg equations of motion.","We investigate the validity and convergence properties of these expansions, both with respect to expansion order and system size, for many-body systems with many-to-one network structures.","We then show how the cumulant expansions may be used to calculate spatially resolved dynamics of organic polaritons.","This enables a study of organic polariton transport in which we observe reversible conversion to dark exciton states and sub-group-velocity propagation.   ","The methods established in this work offer versatile tools for analysing large, many-body open quantum systems and investigating finite-size effects.","Their application reveals the intricate dynamics of organic polaritons resulting from the interplay of strong light-matter coupling and vibrational effects."],"url":"http://arxiv.org/abs/2405.09812v1","category":"cond-mat.other"}
{"created":"2024-05-16 04:42:21","title":"Biomarker Selection for Adaptive Systems","abstract":"Biomarker selection and real-time monitoring of cell dynamics remains an active challenge in cell biology and biomanufacturing. Here, we develop scalable adaptations of classic approaches to sensor selection for biomarker identification on several transcriptomics and biological datasets that are otherwise cannot be studied from a controls perspective. To address challenges in system identification of biological systems and provide robust biomarkers, we propose Dynamic and Structure Guided Sensors Selection (DSS and SGSS), methods by which temporal models and structural experimental data can be used to supplement traditional approaches to sensor selection. These approaches leverage temporal models and experimental data to enhance traditional sensor selection techniques. Unlike conventional methods that assume well-known, fixed dynamics, DSS and SGSS adaptively select sensors that maximize observability while accounting for the time-varying nature of biological systems. Additionally, they incorporate structural information to identify robust sensors even in cases where system dynamics are poorly understood. We validate these two approaches by performing estimation on several high dimensional systems derived from temporal gene expression data from partial observations.","sentences":["Biomarker selection and real-time monitoring of cell dynamics remains an active challenge in cell biology and biomanufacturing.","Here, we develop scalable adaptations of classic approaches to sensor selection for biomarker identification on several transcriptomics and biological datasets that are otherwise cannot be studied from a controls perspective.","To address challenges in system identification of biological systems and provide robust biomarkers, we propose Dynamic and Structure Guided Sensors Selection (DSS and SGSS), methods by which temporal models and structural experimental data can be used to supplement traditional approaches to sensor selection.","These approaches leverage temporal models and experimental data to enhance traditional sensor selection techniques.","Unlike conventional methods that assume well-known, fixed dynamics, DSS and SGSS adaptively select sensors that maximize observability while accounting for the time-varying nature of biological systems.","Additionally, they incorporate structural information to identify robust sensors even in cases where system dynamics are poorly understood.","We validate these two approaches by performing estimation on several high dimensional systems derived from temporal gene expression data from partial observations."],"url":"http://arxiv.org/abs/2405.09809v1","category":"q-bio.MN"}
{"created":"2024-05-16 04:32:21","title":"Three-body forces and Efimov physics in nuclei and atoms","abstract":"This review article presents historical developments and recent advances in our understanding on the three-body forces and Efimov physics, from an interdisciplinary viewpoint encompassing nuclear physics and cold atoms. Theoretical attempts to elucidate the three-body force with the chiral effective field theory are explained, followed by an overview of experiments aimed at observing signatures of the nuclear three-body force. Some recent experimental and theoretical works in the field of cold atoms devoted to measuring and engineering three-body forces among atoms are also presented. As a phenomenon arising from the three-body effect, Efimov physics in both cold atoms and nuclear systems is reviewed.","sentences":["This review article presents historical developments and recent advances in our understanding on the three-body forces and Efimov physics, from an interdisciplinary viewpoint encompassing nuclear physics and cold atoms.","Theoretical attempts to elucidate the three-body force with the chiral effective field theory are explained, followed by an overview of experiments aimed at observing signatures of the nuclear three-body force.","Some recent experimental and theoretical works in the field of cold atoms devoted to measuring and engineering three-body forces among atoms are also presented.","As a phenomenon arising from the three-body effect, Efimov physics in both cold atoms and nuclear systems is reviewed."],"url":"http://arxiv.org/abs/2405.09807v1","category":"nucl-th"}
{"created":"2024-05-16 04:23:17","title":"Quantum annealing for finding maximum-weight independent set of unit-disk graphs","abstract":"Recent progress in quantum computing and quantum simulation of many-body systems with arrays of neutral atoms using Rydberg excitation brought unforeseen opportunities towards computational advantage in solving various optimization problems. The problem of maximum-weight independent set (MWIS) of unit-disk graphs is an example of NP-hard optimization problems. It involves finding the largest set of vertices with the maximum sum of their weights for a graph which has edges connecting all pairs of vertices within a unit distance. This problem can be solved using quantum annealing with an array of interacting Rydberg atoms. For a particular graph, a spatial arrangement of atoms represents vertices of the graph, while the detuning from the resonance at Rydberg excitation defines weights of these vertices. The edges of the graph can be drawn according to the unit disk criterion. MWIS can be obtained by applying a variational quantum adiabatic algorithm (VQAA). We consider driving the quantum system of interacting atoms to the many-body ground state using a non-linear quasi-adiabatic profile for sweeping the Rydberg detuning. We also propose using a quantum wire which is a set of auxiliary atoms of a different chemical element to mediate strong coupling between the remote vertices of the graph. We investigate this effect for different lengths of the quantum wire. We also investigate the quantum phases of matter realizing commensurate and incommensurate phases in 1D and 2D spatial arrangement of the atomic array.","sentences":["Recent progress in quantum computing and quantum simulation of many-body systems with arrays of neutral atoms using Rydberg excitation brought unforeseen opportunities towards computational advantage in solving various optimization problems.","The problem of maximum-weight independent set (MWIS) of unit-disk graphs is an example of NP-hard optimization problems.","It involves finding the largest set of vertices with the maximum sum of their weights for a graph which has edges connecting all pairs of vertices within a unit distance.","This problem can be solved using quantum annealing with an array of interacting Rydberg atoms.","For a particular graph, a spatial arrangement of atoms represents vertices of the graph, while the detuning from the resonance at Rydberg excitation defines weights of these vertices.","The edges of the graph can be drawn according to the unit disk criterion.","MWIS can be obtained by applying a variational quantum adiabatic algorithm (VQAA).","We consider driving the quantum system of interacting atoms to the many-body ground state using a non-linear quasi-adiabatic profile for sweeping the Rydberg detuning.","We also propose using a quantum wire which is a set of auxiliary atoms of a different chemical element to mediate strong coupling between the remote vertices of the graph.","We investigate this effect for different lengths of the quantum wire.","We also investigate the quantum phases of matter realizing commensurate and incommensurate phases in 1D and 2D spatial arrangement of the atomic array."],"url":"http://arxiv.org/abs/2405.09803v1","category":"quant-ph"}
{"created":"2024-05-16 04:14:29","title":"Lagrangian Covector Fluid with Free Surface","abstract":"This paper introduces a novel Lagrangian fluid solver based on covector flow maps. We aim to address the challenges of establishing a robust flow-map solver for incompressible fluids under complex boundary conditions. Our key idea is to use particle trajectories to establish precise flow maps and tailor path integrals of physical quantities along these trajectories to reformulate the Poisson problem during the projection step. We devise a decoupling mechanism based on path-integral identities from flow-map theory. This mechanism integrates long-range flow maps for the main fluid body into a short-range projection framework, ensuring a robust treatment of free boundaries. We show that our method can effectively transform a long-range projection problem with integral boundaries into a Poisson problem with standard boundary conditions -- specifically, zero Dirichlet on the free surface and zero Neumann on solid boundaries. This transformation significantly enhances robustness and accuracy, extending the applicability of flow-map methods to complex free-surface problems.","sentences":["This paper introduces a novel Lagrangian fluid solver based on covector flow maps.","We aim to address the challenges of establishing a robust flow-map solver for incompressible fluids under complex boundary conditions.","Our key idea is to use particle trajectories to establish precise flow maps and tailor path integrals of physical quantities along these trajectories to reformulate the Poisson problem during the projection step.","We devise a decoupling mechanism based on path-integral identities from flow-map theory.","This mechanism integrates long-range flow maps for the main fluid body into a short-range projection framework, ensuring a robust treatment of free boundaries.","We show that our method can effectively transform a long-range projection problem with integral boundaries into a Poisson problem with standard boundary conditions -- specifically, zero Dirichlet on the free surface and zero Neumann on solid boundaries.","This transformation significantly enhances robustness and accuracy, extending the applicability of flow-map methods to complex free-surface problems."],"url":"http://arxiv.org/abs/2405.09801v1","category":"cs.GR"}
{"created":"2024-05-16 03:56:05","title":"Prototype Design of a Digital Low-level RF System for S-band Deflectors","abstract":"S-band deflectors are generally operated on pulsed mode for beam diagnosis. We plan to deploy 5 S-band (2997 MHz) deflectors to accurately measure the longitudinal time distribution of ultra-short electron beam pulses in Shenzhen Superconducting Soft X-ray Free Electron Laser (S3FEL). A microwave system of one deflector consists of a low-level RF system (LLRF), a solid-state amplifier, waveguide couplers, and a klystron, operated in pulse mode with a maximum repetition frequency of 50 Hz. Its microwave amplitude and phase stability must be better than 0.06%/0.08{\\deg} (RMS). This article will introduce the prototype design of the hardware, firmware, and software of the digital LLRF system. We use homemade Local Oscillators (LOs) and commercial cards based on the MicroTCA standard in hardware design. The firmware design will use a Non-IQ demodulation and a pulse feedforward algorithm to suppress noise from high voltage of klystron. The software design is based on the EPICS control system architecture, achieving slow control and interface display functions. This report will also show some preliminary test results.","sentences":["S-band deflectors are generally operated on pulsed mode for beam diagnosis.","We plan to deploy 5 S-band (2997 MHz) deflectors to accurately measure the longitudinal time distribution of ultra-short electron beam pulses in Shenzhen Superconducting Soft X-ray Free Electron Laser (S3FEL).","A microwave system of one deflector consists of a low-level RF system (LLRF), a solid-state amplifier, waveguide couplers, and a klystron, operated in pulse mode with a maximum repetition frequency of 50 Hz.","Its microwave amplitude and phase stability must be better than 0.06%/0.08{\\deg} (RMS).","This article will introduce the prototype design of the hardware, firmware, and software of the digital LLRF system.","We use homemade Local Oscillators (LOs) and commercial cards based on the MicroTCA standard in hardware design.","The firmware design will use a Non-IQ demodulation and a pulse feedforward algorithm to suppress noise from high voltage of klystron.","The software design is based on the EPICS control system architecture, achieving slow control and interface display functions.","This report will also show some preliminary test results."],"url":"http://arxiv.org/abs/2405.09796v1","category":"physics.acc-ph"}
{"created":"2024-05-16 03:26:06","title":"LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation","abstract":"Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.","sentences":["Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers.","However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance.","Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed.","Technically, the meta tokens are first initialized from image tokens via cross-attention.","Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention.","By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes.","Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance."],"url":"http://arxiv.org/abs/2405.09789v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:00","title":"KiDS-1000 and DES-Y1 combined: Cosmology from peak count statistics","abstract":"We analyse the fourth data release of the Kilo Degree Survey (KiDS-1000) and extract cosmological parameter constraints based on the cosmic shear peak count statistics. Peaks are identified in aperture mass maps in which the filter is maximally sensitive to angular scales in the range 2-4arcmin, probing deep into the non-linear regime of structure formation. We interpret our results with a simulation-based inference pipeline, sampling over a broad $w$CDM prior volume and marginalising over uncertainties on shape calibration, photometric redshift distribution, intrinsic alignment and baryonic feedback. Our measurements constrain the structure growth parameter and the amplitude of the non-linear intrinsic alignment model to $\\Sigma_8 \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.60}=0.765^{+0.030}_{-0.030}$ and $A_{\\rm IA}= 0.71^{+0.42}_{-0.42}$, respectively, in agreement with previous KiDS-1000 results based on two-point shear statistics. These results are robust against modelling of the non-linear physics, different scale cuts and selections of tomographic bins. The posterior is also consistent with that from the Dark Energy Survey Year-1 peak count analysis presented in Harnois-D\\'eraps et al (2021), and hence we jointly analyse both surveys. We obtain $\\Sigma_8^{\\rm joint} \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.57}=0.759^{+0.020}_{-0.017}$, in agreement with the Planck $w$CDM results. The shear-CMB tension on this parameter increases to $3.1\\sigma$ when forcing $w=-1.0$, and to $4.1\\sigma$ if comparing instead with $S_{8,\\Lambda{\\rm CDM}}^{\\rm joint} = 0.736^{+0.016}_{-0.018}$, one of the tightest constraints to date on this quantity. (abridged)","sentences":["We analyse the fourth data release of the Kilo Degree Survey (KiDS-1000) and extract cosmological parameter constraints based on the cosmic shear peak count statistics.","Peaks are identified in aperture mass maps in which the filter is maximally sensitive to angular scales in the range 2-4arcmin, probing deep into the non-linear regime of structure formation.","We interpret our results with a simulation-based inference pipeline, sampling over a broad $w$CDM prior volume and marginalising over uncertainties on shape calibration, photometric redshift distribution, intrinsic alignment and baryonic feedback.","Our measurements constrain the structure growth parameter and the amplitude of the non-linear intrinsic alignment model to $\\Sigma_8 \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.60}=0.765^{+0.030}_{-0.030}$ and $A_{\\rm IA}= 0.71^{+0.42}_{-0.42}$, respectively, in agreement with previous KiDS-1000 results based on two-point shear statistics.","These results are robust against modelling of the non-linear physics, different scale cuts and selections of tomographic bins.","The posterior is also consistent with that from the Dark Energy Survey Year-1 peak count analysis presented in Harnois-D\\'eraps et al (2021), and hence we jointly analyse both surveys.","We obtain $\\Sigma_8^{\\rm joint} \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.57}=0.759^{+0.020}_{-0.017}$, in agreement with the Planck $w$CDM results.","The shear-CMB tension on this parameter increases to $3.1\\sigma$ when forcing $w=-1.0$, and to $4.1\\sigma$ if comparing instead with $S_{8,\\Lambda{\\rm CDM}}^{\\rm joint} = 0.736^{+0.016}_{-0.018}$, one of the tightest constraints to date on this quantity.","(abridged)"],"url":"http://arxiv.org/abs/2405.10312v1","category":"astro-ph.CO"}
{"created":"2024-05-16 17:52:59","title":"Heavy-element damage seeding in proteins under X-ray free electron laser illumination conditions","abstract":"The emerging technique of serial femtosecond X-ray crystallography (SFX) can be used to study the structure and dynamics of biological macromolecules to high spatial and temporal resolutions. An ongoing challenge for SFX is the damage caused by the ultrabright X-ray free electron laser pulse. Though it is often assumed that sufficiently femtosecond pulses `outrun' radiation damage, in reality electronic damage processes commence during exposure. We model the electronic damage to protein nanocrystals using a plasma model that tracks the continuous changes to the energy distribution of the unbound electrons. Tracking the continuous energy distribution is of particular importance for distinguishing the influence of differing elements on secondary damage processes. Heavy atoms have a ubiquitous but small presence in protein targets - typically as integral components of the macromolecule and as salts in the solvent. We find that these atoms considerably influence the simulated ionization and scattering behavior of realistic targets due to their rapid seeding of secondary ionization processes. In lysozyme, even the presence of native sulfur atoms significantly contributes to theoretical measures of damage-induced noise for >= 6 keV, 15 fs pulses. Contributing to the effect is that heavy atoms seed `intermediate' energy electron cascades that are particularly effective at ionizing the target on the femtosecond timescale. In addition, the disproportionate effect of heavy atoms means the damage to a protein crystal can be sensitive to their presence in the solvent. Outside of reducing the concentration of heavy atoms in the target, these results suggest the dose limits of SFX targets will be higher where the ionization of deep >~ 6 keV absorption edges is minimized, or, to a lesser extent, when such edges are only ionized with X-rays >> 2 keV above their binding energy.","sentences":["The emerging technique of serial femtosecond X-ray crystallography (SFX) can be used to study the structure and dynamics of biological macromolecules to high spatial and temporal resolutions.","An ongoing challenge for SFX is the damage caused by the ultrabright X-ray free electron laser pulse.","Though it is often assumed that sufficiently femtosecond pulses `outrun' radiation damage, in reality electronic damage processes commence during exposure.","We model the electronic damage to protein nanocrystals using a plasma model that tracks the continuous changes to the energy distribution of the unbound electrons.","Tracking the continuous energy distribution is of particular importance for distinguishing the influence of differing elements on secondary damage processes.","Heavy atoms have a ubiquitous but small presence in protein targets - typically as integral components of the macromolecule and as salts in the solvent.","We find that these atoms considerably influence the simulated ionization and scattering behavior of realistic targets due to their rapid seeding of secondary ionization processes.","In lysozyme, even the presence of native sulfur atoms significantly contributes to theoretical measures of damage-induced noise for >= 6 keV, 15 fs pulses.","Contributing to the effect is that heavy atoms seed `intermediate' energy electron cascades that are particularly effective at ionizing the target on the femtosecond timescale.","In addition, the disproportionate effect of heavy atoms means the damage to a protein crystal can be sensitive to their presence in the solvent.","Outside of reducing the concentration of heavy atoms in the target, these results suggest the dose limits of SFX targets will be higher where the ionization of deep >~ 6 keV absorption edges is minimized, or, to a lesser extent, when such edges are only ionized with X-rays >> 2 keV above their binding energy."],"url":"http://arxiv.org/abs/2405.10298v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 17:41:53","title":"Power-law relaxation of a confined diffusing particle subject to resetting with memory","abstract":"We study the relaxation of a Brownian particle with long range memory under confinement in one dimension. The particle diffuses in an arbitrary confining potential and resets at random times to previously visited positions, chosen with a probability proportional to the local time spend there by the particle since the initial time. This model mimics an animal which moves erratically in its home range and returns preferentially to familiar places from time to time. The steady state density of the position is given by the equilibrium Boltzmann-Gibbs distribution, as in standard diffusion, while the transient part of the density can be obtained through a mapping of the Fokker-Planck equation of the process to a Schr\\\"odinger eigenvalue problem. Due to memory, the approach at large time toward the steady state is critically self-organised, in the sense that it always follows a sluggish power-law form, in contrast to the exponential decay that characterises Markov processes. The exponent of this power-law depends in a simple way on the resetting rate and on the relaxation rate of the Brownian particle in the absence of resetting. We apply these findings to several exactly solvable examples, such as the harmonic, V-shaped and box potentials.","sentences":["We study the relaxation of a Brownian particle with long range memory under confinement in one dimension.","The particle diffuses in an arbitrary confining potential and resets at random times to previously visited positions, chosen with a probability proportional to the local time spend there by the particle since the initial time.","This model mimics an animal which moves erratically in its home range and returns preferentially to familiar places from time to time.","The steady state density of the position is given by the equilibrium Boltzmann-Gibbs distribution, as in standard diffusion, while the transient part of the density can be obtained through a mapping of the Fokker-Planck equation of the process to a Schr\\\"odinger eigenvalue problem.","Due to memory, the approach at large time toward the steady state is critically self-organised, in the sense that it always follows a sluggish power-law form, in contrast to the exponential decay that characterises Markov processes.","The exponent of this power-law depends in a simple way on the resetting rate and on the relaxation rate of the Brownian particle in the absence of resetting.","We apply these findings to several exactly solvable examples, such as the harmonic, V-shaped and box potentials."],"url":"http://arxiv.org/abs/2405.10283v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-16 17:02:23","title":"Biasing & Debiasing based Approach Towards Fair Knowledge Transfer for Equitable Skin Analysis","abstract":"Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated exceptional performance in diagnosing skin diseases, often outperforming dermatologists. However, they have also unveiled biases linked to specific demographic traits, notably concerning diverse skin tones or gender, prompting concerns regarding fairness and limiting their widespread deployment. Researchers are actively working to ensure fairness in AI-based solutions, but existing methods incur an accuracy loss when striving for fairness. To solve this issue, we propose a `two-biased teachers' (i.e., biased on different sensitive attributes) based approach to transfer fair knowledge into the student network. Our approach mitigates biases present in the student network without harming its predictive accuracy. In fact, in most cases, our approach improves the accuracy of the baseline model. To achieve this goal, we developed a weighted loss function comprising biasing and debiasing loss terms. We surpassed available state-of-the-art approaches to attain fairness and also improved the accuracy at the same time. The proposed approach has been evaluated and validated on two dermatology datasets using standard accuracy and fairness evaluation measures. We will make source code publicly available to foster reproducibility and future research.","sentences":["Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated exceptional performance in diagnosing skin diseases, often outperforming dermatologists.","However, they have also unveiled biases linked to specific demographic traits, notably concerning diverse skin tones or gender, prompting concerns regarding fairness and limiting their widespread deployment.","Researchers are actively working to ensure fairness in AI-based solutions, but existing methods incur an accuracy loss when striving for fairness.","To solve this issue, we propose a `two-biased teachers' (i.e., biased on different sensitive attributes) based approach to transfer fair knowledge into the student network.","Our approach mitigates biases present in the student network without harming its predictive accuracy.","In fact, in most cases, our approach improves the accuracy of the baseline model.","To achieve this goal, we developed a weighted loss function comprising biasing and debiasing loss terms.","We surpassed available state-of-the-art approaches to attain fairness and also improved the accuracy at the same time.","The proposed approach has been evaluated and validated on two dermatology datasets using standard accuracy and fairness evaluation measures.","We will make source code publicly available to foster reproducibility and future research."],"url":"http://arxiv.org/abs/2405.10256v1","category":"cs.CV"}
{"created":"2024-05-16 16:51:25","title":"Unifying Partial Synchrony","abstract":"The distributed computing literature considers multiple options for modeling communication. Most simply, communication is categorized as either synchronous or asynchronous. Synchronous communication assumes that messages get delivered within a publicly known timeframe and that parties' clocks are synchronized. Asynchronous communication, on the other hand, only assumes that messages get delivered eventually. A more nuanced approach, or a middle ground between the two extremes, is given by the partially synchronous model, which is arguably the most realistic option. This model comes in two commonly considered flavors:   (i) The Global Stabilization Time (GST) model: after an (unknown) amount of time, the network becomes synchronous. This captures scenarios where network issues are transient.   (ii) The Unknown Latency (UL) model: the network is, in fact, synchronous, but the message delay bound is unknown.   This work formally establishes that any time-agnostic property that can be achieved by a protocol in the UL model can also be achieved by a (possibly different) protocol in the GST model. By time-agnostic, we mean properties that can depend on the order in which events happen but not on time as measured by the parties. Most properties considered in distributed computing are time-agnostic. The converse was already known, even without the time-agnostic requirement, so our result shows that the two network conditions are, under one sensible assumption, equally demanding.","sentences":["The distributed computing literature considers multiple options for modeling communication.","Most simply, communication is categorized as either synchronous or asynchronous.","Synchronous communication assumes that messages get delivered within a publicly known timeframe and that parties' clocks are synchronized.","Asynchronous communication, on the other hand, only assumes that messages get delivered eventually.","A more nuanced approach, or a middle ground between the two extremes, is given by the partially synchronous model, which is arguably the most realistic option.","This model comes in two commonly considered flavors:   (i) The Global Stabilization Time (GST) model: after an (unknown) amount of time, the network becomes synchronous.","This captures scenarios where network issues are transient.   ","(ii) The Unknown Latency (UL) model: the network is, in fact, synchronous, but the message delay bound is unknown.   ","This work formally establishes that any time-agnostic property that can be achieved by a protocol in the UL model can also be achieved by a (possibly different) protocol in the GST model.","By time-agnostic, we mean properties that can depend on the order in which events happen but not on time as measured by the parties.","Most properties considered in distributed computing are time-agnostic.","The converse was already known, even without the time-agnostic requirement, so our result shows that the two network conditions are, under one sensible assumption, equally demanding."],"url":"http://arxiv.org/abs/2405.10249v1","category":"cs.DC"}
{"created":"2024-05-16 16:50:31","title":"Co-Matching: Towards Human-Machine Collaborative Legal Case Matching","abstract":"Recent efforts have aimed to improve AI machines in legal case matching by integrating legal domain knowledge. However, successful legal case matching requires the tacit knowledge of legal practitioners, which is difficult to verbalize and encode into machines. This emphasizes the crucial role of involving legal practitioners in high-stakes legal case matching. To address this, we propose a collaborative matching framework called Co-Matching, which encourages both the machine and the legal practitioner to participate in the matching process, integrating tacit knowledge. Unlike existing methods that rely solely on the machine, Co-Matching allows both the legal practitioner and the machine to determine key sentences and then combine them probabilistically. Co-Matching introduces a method called ProtoEM to estimate human decision uncertainty, facilitating the probabilistic combination. Experimental results demonstrate that Co-Matching consistently outperforms existing legal case matching methods, delivering significant performance improvements over human- and machine-based matching in isolation (on average, +5.51% and +8.71%, respectively). Further analysis shows that Co-Matching also ensures better human-machine collaboration effectiveness. Our study represents a pioneering effort in human-machine collaboration for the matching task, marking a milestone for future collaborative matching studies.","sentences":["Recent efforts have aimed to improve AI machines in legal case matching by integrating legal domain knowledge.","However, successful legal case matching requires the tacit knowledge of legal practitioners, which is difficult to verbalize and encode into machines.","This emphasizes the crucial role of involving legal practitioners in high-stakes legal case matching.","To address this, we propose a collaborative matching framework called Co-Matching, which encourages both the machine and the legal practitioner to participate in the matching process, integrating tacit knowledge.","Unlike existing methods that rely solely on the machine, Co-Matching allows both the legal practitioner and the machine to determine key sentences and then combine them probabilistically.","Co-Matching introduces a method called ProtoEM to estimate human decision uncertainty, facilitating the probabilistic combination.","Experimental results demonstrate that Co-Matching consistently outperforms existing legal case matching methods, delivering significant performance improvements over human- and machine-based matching in isolation (on average, +5.51% and +8.71%, respectively).","Further analysis shows that Co-Matching also ensures better human-machine collaboration effectiveness.","Our study represents a pioneering effort in human-machine collaboration for the matching task, marking a milestone for future collaborative matching studies."],"url":"http://arxiv.org/abs/2405.10248v1","category":"cs.HC"}
{"created":"2024-05-16 16:28:11","title":"Random ReLU Neural Networks as Non-Gaussian Processes","abstract":"We consider a large class of shallow neural networks with randomly initialized parameters and rectified linear unit activation functions. We prove that these random neural networks are well-defined non-Gaussian processes. As a by-product, we demonstrate that these networks are solutions to stochastic differential equations driven by impulsive white noise (combinations of random Dirac measures). These processes are parameterized by the law of the weights and biases as well as the density of activation thresholds in each bounded region of the input domain. We prove that these processes are isotropic and wide-sense self-similar with Hurst exponent $3/2$. We also derive a remarkably simple closed-form expression for their autocovariance function. Our results are fundamentally different from prior work in that we consider a non-asymptotic viewpoint: The number of neurons in each bounded region of the input domain (i.e., the width) is itself a random variable with a Poisson law with mean proportional to the density parameter. Finally, we show that, under suitable hypotheses, as the expected width tends to infinity, these processes can converge in law not only to Gaussian processes, but also to non-Gaussian processes depending on the law of the weights. Our asymptotic results provide a new take on several classical results (wide networks converge to Gaussian processes) as well as some new ones (wide networks can converge to non-Gaussian processes).","sentences":["We consider a large class of shallow neural networks with randomly initialized parameters and rectified linear unit activation functions.","We prove that these random neural networks are well-defined non-Gaussian processes.","As a by-product, we demonstrate that these networks are solutions to stochastic differential equations driven by impulsive white noise (combinations of random Dirac measures).","These processes are parameterized by the law of the weights and biases as well as the density of activation thresholds in each bounded region of the input domain.","We prove that these processes are isotropic and wide-sense self-similar with Hurst exponent $3/2$. We also derive a remarkably simple closed-form expression for their autocovariance function.","Our results are fundamentally different from prior work in that we consider a non-asymptotic viewpoint: The number of neurons in each bounded region of the input domain (i.e., the width) is itself a random variable with a Poisson law with mean proportional to the density parameter.","Finally, we show that, under suitable hypotheses, as the expected width tends to infinity, these processes can converge in law not only to Gaussian processes, but also to non-Gaussian processes depending on the law of the weights.","Our asymptotic results provide a new take on several classical results (wide networks converge to Gaussian processes) as well as some new ones (wide networks can converge to non-Gaussian processes)."],"url":"http://arxiv.org/abs/2405.10229v1","category":"stat.ML"}
{"created":"2024-05-16 15:38:02","title":"Multivariate strong invariance principle and uncertainty assessment for time in-homogeneous cyclic MCMC samplers","abstract":"Time in-homogeneous cyclic Markov chain Monte Carlo (MCMC) samplers, including deterministic scan Gibbs samplers and Metropolis within Gibbs samplers, are extensively used for sampling from multi-dimensional distributions. We establish a multivariate strong invariance principle (SIP) for Markov chains associated with these samplers. The rate of this SIP essentially aligns with the tightest rate available for time homogeneous Markov chains. The SIP implies the strong law of large numbers (SLLN) and the central limit theorem (CLT), and plays an essential role in uncertainty assessments. Using the SIP, we give conditions under which the multivariate batch means estimator for estimating the covariance matrix in the multivariate CLT is strongly consistent. Additionally, we provide conditions for a multivariate fixed volume sequential termination rule, which is associated with the concept of effective sample size (ESS), to be asymptotically valid. Our uncertainty assessment tools are demonstrated through various numerical experiments.","sentences":["Time in-homogeneous cyclic Markov chain Monte Carlo (MCMC) samplers, including deterministic scan Gibbs samplers and Metropolis within Gibbs samplers, are extensively used for sampling from multi-dimensional distributions.","We establish a multivariate strong invariance principle (SIP) for Markov chains associated with these samplers.","The rate of this SIP essentially aligns with the tightest rate available for time homogeneous Markov chains.","The SIP implies the strong law of large numbers (SLLN) and the central limit theorem (CLT), and plays an essential role in uncertainty assessments.","Using the SIP, we give conditions under which the multivariate batch means estimator for estimating the covariance matrix in the multivariate CLT is strongly consistent.","Additionally, we provide conditions for a multivariate fixed volume sequential termination rule, which is associated with the concept of effective sample size (ESS), to be asymptotically valid.","Our uncertainty assessment tools are demonstrated through various numerical experiments."],"url":"http://arxiv.org/abs/2405.10194v1","category":"stat.CO"}
{"created":"2024-05-16 15:10:23","title":"3D-2D crossover and phase shift of beats of quantum oscillations of interlayer magnetoresistance in quasi-2D metals","abstract":"Magnetic quantum oscillations (MQO) are traditionally applied to investigate the electronic structure of metals. In layered quasi-two-dimensional (Q2D) materials the MQO have several qualitative features giving additional useful information, provided their theoretical description is developed. Within the framework of the Kubo formula and the self-consistent Born approximation, we reconsider the phase of beats in the amplitude of Shubnikov oscillations of interlayer conductivity in Q2D metals. We show that the phase shift of beats of the Shubnikov (conductivity) oscillations relative to the de Haas - van Alphen (magnetization) oscillations is larger than expected previously and, under certain conditions, can reach the value of $\\pi/2$, as observed experimentally. We explain the phase inversion of MQO during the 3D - 2D crossover and predict the decrease of relative MQO amplitude of interlayer magnetoresistance in a strong magnetic field, larger than the beat frequency.","sentences":["Magnetic quantum oscillations (MQO) are traditionally applied to investigate the electronic structure of metals.","In layered quasi-two-dimensional (Q2D) materials the MQO have several qualitative features giving additional useful information, provided their theoretical description is developed.","Within the framework of the Kubo formula and the self-consistent Born approximation, we reconsider the phase of beats in the amplitude of Shubnikov oscillations of interlayer conductivity in Q2D metals.","We show that the phase shift of beats of the Shubnikov (conductivity) oscillations relative to the de Haas - van Alphen (magnetization) oscillations is larger than expected previously and, under certain conditions, can reach the value of $\\pi/2$, as observed experimentally.","We explain the phase inversion of MQO during the 3D - 2D crossover and predict the decrease of relative MQO amplitude of interlayer magnetoresistance in a strong magnetic field, larger than the beat frequency."],"url":"http://arxiv.org/abs/2405.10174v1","category":"cond-mat.str-el"}
{"created":"2024-05-16 15:04:34","title":"Filling in the Blanks: A Method to Infer the Substructure Membership and Dynamics of 5D Stars","abstract":"We present and test a method to infer a probability density function (PDF) for the missing vlos of a star with 5D information within $2.5$ kpc. We use stars from the Gaia DR3 RVS catalogue to describe the local orbital structure in action space. This technique also allows us to infer the probability that a 5D star is associated with the Milky Way's stellar Disc or the stellar Halo, which can be further decomposed into known stellar substructures. The method is tested on a 6D Gaia DR3 RVS sample and a 6D Gaia sample crossmatched to groundbased spectroscopic surveys, stripped of their true vlos. The stars predicted vlos, membership probabilities, and inferred structure properties are then compared to the true 6D equivalents, allowing the method's accuracy and limitations to be studied in detail. Our predicted vlos PDFs are statistically consistent with the true vlos, with accurate uncertainties. We find that the vlos of Disc stars can be well constrained, with a median uncertainty of 26 kms. Halo stars are typically less well constrained with a median uncertainty of 72 kms, but those found likely to belong to Halo substructures can be better constrained. The dynamical properties of the total sample and subgroups, such as distributions of integrals of motion and velocities, are also accurately recovered. The group membership probabilities are statistically consistent with our initial labelling, allowing high quality sets to be selected from 5D samples by choosing a trade off between higher expected purity and decreasing expected completeness.","sentences":["We present and test a method to infer a probability density function (PDF) for the missing vlos of a star with 5D information within $2.5$ kpc.","We use stars from the Gaia DR3 RVS catalogue to describe the local orbital structure in action space.","This technique also allows us to infer the probability that a 5D star is associated with the Milky Way's stellar Disc or the stellar Halo, which can be further decomposed into known stellar substructures.","The method is tested on a 6D Gaia DR3 RVS sample and a 6D Gaia sample crossmatched to groundbased spectroscopic surveys, stripped of their true vlos.","The stars predicted vlos, membership probabilities, and inferred structure properties are then compared to the true 6D equivalents, allowing the method's accuracy and limitations to be studied in detail.","Our predicted vlos PDFs are statistically consistent with the true vlos, with accurate uncertainties.","We find that the vlos of Disc stars can be well constrained, with a median uncertainty of 26 kms.","Halo stars are typically less well constrained with a median uncertainty of 72 kms, but those found likely to belong to Halo substructures can be better constrained.","The dynamical properties of the total sample and subgroups, such as distributions of integrals of motion and velocities, are also accurately recovered.","The group membership probabilities are statistically consistent with our initial labelling, allowing high quality sets to be selected from 5D samples by choosing a trade off between higher expected purity and decreasing expected completeness."],"url":"http://arxiv.org/abs/2405.10168v1","category":"astro-ph.GA"}
{"created":"2024-05-16 14:57:45","title":"Electrically Injected mid-infrared GeSn laser on Si operating at 140 K","abstract":"Owing to its true direct bandgap and tunable bandgap energies,GeSn alloys are increasingly attractive as gain media for mid-IR lasers that can be monolithically integrated on Si. Demonstrations of optically pumped GeSn laser at room under pulsed condition and at cryogenic temperature under continuous-wave excitation show great promise of GeSn lasers to be efficient electrically injected light sources on Si. Here we report electrically injected GeSn lasers using Fabry-Perot cavity with 20, 40, and 80 micron ridge widths. A maximum operating temperature of 140 K with lasing threshold of 0.756 kA/cm2 at 77 K and emitting wavelength of 2722 nm at 140 K was obtained. The lower threshold current density compared to previous works was achieved by reducing optical loss and improving the optical confinement. The peak power was measured as 2.2 mW/facet at 77 K.","sentences":["Owing to its true direct bandgap and tunable bandgap energies,GeSn alloys are increasingly attractive as gain media for mid-IR lasers that can be monolithically integrated on Si.","Demonstrations of optically pumped GeSn laser at room under pulsed condition and at cryogenic temperature under continuous-wave excitation show great promise of GeSn lasers to be efficient electrically injected light sources on Si.","Here we report electrically injected GeSn lasers using Fabry-Perot cavity with 20, 40, and 80 micron ridge widths.","A maximum operating temperature of 140 K with lasing threshold of 0.756 kA/cm2 at 77 K and emitting wavelength of 2722 nm at 140 K was obtained.","The lower threshold current density compared to previous works was achieved by reducing optical loss and improving the optical confinement.","The peak power was measured as 2.2 mW/facet at 77 K."],"url":"http://arxiv.org/abs/2405.10163v1","category":"physics.optics"}
{"created":"2024-05-16 14:48:06","title":"Braids, twists, trace and duality in combinatory algebras","abstract":"We investigate a class of combinatory algebras, called ribbon combinatory algebras, in which we can interpret both the braided untyped linear lambda calculus and framed oriented tangles. Any reflexive object in a ribbon category gives rise to a ribbon combinatory algebra. Conversely, From a ribbon combinatory algebra, we can construct a ribbon category with a reflexive object, from which the combinatory algebra can be recovered. To show this, and also to give the equational characterisation of ribbon combinatory algebras, we make use of the internal PRO construction developed in Hasegawa's recent work. Interestingly, we can characterise ribbon combinatory algebras in two different ways: as balanced combinatory algebras with a trace combinator, and as balanced combinatory algebras with duality.","sentences":["We investigate a class of combinatory algebras, called ribbon combinatory algebras, in which we can interpret both the braided untyped linear lambda calculus and framed oriented tangles.","Any reflexive object in a ribbon category gives rise to a ribbon combinatory algebra.","Conversely, From a ribbon combinatory algebra, we can construct a ribbon category with a reflexive object, from which the combinatory algebra can be recovered.","To show this, and also to give the equational characterisation of ribbon combinatory algebras, we make use of the internal PRO construction developed in Hasegawa's recent work.","Interestingly, we can characterise ribbon combinatory algebras in two different ways: as balanced combinatory algebras with a trace combinator, and as balanced combinatory algebras with duality."],"url":"http://arxiv.org/abs/2405.10152v1","category":"cs.LO"}
{"created":"2024-05-16 14:32:51","title":"Light-ray sum rules and the c-anomaly","abstract":"In a four-dimensional quantum field theory that flows between two fixed points under the renormalization group, the change in the conformal anomaly $\\Delta a$ has been related to the average null energy. We extend this result to derive a sum rule for the other anomaly coefficient, $\\Delta c$, in terms of the stress tensor three-point function. While the sum rule for $\\Delta a$ is an expectation value of the averaged null energy operator, and therefore positive, the result for $\\Delta c$ involves the off-diagonal matrix elements, so it does not have a fixed sign.","sentences":["In a four-dimensional quantum field theory that flows between two fixed points under the renormalization group, the change in the conformal anomaly $\\Delta a$ has been related to the average null energy.","We extend this result to derive a sum rule for the other anomaly coefficient, $\\Delta c$, in terms of the stress tensor three-point function.","While the sum rule for $\\Delta a$ is an expectation value of the averaged null energy operator, and therefore positive, the result for $\\Delta c$ involves the off-diagonal matrix elements, so it does not have a fixed sign."],"url":"http://arxiv.org/abs/2405.10137v1","category":"hep-th"}
{"created":"2024-05-16 14:31:07","title":"Turkronicles: Diachronic Resources for the Fast Evolving Turkish Language","abstract":"Over the past century, the Turkish language has undergone substantial changes, primarily driven by governmental interventions. In this work, our goal is to investigate the evolution of the Turkish language since the establishment of T\\\"urkiye in 1923. Thus, we first introduce Turkronicles which is a diachronic corpus for Turkish derived from the Official Gazette of T\\\"urkiye. Turkronicles contains 45,375 documents, detailing governmental actions, making it a pivotal resource for analyzing the linguistic evolution influenced by the state policies. In addition, we expand an existing diachronic Turkish corpus which consists of the records of the Grand National Assembly of T\\\"urkiye by covering additional years. Next, combining these two diachronic corpora, we seek answers for two main research questions: How have the Turkish vocabulary and the writing conventions changed since the 1920s? Our analysis reveals that the vocabularies of two different time periods diverge more as the time between them increases, and newly coined Turkish words take the place of their old counterparts. We also observe changes in writing conventions. In particular, the use of circumflex noticeably decreases and words ending with the letters \"-b\" and \"-d\" are successively replaced with \"-p\" and \"-t\" letters, respectively. Overall, this study quantitatively highlights the dramatic changes in Turkish from various aspects of the language in a diachronic perspective.","sentences":["Over the past century, the Turkish language has undergone substantial changes, primarily driven by governmental interventions.","In this work, our goal is to investigate the evolution of the Turkish language since the establishment of T\\\"urkiye in 1923.","Thus, we first introduce Turkronicles which is a diachronic corpus for Turkish derived from the Official Gazette of T\\\"urkiye.","Turkronicles contains 45,375 documents, detailing governmental actions, making it a pivotal resource for analyzing the linguistic evolution influenced by the state policies.","In addition, we expand an existing diachronic Turkish corpus which consists of the records of the Grand National Assembly of T\\\"urkiye by covering additional years.","Next, combining these two diachronic corpora, we seek answers for two main research questions: How have the Turkish vocabulary and the writing conventions changed since the 1920s?","Our analysis reveals that the vocabularies of two different time periods diverge more as the time between them increases, and newly coined Turkish words take the place of their old counterparts.","We also observe changes in writing conventions.","In particular, the use of circumflex noticeably decreases and words ending with the letters \"-b\" and \"-d\" are successively replaced with \"-p\" and \"-t\" letters, respectively.","Overall, this study quantitatively highlights the dramatic changes in Turkish from various aspects of the language in a diachronic perspective."],"url":"http://arxiv.org/abs/2405.10133v1","category":"cs.CL"}
{"created":"2024-05-16 14:24:44","title":"Estimating a Function and Its Derivatives Under a Smoothness Condition","abstract":"We consider the problem of estimating an unknown function f* and its partial derivatives from a noisy data set of n observations, where we make no assumptions about f* except that it is smooth in the sense that it has square integrable partial derivatives of order m. A natural candidate for the estimator of f* in such a case is the best fit to the data set that satisfies a certain smoothness condition. This estimator can be seen as a least squares estimator subject to an upper bound on some measure of smoothness. Another useful estimator is the one that minimizes the degree of smoothness subject to an upper bound on the average of squared errors. We prove that these two estimators are computable as solutions to quadratic programs, establish the consistency of these estimators and their partial derivatives, and study the convergence rate as n increases to infinity. The effectiveness of the estimators is illustrated numerically in a setting where the value of a stock option and its second derivative are estimated as functions of the underlying stock price.","sentences":["We consider the problem of estimating an unknown function f* and its partial derivatives from a noisy data set of n observations, where we make no assumptions about f* except that it is smooth in the sense that it has square integrable partial derivatives of order m. A natural candidate for the estimator of f* in such a case is the best fit to the data set that satisfies a certain smoothness condition.","This estimator can be seen as a least squares estimator subject to an upper bound on some measure of smoothness.","Another useful estimator is the one that minimizes the degree of smoothness subject to an upper bound on the average of squared errors.","We prove that these two estimators are computable as solutions to quadratic programs, establish the consistency of these estimators and their partial derivatives, and study the convergence rate as n increases to infinity.","The effectiveness of the estimators is illustrated numerically in a setting where the value of a stock option and its second derivative are estimated as functions of the underlying stock price."],"url":"http://arxiv.org/abs/2405.10126v1","category":"stat.ML"}
{"created":"2024-05-16 14:22:49","title":"Asynchronous Federated Stochastic Optimization with Exact Averaging for Heterogeneous Local Objectives","abstract":"Federated learning (FL) was recently proposed to securely train models with data held over multiple locations (\"clients\") under the coordination of a central server. Two major challenges hindering the performance of FL algorithms are long training times caused by straggling clients and a decrease in training accuracy induced by non-iid local distributions (\"client drift\"). In this work we propose and analyze AREA, a new stochastic (sub)gradient algorithm that is robust to client drift and utilizes asynchronous communication to speed up convergence in the presence of stragglers. Moreover, AREA is, to the best of our knowledge, the first method that is both guaranteed to converge under arbitrarily long delays, and converges to an error neighborhood whose size depends only on the variance of the stochastic (sub)gradients used and thus is independent of both the heterogeneity between the local datasets and the length of client delays, without the use of delay-adaptive stepsizes. Our numerical results confirm our theoretical analysis and suggest that AREA outperforms state-of-the-art methods when local data are highly non-iid.","sentences":["Federated learning (FL) was recently proposed to securely train models with data held over multiple locations (\"clients\") under the coordination of a central server.","Two major challenges hindering the performance of FL algorithms are long training times caused by straggling clients and a decrease in training accuracy induced by non-iid local distributions (\"client drift\").","In this work we propose and analyze AREA, a new stochastic (sub)gradient algorithm that is robust to client drift and utilizes asynchronous communication to speed up convergence in the presence of stragglers.","Moreover, AREA is, to the best of our knowledge, the first method that is both guaranteed to converge under arbitrarily long delays, and converges to an error neighborhood whose size depends only on the variance of the stochastic (sub)gradients used and thus is independent of both the heterogeneity between the local datasets and the length of client delays, without the use of delay-adaptive stepsizes.","Our numerical results confirm our theoretical analysis and suggest that AREA outperforms state-of-the-art methods when local data are highly non-iid."],"url":"http://arxiv.org/abs/2405.10123v1","category":"cs.LG"}
{"created":"2024-05-16 14:13:03","title":"Beyond memory-effect matrix-based imaging in scattering media by acousto-optic gating","abstract":"Imaging inside scattering media at optical resolution is a longstanding challenge affecting multiple fields, from bio-medicine to astronomy. In recent years, several groundbreaking techniques for imaging inside scattering media, in particular scattering-matrix based approaches, have shown great promise, but usually suffer from a restricted field of view (FOV) due to their reliance on the optical 'memory-effect'. Here, we demonstrate that by combining acousto-optic spatial-gating with state-of-the-art matrix-based imaging techniques, diffraction-limited imaging beyond the optical memory-effect can be achieved in a robust fashion. Specifically, we show that this can be achieved by computational processing of scattered light fields captured under scanned acousto-optic modulation. The approach can be directly utilized whenever the ultrasound focus size is of the order of the memory-effect range, independently of the scattering angle.","sentences":["Imaging inside scattering media at optical resolution is a longstanding challenge affecting multiple fields, from bio-medicine to astronomy.","In recent years, several groundbreaking techniques for imaging inside scattering media, in particular scattering-matrix based approaches, have shown great promise, but usually suffer from a restricted field of view (FOV) due to their reliance on the optical 'memory-effect'.","Here, we demonstrate that by combining acousto-optic spatial-gating with state-of-the-art matrix-based imaging techniques, diffraction-limited imaging beyond the optical memory-effect can be achieved in a robust fashion.","Specifically, we show that this can be achieved by computational processing of scattered light fields captured under scanned acousto-optic modulation.","The approach can be directly utilized whenever the ultrasound focus size is of the order of the memory-effect range, independently of the scattering angle."],"url":"http://arxiv.org/abs/2405.10118v1","category":"physics.optics"}
{"created":"2024-05-16 14:05:13","title":"Master thesis: High-rate multipartite quantum secret sharing with continuous variables","abstract":"Quantum cryptography has undergone substantial growth and development within the multi-disciplinary field of quantum information in recent years. The field is constantly advancing with new protocols being developed, security measures being improved, and the first practical applications of these technologies being deployed in optical fibers and free space optical beams. In this paper, we present a comprehensive review of a cutting-edge metropolitan-scale protocol for continuous-variable quantum cryptography. The protocol allows an arbitrary number of users to send modulated coherent states to a relay, where a generalised Bell detection creates secure multipartite correlations. These correlations are then distilled into a shared secret key, providing a secure method for quantum secret-sharing. This novel approach to quantum cryptography has the potential to offer high-rate secure multipartite communication using readily available optical components, making it a promising advancement in the field.","sentences":["Quantum cryptography has undergone substantial growth and development within the multi-disciplinary field of quantum information in recent years.","The field is constantly advancing with new protocols being developed, security measures being improved, and the first practical applications of these technologies being deployed in optical fibers and free space optical beams.","In this paper, we present a comprehensive review of a cutting-edge metropolitan-scale protocol for continuous-variable quantum cryptography.","The protocol allows an arbitrary number of users to send modulated coherent states to a relay, where a generalised Bell detection creates secure multipartite correlations.","These correlations are then distilled into a shared secret key, providing a secure method for quantum secret-sharing.","This novel approach to quantum cryptography has the potential to offer high-rate secure multipartite communication using readily available optical components, making it a promising advancement in the field."],"url":"http://arxiv.org/abs/2405.10113v1","category":"quant-ph"}
{"created":"2024-05-16 14:04:01","title":"Spatial Cognition: a Wave Hypothesis","abstract":"Animals build Bayesian 3D models of their surroundings, to control their movements. There is strong selection pressure to make these models as precise as possible, given their sense data. A previous paper has described how a precise 3D model of space can be built by object tracking. This only works if 3D locations are stored with high spatial precision. Neural models of 3D spatial memory have large random errors; too large to support the tracking model. An alternative is described, in which neurons couple to a wave excitation in the brain, representing 3-D space. This can give high spatial precision, fast response, and other benefits. Three lines of evidence support the wave hypothesis: (1) it has better precision and speed than neural spatial memory, and is good enough to support object tracking; (2) the central body of the insect brain, whose form is highly conserved across all insect species, is well suited to hold a wave; and (3) the thalamus, whose round shape is conserved across all mammal species, is well suited to hold a wave. These lines of evidence strongly support the wave hypothesis.","sentences":["Animals build Bayesian 3D models of their surroundings, to control their movements.","There is strong selection pressure to make these models as precise as possible, given their sense data.","A previous paper has described how a precise 3D model of space can be built by object tracking.","This only works if 3D locations are stored with high spatial precision.","Neural models of 3D spatial memory have large random errors; too large to support the tracking model.","An alternative is described, in which neurons couple to a wave excitation in the brain, representing 3-D space.","This can give high spatial precision, fast response, and other benefits.","Three lines of evidence support the wave hypothesis: (1) it has better precision and speed than neural spatial memory, and is good enough to support object tracking; (2) the central body of the insect brain, whose form is highly conserved across all insect species, is well suited to hold a wave; and (3) the thalamus, whose round shape is conserved across all mammal species, is well suited to hold a wave.","These lines of evidence strongly support the wave hypothesis."],"url":"http://arxiv.org/abs/2405.10112v1","category":"q-bio.NC"}
{"created":"2024-05-16 14:03:25","title":"The $a_1$ factorisation coefficient in ${\\overline{B}^0} \\to D^{(*)+}M^{-}$ and ${\\overline{B}^0} \\to D^{(*)+}D_s^{(*)-}$ decays: measurements versus theory","abstract":"Using recent measurements of exclusive B-meson decays we extract the $a_1$ factorization parameter in ${\\overline{B}^{0}_{d}} \\to D^{(*)^+} K^-/\\pi^-$ decay channels. { The values obtained in the four channels are very similar, in agreement with the theoretical expectations obtained in the $m_Q \\to \\infty$ limit, but the measured values differ definitely from the expected central values. } Such differences have already been observed. Using recent data, we improve the accuracy by {a factor close to two} in this comparison. We study possible interpretations for such a difference and conclude that the claimed accuracy of corrections due to soft gluon contributions and finite mass effects has to be revisited before arguing for possible \"New-Physics\" effects. We observe that the corrections to the $m_Q \\to \\infty$ limit are larger in the spectator topology than effects from exchange amplitude contributions. We discuss also the way expected ratios of $a_1$ values, involving hypotheses from theory, are used to obtain the fraction of $B^0_s$ meson production in jets at LHC and conclude that the uncertainties attached to this approach are not well established. Finally, the $a_1$ values extracted from $\\overline{B}^{0}_{d} \\to D^{(*)+} D_s^-$ decays are found to be similar, within uncertainties, to the ones obtained with $K^-$ emission, once expected penguin contributions are corrected. Using the same approach, we note that $a_1$ values measured in channels with a $D_s^{*-}$ emission are about two standard deviations lower than $a_1(D^{(*)+}K^-$).","sentences":["Using recent measurements of exclusive B-meson decays we extract the $a_1$ factorization parameter in ${\\overline{B}^{0}_{d}} \\to D^{(*)^+} K^-/\\pi^-$ decay channels.","{","The values obtained in the four channels are very similar, in agreement with the theoretical expectations obtained in the $m_Q \\to \\infty$ limit, but the measured values differ definitely from the expected central values. }","Such differences have already been observed.","Using recent data, we improve the accuracy by {a factor close to two} in this comparison.","We study possible interpretations for such a difference and conclude that the claimed accuracy of corrections due to soft gluon contributions and finite mass effects has to be revisited before arguing for possible \"New-Physics\" effects.","We observe that the corrections to the $m_Q \\to \\infty$ limit are larger in the spectator topology than effects from exchange amplitude contributions.","We discuss also the way expected ratios of $a_1$ values, involving hypotheses from theory, are used to obtain the fraction of $B^0_s$ meson production in jets at LHC and conclude that the uncertainties attached to this approach are not well established.","Finally, the $a_1$ values extracted from $\\overline{B}^{0}_{d} \\to D^{(*)+} D_s^-$ decays are found to be similar, within uncertainties, to the ones obtained with $K^-$ emission, once expected penguin contributions are corrected.","Using the same approach, we note that $a_1$ values measured in channels with a $D_s^{*-}$ emission are about two standard deviations lower than $a_1(D^{(*)+}K^-$)."],"url":"http://arxiv.org/abs/2405.10111v1","category":"hep-ph"}
{"created":"2024-05-16 14:01:11","title":"Interferometric Purcell suppression of spontaneous emission in a superconducting qubit","abstract":"In superconducting qubits, suppression of spontaneous emission is essential to achieve fast dispersive measurement and reset without sacrificing qubit lifetime. We show that resonator-mediated decay of the qubit mode to the feedline can be suppressed using destructive interference, where the readout resonator is coupled to the feedline at two points. This \"interferometric Purcell filter\" does not require dedicated filter components or impedance mismatch in the feedline, making it suitable for applications such as all-pass readout. We design and fabricate a device with the proposed scheme and demonstrate suppression of resonator-mediated decay that exceeds 2 orders of magnitude over a bandwidth of 400 MHz.","sentences":["In superconducting qubits, suppression of spontaneous emission is essential to achieve fast dispersive measurement and reset without sacrificing qubit lifetime.","We show that resonator-mediated decay of the qubit mode to the feedline can be suppressed using destructive interference, where the readout resonator is coupled to the feedline at two points.","This \"interferometric Purcell filter\" does not require dedicated filter components or impedance mismatch in the feedline, making it suitable for applications such as all-pass readout.","We design and fabricate a device with the proposed scheme and demonstrate suppression of resonator-mediated decay that exceeds 2 orders of magnitude over a bandwidth of 400 MHz."],"url":"http://arxiv.org/abs/2405.10107v1","category":"quant-ph"}
{"created":"2024-05-16 13:55:27","title":"Global analysis of the $U(3)^5$ symmetric SMEFT","abstract":"The $U(3)^5$ symmetry within the SMEFT framework restricts the inclusion of only fully flavor-conserving operators at dimension six. This proceeding presents a global analysis of the SMEFT under this assumption. We provide global constraints on all 41 Wilson coefficients, utilizing leading-order and next-to-leading-order SMEFT predictions for various experiments including parity-violating experiments, Electroweak Precision Observables (EWPO), Higgs physics, top quark interactions, flavor observables, dijet production, and lepton scatterings. We address issues concerning the constraints of specific four-quark operators, investigate correlations between observables at different energy scales, and assess the impact of next-to-leading-order contributions on the global fit.","sentences":["The $U(3)^5$ symmetry within the SMEFT framework restricts the inclusion of only fully flavor-conserving operators at dimension six.","This proceeding presents a global analysis of the SMEFT under this assumption.","We provide global constraints on all 41 Wilson coefficients, utilizing leading-order and next-to-leading-order SMEFT predictions for various experiments including parity-violating experiments, Electroweak Precision Observables (EWPO), Higgs physics, top quark interactions, flavor observables, dijet production, and lepton scatterings.","We address issues concerning the constraints of specific four-quark operators, investigate correlations between observables at different energy scales, and assess the impact of next-to-leading-order contributions on the global fit."],"url":"http://arxiv.org/abs/2405.10101v1","category":"hep-ph"}
{"created":"2024-05-16 13:30:35","title":"Continuous Transfer Learning for UAV Communication-aware Trajectory Design","abstract":"Deep Reinforcement Learning (DRL) emerges as a prime solution for Unmanned Aerial Vehicle (UAV) trajectory planning, offering proficiency in navigating high-dimensional spaces, adaptability to dynamic environments, and making sequential decisions based on real-time feedback. Despite these advantages, the use of DRL for UAV trajectory planning requires significant retraining when the UAV is confronted with a new environment, resulting in wasted resources and time. Therefore, it is essential to develop techniques that can reduce the overhead of retraining DRL models, enabling them to adapt to constantly changing environments. This paper presents a novel method to reduce the need for extensive retraining using a double deep Q network (DDQN) model as a pretrained base, which is subsequently adapted to different urban environments through Continuous Transfer Learning (CTL). Our method involves transferring the learned model weights and adapting the learning parameters, including the learning and exploration rates, to suit each new environment specific characteristics. The effectiveness of our approach is validated in three scenarios, each with different levels of similarity. CTL significantly improves learning speed and success rates compared to DDQN models initiated from scratch. For similar environments, Transfer Learning (TL) improved stability, accelerated convergence by 65%, and facilitated 35% faster adaptation in dissimilar settings.","sentences":["Deep Reinforcement Learning (DRL) emerges as a prime solution for Unmanned Aerial Vehicle (UAV) trajectory planning, offering proficiency in navigating high-dimensional spaces, adaptability to dynamic environments, and making sequential decisions based on real-time feedback.","Despite these advantages, the use of DRL for UAV trajectory planning requires significant retraining when the UAV is confronted with a new environment, resulting in wasted resources and time.","Therefore, it is essential to develop techniques that can reduce the overhead of retraining DRL models, enabling them to adapt to constantly changing environments.","This paper presents a novel method to reduce the need for extensive retraining using a double deep Q network (DDQN) model as a pretrained base, which is subsequently adapted to different urban environments through Continuous Transfer Learning (CTL).","Our method involves transferring the learned model weights and adapting the learning parameters, including the learning and exploration rates, to suit each new environment specific characteristics.","The effectiveness of our approach is validated in three scenarios, each with different levels of similarity.","CTL significantly improves learning speed and success rates compared to DDQN models initiated from scratch.","For similar environments, Transfer Learning (TL) improved stability, accelerated convergence by 65%, and facilitated 35% faster adaptation in dissimilar settings."],"url":"http://arxiv.org/abs/2405.10087v1","category":"eess.SP"}
{"created":"2024-05-16 12:55:43","title":"L\u00e9vy flight for electrons in graphene in the presence of regions with enhanced spin-orbit coupling","abstract":"We propose an electronic L\\'evy glass built from graphene nanoribbons in the presence of regions with enhanced spin-orbit coupling. Although electrons in graphene nanoribbons present a low spin-orbit coupling strength, it can be increased by a proximity effect with an appropriate substrate. We consider graphene nanoribbons with different edge types, which contain circular regions with a tunable Rashba spin-orbit coupling, whose diameter follow a power-law distribution. We find that spin-orbital clusters induce a transition from superdiffusive to diffusive charge transport, similar to what we recently reported for nanoribbons with electrostatic clusters [Phys. Rev. B. 107, 155432 (2023)]. We also investigate spin polarization in the spin-orbital L\\'evy glasses, and show that a finite spin polarization can be found only in the superdiffusive regime. In contrast, the spin polarization vanishes in the diffusive regime, making the electronic L\\'evy glass a useful device whose electronic transmission and spin polarization can be controlled by its Fermi energy. Finally, we apply a multifractal analysis to charge transmission and spin polarization, and find that the transmission time series in the superdiffusive regime are multifractal, while they tend to be monofractal in the diffusive regime. In contrast, spin polarization time series are multifractal in both regimes, characterizing a marked difference between mesoscopic fluctuations of charge transport and spin polarization in the proposed electronic L\\'evy glass.","sentences":["We propose an electronic L\\'evy glass built from graphene nanoribbons in the presence of regions with enhanced spin-orbit coupling.","Although electrons in graphene nanoribbons present a low spin-orbit coupling strength, it can be increased by a proximity effect with an appropriate substrate.","We consider graphene nanoribbons with different edge types, which contain circular regions with a tunable Rashba spin-orbit coupling, whose diameter follow a power-law distribution.","We find that spin-orbital clusters induce a transition from superdiffusive to diffusive charge transport, similar to what we recently reported for nanoribbons with electrostatic clusters [Phys. Rev. B. 107, 155432 (2023)].","We also investigate spin polarization in the spin-orbital L\\'evy glasses, and show that a finite spin polarization can be found only in the superdiffusive regime.","In contrast, the spin polarization vanishes in the diffusive regime, making the electronic L\\'evy glass a useful device whose electronic transmission and spin polarization can be controlled by its Fermi energy.","Finally, we apply a multifractal analysis to charge transmission and spin polarization, and find that the transmission time series in the superdiffusive regime are multifractal, while they tend to be monofractal in the diffusive regime.","In contrast, spin polarization time series are multifractal in both regimes, characterizing a marked difference between mesoscopic fluctuations of charge transport and spin polarization in the proposed electronic L\\'evy glass."],"url":"http://arxiv.org/abs/2405.10066v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 12:16:03","title":"A quality control analysis of the resting state hypothesis via permutation entropy on EEG recordings","abstract":"The analysis of electrophysiological recordings of the human brain in resting state is a key experimental technique in neuroscience. Resting state is indeed the default condition to characterize brain dynamics. Its successful implementation relies both on the capacity of subjects to comply with the requirement of staying awake while not performing any cognitive task, and on the capacity of the experimenter to validate that compliance. Here we propose a novel approach, based on permutation entropy, to provide a quality control of the resting state condition by evaluating its stability during a recording. We combine the calculation of permutation entropy with a method for the estimation of its uncertainty out of a single time series, thus enabling a statistically robust assessment of resting state stationarity. The approach is showcased on electroencephalographic data recorded from young and elderly subjects and considering both eyes-closed and eyes-opened resting state conditions. Besides showing the reliability of the approach, the results showed higher instability in elderly subjects that hint at a qualitative difference between the two age groups with regard to the distribution of unstable activity within the brain. The method is therefore a tool that provides insights on the issue of resting state stability of interest for neuroscience experiments. The method can be applied to other kinds of electrophysiological data like, for example, magnetoencephalographic recordings. In addition, provided that suitable hardware and software processing units are used, its implementation, which consists here of a posteriori analysis, can be translated into a real time one.","sentences":["The analysis of electrophysiological recordings of the human brain in resting state is a key experimental technique in neuroscience.","Resting state is indeed the default condition to characterize brain dynamics.","Its successful implementation relies both on the capacity of subjects to comply with the requirement of staying awake while not performing any cognitive task, and on the capacity of the experimenter to validate that compliance.","Here we propose a novel approach, based on permutation entropy, to provide a quality control of the resting state condition by evaluating its stability during a recording.","We combine the calculation of permutation entropy with a method for the estimation of its uncertainty out of a single time series, thus enabling a statistically robust assessment of resting state stationarity.","The approach is showcased on electroencephalographic data recorded from young and elderly subjects and considering both eyes-closed and eyes-opened resting state conditions.","Besides showing the reliability of the approach, the results showed higher instability in elderly subjects that hint at a qualitative difference between the two age groups with regard to the distribution of unstable activity within the brain.","The method is therefore a tool that provides insights on the issue of resting state stability of interest for neuroscience experiments.","The method can be applied to other kinds of electrophysiological data like, for example, magnetoencephalographic recordings.","In addition, provided that suitable hardware and software processing units are used, its implementation, which consists here of a posteriori analysis, can be translated into a real time one."],"url":"http://arxiv.org/abs/2405.10035v1","category":"q-bio.NC"}
{"created":"2024-05-16 12:03:01","title":"$\u03c4$-Tilting finiteness of group algebras of semidirect products of abelian $p$-groups and abelian $p'$-groups","abstract":"Demonet, Iyama and Jasso introduced a new class of finite dimensional algebras, $\\tau$-tilting finite algebras. It was shown by Eisele, Janssens and Raedschelders that tame blocks of group algebras of finite groups are always $\\tau$-tilting finite. Given the classical result that the representation type (representation finite, tame or wild) of blocks is determined by their defect groups, it is natural to ask what kinds of subgroups control $\\tau$-tilting finiteness of group algebras or their blocks. In this paper, as a positive answer to this question, we demonstrate that $\\tau$-tilting finiteness of a group algebra of a finite group $G$ is controlled by a $p$-hyperfocal subgroup of $G$ under some assumptions on $G$. We consider a group algebra of a finite group $P\\rtimes H$ over an algebraically closed field of positive characteristic $p$, where $P$ is an abelian $p$-group and $H$ is an abelian $p'$-group acting on $P$, and show that $p$-hyperfocal subgroups determine $\\tau$-tilting finiteness of the group algebras in this case.","sentences":["Demonet, Iyama and Jasso introduced a new class of finite dimensional algebras, $\\tau$-tilting finite algebras.","It was shown by Eisele, Janssens and Raedschelders that tame blocks of group algebras of finite groups are always $\\tau$-tilting finite.","Given the classical result that the representation type (representation finite, tame or wild) of blocks is determined by their defect groups, it is natural to ask what kinds of subgroups control $\\tau$-tilting finiteness of group algebras or their blocks.","In this paper, as a positive answer to this question, we demonstrate that $\\tau$-tilting finiteness of a group algebra of a finite group $G$ is controlled by a $p$-hyperfocal subgroup of $G$ under some assumptions on $G$. We consider a group algebra of a finite group $P\\rtimes H$ over an algebraically closed field of positive characteristic $p$, where $P$ is an abelian $p$-group and $H$ is an abelian $p'$-group acting on $P$, and show that $p$-hyperfocal subgroups determine $\\tau$-tilting finiteness of the group algebras in this case."],"url":"http://arxiv.org/abs/2405.10021v1","category":"math.RT"}
{"created":"2024-05-16 11:49:37","title":"Dirac operators on the half-line: stability of spectrum and non-relativistic limit","abstract":"We consider Dirac operators on the half-line, subject to generalised infinite-mass boundary conditions. We derive sufficient conditions which guarantee the stability of the spectrum against possibly non-self-adjoint potential perturbations and study the optimality of the obtained results. Finally, we establish a non-relativistic limit which makes a relationship of the present model to the Robin Laplacian on the half-line.","sentences":["We consider Dirac operators on the half-line, subject to generalised infinite-mass boundary conditions.","We derive sufficient conditions which guarantee the stability of the spectrum against possibly non-self-adjoint potential perturbations and study the optimality of the obtained results.","Finally, we establish a non-relativistic limit which makes a relationship of the present model to the Robin Laplacian on the half-line."],"url":"http://arxiv.org/abs/2405.10009v1","category":"math.SP"}
{"created":"2024-05-16 11:47:40","title":"Sampling Theorem and interpolation formula for non-vanishing signals","abstract":"The paper establishes frequency predictability criteria and presents predictors for two-sided non-vanishing bounded continuous time signals, i.e., for signals from $L_{\\infty}(\\R)$ that do not necessarily vanish at $\\pm\\infty$. The notions of transfer functions, the spectrum gaps, bandlimitness, and high-pass filters are introduced for these signals. This allowed to obtain an analog Whittaker-Shannon-Kotelnikov sampling theorem and a new modification of the corresponding interpolation formula that makes it applicable for non-vanishing signals.","sentences":["The paper establishes frequency predictability criteria and presents predictors for two-sided non-vanishing bounded continuous time signals, i.e., for signals from $L_{\\infty}(\\R)$ that do not necessarily vanish at $\\pm\\infty$. The notions of transfer functions, the spectrum gaps, bandlimitness, and high-pass filters are introduced for these signals.","This allowed to obtain an analog Whittaker-Shannon-Kotelnikov sampling theorem and a new modification of the corresponding interpolation formula that makes it applicable for non-vanishing signals."],"url":"http://arxiv.org/abs/2405.10007v1","category":"cs.IT"}
{"created":"2024-05-16 11:27:20","title":"Probing neutrino-nucleus interaction in DUNE and MicroBooNE","abstract":"The neutrino experiments utilize heavy nuclear targets to achieve high statistics neutrino-nucleus interaction event rate, which leads to systematic uncertainties in the oscillation parameters due to the nuclear effects and uncertainties in the cross-section. Understanding the interaction of neutrinos with the nucleus becomes crucial in determining the oscillation parameters with high precision. We investigate the uncertainty in quasi-elastic interaction due to nuclear effects by selecting exactly 1 proton, 0 pions, and any number of neutrons in the final state using DUNE and MicroBooNE detectors, and the effects on oscillation parameters in the DUNE detector. The kinematic method along with this selection can be used for accurate neutrino energy reconstruction in the quasi-elastic channel where the nuclear effects are inevitable.","sentences":["The neutrino experiments utilize heavy nuclear targets to achieve high statistics neutrino-nucleus interaction event rate, which leads to systematic uncertainties in the oscillation parameters due to the nuclear effects and uncertainties in the cross-section.","Understanding the interaction of neutrinos with the nucleus becomes crucial in determining the oscillation parameters with high precision.","We investigate the uncertainty in quasi-elastic interaction due to nuclear effects by selecting exactly 1 proton, 0 pions, and any number of neutrons in the final state using DUNE and MicroBooNE detectors, and the effects on oscillation parameters in the DUNE detector.","The kinematic method along with this selection can be used for accurate neutrino energy reconstruction in the quasi-elastic channel where the nuclear effects are inevitable."],"url":"http://arxiv.org/abs/2405.09994v1","category":"hep-ph"}
{"created":"2024-05-16 11:12:06","title":"The bosonic string spectrum and the explicit states up to level $10$ from the lightcone and the chaotic behavior of certain string amplitudes","abstract":"We compute the irreps and their multiplicities of bosonic string spectrum up to level 10 and we give explicitly the on shell lightcone states which make the irreps. For scalars and vectors we compute the multiplicity up to level 22 and 19 respectively. The first scalar at odd level appears at level 11. For the bosonic string in non critical dimensions we argue that at level $N$ there are always states transforming as tensors with $ s\\ge N/2$ indices. Only in critical dimensions there are states with $s\\le N/2$. Looking at the explicit coefficients of the combinations needed to make the irreps from the lightcone states we trace the origin of the chaotic behavior of certain cubic amplitudes considered in literature to the extremely precise and sensitive mixtures of states. For example the vectors at level $N=19$ are a linear combinations of states and when the coefficients are normalized to be integer some of them have more than 1200 figures.","sentences":["We compute the irreps and their multiplicities of bosonic string spectrum up to level 10 and we give explicitly the on shell lightcone states which make the irreps.","For scalars and vectors we compute the multiplicity up to level 22 and 19 respectively.","The first scalar at odd level appears at level 11.","For the bosonic string in non critical dimensions we argue that at level $N$ there are always states transforming as tensors with $ s\\ge N/2$ indices.","Only in critical dimensions there are states with $s\\le N/2$. Looking at the explicit coefficients of the combinations needed to make the irreps from the lightcone states we trace the origin of the chaotic behavior of certain cubic amplitudes considered in literature to the extremely precise and sensitive mixtures of states.","For example the vectors at level $N=19$ are a linear combinations of states and when the coefficients are normalized to be integer some of them have more than 1200 figures."],"url":"http://arxiv.org/abs/2405.09987v1","category":"hep-th"}
{"created":"2024-05-16 10:41:31","title":"Language-Oriented Semantic Latent Representation for Image Transmission","abstract":"In the new paradigm of semantic communication (SC), the focus is on delivering meanings behind bits by extracting semantic information from raw data. Recent advances in data-to-text models facilitate language-oriented SC, particularly for text-transformed image communication via image-to-text (I2T) encoding and text-to-image (T2I) decoding. However, although semantically aligned, the text is too coarse to precisely capture sophisticated visual features such as spatial locations, color, and texture, incurring a significant perceptual difference between intended and reconstructed images. To address this limitation, in this paper, we propose a novel language-oriented SC framework that communicates both text and a compressed image embedding and combines them using a latent diffusion model to reconstruct the intended image. Experimental results validate the potential of our approach, which transmits only 2.09\\% of the original image size while achieving higher perceptual similarities in noisy communication channels compared to a baseline SC method that communicates only through text.The code is available at https://github.com/ispamm/Img2Img-SC/ .","sentences":["In the new paradigm of semantic communication (SC), the focus is on delivering meanings behind bits by extracting semantic information from raw data.","Recent advances in data-to-text models facilitate language-oriented SC, particularly for text-transformed image communication via image-to-text (I2T) encoding and text-to-image (T2I) decoding.","However, although semantically aligned, the text is too coarse to precisely capture sophisticated visual features such as spatial locations, color, and texture, incurring a significant perceptual difference between intended and reconstructed images.","To address this limitation, in this paper, we propose a novel language-oriented SC framework that communicates both text and a compressed image embedding and combines them using a latent diffusion model to reconstruct the intended image.","Experimental results validate the potential of our approach, which transmits only 2.09\\% of the original image size while achieving higher perceptual similarities in noisy communication channels compared to a baseline SC method that communicates only through text.","The code is available at https://github.com/ispamm/Img2Img-SC/ ."],"url":"http://arxiv.org/abs/2405.09976v1","category":"cs.CV"}
{"created":"2024-05-16 10:39:49","title":"Distributed Delta-Coloring under Bandwidth Limitations","abstract":"We consider the problem of coloring graphs of maximum degree $\\Delta$ with $\\Delta$ colors in the distributed setting with limited bandwidth. Specifically, we give a $\\mathsf{poly}\\log\\log n$-round randomized algorithm in the CONGEST model. This is close to the lower bound of $\\Omega(\\log \\log n)$ rounds from [Brandt et al., STOC '16], which holds also in the more powerful LOCAL model. The core of our algorithm is a reduction to several special instances of the constructive Lov\\'asz local lemma (LLL) and the $deg+1$-list coloring problem.","sentences":["We consider the problem of coloring graphs of maximum degree $\\Delta$ with $\\Delta$ colors in the distributed setting with limited bandwidth.","Specifically, we give a $\\mathsf{poly}\\log\\log n$-round randomized algorithm in the CONGEST model.","This is close to the lower bound of $\\Omega(\\log \\log n)$ rounds from [Brandt et al., STOC '16], which holds also in the more powerful LOCAL model.","The core of our algorithm is a reduction to several special instances of the constructive Lov\\'asz local lemma (LLL) and the $deg+1$-list coloring problem."],"url":"http://arxiv.org/abs/2405.09975v1","category":"cs.DS"}
{"created":"2024-05-16 10:04:43","title":"Igusa-Todorov distances under singular equivalences and recollements","abstract":"The Igusa-Todorov distances of an Artin algebra give a reasonable way of measuring how far an algebra is from being Igusa-Todorov algebras. We show that the Igusa-Todorov distance is an invariant under the singular equivalence of Morita type with level, and establish a series of bounds for the Igusa-Todorov distances of the algebras involved in a recollement of derived module categories.","sentences":["The Igusa-Todorov distances of an Artin algebra give a reasonable way of measuring how far an algebra is from being Igusa-Todorov algebras.","We show that the Igusa-Todorov distance is an invariant under the singular equivalence of Morita type with level, and establish a series of bounds for the Igusa-Todorov distances of the algebras involved in a recollement of derived module categories."],"url":"http://arxiv.org/abs/2405.09958v1","category":"math.RT"}
{"created":"2024-05-16 09:56:37","title":"Zero-shot counting with a dual-stream neural network model","abstract":"Deep neural networks have provided a computational framework for understanding object recognition, grounded in the neurophysiology of the primate ventral stream, but fail to account for how we process relational aspects of a scene. For example, deep neural networks fail at problems that involve enumerating the number of elements in an array, a problem that in humans relies on parietal cortex. Here, we build a 'dual-stream' neural network model which, equipped with both dorsal and ventral streams, can generalise its counting ability to wholly novel items ('zero-shot' counting). In doing so, it forms spatial response fields and lognormal number codes that resemble those observed in macaque posterior parietal cortex. We use the dual-stream network to make successful predictions about behavioural studies of the human gaze during similar counting tasks.","sentences":["Deep neural networks have provided a computational framework for understanding object recognition, grounded in the neurophysiology of the primate ventral stream, but fail to account for how we process relational aspects of a scene.","For example, deep neural networks fail at problems that involve enumerating the number of elements in an array, a problem that in humans relies on parietal cortex.","Here, we build a 'dual-stream' neural network model which, equipped with both dorsal and ventral streams, can generalise its counting ability to wholly novel items ('zero-shot' counting).","In doing so, it forms spatial response fields and lognormal number codes that resemble those observed in macaque posterior parietal cortex.","We use the dual-stream network to make successful predictions about behavioural studies of the human gaze during similar counting tasks."],"url":"http://arxiv.org/abs/2405.09953v1","category":"q-bio.NC"}
{"created":"2024-05-16 09:34:05","title":"The $\u03ba$-generalised Distribution for Stock Returns","abstract":"Empirical evidence shows stock returns are often heavy-tailed rather than normally distributed. The $\\kappa$-generalised distribution, originated in the context of statistical physics by Kaniadakis, is characterised by the $\\kappa$-exponential function that is asymptotically exponential for small values and asymptotically power law for large values. This proves to be a useful property and makes it a good candidate distribution for many types of quantities. In this paper we focus on fitting historic daily stock returns for the FTSE 100 and the top 100 Nasdaq stocks. Using a Monte-Carlo goodness of fit test there is evidence that the $\\kappa$-generalised distribution is a good fit for a significant proportion of the 200 stock returns analysed.","sentences":["Empirical evidence shows stock returns are often heavy-tailed rather than normally distributed.","The $\\kappa$-generalised distribution, originated in the context of statistical physics by Kaniadakis, is characterised by the $\\kappa$-exponential function that is asymptotically exponential for small values and asymptotically power law for large values.","This proves to be a useful property and makes it a good candidate distribution for many types of quantities.","In this paper we focus on fitting historic daily stock returns for the FTSE 100 and the top 100 Nasdaq stocks.","Using a Monte-Carlo goodness of fit test there is evidence that the $\\kappa$-generalised distribution is a good fit for a significant proportion of the 200 stock returns analysed."],"url":"http://arxiv.org/abs/2405.09929v1","category":"q-fin.ST"}
{"created":"2024-05-16 09:17:06","title":"CCD $UBV(RI)_{KC}$ photometry and dynamics of the open cluster NGC 1513","abstract":"We derive astrophysical parameters of the open cluster NGC 1513 by means of colour indices built with new $CCD\\,UBV(RI)_{KC}$ photometry. Based on early-type members, the mean foreground reddening and total to selective extinction ratio are E(B-V)=0.79$\\pm$0.09 mag and $R_{V}$=2.85$\\pm$0.05. Through the differential grid method, we derive the metal abundance [Fe/H]=-0.06 dex (Z=+0.013), which is consistent with the value [Fe/H]=-0.088 of the bright giant member-LAMOST-695710060. The Z=+0.013 isochrone fit to the V x (B-V) colour-magnitude diagram leads to a turn-off age of 224$\\pm$27 Myr (thus an intermediate-age cluster), and a distance modulus of ($V_{0}$ - $M_{\\rm V}$)=10.90$\\pm$0.15 mag, thus implying a distance from the Sun d=1514$\\pm$105 pc. Within the uncertainties, our photometric distance is consistent with the value d=1435$\\pm$14 pc from the Gaia DR3 parallax. We find signs of small mass segregation through a minimum spanning tree analysis for the 190 most massive stars, together with the rather steep mass function ($\\chi$=+2.39) slope. The high core to half-light radius ratio $R_{core}$/$R_{h}$=0.82, together with the compact half-light to tidal radius ratio $R_{h}$/$R_{t}$=0.22, suggest that it is probably related to cluster-formation effects, due to little dynamical evolution, instead of driving its dynamical evolution by internal relaxation. Indeed, NGC 1513 is located in the second quadrant ($\\ell$=152.59$^{\\circ}$ and Galactocentric distance $R_{GC}$=9.57 kpc), which tends to minimize tidal effects by external processes and tidal disruption. Therefore, internal mass segregation effects in NGC 1513 seem to be more efficient than cluster evaporation processes. We find that NGC 1513 migrated about 0.50 kpc from its birth place.","sentences":["We derive astrophysical parameters of the open cluster NGC 1513 by means of colour indices built with new $CCD\\,UBV(RI)_{KC}$ photometry.","Based on early-type members, the mean foreground reddening and total to selective extinction ratio are E(B-V)=0.79$\\pm$0.09 mag and $R_{V}$=2.85$\\pm$0.05.","Through the differential grid method, we derive the metal abundance [Fe/H]=-0.06 dex (Z=+0.013), which is consistent with the value [Fe/H]=-0.088 of the bright giant member-LAMOST-695710060.","The Z=+0.013 isochrone fit to the V x (B-V) colour-magnitude diagram leads to a turn-off age of 224$\\pm$27 Myr (thus an intermediate-age cluster), and a distance modulus of ($V_{0}$ - $M_{\\rm V}$)=10.90$\\pm$0.15 mag, thus implying a distance from the Sun d=1514$\\pm$105 pc.","Within the uncertainties, our photometric distance is consistent with the value d=1435$\\pm$14 pc from the Gaia DR3 parallax.","We find signs of small mass segregation through a minimum spanning tree analysis for the 190 most massive stars, together with the rather steep mass function ($\\chi$=+2.39) slope.","The high core to half-light radius ratio $R_{core}$/$R_{h}$=0.82, together with the compact half-light to tidal radius ratio $R_{h}$/$R_{t}$=0.22, suggest that it is probably related to cluster-formation effects, due to little dynamical evolution, instead of driving its dynamical evolution by internal relaxation.","Indeed, NGC 1513 is located in the second quadrant ($\\ell$=152.59$^{\\circ}$ and Galactocentric distance $R_{GC}$=9.57 kpc), which tends to minimize tidal effects by external processes and tidal disruption.","Therefore, internal mass segregation effects in NGC 1513 seem to be more efficient than cluster evaporation processes.","We find that NGC 1513 migrated about 0.50 kpc from its birth place."],"url":"http://arxiv.org/abs/2405.09918v1","category":"astro-ph.GA"}
{"created":"2024-05-16 09:08:09","title":"TransMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data","abstract":"Transliterating related languages that use different scripts into a common script shows effectiveness in improving crosslingual transfer in downstream tasks. However, this methodology often makes pretraining a model from scratch unavoidable, as transliteration brings about new subwords not covered in existing multilingual pretrained language models (mPLMs). This is not desired because it takes a lot of computation budget for pretraining. A more promising way is to make full use of available mPLMs. To this end, this paper proposes a simple but effective framework: Transliterate-Merge-Initialize (TransMI), which can create a strong baseline well-suited for data that is transliterated into a common script by exploiting an mPLM and its accompanied tokenizer. TransMI has three stages: (a) transliterate the vocabulary of an mPLM into a common script; (b) merge the new vocabulary with the original vocabulary; and (c) initialize the embeddings of the new subwords. We applied TransMI to three recent strong mPLMs, and our experiments demonstrate that TransMI not only preserves their ability to handle non-transliterated data, but also enables the models to effectively process transliterated data: the results show a consistent improvement of 3% to 34%, varying across different models and tasks. We make our code and models publicly available at \\url{https://github.com/cisnlp/TransMI}.","sentences":["Transliterating related languages that use different scripts into a common script shows effectiveness in improving crosslingual transfer in downstream tasks.","However, this methodology often makes pretraining a model from scratch unavoidable, as transliteration brings about new subwords not covered in existing multilingual pretrained language models (mPLMs).","This is not desired because it takes a lot of computation budget for pretraining.","A more promising way is to make full use of available mPLMs.","To this end, this paper proposes a simple but effective framework: Transliterate-Merge-Initialize (TransMI), which can create a strong baseline well-suited for data that is transliterated into a common script by exploiting an mPLM and its accompanied tokenizer.","TransMI has three stages: (a) transliterate the vocabulary of an mPLM into a common script; (b) merge the new vocabulary with the original vocabulary; and (c) initialize the embeddings of the new subwords.","We applied TransMI to three recent strong mPLMs, and our experiments demonstrate that TransMI not only preserves their ability to handle non-transliterated data, but also enables the models to effectively process transliterated data: the results show a consistent improvement of 3% to 34%, varying across different models and tasks.","We make our code and models publicly available at \\url{https://github.com/cisnlp/TransMI}."],"url":"http://arxiv.org/abs/2405.09913v1","category":"cs.CL"}
{"created":"2024-05-16 08:58:09","title":"Performance testing of a novel short axis photomultiplier tube for the HUNT project","abstract":"Photomultiplier tubes (PMTs) with large-area cathodes are increasingly being used in cosmic-ray experiments to enhance detection efficiency. The optical modules (OMs) of the High-Energy Underwater Neutrino Telescope (HUNT) have employed a brand new N6205 20-inch microchannel plate photomultiplier tube (MCP-PMT) developed by the North Night Vision Science & Technology (Nanjing) Research Institute Co. Ltd. (NNVT). In order to make the 20-inch PMT fit into the 23-inch diameter pressure-resistant glass sphere, NNVT improved the internal structure of PMT and shortened the height of PMT by more than 10~cm. The first batch of these PMTs has been delivered for preliminary research work. This paper describes a specific PMT testing platform built for the first batch of 15 MCP-PMTs, and some performance parameters of PMT, such as P/V ratio, TTS and nonliniearity, are measured.The measurement results show that the new PMT still has good performance and can meet the requirements of HUNT project.","sentences":["Photomultiplier tubes (PMTs) with large-area cathodes are increasingly being used in cosmic-ray experiments to enhance detection efficiency.","The optical modules (OMs) of the High-Energy Underwater Neutrino Telescope (HUNT) have employed a brand new N6205 20-inch microchannel plate photomultiplier tube (MCP-PMT) developed by","the North Night Vision Science & Technology (Nanjing) Research Institute Co. Ltd. (NNVT).","In order to make the 20-inch PMT fit into the 23-inch diameter pressure-resistant glass sphere, NNVT improved the internal structure of PMT and shortened the height of PMT by more than 10~cm.","The first batch of these PMTs has been delivered for preliminary research work.","This paper describes a specific PMT testing platform built for the first batch of 15 MCP-PMTs, and some performance parameters of PMT, such as P/V ratio, TTS and nonliniearity, are measured.","The measurement results show that the new PMT still has good performance and can meet the requirements of HUNT project."],"url":"http://arxiv.org/abs/2405.09910v1","category":"hep-ex"}
{"created":"2024-05-16 08:15:18","title":"Adaptive Proton Therapy Using CBCT-Guided Digital Twins","abstract":"This study aims to develop a digital twin (DT) framework to enhance adaptive proton stereotactic body radiation therapy (SBRT) for prostate cancer. Prostate SBRT has emerged as a leading option for external beam radiotherapy due to its effectiveness and reduced treatment duration. However, interfractional anatomy variations can impact treatment outcomes. This study seeks to address these uncertainties using DT concept, with the goal of improving treatment quality, potentially revolutionizing prostate radiotherapy to offer personalized treatment solutions. Our study presented a pioneering approach that leverages DT technology to enhance adaptive proton SBRT. The framework improves treatment plans by utilizing patient-specific CTV setup uncertainty, which is usually smaller than conventional clinical setups. This research contributes to the ongoing efforts to enhance the efficiency and efficacy of prostate radiotherapy, with ultimate goals of improving patient outcomes and life quality.","sentences":["This study aims to develop a digital twin (DT) framework to enhance adaptive proton stereotactic body radiation therapy (SBRT) for prostate cancer.","Prostate SBRT has emerged as a leading option for external beam radiotherapy due to its effectiveness and reduced treatment duration.","However, interfractional anatomy variations can impact treatment outcomes.","This study seeks to address these uncertainties using DT concept, with the goal of improving treatment quality, potentially revolutionizing prostate radiotherapy to offer personalized treatment solutions.","Our study presented a pioneering approach that leverages DT technology to enhance adaptive proton SBRT.","The framework improves treatment plans by utilizing patient-specific CTV setup uncertainty, which is usually smaller than conventional clinical setups.","This research contributes to the ongoing efforts to enhance the efficiency and efficacy of prostate radiotherapy, with ultimate goals of improving patient outcomes and life quality."],"url":"http://arxiv.org/abs/2405.09891v1","category":"physics.med-ph"}
{"created":"2024-05-16 07:14:32","title":"Density-based clustering algorithm for galaxy group/cluster identification","abstract":"A direct approach to studying the galaxy-halo connection is the analysis of observed groups and clusters of galaxies that trace the underlying dark matter halos, making identifying galaxy clusters and their associated brightest cluster galaxies (BCGs) crucial. We test and propose a robust density-based clustering algorithm that outperforms the traditional Friends-of-Friends (FoF) algorithm in the currently available galaxy group/cluster catalogs. Our new approach is a modified version of the Ordering Points To Identify the Clustering Structure (OPTICS) algorithm, which accounts for line-of-sight positional uncertainties due to redshift space distortions by incorporating a scaling factor, and is thereby referred to as sOPTICS. When tested on both a galaxy group catalog based on semi-analytic galaxy formation simulations and observational data, our algorithm demonstrated robustness to outliers and relative insensitivity to hyperparameter choices. In total, we compared the results of eight clustering algorithms. The proposed density-based clustering method, sOPTICS, outperforms FoF in accurately identifying giant galaxy clusters and their associated BCGs in various environments with higher purity and recovery rate, also successfully recovering 115 BCGs out of 118 reliable BCGs from a large galaxy sample.","sentences":["A direct approach to studying the galaxy-halo connection is the analysis of observed groups and clusters of galaxies that trace the underlying dark matter halos, making identifying galaxy clusters and their associated brightest cluster galaxies (BCGs) crucial.","We test and propose a robust density-based clustering algorithm that outperforms the traditional Friends-of-Friends (FoF) algorithm in the currently available galaxy group/cluster catalogs.","Our new approach is a modified version of the Ordering Points To Identify the Clustering Structure (OPTICS) algorithm, which accounts for line-of-sight positional uncertainties due to redshift space distortions by incorporating a scaling factor, and is thereby referred to as sOPTICS.","When tested on both a galaxy group catalog based on semi-analytic galaxy formation simulations and observational data, our algorithm demonstrated robustness to outliers and relative insensitivity to hyperparameter choices.","In total, we compared the results of eight clustering algorithms.","The proposed density-based clustering method, sOPTICS, outperforms FoF in accurately identifying giant galaxy clusters and their associated BCGs in various environments with higher purity and recovery rate, also successfully recovering 115 BCGs out of 118 reliable BCGs from a large galaxy sample."],"url":"http://arxiv.org/abs/2405.09855v1","category":"astro-ph.CO"}
{"created":"2024-05-16 07:14:13","title":"On the relevance of pre-neural approaches in natural language processing pedagogy","abstract":"While neural approaches using deep learning are the state-of-the-art for natural language processing (NLP) today, pre-neural algorithms and approaches still find a place in NLP textbooks and courses of recent years. In this paper, we compare two introductory NLP courses taught in Australia and India, and examine how Transformer and pre-neural approaches are balanced within the lecture plan and assessments of the courses. We also draw parallels with the objects-first and objects-later debate in CS1 education. We observe that pre-neural approaches add value to student learning by building an intuitive understanding of NLP problems, potential solutions and even Transformer-based models themselves. Despite pre-neural approaches not being state-of-the-art, the paper makes a case for their inclusion in NLP courses today.","sentences":["While neural approaches using deep learning are the state-of-the-art for natural language processing (NLP) today, pre-neural algorithms and approaches still find a place in NLP textbooks and courses of recent years.","In this paper, we compare two introductory NLP courses taught in Australia and India, and examine how Transformer and pre-neural approaches are balanced within the lecture plan and assessments of the courses.","We also draw parallels with the objects-first and objects-later debate in CS1 education.","We observe that pre-neural approaches add value to student learning by building an intuitive understanding of NLP problems, potential solutions and even Transformer-based models themselves.","Despite pre-neural approaches not being state-of-the-art, the paper makes a case for their inclusion in NLP courses today."],"url":"http://arxiv.org/abs/2405.09854v1","category":"cs.CL"}
{"created":"2024-05-16 06:36:38","title":"Impurity bands, line-nodes, and anomalous thermal Hall effect in Weyl superconductors","abstract":"We investigate the anomalous thermal Hall effect (ATHE) in Weyl superconductors realized by the $E_{1u}$ ($p$-wave and $f$-wave) chiral superconducting order for the point group $D_{6h}$. Using the quasiclassical Eilenberger theory, we analyze the influence of the impurity scattering and the line nodal excitations on the ATHE, and compare it with the intrinsic (topological) contribution. Because the transverse response is sensitive to the slope of the density of states at the Fermi surface, the extrinsic ATHE vanishes in both the weak (Born) and strong (unitarity) scattering limits. The thermal Hall conductivity (THC) is maximal at intermediate impurity strengths when there is a large slope of the density states in the impurity bands close to the Fermi energy. Under these conditions, the extrinsic ATHE dominates the intrinsic ATHE even at low temperatures. The extrinsic ATHE is sensitive to line nodal excitations, whereas the intrinsic ATHE is not. When the line nodes in the gap involve the sign change of the order parameter, the extrinsic contribution to the THC is suppressed even though the phase space for low energy excitation is large. In contrast, if the nodes are not accompanied by such a sign change, the extrinsic ATHE is significantly enhanced. Our results form a basis for the comprehensive analysis of anomalous thermal transport in Weyl superconductors.","sentences":["We investigate the anomalous thermal Hall effect (ATHE) in Weyl superconductors realized by the $E_{1u}$ ($p$-wave and $f$-wave) chiral superconducting order for the point group $D_{6h}$. Using the quasiclassical Eilenberger theory, we analyze the influence of the impurity scattering and the line nodal excitations on the ATHE, and compare it with the intrinsic (topological) contribution.","Because the transverse response is sensitive to the slope of the density of states at the Fermi surface, the extrinsic ATHE vanishes in both the weak (Born) and strong (unitarity) scattering limits.","The thermal Hall conductivity (THC) is maximal at intermediate impurity strengths when there is a large slope of the density states in the impurity bands close to the Fermi energy.","Under these conditions, the extrinsic ATHE dominates the intrinsic ATHE even at low temperatures.","The extrinsic ATHE is sensitive to line nodal excitations, whereas the intrinsic ATHE is not.","When the line nodes in the gap involve the sign change of the order parameter, the extrinsic contribution to the THC is suppressed even though the phase space for low energy excitation is large.","In contrast, if the nodes are not accompanied by such a sign change, the extrinsic ATHE is significantly enhanced.","Our results form a basis for the comprehensive analysis of anomalous thermal transport in Weyl superconductors."],"url":"http://arxiv.org/abs/2405.09840v1","category":"cond-mat.supr-con"}
{"created":"2024-05-16 06:28:15","title":"On the edge turbulence in a DTT-like tokamak plasma","abstract":"Turbulent transport provides the main contribution to particle and energy losses in tokamak plasmas, which control is of paramount importance for forthcoming reactors such as the Divertor-Tokamak-Test (DTT) facility under construction at ENEA Frascati. In this work we investigate the characteristic features of drift turbulence at the plasma edge through 3D electro-static fluid simulations. We outline the crucial role of the diffusion coefficient for the emerging turbulent spectra and for the excitation of vortex structures or zonal flows. Moreover, the impact of adding a poloidal magnetic component is discussed considering also a radial shear, and the emergence of anisotropic spectral features is emphasized. The analysis is extended to the case with Dirichlet boundary conditions along the radial direction, instead of the periodic ones usually employed in such kind of analyses.","sentences":["Turbulent transport provides the main contribution to particle and energy losses in tokamak plasmas, which control is of paramount importance for forthcoming reactors such as the Divertor-Tokamak-Test (DTT) facility under construction at ENEA Frascati.","In this work we investigate the characteristic features of drift turbulence at the plasma edge through 3D electro-static fluid simulations.","We outline the crucial role of the diffusion coefficient for the emerging turbulent spectra and for the excitation of vortex structures or zonal flows.","Moreover, the impact of adding a poloidal magnetic component is discussed considering also a radial shear, and the emergence of anisotropic spectral features is emphasized.","The analysis is extended to the case with Dirichlet boundary conditions along the radial direction, instead of the periodic ones usually employed in such kind of analyses."],"url":"http://arxiv.org/abs/2405.09837v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 06:07:31","title":"Nearly Minimax Optimal Regret for Multinomial Logistic Bandit","abstract":"In this paper, we investigate the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model. There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the feature dimension $d$ and the maximum assortment size $K$. Additionally, the variation in reward structures between these bounds complicates the quest for optimality. Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of $\\Omega(d\\sqrt{\\smash[b]{T/K}})$ and propose a constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{\\smash[b]{T/K}})$. Under non-uniform rewards, we prove a lower bound of $\\Omega(d\\sqrt{T})$ and an upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$, also achievable by OFU-MNL+. Our empirical studies support these theoretical findings. To the best of our knowledge, this is the first work in the MNL contextual bandit literature to prove minimax optimality -- for either uniform or non-uniform reward setting -- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors.","sentences":["In this paper, we investigate the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model.","There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the feature dimension $d$ and the maximum assortment size $K$. Additionally, the variation in reward structures between these bounds complicates the quest for optimality.","Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of $\\Omega(d\\sqrt{\\smash[b]{T/K}})$ and propose a constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{\\smash[b]{T/K}})$. Under non-uniform rewards, we prove a lower bound of $\\Omega(d\\sqrt{T})$ and an upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$, also achievable by OFU-MNL+.","Our empirical studies support these theoretical findings.","To the best of our knowledge, this is the first work in the MNL contextual bandit literature to prove minimax optimality -- for either uniform or non-uniform reward setting -- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors."],"url":"http://arxiv.org/abs/2405.09831v1","category":"stat.ML"}
{"created":"2024-05-16 06:06:28","title":"Unveiling the Direct Piezoelectric Effect on Piezo-phototronic Coupling in Ferroelectrics: First Principle Study Assisted Experimental Approach","abstract":"A new study explores the distinct roles of spontaneous polarization and piezoelectric polarization in piezo-phototronic coupling. This investigation focuses on differences in photocatalytic and piezo-photocatalytic performance using sodium bismuth titanate (NBT), a key ferroelectric material. The research aims to identify which type of polarization has a greater influence on piezo-phototronic effects. A theoretical assessment complements the experimental findings, providing additional insights. This study explores the enhanced piezo-phototronic performance of electrospun nanofibers compared to sol-gel particles under different illumination conditions (11W UV, 250W UV, and natural sunlight). Electrospun nanofibers exhibited a rate constant (k) improvement of 2.5 to 3.75 times, whereas sol-gel particles showed only 1.3 to 1.4 times higher performance when ultrasonication was added to photocatalysis. Analysis using first-principle methods revealed that nanofibers had an elastic modulus (C33) about 2.15 times lower than sol-gel particles, indicating greater flexibility. The elongation of lattice along z-axis in the case of nanofibers reduced the covalency in the Bi-O and Ti-O bonds. These structural differences led to reduced spontaneous polarization and piezoelectric stress coefficients (e31 & e33). Despite having lower piezoelectric stress coefficients, higher flexibility in nanofibers led to a higher piezoelectric strain coefficient, 2.66 and 1.97 times greater than sol-gel particles, respectively. This improved the piezo-phototronic coupling for nanofibers.","sentences":["A new study explores the distinct roles of spontaneous polarization and piezoelectric polarization in piezo-phototronic coupling.","This investigation focuses on differences in photocatalytic and piezo-photocatalytic performance using sodium bismuth titanate (NBT), a key ferroelectric material.","The research aims to identify which type of polarization has a greater influence on piezo-phototronic effects.","A theoretical assessment complements the experimental findings, providing additional insights.","This study explores the enhanced piezo-phototronic performance of electrospun nanofibers compared to sol-gel particles under different illumination conditions (11W UV, 250W UV, and natural sunlight).","Electrospun nanofibers exhibited a rate constant (k) improvement of 2.5 to 3.75 times, whereas sol-gel particles showed only 1.3 to 1.4 times higher performance when ultrasonication was added to photocatalysis.","Analysis using first-principle methods revealed that nanofibers had an elastic modulus (C33) about 2.15 times lower than sol-gel particles, indicating greater flexibility.","The elongation of lattice along z-axis in the case of nanofibers reduced the covalency in the Bi-O and Ti-O bonds.","These structural differences led to reduced spontaneous polarization and piezoelectric stress coefficients (e31 & e33).","Despite having lower piezoelectric stress coefficients, higher flexibility in nanofibers led to a higher piezoelectric strain coefficient, 2.66 and 1.97 times greater than sol-gel particles, respectively.","This improved the piezo-phototronic coupling for nanofibers."],"url":"http://arxiv.org/abs/2405.09830v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 03:47:43","title":"Assessing carrier mobility, dopability, and defect tolerance in the chalcogenide perovskite BaZrS$_3$","abstract":"The chalcogenide perovskite BaZrS$_3$ has attracted much attention as a promising solar absorber for thin-film photovoltaics. Here, we use first-principles calculations to evaluate its carrier transport and defect properties. We find that BaZrS$_3$ has a phonon-limited electron mobility of 37 cm$^2$/Vs comparable to that in halide perovskites but lower hole mobility of 11 cm$^2$/Vs. The defect computations indicate that BaZrS$_3$ is intrinsically n-type due to shallow sulfur vacancies, but that strong compensation by sulfur vacancies will prevent attempts to make it p-type. We also establish that BaZrS$_3$ is a defect-tolerant absorber with few low formation energy, deep intrinsic defects. Among the deep defects, sulfur interstitials are the strongest nonradiative recombination centers which in sulfur-rich conditions would limit the carrier lifetime to 10 ns. Our work highlights the material's intrinsic limitations in carrier mobility and suggests suppressing the formation of sulfur interstitials to reach long carrier lifetime.","sentences":["The chalcogenide perovskite BaZrS$_3$ has attracted much attention as a promising solar absorber for thin-film photovoltaics.","Here, we use first-principles calculations to evaluate its carrier transport and defect properties.","We find that BaZrS$_3$ has a phonon-limited electron mobility of 37 cm$^2$/Vs comparable to that in halide perovskites but lower hole mobility of 11 cm$^2$/Vs.","The defect computations indicate that BaZrS$_3$ is intrinsically n-type due to shallow sulfur vacancies, but that strong compensation by sulfur vacancies will prevent attempts to make it p-type.","We also establish that BaZrS$_3$ is a defect-tolerant absorber with few low formation energy, deep intrinsic defects.","Among the deep defects, sulfur interstitials are the strongest nonradiative recombination centers which in sulfur-rich conditions would limit the carrier lifetime to 10 ns.","Our work highlights the material's intrinsic limitations in carrier mobility and suggests suppressing the formation of sulfur interstitials to reach long carrier lifetime."],"url":"http://arxiv.org/abs/2405.09793v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 03:42:15","title":"Challenging theories of dark energy with levitated force sensor","abstract":"The nature of dark energy is one of the most outstanding problems in physical science, and various theories have been proposed. It is therefore essential to directly verify or rule out these theories experimentally. However, despite substantial efforts in astrophysical observations and laboratory experiments, previous tests have not yet acquired enough accuracy to provide decisive conclusions as to the validity of these theories. Here, using a diamagnetically levitated force sensor, we carry out a test on one of the most compelling explanations for dark energy to date, namely the Chameleon theory, an ultra-light scalar field with screening mechanisms, which couples to normal-matter fields and leaves a detectable fifth force. Our results extend previous results by nearly two orders of magnitude to the entire physical plausible parameter space of cosmologically viable chameleon models. We find no evidence for such a fifth force. Our results decisively rule out the basic chameleon model as a candidate for dark energy. Our work, thus, demonstrates the robustness of laboratory experiments in unveiling the nature of dark energy in the future. The methodology developed here can be further applied to study a broad range of fundamental physics.","sentences":["The nature of dark energy is one of the most outstanding problems in physical science, and various theories have been proposed.","It is therefore essential to directly verify or rule out these theories experimentally.","However, despite substantial efforts in astrophysical observations and laboratory experiments, previous tests have not yet acquired enough accuracy to provide decisive conclusions as to the validity of these theories.","Here, using a diamagnetically levitated force sensor, we carry out a test on one of the most compelling explanations for dark energy to date, namely the Chameleon theory, an ultra-light scalar field with screening mechanisms, which couples to normal-matter fields and leaves a detectable fifth force.","Our results extend previous results by nearly two orders of magnitude to the entire physical plausible parameter space of cosmologically viable chameleon models.","We find no evidence for such a fifth force.","Our results decisively rule out the basic chameleon model as a candidate for dark energy.","Our work, thus, demonstrates the robustness of laboratory experiments in unveiling the nature of dark energy in the future.","The methodology developed here can be further applied to study a broad range of fundamental physics."],"url":"http://arxiv.org/abs/2405.09791v1","category":"gr-qc"}
{"created":"2024-05-16 02:52:21","title":"Integrating Uncertainty-Aware Human Motion Prediction into Graph-Based Manipulator Motion Planning","abstract":"There has been a growing utilization of industrial robots as complementary collaborators for human workers in re-manufacturing sites. Such a human-robot collaboration (HRC) aims to assist human workers in improving the flexibility and efficiency of labor-intensive tasks. In this paper, we propose a human-aware motion planning framework for HRC to effectively compute collision-free motions for manipulators when conducting collaborative tasks with humans. We employ a neural human motion prediction model to enable proactive planning for manipulators. Particularly, rather than blindly trusting and utilizing predicted human trajectories in the manipulator planning, we quantify uncertainties of the neural prediction model to further ensure human safety. Moreover, we integrate the uncertainty-aware prediction into a graph that captures key workspace elements and illustrates their interconnections. Then a graph neural network is leveraged to operate on the constructed graph. Consequently, robot motion planning considers both the dependencies among all the elements in the workspace and the potential influence of future movements of human workers. We experimentally validate the proposed planning framework using a 6-degree-of-freedom manipulator in a shared workspace where a human is performing disassembling tasks. The results demonstrate the benefits of our approach in terms of improving the smoothness and safety of HRC. A brief video introduction of this work is available as the supplemental materials.","sentences":["There has been a growing utilization of industrial robots as complementary collaborators for human workers in re-manufacturing sites.","Such a human-robot collaboration (HRC) aims to assist human workers in improving the flexibility and efficiency of labor-intensive tasks.","In this paper, we propose a human-aware motion planning framework for HRC to effectively compute collision-free motions for manipulators when conducting collaborative tasks with humans.","We employ a neural human motion prediction model to enable proactive planning for manipulators.","Particularly, rather than blindly trusting and utilizing predicted human trajectories in the manipulator planning, we quantify uncertainties of the neural prediction model to further ensure human safety.","Moreover, we integrate the uncertainty-aware prediction into a graph that captures key workspace elements and illustrates their interconnections.","Then a graph neural network is leveraged to operate on the constructed graph.","Consequently, robot motion planning considers both the dependencies among all the elements in the workspace and the potential influence of future movements of human workers.","We experimentally validate the proposed planning framework using a 6-degree-of-freedom manipulator in a shared workspace where a human is performing disassembling tasks.","The results demonstrate the benefits of our approach in terms of improving the smoothness and safety of HRC.","A brief video introduction of this work is available as the supplemental materials."],"url":"http://arxiv.org/abs/2405.09779v1","category":"cs.RO"}
{"created":"2024-05-16 02:22:09","title":"Harmonizing Generalization and Personalization in Federated Prompt Learning","abstract":"Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning. The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning. Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model's ability to generalize effectively. To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL. To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt. Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL. We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization.","sentences":["Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning.","The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning.","Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model's ability to generalize effectively.","To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL.","To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt.","Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL.","We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization."],"url":"http://arxiv.org/abs/2405.09771v1","category":"cs.LG"}
{"created":"2024-05-16 02:10:19","title":"Reassessment of the dipole in the distribution of quasars on the sky","abstract":"We investigate claims of an anomalously large amplitude of the dipole in the distribution of quasars on the sky. Two main issues indicate that the systematic uncertainties in the derived quasar-density dipole are underestimated. Firstly, the spatial distribution of the quasars is not a pure dipole, possessing low-order multipoles of comparable size to the dipole. These multipoles are unexpected and presumably caused by unknown systematic effects; we cannot be confident that the dipole amplitude is not also affected by the same systematics until the origin of these fluctuations is understood. Secondly, the 50 percent sky cut associated with the quasar catalogue strongly couples the multipoles, meaning that the power estimate at ell=1 contains significant contributions from ell>1. In particular, the dominant quadrupole mode in the Galactic mask strongly couples the dipole with the octupole, leading to a large uncertainty in the dipole amplitude. Together these issues mean that the dipole in the quasar catalogue has an uncertainty large enough that consistency with the cosmic microwave background (CMB) dipole cannot be ruled out. More generally, current data sets are insufficiently clean to robustly measure the quasar dipole and future studies will require samples that are larger (preferably covering more of the sky) and free of systematic effects to make strong claims regarding their consistency with the CMB dipole.","sentences":["We investigate claims of an anomalously large amplitude of the dipole in the distribution of quasars on the sky.","Two main issues indicate that the systematic uncertainties in the derived quasar-density dipole are underestimated.","Firstly, the spatial distribution of the quasars is not a pure dipole, possessing low-order multipoles of comparable size to the dipole.","These multipoles are unexpected and presumably caused by unknown systematic effects; we cannot be confident that the dipole amplitude is not also affected by the same systematics until the origin of these fluctuations is understood.","Secondly, the 50 percent sky cut associated with the quasar catalogue strongly couples the multipoles, meaning that the power estimate at ell=1 contains significant contributions from ell>1.","In particular, the dominant quadrupole mode in the Galactic mask strongly couples the dipole with the octupole, leading to a large uncertainty in the dipole amplitude.","Together these issues mean that the dipole in the quasar catalogue has an uncertainty large enough that consistency with the cosmic microwave background (CMB) dipole cannot be ruled out.","More generally, current data sets are insufficiently clean to robustly measure the quasar dipole and future studies will require samples that are larger (preferably covering more of the sky) and free of systematic effects to make strong claims regarding their consistency with the CMB dipole."],"url":"http://arxiv.org/abs/2405.09762v1","category":"astro-ph.CO"}
{"created":"2024-05-16 01:50:50","title":"Give and Take: An End-To-End Investigation of Giveaway Scam Conversion Rates","abstract":"Scams -- fraudulent schemes designed to swindle money from victims -- have existed for as long as recorded history. However, the Internet's combination of low communication cost, global reach, and functional anonymity has allowed scam volumes to reach new heights. Designing effective interventions requires first understanding the context: how scammers reach potential victims, the earnings they make, and any potential bottlenecks for durable interventions. In this short paper, we focus on these questions in the context of cryptocurrency giveaway scams, where victims are tricked into irreversibly transferring funds to scammers under the pretense of even greater returns. Combining data from Twitter, YouTube and Twitch livestreams, landing pages, and cryptocurrency blockchains, we measure how giveaway scams operate at scale. We find that 1 in 1000 scam tweets, and 4 in 100,000 livestream views, net a victim, and that scammers managed to extract nearly \\$4.62 million from just hundreds of victims during our measurement window.","sentences":["Scams -- fraudulent schemes designed to swindle money from victims -- have existed for as long as recorded history.","However, the Internet's combination of low communication cost, global reach, and functional anonymity has allowed scam volumes to reach new heights.","Designing effective interventions requires first understanding the context: how scammers reach potential victims, the earnings they make, and any potential bottlenecks for durable interventions.","In this short paper, we focus on these questions in the context of cryptocurrency giveaway scams, where victims are tricked into irreversibly transferring funds to scammers under the pretense of even greater returns.","Combining data from Twitter, YouTube and Twitch livestreams, landing pages, and cryptocurrency blockchains, we measure how giveaway scams operate at scale.","We find that 1 in 1000 scam tweets, and 4 in 100,000 livestream views, net a victim, and that scammers managed to extract nearly \\$4.62 million from just hundreds of victims during our measurement window."],"url":"http://arxiv.org/abs/2405.09757v1","category":"cs.CR"}
{"created":"2024-05-16 01:20:04","title":"Scalar curvature lower bounds on asymptotically flat manifolds","abstract":"In this paper, we consider the scalar curvature in the distributional sense of \\cite{MR3366052} and the scalar curvature lower bound in the $\\beta-$weak $(\\beta\\in(0, \\frac{1}{2}))$ sense of \\cite{MR4685089} on an asymptotically flat $n-$manifold with a $W^{1,p}(p>n)$ metric. We first show that the scalar curvature lower bound under the Ricci-DeTurck flow depends on the scalar curvature lower bound in the $\\beta-$weak sense and the time. Then we prove that the lower bound of the distributional scalar curvature of a $W^{1, p}$ metric coincides with the lower bound of the scalar curvature in the $\\beta-$weak sense at infinity.","sentences":["In this paper, we consider the scalar curvature in the distributional sense of \\cite{MR3366052} and the scalar curvature lower bound in the $\\beta-$weak $(\\beta\\in(0, \\frac{1}{2}))$ sense of \\cite{MR4685089} on an asymptotically flat $n-$manifold with a $W^{1,p}(p>n)$ metric.","We first show that the scalar curvature lower bound under the Ricci-DeTurck flow depends on the scalar curvature lower bound in the $\\beta-$weak sense and the time.","Then we prove that the lower bound of the distributional scalar curvature of a $W^{1, p}$ metric coincides with the lower bound of the scalar curvature in the $\\beta-$weak sense at infinity."],"url":"http://arxiv.org/abs/2405.09750v1","category":"math.DG"}
{"created":"2024-05-16 01:09:33","title":"NIFTY Financial News Headlines Dataset","abstract":"We introduce and make publicly available the NIFTY Financial News Headlines dataset, designed to facilitate and advance research in financial market forecasting using large language models (LLMs). This dataset comprises two distinct versions tailored for different modeling approaches: (i) NIFTY-LM, which targets supervised fine-tuning (SFT) of LLMs with an auto-regressive, causal language-modeling objective, and (ii) NIFTY-RL, formatted specifically for alignment methods (like reinforcement learning from human feedback (RLHF)) to align LLMs via rejection sampling and reward modeling. Each dataset version provides curated, high-quality data incorporating comprehensive metadata, market indices, and deduplicated financial news headlines systematically filtered and ranked to suit modern LLM frameworks. We also include experiments demonstrating some applications of the dataset in tasks like stock price movement and the role of LLM embeddings in information acquisition/richness. The NIFTY dataset along with utilities (like truncating prompt's context length systematically) are available on Hugging Face at https://huggingface.co/datasets/raeidsaqur/NIFTY.","sentences":["We introduce and make publicly available the NIFTY Financial News Headlines dataset, designed to facilitate and advance research in financial market forecasting using large language models (LLMs).","This dataset comprises two distinct versions tailored for different modeling approaches: (i) NIFTY-LM, which targets supervised fine-tuning (SFT) of LLMs with an auto-regressive, causal language-modeling objective, and (ii) NIFTY-RL, formatted specifically for alignment methods (like reinforcement learning from human feedback (RLHF)) to align LLMs via rejection sampling and reward modeling.","Each dataset version provides curated, high-quality data incorporating comprehensive metadata, market indices, and deduplicated financial news headlines systematically filtered and ranked to suit modern LLM frameworks.","We also include experiments demonstrating some applications of the dataset in tasks like stock price movement and the role of LLM embeddings in information acquisition/richness.","The NIFTY dataset along with utilities (like truncating prompt's context length systematically) are available on Hugging Face at https://huggingface.co/datasets/raeidsaqur/NIFTY."],"url":"http://arxiv.org/abs/2405.09747v1","category":"q-fin.CP"}
{"created":"2024-05-16 00:44:58","title":"Infinite differentiability of the free energy for a Derrida-Retaux system","abstract":"We consider a recursive system which was introduced by Derrida and Retaux (J. Stat. Phys. ${\\bf 156}$ (2014) 268-290) as a toy model to study the depinning transition in presence of disorder. Derrida and Retaux predicted the free energy $F_\\infty(p)$ of the system exhibit quite an unusual physical phenomenon which is an infinite order phase transition. Hu and Shi (J. Stat. Phys. ${\\bf 172}$ (2018) 718-741) studied a special situation and obtained other behavior of the free energy, while insisted on $p=p_c$ being an essential singularity. Recently, Chen, Dagard, Derrida, Hu, Lifshits and Shi (Ann. Probab. ${\\bf 49}$ (2021) 637-670) confirmed the Derrida-Retaux conjecture under suitable integrability condition. However, in the mathematical review, it is still unknown whether the free energy is infinitely differentiable at the critical point. So that, we continue to study the infinite differentiability of the free energy in this paper.","sentences":["We consider a recursive system which was introduced by Derrida and Retaux (J. Stat.","Phys.","${\\bf 156}$ (2014) 268-290) as a toy model to study the depinning transition in presence of disorder.","Derrida and Retaux predicted the free energy $F_\\infty(p)$ of the system exhibit quite an unusual physical phenomenon which is an infinite order phase transition.","Hu and Shi (J. Stat.","Phys.","${\\bf 172}$ (2018) 718-741) studied a special situation and obtained other behavior of the free energy, while insisted on $p=p_c$ being an essential singularity.","Recently, Chen, Dagard, Derrida, Hu, Lifshits and Shi (Ann. Probab.","${\\bf 49}$ (2021) 637-670) confirmed the Derrida-Retaux conjecture under suitable integrability condition.","However, in the mathematical review, it is still unknown whether the free energy is infinitely differentiable at the critical point.","So that, we continue to study the infinite differentiability of the free energy in this paper."],"url":"http://arxiv.org/abs/2405.09741v1","category":"math.PR"}
{"created":"2024-05-16 00:08:29","title":"On the conjugacy separability of ordinary and generalized Baumslag-Solitar groups","abstract":"Let $\\mathcal{C}$ be a class of groups. A group $X$ is said to be residually a $\\mathcal{C}$-group (conjugacy $\\mathcal{C}$-separable) if, for any elements $x,y \\in X$ that are not equal (not conjugate in $X$), there exists a homomorphism $\\sigma$ of $X$ onto a group from $\\mathcal{C}$ such that the elements $x\\sigma$ and $y\\sigma$ are still not equal (respectively, not conjugate in $X\\sigma$). A generalized Baumslag-Solitar group or GBS-group is the fundamental group of a finite connected graph of groups whose all vertex and edge groups are infinite cyclic. An ordinary Baumslag-Solitar group is the GBS-group that corresponds to a graph containing only one vertex and one loop. Suppose that the class $\\mathcal{C}$ consists of periodic groups and is closed under taking subgroups and unrestricted wreath products. We prove that a non-solvable GBS-group is conjugacy $\\mathcal{C}$-separable if and only if it is residually a $\\mathcal{C}$-group. We also find a criterion for a solvable GBS-group to be conjugacy $\\mathcal{C}$-separable. As a corollary, we prove that an arbitrary GBS-group is conjugacy (finite) separable if and only if it is residually finite.","sentences":["Let $\\mathcal{C}$ be a class of groups.","A group $X$ is said to be residually a $\\mathcal{C}$-group (conjugacy $\\mathcal{C}$-separable) if, for any elements $x,y \\in X$ that are not equal (not conjugate in $X$), there exists a homomorphism $\\sigma$ of $X$ onto a group from $\\mathcal{C}$ such that the elements $x\\sigma$ and $y\\sigma$ are still not equal (respectively, not conjugate in $X\\sigma$).","A generalized Baumslag-Solitar group or GBS-group is the fundamental group of a finite connected graph of groups whose all vertex and edge groups are infinite cyclic.","An ordinary Baumslag-Solitar group is the GBS-group that corresponds to a graph containing only one vertex and one loop.","Suppose that the class $\\mathcal{C}$ consists of periodic groups and is closed under taking subgroups and unrestricted wreath products.","We prove that a non-solvable GBS-group is conjugacy $\\mathcal{C}$-separable if and only if it is residually a $\\mathcal{C}$-group.","We also find a criterion for a solvable GBS-group to be conjugacy $\\mathcal{C}$-separable.","As a corollary, we prove that an arbitrary GBS-group is conjugacy (finite) separable if and only if it is residually finite."],"url":"http://arxiv.org/abs/2405.09736v1","category":"math.GR"}
{"created":"2024-05-15 22:31:29","title":"DP-RuL: Differentially-Private Rule Learning for Clinical Decision Support Systems","abstract":"Serious privacy concerns arise with the use of patient data in rule-based clinical decision support systems (CDSS). The goal of a privacy-preserving CDSS is to learn a population ruleset from individual clients' local rulesets, while protecting the potentially sensitive information contained in the rulesets. We present the first work focused on this problem and develop a framework for learning population rulesets with local differential privacy (LDP), suitable for use within a distributed CDSS and other distributed settings. Our rule discovery protocol uses a Monte-Carlo Tree Search (MCTS) method integrated with LDP to search a rule grammar in a structured way and find rule structures clients are likely to have. Randomized response queries are sent to clients to determine promising paths to search within the rule grammar. In addition, we introduce an adaptive budget allocation method which dynamically determines how much privacy loss budget to use at each query, resulting in better privacy-utility trade-offs. We evaluate our approach using three clinical datasets and find that we are able to learn population rulesets with high coverage (breadth of rules) and clinical utility even at low privacy loss budgets.","sentences":["Serious privacy concerns arise with the use of patient data in rule-based clinical decision support systems (CDSS).","The goal of a privacy-preserving CDSS is to learn a population ruleset from individual clients' local rulesets, while protecting the potentially sensitive information contained in the rulesets.","We present the first work focused on this problem and develop a framework for learning population rulesets with local differential privacy (LDP), suitable for use within a distributed CDSS and other distributed settings.","Our rule discovery protocol uses a Monte-Carlo Tree Search (MCTS) method integrated with LDP to search a rule grammar in a structured way and find rule structures clients are likely to have.","Randomized response queries are sent to clients to determine promising paths to search within the rule grammar.","In addition, we introduce an adaptive budget allocation method which dynamically determines how much privacy loss budget to use at each query, resulting in better privacy-utility trade-offs.","We evaluate our approach using three clinical datasets and find that we are able to learn population rulesets with high coverage (breadth of rules) and clinical utility even at low privacy loss budgets."],"url":"http://arxiv.org/abs/2405.09721v1","category":"cs.CR"}
{"created":"2024-05-15 22:08:19","title":"Attention-aided Outdoor Localization in Commercial 5G NR Systems","abstract":"The integration of high-precision cellular localization and machine learning (ML) is considered a cornerstone technique in future cellular navigation systems, offering unparalleled accuracy and functionality. This study focuses on localization based on uplink channel measurements in a fifth-generation (5G) new radio (NR) system. An attention-aided ML-based single-snapshot localization pipeline is presented, which consists of several cascaded blocks, namely a signal processing block, an attention-aided block, and an uncertainty estimation block. Specifically, the signal processing block generates an impulse response beam matrix for all beams. The attention-aided block trains on the channel impulse responses using an attention-aided network, which captures the correlation between impulse responses for different beams. The uncertainty estimation block predicts the probability density function of the UE position, thereby also indicating the confidence level of the localization result. Two representative uncertainty estimation techniques, the negative log-likelihood and the regression-by-classification techniques, are applied and compared. Furthermore, for dynamic measurements with multiple snapshots available, we combine the proposed pipeline with a Kalman filter to enhance localization accuracy. To evaluate our approach, we extract channel impulse responses for different beams from a commercial base station. The outdoor measurement campaign covers Line-of-Sight (LoS), Non-Line-of-Sight (NLoS), and a mix of LoS and NLoS scenarios. The results show that sub-meter localization accuracy can be achieved.","sentences":["The integration of high-precision cellular localization and machine learning (ML) is considered a cornerstone technique in future cellular navigation systems, offering unparalleled accuracy and functionality.","This study focuses on localization based on uplink channel measurements in a fifth-generation (5G) new radio (NR) system.","An attention-aided ML-based single-snapshot localization pipeline is presented, which consists of several cascaded blocks, namely a signal processing block, an attention-aided block, and an uncertainty estimation block.","Specifically, the signal processing block generates an impulse response beam matrix for all beams.","The attention-aided block trains on the channel impulse responses using an attention-aided network, which captures the correlation between impulse responses for different beams.","The uncertainty estimation block predicts the probability density function of the UE position, thereby also indicating the confidence level of the localization result.","Two representative uncertainty estimation techniques, the negative log-likelihood and the regression-by-classification techniques, are applied and compared.","Furthermore, for dynamic measurements with multiple snapshots available, we combine the proposed pipeline with a Kalman filter to enhance localization accuracy.","To evaluate our approach, we extract channel impulse responses for different beams from a commercial base station.","The outdoor measurement campaign covers Line-of-Sight (LoS), Non-Line-of-Sight (NLoS), and a mix of LoS and NLoS scenarios.","The results show that sub-meter localization accuracy can be achieved."],"url":"http://arxiv.org/abs/2405.09715v1","category":"eess.SP"}
{"created":"2024-05-15 21:58:56","title":"Cosmological Singularity and Power-Law Solutions in Modified Gravity","abstract":"A bouncing Universe avoids the big-bang singularity. Using the time-like and null Raychaudhhuri equations, we explore whether the bounce near the big-bang, within a broad spectrum of modified theories of gravity, allows for cosmologically relevant power-law solutions under reasonable physical conditions. Our study shows that certain modified theories of gravity, such as Stelle gravity, do not demonstrate singularity resolution under any reasonable conditions, while others including $f(R)$ gravity and Brans-Dicke theory can demonstrate singularity resolution under suitable conditions. For these theories, we show that the accelerating solution is slightly favoured over ekypyrosis.","sentences":["A bouncing Universe avoids the big-bang singularity.","Using the time-like and null Raychaudhhuri equations, we explore whether the bounce near the big-bang, within a broad spectrum of modified theories of gravity, allows for cosmologically relevant power-law solutions under reasonable physical conditions.","Our study shows that certain modified theories of gravity, such as Stelle gravity, do not demonstrate singularity resolution under any reasonable conditions, while others including $f(R)$ gravity and Brans-Dicke theory can demonstrate singularity resolution under suitable conditions.","For these theories, we show that the accelerating solution is slightly favoured over ekypyrosis."],"url":"http://arxiv.org/abs/2405.09714v1","category":"gr-qc"}
{"created":"2024-05-15 21:51:45","title":"The lattice of submonoids of the uniform block permutations containing the symmetric group","abstract":"We study the lattice of submonoids of the uniform block permutation monoid containing the symmetric group (which is its group of units). We prove that this lattice is distributive under union and intersection by relating the submonoids containing the symmetric group to downsets in a new partial order on integer partitions. Furthermore, we show that the sizes of the $\\mathscr{J}$-classes of the uniform block permutation monoid are sums of squares of dimensions of irreducible modules of the monoid algebra.","sentences":["We study the lattice of submonoids of the uniform block permutation monoid containing the symmetric group (which is its group of units).","We prove that this lattice is distributive under union and intersection by relating the submonoids containing the symmetric group to downsets in a new partial order on integer partitions.","Furthermore, we show that the sizes of the $\\mathscr{J}$-classes of the uniform block permutation monoid are sums of squares of dimensions of irreducible modules of the monoid algebra."],"url":"http://arxiv.org/abs/2405.09710v1","category":"math.CO"}
{"created":"2024-05-15 21:13:54","title":"Point2SSM++: Self-Supervised Learning of Anatomical Shape Models from Point Clouds","abstract":"Correspondence-based statistical shape modeling (SSM) stands as a powerful technology for morphometric analysis in clinical research. SSM facilitates population-level characterization and quantification of anatomical shapes such as bones and organs, aiding in pathology and disease diagnostics and treatment planning. Despite its potential, SSM remains under-utilized in medical research due to the significant overhead associated with automatic construction methods, which demand complete, aligned shape surface representations. Additionally, optimization-based techniques rely on bias-inducing assumptions or templates and have prolonged inference times as the entire cohort is simultaneously optimized. To overcome these challenges, we introduce Point2SSM++, a principled, self-supervised deep learning approach that directly learns correspondence points from point cloud representations of anatomical shapes. Point2SSM++ is robust to misaligned and inconsistent input, providing SSM that accurately samples individual shape surfaces while effectively capturing population-level statistics. Additionally, we present principled extensions of Point2SSM++ to adapt it for dynamic spatiotemporal and multi-anatomy use cases, demonstrating the broad versatility of the Point2SSM++ framework. Furthermore, we present extensions of Point2SSM++ tailored for dynamic spatiotemporal and multi-anatomy scenarios, showcasing the broad versatility of the framework. Through extensive validation across diverse anatomies, evaluation metrics, and clinically relevant downstream tasks, we demonstrate Point2SSM++'s superiority over existing state-of-the-art deep learning models and traditional approaches. Point2SSM++ substantially enhances the feasibility of SSM generation and significantly broadens its array of potential clinical applications.","sentences":["Correspondence-based statistical shape modeling (SSM) stands as a powerful technology for morphometric analysis in clinical research.","SSM facilitates population-level characterization and quantification of anatomical shapes such as bones and organs, aiding in pathology and disease diagnostics and treatment planning.","Despite its potential, SSM remains under-utilized in medical research due to the significant overhead associated with automatic construction methods, which demand complete, aligned shape surface representations.","Additionally, optimization-based techniques rely on bias-inducing assumptions or templates and have prolonged inference times as the entire cohort is simultaneously optimized.","To overcome these challenges, we introduce Point2SSM++, a principled, self-supervised deep learning approach that directly learns correspondence points from point cloud representations of anatomical shapes.","Point2SSM++ is robust to misaligned and inconsistent input, providing SSM that accurately samples individual shape surfaces while effectively capturing population-level statistics.","Additionally, we present principled extensions of Point2SSM++ to adapt it for dynamic spatiotemporal and multi-anatomy use cases, demonstrating the broad versatility of the Point2SSM++ framework.","Furthermore, we present extensions of Point2SSM++ tailored for dynamic spatiotemporal and multi-anatomy scenarios, showcasing the broad versatility of the framework.","Through extensive validation across diverse anatomies, evaluation metrics, and clinically relevant downstream tasks, we demonstrate Point2SSM++'s superiority over existing state-of-the-art deep learning models and traditional approaches.","Point2SSM++ substantially enhances the feasibility of SSM generation and significantly broadens its array of potential clinical applications."],"url":"http://arxiv.org/abs/2405.09707v1","category":"cs.CV"}
{"created":"2024-05-15 21:09:14","title":"Canonical transformations applied to the non-free Landau electron","abstract":"The method previously used to solve Schr\\\"odinger equation by a unitary transformation for a electron under the influence of a constant magnetic field is used to obtain a non-free Landau electron wave function. The physical meaning of this wave function is discussed based on the conserved properties of the transformed Hamiltonian.","sentences":["The method previously used to solve Schr\\\"odinger equation by a unitary transformation for a electron under the influence of a constant magnetic field is used to obtain a non-free Landau electron wave function.","The physical meaning of this wave function is discussed based on the conserved properties of the transformed Hamiltonian."],"url":"http://arxiv.org/abs/2405.09706v1","category":"quant-ph"}
{"created":"2024-05-15 20:57:58","title":"A SN Ia Near a Globular Cluster in the Early-Type Galaxy NGC 5353","abstract":"No progenitor of a Type Ia supernova is known, but in old population early-type galaxies, one may find SN Ia associated with globular clusters, yielding a population age and metallicity. It also provides insight into the formation path and the SN enhancement rate in globular clusters. We sought to find such associations and identified SN 2019ein to be within the ground-based optical positional uncertainty of a globular cluster candidate within the early-type galaxy NGC 5353 at about 30 Mpc distance. We reduced the positional uncertainties by obtaining Hubble Space Telescope images with the Advanced Camera for Surveys, using filters F475W and F814W and obtained in June 2020. We find that the globular cluster candidate has a magnitude, color, and angular extent that are consistent with it being a typical globular cluster. The separation between the globular cluster and SN 2019ein is 0.43'', or 59 pc in projection. The chance occurrence with a random globular cluster is about 3%, favoring but not proving an association. If the SN progenitor originated in the globular cluster, one scenario is that SN 2019ein was previously a double degenerate white dwarf binary that was dynamically ejected from the globular cluster and exploded within 10 Myr; models do not predict this to be common. Another, but less likely scenario is where the progenitor remained bound to the globular cluster, allowing the double degenerate binary to inspiral on a much longer timescale before producing a SN.","sentences":["No progenitor of a Type Ia supernova is known, but in old population early-type galaxies, one may find SN Ia associated with globular clusters, yielding a population age and metallicity.","It also provides insight into the formation path and the SN enhancement rate in globular clusters.","We sought to find such associations and identified SN 2019ein to be within the ground-based optical positional uncertainty of a globular cluster candidate within the early-type galaxy NGC 5353 at about 30 Mpc distance.","We reduced the positional uncertainties by obtaining Hubble Space Telescope images with the Advanced Camera for Surveys, using filters F475W and F814W and obtained in June 2020.","We find that the globular cluster candidate has a magnitude, color, and angular extent that are consistent with it being a typical globular cluster.","The separation between the globular cluster and SN 2019ein is 0.43'', or 59 pc in projection.","The chance occurrence with a random globular cluster is about 3%, favoring but not proving an association.","If the SN progenitor originated in the globular cluster, one scenario is that SN 2019ein was previously a double degenerate white dwarf binary that was dynamically ejected from the globular cluster and exploded within 10 Myr; models do not predict this to be common.","Another, but less likely scenario is where the progenitor remained bound to the globular cluster, allowing the double degenerate binary to inspiral on a much longer timescale before producing a SN."],"url":"http://arxiv.org/abs/2405.09701v1","category":"astro-ph.HE"}
{"created":"2024-05-15 20:47:59","title":"Weakly Supervised Bayesian Shape Modeling from Unsegmented Medical Images","abstract":"Anatomical shape analysis plays a pivotal role in clinical research and hypothesis testing, where the relationship between form and function is paramount. Correspondence-based statistical shape modeling (SSM) facilitates population-level morphometrics but requires a cumbersome, potentially bias-inducing construction pipeline. Recent advancements in deep learning have streamlined this process in inference by providing SSM prediction directly from unsegmented medical images. However, the proposed approaches are fully supervised and require utilizing a traditional SSM construction pipeline to create training data, thus inheriting the associated burdens and limitations. To address these challenges, we introduce a weakly supervised deep learning approach to predict SSM from images using point cloud supervision. Specifically, we propose reducing the supervision associated with the state-of-the-art fully Bayesian variational information bottleneck DeepSSM (BVIB-DeepSSM) model. BVIB-DeepSSM is an effective, principled framework for predicting probabilistic anatomical shapes from images with quantification of both aleatoric and epistemic uncertainties. Whereas the original BVIB-DeepSSM method requires strong supervision in the form of ground truth correspondence points, the proposed approach utilizes weak supervision via point cloud surface representations, which are more readily obtainable. Furthermore, the proposed approach learns correspondence in a completely data-driven manner without prior assumptions about the expected variability in shape cohort. Our experiments demonstrate that this approach yields similar accuracy and uncertainty estimation to the fully supervised scenario while substantially enhancing the feasibility of model training for SSM construction.","sentences":["Anatomical shape analysis plays a pivotal role in clinical research and hypothesis testing, where the relationship between form and function is paramount.","Correspondence-based statistical shape modeling (SSM) facilitates population-level morphometrics but requires a cumbersome, potentially bias-inducing construction pipeline.","Recent advancements in deep learning have streamlined this process in inference by providing SSM prediction directly from unsegmented medical images.","However, the proposed approaches are fully supervised and require utilizing a traditional SSM construction pipeline to create training data, thus inheriting the associated burdens and limitations.","To address these challenges, we introduce a weakly supervised deep learning approach to predict SSM from images using point cloud supervision.","Specifically, we propose reducing the supervision associated with the state-of-the-art fully Bayesian variational information bottleneck DeepSSM (BVIB-DeepSSM) model.","BVIB-DeepSSM is an effective, principled framework for predicting probabilistic anatomical shapes from images with quantification of both aleatoric and epistemic uncertainties.","Whereas the original BVIB-DeepSSM method requires strong supervision in the form of ground truth correspondence points, the proposed approach utilizes weak supervision via point cloud surface representations, which are more readily obtainable.","Furthermore, the proposed approach learns correspondence in a completely data-driven manner without prior assumptions about the expected variability in shape cohort.","Our experiments demonstrate that this approach yields similar accuracy and uncertainty estimation to the fully supervised scenario while substantially enhancing the feasibility of model training for SSM construction."],"url":"http://arxiv.org/abs/2405.09697v1","category":"cs.CV"}
{"created":"2024-05-15 20:38:34","title":"The appeal of small molecules for practical nonlinear optics","abstract":"Small organic molecules with a {\\pi}-conjugated system that consists of only a few double or triple bonds can have significantly smaller optical excitation energies when equipped with donor- and acceptor groups, which raises the quantum limits to the molecular polarizabilities. As a consequence, third-order nonlinear optical polarizabilities become orders of magnitude larger than those of molecules of similar size without donor-acceptor substitution. This enables strong third-order nonlinear optical effects (as high as 1000 times those of silica glass) in dense, amorphous monolithic assemblies. These properties, accompanied by the possibility of deposition from the vapor phase and of electric-field poling at higher temperatures, make the resulting materials competitive towards adding an active nonlinear optical or electro-optic functionality to state-of-the-art integrated photonics platforms.","sentences":["Small organic molecules with a {\\pi}-conjugated system that consists of only a few double or triple bonds can have significantly smaller optical excitation energies when equipped with donor- and acceptor groups, which raises the quantum limits to the molecular polarizabilities.","As a consequence, third-order nonlinear optical polarizabilities become orders of magnitude larger than those of molecules of similar size without donor-acceptor substitution.","This enables strong third-order nonlinear optical effects (as high as 1000 times those of silica glass) in dense, amorphous monolithic assemblies.","These properties, accompanied by the possibility of deposition from the vapor phase and of electric-field poling at higher temperatures, make the resulting materials competitive towards adding an active nonlinear optical or electro-optic functionality to state-of-the-art integrated photonics platforms."],"url":"http://arxiv.org/abs/2405.09690v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 19:57:16","title":"Construction of a Basis of K\u00e4hler differentials for generic plane branches","abstract":"Let ${\\mathcal C}$ be a fixed equisingularity class of irreducible germs of complex analytic plane curves. We compute a basis of the ${\\mathbb C}[[x]]$-module of K\\\"ahler differentials for generic $\\Gamma \\in {\\mathcal C}$, algorithmically, and study its behaviour under blow-up.   As an application, we give an alternative proof for a formula of Genzmer, that provides the dimension of the generic component of the moduli of analytic classes in the equisingularity class of $\\Gamma$.","sentences":["Let ${\\mathcal C}$ be a fixed equisingularity class of irreducible germs of complex analytic plane curves.","We compute a basis of the ${\\mathbb C}[[x]]$-module of K\\\"ahler differentials for generic $\\Gamma \\in {\\mathcal C}$, algorithmically, and study its behaviour under blow-up.   ","As an application, we give an alternative proof for a formula of Genzmer, that provides the dimension of the generic component of the moduli of analytic classes in the equisingularity class of $\\Gamma$."],"url":"http://arxiv.org/abs/2405.09684v1","category":"math.AG"}
{"created":"2024-05-15 19:16:52","title":"Bounds on Fluctuations of First Passage Times for Counting Observables in Classical and Quantum Markov Processes","abstract":"We study the statistics of first passage times (FPTs) of trajectory observables in both classical and quantum Markov processes. We consider specifically the FPTs of counting observables, that is, the times to reach a certain threshold of a trajectory quantity which takes values in the positive integers and is non-decreasing in time. For classical continuous-time Markov chains we rigorously prove: (i) a large deviation principle (LDP) for FPTs, whose corollary is a strong law of large numbers; (ii) a concentration inequality for the FPT of the dynamical activity, which provides an upper bound to the probability of its fluctuations to all orders; and (iii) an upper bound to the probability of the tails for the FPT of an arbitrary counting observable. For quantum Markov processes we rigorously prove: (iv) the quantum version of the LDP, and subsequent strong law of large numbers, for the FPTs of generic counts of quantum jumps; (v) a concentration bound for the the FPT of total number of quantum jumps, which provides an upper bound to the probability of its fluctuations to all orders, together with a similar bound for the sub-class of quantum reset processes which requires less strict irreducibility conditions; and (vi) a tail bound for the FPT of arbitrary counts. Our results allow to extend to FPTs the so-called \"inverse thermodynamic uncertainty relations\" that upper bound the size of fluctuations in time-integrated quantities. We illustrate our results with simple examples.","sentences":["We study the statistics of first passage times (FPTs) of trajectory observables in both classical and quantum Markov processes.","We consider specifically the FPTs of counting observables, that is, the times to reach a certain threshold of a trajectory quantity which takes values in the positive integers and is non-decreasing in time.","For classical continuous-time Markov chains we rigorously prove: (i) a large deviation principle (LDP) for FPTs, whose corollary is a strong law of large numbers; (ii) a concentration inequality for the FPT of the dynamical activity, which provides an upper bound to the probability of its fluctuations to all orders; and (iii) an upper bound to the probability of the tails for the FPT of an arbitrary counting observable.","For quantum Markov processes we rigorously prove: (iv) the quantum version of the LDP, and subsequent strong law of large numbers, for the FPTs of generic counts of quantum jumps; (v) a concentration bound for the the FPT of total number of quantum jumps, which provides an upper bound to the probability of its fluctuations to all orders, together with a similar bound for the sub-class of quantum reset processes which requires less strict irreducibility conditions; and (vi) a tail bound for the FPT of arbitrary counts.","Our results allow to extend to FPTs the so-called \"inverse thermodynamic uncertainty relations\" that upper bound the size of fluctuations in time-integrated quantities.","We illustrate our results with simple examples."],"url":"http://arxiv.org/abs/2405.09669v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 19:04:03","title":"Geometric Analysis of Energy Minimizing Maps: Tangent Maps and Singularities","abstract":"Energy minimizing maps (E.M.M.s) play a central role in the calculus of variations, partial differential equations (PDEs), and geometric analysis. These maps are often embedded into $C^\\infty$ Riemannian manifolds to minimize the Dirichlet Energy functional under certain prescribed conditions. For understanding physical phenomena where systems naturally evolve to states of minimal energy, the geometric analysis of these maps has provided elucidating insights. This paper explores the geometric and analytic properties of energy minimizing maps, tangent maps, and the singular set (sing(u)). We begin by establishing key concepts from analysis, including the Sobolev Space $W^{1,2}$ harmonic functions, and Hausdorff dimension. Significant results about the density function, its upper semi-continuity, and the compactness theorem for tangent maps, and theorems for homogeneous degree zero minimizers are presented. Also analyzed in detail is the singular set (sing(u)), its Hausdorff dimension, and geometric structure. We conclude with open problems that are rich in research potential, and the far-reaching implications if these problems are to be solved.","sentences":["Energy minimizing maps (E.M.M.s) play a central role in the calculus of variations, partial differential equations (PDEs), and geometric analysis.","These maps are often embedded into $C^\\infty$ Riemannian manifolds to minimize the Dirichlet Energy functional under certain prescribed conditions.","For understanding physical phenomena where systems naturally evolve to states of minimal energy, the geometric analysis of these maps has provided elucidating insights.","This paper explores the geometric and analytic properties of energy minimizing maps, tangent maps, and the singular set (sing(u)).","We begin by establishing key concepts from analysis, including the Sobolev Space $W^{1,2}$ harmonic functions, and Hausdorff dimension.","Significant results about the density function, its upper semi-continuity, and the compactness theorem for tangent maps, and theorems for homogeneous degree zero minimizers are presented.","Also analyzed in detail is the singular set (sing(u)), its Hausdorff dimension, and geometric structure.","We conclude with open problems that are rich in research potential, and the far-reaching implications if these problems are to be solved."],"url":"http://arxiv.org/abs/2405.09661v1","category":"math.AP"}
{"created":"2024-05-15 19:03:08","title":"Fast Two-Time-Scale Stochastic Gradient Method with Applications in Reinforcement Learning","abstract":"Two-time-scale optimization is a framework introduced in Zeng et al. (2024) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL). Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator. In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts. The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables. These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm. We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, convexity, Polyak-Lojasiewicz condition, and general non-convexity. These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm. When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art. Finally, we support our theoretical results with numerical simulations in RL.","sentences":["Two-time-scale optimization is a framework introduced in Zeng et al.","(2024) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL).","Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator.","In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts.","The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables.","These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm.","We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, convexity, Polyak-Lojasiewicz condition, and general non-convexity.","These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm.","When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art.","Finally, we support our theoretical results with numerical simulations in RL."],"url":"http://arxiv.org/abs/2405.09660v1","category":"math.OC"}
{"created":"2024-05-15 18:45:43","title":"A remedy to mitigate tensile instability in SPH for simulating large deformation and failure of geomaterials","abstract":"Large deformation analysis in geomechanics plays an important role in understanding the nature of post-failure flows and hazards associated with landslides under different natural calamities. In this study, a SPH framework is proposed for large deformation and failure analysis of geomaterials. An adaptive B-spline kernel function in combination with a pressure zone approach is proposed to counteract the numerical issues associated with tensile instability. The proposed algorithm is validated using a soil cylinder drop problem, and the results are compared with FEM. Finally, the effectiveness of the proposed algorithm in the successful removal of tensile instability and stress noise is demonstrated using the well-studied slope failure simulation of a cohesive soil vertical cut.","sentences":["Large deformation analysis in geomechanics plays an important role in understanding the nature of post-failure flows and hazards associated with landslides under different natural calamities.","In this study, a SPH framework is proposed for large deformation and failure analysis of geomaterials.","An adaptive B-spline kernel function in combination with a pressure zone approach is proposed to counteract the numerical issues associated with tensile instability.","The proposed algorithm is validated using a soil cylinder drop problem, and the results are compared with FEM.","Finally, the effectiveness of the proposed algorithm in the successful removal of tensile instability and stress noise is demonstrated using the well-studied slope failure simulation of a cohesive soil vertical cut."],"url":"http://arxiv.org/abs/2405.09654v1","category":"cs.CE"}
{"created":"2024-05-15 18:13:09","title":"Automatically generating decision-support chatbots based on DMN models","abstract":"How decisions are being made is of utmost importance within organizations. The explicit representation of business logic facilitates identifying and adopting the criteria needed to make a particular decision and drives initiatives to automate repetitive decisions. The last decade has seen a surge in both the adoption of decision modeling standards such as DMN and the use of software tools such as chatbots, which seek to automate parts of the process by interacting with users to guide them in executing tasks or providing information. However, building a chatbot is not a trivial task, as it requires extensive knowledge of the business domain as well as technical knowledge for implementing the tool. In this paper, we build on these two requirements to propose an approach for the automatic generation of fully functional, ready-to-use decisions-support chatbots based on a DNM decision model. With the aim of reducing chatbots development time and to allowing non-technical users the possibility of developing chatbots specific to their domain, all necessary phases for the generation of the chatbot were implemented in the Demabot tool. The evaluation was conducted with potential developers and end users. The results showed that Demabot generates chatbots that are correct and allow for acceptably smooth communication with the user. Furthermore, Demabots's help and customization options are considered useful and correct, while the tool can also help to reduce development time and potential errors.","sentences":["How decisions are being made is of utmost importance within organizations.","The explicit representation of business logic facilitates identifying and adopting the criteria needed to make a particular decision and drives initiatives to automate repetitive decisions.","The last decade has seen a surge in both the adoption of decision modeling standards such as DMN and the use of software tools such as chatbots, which seek to automate parts of the process by interacting with users to guide them in executing tasks or providing information.","However, building a chatbot is not a trivial task, as it requires extensive knowledge of the business domain as well as technical knowledge for implementing the tool.","In this paper, we build on these two requirements to propose an approach for the automatic generation of fully functional, ready-to-use decisions-support chatbots based on a DNM decision model.","With the aim of reducing chatbots development time and to allowing non-technical users the possibility of developing chatbots specific to their domain, all necessary phases for the generation of the chatbot were implemented in the Demabot tool.","The evaluation was conducted with potential developers and end users.","The results showed that Demabot generates chatbots that are correct and allow for acceptably smooth communication with the user.","Furthermore, Demabots's help and customization options are considered useful and correct, while the tool can also help to reduce development time and potential errors."],"url":"http://arxiv.org/abs/2405.09645v1","category":"cs.SE"}
{"created":"2024-05-15 18:00:07","title":"Permutation tests for quantum state identity","abstract":"The quantum analogue of the equality function, known as the quantum state identity problem, is the task of deciding whether $n$ unknown quantum states are equal or unequal, given the promise that all states are either pairwise orthogonal or identical. Under the one-sided error requirement, it is known that the permutation test is optimal for this task, and for two input states this coincides with the well-known Swap test. Until now, the optimal measurement in the general two-sided error regime was unknown. Under more specific promises, the problem can be solved approximately or even optimally with simpler tests, such as the circle test.   This work attempts to capture the underlying structure of (fine-grained formulations of) the quantum state identity problem. Using tools from semi-definite programming and representation theory, we (i) give an optimal test for any input distribution without the one-sided error requirement by writing the problem as an SDP, giving the exact solutions to the primal and dual programs and showing that the two values coincide; (ii) propose a general $G$-test which uses an arbitrary subgroup $G$ of $\\text{S}_n$, giving an analytic expression of the performance of the specific test, and (iii) give an approximation of the permutation test using only a classical permutation and $n-1$ Swap tests.","sentences":["The quantum analogue of the equality function, known as the quantum state identity problem, is the task of deciding whether $n$ unknown quantum states are equal or unequal, given the promise that all states are either pairwise orthogonal or identical.","Under the one-sided error requirement, it is known that the permutation test is optimal for this task, and for two input states this coincides with the well-known Swap test.","Until now, the optimal measurement in the general two-sided error regime was unknown.","Under more specific promises, the problem can be solved approximately or even optimally with simpler tests, such as the circle test.   ","This work attempts to capture the underlying structure of (fine-grained formulations of) the quantum state identity problem.","Using tools from semi-definite programming and representation theory, we (i) give an optimal test for any input distribution without the one-sided error requirement by writing the problem as an SDP, giving the exact solutions to the primal and dual programs and showing that the two values coincide; (ii) propose a general $G$-test which uses an arbitrary subgroup $G$ of $\\text{S}_n$, giving an analytic expression of the performance of the specific test, and (iii) give an approximation of the permutation test using only a classical permutation and $n-1$ Swap tests."],"url":"http://arxiv.org/abs/2405.09626v1","category":"quant-ph"}
{"created":"2024-05-15 18:00:06","title":"The tangled warp of the Milky Way","abstract":"We determine the influence of the Milky Way's warp on the kinematics of stars across the disc, and therefore measure its precession rate and line of nodes under different assumptions. We do this by applying Jeans' first equation to a model of a rigidly precessing warp. The predictions of these models are fit to the average vertical velocities of stars with measured line-of-sight velocities in Gaia DR3 data. We test models in which the warp's line of nodes and precession speed are fixed, and models in which they are allowed to vary linearly with radius. We also test models in which the velocity of stars radially in the disc is included in Jeans' equation. The kinematic data is best fit by models with a line of nodes that is 40 degrees offset from the Sun's Galactic azimuth, significantly leading the line of nodes found from the positions of stars. These models have a warp precession speed of around 13 km/s/kpc in the direction of Galactic rotation, close to other recent estimates. We find that including the velocity of stars radially in the disc in our kinematic model leads to a significantly worse fit to the data, and implausible warp parameters. We conclude that the Milky Way's warp appears to be rapidly precessing, but the structure and kinematics of the warped disc are not consistent within the approximation of a fixed, precessing, warp shape. This implies that the Milky Way's warp is dynamically evolving, which is a challenge to models of the warp's creation, and must be considered in the context of other known disturbances of the disc.","sentences":["We determine the influence of the Milky Way's warp on the kinematics of stars across the disc, and therefore measure its precession rate and line of nodes under different assumptions.","We do this by applying Jeans' first equation to a model of a rigidly precessing warp.","The predictions of these models are fit to the average vertical velocities of stars with measured line-of-sight velocities in Gaia DR3 data.","We test models in which the warp's line of nodes and precession speed are fixed, and models in which they are allowed to vary linearly with radius.","We also test models in which the velocity of stars radially in the disc is included in Jeans' equation.","The kinematic data is best fit by models with a line of nodes that is 40 degrees offset from the Sun's Galactic azimuth, significantly leading the line of nodes found from the positions of stars.","These models have a warp precession speed of around 13 km/s/kpc in the direction of Galactic rotation, close to other recent estimates.","We find that including the velocity of stars radially in the disc in our kinematic model leads to a significantly worse fit to the data, and implausible warp parameters.","We conclude that the Milky Way's warp appears to be rapidly precessing, but the structure and kinematics of the warped disc are not consistent within the approximation of a fixed, precessing, warp shape.","This implies that the Milky Way's warp is dynamically evolving, which is a challenge to models of the warp's creation, and must be considered in the context of other known disturbances of the disc."],"url":"http://arxiv.org/abs/2405.09624v1","category":"astro-ph.GA"}
{"created":"2024-05-15 18:00:04","title":"COSMOS-Web: The Role of Galaxy Interactions and Disk Instabilities in Producing Starbursts at z<4","abstract":"We study of the role of galaxy-galaxy interactions and disk instabilities in producing starburst activity in galaxies out to z=4. For this, we use a sample of 387 galaxies with robust total star formation rate measurements from Herschel, gas masses from ALMA, stellar masses and redshifts from multi-band photometry, and JWST/NIRCam rest-frame optical imaging. Using mass-controlled samples, we find an increased fraction of interacting galaxies in the starburst regime at all redshifts out to z=4. This increase correlates with star formation efficiency (SFE), but not with gas fraction. However, the correlation is weak (and only significant out to z=2), which could be explained by the short duration of SFE increase during interaction. In addition, we find that isolated disk galaxies make up a significant fraction of the starburst population. The fraction of such galaxies with star-forming clumps (\"clumpy disks\") is significantly increased compared to the main-sequence disk population. Furthermore, this fraction directly correlates with SFE. This is direct observational evidence for a long-term increase of SFE maintained due to disk instabilities, contributing to the majority of starburst galaxies in our sample and hence to substantial mass growth in these systems. This result could also be of importance for explaining the growth of the most massive galaxies at z>6.","sentences":["We study of the role of galaxy-galaxy interactions and disk instabilities in producing starburst activity in galaxies out to z=4.","For this, we use a sample of 387 galaxies with robust total star formation rate measurements from Herschel, gas masses from ALMA, stellar masses and redshifts from multi-band photometry, and JWST/NIRCam rest-frame optical imaging.","Using mass-controlled samples, we find an increased fraction of interacting galaxies in the starburst regime at all redshifts out to z=4.","This increase correlates with star formation efficiency (SFE), but not with gas fraction.","However, the correlation is weak (and only significant out to z=2), which could be explained by the short duration of SFE increase during interaction.","In addition, we find that isolated disk galaxies make up a significant fraction of the starburst population.","The fraction of such galaxies with star-forming clumps (\"clumpy disks\") is significantly increased compared to the main-sequence disk population.","Furthermore, this fraction directly correlates with SFE.","This is direct observational evidence for a long-term increase of SFE maintained due to disk instabilities, contributing to the majority of starburst galaxies in our sample and hence to substantial mass growth in these systems.","This result could also be of importance for explaining the growth of the most massive galaxies at z>6."],"url":"http://arxiv.org/abs/2405.09619v1","category":"astro-ph.GA"}
{"created":"2024-05-15 18:00:03","title":"Three-dimensional quantum Hall states as a chiral electromagnetic filter","abstract":"Extensive research has explored the optical properties of topological insulating materials, driven by their inherent stability and potential applications. In this study, we unveil a novel functionality of three-dimensional integer quantum Hall (3D IQH) states as broad-band filters for circularly polarized light, particularly effective in the terahertz (THz) frequency range under realistic system parameters. We also investigate the impact of practical imperfections, demonstrating the resilience of this filtering effect. Our findings reveal that this phenomenon is independent of the microscopic origin of the 3D IQH state, prompting discussions on its feasibility across diverse candidate materials. These results contribute to our understanding of fundamental optical properties and hold promise for practical applications in optical technologies.","sentences":["Extensive research has explored the optical properties of topological insulating materials, driven by their inherent stability and potential applications.","In this study, we unveil a novel functionality of three-dimensional integer quantum Hall (3D IQH) states as broad-band filters for circularly polarized light, particularly effective in the terahertz (THz) frequency range under realistic system parameters.","We also investigate the impact of practical imperfections, demonstrating the resilience of this filtering effect.","Our findings reveal that this phenomenon is independent of the microscopic origin of the 3D IQH state, prompting discussions on its feasibility across diverse candidate materials.","These results contribute to our understanding of fundamental optical properties and hold promise for practical applications in optical technologies."],"url":"http://arxiv.org/abs/2405.09617v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 18:00:02","title":"Characterizing MPS and PEPS Preparable via Measurement and Feedback","abstract":"Preparing long-range entangled states poses significant challenges for near-term quantum devices. It is known that measurement and feedback (MF) can aid this task by allowing the preparation of certain paradigmatic long-range entangled states with only constant circuit depth. Here we systematically explore the structure of states that can be prepared using constant-depth local circuits and a single MF round. Using the framework of tensor networks, the preparability under MF translates to tensor symmetries. We detail the structure of matrix-product states (MPS) and projected entangled-pair states (PEPS) that can be prepared using MF, revealing the coexistence of Clifford-like properties and magic. Furthermore, we provide analytic solutions to states exhibiting MF symmetries akin to the symmetry-protected topological order in one dimension and the topological order in two dimensions, and we discuss their characteristics. Finally, we discuss the analogous implementation of operators via MF, providing a structural theorem that connects to the well-known Clifford teleportation.","sentences":["Preparing long-range entangled states poses significant challenges for near-term quantum devices.","It is known that measurement and feedback (MF) can aid this task by allowing the preparation of certain paradigmatic long-range entangled states with only constant circuit depth.","Here we systematically explore the structure of states that can be prepared using constant-depth local circuits and a single MF round.","Using the framework of tensor networks, the preparability under MF translates to tensor symmetries.","We detail the structure of matrix-product states (MPS) and projected entangled-pair states (PEPS) that can be prepared using MF, revealing the coexistence of Clifford-like properties and magic.","Furthermore, we provide analytic solutions to states exhibiting MF symmetries akin to the symmetry-protected topological order in one dimension and the topological order in two dimensions, and we discuss their characteristics.","Finally, we discuss the analogous implementation of operators via MF, providing a structural theorem that connects to the well-known Clifford teleportation."],"url":"http://arxiv.org/abs/2405.09615v1","category":"quant-ph"}
{"created":"2024-05-15 18:00:01","title":"Computable entanglement cost","abstract":"Quantum information theory is plagued by the problem of regularisations, which require the evaluation of formidable asymptotic quantities. This makes it computationally intractable to gain a precise quantitative understanding of the ultimate efficiency of key operational tasks such as entanglement manipulation. Here we consider the problem of computing the asymptotic entanglement cost of preparing noisy quantum states under quantum operations with positive partial transpose (PPT). A previously claimed solution to this problem is shown to be incorrect. We construct instead an alternative solution in the form of two hierarchies of semi-definite programs that converge to the true asymptotic value of the entanglement cost from above and from below. Our main result establishes that this convergence happens exponentially fast, thus yielding an efficient algorithm that approximates the cost up to an additive error $\\varepsilon$ in time $\\mathrm{poly}\\big(D,\\,\\log(1/\\varepsilon)\\big)$, where $D$ is the underlying Hilbert space dimension. To our knowledge, this is the first time that an asymptotic entanglement measure is shown to be efficiently computable despite no closed-form formula being available.","sentences":["Quantum information theory is plagued by the problem of regularisations, which require the evaluation of formidable asymptotic quantities.","This makes it computationally intractable to gain a precise quantitative understanding of the ultimate efficiency of key operational tasks such as entanglement manipulation.","Here we consider the problem of computing the asymptotic entanglement cost of preparing noisy quantum states under quantum operations with positive partial transpose (PPT).","A previously claimed solution to this problem is shown to be incorrect.","We construct instead an alternative solution in the form of two hierarchies of semi-definite programs that converge to the true asymptotic value of the entanglement cost from above and from below.","Our main result establishes that this convergence happens exponentially fast, thus yielding an efficient algorithm that approximates the cost up to an additive error $\\varepsilon$ in time $\\mathrm{poly}\\big(D,\\,\\log(1/\\varepsilon)\\big)$, where $D$ is the underlying Hilbert space dimension.","To our knowledge, this is the first time that an asymptotic entanglement measure is shown to be efficiently computable despite no closed-form formula being available."],"url":"http://arxiv.org/abs/2405.09613v1","category":"quant-ph"}
{"created":"2024-05-16 12:03:58","title":"Monaural speech enhancement on drone via Adapter based transfer learning","abstract":"Monaural Speech enhancement on drones is challenging because the ego-noise from the rotating motors and propellers leads to extremely low signal-to-noise ratios at onboard microphones. Although recent masking-based deep neural network methods excel in monaural speech enhancement, they struggle in the challenging drone noise scenario. Furthermore, existing drone noise datasets are limited, causing models to overfit. Considering the harmonic nature of drone noise, this paper proposes a frequency domain bottleneck adapter to enable transfer learning. Specifically, the adapter's parameters are trained on drone noise while retaining the parameters of the pre-trained Frequency Recurrent Convolutional Recurrent Network (FRCRN) fixed. Evaluation results demonstrate the proposed method can effectively enhance speech quality. Moreover, it is a more efficient alternative to fine-tuning models for various drone types, which typically requires substantial computational resources.","sentences":["Monaural Speech enhancement on drones is challenging because the ego-noise from the rotating motors and propellers leads to extremely low signal-to-noise ratios at onboard microphones.","Although recent masking-based deep neural network methods excel in monaural speech enhancement, they struggle in the challenging drone noise scenario.","Furthermore, existing drone noise datasets are limited, causing models to overfit.","Considering the harmonic nature of drone noise, this paper proposes a frequency domain bottleneck adapter to enable transfer learning.","Specifically, the adapter's parameters are trained on drone noise while retaining the parameters of the pre-trained Frequency Recurrent Convolutional Recurrent Network (FRCRN) fixed.","Evaluation results demonstrate the proposed method can effectively enhance speech quality.","Moreover, it is a more efficient alternative to fine-tuning models for various drone types, which typically requires substantial computational resources."],"url":"http://arxiv.org/abs/2405.10022v1","category":"eess.AS"}
{"created":"2024-05-16 10:11:18","title":"CatCMA : Stochastic Optimization for Mixed-Category Problems","abstract":"Black-box optimization problems often require simultaneously optimizing different types of variables, such as continuous, integer, and categorical variables. Unlike integer variables, categorical variables do not necessarily have a meaningful order, and the discretization approach of continuous variables does not work well. Although several Bayesian optimization methods can deal with mixed-category black-box optimization (MC-BBO), they suffer from a lack of scalability to high-dimensional problems and internal computational cost. This paper proposes CatCMA, a stochastic optimization method for MC-BBO problems, which employs the joint probability distribution of multivariate Gaussian and categorical distributions as the search distribution. CatCMA updates the parameters of the joint probability distribution in the natural gradient direction. CatCMA also incorporates the acceleration techniques used in the covariance matrix adaptation evolution strategy (CMA-ES) and the stochastic natural gradient method, such as step-size adaptation and learning rate adaptation. In addition, we restrict the ranges of the categorical distribution parameters by margin to prevent premature convergence and analytically derive a promising margin setting. Numerical experiments show that the performance of CatCMA is superior and more robust to problem dimensions compared to state-of-the-art Bayesian optimization algorithms.","sentences":["Black-box optimization problems often require simultaneously optimizing different types of variables, such as continuous, integer, and categorical variables.","Unlike integer variables, categorical variables do not necessarily have a meaningful order, and the discretization approach of continuous variables does not work well.","Although several Bayesian optimization methods can deal with mixed-category black-box optimization (MC-BBO), they suffer from a lack of scalability to high-dimensional problems and internal computational cost.","This paper proposes CatCMA, a stochastic optimization method for MC-BBO problems, which employs the joint probability distribution of multivariate Gaussian and categorical distributions as the search distribution.","CatCMA updates the parameters of the joint probability distribution in the natural gradient direction.","CatCMA also incorporates the acceleration techniques used in the covariance matrix adaptation evolution strategy (CMA-ES) and the stochastic natural gradient method, such as step-size adaptation and learning rate adaptation.","In addition, we restrict the ranges of the categorical distribution parameters by margin to prevent premature convergence and analytically derive a promising margin setting.","Numerical experiments show that the performance of CatCMA is superior and more robust to problem dimensions compared to state-of-the-art Bayesian optimization algorithms."],"url":"http://arxiv.org/abs/2405.09962v1","category":"cs.NE"}
{"created":"2024-05-16 09:43:51","title":"Machine-Learning Enhanced Predictors for Accelerated Convergence of Partitioned Fluid-Structure Interaction Simulations","abstract":"Stable partitioned techniques for simulating unsteady fluid-structure interaction (FSI) are known to be computationally expensive when high added-mass is involved. Multiple coupling strategies have been developed to accelerate these simulations, but often use predictors in the form of simple finite-difference extrapolations. In this work, we propose a non-intrusive data-driven predictor that couples reduced-order models of both the solid and fluid subproblems, providing an initial guess for the nonlinear problem of the next time step calculation. Each reduced order model is composed of a nonlinear encoder-regressor-decoder architecture and is equipped with an adaptive update strategy that adds robustness for extrapolation. In doing so, the proposed methodology leverages physics-based insights from high-fidelity solvers, thus establishing a physics-aware machine learning predictor. Using three strongly coupled FSI examples, this study demonstrates the improved convergence obtained with the new predictor and the overall computational speedup realized compared to classical approaches.","sentences":["Stable partitioned techniques for simulating unsteady fluid-structure interaction (FSI) are known to be computationally expensive when high added-mass is involved.","Multiple coupling strategies have been developed to accelerate these simulations, but often use predictors in the form of simple finite-difference extrapolations.","In this work, we propose a non-intrusive data-driven predictor that couples reduced-order models of both the solid and fluid subproblems, providing an initial guess for the nonlinear problem of the next time step calculation.","Each reduced order model is composed of a nonlinear encoder-regressor-decoder architecture and is equipped with an adaptive update strategy that adds robustness for extrapolation.","In doing so, the proposed methodology leverages physics-based insights from high-fidelity solvers, thus establishing a physics-aware machine learning predictor.","Using three strongly coupled FSI examples, this study demonstrates the improved convergence obtained with the new predictor and the overall computational speedup realized compared to classical approaches."],"url":"http://arxiv.org/abs/2405.09941v1","category":"cs.CE"}
{"created":"2024-05-16 08:45:36","title":"How planets form by pebble accretion V. Silicate rainout delays contraction of sub-Neptunes","abstract":"The characterization of Super-Earth-to-Neptune sized exoplanets relies heavily on our understanding of their formation and evolution. In this study, we link a model of planet formation by pebble accretion to the planets' long-term observational properties by calculating the interior evolution, starting from the dissipation of the protoplanetary disk. We investigate the evolution of the interior structure in 5-20 Earth masses planets, accounting for silicate redistribution caused by convective mixing, rainout (condensation and settling), and mass loss. Specifically, we have followed the fate of the hot silicate vapor that remained in the planet's envelope after planet formation, as the planet cools. We find that disk dissipation is followed by a rapid contraction of the envelope within 10 Myr. Subsequent cooling leads to substantial growth of the planetary core through silicate rainout, accompanied by inflated radii, in comparison to the standard models of planets that formed with core-envelope structure. We examine the dependence of rainout on the planet's envelope mass, distance from its host star, its silicate mass, and the atmospheric opacity. We find that the population of planets formed with polluted envelopes can be roughly divided in three groups, based on the mass of their gas envelopes: bare rocky cores that have shed their envelopes, super-Earth planets with a core-envelope structure, and Neptune-like planets with diluted cores that undergo gradual rainout. For polluted planets formed with envelope masses below 0.4 Earth mass, we anticipate that the inflation of the planet's radius caused by rainout will enhance mass loss by a factor of 2-8 compared to planets with non-polluted envelopes. Our model provides an explanation for bridging the gap between the predicted composition gradients in massive planets and the core-envelope structure in smaller planets.","sentences":["The characterization of Super-Earth-to-Neptune sized exoplanets relies heavily on our understanding of their formation and evolution.","In this study, we link a model of planet formation by pebble accretion to the planets' long-term observational properties by calculating the interior evolution, starting from the dissipation of the protoplanetary disk.","We investigate the evolution of the interior structure in 5-20 Earth masses planets, accounting for silicate redistribution caused by convective mixing, rainout (condensation and settling), and mass loss.","Specifically, we have followed the fate of the hot silicate vapor that remained in the planet's envelope after planet formation, as the planet cools.","We find that disk dissipation is followed by a rapid contraction of the envelope within 10 Myr.","Subsequent cooling leads to substantial growth of the planetary core through silicate rainout, accompanied by inflated radii, in comparison to the standard models of planets that formed with core-envelope structure.","We examine the dependence of rainout on the planet's envelope mass, distance from its host star, its silicate mass, and the atmospheric opacity.","We find that the population of planets formed with polluted envelopes can be roughly divided in three groups, based on the mass of their gas envelopes: bare rocky cores that have shed their envelopes, super-Earth planets with a core-envelope structure, and Neptune-like planets with diluted cores that undergo gradual rainout.","For polluted planets formed with envelope masses below 0.4 Earth mass, we anticipate that the inflation of the planet's radius caused by rainout will enhance mass loss by a factor of 2-8 compared to planets with non-polluted envelopes.","Our model provides an explanation for bridging the gap between the predicted composition gradients in massive planets and the core-envelope structure in smaller planets."],"url":"http://arxiv.org/abs/2405.09900v1","category":"astro-ph.EP"}
{"created":"2024-05-16 06:38:28","title":"Simultaneous Identification of Sparse Structures and Communities in Heterogeneous Graphical Models","abstract":"Exploring and detecting community structures hold significant importance in genetics, social sciences, neuroscience, and finance. Especially in graphical models, community detection can encourage the exploration of sets of variables with group-like properties. In this paper, within the framework of Gaussian graphical models, we introduce a novel decomposition of the underlying graphical structure into a sparse part and low-rank diagonal blocks (non-overlapped communities). We illustrate the significance of this decomposition through two modeling perspectives and propose a three-stage estimation procedure with a fast and efficient algorithm for the identification of the sparse structure and communities. Also on the theoretical front, we establish conditions for local identifiability and extend the traditional irrepresentability condition to an adaptive form by constructing an effective norm, which ensures the consistency of model selection for the adaptive $\\ell_1$ penalized estimator in the second stage. Moreover, we also provide the clustering error bound for the K-means procedure in the third stage. Extensive numerical experiments are conducted to demonstrate the superiority of the proposed method over existing approaches in estimating graph structures. Furthermore, we apply our method to the stock return data, revealing its capability to accurately identify non-overlapped community structures.","sentences":["Exploring and detecting community structures hold significant importance in genetics, social sciences, neuroscience, and finance.","Especially in graphical models, community detection can encourage the exploration of sets of variables with group-like properties.","In this paper, within the framework of Gaussian graphical models, we introduce a novel decomposition of the underlying graphical structure into a sparse part and low-rank diagonal blocks (non-overlapped communities).","We illustrate the significance of this decomposition through two modeling perspectives and propose a three-stage estimation procedure with a fast and efficient algorithm for the identification of the sparse structure and communities.","Also on the theoretical front, we establish conditions for local identifiability and extend the traditional irrepresentability condition to an adaptive form by constructing an effective norm, which ensures the consistency of model selection for the adaptive $\\ell_1$ penalized estimator in the second stage.","Moreover, we also provide the clustering error bound for the K-means procedure in the third stage.","Extensive numerical experiments are conducted to demonstrate the superiority of the proposed method over existing approaches in estimating graph structures.","Furthermore, we apply our method to the stock return data, revealing its capability to accurately identify non-overlapped community structures."],"url":"http://arxiv.org/abs/2405.09841v1","category":"stat.ML"}
{"created":"2024-05-16 05:37:06","title":"Densely Distilling Cumulative Knowledge for Continual Learning","abstract":"Continual learning, involving sequential training on diverse tasks, often faces catastrophic forgetting. While knowledge distillation-based approaches exhibit notable success in preventing forgetting, we pinpoint a limitation in their ability to distill the cumulative knowledge of all the previous tasks. To remedy this, we propose Dense Knowledge Distillation (DKD). DKD uses a task pool to track the model's capabilities. It partitions the output logits of the model into dense groups, each corresponding to a task in the task pool. It then distills all tasks' knowledge using all groups. However, using all the groups can be computationally expensive, we also suggest random group selection in each optimization step. Moreover, we propose an adaptive weighting scheme, which balances the learning of new classes and the retention of old classes, based on the count and similarity of the classes. Our DKD outperforms recent state-of-the-art baselines across diverse benchmarks and scenarios. Empirical analysis underscores DKD's ability to enhance model stability, promote flatter minima for improved generalization, and remains robust across various memory budgets and task orders. Moreover, it seamlessly integrates with other CL methods to boost performance and proves versatile in offline scenarios like model compression.","sentences":["Continual learning, involving sequential training on diverse tasks, often faces catastrophic forgetting.","While knowledge distillation-based approaches exhibit notable success in preventing forgetting, we pinpoint a limitation in their ability to distill the cumulative knowledge of all the previous tasks.","To remedy this, we propose Dense Knowledge Distillation (DKD).","DKD uses a task pool to track the model's capabilities.","It partitions the output logits of the model into dense groups, each corresponding to a task in the task pool.","It then distills all tasks' knowledge using all groups.","However, using all the groups can be computationally expensive, we also suggest random group selection in each optimization step.","Moreover, we propose an adaptive weighting scheme, which balances the learning of new classes and the retention of old classes, based on the count and similarity of the classes.","Our DKD outperforms recent state-of-the-art baselines across diverse benchmarks and scenarios.","Empirical analysis underscores DKD's ability to enhance model stability, promote flatter minima for improved generalization, and remains robust across various memory budgets and task orders.","Moreover, it seamlessly integrates with other CL methods to boost performance and proves versatile in offline scenarios like model compression."],"url":"http://arxiv.org/abs/2405.09820v1","category":"cs.LG"}
{"created":"2024-05-16 04:13:17","title":"Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution","abstract":"In this paper, we dive into the reliability concerns of Integrated Gradients (IG), a prevalent feature attribution method for black-box deep learning models. We particularly address two predominant challenges associated with IG: the generation of noisy feature visualizations for vision models and the vulnerability to adversarial attributional attacks. Our approach involves an adaptation of path-based feature attribution, aligning the path of attribution more closely to the intrinsic geometry of the data manifold. Our experiments utilise deep generative models applied to several real-world image datasets. They demonstrate that IG along the geodesics conforms to the curved geometry of the Riemannian data manifold, generating more perceptually intuitive explanations and, subsequently, substantially increasing robustness to targeted attributional attacks.","sentences":["In this paper, we dive into the reliability concerns of Integrated Gradients (IG), a prevalent feature attribution method for black-box deep learning models.","We particularly address two predominant challenges associated with IG: the generation of noisy feature visualizations for vision models and the vulnerability to adversarial attributional attacks.","Our approach involves an adaptation of path-based feature attribution, aligning the path of attribution more closely to the intrinsic geometry of the data manifold.","Our experiments utilise deep generative models applied to several real-world image datasets.","They demonstrate that IG along the geodesics conforms to the curved geometry of the Riemannian data manifold, generating more perceptually intuitive explanations and, subsequently, substantially increasing robustness to targeted attributional attacks."],"url":"http://arxiv.org/abs/2405.09800v1","category":"cs.LG"}
{"created":"2024-05-16 03:19:52","title":"IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency","abstract":"Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries can maliciously trigger model misclassifications by implanting a hidden backdoor during model training. This paper proposes a simple yet effective input-level backdoor detection (dubbed IBD-PSC) as a 'firewall' to filter out malicious testing images. Our method is motivated by an intriguing phenomenon, i.e., parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples are significantly more consistent than those of benign ones when amplifying model parameters. In particular, we provide theoretical analysis to safeguard the foundations of the PSC phenomenon. We also design an adaptive method to select BN layers to scale up for effective detection. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our IBD-PSC method and its resistance to adaptive attacks.","sentences":["Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries can maliciously trigger model misclassifications by implanting a hidden backdoor during model training.","This paper proposes a simple yet effective input-level backdoor detection (dubbed IBD-PSC) as a 'firewall' to filter out malicious testing images.","Our method is motivated by an intriguing phenomenon, i.e., parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples are significantly more consistent than those of benign ones when amplifying model parameters.","In particular, we provide theoretical analysis to safeguard the foundations of the PSC phenomenon.","We also design an adaptive method to select BN layers to scale up for effective detection.","Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our IBD-PSC method and its resistance to adaptive attacks."],"url":"http://arxiv.org/abs/2405.09786v1","category":"cs.LG"}
{"created":"2024-05-16 02:46:19","title":"Rethinking Barely-Supervised Segmentation from an Unsupervised Domain Adaptation Perspective","abstract":"This paper investigates an extremely challenging problem, barely-supervised medical image segmentation (BSS), where the training dataset comprises limited labeled data with only single-slice annotations and numerous unlabeled images. Currently, state-of-the-art (SOTA) BSS methods utilize a registration-based paradigm, depending on image registration to propagate single-slice annotations into volumetric pseudo labels for constructing a complete labeled set. However, this paradigm has a critical limitation: the pseudo labels generated by image registration are unreliable and noisy. Motivated by this, we propose a new perspective: training a model using only single-annotated slices as the labeled set without relying on image registration. To this end, we formulate BSS as an unsupervised domain adaptation (UDA) problem. Specifically, we first design a novel noise-free labeled data construction algorithm (NFC) for slice-to-volume labeled data synthesis, which may result in a side effect: domain shifts between the synthesized images and the original images. Then, a frequency and spatial mix-up strategy (FSX) is further introduced to mitigate the domain shifts for UDA. Extensive experiments demonstrate that our method provides a promising alternative for BSS. Remarkably, the proposed method with only one labeled slice achieves an 80.77% dice score on left atrial segmentation, outperforming the SOTA by 61.28%. The code will be released upon the publication of this paper.","sentences":["This paper investigates an extremely challenging problem, barely-supervised medical image segmentation (BSS), where the training dataset comprises limited labeled data with only single-slice annotations and numerous unlabeled images.","Currently, state-of-the-art (SOTA) BSS methods utilize a registration-based paradigm, depending on image registration to propagate single-slice annotations into volumetric pseudo labels for constructing a complete labeled set.","However, this paradigm has a critical limitation: the pseudo labels generated by image registration are unreliable and noisy.","Motivated by this, we propose a new perspective: training a model using only single-annotated slices as the labeled set without relying on image registration.","To this end, we formulate BSS as an unsupervised domain adaptation (UDA) problem.","Specifically, we first design a novel noise-free labeled data construction algorithm (NFC) for slice-to-volume labeled data synthesis, which may result in a side effect: domain shifts between the synthesized images and the original images.","Then, a frequency and spatial mix-up strategy (FSX) is further introduced to mitigate the domain shifts for UDA.","Extensive experiments demonstrate that our method provides a promising alternative for BSS.","Remarkably, the proposed method with only one labeled slice achieves an 80.77% dice score on left atrial segmentation, outperforming the SOTA by 61.28%.","The code will be released upon the publication of this paper."],"url":"http://arxiv.org/abs/2405.09777v1","category":"cs.CV"}
{"created":"2024-05-16 01:25:30","title":"Time-Varying Graph Signal Recovery Using High-Order Smoothness and Adaptive Low-rankness","abstract":"Time-varying graph signal recovery has been widely used in many applications, including climate change, environmental hazard monitoring, and epidemic studies. It is crucial to choose appropriate regularizations to describe the characteristics of the underlying signals, such as the smoothness of the signal over the graph domain and the low-rank structure of the spatial-temporal signal modeled in a matrix form. As one of the most popular options, the graph Laplacian is commonly adopted in designing graph regularizations for reconstructing signals defined on a graph from partially observed data. In this work, we propose a time-varying graph signal recovery method based on the high-order Sobolev smoothness and an error-function weighted nuclear norm regularization to enforce the low-rankness. Two efficient algorithms based on the alternating direction method of multipliers and iterative reweighting are proposed, and convergence of one algorithm is shown in detail. We conduct various numerical experiments on synthetic and real-world data sets to demonstrate the proposed method's effectiveness compared to the state-of-the-art in graph signal recovery.","sentences":["Time-varying graph signal recovery has been widely used in many applications, including climate change, environmental hazard monitoring, and epidemic studies.","It is crucial to choose appropriate regularizations to describe the characteristics of the underlying signals, such as the smoothness of the signal over the graph domain and the low-rank structure of the spatial-temporal signal modeled in a matrix form.","As one of the most popular options, the graph Laplacian is commonly adopted in designing graph regularizations for reconstructing signals defined on a graph from partially observed data.","In this work, we propose a time-varying graph signal recovery method based on the high-order Sobolev smoothness and an error-function weighted nuclear norm regularization to enforce the low-rankness.","Two efficient algorithms based on the alternating direction method of multipliers and iterative reweighting are proposed, and convergence of one algorithm is shown in detail.","We conduct various numerical experiments on synthetic and real-world data sets to demonstrate the proposed method's effectiveness compared to the state-of-the-art in graph signal recovery."],"url":"http://arxiv.org/abs/2405.09752v1","category":"eess.SP"}
{"created":"2024-05-15 20:48:52","title":"A Deep Joint Source-Channel Coding Scheme for Hybrid Mobile Multi-hop Networks","abstract":"Efficient data transmission across mobile multi-hop networks that connect edge devices to core servers presents significant challenges, particularly due to the variability in link qualities between wireless and wired segments. This variability necessitates a robust transmission scheme that transcends the limitations of existing deep joint source-channel coding (DeepJSCC) strategies, which often struggle at the intersection of analog and digital methods. Addressing this need, this paper introduces a novel hybrid DeepJSCC framework, h-DJSCC, tailored for effective image transmission from edge devices through a network architecture that includes initial wireless transmission followed by multiple wired hops. Our approach harnesses the strengths of DeepJSCC for the initial, variable-quality wireless link to avoid the cliff effect inherent in purely digital schemes. For the subsequent wired hops, which feature more stable and high-capacity connections, we implement digital compression and forwarding techniques to prevent noise accumulation. This dual-mode strategy is adaptable even in scenarios with limited knowledge of the image distribution, enhancing the framework's robustness and utility. Extensive numerical simulations demonstrate that our hybrid solution outperforms traditional fully digital approaches by effectively managing transitions between different network segments and optimizing for variable signal-to-noise ratios (SNRs). We also introduce a fully adaptive h-DJSCC architecture capable of adjusting to different network conditions and achieving diverse rate-distortion objectives, thereby reducing the memory requirements on network nodes.","sentences":["Efficient data transmission across mobile multi-hop networks that connect edge devices to core servers presents significant challenges, particularly due to the variability in link qualities between wireless and wired segments.","This variability necessitates a robust transmission scheme that transcends the limitations of existing deep joint source-channel coding (DeepJSCC) strategies, which often struggle at the intersection of analog and digital methods.","Addressing this need, this paper introduces a novel hybrid DeepJSCC framework, h-DJSCC, tailored for effective image transmission from edge devices through a network architecture that includes initial wireless transmission followed by multiple wired hops.","Our approach harnesses the strengths of DeepJSCC for the initial, variable-quality wireless link to avoid the cliff effect inherent in purely digital schemes.","For the subsequent wired hops, which feature more stable and high-capacity connections, we implement digital compression and forwarding techniques to prevent noise accumulation.","This dual-mode strategy is adaptable even in scenarios with limited knowledge of the image distribution, enhancing the framework's robustness and utility.","Extensive numerical simulations demonstrate that our hybrid solution outperforms traditional fully digital approaches by effectively managing transitions between different network segments and optimizing for variable signal-to-noise ratios (SNRs).","We also introduce a fully adaptive h-DJSCC architecture capable of adjusting to different network conditions and achieving diverse rate-distortion objectives, thereby reducing the memory requirements on network nodes."],"url":"http://arxiv.org/abs/2405.09698v1","category":"eess.SP"}
{"created":"2024-05-15 20:46:12","title":"Phase holonomy underlies puzzling temporal patterns in Kuramoto models with two sub-populations","abstract":"We present a geometric investigation of curious dynamical behaviors previously reported in Kuramoto models with two sub-populations. Our study demonstrates that chimeras and traveling waves in such models are associated with the birth of geometric phase. Although manifestations of geometric phase are frequent in various fields of Physics, this is the first time (to our best knowledge) that such a phenomenon is exposed in ensembles of Kuramoto oscillators or, more broadly, in complex systems.","sentences":["We present a geometric investigation of curious dynamical behaviors previously reported in Kuramoto models with two sub-populations.","Our study demonstrates that chimeras and traveling waves in such models are associated with the birth of geometric phase.","Although manifestations of geometric phase are frequent in various fields of Physics, this is the first time (to our best knowledge) that such a phenomenon is exposed in ensembles of Kuramoto oscillators or, more broadly, in complex systems."],"url":"http://arxiv.org/abs/2405.09696v1","category":"nlin.AO"}
{"created":"2024-05-15 19:53:52","title":"Synth-to-Real Unsupervised Domain Adaptation for Instance Segmentation","abstract":"Unsupervised Domain Adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain. While UDA methods for synthetic to real-world domains (synth-to-real) show remarkable performance in tasks such as semantic segmentation and object detection, very few were proposed for the instance segmentation task. In this paper, we introduce UDA4Inst, a model of synth-to-real UDA for instance segmentation in autonomous driving. We propose a novel cross-domain bidirectional data mixing method at the instance level to fully leverage the data from both source and target domains. Rare-class balancing and category module training are also employed to further improve the performance. It is worth noting that we are the first to demonstrate results on two new synth-to-real instance segmentation benchmarks, with 39.0 mAP on UrbanSyn->Cityscapes and 35.7 mAP on Synscapes->Cityscapes. UDA4Inst also achieves the state-of-the-art result on SYNTHIA->Cityscapes with 31.3 mAP, +15.6 higher than the latest approach. Our code will be released.","sentences":["Unsupervised Domain Adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain.","While UDA methods for synthetic to real-world domains (synth-to-real) show remarkable performance in tasks such as semantic segmentation and object detection, very few were proposed for the instance segmentation task.","In this paper, we introduce UDA4Inst, a model of synth-to-real UDA for instance segmentation in autonomous driving.","We propose a novel cross-domain bidirectional data mixing method at the instance level to fully leverage the data from both source and target domains.","Rare-class balancing and category module training are also employed to further improve the performance.","It is worth noting that we are the first to demonstrate results on two new synth-to-real instance segmentation benchmarks, with 39.0 mAP on UrbanSyn->Cityscapes and 35.7 mAP on Synscapes->Cityscapes.","UDA4Inst also achieves the state-of-the-art result on SYNTHIA->Cityscapes with 31.3 mAP, +15.6 higher than the latest approach.","Our code will be released."],"url":"http://arxiv.org/abs/2405.09682v1","category":"cs.CV"}
{"created":"2024-05-15 19:23:26","title":"Time-connected phase slips in current-driven two-band superconducting wires","abstract":"We study quasi-one dimensional wires of two-band superconductors driven by an electrical current. We find that the onset of dissipation can occur with the nucleation of time-connected phase slips (t-PS). The topological structure of the t-PS consists of two phase slips (one in each order parameter) separated in time and connected via an interband vortex string along the time direction. This shows as a two-peak structure in voltage vs. time. We discuss the conditions for observing t-PS, depending on the interband coupling strength and the relaxation time scales for each order parameter.","sentences":["We study quasi-one dimensional wires of two-band superconductors driven by an electrical current.","We find that the onset of dissipation can occur with the nucleation of time-connected phase slips (t-PS).","The topological structure of the t-PS consists of two phase slips (one in each order parameter) separated in time and connected via an interband vortex string along the time direction.","This shows as a two-peak structure in voltage vs. time.","We discuss the conditions for observing t-PS, depending on the interband coupling strength and the relaxation time scales for each order parameter."],"url":"http://arxiv.org/abs/2405.09671v1","category":"cond-mat.supr-con"}
{"created":"2024-05-15 18:08:30","title":"Entanglement dynamics of two modes coupled through a dissipative movable mirror in an optomechanical system","abstract":"Nonclassical states are an important class of states in quantum mechanics, especially for their applications in quantum information theory. Optomechanical systems serve as an invaluable platform for exploring and harnessing these states test bed for search and application of such states. In this study, we focused on the studied the mirror-in-the-middle optomechanical system. We observed that in the absence of losses, a coherent state evolves into a entangled one. Furthermore, we demonstrate that the generation of a Schr\\\"odinger-cat state depends on the optomechanical coupling. We exactly solved the Gorini-Kossalokowinki-Sudarshan-Lindblad master equation, highlighting the direct influence of the reservoir on the dynamics when mechanical losses are considered. We later discussed vacuum one-photon superposition states to obtain exact entanglement dynamics using concurrence as a quantifier. Our results show that the overall entanglement of the system is attenuated by mechanical losses in the mirror.","sentences":["Nonclassical states are an important class of states in quantum mechanics, especially for their applications in quantum information theory.","Optomechanical systems serve as an invaluable platform for exploring and harnessing these states test bed for search and application of such states.","In this study, we focused on the studied the mirror-in-the-middle optomechanical system.","We observed that in the absence of losses, a coherent state evolves into a entangled one.","Furthermore, we demonstrate that the generation of a Schr\\\"odinger-cat state depends on the optomechanical coupling.","We exactly solved the Gorini-Kossalokowinki-Sudarshan-Lindblad master equation, highlighting the direct influence of the reservoir on the dynamics when mechanical losses are considered.","We later discussed vacuum one-photon superposition states to obtain exact entanglement dynamics using concurrence as a quantifier.","Our results show that the overall entanglement of the system is attenuated by mechanical losses in the mirror."],"url":"http://arxiv.org/abs/2405.09641v1","category":"quant-ph"}
{"created":"2024-05-15 18:07:19","title":"Open Effective Field Theories for cosmology","abstract":"When energy is not conserved, imprints of new physics on observable cosmology might not follow the rules of local effective actions. By capturing dissipative and diffusive effects, open effective field theories account for the possibly non-Hamiltonian evolution of cosmological inhomogeneities interacting with an unspecified environment. In this proceeding, we briefly discuss recent progress made towards their implementation in primordial cosmology. Our approach recovers the usual effective field theory of inflation in a certain limit and extends it to account for local dissipation and noises. Non-Gaussianities are generated that peak in the equilateral configuration for large dissipation and in the folded configurations for small dissipation. The construction provides an embedding for local dissipative models of inflation and a framework to study quantum information aspects of the inflationary models.","sentences":["When energy is not conserved, imprints of new physics on observable cosmology might not follow the rules of local effective actions.","By capturing dissipative and diffusive effects, open effective field theories account for the possibly non-Hamiltonian evolution of cosmological inhomogeneities interacting with an unspecified environment.","In this proceeding, we briefly discuss recent progress made towards their implementation in primordial cosmology.","Our approach recovers the usual effective field theory of inflation in a certain limit and extends it to account for local dissipation and noises.","Non-Gaussianities are generated that peak in the equilateral configuration for large dissipation and in the folded configurations for small dissipation.","The construction provides an embedding for local dissipative models of inflation and a framework to study quantum information aspects of the inflationary models."],"url":"http://arxiv.org/abs/2405.09639v1","category":"astro-ph.CO"}
{"created":"2024-05-15 18:01:15","title":"Quantum switch instabilities with an open control","abstract":"The superposition of causal order shows promise in various quantum technologies. However, the fragility of quantum systems arising from environmental interactions, leading to dissipative behavior and irreversibility, demands a deeper understanding of the possible instabilities in the coherent control of causal orders. In this work, we employ a collisional model to investigate the impact of an open control system on the generation of interference between two causal orders. We present the environmental instabilities for the switch of two arbitrary quantum operations and examine the influence of environmental temperature on each potential outcome of control post-selection. Additionally, we explore how environmental instabilities affect protocol performance, including switching between mutually unbiased measurement observables and refrigeration powered by causal order superposition, providing insights into broader implications.","sentences":["The superposition of causal order shows promise in various quantum technologies.","However, the fragility of quantum systems arising from environmental interactions, leading to dissipative behavior and irreversibility, demands a deeper understanding of the possible instabilities in the coherent control of causal orders.","In this work, we employ a collisional model to investigate the impact of an open control system on the generation of interference between two causal orders.","We present the environmental instabilities for the switch of two arbitrary quantum operations and examine the influence of environmental temperature on each potential outcome of control post-selection.","Additionally, we explore how environmental instabilities affect protocol performance, including switching between mutually unbiased measurement observables and refrigeration powered by causal order superposition, providing insights into broader implications."],"url":"http://arxiv.org/abs/2405.09631v1","category":"quant-ph"}
{"created":"2024-05-15 18:00:01","title":"Imprint of 'local opacity' effect in gamma-ray spectrum of blazar jet","abstract":"Relativistic jets from accreting supermassive black holes at cosmological distances can be powerful emitters of $\\gamma$-rays. However, the precise mechanisms and locations responsible for the dissipation of energy within these jets, leading to observable $\\gamma$-ray radiation, remain elusive. We detect evidence for an intrinsic absorption feature in the $\\gamma$-ray spectrum at energies exceeding $10\\,$GeV, presumably due to the photon-photon pair production of $\\gamma$-rays with low ionization lines at the outer edge of Broad-line region (BLR), during the high-flux state of the flat-spectrum radio quasar PKS 1424$-$418. The feature can be discriminated from the turnover at higher energies resulting from $\\gamma$-ray absorption in the extragalactic background light. It is absent in the low-flux states supporting the interpretation that powerful dissipation events within or at the edge of the BLR evolve into fainter $\\gamma$-ray emitting zones outside the BLR, possibly associated with the moving VLBI radio knots. The inferred location of $\\gamma$-ray emission zone is consistent with the observed variability time scale of the brightest flare, provided that the flare is attributed to external Compton scattering with BLR photons.","sentences":["Relativistic jets from accreting supermassive black holes at cosmological distances can be powerful emitters of $\\gamma$-rays.","However, the precise mechanisms and locations responsible for the dissipation of energy within these jets, leading to observable $\\gamma$-ray radiation, remain elusive.","We detect evidence for an intrinsic absorption feature in the $\\gamma$-ray spectrum at energies exceeding $10\\,$GeV, presumably due to the photon-photon pair production of $\\gamma$-rays with low ionization lines at the outer edge of Broad-line region (BLR), during the high-flux state of the flat-spectrum radio quasar PKS 1424$-$418.","The feature can be discriminated from the turnover at higher energies resulting from $\\gamma$-ray absorption in the extragalactic background light.","It is absent in the low-flux states supporting the interpretation that powerful dissipation events within or at the edge of the BLR evolve into fainter $\\gamma$-ray emitting zones outside the BLR, possibly associated with the moving VLBI radio knots.","The inferred location of $\\gamma$-ray emission zone is consistent with the observed variability time scale of the brightest flare, provided that the flare is attributed to external Compton scattering with BLR photons."],"url":"http://arxiv.org/abs/2405.09612v1","category":"astro-ph.HE"}
{"created":"2024-05-15 09:24:36","title":"BxC Toolkit: Generating Tailored Turbulent 3D Magnetic Fields","abstract":"Turbulent states are ubiquitous in plasmas and the understanding of turbulence is fundamental in modern astrophysics. Numerical simulations, which are the state-of-the-art approach to the study of turbulence, require substantial computing resources. Recently, attention shifted to methods for generating synthetic turbulent magnetic fields, affordably creating fields with parameter-controlled characteristic features of turbulence. In this context, the BxC toolkit was developed and validated against direct numerical simulations (DNS) of isotropic turbulent magnetic fields. Here, we demonstrate novel extensions of BxC to generate realistic turbulent magnetic fields in a fast, controlled, geometric approach. First, we perform a parameter study to determine quantitative relations between the BxC input parameters and desired characteristic features of the turbulent power spectrum, such as the extent of the inertial range, its spectral slope, and the injection and dissipation scale. Second, we introduce in the model a set of structured background magnetic fields B0, as a natural and more realistic extension to the purely isotropic turbulent fields. Third, we extend the model to include anisotropic turbulence properties in the generated fields. With all these extensions combined, our tool can quickly generate any desired structured magnetic field with controlled, anisotropic turbulent fluctuations, faster by orders of magnitude with respect to DNSs. These can be used, e.g., to provide initial conditions for DNS simulations or easily generate synthetic data for many astrophysical settings, all at otherwise unaffordable resolutions.","sentences":["Turbulent states are ubiquitous in plasmas and the understanding of turbulence is fundamental in modern astrophysics.","Numerical simulations, which are the state-of-the-art approach to the study of turbulence, require substantial computing resources.","Recently, attention shifted to methods for generating synthetic turbulent magnetic fields, affordably creating fields with parameter-controlled characteristic features of turbulence.","In this context, the BxC toolkit was developed and validated against direct numerical simulations (DNS) of isotropic turbulent magnetic fields.","Here, we demonstrate novel extensions of BxC to generate realistic turbulent magnetic fields in a fast, controlled, geometric approach.","First, we perform a parameter study to determine quantitative relations between the BxC input parameters and desired characteristic features of the turbulent power spectrum, such as the extent of the inertial range, its spectral slope, and the injection and dissipation scale.","Second, we introduce in the model a set of structured background magnetic fields B0, as a natural and more realistic extension to the purely isotropic turbulent fields.","Third, we extend the model to include anisotropic turbulence properties in the generated fields.","With all these extensions combined, our tool can quickly generate any desired structured magnetic field with controlled, anisotropic turbulent fluctuations, faster by orders of magnitude with respect to DNSs.","These can be used, e.g., to provide initial conditions for DNS simulations or easily generate synthetic data for many astrophysical settings, all at otherwise unaffordable resolutions."],"url":"http://arxiv.org/abs/2405.09587v1","category":"physics.plasm-ph"}
{"created":"2024-05-15 02:34:06","title":"AD-Aligning: Emulating Human-like Generalization for Cognitive Domain Adaptation in Deep Learning","abstract":"Domain adaptation is pivotal for enabling deep learning models to generalize across diverse domains, a task complicated by variations in presentation and cognitive nuances. In this paper, we introduce AD-Aligning, a novel approach that combines adversarial training with source-target domain alignment to enhance generalization capabilities. By pretraining with Coral loss and standard loss, AD-Aligning aligns target domain statistics with those of the pretrained encoder, preserving robustness while accommodating domain shifts. Through extensive experiments on diverse datasets and domain shift scenarios, including noise-induced shifts and cognitive domain adaptation tasks, we demonstrate AD-Aligning's superior performance compared to existing methods such as Deep Coral and ADDA. Our findings highlight AD-Aligning's ability to emulate the nuanced cognitive processes inherent in human perception, making it a promising solution for real-world applications requiring adaptable and robust domain adaptation strategies.","sentences":["Domain adaptation is pivotal for enabling deep learning models to generalize across diverse domains, a task complicated by variations in presentation and cognitive nuances.","In this paper, we introduce AD-Aligning, a novel approach that combines adversarial training with source-target domain alignment to enhance generalization capabilities.","By pretraining with Coral loss and standard loss, AD-Aligning aligns target domain statistics with those of the pretrained encoder, preserving robustness while accommodating domain shifts.","Through extensive experiments on diverse datasets and domain shift scenarios, including noise-induced shifts and cognitive domain adaptation tasks, we demonstrate AD-Aligning's superior performance compared to existing methods such as Deep Coral and ADDA.","Our findings highlight AD-Aligning's ability to emulate the nuanced cognitive processes inherent in human perception, making it a promising solution for real-world applications requiring adaptable and robust domain adaptation strategies."],"url":"http://arxiv.org/abs/2405.09582v1","category":"cs.CV"}
{"created":"2024-05-14 17:59:57","title":"The RoboDrive Challenge: Drive Anytime Anywhere in Any Condition","abstract":"In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles. Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems. The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities. Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances. This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers. The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness. These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability. Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios. Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of driving perception systems. This challenge has set a new benchmark in the field, providing a rich repository of techniques expected to guide future research in this field.","sentences":["In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles.","Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems.","The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities.","Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances.","This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers.","The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness.","These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability.","Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios.","Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of driving perception systems.","This challenge has set a new benchmark in the field, providing a rich repository of techniques expected to guide future research in this field."],"url":"http://arxiv.org/abs/2405.08816v1","category":"cs.CV"}
{"created":"2024-05-14 17:38:17","title":"Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","abstract":"This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.","sentences":["This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling.","Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically.","We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters.","We also provide an ablation study of KAN-specific parameters impact on performance.","The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics."],"url":"http://arxiv.org/abs/2405.08790v1","category":"eess.SP"}
{"created":"2024-05-14 17:35:27","title":"Incorporating Clinical Guidelines through Adapting Multi-modal Large Language Model for Prostate Cancer PI-RADS Scoring","abstract":"The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the diagnosis of clinically significant prostate cancer through MRI imaging. Current deep learning-based PI-RADS scoring methods often lack the incorporation of essential PI-RADS clinical guidelines~(PICG) utilized by radiologists, potentially compromising scoring accuracy. This paper introduces a novel approach that adapts a multi-modal large language model (MLLM) to incorporate PICG into PI-RADS scoring without additional annotations and network parameters. We present a two-stage fine-tuning process aimed at adapting MLLMs originally trained on natural images to the MRI data domain while effectively integrating the PICG. In the first stage, we develop a domain adapter layer specifically tailored for processing 3D MRI image inputs and design the MLLM instructions to differentiate MRI modalities effectively. In the second stage, we translate PICG into guiding instructions for the model to generate PICG-guided image features. Through feature distillation, we align scoring network features with the PICG-guided image feature, enabling the scoring network to effectively incorporate the PICG information. We develop our model on a public dataset and evaluate it in a real-world challenging in-house dataset. Experimental results demonstrate that our approach improves the performance of current scoring networks.","sentences":["The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the diagnosis of clinically significant prostate cancer through MRI imaging.","Current deep learning-based PI-RADS scoring methods often lack the incorporation of essential PI-RADS clinical guidelines~(PICG) utilized by radiologists, potentially compromising scoring accuracy.","This paper introduces a novel approach that adapts a multi-modal large language model (MLLM) to incorporate PICG into PI-RADS scoring without additional annotations and network parameters.","We present a two-stage fine-tuning process aimed at adapting MLLMs originally trained on natural images to the MRI data domain while effectively integrating the PICG.","In the first stage, we develop a domain adapter layer specifically tailored for processing 3D MRI image inputs and design the MLLM instructions to differentiate MRI modalities effectively.","In the second stage, we translate PICG into guiding instructions for the model to generate PICG-guided image features.","Through feature distillation, we align scoring network features with the PICG-guided image feature, enabling the scoring network to effectively incorporate the PICG information.","We develop our model on a public dataset and evaluate it in a real-world challenging in-house dataset.","Experimental results demonstrate that our approach improves the performance of current scoring networks."],"url":"http://arxiv.org/abs/2405.08786v1","category":"cs.CV"}
{"created":"2024-05-14 16:40:37","title":"Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach","abstract":"With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network.","sentences":["With the proliferation of edge devices, there is a significant increase in attack surface on these devices.","The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices.","This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time.","Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally.","LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives.","Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies.","The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge.","Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network."],"url":"http://arxiv.org/abs/2405.08755v1","category":"cs.CR"}
{"created":"2024-05-14 16:28:00","title":"CATEcor: an Open Science, Shaded-Truss, Externally-Occulted Coronagraph","abstract":"We present the design of a portable coronagraph, CATEcor, that incorporates a novel \"shaded truss\" style of external occultation and serves as a proof-of-concept for that family of coronagraphs. The shaded truss design style has the potential for broad application in various scientific settings. We conceived CATEcor itself as a simple instrument to observe the corona during the darker skies available during a partial solar eclipse, or for students or interested amateurs to detect the corona under ideal non-eclipsed conditions. CATEcor is therefore optimized for simplicity and accessibility to the public. It is implemented using an existing dioptric telescope and an adapter rig that mounts in front of the objective lens, restricting the telescope aperture and providing external occultation. The adapter rig, including occulter, is fabricated using fusion deposition modeling (FDM; colloquially \"3D printing\"), greatly reducing cost. The structure is designed to be integrated with moderate care and may be replicated in a university or amateur setting. While CATEcor is a simple demonstration unit, the design concept, process, and trades are useful for other more sophisticated coronagraphs in the same general family, which might operate under normal daytime skies outside the annular-eclipse conditions used for CATEcor.","sentences":["We present the design of a portable coronagraph, CATEcor, that incorporates a novel \"shaded truss\" style of external occultation and serves as a proof-of-concept for that family of coronagraphs.","The shaded truss design style has the potential for broad application in various scientific settings.","We conceived CATEcor itself as a simple instrument to observe the corona during the darker skies available during a partial solar eclipse, or for students or interested amateurs to detect the corona under ideal non-eclipsed conditions.","CATEcor is therefore optimized for simplicity and accessibility to the public.","It is implemented using an existing dioptric telescope and an adapter rig that mounts in front of the objective lens, restricting the telescope aperture and providing external occultation.","The adapter rig, including occulter, is fabricated using fusion deposition modeling (FDM; colloquially \"3D printing\"), greatly reducing cost.","The structure is designed to be integrated with moderate care and may be replicated in a university or amateur setting.","While CATEcor is a simple demonstration unit, the design concept, process, and trades are useful for other more sophisticated coronagraphs in the same general family, which might operate under normal daytime skies outside the annular-eclipse conditions used for CATEcor."],"url":"http://arxiv.org/abs/2405.08739v1","category":"astro-ph.IM"}
{"created":"2024-05-14 16:23:25","title":"Adaptive Time Stepping for a Two-Time Integro-Differential Equation in Non-Equilibrium Quantum Dynamics","abstract":"The non-equilibrium Green's function gives access to one-body observables for quantum systems. Of particular interest are quantities such as density, currents, and absorption spectra which are important for interpreting experimental results in quantum transport and spectroscopy. We present an integration scheme for the Green's function's equations of motion, the Kadanoff-Baym equations (KBE), which is both adaptive in the time integrator step size and method order as well as the history integration order. We analyze the importance of solving the KBE self-consistently and show that adapting the order of history integral evaluation is important for obtaining accurate results. To examine the efficiency of our method, we compare runtimes to a state of the art fixed time step integrator for several test systems and show an order of magnitude speedup at similar levels of accuracy.","sentences":["The non-equilibrium Green's function gives access to one-body observables for quantum systems.","Of particular interest are quantities such as density, currents, and absorption spectra which are important for interpreting experimental results in quantum transport and spectroscopy.","We present an integration scheme for the Green's function's equations of motion, the Kadanoff-Baym equations (KBE), which is both adaptive in the time integrator step size and method order as well as the history integration order.","We analyze the importance of solving the KBE self-consistently and show that adapting the order of history integral evaluation is important for obtaining accurate results.","To examine the efficiency of our method, we compare runtimes to a state of the art fixed time step integrator for several test systems and show an order of magnitude speedup at similar levels of accuracy."],"url":"http://arxiv.org/abs/2405.08737v1","category":"physics.comp-ph"}
{"created":"2024-05-14 16:16:43","title":"Dynamic On-Palm Manipulation via Controlled Sliding","abstract":"Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure. Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding. However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation. In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes. We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task. We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks. Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task. Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach. To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions.","sentences":["Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure.","Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding.","However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation.","In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes.","We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task.","We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks.","Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task.","Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach.","To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions."],"url":"http://arxiv.org/abs/2405.08731v1","category":"cs.RO"}
{"created":"2024-05-14 15:51:52","title":"Data-driven Force Observer for Human-Robot Interaction with Series Elastic Actuators using Gaussian Processes","abstract":"Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction. Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components. While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments. In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression. By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate. Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications. We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario.","sentences":["Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction.","Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components.","While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments.","In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression.","By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate.","Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications.","We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario."],"url":"http://arxiv.org/abs/2405.08711v1","category":"cs.RO"}
{"created":"2024-05-14 15:47:31","title":"Design and Analysis of Resilient Vehicular Platoon Systems over Wireless Networks","abstract":"Connected vehicular platoons provide a promising solution to improve traffic efficiency and ensure road safety. Vehicles in a platoon utilize on-board sensors and wireless vehicle-to-vehicle (V2V) links to share traffic information for cooperative adaptive cruise control. To process real-time control and alert information, there is a need to ensure clock synchronization among the platoon's vehicles. However, adversaries can jeopardize the operation of the platoon by attacking the local clocks of vehicles, leading to clock offsets with the platoon's reference clock. In this paper, a novel framework is proposed for analyzing the resilience of vehicular platoons that are connected using V2V links. In particular, a resilient design based on a diffusion protocol is proposed to re-synchronize the attacked vehicle through wireless V2V links thereby mitigating the impact of variance of the transmission delay during recovery. Then, a novel metric named temporal conditional mean exceedance is defined and analyzed in order to characterize the resilience of the platoon. Subsequently, the conditions pertaining to the V2V links and recovery time needed for a resilient design are derived. Numerical results show that the proposed resilient design is feasible in face of a nine-fold increase in the variance of transmission delay compared to a baseline designed for reliability. Moreover, the proposed approach improves the reliability, defined as the probability of meeting a desired clock offset error requirement, by 45% compared to the baseline.","sentences":["Connected vehicular platoons provide a promising solution to improve traffic efficiency and ensure road safety.","Vehicles in a platoon utilize on-board sensors and wireless vehicle-to-vehicle (V2V) links to share traffic information for cooperative adaptive cruise control.","To process real-time control and alert information, there is a need to ensure clock synchronization among the platoon's vehicles.","However, adversaries can jeopardize the operation of the platoon by attacking the local clocks of vehicles, leading to clock offsets with the platoon's reference clock.","In this paper, a novel framework is proposed for analyzing the resilience of vehicular platoons that are connected using V2V links.","In particular, a resilient design based on a diffusion protocol is proposed to re-synchronize the attacked vehicle through wireless V2V links thereby mitigating the impact of variance of the transmission delay during recovery.","Then, a novel metric named temporal conditional mean exceedance is defined and analyzed in order to characterize the resilience of the platoon.","Subsequently, the conditions pertaining to the V2V links and recovery time needed for a resilient design are derived.","Numerical results show that the proposed resilient design is feasible in face of a nine-fold increase in the variance of transmission delay compared to a baseline designed for reliability.","Moreover, the proposed approach improves the reliability, defined as the probability of meeting a desired clock offset error requirement, by 45% compared to the baseline."],"url":"http://arxiv.org/abs/2405.08706v1","category":"eess.SY"}
{"created":"2024-05-14 15:28:37","title":"Calculating response functions of coupled oscillators using quantum phase estimation","abstract":"We study the problem of estimating frequency response functions of systems of coupled, classical harmonic oscillators using a quantum computer. The functional form of these response functions can be mapped to a corresponding eigenproblem of a Hermitian matrix $H$, thus suggesting the use of quantum phase estimation. Our proposed quantum algorithm operates in the standard $s$-sparse, oracle-based query access model. For a network of $N$ oscillators with maximum norm $\\lVert H \\rVert_{\\mathrm{max}}$, and when the eigenvalue tolerance $\\varepsilon$ is much smaller than the minimum eigenvalue gap, we use $\\mathcal{O}(\\log(N s \\lVert H \\rVert_{\\mathrm{max}}/\\varepsilon)$ algorithmic qubits and obtain a rigorous worst-case query complexity upper bound $\\mathcal{O}(s \\lVert H \\rVert_{\\mathrm{max}}/(\\delta^2 \\varepsilon) )$ up to logarithmic factors, where $\\delta$ denotes the desired precision on the coefficients appearing in the response functions. Crucially, our proposal does not suffer from the infamous state preparation bottleneck and can as such potentially achieve large quantum speedups compared to relevant classical methods. As a proof-of-principle of exponential quantum speedup, we show that a simple adaptation of our algorithm solves the random glued-trees problem in polynomial time. We discuss practical limitations as well as potential improvements for quantifying finite size, end-to-end complexities for application to relevant instances.","sentences":["We study the problem of estimating frequency response functions of systems of coupled, classical harmonic oscillators using a quantum computer.","The functional form of these response functions can be mapped to a corresponding eigenproblem of a Hermitian matrix $H$, thus suggesting the use of quantum phase estimation.","Our proposed quantum algorithm operates in the standard $s$-sparse, oracle-based query access model.","For a network of $N$ oscillators with maximum norm $\\lVert H \\rVert_{\\mathrm{max}}$, and when the eigenvalue tolerance $\\varepsilon$ is much smaller than the minimum eigenvalue gap, we use $\\mathcal{O}(\\log(N s \\lVert H \\rVert_{\\mathrm{max}}/\\varepsilon)$ algorithmic qubits and obtain a rigorous worst-case query complexity upper bound $\\mathcal{O}(s \\lVert H \\rVert_{\\mathrm{max}}/(\\delta^2 \\varepsilon) )$ up to logarithmic factors, where $\\delta$ denotes the desired precision on the coefficients appearing in the response functions.","Crucially, our proposal does not suffer from the infamous state preparation bottleneck and can as such potentially achieve large quantum speedups compared to relevant classical methods.","As a proof-of-principle of exponential quantum speedup, we show that a simple adaptation of our algorithm solves the random glued-trees problem in polynomial time.","We discuss practical limitations as well as potential improvements for quantifying finite size, end-to-end complexities for application to relevant instances."],"url":"http://arxiv.org/abs/2405.08694v1","category":"quant-ph"}
{"created":"2024-05-14 15:26:42","title":"Extending Non-Perturbative Simulation Techniques for Open-Quantum Systems to Excited-State Proton Transfer and Ultrafast Non-Adiabatic Dynamics","abstract":"Excited state proton transfer is an ubiquitous phenomenon in biology and chemistry, spanning from the ultrafast reactions of photo-bases and acids to light-driven, enzymatic catalysis and photosynthesis. However, the simulation of such dynamics involves multiple challenges, since high-dimensional, out-of-equilibrium vibronic states play a crucial role, while a fully quantum description of the proton's dissipative, real-space dynamics is also required. In this work, we extend the powerful Matrix Product State approach to open quantum systems (TEDOPA) to study these demanding dynamics, and also more general non-adiabatic processes that can appear in complex photochemistry subject to strong laser driving. As an illustration, we initially consider an open model of a four-level electronic system interacting with hundreds of intramolecular vibrations that drive ultrafast excited state proton transfer, as well as an explicit photonic environment that allows us to directly monitor the resulting dual fluorescence in this system. We then demonstrate how to include a continuous 'reaction coordinate' of the proton transfer that allows numerically exact simulations that can be understood, visualized and interpreted in the familiar language of diabatic and adiabatic dynamics on potential surfaces, while also retaining an exact quantum treatment of dissipation and driving effects that could be used to study diverse problems in ultrafast photochemistry.","sentences":["Excited state proton transfer is an ubiquitous phenomenon in biology and chemistry, spanning from the ultrafast reactions of photo-bases and acids to light-driven, enzymatic catalysis and photosynthesis.","However, the simulation of such dynamics involves multiple challenges, since high-dimensional, out-of-equilibrium vibronic states play a crucial role, while a fully quantum description of the proton's dissipative, real-space dynamics is also required.","In this work, we extend the powerful Matrix Product State approach to open quantum systems (TEDOPA) to study these demanding dynamics, and also more general non-adiabatic processes that can appear in complex photochemistry subject to strong laser driving.","As an illustration, we initially consider an open model of a four-level electronic system interacting with hundreds of intramolecular vibrations that drive ultrafast excited state proton transfer, as well as an explicit photonic environment that allows us to directly monitor the resulting dual fluorescence in this system.","We then demonstrate how to include a continuous 'reaction coordinate' of the proton transfer that allows numerically exact simulations that can be understood, visualized and interpreted in the familiar language of diabatic and adiabatic dynamics on potential surfaces, while also retaining an exact quantum treatment of dissipation and driving effects that could be used to study diverse problems in ultrafast photochemistry."],"url":"http://arxiv.org/abs/2405.08693v1","category":"physics.chem-ph"}
{"created":"2024-05-14 15:04:46","title":"Achieving Fairness Through Channel Pruning for Dermatological Disease Diagnosis","abstract":"Numerous studies have revealed that deep learning-based medical image classification models may exhibit bias towards specific demographic attributes, such as race, gender, and age. Existing bias mitigation methods often achieve high level of fairness at the cost of significant accuracy degradation. In response to this challenge, we propose an innovative and adaptable Soft Nearest Neighbor Loss-based channel pruning framework, which achieves fairness through channel pruning. Traditionally, channel pruning is utilized to accelerate neural network inference. However, our work demonstrates that pruning can also be a potent tool for achieving fairness. Our key insight is that different channels in a layer contribute differently to the accuracy of different groups. By selectively pruning critical channels that lead to the accuracy difference between the privileged and unprivileged groups, we can effectively improve fairness without sacrificing accuracy significantly. Experiments conducted on two skin lesion diagnosis datasets across multiple sensitive attributes validate the effectiveness of our method in achieving state-of-the-art trade-off between accuracy and fairness. Our code is available at https://github.com/Kqp1227/Sensitive-Channel-Pruning.","sentences":["Numerous studies have revealed that deep learning-based medical image classification models may exhibit bias towards specific demographic attributes, such as race, gender, and age.","Existing bias mitigation methods often achieve high level of fairness at the cost of significant accuracy degradation.","In response to this challenge, we propose an innovative and adaptable Soft Nearest Neighbor Loss-based channel pruning framework, which achieves fairness through channel pruning.","Traditionally, channel pruning is utilized to accelerate neural network inference.","However, our work demonstrates that pruning can also be a potent tool for achieving fairness.","Our key insight is that different channels in a layer contribute differently to the accuracy of different groups.","By selectively pruning critical channels that lead to the accuracy difference between the privileged and unprivileged groups, we can effectively improve fairness without sacrificing accuracy significantly.","Experiments conducted on two skin lesion diagnosis datasets across multiple sensitive attributes validate the effectiveness of our method in achieving state-of-the-art trade-off between accuracy and fairness.","Our code is available at https://github.com/Kqp1227/Sensitive-Channel-Pruning."],"url":"http://arxiv.org/abs/2405.08681v1","category":"cs.CV"}
{"created":"2024-05-14 14:55:15","title":"EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth Estimation from Any Endoscopic Camera","abstract":"Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization. Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance. This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation. We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes. Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters. Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder. Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs. Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics. Code is available at https://github.com/BeileiCui/EndoDAC.","sentences":["Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization.","Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance.","This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation.","We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes.","Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters.","Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder.","Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs.","Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics.","Code is available at https://github.com/BeileiCui/EndoDAC."],"url":"http://arxiv.org/abs/2405.08672v1","category":"eess.IV"}
{"created":"2024-05-14 14:51:12","title":"Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research","abstract":"Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.","sentences":["Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs.","However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia.","To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework.","GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources.","By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings.","Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations.","Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach.","Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm.","Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry."],"url":"http://arxiv.org/abs/2405.08668v1","category":"cs.CV"}
{"created":"2024-05-14 14:10:48","title":"A Fast and Scalable Pathwise-Solver for Group Lasso and Elastic Net Penalized Regression via Block-Coordinate Descent","abstract":"We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path. Special attention is given when the loss is the usual least squares loss (Gaussian loss). We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate. Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets. Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet.","sentences":["We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path.","Special attention is given when the loss is the usual least squares loss (Gaussian loss).","We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate.","Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets.","Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet."],"url":"http://arxiv.org/abs/2405.08631v1","category":"stat.CO"}
{"created":"2024-05-14 14:08:55","title":"Beyond Quantum Annealing: Optimal control solutions to MaxCut problems","abstract":"Quantum Annealing (QA) relies on mixing two Hamiltonian terms, a simple driver and a complex problem Hamiltonian, in a linear combination. The time-dependent schedule for this mixing is often taken to be linear in time: improving on this linear choice is known to be essential and has proven to be difficult. Here, we present different techniques for improving on the linear-schedule QA along two directions, conceptually distinct but leading to similar outcomes: 1) the first approach consists of constructing a Trotter-digitized QA (dQA) with schedules parameterized in terms of Fourier modes or Chebyshev polynomials, inspired by the Chopped Random Basis algorithm (CRAB) for optimal control in continuous time; 2) the second approach is technically a Quantum Approximate Optimization Algorithm (QAOA), whose solutions are found iteratively using linear interpolation or expansion in Fourier modes. Both approaches emphasize finding smooth optimal schedule parameters, ultimately leading to hybrid quantum-classical variational algorithms of the alternating Hamiltonian Ansatz type. We apply these techniques to MaxCut problems on weighted 3-regular graphs with N = 14 sites, focusing on hard instances that exhibit a small spectral gap, for which a standard linear-schedule QA performs poorly. We characterize the physics behind the optimal protocols for both the dQA and QAOA approaches, discovering shortcuts to adiabaticity-like dynamics. Furthermore, we study the transferability of such smooth solutions among hard instances of MaxCut at different circuit depths. Finally, we show that the smoothness pattern of these protocols obtained in a digital setting enables us to adapt them to continuous-time evolution, contrarily to generic non-smooth solutions. This procedure results in an optimized quantum annealing schedule that is implementable on analog devices.","sentences":["Quantum Annealing (QA) relies on mixing two Hamiltonian terms, a simple driver and a complex problem Hamiltonian, in a linear combination.","The time-dependent schedule for this mixing is often taken to be linear in time: improving on this linear choice is known to be essential and has proven to be difficult.","Here, we present different techniques for improving on the linear-schedule QA along two directions, conceptually distinct but leading to similar outcomes: 1) the first approach consists of constructing a Trotter-digitized QA (dQA) with schedules parameterized in terms of Fourier modes or Chebyshev polynomials, inspired by the Chopped Random Basis algorithm (CRAB) for optimal control in continuous time; 2) the second approach is technically a Quantum Approximate Optimization Algorithm (QAOA), whose solutions are found iteratively using linear interpolation or expansion in Fourier modes.","Both approaches emphasize finding smooth optimal schedule parameters, ultimately leading to hybrid quantum-classical variational algorithms of the alternating Hamiltonian Ansatz type.","We apply these techniques to MaxCut problems on weighted 3-regular graphs with N = 14 sites, focusing on hard instances that exhibit a small spectral gap, for which a standard linear-schedule QA performs poorly.","We characterize the physics behind the optimal protocols for both the dQA and QAOA approaches, discovering shortcuts to adiabaticity-like dynamics.","Furthermore, we study the transferability of such smooth solutions among hard instances of MaxCut at different circuit depths.","Finally, we show that the smoothness pattern of these protocols obtained in a digital setting enables us to adapt them to continuous-time evolution, contrarily to generic non-smooth solutions.","This procedure results in an optimized quantum annealing schedule that is implementable on analog devices."],"url":"http://arxiv.org/abs/2405.08630v1","category":"quant-ph"}
{"created":"2024-05-14 13:37:13","title":"EVDA: Evolving Deepfake Audio Detection Continual Learning Benchmark","abstract":"The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging. Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types. Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework. To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection. EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o. It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM). Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods","sentences":["The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging.","Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types.","Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework.","To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection.","EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o.","It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM).","Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods"],"url":"http://arxiv.org/abs/2405.08596v2","category":"cs.SD"}
{"created":"2024-05-14 13:05:16","title":"Rethinking the adaptive relationship between Encoder Layers and Decoder Layers","abstract":"This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English. The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining. Four experiments were conducted in total. The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance. However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential.","sentences":["This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English.","The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining.","Four experiments were conducted in total.","The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance.","However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential."],"url":"http://arxiv.org/abs/2405.08570v1","category":"cs.CL"}
{"created":"2024-05-14 12:24:52","title":"Self-Distillation Improves DNA Sequence Inference","abstract":"Self-supervised pretraining (SSP) has been recognized as a method to enhance prediction accuracy in various downstream tasks. However, its efficacy for DNA sequences remains somewhat constrained. This limitation stems primarily from the fact that most existing SSP approaches in genomics focus on masked language modeling of individual sequences, neglecting the crucial aspect of encoding statistics across multiple sequences. To overcome this challenge, we introduce an innovative deep neural network model, which incorporates collaborative learning between a `student' and a `teacher' subnetwork. In this model, the student subnetwork employs masked learning on nucleotides and progressively adapts its parameters to the teacher subnetwork through an exponential moving average approach. Concurrently, both subnetworks engage in contrastive learning, deriving insights from two augmented representations of the input sequences. This self-distillation process enables our model to effectively assimilate both contextual information from individual sequences and distributional data across the sequence population. We validated our approach with preliminary pretraining using the human reference genome, followed by applying it to 20 downstream inference tasks. The empirical results from these experiments demonstrate that our novel method significantly boosts inference performance across the majority of these tasks. Our code is available at https://github.com/wiedersehne/FinDNA.","sentences":["Self-supervised pretraining (SSP) has been recognized as a method to enhance prediction accuracy in various downstream tasks.","However, its efficacy for DNA sequences remains somewhat constrained.","This limitation stems primarily from the fact that most existing SSP approaches in genomics focus on masked language modeling of individual sequences, neglecting the crucial aspect of encoding statistics across multiple sequences.","To overcome this challenge, we introduce an innovative deep neural network model, which incorporates collaborative learning between a `student' and a `teacher' subnetwork.","In this model, the student subnetwork employs masked learning on nucleotides and progressively adapts its parameters to the teacher subnetwork through an exponential moving average approach.","Concurrently, both subnetworks engage in contrastive learning, deriving insights from two augmented representations of the input sequences.","This self-distillation process enables our model to effectively assimilate both contextual information from individual sequences and distributional data across the sequence population.","We validated our approach with preliminary pretraining using the human reference genome, followed by applying it to 20 downstream inference tasks.","The empirical results from these experiments demonstrate that our novel method significantly boosts inference performance across the majority of these tasks.","Our code is available at https://github.com/wiedersehne/FinDNA."],"url":"http://arxiv.org/abs/2405.08538v1","category":"cs.LG"}
{"created":"2024-05-14 12:14:58","title":"Parameter-Efficient Instance-Adaptive Neural Video Compression","abstract":"Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to the standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines. However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data. The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video. However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training. In this work, we propose a parameter-efficient instance-adaptive video compression framework. Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences. The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms. Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings. We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC.","sentences":["Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to the standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines.","However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data.","The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video.","However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training.","In this work, we propose a parameter-efficient instance-adaptive video compression framework.","Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences.","The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms.","Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings.","We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC."],"url":"http://arxiv.org/abs/2405.08530v1","category":"eess.IV"}
{"created":"2024-05-14 11:37:26","title":"Falcon 7b for Software Mention Detection in Scholarly Documents","abstract":"This paper aims to tackle the challenge posed by the increasing integration of software tools in research across various disciplines by investigating the application of Falcon-7b for the detection and classification of software mentions within scholarly texts. Specifically, the study focuses on solving Subtask I of the Software Mention Detection in Scholarly Publications (SOMD), which entails identifying and categorizing software mentions from academic literature. Through comprehensive experimentation, the paper explores different training strategies, including a dual-classifier approach, adaptive sampling, and weighted loss scaling, to enhance detection accuracy while overcoming the complexities of class imbalance and the nuanced syntax of scholarly writing. The findings highlight the benefits of selective labelling and adaptive sampling in improving the model's performance. However, they also indicate that integrating multiple strategies does not necessarily result in cumulative improvements. This research offers insights into the effective application of large language models for specific tasks such as SOMD, underlining the importance of tailored approaches to address the unique challenges presented by academic text analysis.","sentences":["This paper aims to tackle the challenge posed by the increasing integration of software tools in research across various disciplines by investigating the application of Falcon-7b for the detection and classification of software mentions within scholarly texts.","Specifically, the study focuses on solving Subtask I of the Software Mention Detection in Scholarly Publications (SOMD), which entails identifying and categorizing software mentions from academic literature.","Through comprehensive experimentation, the paper explores different training strategies, including a dual-classifier approach, adaptive sampling, and weighted loss scaling, to enhance detection accuracy while overcoming the complexities of class imbalance and the nuanced syntax of scholarly writing.","The findings highlight the benefits of selective labelling and adaptive sampling in improving the model's performance.","However, they also indicate that integrating multiple strategies does not necessarily result in cumulative improvements.","This research offers insights into the effective application of large language models for specific tasks such as SOMD, underlining the importance of tailored approaches to address the unique challenges presented by academic text analysis."],"url":"http://arxiv.org/abs/2405.08514v1","category":"cs.LG"}
{"created":"2024-05-14 10:51:48","title":"A randomly generated Majorana neutrino mass matrix using Adaptive Monte Carlo method","abstract":"A randomly generated complex symmetric matrix using Adaptive Monte Carlo method, is taken as a general form of Majorana neutrino mass matrix, which is diagonalized by the use of eigenvectors. We extract all the neutrino oscillation parameters i.e. two mass-squared differences ($\\Delta m_{21}^2$ and $\\Delta m_{32}^2$ ), three mixing angles ($\\theta_{12}$, $\\theta_{13}$, $\\theta_{23}$) and three phases i.e. one Dirac CP violating phase ($\\delta_{CP}$) and two Majorana phases ($\\alpha$ and $\\beta$). The charge-parity (CP) violating phases are extracted from the mixing matrix constructed with the eigenvectors of the Hermitian matrix formed by the complex symmetric matrix. All the neutrino oscillation parameters within 3$\\sigma$ bound are allowed in both normal hierarchy (NH) and inverted hierarchy (IH) consistent with the latest Planck cosmological upper bound, $\\sum\\vert m_i\\vert<0.12$ eV. This latest cosmological upper bound is allowed only in three cases of zero texture for $m_{11}=0$; $m_{11},m_{12}=0$ and $m_{11},m_{13}=0$ in normal hierarchy whereas none of zero texture is allowed in inverted hierarchy. We also study effective neutrino masses $m_{\\beta}$ in tritium beta decay and $m_{\\beta\\beta}$ in neutrinoless double beta decay.","sentences":["A randomly generated complex symmetric matrix using Adaptive Monte Carlo method, is taken as a general form of Majorana neutrino mass matrix, which is diagonalized by the use of eigenvectors.","We extract all the neutrino oscillation parameters i.e. two mass-squared differences ($\\Delta m_{21}^2$ and $\\Delta m_{32}^2$ ), three mixing angles ($\\theta_{12}$, $\\theta_{13}$, $\\theta_{23}$) and three phases i.e. one Dirac CP violating phase ($\\delta_{CP}$) and two Majorana phases ($\\alpha$ and $\\beta$).","The charge-parity (CP) violating phases are extracted from the mixing matrix constructed with the eigenvectors of the Hermitian matrix formed by the complex symmetric matrix.","All the neutrino oscillation parameters within 3$\\sigma$ bound are allowed in both normal hierarchy (NH) and inverted hierarchy (IH) consistent with the latest Planck cosmological upper bound, $\\sum\\vert m_i\\vert<0.12$ eV. This latest cosmological upper bound is allowed only in three cases of zero texture for $m_{11}=0$; $m_{11},m_{12}=0$ and $m_{11},m_{13}=0$ in normal hierarchy whereas none of zero texture is allowed in inverted hierarchy.","We also study effective neutrino masses $m_{\\beta}$ in tritium beta decay and $m_{\\beta\\beta}$ in neutrinoless double beta decay."],"url":"http://arxiv.org/abs/2405.08495v1","category":"hep-ph"}
{"created":"2024-05-14 09:31:31","title":"Evaluating LLMs at Evaluating Temporal Generalization","abstract":"The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing. However, traditional benchmarks, which are often static, fail to capture the continually changing information landscape, leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios. Furthermore, these benchmarks do not adequately measure the models' capabilities over a broader temporal range or their adaptability over time. We examine current LLMs in terms of temporal generalization and bias, revealing that various temporal biases emerge in both language likelihood and prognostic prediction. This serves as a caution for LLM practitioners to pay closer attention to mitigating temporal biases. Also, we propose an evaluation framework Freshbench for dynamically generating benchmarks from the most recent real-world prognostication prediction. Our code is available at https://github.com/FreedomIntelligence/FreshBench. The dataset will be released soon.","sentences":["The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing.","However, traditional benchmarks, which are often static, fail to capture the continually changing information landscape, leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios.","Furthermore, these benchmarks do not adequately measure the models' capabilities over a broader temporal range or their adaptability over time.","We examine current LLMs in terms of temporal generalization and bias, revealing that various temporal biases emerge in both language likelihood and prognostic prediction.","This serves as a caution for LLM practitioners to pay closer attention to mitigating temporal biases.","Also, we propose an evaluation framework Freshbench for dynamically generating benchmarks from the most recent real-world prognostication prediction.","Our code is available at https://github.com/FreedomIntelligence/FreshBench.","The dataset will be released soon."],"url":"http://arxiv.org/abs/2405.08460v1","category":"cs.CL"}
{"created":"2024-05-14 08:35:39","title":"Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More","abstract":"Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable. Karalias & Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO. First, we concretize the targets for objective construction and derandomization with theoretical justification. Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed.","sentences":["Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable.","Karalias & Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization.","Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization.","However, each component confronts unique challenges.","First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial.","Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding.","In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO.","First, we concretize the targets for objective construction and derandomization with theoretical justification.","Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets.","Finally, we apply the derivations to various CO problems.","Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t.","both optimization quality and speed."],"url":"http://arxiv.org/abs/2405.08424v1","category":"cs.LG"}
{"created":"2024-05-16 17:16:47","title":"Production of electroweak gauge bosons at forward rapidities in the color - dipole $S$ - matrix framework","abstract":"The cross-section for the production of an electroweak gauge boson ($G = W^{\\pm}, Z^0, \\gamma$) at forward rapidities in $pp$ collisions is derived within the color - dipole $S$ - matrix framework. We present the full expressions for the differential cross-section of the $q p \\rightarrow G X$ process in the impact parameter and transverse momentum spaces, considering the longitudinal and transverse polarizations of the gauge boson. The particular cases associated with the Drell - Yan process and real photon production are discussed. We demonstrate that the final formulae are expressed in terms of the dipole - proton cross-section or the unintegrated gluon distribution, and can be used to estimate the impact of the saturation effects in the gauge boson production at the LHC and future colliders.","sentences":["The cross-section for the production of an electroweak gauge boson ($G = W^{\\pm}, Z^0, \\gamma$) at forward rapidities in $pp$ collisions is derived within the color - dipole $S$ - matrix framework.","We present the full expressions for the differential cross-section of the $q p \\rightarrow G X$ process in the impact parameter and transverse momentum spaces, considering the longitudinal and transverse polarizations of the gauge boson.","The particular cases associated with the Drell - Yan process and real photon production are discussed.","We demonstrate that the final formulae are expressed in terms of the dipole - proton cross-section or the unintegrated gluon distribution, and can be used to estimate the impact of the saturation effects in the gauge boson production at the LHC and future colliders."],"url":"http://arxiv.org/abs/2405.10265v1","category":"hep-ph"}
{"created":"2024-05-16 16:40:14","title":"Pentagon equations, Vorono\u00ef tilings and pure braid groups invariant","abstract":"In the present paper, we construct $(2n-4)\\times (2n-4)$ matrices corresponding to the motion of points on standard round sphere from the point of view of Delaunay triangulations. We define homomorphism from spherical pure braids on $n$ strands to the product of these matrices for $n>5$.","sentences":["In the present paper, we construct $(2n-4)\\times (2n-4)$ matrices corresponding to the motion of points on standard round sphere from the point of view of Delaunay triangulations.","We define homomorphism from spherical pure braids on $n$ strands to the product of these matrices for $n>5$."],"url":"http://arxiv.org/abs/2405.10240v1","category":"math.AT"}
{"created":"2024-05-16 15:46:30","title":"Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level","abstract":"Scientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents. Our code will be available at \\url{https://github.com/MoLICHENXI/HAESum}","sentences":["Scientific document summarization has been a challenging task due to the long structure of the input text.","The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization.","However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations.","In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure.","First, intra-sentence relations are learned using a local heterogeneous graph.","Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations.","We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents.","Our code will be available at \\url{https://github.com/MoLICHENXI/HAESum}"],"url":"http://arxiv.org/abs/2405.10202v1","category":"cs.CL"}
{"created":"2024-05-16 15:44:57","title":"Interplay between Domain Walls in Type-II Superconductors and Gradients of Temperature/Spin Density","abstract":"We theoretically investigate the motion of a domain wall in type-II superconductors driven by inhomogeneities of temperature or spin density. The model consists of the time-dependent Ginzburg-Landau equation and the thermal/spin diffusion equation whose transport coefficients (the thermal/spin conductivity and the spin relaxation time) depend on the order parameter, interpolating the values for the superconducting and the normal states. Both numerical and analytical calculations suggest that the domain wall moves toward the higher temperature/spin-density region where the order parameter is suppressed. Both motions can be understood as dynamical processes minimizing the loss of the condensation energy. We also clearly define the forces on the domain wall according to their origins.","sentences":["We theoretically investigate the motion of a domain wall in type-II superconductors driven by inhomogeneities of temperature or spin density.","The model consists of the time-dependent Ginzburg-Landau equation and the thermal/spin diffusion equation whose transport coefficients (the thermal/spin conductivity and the spin relaxation time) depend on the order parameter, interpolating the values for the superconducting and the normal states.","Both numerical and analytical calculations suggest that the domain wall moves toward the higher temperature/spin-density region where the order parameter is suppressed.","Both motions can be understood as dynamical processes minimizing the loss of the condensation energy.","We also clearly define the forces on the domain wall according to their origins."],"url":"http://arxiv.org/abs/2405.10200v1","category":"cond-mat.supr-con"}
{"created":"2024-05-16 15:24:57","title":"Scattering problem for Vlasov-type equations on the $d$-dimensional torus with Gevrey data","abstract":"In this article, we consider Vlasov-type equations describing the evolution of single-species type plasmas, such as those composed of electrons (Vlasov-Poisson) or ions (screened Vlasov-Poisson/Vlasov-Poisson with massless electrons). We solve the final data problem on the torus $\\mathbb{T}^d$, $d \\geq 1$, by considering asymptotic states of regularity Gevrey-$\\frac{1}{\\gamma}$ with $\\gamma>\\frac13$, small perturbations of homogeneous equilibria satisfying the Penrose stability condition. This extends to the Gevrey perturbative case, and to higher dimension, the scattering result in analytic regularity obtained by E. Caglioti and C. Maffei in [14], and answers an open question raised by J. Bedrossian in arXiv:2211.13707.","sentences":["In this article, we consider Vlasov-type equations describing the evolution of single-species type plasmas, such as those composed of electrons (Vlasov-Poisson) or ions (screened Vlasov-Poisson/Vlasov-Poisson with massless electrons).","We solve the final data problem on the torus $\\mathbb{T}^d$, $d \\geq 1$, by considering asymptotic states of regularity Gevrey-$\\frac{1}{\\gamma}$ with $\\gamma>\\frac13$, small perturbations of homogeneous equilibria satisfying the Penrose stability condition.","This extends to the Gevrey perturbative case, and to higher dimension, the scattering result in analytic regularity obtained by E. Caglioti and C. Maffei in [14], and answers an open question raised by J. Bedrossian in arXiv:2211.13707."],"url":"http://arxiv.org/abs/2405.10182v1","category":"math.AP"}
{"created":"2024-05-16 14:35:50","title":"Relational DNN Verification With Cross Executional Bound Refinement","abstract":"We focus on verifying relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations (UAP), certified worst-case hamming distance for binary string classifications, etc. Precise verification of these properties requires reasoning about multiple executions of the same DNN. However, most of the existing works in DNN verification only handle properties defined over single executions and as a result, are imprecise for relational properties. Though few recent works for relational DNN verification, capture linear dependencies between the inputs of multiple executions, they do not leverage dependencies between the outputs of hidden layers producing imprecise results. We develop a scalable relational verifier RACoon that utilizes cross-execution dependencies at all layers of the DNN gaining substantial precision over SOTA baselines on a wide range of datasets, networks, and relational properties.","sentences":["We focus on verifying relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations (UAP), certified worst-case hamming distance for binary string classifications, etc.","Precise verification of these properties requires reasoning about multiple executions of the same DNN.","However, most of the existing works in DNN verification only handle properties defined over single executions and as a result, are imprecise for relational properties.","Though few recent works for relational DNN verification, capture linear dependencies between the inputs of multiple executions, they do not leverage dependencies between the outputs of hidden layers producing imprecise results.","We develop a scalable relational verifier RACoon that utilizes cross-execution dependencies at all layers of the DNN gaining substantial precision over SOTA baselines on a wide range of datasets, networks, and relational properties."],"url":"http://arxiv.org/abs/2405.10143v1","category":"cs.LG"}
{"created":"2024-05-16 14:03:05","title":"Source identification using different moment hierarchies","abstract":"This work considers the localization of a bioluminescent source in a tissue-mimicking phantom. To model the light propagation in the resulting inverse problem the radiative transfer equation is approximated by using hierarchical simplified moment approximations. The inverse problem is then solved by using a consensus-based optimization algorithm.","sentences":["This work considers the localization of a bioluminescent source in a tissue-mimicking phantom.","To model the light propagation in the resulting inverse problem the radiative transfer equation is approximated by using hierarchical simplified moment approximations.","The inverse problem is then solved by using a consensus-based optimization algorithm."],"url":"http://arxiv.org/abs/2405.10110v1","category":"q-bio.QM"}
{"created":"2024-05-16 12:57:03","title":"MrRegNet: Multi-resolution Mask Guided Convolutional Neural Network for Medical Image Registration with Large Deformations","abstract":"Deformable image registration (alignment) is highly sought after in numerous clinical applications, such as computer aided diagnosis and disease progression analysis. Deep Convolutional Neural Network (DCNN)-based image registration methods have demonstrated advantages in terms of registration accuracy and computational speed. However, while most methods excel at global alignment, they often perform worse in aligning local regions. To address this challenge, this paper proposes a mask-guided encoder-decoder DCNN-based image registration method, named as MrRegNet. This approach employs a multi-resolution encoder for feature extraction and subsequently estimates multi-resolution displacement fields in the decoder to handle the substantial deformation of images. Furthermore, segmentation masks are employed to direct the model's attention toward aligning local regions. The results show that the proposed method outperforms traditional methods like Demons and a well-known deep learning method, VoxelMorph, on a public 3D brain MRI dataset (OASIS) and a local 2D brain MRI dataset with large deformations. Importantly, the image alignment accuracies are significantly improved at local regions guided by segmentation masks. Github link:https://github.com/ruizhe-l/MrRegNet.","sentences":["Deformable image registration (alignment) is highly sought after in numerous clinical applications, such as computer aided diagnosis and disease progression analysis.","Deep Convolutional Neural Network (DCNN)-based image registration methods have demonstrated advantages in terms of registration accuracy and computational speed.","However, while most methods excel at global alignment, they often perform worse in aligning local regions.","To address this challenge, this paper proposes a mask-guided encoder-decoder DCNN-based image registration method, named as MrRegNet.","This approach employs a multi-resolution encoder for feature extraction and subsequently estimates multi-resolution displacement fields in the decoder to handle the substantial deformation of images.","Furthermore, segmentation masks are employed to direct the model's attention toward aligning local regions.","The results show that the proposed method outperforms traditional methods like Demons and a well-known deep learning method, VoxelMorph, on a public 3D brain MRI dataset (OASIS) and a local 2D brain MRI dataset with large deformations.","Importantly, the image alignment accuracies are significantly improved at local regions guided by segmentation masks.","Github link:https://github.com/ruizhe-l/MrRegNet."],"url":"http://arxiv.org/abs/2405.10068v1","category":"eess.IV"}
{"created":"2024-05-16 12:41:25","title":"Deep Neural Network-assisted improvement of quantum compressed sensing tomography","abstract":"Quantum compressed sensing is the fundamental tool for low-rank density matrix tomographic reconstruction in the informationally incomplete case. We examine situations where the acquired information is not enough to allow one to obtain a precise compressed sensing reconstruction. In this scenario, we propose a Deep Neural Network-based post-processing to improve the initial reconstruction provided by compressed sensing. The idea is to treat the estimated state as a noisy input for the network and perform a deep-supervised denoising task. After the network is applied, a projection onto the space of feasible density matrices is performed to obtain an improved final state estimation. We demonstrate through numerical experiments the improvement obtained by the denoising process and exploit the possibility of looping the inference scheme to obtain further advantages. Finally, we test the resilience of the approach to out-of-distribution data.","sentences":["Quantum compressed sensing is the fundamental tool for low-rank density matrix tomographic reconstruction in the informationally incomplete case.","We examine situations where the acquired information is not enough to allow one to obtain a precise compressed sensing reconstruction.","In this scenario, we propose a Deep Neural Network-based post-processing to improve the initial reconstruction provided by compressed sensing.","The idea is to treat the estimated state as a noisy input for the network and perform a deep-supervised denoising task.","After the network is applied, a projection onto the space of feasible density matrices is performed to obtain an improved final state estimation.","We demonstrate through numerical experiments the improvement obtained by the denoising process and exploit the possibility of looping the inference scheme to obtain further advantages.","Finally, we test the resilience of the approach to out-of-distribution data."],"url":"http://arxiv.org/abs/2405.10052v1","category":"quant-ph"}
{"created":"2024-05-16 10:01:16","title":"Dual-band feature selection for maturity classification of specialty crops by hyperspectral imaging","abstract":"The maturity classification of specialty crops such as strawberries and tomatoes is an essential agricultural downstream activity for selective harvesting and quality control (QC) at production and packaging sites. Recent advancements in Deep Learning (DL) have produced encouraging results in color images for maturity classification applications. However, hyperspectral imaging (HSI) outperforms methods based on color vision. Multivariate analysis methods and Convolutional Neural Networks (CNN) deliver promising results; however, a large amount of input data and the associated preprocessing requirements cause hindrances in practical application. Conventionally, the reflectance intensity in a given electromagnetic spectrum is employed in estimating fruit maturity. We present a feature extraction method to empirically demonstrate that the peak reflectance in subbands such as 500-670 nm (pigment band) and the wavelength of the peak position, and contrarily, the trough reflectance and its corresponding wavelength within 671-790 nm (chlorophyll band) are convenient to compute yet distinctive features for the maturity classification. The proposed feature selection method is beneficial because preprocessing, such as dimensionality reduction, is avoided before every prediction. The feature set is designed to capture these traits. The best SOTA methods, among 3D-CNN, 1D-CNN, and SVM, achieve at most 90.0 % accuracy for strawberries and 92.0 % for tomatoes on our dataset. Results show that the proposed method outperforms the SOTA as it yields an accuracy above 98.0 % in strawberry and 96.0 % in tomato classification. A comparative analysis of the time efficiency of these methods is also conducted, which shows the proposed method performs prediction at 13 Frames Per Second (FPS) compared to the maximum 1.16 FPS attained by the full-spectrum SVM classifier.","sentences":["The maturity classification of specialty crops such as strawberries and tomatoes is an essential agricultural downstream activity for selective harvesting and quality control (QC) at production and packaging sites.","Recent advancements in Deep Learning (DL) have produced encouraging results in color images for maturity classification applications.","However, hyperspectral imaging (HSI) outperforms methods based on color vision.","Multivariate analysis methods and Convolutional Neural Networks (CNN) deliver promising results; however, a large amount of input data and the associated preprocessing requirements cause hindrances in practical application.","Conventionally, the reflectance intensity in a given electromagnetic spectrum is employed in estimating fruit maturity.","We present a feature extraction method to empirically demonstrate that the peak reflectance in subbands such as 500-670 nm (pigment band) and the wavelength of the peak position, and contrarily, the trough reflectance and its corresponding wavelength within 671-790 nm (chlorophyll band) are convenient to compute yet distinctive features for the maturity classification.","The proposed feature selection method is beneficial because preprocessing, such as dimensionality reduction, is avoided before every prediction.","The feature set is designed to capture these traits.","The best SOTA methods, among 3D-CNN, 1D-CNN, and SVM, achieve at most 90.0 % accuracy for strawberries and 92.0 % for tomatoes on our dataset.","Results show that the proposed method outperforms the SOTA as it yields an accuracy above 98.0 % in strawberry and 96.0 % in tomato classification.","A comparative analysis of the time efficiency of these methods is also conducted, which shows the proposed method performs prediction at 13 Frames Per Second (FPS) compared to the maximum 1.16 FPS attained by the full-spectrum SVM classifier."],"url":"http://arxiv.org/abs/2405.09955v1","category":"cs.CV"}
{"created":"2024-05-16 08:59:20","title":"Scaling convolutional neural networks achieves expert-level seizure detection in neonatal EEG","abstract":"Background: Neonatal seizures are a neurological emergency that require urgent treatment. They are hard to diagnose clinically and can go undetected if EEG monitoring is unavailable. EEG interpretation requires specialised expertise which is not widely available. Algorithms to detect EEG seizures can address this limitation but have yet to reach widespread clinical adoption.   Methods: Retrospective EEG data from 332 neonates was used to develop and validate a seizure-detection model. The model was trained and tested with a development dataset ($n=202$) that was annotated with over 12k seizure events on a per-channel basis. This dataset was used to develop a convolutional neural network (CNN) using a modern architecture and training methods. The final model was then validated on two independent multi-reviewer datasets ($n=51$ and $n=79$).   Results: Increasing dataset and model size improved model performance: Matthews correlation coefficient (MCC) and Pearson's correlation ($r$) increased by up to 50% with data scaling and up to 15% with model scaling. Over 50k hours of annotated single-channel EEG was used for training a model with 21 million parameters. State-of-the-art was achieved on an open-access dataset (MCC=0.764, $r=0.824$, and AUC=0.982). The CNN attains expert-level performance on both held-out validation sets, with no significant difference in inter-rater agreement among the experts and among experts and algorithm ($\\Delta \\kappa < -0.095$, $p>0.05$).   Conclusion: With orders of magnitude increases in data and model scale we have produced a new state-of-the-art model for neonatal seizure detection. Expert-level equivalence on completely unseen data, a first in this field, provides a strong indication that the model is ready for further clinical validation.","sentences":["Background: Neonatal seizures are a neurological emergency that require urgent treatment.","They are hard to diagnose clinically and can go undetected if EEG monitoring is unavailable.","EEG interpretation requires specialised expertise which is not widely available.","Algorithms to detect EEG seizures can address this limitation but have yet to reach widespread clinical adoption.   ","Methods: Retrospective EEG data from 332 neonates was used to develop and validate a seizure-detection model.","The model was trained and tested with a development dataset ($n=202$) that was annotated with over 12k seizure events on a per-channel basis.","This dataset was used to develop a convolutional neural network (CNN) using a modern architecture and training methods.","The final model was then validated on two independent multi-reviewer datasets ($n=51$ and $n=79$).   ","Results: Increasing dataset and model size improved model performance: Matthews correlation coefficient (MCC) and Pearson's correlation ($r$) increased by up to 50% with data scaling and up to 15% with model scaling.","Over 50k hours of annotated single-channel EEG was used for training a model with 21 million parameters.","State-of-the-art was achieved on an open-access dataset (MCC=0.764, $r=0.824$, and AUC=0.982).","The CNN attains expert-level performance on both held-out validation sets, with no significant difference in inter-rater agreement among the experts and among experts and algorithm ($\\Delta \\kappa < -0.095$, $p>0.05$).   ","Conclusion: With orders of magnitude increases in data and model scale we have produced a new state-of-the-art model for neonatal seizure detection.","Expert-level equivalence on completely unseen data, a first in this field, provides a strong indication that the model is ready for further clinical validation."],"url":"http://arxiv.org/abs/2405.09911v1","category":"cs.LG"}
{"created":"2024-05-16 08:03:41","title":"Deep Learning-Based Quasi-Conformal Surface Registration for Partial 3D Faces Applied to Facial Recognition","abstract":"3D face registration is an important process in which a 3D face model is aligned and mapped to a template face. However, the task of 3D face registration becomes particularly challenging when dealing with partial face data, where only limited facial information is available. To address this challenge, this paper presents a novel deep learning-based approach that combines quasi-conformal geometry with deep neural networks for partial face registration. The proposed framework begins with a Landmark Detection Network that utilizes curvature information to detect the presence of facial features and estimate their corresponding coordinates. These facial landmark features serve as essential guidance for the registration process. To establish a dense correspondence between the partial face and the template surface, a registration network based on quasiconformal theories is employed. The registration network establishes a bijective quasiconformal surface mapping aligning corresponding partial faces based on detected landmarks and curvature values. It consists of the Coefficients Prediction Network, which outputs the optimal Beltrami coefficient representing the surface mapping. The Beltrami coefficient quantifies the local geometric distortion of the mapping. By controlling the magnitude of the Beltrami coefficient through a suitable activation function, the bijectivity and geometric distortion of the mapping can be controlled. The Beltrami coefficient is then fed into the Beltrami solver network to reconstruct the corresponding mapping. The surface registration enables the acquisition of corresponding regions and the establishment of point-wise correspondence between different partial faces, facilitating precise shape comparison through the evaluation of point-wise geometric differences at these corresponding regions. Experimental results demonstrate the effectiveness of the proposed method.","sentences":["3D face registration is an important process in which a 3D face model is aligned and mapped to a template face.","However, the task of 3D face registration becomes particularly challenging when dealing with partial face data, where only limited facial information is available.","To address this challenge, this paper presents a novel deep learning-based approach that combines quasi-conformal geometry with deep neural networks for partial face registration.","The proposed framework begins with a Landmark Detection Network that utilizes curvature information to detect the presence of facial features and estimate their corresponding coordinates.","These facial landmark features serve as essential guidance for the registration process.","To establish a dense correspondence between the partial face and the template surface, a registration network based on quasiconformal theories is employed.","The registration network establishes a bijective quasiconformal surface mapping aligning corresponding partial faces based on detected landmarks and curvature values.","It consists of the Coefficients Prediction Network, which outputs the optimal Beltrami coefficient representing the surface mapping.","The Beltrami coefficient quantifies the local geometric distortion of the mapping.","By controlling the magnitude of the Beltrami coefficient through a suitable activation function, the bijectivity and geometric distortion of the mapping can be controlled.","The Beltrami coefficient is then fed into the Beltrami solver network to reconstruct the corresponding mapping.","The surface registration enables the acquisition of corresponding regions and the establishment of point-wise correspondence between different partial faces, facilitating precise shape comparison through the evaluation of point-wise geometric differences at these corresponding regions.","Experimental results demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.09880v1","category":"cs.CV"}
{"created":"2024-05-16 07:50:02","title":"Dual3D: Efficient and Consistent Text-to-3D Generation with Dual-mode Multi-view Latent Diffusion","abstract":"We present Dual3D, a novel text-to-3D generation framework that generates high-quality 3D assets from texts in only $1$ minute.The key component is a dual-mode multi-view latent diffusion model. Given the noisy multi-view latents, the 2D mode can efficiently denoise them with a single latent denoising network, while the 3D mode can generate a tri-plane neural surface for consistent rendering-based denoising. Most modules for both modes are tuned from a pre-trained text-to-image latent diffusion model to circumvent the expensive cost of training from scratch. To overcome the high rendering cost during inference, we propose the dual-mode toggling inference strategy to use only $1/10$ denoising steps with 3D mode, successfully generating a 3D asset in just $10$ seconds without sacrificing quality. The texture of the 3D asset can be further enhanced by our efficient texture refinement process in a short time. Extensive experiments demonstrate that our method delivers state-of-the-art performance while significantly reducing generation time. Our project page is available at https://dual3d.github.io","sentences":["We present Dual3D, a novel text-to-3D generation framework that generates high-quality 3D assets from texts in only $1$ minute.","The key component is a dual-mode multi-view latent diffusion model.","Given the noisy multi-view latents, the 2D mode can efficiently denoise them with a single latent denoising network, while the 3D mode can generate a tri-plane neural surface for consistent rendering-based denoising.","Most modules for both modes are tuned from a pre-trained text-to-image latent diffusion model to circumvent the expensive cost of training from scratch.","To overcome the high rendering cost during inference, we propose the dual-mode toggling inference strategy to use only $1/10$ denoising steps with 3D mode, successfully generating a 3D asset in just $10$ seconds without sacrificing quality.","The texture of the 3D asset can be further enhanced by our efficient texture refinement process in a short time.","Extensive experiments demonstrate that our method delivers state-of-the-art performance while significantly reducing generation time.","Our project page is available at https://dual3d.github.io"],"url":"http://arxiv.org/abs/2405.09874v1","category":"cs.CV"}
{"created":"2024-05-16 07:48:51","title":"Conformal metrics with finite total Q-curvature revisited","abstract":"Given a conformal metric with finite total Q-curvature on $\\mathbb{R}^n$ for $n\\geq4$, we show that the sign of scalar curvature near infinity control not only the upper bound but also the lower bound of Q-curvature integral which is a new phenomenon. Meanwhile, for general complete non-compact four-manifolds with simple ends, we also obtain similar control of the lower bound of Q-curvature integral.","sentences":["Given a conformal metric with finite total Q-curvature on $\\mathbb{R}^n$ for $n\\geq4$, we show that the sign of scalar curvature near infinity control not only the upper bound but also the lower bound of Q-curvature integral which is a new phenomenon.","Meanwhile, for general complete non-compact four-manifolds with simple ends, we also obtain similar control of the lower bound of Q-curvature integral."],"url":"http://arxiv.org/abs/2405.09872v1","category":"math.DG"}
{"created":"2024-05-16 07:42:39","title":"Solar multi-object multi-frame blind deconvolution with a spatially variant convolution neural emulator","abstract":"The study of astronomical phenomena through ground-based observations is always challenged by the distorting effects of Earth's atmosphere. Traditional methods of post-facto image correction, essential for correcting these distortions, often rely on simplifying assumptions that limit their effectiveness, particularly in the presence of spatially variant atmospheric turbulence. Such cases are often solved by partitioning the field-of-view into small patches, deconvolving each patch independently, and merging all patches together. This approach is often inefficient and can produce artifacts. Recent advancements in computational techniques and the advent of deep learning offer new pathways to address these limitations. This paper introduces a novel framework leveraging a deep neural network to emulate spatially variant convolutions, offering a breakthrough in the efficiency and accuracy of astronomical image deconvolution. By training on a dataset of images convolved with spatially invariant point spread functions and validating its generalizability to spatially variant conditions, this approach presents a significant advancement over traditional methods. The convolution emulator is used as a forward model in a multi-object multi-frame blind deconvolution algorithm for solar images. The emulator enables the deconvolution of solar observations across large fields of view without resorting to patch-wise mosaicking, thus avoiding artifacts associated with such techniques. This method represents a significant computational advantage, reducing processing times by orders of magnitude.","sentences":["The study of astronomical phenomena through ground-based observations is always challenged by the distorting effects of Earth's atmosphere.","Traditional methods of post-facto image correction, essential for correcting these distortions, often rely on simplifying assumptions that limit their effectiveness, particularly in the presence of spatially variant atmospheric turbulence.","Such cases are often solved by partitioning the field-of-view into small patches, deconvolving each patch independently, and merging all patches together.","This approach is often inefficient and can produce artifacts.","Recent advancements in computational techniques and the advent of deep learning offer new pathways to address these limitations.","This paper introduces a novel framework leveraging a deep neural network to emulate spatially variant convolutions, offering a breakthrough in the efficiency and accuracy of astronomical image deconvolution.","By training on a dataset of images convolved with spatially invariant point spread functions and validating its generalizability to spatially variant conditions, this approach presents a significant advancement over traditional methods.","The convolution emulator is used as a forward model in a multi-object multi-frame blind deconvolution algorithm for solar images.","The emulator enables the deconvolution of solar observations across large fields of view without resorting to patch-wise mosaicking, thus avoiding artifacts associated with such techniques.","This method represents a significant computational advantage, reducing processing times by orders of magnitude."],"url":"http://arxiv.org/abs/2405.09864v1","category":"astro-ph.IM"}
{"created":"2024-05-16 07:30:47","title":"Risk-Sensitive Online Algorithms","abstract":"We initiate the study of risk-sensitive online algorithms, in which risk measures are used in the competitive analysis of randomized online algorithms. We introduce the CVaR$_\\delta$-competitive ratio ($\\delta$-CR) using the conditional value-at-risk of an algorithm's cost, which measures the expectation of the $(1-\\delta)$-fraction of worst outcomes against the offline optimal cost, and use this measure to study three online optimization problems: continuous-time ski rental, discrete-time ski rental, and one-max search. The structure of the optimal $\\delta$-CR and algorithm varies significantly between problems: we prove that the optimal $\\delta$-CR for continuous-time ski rental is $2-2^{-\\Theta(\\frac{1}{1-\\delta})}$, obtained by an algorithm described by a delay differential equation. In contrast, in discrete-time ski rental with buying cost $B$, there is an abrupt phase transition at $\\delta = 1 - \\Theta(\\frac{1}{\\log B})$, after which the classic deterministic strategy is optimal. Similarly, one-max search exhibits a phase transition at $\\delta = \\frac{1}{2}$, after which the classic deterministic strategy is optimal; we also obtain an algorithm that is asymptotically optimal as $\\delta \\downarrow 0$ that arises as the solution to a delay differential equation.","sentences":["We initiate the study of risk-sensitive online algorithms, in which risk measures are used in the competitive analysis of randomized online algorithms.","We introduce the CVaR$_\\delta$-competitive ratio ($\\delta$-CR) using the conditional value-at-risk of an algorithm's cost, which measures the expectation of the $(1-\\delta)$-fraction of worst outcomes against the offline optimal cost, and use this measure to study three online optimization problems: continuous-time ski rental, discrete-time ski rental, and one-max search.","The structure of the optimal $\\delta$-CR and algorithm varies significantly between problems: we prove that the optimal $\\delta$-CR for continuous-time ski rental is $2-2^{-\\Theta(\\frac{1}{1-\\delta})}$, obtained by an algorithm described by a delay differential equation.","In contrast, in discrete-time ski rental with buying cost $B$, there is an abrupt phase transition at $\\delta = 1 - \\Theta(\\frac{1}{\\log B})$, after which the classic deterministic strategy is optimal.","Similarly, one-max search exhibits a phase transition at $\\delta = \\frac{1}{2}$, after which the classic deterministic strategy is optimal; we also obtain an algorithm that is asymptotically optimal as $\\delta \\downarrow 0$ that arises as the solution to a delay differential equation."],"url":"http://arxiv.org/abs/2405.09859v1","category":"cs.DS"}
{"created":"2024-05-16 06:20:15","title":"A high-order Eulerian-Lagrangian Runge-Kutta finite volume (EL-RK-FV) method for scalar conservation laws","abstract":"We present a class of high-order Eulerian-Lagrangian Runge-Kutta finite volume methods that can numerically solve Burgers' equation with shock formations, which could be extended to general scalar conservation laws. Eulerian-Lagrangian (EL) and semi-Lagrangian (SL) methods have recently seen increased development and have become a staple for allowing large time-stepping sizes. Yet, maintaining relatively large time-stepping sizes post shock formation remains quite challenging. Our proposed scheme integrates the partial differential equation on a space-time region partitioned by linear approximations to the characteristics determined by the Rankine-Hugoniot jump condition. We trace the characteristics forward in time and present a merging procedure for the mesh cells to handle intersecting characteristics due to shocks. Following this partitioning, we write the equation in a time-differential form and evolve with Runge-Kutta methods in a method-of-lines fashion. High-resolution methods such as ENO and WENO-AO schemes are used for spatial reconstruction. Extension to higher dimensions is done via dimensional splitting. Numerical experiments demonstrate our scheme's high-order accuracy and ability to sharply capture post-shock solutions with large time-stepping sizes.","sentences":["We present a class of high-order Eulerian-Lagrangian Runge-Kutta finite volume methods that can numerically solve Burgers' equation with shock formations, which could be extended to general scalar conservation laws.","Eulerian-Lagrangian (EL) and semi-Lagrangian (SL) methods have recently seen increased development and have become a staple for allowing large time-stepping sizes.","Yet, maintaining relatively large time-stepping sizes post shock formation remains quite challenging.","Our proposed scheme integrates the partial differential equation on a space-time region partitioned by linear approximations to the characteristics determined by the Rankine-Hugoniot jump condition.","We trace the characteristics forward in time and present a merging procedure for the mesh cells to handle intersecting characteristics due to shocks.","Following this partitioning, we write the equation in a time-differential form and evolve with Runge-Kutta methods in a method-of-lines fashion.","High-resolution methods such as ENO and WENO-AO schemes are used for spatial reconstruction.","Extension to higher dimensions is done via dimensional splitting.","Numerical experiments demonstrate our scheme's high-order accuracy and ability to sharply capture post-shock solutions with large time-stepping sizes."],"url":"http://arxiv.org/abs/2405.09835v1","category":"math.NA"}
{"created":"2024-05-16 06:14:49","title":"Quantum cosmology as automorphic dynamics","abstract":"We revisit pure quantum cosmology in three dimensions. The Wheeler-DeWitt equation can be solved perturbatively and the dynamics reduces to a particle on moduli space. Its time evolution is equivalent to the $T\\overline{T}$ deformation. Focusing on spacetimes with torus slices, we show that inflationary cosmologies correspond to particle trajectories in Artin's billiard. The resulting automorphic dynamics is developed both from a first and second quantized perspectives. Our main application is to give an interpretation for the Hartle-Hawking state which is here the analytic continuation of the Maloney-Witten partition function. We obtain its spectral decomposition and an exact representation as an average involving the M\\\"obius function.","sentences":["We revisit pure quantum cosmology in three dimensions.","The Wheeler-DeWitt equation can be solved perturbatively and the dynamics reduces to a particle on moduli space.","Its time evolution is equivalent to the $T\\overline{T}$ deformation.","Focusing on spacetimes with torus slices, we show that inflationary cosmologies correspond to particle trajectories in Artin's billiard.","The resulting automorphic dynamics is developed both from a first and second quantized perspectives.","Our main application is to give an interpretation for the Hartle-Hawking state which is here the analytic continuation of the Maloney-Witten partition function.","We obtain its spectral decomposition and an exact representation as an average involving the M\\\"obius function."],"url":"http://arxiv.org/abs/2405.09833v1","category":"hep-th"}
{"created":"2024-05-16 05:56:03","title":"Parallel Backpropagation for Shared-Feature Visualization","abstract":"High-level visual brain regions contain subareas in which neurons appear to respond more strongly to examples of a particular semantic category, like faces or bodies, rather than objects. However, recent work has shown that while this finding holds on average, some out-of-category stimuli also activate neurons in these regions. This may be due to visual features common among the preferred class also being present in other images. Here, we propose a deep-learning-based approach for visualizing these features. For each neuron, we identify relevant visual features driving its selectivity by modelling responses to images based on latent activations of a deep neural network. Given an out-of-category image which strongly activates the neuron, our method first identifies a reference image from the preferred category yielding a similar feature activation pattern. We then backpropagate latent activations of both images to the pixel level, while enhancing the identified shared dimensions and attenuating non-shared features. The procedure highlights image regions containing shared features driving responses of the model neuron. We apply the algorithm to novel recordings from body-selective regions in macaque IT cortex in order to understand why some images of objects excite these neurons. Visualizations reveal object parts which resemble parts of a macaque body, shedding light on neural preference of these objects.","sentences":["High-level visual brain regions contain subareas in which neurons appear to respond more strongly to examples of a particular semantic category, like faces or bodies, rather than objects.","However, recent work has shown that while this finding holds on average, some out-of-category stimuli also activate neurons in these regions.","This may be due to visual features common among the preferred class also being present in other images.","Here, we propose a deep-learning-based approach for visualizing these features.","For each neuron, we identify relevant visual features driving its selectivity by modelling responses to images based on latent activations of a deep neural network.","Given an out-of-category image which strongly activates the neuron, our method first identifies a reference image from the preferred category yielding a similar feature activation pattern.","We then backpropagate latent activations of both images to the pixel level, while enhancing the identified shared dimensions and attenuating non-shared features.","The procedure highlights image regions containing shared features driving responses of the model neuron.","We apply the algorithm to novel recordings from body-selective regions in macaque IT cortex in order to understand why some images of objects excite these neurons.","Visualizations reveal object parts which resemble parts of a macaque body, shedding light on neural preference of these objects."],"url":"http://arxiv.org/abs/2405.09827v1","category":"cs.CV"}
{"created":"2024-05-16 05:19:18","title":"On the scalar curvature rigidity for mainifolds with non-positive Yamabe invariant","abstract":"In this paper, we study scalar curvature rigidity of non-smooth metrics on smooth manifolds with non-positive Yamabe invariant. We prove that if the scalar curvature is not less than the Yamabe invariant in distributional sense, then the manifold must be isometric to an Einstein manifold. This result extends Theorem 1.4 in Jiang, Sheng and the first author (Sci. China Math. 66 (2023) no. 6, 1141-1160), from a special case where the manifolds have zero Yamabe invariant to general cases where the manifolds have non-positive Yamabe invariant. This result depends highly on an analysis and estimates of geometric evolution equations.","sentences":["In this paper, we study scalar curvature rigidity of non-smooth metrics on smooth manifolds with non-positive Yamabe invariant.","We prove that if the scalar curvature is not less than the Yamabe invariant in distributional sense, then the manifold must be isometric to an Einstein manifold.","This result extends Theorem 1.4 in Jiang, Sheng and the first author (Sci. China Math. 66 (2023) no. 6, 1141-1160), from a special case where the manifolds have zero Yamabe invariant to general cases where the manifolds have non-positive Yamabe invariant.","This result depends highly on an analysis and estimates of geometric evolution equations."],"url":"http://arxiv.org/abs/2405.09816v1","category":"math.AP"}
{"created":"2024-05-16 02:14:58","title":"Compact quantum algorithms that can potentially maintain quantum advantage for solving time-dependent differential equations","abstract":"Many claims of computational advantages have been made for quantum computing over classical, but they have not been demonstrated for practical problems. Here, we present algorithms for solving time-dependent PDEs governing fluid flow problems. We build on an idea based on linear combination of unitaries to simulate non-unitary, non-Hermitian quantum systems, and generate hybrid quantum-classical algorithms that efficiently perform iterative matrix-vector multiplication and matrix inversion operations. These algorithms lead to low-depth quantum circuits that protect quantum advantage, with the best-case asymptotic complexities that are near-optimal. We demonstrate the performance of the algorithms by conducting: (a) ideal state-vector simulations using an in-house, high performance, quantum simulator called $\\textit{QFlowS}$; (b) experiments on a real quantum device (IBM Cairo); and (c) noisy simulations using Qiskit Aer. We also provide device specifications such as error-rates (noise) and state sampling (measurement) to accurately perform convergent flow simulations on noisy devices.","sentences":["Many claims of computational advantages have been made for quantum computing over classical, but they have not been demonstrated for practical problems.","Here, we present algorithms for solving time-dependent PDEs governing fluid flow problems.","We build on an idea based on linear combination of unitaries to simulate non-unitary, non-Hermitian quantum systems, and generate hybrid quantum-classical algorithms that efficiently perform iterative matrix-vector multiplication and matrix inversion operations.","These algorithms lead to low-depth quantum circuits that protect quantum advantage, with the best-case asymptotic complexities that are near-optimal.","We demonstrate the performance of the algorithms by conducting: (a) ideal state-vector simulations using an in-house, high performance, quantum simulator called $\\textit{QFlowS}$; (b) experiments on a real quantum device (IBM Cairo); and (c) noisy simulations using Qiskit Aer.","We also provide device specifications such as error-rates (noise) and state sampling (measurement) to accurately perform convergent flow simulations on noisy devices."],"url":"http://arxiv.org/abs/2405.09767v1","category":"quant-ph"}
{"created":"2024-05-16 02:00:10","title":"Spatial-temporal manipulations of visible nanosecond sub-pulse sequences in an actively Q-switched Pr:YLF laser","abstract":"Pulsed visible lasers either by Q-switching or mode locking have been attracting intense attentions both in solid-state laser and fiber laser. Here, we report on the simultaneous manipulation of reconfigurable sub-pulse sequences and customizable high-order vortex beams in an actively Q-switched visible laser. On the one hand, pulse sequences with up to 4 sub-pulses could be generated and fully controlled by means of an acoustic-optic modulator driven by an arbitrary waveform generator. Both pulse number and pulse intensity can be manipulated through the programmable step-signal, which is also theoretically simulated through the rate equations. On the other hand, assisted by the off-axis pumping technique and the astigmatic mode conversion, the laser cavity could emit high-quality vortex beams carrying Laguerre-Gaussian modes up to 30th order. To the best of our knowledge, this is the most flexible active manipulations not only on the intensity distribution of the transverse modes but also on the temporal distribution of the pulse sequences in a visible laser. The versatile manipulating techniques in this work could be immediately implemented into all other solid-state lasers to obtain sub-pulse vortex beams, which may provide enhanced functionality and flexibility for a large range of laser systems.","sentences":["Pulsed visible lasers either by Q-switching or mode locking have been attracting intense attentions both in solid-state laser and fiber laser.","Here, we report on the simultaneous manipulation of reconfigurable sub-pulse sequences and customizable high-order vortex beams in an actively Q-switched visible laser.","On the one hand, pulse sequences with up to 4 sub-pulses could be generated and fully controlled by means of an acoustic-optic modulator driven by an arbitrary waveform generator.","Both pulse number and pulse intensity can be manipulated through the programmable step-signal, which is also theoretically simulated through the rate equations.","On the other hand, assisted by the off-axis pumping technique and the astigmatic mode conversion, the laser cavity could emit high-quality vortex beams carrying Laguerre-Gaussian modes up to 30th order.","To the best of our knowledge, this is the most flexible active manipulations not only on the intensity distribution of the transverse modes but also on the temporal distribution of the pulse sequences in a visible laser.","The versatile manipulating techniques in this work could be immediately implemented into all other solid-state lasers to obtain sub-pulse vortex beams, which may provide enhanced functionality and flexibility for a large range of laser systems."],"url":"http://arxiv.org/abs/2405.09758v1","category":"physics.optics"}
{"created":"2024-05-16 01:45:55","title":"An Autoencoder and Generative Adversarial Networks Approach for Multi-Omics Data Imbalanced Class Handling and Classification","abstract":"In the relentless efforts in enhancing medical diagnostics, the integration of state-of-the-art machine learning methodologies has emerged as a promising research area. In molecular biology, there has been an explosion of data generated from multi-omics sequencing. The advent sequencing equipment can provide large number of complicated measurements per one experiment. Therefore, traditional statistical methods face challenging tasks when dealing with such high dimensional data. However, most of the information contained in these datasets is redundant or unrelated and can be effectively reduced to significantly fewer variables without losing much information. Dimensionality reduction techniques are mathematical procedures that allow for this reduction; they have largely been developed through statistics and machine learning disciplines. The other challenge in medical datasets is having an imbalanced number of samples in the classes, which leads to biased results in machine learning models. This study, focused on tackling these challenges in a neural network that incorporates autoencoder to extract latent space of the features, and Generative Adversarial Networks (GAN) to generate synthetic samples. Latent space is the reduced dimensional space that captures the meaningful features of the original data. Our model starts with feature selection to select the discriminative features before feeding them to the neural network. Then, the model predicts the outcome of cancer for different datasets. The proposed model outperformed other existing models by scoring accuracy of 95.09% for bladder cancer dataset and 88.82% for the breast cancer dataset.","sentences":["In the relentless efforts in enhancing medical diagnostics, the integration of state-of-the-art machine learning methodologies has emerged as a promising research area.","In molecular biology, there has been an explosion of data generated from multi-omics sequencing.","The advent sequencing equipment can provide large number of complicated measurements per one experiment.","Therefore, traditional statistical methods face challenging tasks when dealing with such high dimensional data.","However, most of the information contained in these datasets is redundant or unrelated and can be effectively reduced to significantly fewer variables without losing much information.","Dimensionality reduction techniques are mathematical procedures that allow for this reduction; they have largely been developed through statistics and machine learning disciplines.","The other challenge in medical datasets is having an imbalanced number of samples in the classes, which leads to biased results in machine learning models.","This study, focused on tackling these challenges in a neural network that incorporates autoencoder to extract latent space of the features, and Generative Adversarial Networks (GAN) to generate synthetic samples.","Latent space is the reduced dimensional space that captures the meaningful features of the original data.","Our model starts with feature selection to select the discriminative features before feeding them to the neural network.","Then, the model predicts the outcome of cancer for different datasets.","The proposed model outperformed other existing models by scoring accuracy of 95.09% for bladder cancer dataset and 88.82% for the breast cancer dataset."],"url":"http://arxiv.org/abs/2405.09756v1","category":"cs.LG"}
{"created":"2024-05-16 01:01:23","title":"JIGGLE: An Active Sensing Framework for Boundary Parameters Estimation in Deformable Surgical Environments","abstract":"Surgical automation can improve the accessibility and consistency of life saving procedures. Most surgeries require separating layers of tissue to access the surgical site, and suturing to reattach incisions. These tasks involve deformable manipulation to safely identify and alter tissue attachment (boundary) topology. Due to poor visual acuity and frequent occlusions, surgeons tend to carefully manipulate the tissue in ways that enable inference of the tissue's attachment points without causing unsafe tearing. In a similar fashion, we propose JIGGLE, a framework for estimation and interactive sensing of unknown boundary parameters in deformable surgical environments. This framework has two key components: (1) a probabilistic estimation to identify the current attachment points, achieved by integrating a differentiable soft-body simulator with an extended Kalman filter (EKF), and (2) an optimization-based active control pipeline that generates actions to maximize information gain of the tissue attachments, while simultaneously minimizing safety costs. The robustness of our estimation approach is demonstrated through experiments with real animal tissue, where we infer sutured attachment points using stereo endoscope observations. We also demonstrate the capabilities of our method in handling complex topological changes such as cutting and suturing.","sentences":["Surgical automation can improve the accessibility and consistency of life saving procedures.","Most surgeries require separating layers of tissue to access the surgical site, and suturing to reattach incisions.","These tasks involve deformable manipulation to safely identify and alter tissue attachment (boundary) topology.","Due to poor visual acuity and frequent occlusions, surgeons tend to carefully manipulate the tissue in ways that enable inference of the tissue's attachment points without causing unsafe tearing.","In a similar fashion, we propose JIGGLE, a framework for estimation and interactive sensing of unknown boundary parameters in deformable surgical environments.","This framework has two key components: (1) a probabilistic estimation to identify the current attachment points, achieved by integrating a differentiable soft-body simulator with an extended Kalman filter (EKF), and (2) an optimization-based active control pipeline that generates actions to maximize information gain of the tissue attachments, while simultaneously minimizing safety costs.","The robustness of our estimation approach is demonstrated through experiments with real animal tissue, where we infer sutured attachment points using stereo endoscope observations.","We also demonstrate the capabilities of our method in handling complex topological changes such as cutting and suturing."],"url":"http://arxiv.org/abs/2405.09743v1","category":"cs.RO"}
{"created":"2024-05-16 00:52:03","title":"Random Scaling and Momentum for Non-smooth Non-convex Optimization","abstract":"Training neural networks requires optimizing a loss function that may be highly irregular, and in particular neither convex nor smooth. Popular training algorithms are based on stochastic gradient descent with momentum (SGDM), for which classical analysis applies only if the loss is either convex or smooth. We show that a very small modification to SGDM closes this gap: simply scale the update at each time point by an exponentially distributed random scalar. The resulting algorithm achieves optimal convergence guarantees. Intriguingly, this result is not derived by a specific analysis of SGDM: instead, it falls naturally out of a more general framework for converting online convex optimization algorithms to non-convex optimization algorithms.","sentences":["Training neural networks requires optimizing a loss function that may be highly irregular, and in particular neither convex nor smooth.","Popular training algorithms are based on stochastic gradient descent with momentum (SGDM), for which classical analysis applies only if the loss is either convex or smooth.","We show that a very small modification to SGDM closes this gap: simply scale the update at each time point by an exponentially distributed random scalar.","The resulting algorithm achieves optimal convergence guarantees.","Intriguingly, this result is not derived by a specific analysis of SGDM: instead, it falls naturally out of a more general framework for converting online convex optimization algorithms to non-convex optimization algorithms."],"url":"http://arxiv.org/abs/2405.09742v1","category":"cs.LG"}
{"created":"2024-05-16 00:34:53","title":"Remark on the scattering theory of the nonlinear Schr\u00f6dinger equation on the cylinders","abstract":"In this article, we consider the nonlinear Schr\\\"odinger equation on the cylinder $\\mathbb{R}^d\\times \\mathbb{T}$. In the long range case, we show there is no linear scattering state of the nonlinear Schr\\\"odinger equation on $\\mathbb{R}^d \\times \\mathbb{T}$. In the short range case, we show the decay and scattering of solutions of the nonlinear Schr\\\"odinger equation on $\\mathbb{R}^d \\times \\mathbb{T}$ for small data.","sentences":["In this article, we consider the nonlinear Schr\\\"odinger equation on the cylinder $\\mathbb{R}^d\\times \\mathbb{T}$. In the long range case, we show there is no linear scattering state of the nonlinear Schr\\\"odinger equation on $\\mathbb{R}^d \\times \\mathbb{T}$.","In the short range case, we show the decay and scattering of solutions of the nonlinear Schr\\\"odinger equation on $\\mathbb{R}^d \\times \\mathbb{T}$ for small data."],"url":"http://arxiv.org/abs/2405.09740v1","category":"math.AP"}
{"created":"2024-05-15 23:51:35","title":"Dynamics of a higher-dimension Einstein-Scalar-Gauss-Bonnet cosmology","abstract":"We study the dynamics of the field equations in a five-dimensional spatially flat Friedmann-Lema\\^itre-Robertson-Walker metric in the context of a Gauss-Bonnet-Scalar field theory where the quintessence scalar field is coupled to the Gauss-Bonnet scalar. Contrary to the four-dimensional Gauss-Bonnet theory, where the Gauss-Bonnet term does not contribute to the field equations, in this five-dimensional Einstein-Scalar-Gauss-Bonnet model, the Gauss-Bonnet term contributes to the field equations even when the coupling function is a constant. Additionally, we consider a more general coupling described by a power-law function. For the scalar field potential, we consider the exponential function. For each choice of the coupling function, we define a set of dimensionless variables and write the field equations into a system of ordinary differential equations. We perform a detailed analysis of the dynamics for both systems and classify the stability of the equilibrium points. We determine the presence of scaling and super-collapsing solutions using the cosmological deceleration parameter. This means that our models can explain the Universe's early and late-time acceleration phases. Consequently, this model can be used to study inflation or as a dark energy candidate.","sentences":["We study the dynamics of the field equations in a five-dimensional spatially flat Friedmann-Lema\\^itre-Robertson-Walker metric in the context of a Gauss-Bonnet-Scalar field theory where the quintessence scalar field is coupled to the Gauss-Bonnet scalar.","Contrary to the four-dimensional Gauss-Bonnet theory, where the Gauss-Bonnet term does not contribute to the field equations, in this five-dimensional Einstein-Scalar-Gauss-Bonnet model, the Gauss-Bonnet term contributes to the field equations even when the coupling function is a constant.","Additionally, we consider a more general coupling described by a power-law function.","For the scalar field potential, we consider the exponential function.","For each choice of the coupling function, we define a set of dimensionless variables and write the field equations into a system of ordinary differential equations.","We perform a detailed analysis of the dynamics for both systems and classify the stability of the equilibrium points.","We determine the presence of scaling and super-collapsing solutions using the cosmological deceleration parameter.","This means that our models can explain the Universe's early and late-time acceleration phases.","Consequently, this model can be used to study inflation or as a dark energy candidate."],"url":"http://arxiv.org/abs/2405.09732v1","category":"gr-qc"}
{"created":"2024-05-15 23:18:19","title":"Boundary layer expansions of the steady MHD equations in a bounded domain","abstract":"In this paper, we investigate the validity of boundary layer expansions for the MHD system in a rectangle. We describe the solution up to the third order when the tangential magnetic field is much smaller or much larger than the tangential velocity field, thereby extending \\cite{u1.17}.","sentences":["In this paper, we investigate the validity of boundary layer expansions for the MHD system in a rectangle.","We describe the solution up to the third order when the tangential magnetic field is much smaller or much larger than the tangential velocity field, thereby extending \\cite{u1.17}."],"url":"http://arxiv.org/abs/2405.09726v1","category":"math.AP"}
{"created":"2024-05-15 23:07:36","title":"Optical Properties of Gated Bilayer Graphene Quantum Dots with Trigonal Warping","abstract":"We determine the optical properties of gated bilayer graphene quantum dots with trigonal warping (TW) of single-particle energy spectra. The lateral structure of metallic gates confines electrons and holes in a quantum dot (QD) electrostatically. The gated bilayer graphene energy spectrum is characterized by two K-valleys surrounded by three minivalleys with energies depending on the applied vertical electric field. Employing an atomistic tight-binding model, we compute the single-particle QD states and analyze the influence of TW on the energy spectrum as the lateral confining potential depth varies. We find a regime where the QD levels are dominated by the presence of three minivalleys around each K-valley. Next, we compute dipole matrix elements and analyze the oscillator strengths and optical selection rules for optical valence to conduction band transitions. We then include electron-electron interactions by first computing the microscopic Coulomb matrix elements, electron self-energy, and solving the Bethe-Salpeter equation to obtain the excitonic spectrum. Finally, we obtain the absorption spectrum for a shallow confining potential depth, which further amplifies the effects of TW on the optical properties. Our results predict the existence of two degenerate bright exciton states, each built of the three minivalley states that do not exist in the deep confinement regime, where the effects of TW are negligible.","sentences":["We determine the optical properties of gated bilayer graphene quantum dots with trigonal warping (TW) of single-particle energy spectra.","The lateral structure of metallic gates confines electrons and holes in a quantum dot (QD) electrostatically.","The gated bilayer graphene energy spectrum is characterized by two K-valleys surrounded by three minivalleys with energies depending on the applied vertical electric field.","Employing an atomistic tight-binding model, we compute the single-particle QD states and analyze the influence of TW on the energy spectrum as the lateral confining potential depth varies.","We find a regime where the QD levels are dominated by the presence of three minivalleys around each K-valley.","Next, we compute dipole matrix elements and analyze the oscillator strengths and optical selection rules for optical valence to conduction band transitions.","We then include electron-electron interactions by first computing the microscopic Coulomb matrix elements, electron self-energy, and solving the Bethe-Salpeter equation to obtain the excitonic spectrum.","Finally, we obtain the absorption spectrum for a shallow confining potential depth, which further amplifies the effects of TW on the optical properties.","Our results predict the existence of two degenerate bright exciton states, each built of the three minivalley states that do not exist in the deep confinement regime, where the effects of TW are negligible."],"url":"http://arxiv.org/abs/2405.09725v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 22:18:39","title":"From NeRFs to Gaussian Splats, and Back","abstract":"For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch.","sentences":["For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs.","We develop a procedure to convert back and forth between the two.","Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch."],"url":"http://arxiv.org/abs/2405.09717v1","category":"cs.CV"}
{"created":"2024-05-15 21:54:23","title":"Deep Learning-Enabled One-Bit DoA Estimation","abstract":"Unrolled deep neural networks have attracted significant attention for their success in various practical applications. In this paper, we explore an application of deep unrolling in the direction of arrival (DoA) estimation problem when coarse quantization is applied to the measurements. We present a compressed sensing formulation for DoA estimation from one-bit data in which estimating target DoAs requires recovering a sparse signal from a limited number of severely quantized linear measurements. In particular, we exploit covariance recovery from one-bit dither samples. To recover the covariance of transmitted signal, the learned iterative shrinkage and thresholding algorithm (LISTA) is employed fed by one-bit data. We demonstrate that the upper bound of estimation performance is governed by the recovery error of the transmitted signal covariance matrix. Through numerical experiments, we demonstrate the proposed LISTA-based algorithm's capability in estimating target locations. The code employed in this study is available online.","sentences":["Unrolled deep neural networks have attracted significant attention for their success in various practical applications.","In this paper, we explore an application of deep unrolling in the direction of arrival (DoA) estimation problem when coarse quantization is applied to the measurements.","We present a compressed sensing formulation for DoA estimation from one-bit data in which estimating target DoAs requires recovering a sparse signal from a limited number of severely quantized linear measurements.","In particular, we exploit covariance recovery from one-bit dither samples.","To recover the covariance of transmitted signal, the learned iterative shrinkage and thresholding algorithm (LISTA) is employed fed by one-bit data.","We demonstrate that the upper bound of estimation performance is governed by the recovery error of the transmitted signal covariance matrix.","Through numerical experiments, we demonstrate the proposed LISTA-based algorithm's capability in estimating target locations.","The code employed in this study is available online."],"url":"http://arxiv.org/abs/2405.09712v1","category":"eess.SP"}
{"created":"2024-05-15 20:58:28","title":"Black bounces in conformal Killing gravity","abstract":"In this work, we analyse black bounce solutions in the recently proposed ``Conformal Killing gravity'' (CKG), by coupling the theory to nonlinear electrodynamics (NLED) and scalar fields. The original motivation of the theory was essentially to fulfil specific criteria that are absent in existing gravitational theories, namely, to obtain the cosmological constant as an integration constant, derive the energy-momentum conservation law as a consequence of the gravitational field equations, rather than assuming it, and not necessarily considering conformally flat metrics as vacuum solutions. In this work, we extend the static and spherically symmetric solutions obtained in the literature, and explore the possibility of black bounces in CKG, coupled to NLED and scalar fields. We find novel NLED Lagrangian densities and scalar potentials, and extend the class of black bounce solutions found in the literature. Furthermore, within black bounce geometries, we find generalizations of the Bardeen-type and Simpson-Visser geometries and explore the regularity conditions of the solutions.","sentences":["In this work, we analyse black bounce solutions in the recently proposed ``Conformal Killing gravity'' (CKG), by coupling the theory to nonlinear electrodynamics (NLED) and scalar fields.","The original motivation of the theory was essentially to fulfil specific criteria that are absent in existing gravitational theories, namely, to obtain the cosmological constant as an integration constant, derive the energy-momentum conservation law as a consequence of the gravitational field equations, rather than assuming it, and not necessarily considering conformally flat metrics as vacuum solutions.","In this work, we extend the static and spherically symmetric solutions obtained in the literature, and explore the possibility of black bounces in CKG, coupled to NLED and scalar fields.","We find novel NLED Lagrangian densities and scalar potentials, and extend the class of black bounce solutions found in the literature.","Furthermore, within black bounce geometries, we find generalizations of the Bardeen-type and Simpson-Visser geometries and explore the regularity conditions of the solutions."],"url":"http://arxiv.org/abs/2405.09702v1","category":"gr-qc"}
{"created":"2024-05-15 20:51:15","title":"Mapping Differential Protein-Protein Interaction Networks using Affinity Purification Mass Spectrometry","abstract":"Proteins congregate into complexes to perform fundamental cellular functions. Phenotypic outcomes, in health and disease, are often mechanistically driven by the remodeling of protein complexes by protein coding mutations or cellular signaling changes in response to molecular cues. Here, we present an affinity purification mass spectrometry (APMS) proteomics protocol to quantify and visualize global changes in protein protein interaction (PPI) networks between pairwise conditions. We describe steps for expressing affinity tagged bait proteins in mammalian cells, identifying purified protein complexes, quantifying differential PPIs, and visualizing differential PPI networks. Specifically, this protocol details steps for designing affinity tagged bait gene constructs, transfection, affinity purification, mass spectrometry sample preparation, data acquisition, database search, data quality control, PPI confidence scoring, cross run normalization, statistical data analysis, and differential PPI visualization. Our protocol discusses caveats and limitations with applicability across cell types and biological areas.","sentences":["Proteins congregate into complexes to perform fundamental cellular functions.","Phenotypic outcomes, in health and disease, are often mechanistically driven by the remodeling of protein complexes by protein coding mutations or cellular signaling changes in response to molecular cues.","Here, we present an affinity purification mass spectrometry (APMS) proteomics protocol to quantify and visualize global changes in protein protein interaction (PPI) networks between pairwise conditions.","We describe steps for expressing affinity tagged bait proteins in mammalian cells, identifying purified protein complexes, quantifying differential PPIs, and visualizing differential PPI networks.","Specifically, this protocol details steps for designing affinity tagged bait gene constructs, transfection, affinity purification, mass spectrometry sample preparation, data acquisition, database search, data quality control, PPI confidence scoring, cross run normalization, statistical data analysis, and differential PPI visualization.","Our protocol discusses caveats and limitations with applicability across cell types and biological areas."],"url":"http://arxiv.org/abs/2405.09699v1","category":"q-bio.QM"}
{"created":"2024-05-15 19:39:32","title":"How Stellar Stream Torsion may reveal aspherical Dark Matter Haloes","abstract":"Flat rotation curves follow from elongated Dark Matter distributions, as shown by our earlier competitive fits to the SPARC database. Intending to probe that distortion of the DM halo one needs observables not contained by the galactic plane. Stellar streams are caused by tidal stretching of massive substructures such as satellite dwarf galaxies, and would lie on a plane should the DM-halo gravitational field be spherically symmetric. But if the field does not display such spherical symmetry, stellar trajectories, as well as stellar streams, should torsion out of the plane. This is where the torsion of the stream can be of use: it is a local observable that measures the deviation from planarity of a curve; thus, it quantifies how noncentral the gravitational potential is. We have performed small simulations to confirm that indeed a galactic central force produces negligible torsion, and quantified the torsion for prolate haloes instead. Examining observational data, we select several streams at large distance from the galactic center, as most promising for the study, and by means of helicoidal fits extract their differential torsion. We see that their torsion is much larger than expected for a central spherical bulb alone, pointing to an elongated Milky Way halo.","sentences":["Flat rotation curves follow from elongated Dark Matter distributions, as shown by our earlier competitive fits to the SPARC database.","Intending to probe that distortion of the DM halo one needs observables not contained by the galactic plane.","Stellar streams are caused by tidal stretching of massive substructures such as satellite dwarf galaxies, and would lie on a plane should the DM-halo gravitational field be spherically symmetric.","But if the field does not display such spherical symmetry, stellar trajectories, as well as stellar streams, should torsion out of the plane.","This is where the torsion of the stream can be of use: it is a local observable that measures the deviation from planarity of a curve; thus, it quantifies how noncentral the gravitational potential is.","We have performed small simulations to confirm that indeed a galactic central force produces negligible torsion, and quantified the torsion for prolate haloes instead.","Examining observational data, we select several streams at large distance from the galactic center, as most promising for the study, and by means of helicoidal fits extract their differential torsion.","We see that their torsion is much larger than expected for a central spherical bulb alone, pointing to an elongated Milky Way halo."],"url":"http://arxiv.org/abs/2405.09678v1","category":"astro-ph.GA"}
{"created":"2024-05-15 18:53:11","title":"Higgs and precision physics at CMS","abstract":"Since the discovery of the Higgs boson in 2012, significant progress has been made in measuring several properties related to the Higgs boson. The large dataset available now facilitates precise measurements of the Higgs boson mass, natural width, couplings, production cross sections, and even differential and fiducial cross sections. The latest precision measurements performed by the CMS experiment in the Higgs sector are presented in this note.","sentences":["Since the discovery of the Higgs boson in 2012, significant progress has been made in measuring several properties related to the Higgs boson.","The large dataset available now facilitates precise measurements of the Higgs boson mass, natural width, couplings, production cross sections, and even differential and fiducial cross sections.","The latest precision measurements performed by the CMS experiment in the Higgs sector are presented in this note."],"url":"http://arxiv.org/abs/2405.09658v1","category":"hep-ex"}
{"created":"2024-05-15 18:01:03","title":"Dynamical coupling of Keplerian orbits in a hierarchical four-body system: from the Galactic Centre to compact planetary systems","abstract":"This study focuses on the long-term evolution of two bodies in nearby initially coplanar orbits around a central dominant body perturbed by a fourth body on a distant Keplerian orbit. Our previous works that considered this setup enforced circular orbits by adding a spherical potential of extended mass, which dampens Kozai--Lidov oscillations; it led to two qualitatively different modes of the evolution of the nearby orbits. In one scenario, their mutual interaction exceeds the effect of differential precession caused by a perturbing body. This results in a long-term coherent evolution, with nearly coplanar orbits experiencing only small oscillations of inclination. We extend the previous work by (i) considering post-Newtonian corrections to the gravity of the central body, either instead of or in addition to the potential of extended mass, (ii) relaxing the requirement of strictly circular orbits, and (iii) removing the strict requirement of complete Kozai--Lidov damping. Thus, we identify the modes of inter-orbital interaction described for the zero-eccentricity case in the more general situation, which allows for its applicability to a much broader range of astrophysical systems than considered initially. In this work, we scale the systems to the orbits of S-stars; we consider the clockwise disc to represent the perturbing body, with post-Newtonian corrections to the gravity of Sagittarius A* playing the role of damping potential. Considering post-Newtonian corrections, even stellar-mass central bodies in compact planetary systems can allow for the coupled evolution of Keplerian orbits.","sentences":["This study focuses on the long-term evolution of two bodies in nearby initially coplanar orbits around a central dominant body perturbed by a fourth body on a distant Keplerian orbit.","Our previous works that considered this setup enforced circular orbits by adding a spherical potential of extended mass, which dampens Kozai--Lidov oscillations; it led to two qualitatively different modes of the evolution of the nearby orbits.","In one scenario, their mutual interaction exceeds the effect of differential precession caused by a perturbing body.","This results in a long-term coherent evolution, with nearly coplanar orbits experiencing only small oscillations of inclination.","We extend the previous work by (i) considering post-Newtonian corrections to the gravity of the central body, either instead of or in addition to the potential of extended mass, (ii) relaxing the requirement of strictly circular orbits, and (iii) removing the strict requirement of complete Kozai--Lidov damping.","Thus, we identify the modes of inter-orbital interaction described for the zero-eccentricity case in the more general situation, which allows for its applicability to a much broader range of astrophysical systems than considered initially.","In this work, we scale the systems to the orbits of S-stars; we consider the clockwise disc to represent the perturbing body, with post-Newtonian corrections to the gravity of Sagittarius A* playing the role of damping potential.","Considering post-Newtonian corrections, even stellar-mass central bodies in compact planetary systems can allow for the coupled evolution of Keplerian orbits."],"url":"http://arxiv.org/abs/2405.09630v1","category":"astro-ph.GA"}
{"created":"2024-05-15 18:00:04","title":"Electron heating in high Mach number collisionless shocks","abstract":"The energy partition in high Mach number collisionless shock waves is central to a wide range of high-energy astrophysical environments. We present a new theoretical model for electron heating that accounts for the energy exchange between electrons and ions at the shock. The fundamental mechanism relies on the difference in inertia between electrons and ions, resulting in differential scattering of the particles off a decelerating magnetically-dominated microturbulence across the shock transition. We show that the self-consistent interplay between the resulting ambipolar-type electric field and diffusive transport of electrons leads to efficient heating in the magnetic field produced by the Weibel instability in the high-Mach number regime and is consistent with fully kinetic simulations.","sentences":["The energy partition in high Mach number collisionless shock waves is central to a wide range of high-energy astrophysical environments.","We present a new theoretical model for electron heating that accounts for the energy exchange between electrons and ions at the shock.","The fundamental mechanism relies on the difference in inertia between electrons and ions, resulting in differential scattering of the particles off a decelerating magnetically-dominated microturbulence across the shock transition.","We show that the self-consistent interplay between the resulting ambipolar-type electric field and diffusive transport of electrons leads to efficient heating in the magnetic field produced by the Weibel instability in the high-Mach number regime and is consistent with fully kinetic simulations."],"url":"http://arxiv.org/abs/2405.09618v1","category":"astro-ph.HE"}
{"created":"2024-05-15 18:00:02","title":"Positivity bounds on electromagnetic properties of media","abstract":"We study the constraints imposed on the electromagnetic response of general media by microcausality (commutators of local fields vanish outside the light cone) and positivity of the imaginary parts (the medium can only absorb energy from the external field). The equations of motion for the average electromagnetic field in a medium -- the macroscopic Maxwell equations -- can be derived from the in-in effective action and the effect of the medium is encoded in the electric and magnetic permeabilities $\\varepsilon(\\omega,|\\boldsymbol{k}|)$ and $\\mu(\\omega,|\\boldsymbol{k}|)$. Microcausality implies analyticity of the retarded Green's functions when the imaginary part of the $4$-vector $(\\omega,\\boldsymbol{k})$ lies in forward light cone. With appropriate assumptions about the behavior of the medium at high frequencies one derives dispersion relations, originally studied by Leontovich. In the case of dielectrics these relations, combined with the positivity of the imaginary parts, imply bounds on the low-energy values of the response, $\\varepsilon(0,0)$ and $\\mu(0,0)$. In particular the quantities $\\varepsilon(0,0)-1$ and $\\varepsilon(0,0) - 1/\\mu(0,0)$ are constrained to be positive and equal to integrals over the imaginary parts of the response. We discuss various improvements of these bounds in the case of non-relativistic media and with additional assumptions about the UV behavior.","sentences":["We study the constraints imposed on the electromagnetic response of general media by microcausality (commutators of local fields vanish outside the light cone) and positivity of the imaginary parts (the medium can only absorb energy from the external field).","The equations of motion for the average electromagnetic field in a medium -- the macroscopic Maxwell equations -- can be derived from the in-in effective action and the effect of the medium is encoded in the electric and magnetic permeabilities $\\varepsilon(\\omega,|\\boldsymbol{k}|)$ and $\\mu(\\omega,|\\boldsymbol{k}|)$. Microcausality implies analyticity of the retarded Green's functions when the imaginary part of the $4$-vector $(\\omega,\\boldsymbol{k})$ lies in forward light cone.","With appropriate assumptions about the behavior of the medium at high frequencies one derives dispersion relations, originally studied by Leontovich.","In the case of dielectrics these relations, combined with the positivity of the imaginary parts, imply bounds on the low-energy values of the response, $\\varepsilon(0,0)$ and $\\mu(0,0)$. In particular the quantities $\\varepsilon(0,0)-1$ and $\\varepsilon(0,0) - 1/\\mu(0,0)$ are constrained to be positive and equal to integrals over the imaginary parts of the response.","We discuss various improvements of these bounds in the case of non-relativistic media and with additional assumptions about the UV behavior."],"url":"http://arxiv.org/abs/2405.09614v1","category":"hep-th"}
{"created":"2024-05-15 18:00:01","title":"Learning 3-Manifold Triangulations","abstract":"Real 3-manifold triangulations can be uniquely represented by isomorphism signatures. Databases of these isomorphism signatures are generated for a variety of 3-manifolds and knot complements, using SnapPy and Regina, then these language-like inputs are used to train various machine learning architectures to differentiate the manifolds, as well as their Dehn surgeries, via their triangulations. Gradient saliency analysis then extracts key parts of this language-like encoding scheme from the trained models. The isomorphism signature databases are taken from the 3-manifolds' Pachner graphs, which are also generated in bulk for some selected manifolds of focus and for the subset of the SnapPy orientable cusped census with $<8$ initial tetrahedra. These Pachner graphs are further analysed through the lens of network science to identify new structure in the triangulation representation; in particular for the hyperbolic case, a relation between the length of the shortest geodesic (systole) and the size of the Pachner graph's ball is observed.","sentences":["Real 3-manifold triangulations can be uniquely represented by isomorphism signatures.","Databases of these isomorphism signatures are generated for a variety of 3-manifolds and knot complements, using SnapPy and Regina, then these language-like inputs are used to train various machine learning architectures to differentiate the manifolds, as well as their Dehn surgeries, via their triangulations.","Gradient saliency analysis then extracts key parts of this language-like encoding scheme from the trained models.","The isomorphism signature databases are taken from the 3-manifolds' Pachner graphs, which are also generated in bulk for some selected manifolds of focus and for the subset of the SnapPy orientable cusped census with $<8$ initial tetrahedra.","These Pachner graphs are further analysed through the lens of network science to identify new structure in the triangulation representation; in particular for the hyperbolic case, a relation between the length of the shortest geodesic (systole) and the size of the Pachner graph's ball is observed."],"url":"http://arxiv.org/abs/2405.09610v1","category":"math.GT"}
{"created":"2024-05-15 17:32:31","title":"Comparative Analysis of Predicting Subsequent Steps in H\u00e9non Map","abstract":"This paper explores the prediction of subsequent steps in H\\'enon Map using various machine learning techniques. The H\\'enon map, well known for its chaotic behaviour, finds applications in various fields including cryptography, image encryption, and pattern recognition. Machine learning methods, particularly deep learning, are increasingly essential for understanding and predicting chaotic phenomena. This study evaluates the performance of different machine learning models including Random Forest, Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) networks, Support Vector Machines (SVM), and Feed Forward Neural Networks (FNN) in predicting the evolution of the H\\'enon map. Results indicate that LSTM network demonstrate superior predictive accuracy, particularly in extreme event prediction. Furthermore, a comparison between LSTM and FNN models reveals the LSTM's advantage, especially for longer prediction horizons and larger datasets. This research underscores the significance of machine learning in elucidating chaotic dynamics and highlights the importance of model selection and dataset size in forecasting subsequent steps in chaotic systems.","sentences":["This paper explores the prediction of subsequent steps in H\\'enon Map using various machine learning techniques.","The H\\'enon map, well known for its chaotic behaviour, finds applications in various fields including cryptography, image encryption, and pattern recognition.","Machine learning methods, particularly deep learning, are increasingly essential for understanding and predicting chaotic phenomena.","This study evaluates the performance of different machine learning models including Random Forest, Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) networks, Support Vector Machines (SVM), and Feed Forward Neural Networks (FNN) in predicting the evolution of the H\\'enon map.","Results indicate that LSTM network demonstrate superior predictive accuracy, particularly in extreme event prediction.","Furthermore, a comparison between LSTM and FNN models reveals the LSTM's advantage, especially for longer prediction horizons and larger datasets.","This research underscores the significance of machine learning in elucidating chaotic dynamics and highlights the importance of model selection and dataset size in forecasting subsequent steps in chaotic systems."],"url":"http://arxiv.org/abs/2405.10190v1","category":"cs.LG"}
{"created":"2024-05-15 15:55:21","title":"On the existence of geodesic vector fields on closed surfaces","abstract":"We construct an example of a Riemannian metric on the 2-torus such that its universal cover does not admit global Riemann normal coordinates.","sentences":["We construct an example of a Riemannian metric on the 2-torus such that its universal cover does not admit global Riemann normal coordinates."],"url":"http://arxiv.org/abs/2405.09604v1","category":"math.DG"}
{"created":"2024-05-16 17:49:46","title":"Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees","abstract":"In nonsmooth, nonconvex stochastic optimization, understanding the uniform convergence of subdifferential mappings is crucial for analyzing stationary points of sample average approximations of risk as they approach the population risk. Yet, characterizing this convergence remains a fundamental challenge.   This work introduces a novel perspective by connecting the uniform convergence of subdifferential mappings to that of subgradient mappings as empirical risk converges to the population risk. We prove that, for stochastic weakly-convex objectives, and within any open set, a uniform bound on the convergence of subgradients -- chosen arbitrarily from the corresponding subdifferential sets -- translates to a uniform bound on the convergence of the subdifferential sets itself, measured by the Hausdorff metric.   Using this technique, we derive uniform convergence rates for subdifferential sets of stochastic convex-composite objectives. Our results do not rely on key distributional assumptions in the literature, which require the population and finite sample subdifferentials to be continuous in the Hausdorff metric, yet still provide tight convergence rates. These guarantees lead to new insights into the nonsmooth landscapes of such objectives within finite samples.","sentences":["In nonsmooth, nonconvex stochastic optimization, understanding the uniform convergence of subdifferential mappings is crucial for analyzing stationary points of sample average approximations of risk as they approach the population risk.","Yet, characterizing this convergence remains a fundamental challenge.   ","This work introduces a novel perspective by connecting the uniform convergence of subdifferential mappings to that of subgradient mappings as empirical risk converges to the population risk.","We prove that, for stochastic weakly-convex objectives, and within any open set, a uniform bound on the convergence of subgradients -- chosen arbitrarily from the corresponding subdifferential sets -- translates to a uniform bound on the convergence of the subdifferential sets itself, measured by the Hausdorff metric.   ","Using this technique, we derive uniform convergence rates for subdifferential sets of stochastic convex-composite objectives.","Our results do not rely on key distributional assumptions in the literature, which require the population and finite sample subdifferentials to be continuous in the Hausdorff metric, yet still provide tight convergence rates.","These guarantees lead to new insights into the nonsmooth landscapes of such objectives within finite samples."],"url":"http://arxiv.org/abs/2405.10289v1","category":"math.OC"}
{"created":"2024-05-16 17:45:54","title":"Quantum Vision Transformers for Quark-Gluon Classification","abstract":"We introduce a hybrid quantum-classical vision transformer architecture, notable for its integration of variational quantum circuits within both the attention mechanism and the multi-layer perceptrons. The research addresses the critical challenge of computational efficiency and resource constraints in analyzing data from the upcoming High Luminosity Large Hadron Collider, presenting the architecture as a potential solution. In particular, we evaluate our method by applying the model to multi-detector jet images from CMS Open Data. The goal is to distinguish quark-initiated from gluon-initiated jets. We successfully train the quantum model and evaluate it via numerical simulations. Using this approach, we achieve classification performance almost on par with the one obtained with the completely classical architecture, considering a similar number of parameters.","sentences":["We introduce a hybrid quantum-classical vision transformer architecture, notable for its integration of variational quantum circuits within both the attention mechanism and the multi-layer perceptrons.","The research addresses the critical challenge of computational efficiency and resource constraints in analyzing data from the upcoming High Luminosity Large Hadron Collider, presenting the architecture as a potential solution.","In particular, we evaluate our method by applying the model to multi-detector jet images from CMS Open Data.","The goal is to distinguish quark-initiated from gluon-initiated jets.","We successfully train the quantum model and evaluate it via numerical simulations.","Using this approach, we achieve classification performance almost on par with the one obtained with the completely classical architecture, considering a similar number of parameters."],"url":"http://arxiv.org/abs/2405.10284v1","category":"quant-ph"}
{"created":"2024-05-16 16:49:31","title":"Alternative ranking measures to predict international football results","abstract":"Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 World Cup and for the 2024 Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.","sentences":["Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities.","In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 World Cup and for the 2024 Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams.","More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football.","The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs."],"url":"http://arxiv.org/abs/2405.10247v1","category":"stat.AP"}
{"created":"2024-05-16 16:47:46","title":"Towards Task-Compatible Compressible Representations","abstract":"We identify an issue in multi-task learnable compression, in which a representation learned for one task does not positively contribute to the rate-distortion performance of a different task as much as expected, given the estimated amount of information available in it. We interpret this issue using the predictive $\\mathcal{V}$-information framework. In learnable scalable coding, previous work increased the utilization of side-information for input reconstruction by also rewarding input reconstruction when learning this shared representation. We evaluate the impact of this idea in the context of input reconstruction more rigorously and extended it to other computer vision tasks. We perform experiments using representations trained for object detection on COCO 2017 and depth estimation on the Cityscapes dataset, and use them to assist in image reconstruction and semantic segmentation tasks. The results show considerable improvements in the rate-distortion performance of the assisted tasks. Moreover, using the proposed representations, the performance of the base tasks are also improved. Results suggest that the proposed method induces simpler representations that are more compatible with downstream processes.","sentences":["We identify an issue in multi-task learnable compression, in which a representation learned for one task does not positively contribute to the rate-distortion performance of a different task as much as expected, given the estimated amount of information available in it.","We interpret this issue using the predictive $\\mathcal{V}$-information framework.","In learnable scalable coding, previous work increased the utilization of side-information for input reconstruction by also rewarding input reconstruction when learning this shared representation.","We evaluate the impact of this idea in the context of input reconstruction more rigorously and extended it to other computer vision tasks.","We perform experiments using representations trained for object detection on COCO 2017 and depth estimation on the Cityscapes dataset, and use them to assist in image reconstruction and semantic segmentation tasks.","The results show considerable improvements in the rate-distortion performance of the assisted tasks.","Moreover, using the proposed representations, the performance of the base tasks are also improved.","Results suggest that the proposed method induces simpler representations that are more compatible with downstream processes."],"url":"http://arxiv.org/abs/2405.10244v1","category":"cs.CV"}
{"created":"2024-05-16 11:26:20","title":"Learning BPS Spectra and the Gap Conjecture","abstract":"We explore statistical properties of BPS q-series for 3d N=2 strongly coupled supersymmetric theories that correspond to a particular family of 3-manifolds Y. We discover that gaps between exponents in the q-series are statistically more significant at the beginning of the q-series compared to gaps that appear in higher powers of q. Our observations are obtained by calculating saliencies of q-series features used as input data for principal component analysis, which is a standard example of an explainable machine learning technique that allows for a direct calculation and a better analysis of feature saliencies.","sentences":["We explore statistical properties of BPS q-series for 3d N=2 strongly coupled supersymmetric theories that correspond to a particular family of 3-manifolds Y. We discover that gaps between exponents in the q-series are statistically more significant at the beginning of the q-series compared to gaps that appear in higher powers of q.","Our observations are obtained by calculating saliencies of q-series features used as input data for principal component analysis, which is a standard example of an explainable machine learning technique that allows for a direct calculation and a better analysis of feature saliencies."],"url":"http://arxiv.org/abs/2405.09993v1","category":"hep-th"}
{"created":"2024-05-16 11:18:32","title":"A Gaussian Process Model for Ordinal Data with Applications to Chemoinformatics","abstract":"With the proliferation of screening tools for chemical testing, it is now possible to create vast databases of chemicals easily. However, rigorous statistical methodologies employed to analyse these databases are in their infancy, and further development to facilitate chemical discovery is imperative. In this paper, we present conditional Gaussian process models to predict ordinal outcomes from chemical experiments, where the inputs are chemical compounds. We implement the Tanimoto distance, a metric on the chemical space, within the covariance of the Gaussian processes to capture correlated effects in the chemical space. A novel aspect of our model is that the kernel contains a scaling parameter, a feature not previously examined in the literature, that controls the strength of the correlation between elements of the chemical space. Using molecular fingerprints, a numerical representation of a compound's location within the chemical space, we show that accounting for correlation amongst chemical compounds improves predictive performance over the uncorrelated model, where effects are assumed to be independent. Moreover, we present a genetic algorithm for the facilitation of chemical discovery and identification of important features to the compound's efficacy. A simulation study is conducted to demonstrate the suitability of the proposed methods. Our proposed methods are demonstrated on a hazard classification problem of organic solvents.","sentences":["With the proliferation of screening tools for chemical testing, it is now possible to create vast databases of chemicals easily.","However, rigorous statistical methodologies employed to analyse these databases are in their infancy, and further development to facilitate chemical discovery is imperative.","In this paper, we present conditional Gaussian process models to predict ordinal outcomes from chemical experiments, where the inputs are chemical compounds.","We implement the Tanimoto distance, a metric on the chemical space, within the covariance of the Gaussian processes to capture correlated effects in the chemical space.","A novel aspect of our model is that the kernel contains a scaling parameter, a feature not previously examined in the literature, that controls the strength of the correlation between elements of the chemical space.","Using molecular fingerprints, a numerical representation of a compound's location within the chemical space, we show that accounting for correlation amongst chemical compounds improves predictive performance over the uncorrelated model, where effects are assumed to be independent.","Moreover, we present a genetic algorithm for the facilitation of chemical discovery and identification of important features to the compound's efficacy.","A simulation study is conducted to demonstrate the suitability of the proposed methods.","Our proposed methods are demonstrated on a hazard classification problem of organic solvents."],"url":"http://arxiv.org/abs/2405.09989v1","category":"stat.AP"}
{"created":"2024-05-16 09:35:24","title":"Stock Market Dynamics Through Deep Learning Context","abstract":"Studies conducted on financial market prediction lack a comprehensive feature set that can carry a broad range of contributing factors; therefore, leading to imprecise results. Furthermore, while cooperating with the most recent innovations in explainable AI, studies have not provided an illustrative summary of market-driving factors using this powerful tool. Therefore, in this study, we propose a novel feature matrix that holds a broad range of features including Twitter content and market historical data to perform a binary classification task of one step ahead prediction. The utilization of our proposed feature matrix not only leads to improved prediction accuracy when compared to existing feature representations, but also its combination with explainable AI allows us to introduce a fresh analysis approach regarding the importance of the market-driving factors included. Thanks to the Lime interpretation technique, our interpretation study shows that the volume of tweets is the most important factor included in our feature matrix that drives the market's movements.","sentences":["Studies conducted on financial market prediction lack a comprehensive feature set that can carry a broad range of contributing factors; therefore, leading to imprecise results.","Furthermore, while cooperating with the most recent innovations in explainable AI, studies have not provided an illustrative summary of market-driving factors using this powerful tool.","Therefore, in this study, we propose a novel feature matrix that holds a broad range of features including Twitter content and market historical data to perform a binary classification task of one step ahead prediction.","The utilization of our proposed feature matrix not only leads to improved prediction accuracy when compared to existing feature representations, but also its combination with explainable AI allows us to introduce a fresh analysis approach regarding the importance of the market-driving factors included.","Thanks to the Lime interpretation technique, our interpretation study shows that the volume of tweets is the most important factor included in our feature matrix that drives the market's movements."],"url":"http://arxiv.org/abs/2405.09932v1","category":"cs.CE"}
{"created":"2024-05-16 08:36:20","title":"Confidence Estimation in Unsupervised Deep Change Vector Analysis","abstract":"Unsupervised transfer learning-based change detection methods exploit the feature extraction capability of pre-trained networks to distinguish changed pixels from the unchanged ones. However, their performance may vary significantly depending on several geographical and model-related aspects. In many applications, it is of utmost importance to provide trustworthy or confident results, even if over a subset of pixels. The core challenge in this problem is to identify changed pixels and confident pixels in an unsupervised manner. To address this, we propose a two-network model - one tasked with mere change detection and the other with confidence estimation. While the change detection network can be used in conjunction with popular transfer learning-based change detection methods such as Deep Change Vector Analysis, the confidence estimation network operates similarly to a randomized smoothing model. By ingesting ensembles of inputs perturbed by noise, it creates a distribution over the output and assigns confidence to each pixel's outcome. We tested the proposed method on three different Earth observation sensors: optical, Synthetic Aperture Radar, and hyperspectral sensors.","sentences":["Unsupervised transfer learning-based change detection methods exploit the feature extraction capability of pre-trained networks to distinguish changed pixels from the unchanged ones.","However, their performance may vary significantly depending on several geographical and model-related aspects.","In many applications, it is of utmost importance to provide trustworthy or confident results, even if over a subset of pixels.","The core challenge in this problem is to identify changed pixels and confident pixels in an unsupervised manner.","To address this, we propose a two-network model - one tasked with mere change detection and the other with confidence estimation.","While the change detection network can be used in conjunction with popular transfer learning-based change detection methods such as Deep Change Vector Analysis, the confidence estimation network operates similarly to a randomized smoothing model.","By ingesting ensembles of inputs perturbed by noise, it creates a distribution over the output and assigns confidence to each pixel's outcome.","We tested the proposed method on three different Earth observation sensors: optical, Synthetic Aperture Radar, and hyperspectral sensors."],"url":"http://arxiv.org/abs/2405.09896v1","category":"eess.IV"}
{"created":"2024-05-16 07:25:15","title":"Towards Realistic Incremental Scenario in Class Incremental Semantic Segmentation","abstract":"This paper addresses the unrealistic aspect of the commonly adopted Continuous Incremental Semantic Segmentation (CISS) scenario, termed overlapped. We point out that overlapped allows the same image to reappear in future tasks with different pixel labels, which is far from practical incremental learning scenarios. Moreover, we identified that this flawed scenario may lead to biased results for two commonly used techniques in CISS, pseudo-labeling and exemplar memory, resulting in unintended advantages or disadvantages for certain techniques. To mitigate this, a practical scenario called partitioned is proposed, in which the dataset is first divided into distinct subsets representing each class, and then the subsets are assigned to each corresponding task. This efficiently addresses the issue above while meeting the requirement of CISS scenario, such as capturing the background shifts. Furthermore, we identify and address the code implementation issues related to retrieving data from the exemplar memory, which was ignored in previous works. Lastly, we introduce a simple yet competitive memory-based baseline, MiB-AugM, that handles background shifts of current tasks in the exemplar memory. This baseline achieves state-of-the-art results across multiple tasks involving learning numerous new classes.","sentences":["This paper addresses the unrealistic aspect of the commonly adopted Continuous Incremental Semantic Segmentation (CISS) scenario, termed overlapped.","We point out that overlapped allows the same image to reappear in future tasks with different pixel labels, which is far from practical incremental learning scenarios.","Moreover, we identified that this flawed scenario may lead to biased results for two commonly used techniques in CISS, pseudo-labeling and exemplar memory, resulting in unintended advantages or disadvantages for certain techniques.","To mitigate this, a practical scenario called partitioned is proposed, in which the dataset is first divided into distinct subsets representing each class, and then the subsets are assigned to each corresponding task.","This efficiently addresses the issue above while meeting the requirement of CISS scenario, such as capturing the background shifts.","Furthermore, we identify and address the code implementation issues related to retrieving data from the exemplar memory, which was ignored in previous works.","Lastly, we introduce a simple yet competitive memory-based baseline, MiB-AugM, that handles background shifts of current tasks in the exemplar memory.","This baseline achieves state-of-the-art results across multiple tasks involving learning numerous new classes."],"url":"http://arxiv.org/abs/2405.09858v1","category":"cs.CV"}
{"created":"2024-05-16 07:00:44","title":"Region of Interest Detection in Melanocytic Skin Tumor Whole Slide Images -- Nevus & Melanoma","abstract":"Automated region of interest detection in histopathological image analysis is a challenging and important topic with tremendous potential impact on clinical practice. The deep-learning methods used in computational pathology may help us to reduce costs and increase the speed and accuracy of cancer diagnosis. We started with the UNC Melanocytic Tumor Dataset cohort that contains 160 hematoxylin and eosin whole-slide images of primary melanomas (86) and nevi (74). We randomly assigned 80% (134) as a training set and built an in-house deep-learning method to allow for classification, at the slide level, of nevi and melanomas. The proposed method performed well on the other 20% (26) test dataset; the accuracy of the slide classification task was 92.3% and our model also performed well in terms of predicting the region of interest annotated by the pathologists, showing excellent performance of our model on melanocytic skin tumors. Even though we tested the experiments on the skin tumor dataset, our work could also be extended to other medical image detection problems to benefit the clinical evaluation and diagnosis of different tumors.","sentences":["Automated region of interest detection in histopathological image analysis is a challenging and important topic with tremendous potential impact on clinical practice.","The deep-learning methods used in computational pathology may help us to reduce costs and increase the speed and accuracy of cancer diagnosis.","We started with the UNC Melanocytic Tumor Dataset cohort that contains 160 hematoxylin and eosin whole-slide images of primary melanomas (86) and nevi (74).","We randomly assigned 80% (134) as a training set and built an in-house deep-learning method to allow for classification, at the slide level, of nevi and melanomas.","The proposed method performed well on the other 20% (26) test dataset; the accuracy of the slide classification task was 92.3% and our model also performed well in terms of predicting the region of interest annotated by the pathologists, showing excellent performance of our model on melanocytic skin tumors.","Even though we tested the experiments on the skin tumor dataset, our work could also be extended to other medical image detection problems to benefit the clinical evaluation and diagnosis of different tumors."],"url":"http://arxiv.org/abs/2405.09851v1","category":"eess.IV"}
{"created":"2024-05-16 04:51:25","title":"A Payoff-Based Policy Gradient Method in Stochastic Games with Long-Run Average Payoffs","abstract":"Despite the significant potential for various applications, stochastic games with long-run average payoffs have received limited scholarly attention, particularly concerning the development of learning algorithms for them due to the challenges of mathematical analysis. In this paper, we study the stochastic games with long-run average payoffs and present an equivalent formulation for individual payoff gradients by defining advantage functions which will be proved to be bounded. This discovery allows us to demonstrate that the individual payoff gradient function is Lipschitz continuous with respect to the policy profile and that the value function of the games exhibits the gradient dominance property. Leveraging these insights, we devise a payoff-based gradient estimation approach and integrate it with the Regularized Robbins-Monro method from stochastic approximation theory to construct a bandit learning algorithm suited for stochastic games with long-run average payoffs. Additionally, we prove that if all players adopt our algorithm, the policy profile employed will asymptotically converge to a Nash equilibrium with probability one, provided that all Nash equilibria are globally neutrally stable and a globally variationally stable Nash equilibrium exists. This condition represents a wide class of games, including monotone games.","sentences":["Despite the significant potential for various applications, stochastic games with long-run average payoffs have received limited scholarly attention, particularly concerning the development of learning algorithms for them due to the challenges of mathematical analysis.","In this paper, we study the stochastic games with long-run average payoffs and present an equivalent formulation for individual payoff gradients by defining advantage functions which will be proved to be bounded.","This discovery allows us to demonstrate that the individual payoff gradient function is Lipschitz continuous with respect to the policy profile and that the value function of the games exhibits the gradient dominance property.","Leveraging these insights, we devise a payoff-based gradient estimation approach and integrate it with the Regularized Robbins-Monro method from stochastic approximation theory to construct a bandit learning algorithm suited for stochastic games with long-run average payoffs.","Additionally, we prove that if all players adopt our algorithm, the policy profile employed will asymptotically converge to a Nash equilibrium with probability one, provided that all Nash equilibria are globally neutrally stable and a globally variationally stable Nash equilibrium exists.","This condition represents a wide class of games, including monotone games."],"url":"http://arxiv.org/abs/2405.09811v1","category":"cs.GT"}
{"created":"2024-05-16 04:01:53","title":"Identification of Single-Treatment Effects in Factorial Experiments","abstract":"Despite their cost, randomized controlled trials (RCTs) are widely regarded as gold-standard evidence in disciplines ranging from social science to medicine. In recent decades, researchers have increasingly sought to reduce the resource burden of repeated RCTs with factorial designs that simultaneously test multiple hypotheses, e.g. experiments that evaluate the effects of many medications or products simultaneously. Here I show that when multiple interventions are randomized in experiments, the effect any single intervention would have outside the experimental setting is not identified absent heroic assumptions, even if otherwise perfectly realistic conditions are achieved. This happens because single-treatment effects involve a counterfactual world with a single focal intervention, allowing other variables to take their natural values (which may be confounded or modified by the focal intervention). In contrast, observational studies and factorial experiments provide information about potential-outcome distributions with zero and multiple interventions, respectively. In this paper, I formalize sufficient conditions for the identifiability of those isolated quantities. I show that researchers who rely on this type of design have to justify either linearity of functional forms or -- in the nonparametric case -- specify with Directed Acyclic Graphs how variables are related in the real world. Finally, I develop nonparametric sharp bounds -- i.e., maximally informative best-/worst-case estimates consistent with limited RCT data -- that show when extrapolations about effect signs are empirically justified. These new results are illustrated with simulated data.","sentences":["Despite their cost, randomized controlled trials (RCTs) are widely regarded as gold-standard evidence in disciplines ranging from social science to medicine.","In recent decades, researchers have increasingly sought to reduce the resource burden of repeated RCTs with factorial designs that simultaneously test multiple hypotheses, e.g. experiments that evaluate the effects of many medications or products simultaneously.","Here I show that when multiple interventions are randomized in experiments, the effect any single intervention would have outside the experimental setting is not identified absent heroic assumptions, even if otherwise perfectly realistic conditions are achieved.","This happens because single-treatment effects involve a counterfactual world with a single focal intervention, allowing other variables to take their natural values (which may be confounded or modified by the focal intervention).","In contrast, observational studies and factorial experiments provide information about potential-outcome distributions with zero and multiple interventions, respectively.","In this paper, I formalize sufficient conditions for the identifiability of those isolated quantities.","I show that researchers who rely on this type of design have to justify either linearity of functional forms or -- in the nonparametric case -- specify with Directed Acyclic Graphs how variables are related in the real world.","Finally, I develop nonparametric sharp bounds -- i.e., maximally informative best-/worst-case estimates consistent with limited RCT data -- that show when extrapolations about effect signs are empirically justified.","These new results are illustrated with simulated data."],"url":"http://arxiv.org/abs/2405.09797v1","category":"stat.ME"}
{"created":"2024-05-16 03:23:57","title":"Analysis of the BraTS 2023 Intracranial Meningioma Segmentation Challenge","abstract":"We describe the design and results from the BraTS 2023 Intracranial Meningioma Segmentation Challenge. The BraTS Meningioma Challenge differed from prior BraTS Glioma challenges in that it focused on meningiomas, which are typically benign extra-axial tumors with diverse radiologic and anatomical presentation and a propensity for multiplicity. Nine participating teams each developed deep-learning automated segmentation models using image data from the largest multi-institutional systematically expert annotated multilabel multi-sequence meningioma MRI dataset to date, which included 1000 training set cases, 141 validation set cases, and 283 hidden test set cases. Each case included T2, T2/FLAIR, T1, and T1Gd brain MRI sequences with associated tumor compartment labels delineating enhancing tumor, non-enhancing tumor, and surrounding non-enhancing T2/FLAIR hyperintensity. Participant automated segmentation models were evaluated and ranked based on a scoring system evaluating lesion-wise metrics including dice similarity coefficient (DSC) and 95% Hausdorff Distance. The top ranked team had a lesion-wise median dice similarity coefficient (DSC) of 0.976, 0.976, and 0.964 for enhancing tumor, tumor core, and whole tumor, respectively and a corresponding average DSC of 0.899, 0.904, and 0.871, respectively. These results serve as state-of-the-art benchmarks for future pre-operative meningioma automated segmentation algorithms. Additionally, we found that 1286 of 1424 cases (90.3%) had at least 1 compartment voxel abutting the edge of the skull-stripped image edge, which requires further investigation into optimal pre-processing face anonymization steps.","sentences":["We describe the design and results from the BraTS 2023 Intracranial Meningioma Segmentation Challenge.","The BraTS Meningioma Challenge differed from prior BraTS Glioma challenges in that it focused on meningiomas, which are typically benign extra-axial tumors with diverse radiologic and anatomical presentation and a propensity for multiplicity.","Nine participating teams each developed deep-learning automated segmentation models using image data from the largest multi-institutional systematically expert annotated multilabel multi-sequence meningioma MRI dataset to date, which included 1000 training set cases, 141 validation set cases, and 283 hidden test set cases.","Each case included T2, T2/FLAIR, T1, and T1Gd brain MRI sequences with associated tumor compartment labels delineating enhancing tumor, non-enhancing tumor, and surrounding non-enhancing T2/FLAIR hyperintensity.","Participant automated segmentation models were evaluated and ranked based on a scoring system evaluating lesion-wise metrics including dice similarity coefficient (DSC) and 95% Hausdorff Distance.","The top ranked team had a lesion-wise median dice similarity coefficient (DSC) of 0.976, 0.976, and 0.964 for enhancing tumor, tumor core, and whole tumor, respectively and a corresponding average DSC of 0.899, 0.904, and 0.871, respectively.","These results serve as state-of-the-art benchmarks for future pre-operative meningioma automated segmentation algorithms.","Additionally, we found that 1286 of 1424 cases (90.3%) had at least 1 compartment voxel abutting the edge of the skull-stripped image edge, which requires further investigation into optimal pre-processing face anonymization steps."],"url":"http://arxiv.org/abs/2405.09787v1","category":"eess.IV"}
{"created":"2024-05-16 02:08:17","title":"Combining RL and IL using a dynamic, performance-based modulation over learning signals and its application to local planning","abstract":"This paper proposes a method to combine reinforcement learning (RL) and imitation learning (IL) using a dynamic, performance-based modulation over learning signals. The proposed method combines RL and behavioral cloning (IL), or corrective feedback in the action space (interactive IL/IIL), by dynamically weighting the losses to be optimized, taking into account the backpropagated gradients used to update the policy and the agent's estimated performance. In this manner, RL and IL/IIL losses are combined by equalizing their impact on the policy's updates, while modulating said impact such that IL signals are prioritized at the beginning of the learning process, and as the agent's performance improves, the RL signals become progressively more relevant, allowing for a smooth transition from pure IL/IIL to pure RL. The proposed method is used to learn local planning policies for mobile robots, synthesizing IL/IIL signals online by means of a scripted policy. An extensive evaluation of the application of the proposed method to this task is performed in simulations, and it is empirically shown that it outperforms pure RL in terms of sample efficiency (achieving the same level of performance in the training environment utilizing approximately 4 times less experiences), while consistently producing local planning policies with better performance metrics (achieving an average success rate of 0.959 in an evaluation environment, outperforming pure RL by 12.5% and pure IL by 13.9%). Furthermore, the obtained local planning policies are successfully deployed in the real world without performing any major fine tuning. The proposed method can extend existing RL algorithms, and is applicable to other problems for which generating IL/IIL signals online is feasible. A video summarizing some of the real world experiments that were conducted can be found in https://youtu.be/mZlaXn9WGzw.","sentences":["This paper proposes a method to combine reinforcement learning (RL) and imitation learning (IL) using a dynamic, performance-based modulation over learning signals.","The proposed method combines RL and behavioral cloning (IL), or corrective feedback in the action space (interactive IL/IIL), by dynamically weighting the losses to be optimized, taking into account the backpropagated gradients used to update the policy and the agent's estimated performance.","In this manner, RL and IL/IIL losses are combined by equalizing their impact on the policy's updates, while modulating said impact such that IL signals are prioritized at the beginning of the learning process, and as the agent's performance improves, the RL signals become progressively more relevant, allowing for a smooth transition from pure IL/IIL to pure RL.","The proposed method is used to learn local planning policies for mobile robots, synthesizing IL/IIL signals online by means of a scripted policy.","An extensive evaluation of the application of the proposed method to this task is performed in simulations, and it is empirically shown that it outperforms pure RL in terms of sample efficiency (achieving the same level of performance in the training environment utilizing approximately 4 times less experiences), while consistently producing local planning policies with better performance metrics (achieving an average success rate of 0.959 in an evaluation environment, outperforming pure RL by 12.5% and pure IL by 13.9%).","Furthermore, the obtained local planning policies are successfully deployed in the real world without performing any major fine tuning.","The proposed method can extend existing RL algorithms, and is applicable to other problems for which generating IL/IIL signals online is feasible.","A video summarizing some of the real world experiments that were conducted can be found in https://youtu.be/mZlaXn9WGzw."],"url":"http://arxiv.org/abs/2405.09760v1","category":"cs.RO"}
{"created":"2024-05-15 23:06:47","title":"Parametrized Energy-Efficient Quantum Kernels for Network Service Fault Diagnosis","abstract":"In quantum kernel learning, the primary method involves using a quantum computer to calculate the inner product between feature vectors, thereby obtaining a Gram matrix used as a kernel in machine learning models such as support vector machines (SVMs). However, a method for consistently achieving high performance has not been established. In this study, we investigate the diagnostic accuracy using a commercial dataset of a network service fault diagnosis system used by telecommunications carriers, focusing on quantum kernel learning, and propose a method to stably achieve high performance.We show significant performance improvements and an efficient achievement of high performance over conventional methods can be attained by applying quantum entanglement in the portion of the general quantum circuit used to create the quantum kernel, through input data parameter mapping and parameter tuning related to relative phase angles. Furthermore, experimental validation of the quantum kernel was conducted using IBM' s superconducting quantum computer IBM-Kawasaki, and its practicality was verified while applying the error suppression feature of Q-CTRL' s Fire Opal.","sentences":["In quantum kernel learning, the primary method involves using a quantum computer to calculate the inner product between feature vectors, thereby obtaining a Gram matrix used as a kernel in machine learning models such as support vector machines (SVMs).","However, a method for consistently achieving high performance has not been established.","In this study, we investigate the diagnostic accuracy using a commercial dataset of a network service fault diagnosis system used by telecommunications carriers, focusing on quantum kernel learning, and propose a method to stably achieve high performance.","We show significant performance improvements and an efficient achievement of high performance over conventional methods can be attained by applying quantum entanglement in the portion of the general quantum circuit used to create the quantum kernel, through input data parameter mapping and parameter tuning related to relative phase angles.","Furthermore, experimental validation of the quantum kernel was conducted using IBM' s superconducting quantum computer IBM-Kawasaki, and its practicality was verified while applying the error suppression feature of Q-CTRL' s Fire Opal."],"url":"http://arxiv.org/abs/2405.09724v1","category":"quant-ph"}
{"created":"2024-05-15 22:11:52","title":"Illumination Histogram Consistency Metric for Quantitative Assessment of Video Sequences","abstract":"The advances in deep generative models have greatly accelerate the process of video procession such as video enhancement and synthesis. Learning spatio-temporal video models requires to capture the temporal dynamics of a scene, in addition to the visual appearance of individual frames. Illumination consistency, which reflects the variations of illumination in the dynamic video sequences, play a vital role in video processing. Unfortunately, to date, no well-accepted quantitative metric has been proposed for video illumination consistency evaluation. In this paper, we propose a illumination histogram consistency (IHC) metric to quantitatively and automatically evaluate the illumination consistency of the video sequences. IHC measures the illumination variation of any video sequence based on the illumination histogram discrepancies across all the frames in the video sequence. Specifically, given a video sequence, we first estimate the illumination map of each individual frame using the Retinex model; Then, using the illumination maps, the mean illumination histogram of the video sequence is computed by the mean operation across all the frames; Next, we compute the illumination histogram discrepancy between each individual frame and the mean illumination histogram and sum up all the illumination histogram discrepancies to represent the illumination variations of the video sequence. Finally, we obtain the IHC score from the illumination histogram discrepancies via normalization and subtraction operations. Experiments are conducted to illustrate the performance of the proposed IHC metric and its capability to measure the illumination variations in video sequences. The source code is available on \\url{https://github.com/LongChenCV/IHC-Metric}.","sentences":["The advances in deep generative models have greatly accelerate the process of video procession such as video enhancement and synthesis.","Learning spatio-temporal video models requires to capture the temporal dynamics of a scene, in addition to the visual appearance of individual frames.","Illumination consistency, which reflects the variations of illumination in the dynamic video sequences, play a vital role in video processing.","Unfortunately, to date, no well-accepted quantitative metric has been proposed for video illumination consistency evaluation.","In this paper, we propose a illumination histogram consistency (IHC) metric to quantitatively and automatically evaluate the illumination consistency of the video sequences.","IHC measures the illumination variation of any video sequence based on the illumination histogram discrepancies across all the frames in the video sequence.","Specifically, given a video sequence, we first estimate the illumination map of each individual frame using the Retinex model; Then, using the illumination maps, the mean illumination histogram of the video sequence is computed by the mean operation across all the frames; Next, we compute the illumination histogram discrepancy between each individual frame and the mean illumination histogram and sum up all the illumination histogram discrepancies to represent the illumination variations of the video sequence.","Finally, we obtain the IHC score from the illumination histogram discrepancies via normalization and subtraction operations.","Experiments are conducted to illustrate the performance of the proposed IHC metric and its capability to measure the illumination variations in video sequences.","The source code is available on \\url{https://github.com/LongChenCV/IHC-Metric}."],"url":"http://arxiv.org/abs/2405.09716v1","category":"eess.IV"}
{"created":"2024-05-15 19:36:08","title":"The radius of statistical efficiency","abstract":"Classical results in asymptotic statistics show that the Fisher information matrix controls the difficulty of estimating a statistical model from observed data. In this work, we introduce a companion measure of robustness of an estimation problem: the radius of statistical efficiency (RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular. We compute RSE up to numerical constants for a variety of test bed problems, including principal component analysis, generalized linear models, phase retrieval, bilinear sensing, and matrix completion. In all cases, the RSE quantifies the compatibility between the covariance of the population data and the latent model parameter. Interestingly, we observe a precise reciprocal relationship between RSE and the intrinsic complexity/sensitivity of the problem instance, paralleling the classical Eckart-Young theorem in numerical analysis.","sentences":["Classical results in asymptotic statistics show that the Fisher information matrix controls the difficulty of estimating a statistical model from observed data.","In this work, we introduce a companion measure of robustness of an estimation problem: the radius of statistical efficiency (RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular.","We compute RSE up to numerical constants for a variety of test bed problems, including principal component analysis, generalized linear models, phase retrieval, bilinear sensing, and matrix completion.","In all cases, the RSE quantifies the compatibility between the covariance of the population data and the latent model parameter.","Interestingly, we observe a precise reciprocal relationship between RSE and the intrinsic complexity/sensitivity of the problem instance, paralleling the classical Eckart-Young theorem in numerical analysis."],"url":"http://arxiv.org/abs/2405.09676v1","category":"math.ST"}
{"created":"2024-05-15 19:09:16","title":"Sign-Alternating Thermoelectric Quantum Oscillations and Insulating Landau Levels in Monolayer WTe2","abstract":"The detection of Landau-level-like energy structures near the chemical potential of an insulator is essential to the search for a class of correlated electronic matter hosting charge-neutral fermions and Fermi surfaces, a long-proposed concept that remains elusive experimentally. Here we introduce and demonstrate that the magneto-thermoelectric response of a quantum insulator can reveal critical information not available via other approaches. We report large quantum oscillations (QOs) in the Seebeck response of the hole-doped insulating state of monolayer tungsten ditelluride (WTe2) in magnetic fields. The QOs remarkably undergo sign-changes as the field is swept, mimicking those in metals with Landau quantization. The sign-change in the thermoelectric response directly implies the presence of a field-induced Landau-level-like structure at the chemical potential of the insulator. Our results reinforce WTe2 as a platform for investigating insulating Landau levels and mobile neutral fermions in two-dimensional insulators.","sentences":["The detection of Landau-level-like energy structures near the chemical potential of an insulator is essential to the search for a class of correlated electronic matter hosting charge-neutral fermions and Fermi surfaces, a long-proposed concept that remains elusive experimentally.","Here we introduce and demonstrate that the magneto-thermoelectric response of a quantum insulator can reveal critical information not available via other approaches.","We report large quantum oscillations (QOs) in the Seebeck response of the hole-doped insulating state of monolayer tungsten ditelluride (WTe2) in magnetic fields.","The QOs remarkably undergo sign-changes as the field is swept, mimicking those in metals with Landau quantization.","The sign-change in the thermoelectric response directly implies the presence of a field-induced Landau-level-like structure at the chemical potential of the insulator.","Our results reinforce WTe2 as a platform for investigating insulating Landau levels and mobile neutral fermions in two-dimensional insulators."],"url":"http://arxiv.org/abs/2405.09665v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 18:46:52","title":"Beyond Repetition: The Role of Varied Questioning and Feedback in Knowledge Generalization","abstract":"This study examines the effects of question type and feedback on learning outcomes in a hybrid graduate-level course. By analyzing data from 32 students over 30,198 interactions, we assess the efficacy of unique versus repeated questions and the impact of feedback on student learning. The findings reveal students demonstrate significantly better knowledge generalization when encountering unique questions compared to repeated ones, even though they perform better with repeated opportunities. Moreover, we find that the timing of explanatory feedback is a more robust predictor of learning outcomes than the practice opportunities themselves. These insights suggest that educational practices and technological platforms should prioritize a variety of questions to enhance the learning process. The study also highlights the critical role of feedback; opportunities preceding feedback are less effective in enhancing learning.","sentences":["This study examines the effects of question type and feedback on learning outcomes in a hybrid graduate-level course.","By analyzing data from 32 students over 30,198 interactions, we assess the efficacy of unique versus repeated questions and the impact of feedback on student learning.","The findings reveal students demonstrate significantly better knowledge generalization when encountering unique questions compared to repeated ones, even though they perform better with repeated opportunities.","Moreover, we find that the timing of explanatory feedback is a more robust predictor of learning outcomes than the practice opportunities themselves.","These insights suggest that educational practices and technological platforms should prioritize a variety of questions to enhance the learning process.","The study also highlights the critical role of feedback; opportunities preceding feedback are less effective in enhancing learning."],"url":"http://arxiv.org/abs/2405.09655v1","category":"cs.HC"}
{"created":"2024-05-15 18:10:57","title":"LeaPP: Learning Pathways to Polymorphs through machine learning analysis of atomic trajectories","abstract":"Understanding the mechanisms underlying crystal formation is crucial. For most systems, crystallization typically goes through a nucleation process that involves dynamics that happen at short time and length scales. Due to this, molecular dynamics serves as a powerful tool to study this phenomenon. Existing approaches to study the mechanism often focus analysis on static snapshots of the global configuration, potentially overlooking subtle local fluctuations and history of the atoms involved in the formation of solid nuclei. To address this limitation, we propose a methodology that categorizes nucleation pathways into reactive pathways based on the time evolution of constituent atoms. Our approach effectively captures the diverse structural pathways explored by crystallizing Lennard-Jones-like particles and solidifying Ni$_3$Al, providing a more nuanced understanding of nucleating pathways. Moreover, our methodology enables the prediction of the resulting polymorph from each reactive trajectory. This deep learning-assisted comprehensive analysis offers an alternative view of crystal nucleation mechanisms and pathways.","sentences":["Understanding the mechanisms underlying crystal formation is crucial.","For most systems, crystallization typically goes through a nucleation process that involves dynamics that happen at short time and length scales.","Due to this, molecular dynamics serves as a powerful tool to study this phenomenon.","Existing approaches to study the mechanism often focus analysis on static snapshots of the global configuration, potentially overlooking subtle local fluctuations and history of the atoms involved in the formation of solid nuclei.","To address this limitation, we propose a methodology that categorizes nucleation pathways into reactive pathways based on the time evolution of constituent atoms.","Our approach effectively captures the diverse structural pathways explored by crystallizing Lennard-Jones-like particles and solidifying Ni$_3$Al, providing a more nuanced understanding of nucleating pathways.","Moreover, our methodology enables the prediction of the resulting polymorph from each reactive trajectory.","This deep learning-assisted comprehensive analysis offers an alternative view of crystal nucleation mechanisms and pathways."],"url":"http://arxiv.org/abs/2405.09642v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 14:56:17","title":"Fully Automated OCT-based Tissue Screening System","abstract":"This study introduces a groundbreaking optical coherence tomography (OCT) imaging system dedicated for high-throughput screening applications using ex vivo tissue culture. Leveraging OCT's non-invasive, high-resolution capabilities, the system is equipped with a custom-designed motorized platform and tissue detection ability for automated, successive imaging across samples. Transformer-based deep learning segmentation algorithms further ensure robust, consistent, and efficient readouts meeting the standards for screening assays. Validated using retinal explant cultures from a mouse model of retinal degeneration, the system provides robust, rapid, reliable, unbiased, and comprehensive readouts of tissue response to treatments. This fully automated OCT-based system marks a significant advancement in tissue screening, promising to transform drug discovery, as well as other relevant research fields.","sentences":["This study introduces a groundbreaking optical coherence tomography (OCT) imaging system dedicated for high-throughput screening applications using ex vivo tissue culture.","Leveraging OCT's non-invasive, high-resolution capabilities, the system is equipped with a custom-designed motorized platform and tissue detection ability for automated, successive imaging across samples.","Transformer-based deep learning segmentation algorithms further ensure robust, consistent, and efficient readouts meeting the standards for screening assays.","Validated using retinal explant cultures from a mouse model of retinal degeneration, the system provides robust, rapid, reliable, unbiased, and comprehensive readouts of tissue response to treatments.","This fully automated OCT-based system marks a significant advancement in tissue screening, promising to transform drug discovery, as well as other relevant research fields."],"url":"http://arxiv.org/abs/2405.09601v1","category":"physics.med-ph"}
{"created":"2024-05-16 17:57:15","title":"Fault Tolerance Embedded in a Quantum-Gap-Estimation Algorithm with Trial-State Optimization","abstract":"We construct a hybrid quantum algorithm to estimate gaps in many-body energy spectra and prove that it is inherently fault-tolerant to global multi-qubit depolarizing noise. Using trial-state optimization without active error correction, we show that the spectral peak of an exact target gap can be amplified beyond the noise threshold, thereby reducing gap-estimate error. We numerically verify fault tolerance using the Qiskit Aer simulator with a model of common mid-circuit noise channels. Our results reveal the potential for accurate quantum simulations on near-term noisy quantum computers.","sentences":["We construct a hybrid quantum algorithm to estimate gaps in many-body energy spectra and prove that it is inherently fault-tolerant to global multi-qubit depolarizing noise.","Using trial-state optimization without active error correction, we show that the spectral peak of an exact target gap can be amplified beyond the noise threshold, thereby reducing gap-estimate error.","We numerically verify fault tolerance using the Qiskit Aer simulator with a model of common mid-circuit noise channels.","Our results reveal the potential for accurate quantum simulations on near-term noisy quantum computers."],"url":"http://arxiv.org/abs/2405.10306v1","category":"quant-ph"}
{"created":"2024-05-16 17:33:50","title":"Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers","abstract":"Numerous recent works aim to enhance the efficacy of Large Language Models (LLMs) through strategic prompting. In particular, the Optimization by PROmpting (OPRO) approach provides state-of-the-art performance by leveraging LLMs as optimizers where the optimization task is to find instructions that maximize the task accuracy. In this paper, we revisit OPRO for automated prompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral 7B. Our investigation reveals that OPRO shows limited effectiveness in small-scale LLMs, with limited inference capabilities constraining optimization ability. We suggest future automatic prompting engineering to consider both model capabilities and computational costs. Additionally, for small-scale LLMs, we recommend direct instructions that clearly outline objectives and methodologies as robust prompt baselines, ensuring efficient and effective prompt engineering in ongoing research.","sentences":["Numerous recent works aim to enhance the efficacy of Large Language Models (LLMs) through strategic prompting.","In particular, the Optimization by PROmpting (OPRO) approach provides state-of-the-art performance by leveraging LLMs as optimizers where the optimization task is to find instructions that maximize the task accuracy.","In this paper, we revisit OPRO for automated prompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral 7B.","Our investigation reveals that OPRO shows limited effectiveness in small-scale LLMs, with limited inference capabilities constraining optimization ability.","We suggest future automatic prompting engineering to consider both model capabilities and computational costs.","Additionally, for small-scale LLMs, we recommend direct instructions that clearly outline objectives and methodologies as robust prompt baselines, ensuring efficient and effective prompt engineering in ongoing research."],"url":"http://arxiv.org/abs/2405.10276v1","category":"cs.CL"}
{"created":"2024-05-16 17:02:43","title":"End-to-End Optimization of Directly Modulated Laser Links using Chirp-Aware Modeling","abstract":"The rate and reach of directly-modulated laser links is often limited by the interplay between chirp and fiber chromatic dispersion. We address this by optimizing the transmitter, receiver, bias and peak-to-peak current to the laser jointly. Our approach outperforms Volterra post-equalization at various symbol rates.","sentences":["The rate and reach of directly-modulated laser links is often limited by the interplay between chirp and fiber chromatic dispersion.","We address this by optimizing the transmitter, receiver, bias and peak-to-peak current to the laser jointly.","Our approach outperforms Volterra post-equalization at various symbol rates."],"url":"http://arxiv.org/abs/2405.10257v1","category":"eess.SP"}
{"created":"2024-05-16 14:29:02","title":"PyOptInterface: Design and implementation of an efficient modeling language for mathematical optimization","abstract":"This paper introduces the design and implementation of PyOptInterface, a modeling language for mathematical optimization embedded in Python programming language. PyOptInterface uses lightweight and compact data structure to bridge high-level entities in optimization models like variables and constraints to internal indices of optimizers efficiently. It supports a variety of optimization solvers and a range of common problem classes. We provide benchmarks to exhibit the competitive performance of PyOptInterface compared with other state-of-the-art modeling languages.","sentences":["This paper introduces the design and implementation of PyOptInterface, a modeling language for mathematical optimization embedded in Python programming language.","PyOptInterface uses lightweight and compact data structure to bridge high-level entities in optimization models like variables and constraints to internal indices of optimizers efficiently.","It supports a variety of optimization solvers and a range of common problem classes.","We provide benchmarks to exhibit the competitive performance of PyOptInterface compared with other state-of-the-art modeling languages."],"url":"http://arxiv.org/abs/2405.10130v1","category":"cs.MS"}
{"created":"2024-05-16 14:24:37","title":"A Recursive Lower Bound on the Energy Improvement of the Quantum Approximate Optimization Algorithm","abstract":"The quantum approximate optimization algorithm (QAOA) uses a quantum computer to implement a variational method with $2p$ layers of alternating unitary operators, optimized by a classical computer to minimize a cost function. While rigorous performance guarantees exist for the QAOA at small depths $p$, the behavior at large depths remains less clear, though simulations suggest exponentially fast convergence for certain problems. In this work, we gain insights into the deep QAOA using an analytic expansion of the cost function around transition states. Transition states are constructed in a recursive manner: from the local minima of the QAOA with $p$ layers we obtain transition states of the QAOA with $p+1$ layers, which are stationary points characterized by a unique direction of negative curvature. We construct an analytic estimate of the negative curvature and the corresponding direction in parameter space at each transition state. The expansion of the QAOA cost function along the negative direction to the quartic order gives a lower bound of the QAOA cost function improvement. We provide physical intuition behind the analytic expressions for the local curvature and quartic expansion coefficient. Our numerical study confirms the accuracy of our approximations and reveals that the obtained bound and the true value of the QAOA cost function gain have a characteristic exponential decrease with the number of layers $p$, with the bound decreasing more rapidly. Our study establishes an analytical method for recursively studying the QAOA that is applicable in the regime of high circuit depth.","sentences":["The quantum approximate optimization algorithm (QAOA) uses a quantum computer to implement a variational method with $2p$ layers of alternating unitary operators, optimized by a classical computer to minimize a cost function.","While rigorous performance guarantees exist for the QAOA at small depths $p$, the behavior at large depths remains less clear, though simulations suggest exponentially fast convergence for certain problems.","In this work, we gain insights into the deep QAOA using an analytic expansion of the cost function around transition states.","Transition states are constructed in a recursive manner: from the local minima of the QAOA with $p$ layers we obtain transition states of the QAOA with $p+1$ layers, which are stationary points characterized by a unique direction of negative curvature.","We construct an analytic estimate of the negative curvature and the corresponding direction in parameter space at each transition state.","The expansion of the QAOA cost function along the negative direction to the quartic order gives a lower bound of the QAOA cost function improvement.","We provide physical intuition behind the analytic expressions for the local curvature and quartic expansion coefficient.","Our numerical study confirms the accuracy of our approximations and reveals that the obtained bound and the true value of the QAOA cost function gain have a characteristic exponential decrease with the number of layers $p$, with the bound decreasing more rapidly.","Our study establishes an analytical method for recursively studying the QAOA that is applicable in the regime of high circuit depth."],"url":"http://arxiv.org/abs/2405.10125v1","category":"quant-ph"}
{"created":"2024-05-16 07:48:50","title":"Servo Integrated Nonlinear Model Predictive Control for Overactuated Tiltable-Quadrotors","abstract":"Quadrotors are widely employed across various domains, yet the conventional type faces limitations due to underactuation, where attitude control is closely tied to positional adjustments. In contrast, quadrotors equipped with tiltable rotors offer overactuation, empowering them to track both position and attitude trajectories. However, the nonlinear dynamics of the drone body and the sluggish response of tilting servos pose challenges for conventional cascade controllers. In this study, we propose a control methodology for tilting-rotor quadrotors based on nonlinear model predictive control (NMPC). Unlike conventional approaches, our method preserves the full dynamics without simplification and utilizes actuator commands directly as control inputs. Notably, we incorporate a first-order servo model within the NMPC framework. Through simulation, we observe that integrating the servo dynamics not only enhances control performance but also accelerates convergence. To assess the efficacy of our approach, we fabricate a tiltable-quadrotor and deploy the algorithm onboard at a frequency of 100Hz. Extensive real-world experiments demonstrate rapid, robust, and smooth pose tracking performance.","sentences":["Quadrotors are widely employed across various domains, yet the conventional type faces limitations due to underactuation, where attitude control is closely tied to positional adjustments.","In contrast, quadrotors equipped with tiltable rotors offer overactuation, empowering them to track both position and attitude trajectories.","However, the nonlinear dynamics of the drone body and the sluggish response of tilting servos pose challenges for conventional cascade controllers.","In this study, we propose a control methodology for tilting-rotor quadrotors based on nonlinear model predictive control (NMPC).","Unlike conventional approaches, our method preserves the full dynamics without simplification and utilizes actuator commands directly as control inputs.","Notably, we incorporate a first-order servo model within the NMPC framework.","Through simulation, we observe that integrating the servo dynamics not only enhances control performance but also accelerates convergence.","To assess the efficacy of our approach, we fabricate a tiltable-quadrotor and deploy the algorithm onboard at a frequency of 100Hz. Extensive real-world experiments demonstrate rapid, robust, and smooth pose tracking performance."],"url":"http://arxiv.org/abs/2405.09871v1","category":"cs.RO"}
{"created":"2024-05-16 07:45:34","title":"Optimality conditions at infinity for nonsmooth minimax programming","abstract":"This paper is devoted to study of optimality conditions at infinity in nonsmooth minimax programming problems and applications. By means of the limiting subdifferential and normal cone at infinity, we dirive necessary and sufficient optimality conditions of Karush--Kuhn--Tucker type for nonsmooth minimax programming problems with constraint. The obtained results are applied to a nonsmooth vector optimization problem.","sentences":["This paper is devoted to study of optimality conditions at infinity in nonsmooth minimax programming problems and applications.","By means of the limiting subdifferential and normal cone at infinity, we dirive necessary and sufficient optimality conditions of Karush--Kuhn--Tucker type for nonsmooth minimax programming problems with constraint.","The obtained results are applied to a nonsmooth vector optimization problem."],"url":"http://arxiv.org/abs/2405.09869v1","category":"math.OC"}
{"created":"2024-05-16 07:38:49","title":"Performance of Quantum Networks Using Heterogeneous Link Architectures","abstract":"The heterogeneity of quantum link architectures is an essential theme in designing quantum networks for technological interoperability and possibly performance optimization. However, the performance of heterogeneously connected quantum links has not yet been addressed. Here, we investigate the integration of two inherently different technologies, with one link where the photons flow from the nodes toward a device in the middle of the link, and a different link where pairs of photons flow from a device in the middle towards the nodes. We utilize the quantum internet simulator QuISP to conduct simulations. We first optimize the existing photon pair protocol for a single link by taking the pulse rate into account. Here, we find that increasing the pulse rate can actually decrease the overall performance. Using our optimized links, we demonstrate that heterogeneous networks actually work. Their performance is highly dependent on link configuration, but we observe no significant decrease in generation rate compared to homogeneous networks. This work provides insights into the phenomena we likely will observe when introducing technological heterogeneity into quantum networks, which is crucial for creating a scalable and robust quantum internetwork.","sentences":["The heterogeneity of quantum link architectures is an essential theme in designing quantum networks for technological interoperability and possibly performance optimization.","However, the performance of heterogeneously connected quantum links has not yet been addressed.","Here, we investigate the integration of two inherently different technologies, with one link where the photons flow from the nodes toward a device in the middle of the link, and a different link where pairs of photons flow from a device in the middle towards the nodes.","We utilize the quantum internet simulator QuISP to conduct simulations.","We first optimize the existing photon pair protocol for a single link by taking the pulse rate into account.","Here, we find that increasing the pulse rate can actually decrease the overall performance.","Using our optimized links, we demonstrate that heterogeneous networks actually work.","Their performance is highly dependent on link configuration, but we observe no significant decrease in generation rate compared to homogeneous networks.","This work provides insights into the phenomena we likely will observe when introducing technological heterogeneity into quantum networks, which is crucial for creating a scalable and robust quantum internetwork."],"url":"http://arxiv.org/abs/2405.09862v1","category":"quant-ph"}
{"created":"2024-05-16 07:38:34","title":"Optimal Switching Networks for Paired-Egress Bell State Analyzer Pools","abstract":"To scale quantum computers to useful levels, we must build networks of quantum computational nodes that can share entanglement for use in distributed forms of quantum algorithms. In one proposed architecture, node-to-node entanglement is created when nodes emit photons entangled with stationary memories, with the photons routed through a switched interconnect to a shared pool of Bell state analyzers (BSAs). Designs that optimize switching circuits will reduce loss and crosstalk, raising entanglement rates and fidelity. We present optimal designs for switched interconnects constrained to planar layouts, appropriate for silicon waveguides and Mach-Zehnder interferometer (MZI) $2 \\times 2$ switch points. The architectures for the optimal designs are scalable and algorithmically structured to pair any arbitrary inputs in a rearrangeable, non-blocking way. For pairing $N$ inputs, $N(N - 2)/4$ switches are required, which is less than half of number of switches required for full permutation switching networks. An efficient routing algorithm is also presented for each architecture. These designs can also be employed in reverse for entanglement generation using a shared pool of entangled paired photon sources.","sentences":["To scale quantum computers to useful levels, we must build networks of quantum computational nodes that can share entanglement for use in distributed forms of quantum algorithms.","In one proposed architecture, node-to-node entanglement is created when nodes emit photons entangled with stationary memories, with the photons routed through a switched interconnect to a shared pool of Bell state analyzers (BSAs).","Designs that optimize switching circuits will reduce loss and crosstalk, raising entanglement rates and fidelity.","We present optimal designs for switched interconnects constrained to planar layouts, appropriate for silicon waveguides and Mach-Zehnder interferometer (MZI) $2 \\times 2$ switch points.","The architectures for the optimal designs are scalable and algorithmically structured to pair any arbitrary inputs in a rearrangeable, non-blocking way.","For pairing $N$ inputs, $N(N - 2)/4$ switches are required, which is less than half of number of switches required for full permutation switching networks.","An efficient routing algorithm is also presented for each architecture.","These designs can also be employed in reverse for entanglement generation using a shared pool of entangled paired photon sources."],"url":"http://arxiv.org/abs/2405.09860v1","category":"quant-ph"}
{"created":"2024-05-16 03:01:06","title":"Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection","abstract":"This paper explores the size-invariance of evaluation metrics in Salient Object Detection (SOD), especially when multiple targets of diverse sizes co-exist in the same image. We observe that current metrics are size-sensitive, where larger objects are focused, and smaller ones tend to be ignored. We argue that the evaluation should be size-invariant because bias based on size is unjustified without additional semantic information. In pursuit of this, we propose a generic approach that evaluates each salient object separately and then combines the results, effectively alleviating the imbalance. We further develop an optimization framework tailored to this goal, achieving considerable improvements in detecting objects of different sizes. Theoretically, we provide evidence supporting the validity of our new metrics and present the generalization analysis of SOD. Extensive experiments demonstrate the effectiveness of our method. The code is available at https://github.com/Ferry-Li/SI-SOD.","sentences":["This paper explores the size-invariance of evaluation metrics in Salient Object Detection (SOD), especially when multiple targets of diverse sizes co-exist in the same image.","We observe that current metrics are size-sensitive, where larger objects are focused, and smaller ones tend to be ignored.","We argue that the evaluation should be size-invariant because bias based on size is unjustified without additional semantic information.","In pursuit of this, we propose a generic approach that evaluates each salient object separately and then combines the results, effectively alleviating the imbalance.","We further develop an optimization framework tailored to this goal, achieving considerable improvements in detecting objects of different sizes.","Theoretically, we provide evidence supporting the validity of our new metrics and present the generalization analysis of SOD.","Extensive experiments demonstrate the effectiveness of our method.","The code is available at https://github.com/Ferry-Li/SI-SOD."],"url":"http://arxiv.org/abs/2405.09782v1","category":"cs.CV"}
{"created":"2024-05-16 02:46:44","title":"Beam Pattern Modulation Embedded Hybrid Transceiver Optimization for Integrated Sensing and Communication","abstract":"Integrated sensing and communication (ISAC) emerges as a promising technology for B5G/6G, particularly in the millimeter-wave (mmWave) band. However, the widely utilized hybrid architecture in mmWave systems compromises multiplexing gain due to the constraints of limited radio frequency chains. Moreover, additional sensing functionalities exacerbate the impairment of spectrum efficiency (SE). In this paper, we present an optimized beam pattern modulation-embedded ISAC (BPM-ISAC) transceiver design, which spares one RF chain for sensing and the others for communication. To compensate for the reduced SE, index modulation across communication beams is applied. We formulate an optimization problem aimed at minimizing the mean squared error (MSE) of the sensing beampattern, subject to a symbol MSE constraint. This problem is then solved by sequentially optimizing the analog and digital parts. Both the multi-aperture structure (MAS) and the multi-beam structure (MBS) are considered for the design of the analog part. We conduct theoretical analysis on the asymptotic pairwise error probability (APEP) and the Cram\\'er-Rao bound (CRB) of direction of arrival (DoA) estimation. Numerical simulations validate the overall enhanced ISAC performance over existing alternatives.","sentences":["Integrated sensing and communication (ISAC) emerges as a promising technology for B5G/6G, particularly in the millimeter-wave (mmWave) band.","However, the widely utilized hybrid architecture in mmWave systems compromises multiplexing gain due to the constraints of limited radio frequency chains.","Moreover, additional sensing functionalities exacerbate the impairment of spectrum efficiency (SE).","In this paper, we present an optimized beam pattern modulation-embedded ISAC (BPM-ISAC) transceiver design, which spares one RF chain for sensing and the others for communication.","To compensate for the reduced SE, index modulation across communication beams is applied.","We formulate an optimization problem aimed at minimizing the mean squared error (MSE) of the sensing beampattern, subject to a symbol MSE constraint.","This problem is then solved by sequentially optimizing the analog and digital parts.","Both the multi-aperture structure (MAS) and the multi-beam structure (MBS) are considered for the design of the analog part.","We conduct theoretical analysis on the asymptotic pairwise error probability (APEP) and the Cram\\'er-Rao bound (CRB) of direction of arrival (DoA) estimation.","Numerical simulations validate the overall enhanced ISAC performance over existing alternatives."],"url":"http://arxiv.org/abs/2405.09778v1","category":"eess.SP"}
{"created":"2024-05-16 02:10:42","title":"Clearing time randomization and transaction fees for auction market design","abstract":"Flaws of a continuous limit order book mechanism raise the question of whether a continuous trading session and a periodic auction session would bring better efficiency. This paper wants to go further in designing a periodic auction when both a continuous market and a periodic auction market are available to traders. In a periodic auction, we discover that a strategic trader could take advantage of the accumulated information available along the auction duration by arriving at the latest moment before the auction closes, increasing the price impact on the market. Such price impact moves the clearing price away from the efficient price and may disturb the efficiency of a periodic auction market. We thus propose and quantify the effect of two remedies to mitigate these flaws: randomizing the auction's closing time and optimally designing a transaction fees policy. Our results show that these policies encourage a strategic trader to send their orders earlier to enhance the efficiency of the auction market, illustrated by data extracted from Alphabet and Apple stocks.","sentences":["Flaws of a continuous limit order book mechanism raise the question of whether a continuous trading session and a periodic auction session would bring better efficiency.","This paper wants to go further in designing a periodic auction when both a continuous market and a periodic auction market are available to traders.","In a periodic auction, we discover that a strategic trader could take advantage of the accumulated information available along the auction duration by arriving at the latest moment before the auction closes, increasing the price impact on the market.","Such price impact moves the clearing price away from the efficient price and may disturb the efficiency of a periodic auction market.","We thus propose and quantify the effect of two remedies to mitigate these flaws: randomizing the auction's closing time and optimally designing a transaction fees policy.","Our results show that these policies encourage a strategic trader to send their orders earlier to enhance the efficiency of the auction market, illustrated by data extracted from Alphabet and Apple stocks."],"url":"http://arxiv.org/abs/2405.09764v1","category":"q-fin.TR"}
{"created":"2024-05-15 23:32:27","title":"The proper way to spatially decompose the gravitational-wave origin in stellar collapse simulations","abstract":"Gravitational waves (GWs) hold great potential for an unobscured view of protoneutron stars (PNSs) formed as a result of stellar collapses. While waiting for discovery, deepening the understanding of GW emission in theory is beneficial for both optimizing searching strategies and deciphering the eventual data. One significant aspect is the spatially dependent contribution to the overall GW signal extracted from sophisticated hydrodynamic simulations. I present the proper way to perform the spatial decomposition of GW strain with the quadrupole formula in the slow-motion and weak-field approximation. Then I demonstrate the approach using the results of a 2D axisymmetric pseudo-Newtonian hydrodynamic simulation of core-collapse supernova. I discuss the possible misleading interpretation based on the incorrect method in the literature that favors the dominant contribution by the PNS convective layer. Moreover, with the correct approach, the GW spatial profiles agree well with those calculated from a consistent perturbative method. This work re-emphasizes the global emission picture of GWs from PNS and motivates future prudent analyses with 3D simulations.","sentences":["Gravitational waves (GWs) hold great potential for an unobscured view of protoneutron stars (PNSs) formed as a result of stellar collapses.","While waiting for discovery, deepening the understanding of GW emission in theory is beneficial for both optimizing searching strategies and deciphering the eventual data.","One significant aspect is the spatially dependent contribution to the overall GW signal extracted from sophisticated hydrodynamic simulations.","I present the proper way to perform the spatial decomposition of GW strain with the quadrupole formula in the slow-motion and weak-field approximation.","Then I demonstrate the approach using the results of a 2D axisymmetric pseudo-Newtonian hydrodynamic simulation of core-collapse supernova.","I discuss the possible misleading interpretation based on the incorrect method in the literature that favors the dominant contribution by the PNS convective layer.","Moreover, with the correct approach, the GW spatial profiles agree well with those calculated from a consistent perturbative method.","This work re-emphasizes the global emission picture of GWs from PNS and motivates future prudent analyses with 3D simulations."],"url":"http://arxiv.org/abs/2405.09729v1","category":"astro-ph.HE"}
{"created":"2024-05-15 23:21:20","title":"Inference in higher-order undirected graphical models and binary polynomial optimization","abstract":"We consider the problem of inference in higher-order undirected graphical models with binary labels. We formulate this problem as a binary polynomial optimization problem and propose several linear programming relaxations for it. We compare the strength of the proposed linear programming relaxations theoretically. Finally, we demonstrate the effectiveness of these relaxations by performing a computational study for two important applications, namely, image restoration and decoding error-correcting codes.","sentences":["We consider the problem of inference in higher-order undirected graphical models with binary labels.","We formulate this problem as a binary polynomial optimization problem and propose several linear programming relaxations for it.","We compare the strength of the proposed linear programming relaxations theoretically.","Finally, we demonstrate the effectiveness of these relaxations by performing a computational study for two important applications, namely, image restoration and decoding error-correcting codes."],"url":"http://arxiv.org/abs/2405.09727v1","category":"math.OC"}
{"created":"2024-05-15 20:42:32","title":"Calibrating the WSA model in EUHFORIA based on PSP observations","abstract":"We employ Parker Solar Probe (PSP) observations during the latest solar minimum period (years 2018 -2021) to calibrate the version of the Wang-Sheeley-Arge (WSA) coronal model used in the European space weather forecasting tool EUHFORIA. WSA provides a set of boundary conditions at 0.1 au necessary to initiate the heliospheric part of EUHFORIA, namely, the domain extending beyond the solar Alfvenic point. To calibrate WSA, we observationally constrain four constants in the WSA semi-empirical formula based on PSP observations. We show how the updated (after the calibration) WSA boundary conditions at 0.1 au are compared to PSP observations at similar distances, and we further propagate these conditions in the heliosphere according to EUHFORIAs magnetohydrodynamic (MHD) approach. We assess the predictions at Earth based on the Dynamic Time Warping technique. Our findings suggest that, for the period of interest, the WSA configurations which resembled optimally the PSP observations close to the Sun, were different than the ones needed to provide better predictions at Earth. One reason for this discrepancy can be attributed to the scarcity of fast solar wind velocities recorded by PSP. The calibration of the model was performed based on unexpectedly slow velocities that did not allow us to achieve generally and globally improved solar wind predictions, compared to older studies. Other reasons can be attributed to missing physical processes from the heliospheric part of EUHFORIA but also the fact that the currently employed WSA relationship, as coupled to the heliospheric MHD domain, may need a global reformulation beyond that of just updating the four constant factors that were taken into account in this study.","sentences":["We employ Parker Solar Probe (PSP) observations during the latest solar minimum period (years 2018 -2021) to calibrate the version of the Wang-Sheeley-Arge (WSA) coronal model used in the European space weather forecasting tool EUHFORIA.","WSA provides a set of boundary conditions at 0.1 au necessary to initiate the heliospheric part of EUHFORIA, namely, the domain extending beyond the solar Alfvenic point.","To calibrate WSA, we observationally constrain four constants in the WSA semi-empirical formula based on PSP observations.","We show how the updated (after the calibration)","WSA boundary conditions at 0.1 au are compared to PSP observations at similar distances, and we further propagate these conditions in the heliosphere according to EUHFORIAs magnetohydrodynamic (MHD) approach.","We assess the predictions at Earth based on the Dynamic Time Warping technique.","Our findings suggest that, for the period of interest, the WSA configurations which resembled optimally the PSP observations close to the Sun, were different than the ones needed to provide better predictions at Earth.","One reason for this discrepancy can be attributed to the scarcity of fast solar wind velocities recorded by PSP.","The calibration of the model was performed based on unexpectedly slow velocities that did not allow us to achieve generally and globally improved solar wind predictions, compared to older studies.","Other reasons can be attributed to missing physical processes from the heliospheric part of EUHFORIA but also the fact that the currently employed WSA relationship, as coupled to the heliospheric MHD domain, may need a global reformulation beyond that of just updating the four constant factors that were taken into account in this study."],"url":"http://arxiv.org/abs/2405.09693v1","category":"astro-ph.SR"}
{"created":"2024-05-15 19:38:47","title":"Homogenization of non-local energies on disconnected sets","abstract":"We consider the problem of the homogenization of non-local quadratic energies defined on $\\delta$-periodic disconnected sets defined by a double integral, depending on a kernel concentrated at scale $\\varepsilon$. For kernels with unbounded support we show that we may have three regimes: (i) $\\varepsilon<\\!<\\delta$, for which the $\\Gamma$-limit even in the strong topology of $L^2$ is $0$; (ii) $\\frac\\varepsilon\\delta\\to\\kappa$, in which the energies are coercive with respect to a convergence of interpolated functions, and the limit is governed by a non-local homogenization formula parameterized by $\\kappa$; (iii) $\\delta<\\!<\\varepsilon$, for which the $\\Gamma$-limit is computed with respect to a coarse-grained convergence and exhibits a separation-of-scales effect; namely, it is the same as the one obtained by formally first letting $\\delta\\to 0$ (which turns out to be a pointwise weak limit, thanks to an iterated use of Jensen's inequality), and then, noting that the outcome is a nonlocal energy studied by Bourgain, Brezis and Mironescu, letting $\\varepsilon\\to0$. A slightly more complex description is necessary for case (ii) if the kernel is compactly supported.","sentences":["We consider the problem of the homogenization of non-local quadratic energies defined on $\\delta$-periodic disconnected sets defined by a double integral, depending on a kernel concentrated at scale $\\varepsilon$. For kernels with unbounded support we show that we may have three regimes: (i) $\\varepsilon<\\!<\\delta$, for which the $\\Gamma$-limit even in the strong topology of $L^2$ is $0$; (ii) $\\frac\\varepsilon\\delta\\to\\kappa$, in which the energies are coercive with respect to a convergence of interpolated functions, and the limit is governed by a non-local homogenization formula parameterized by $\\kappa$; (iii) $\\delta<\\!<\\varepsilon$, for which the $\\Gamma$-limit is computed with respect to a coarse-grained convergence and exhibits a separation-of-scales effect; namely, it is the same as the one obtained by formally first letting $\\delta\\to 0$ (which turns out to be a pointwise weak limit, thanks to an iterated use of Jensen's inequality), and then, noting that the outcome is a nonlocal energy studied by Bourgain, Brezis and Mironescu, letting $\\varepsilon\\to0$. A slightly more complex description is necessary for case (ii) if the kernel is compactly supported."],"url":"http://arxiv.org/abs/2405.09677v1","category":"math.AP"}
{"created":"2024-05-15 19:25:17","title":"Eulerian-Lagrangian Fluid Simulation on Particle Flow Maps","abstract":"We propose a novel Particle Flow Map (PFM) method to enable accurate long-range advection for incompressible fluid simulation. The foundation of our method is the observation that a particle trajectory generated in a forward simulation naturally embodies a perfect flow map. Centered on this concept, we have developed an Eulerian-Lagrangian framework comprising four essential components: Lagrangian particles for a natural and precise representation of bidirectional flow maps; a dual-scale map representation to accommodate the mapping of various flow quantities; a particle-to-grid interpolation scheme for accurate quantity transfer from particles to grid nodes; and a hybrid impulse-based solver to enforce incompressibility on the grid. The efficacy of PFM has been demonstrated through various simulation scenarios, highlighting the evolution of complex vortical structures and the details of turbulent flows. Notably, compared to NFM, PFM reduces computing time by up to 49 times and memory consumption by up to 41%, while enhancing vorticity preservation as evidenced in various tests like leapfrog, vortex tube, and turbulent flow.","sentences":["We propose a novel Particle Flow Map (PFM) method to enable accurate long-range advection for incompressible fluid simulation.","The foundation of our method is the observation that a particle trajectory generated in a forward simulation naturally embodies a perfect flow map.","Centered on this concept, we have developed an Eulerian-Lagrangian framework comprising four essential components: Lagrangian particles for a natural and precise representation of bidirectional flow maps; a dual-scale map representation to accommodate the mapping of various flow quantities; a particle-to-grid interpolation scheme for accurate quantity transfer from particles to grid nodes; and a hybrid impulse-based solver to enforce incompressibility on the grid.","The efficacy of PFM has been demonstrated through various simulation scenarios, highlighting the evolution of complex vortical structures and the details of turbulent flows.","Notably, compared to NFM, PFM reduces computing time by up to 49 times and memory consumption by up to 41%, while enhancing vorticity preservation as evidenced in various tests like leapfrog, vortex tube, and turbulent flow."],"url":"http://arxiv.org/abs/2405.09672v1","category":"cs.GR"}
{"created":"2024-05-15 05:33:49","title":"Restless Bandit Problem with Rewards Generated by a Linear Gaussian Dynamical System","abstract":"The stochastic multi-armed bandit problem studies decision-making under uncertainty. In the problem, the learner interacts with an environment by choosing an action at each round, where a round is an instance of an interaction. In response, the environment reveals a reward, which is sampled from a stochastic process, to the learner. The goal of the learner is to maximize cumulative reward. A specific variation of the stochastic multi-armed bandit problem is the restless bandit, where the reward for each action is sampled from a Markov chain. The restless bandit with a discrete state-space is a well-studied problem, but to the best of our knowledge, not many results exist for the continuous state-space version which has many applications such as hyperparameter optimization. In this work, we tackle the restless bandit with continuous state-space by assuming the rewards are the inner product of an action vector and a state vector generated by a linear Gaussian dynamical system. To predict the reward for each action, we propose a method that takes a linear combination of previously observed rewards for predicting each action's next reward. We show that, regardless of the sequence of previous actions chosen, the reward sampled for any previously chosen action can be used for predicting another action's future reward, i.e. the reward sampled for action 1 at round $t-1$ can be used for predicting the reward for action $2$ at round $t$. This is accomplished by designing a modified Kalman filter with a matrix representation that can be learned for reward prediction. Numerical evaluations are carried out on a set of linear Gaussian dynamical systems.","sentences":["The stochastic multi-armed bandit problem studies decision-making under uncertainty.","In the problem, the learner interacts with an environment by choosing an action at each round, where a round is an instance of an interaction.","In response, the environment reveals a reward, which is sampled from a stochastic process, to the learner.","The goal of the learner is to maximize cumulative reward.","A specific variation of the stochastic multi-armed bandit problem is the restless bandit, where the reward for each action is sampled from a Markov chain.","The restless bandit with a discrete state-space is a well-studied problem, but to the best of our knowledge, not many results exist for the continuous state-space version which has many applications such as hyperparameter optimization.","In this work, we tackle the restless bandit with continuous state-space by assuming the rewards are the inner product of an action vector and a state vector generated by a linear Gaussian dynamical system.","To predict the reward for each action, we propose a method that takes a linear combination of previously observed rewards for predicting each action's next reward.","We show that, regardless of the sequence of previous actions chosen, the reward sampled for any previously chosen action can be used for predicting another action's future reward, i.e. the reward sampled for action 1 at round $t-1$ can be used for predicting the reward for action $2$ at round $t$. This is accomplished by designing a modified Kalman filter with a matrix representation that can be learned for reward prediction.","Numerical evaluations are carried out on a set of linear Gaussian dynamical systems."],"url":"http://arxiv.org/abs/2405.09584v1","category":"stat.ML"}
{"created":"2024-05-16 17:38:38","title":"The dynamics and electromagnetic signatures of accretion in unequal mass binary black hole inspirals","abstract":"We present a theoretical study of the gravitational wave (GW) driven inspirals of accreting black hole binaries with mass $M = 10^7 M_\\odot$ and mass ratios between $10^{-3}$ and $10^{-1}$. Our results are based on analytic estimates, and grid-based hydrodynamics simulations run for many thousands of binary orbits before the merger. We show that the GW inspiral is evident in the light curves and color evolution of a binary-hosting quasar, over years to decades before a merger. The long-term electromagnetic (EM) signature is characterized by a gradual UV brightening, and X-ray dimming, followed by an X-ray disappearance hours to days before the GW burst, and finally a years-like re-brightening as the disk relaxes and refuels the remnant black hole. These timescales are surprisingly insensitive to the amplitude of viscous stress in the disk. The spectrum of quasi-thermal disk emission shows two peaks: one in the UV, and another in the X-ray, associated with the outer and circum-secondary disks respectively; emission from the inner disk is suppressed because the secondary consumes most of the inflowing gas. We discuss implications for real-time and archival EM followup of GW bursts detected by LISA.","sentences":["We present a theoretical study of the gravitational wave (GW) driven inspirals of accreting black hole binaries with mass $M = 10^7 M_\\odot$ and mass ratios between $10^{-3}$ and $10^{-1}$. Our results are based on analytic estimates, and grid-based hydrodynamics simulations run for many thousands of binary orbits before the merger.","We show that the GW inspiral is evident in the light curves and color evolution of a binary-hosting quasar, over years to decades before a merger.","The long-term electromagnetic (EM) signature is characterized by a gradual UV brightening, and X-ray dimming, followed by an X-ray disappearance hours to days before the GW burst, and finally a years-like re-brightening as the disk relaxes and refuels the remnant black hole.","These timescales are surprisingly insensitive to the amplitude of viscous stress in the disk.","The spectrum of quasi-thermal disk emission shows two peaks: one in the UV, and another in the X-ray, associated with the outer and circum-secondary disks respectively; emission from the inner disk is suppressed because the secondary consumes most of the inflowing gas.","We discuss implications for real-time and archival EM followup of GW bursts detected by LISA."],"url":"http://arxiv.org/abs/2405.10281v1","category":"astro-ph.HE"}
{"created":"2024-05-16 17:30:55","title":"Simultaneous Haar Indistinguishability with Applications to Unclonable Cryptography","abstract":"Unclonable cryptography is concerned with leveraging the no-cloning principle to build cryptographic primitives that are otherwise impossible to achieve classically. Understanding the feasibility of unclonable encryption, one of the key unclonable primitives, satisfying indistinguishability security in the plain model has been a major open question in the area. So far, the existing constructions of unclonable encryption are either in the quantum random oracle model or are based on new conjectures.   We present a new approach to unclonable encryption via a reduction to a novel question about nonlocal quantum state discrimination: how well can non-communicating -- but entangled -- players distinguish between different distributions over quantum states? We call this task simultaneous state indistinguishability. Our main technical result is showing that the players cannot distinguish between each player receiving independently-chosen Haar random states versus all players receiving the same Haar random state.   We leverage this result to present the first construction of unclonable encryption satisfying indistinguishability security, with quantum decryption keys, in the plain model. We also show other implications to single-decryptor encryption and leakage-resilient secret sharing.","sentences":["Unclonable cryptography is concerned with leveraging the no-cloning principle to build cryptographic primitives that are otherwise impossible to achieve classically.","Understanding the feasibility of unclonable encryption, one of the key unclonable primitives, satisfying indistinguishability security in the plain model has been a major open question in the area.","So far, the existing constructions of unclonable encryption are either in the quantum random oracle model or are based on new conjectures.   ","We present a new approach to unclonable encryption via a reduction to a novel question about nonlocal quantum state discrimination: how well can non-communicating -- but entangled -- players distinguish between different distributions over quantum states?","We call this task simultaneous state indistinguishability.","Our main technical result is showing that the players cannot distinguish between each player receiving independently-chosen Haar random states versus all players receiving the same Haar random state.   ","We leverage this result to present the first construction of unclonable encryption satisfying indistinguishability security, with quantum decryption keys, in the plain model.","We also show other implications to single-decryptor encryption and leakage-resilient secret sharing."],"url":"http://arxiv.org/abs/2405.10274v1","category":"quant-ph"}
{"created":"2024-05-16 17:24:21","title":"The Magic in Nuclear and Hypernuclear Forces","abstract":"Toward an improved understanding of the role of quantum information in nuclei and exotic matter, we examine the magic (non-stabilizerness) in low-energy strong interaction processes. As stabilizer states can be prepared efficiently using classical computers, and include classes of entangled states, it is magic and fluctuations in magic, along with entanglement, that determine resource requirements for quantum simulations. As a measure of fluctuations in magic induced by scattering, the \"magic power\" of the S-matrix is introduced. Using experimentally-determined scattering phase shifts and mixing parameters, the magic power in nucleon-nucleon and hyperon-nucleon scattering, along with the magic in the deuteron, are found to exhibit interesting features. The $\\Sigma^-$-baryon is identified as a potential candidate catalyst for enhanced spreading of magic and entanglement in dense matter, depending on in-medium decoherence.","sentences":["Toward an improved understanding of the role of quantum information in nuclei and exotic matter, we examine the magic (non-stabilizerness) in low-energy strong interaction processes.","As stabilizer states can be prepared efficiently using classical computers, and include classes of entangled states, it is magic and fluctuations in magic, along with entanglement, that determine resource requirements for quantum simulations.","As a measure of fluctuations in magic induced by scattering, the \"magic power\" of the S-matrix is introduced.","Using experimentally-determined scattering phase shifts and mixing parameters, the magic power in nucleon-nucleon and hyperon-nucleon scattering, along with the magic in the deuteron, are found to exhibit interesting features.","The $\\Sigma^-$-baryon is identified as a potential candidate catalyst for enhanced spreading of magic and entanglement in dense matter, depending on in-medium decoherence."],"url":"http://arxiv.org/abs/2405.10268v1","category":"nucl-th"}
{"created":"2024-05-16 17:13:06","title":"Summary of CKM 2023 working group 5: Direct CP violation (DCPV) including $\u03c6_{3}/\u03b3$ from $B\\to DK$, DCPV effects, branching fractions and polarisation in charmless $B_{(s)}$ decays","abstract":"In this contribution a summary of the activities of Working Group 5 (WG5) presented during the 12th International Workshop on the CKM Unitarity Triangle (CKM2023) is reported. This includes new results on $\\phi_{3}/\\gamma$ measurements using $B\\to DK$ decays, search for $CP$ violation using charmless $B$ decays and $b$-Baryon decays, measurement of branching ratios in hadronic $B$ to charm decays, and theory of three-body nonleptonic $B$ decays.","sentences":["In this contribution a summary of the activities of Working Group 5 (WG5) presented during the 12th International Workshop on the CKM Unitarity Triangle (CKM2023) is reported.","This includes new results on $\\phi_{3}/\\gamma$ measurements using $B\\to DK$ decays, search for $CP$ violation using charmless $B$ decays and $b$-Baryon decays, measurement of branching ratios in hadronic $B$ to charm decays, and theory of three-body nonleptonic $B$ decays."],"url":"http://arxiv.org/abs/2405.10261v1","category":"hep-ex"}
{"created":"2024-05-16 16:10:41","title":"SoK: Prudent Evaluation Practices for Fuzzing","abstract":"Fuzzing has proven to be a highly effective approach to uncover software bugs over the past decade. After AFL popularized the groundbreaking concept of lightweight coverage feedback, the field of fuzzing has seen a vast amount of scientific work proposing new techniques, improving methodological aspects of existing strategies, or porting existing methods to new domains. All such work must demonstrate its merit by showing its applicability to a problem, measuring its performance, and often showing its superiority over existing works in a thorough, empirical evaluation. Yet, fuzzing is highly sensitive to its target, environment, and circumstances, e.g., randomness in the testing process. After all, relying on randomness is one of the core principles of fuzzing, governing many aspects of a fuzzer's behavior. Combined with the often highly difficult to control environment, the reproducibility of experiments is a crucial concern and requires a prudent evaluation setup. To address these threats to validity, several works, most notably Evaluating Fuzz Testing by Klees et al., have outlined how a carefully designed evaluation setup should be implemented, but it remains unknown to what extent their recommendations have been adopted in practice. In this work, we systematically analyze the evaluation of 150 fuzzing papers published at the top venues between 2018 and 2023. We study how existing guidelines are implemented and observe potential shortcomings and pitfalls. We find a surprising disregard of the existing guidelines regarding statistical tests and systematic errors in fuzzing evaluations. For example, when investigating reported bugs, ...","sentences":["Fuzzing has proven to be a highly effective approach to uncover software bugs over the past decade.","After AFL popularized the groundbreaking concept of lightweight coverage feedback, the field of fuzzing has seen a vast amount of scientific work proposing new techniques, improving methodological aspects of existing strategies, or porting existing methods to new domains.","All such work must demonstrate its merit by showing its applicability to a problem, measuring its performance, and often showing its superiority over existing works in a thorough, empirical evaluation.","Yet, fuzzing is highly sensitive to its target, environment, and circumstances, e.g., randomness in the testing process.","After all, relying on randomness is one of the core principles of fuzzing, governing many aspects of a fuzzer's behavior.","Combined with the often highly difficult to control environment, the reproducibility of experiments is a crucial concern and requires a prudent evaluation setup.","To address these threats to validity, several works, most notably Evaluating Fuzz Testing by Klees et al., have outlined how a carefully designed evaluation setup should be implemented, but it remains unknown to what extent their recommendations have been adopted in practice.","In this work, we systematically analyze the evaluation of 150 fuzzing papers published at the top venues between 2018 and 2023.","We study how existing guidelines are implemented and observe potential shortcomings and pitfalls.","We find a surprising disregard of the existing guidelines regarding statistical tests and systematic errors in fuzzing evaluations.","For example, when investigating reported bugs, ..."],"url":"http://arxiv.org/abs/2405.10220v1","category":"cs.SE"}
{"created":"2024-05-16 16:10:30","title":"Current Views on Mechanisms of the FLASH Effect in Cancer Radiotherapy","abstract":"FLASH radiotherapy (FLASH-RT) is a new modality of radiotherapy by delivering doses with ultra-high dose rates. FLASH-RT has the ability to suppress tumor growth while sparing normal tissues, known as the FLASH effect. Although FLASH effect has proved valid in various models by different ionizing radiations, the exact underlying mechanism is still unclear. This article summarizes mainstream hypotheses of FLASH effect at physicochemical and biological levels, including oxygen depletion and free radical reactions, nuclear and mitochondria damage, as well as immune response. These hypotheses contribute reasonable explanations to the FLASH effect, and are interconnected according to the chronological order of the organism's response to ionizing radiation. By collating the existing consensus, evidence, and hypotheses, this article provides a comprehensive overview of potential mechanisms of FLASH effect and practical guidance for future investigation in the field of FLASH-RT.","sentences":["FLASH radiotherapy (FLASH-RT) is a new modality of radiotherapy by delivering doses with ultra-high dose rates.","FLASH-RT has the ability to suppress tumor growth while sparing normal tissues, known as the FLASH effect.","Although FLASH effect has proved valid in various models by different ionizing radiations, the exact underlying mechanism is still unclear.","This article summarizes mainstream hypotheses of FLASH effect at physicochemical and biological levels, including oxygen depletion and free radical reactions, nuclear and mitochondria damage, as well as immune response.","These hypotheses contribute reasonable explanations to the FLASH effect, and are interconnected according to the chronological order of the organism's response to ionizing radiation.","By collating the existing consensus, evidence, and hypotheses, this article provides a comprehensive overview of potential mechanisms of FLASH effect and practical guidance for future investigation in the field of FLASH-RT."],"url":"http://arxiv.org/abs/2405.10219v1","category":"physics.med-ph"}
{"created":"2024-05-16 15:50:48","title":"Large-$N$ integrated correlators in $\\mathcal{N}=4$ SYM: when resurgence meets modularity","abstract":"Exact expressions for certain integrated correlators of four half-BPS operators in $\\mathcal{N}=4$ supersymmetric Yang-Mills theory with gauge group $SU(N)$ have been recently obtained thanks to a beautiful interplay between supersymmetric localisation and modular invariance. The large-$N$ expansion at fixed Yang-Mills coupling of such integrated correlators produces an asymptotic series of perturbative terms, holographically related to higher derivative interactions in the low energy expansion of the type IIB effective action, as well as exponentially suppressed corrections at large $N$, interpreted as contributions from coincident $(p,q)$-string world-sheet instantons. In this work we define a manifestly modular invariant Borel resummation of the perturbative large-$N$ expansion of these integrated correlators, from which we extract the exact non-perturbative large-$N$ sectors via resurgence analysis. Furthermore, we show that in the 't Hooft limit such modular invariant non-perturbative completions reduce to known resurgent genus expansions. Finally, we clarify how the same non-perturbative data is encoded in the decomposition of the integrated correlators based on $\\rm{SL}(2,\\mathbb{Z})$ spectral theory.","sentences":["Exact expressions for certain integrated correlators of four half-BPS operators in $\\mathcal{N}=4$ supersymmetric Yang-Mills theory with gauge group $SU(N)$ have been recently obtained thanks to a beautiful interplay between supersymmetric localisation and modular invariance.","The large-$N$ expansion at fixed Yang-Mills coupling of such integrated correlators produces an asymptotic series of perturbative terms, holographically related to higher derivative interactions in the low energy expansion of the type IIB effective action, as well as exponentially suppressed corrections at large $N$, interpreted as contributions from coincident $(p,q)$-string world-sheet instantons.","In this work we define a manifestly modular invariant Borel resummation of the perturbative large-$N$ expansion of these integrated correlators, from which we extract the exact non-perturbative large-$N$ sectors via resurgence analysis.","Furthermore, we show that in the 't Hooft limit such modular invariant non-perturbative completions reduce to known resurgent genus expansions.","Finally, we clarify how the same non-perturbative data is encoded in the decomposition of the integrated correlators based on $\\rm{SL}(2,\\mathbb{Z})$ spectral theory."],"url":"http://arxiv.org/abs/2405.10204v1","category":"hep-th"}
{"created":"2024-05-16 15:33:42","title":"Amenable actions on ill-behaved simple C*-algebras","abstract":"By combining R{\\o}rdam's construction and author's previous construction, we provide the first examples of amenable actions on simple separable nuclear C*-algebras that are neither stable finite nor purely infinite. For free groups, we also provide unital examples. We arrange the actions so that the crossed products are still simple with both a finite and an infinite projection.","sentences":["By combining R{\\o}rdam's construction and author's previous construction, we provide the first examples of amenable actions on simple separable nuclear C*-algebras that are neither stable finite nor purely infinite.","For free groups, we also provide unital examples.","We arrange the actions so that the crossed products are still simple with both a finite and an infinite projection."],"url":"http://arxiv.org/abs/2405.10191v1","category":"math.OA"}
{"created":"2024-05-16 15:23:08","title":"Model-independent Reconstruction of UV Luminosity Function and Reionization Epoch","abstract":"We conduct a first comprehensive study of the Luminosity Function (LF) using a non-parametric approach. We use Gaussian Process to fit available luminosity data between redshifts $z \\sim 2-8$. Our free-form LF in the non-parametric approach rules out the conventional Schechter function model to describe the luminosity-magnitude relation at redshifts $z=3$ and $4$. Hints of deviation from the Schechter function are also noticed at redshifts 2, 7 and 8 at lower statistical significance. Significant deviation starts for brighter ionizing sources at $M_{\\rm UV} \\lesssim -21$. The UV luminosity density data at different redshifts are then derived by integrating the LFs obtained from both methods with a truncation magnitude of $-17$. In our analysis, we also include the first 90 arcmin$^2$ JWST/NIRCam data at $z \\sim 9-12$. Since at larger magnitudes, we do not find major deviations from the Schechter function, the integrated luminosity density differs marginally between the two methods. Finally, we obtain the history of reionization from a joint analysis of UV luminosity density data along with the ionization fraction data and Planck observation of Cosmic Microwave Background. The history of reionization is not affected by the deviation of LFs from Schechter function at lower magnitudes. We derive reionization optical depth to be $\\tau_{\\rm re}=0.0494^{+0.0007}_{-0.0006}$ and the duration between 10$\\% $ and 90$\\%$ completion of ionization process is found to be $\\Delta z\\sim 1.627^{+0.059}_{-0.071}$.","sentences":["We conduct a first comprehensive study of the Luminosity Function (LF) using a non-parametric approach.","We use Gaussian Process to fit available luminosity data between redshifts $z \\sim 2-8$. Our free-form LF in the non-parametric approach rules out the conventional Schechter function model to describe the luminosity-magnitude relation at redshifts $z=3$ and $4$. Hints of deviation from the Schechter function are also noticed at redshifts 2, 7 and 8 at lower statistical significance.","Significant deviation starts for brighter ionizing sources at $M_{\\rm UV} \\lesssim -21$.","The UV luminosity density data at different redshifts are then derived by integrating the LFs obtained from both methods with a truncation magnitude of $-17$.","In our analysis, we also include the first 90 arcmin$^2$ JWST/NIRCam data at $z \\sim 9-12$. Since at larger magnitudes, we do not find major deviations from the Schechter function, the integrated luminosity density differs marginally between the two methods.","Finally, we obtain the history of reionization from a joint analysis of UV luminosity density data along with the ionization fraction data and Planck observation of Cosmic Microwave Background.","The history of reionization is not affected by the deviation of LFs from Schechter function at lower magnitudes.","We derive reionization optical depth to be $\\tau_{\\rm re}=0.0494^{+0.0007}_{-0.0006}$ and the duration between 10$\\% $ and 90$\\%$ completion of ionization process is found to be $\\Delta z\\sim 1.627^{+0.059}_{-0.071}$."],"url":"http://arxiv.org/abs/2405.10180v1","category":"astro-ph.GA"}
{"created":"2024-05-16 14:34:58","title":"The flaring activity of blazar AO 0235+164 during year 2021","abstract":"Context. The blazar AO 0235+164, located at redshift $z=0.94$, has displayed interesting and repeating flaring activity in the past, the latest episodes occurring in 2008 and 2015. In 2020, the source brightened again, starting a new flaring episode that peaked in 2021. Aims. We study the origin and properties of the 2021 flare in relation to previous studies and the historical behavior of the source, in particular to the 2008 and 2015 flaring episodes. Methods. We analyze the multi-wavelength photo-polarimetric evolution of the source. From Very Long Baseline Array images, we derive the kinematic parameters of new components associated with the 2021 flare. We use this information to constrain a model for the spectral energy distribution of the emission during the flaring period. We propose an analytical geometric model to test whether the observed wobbling of the jet is consistent with precession. Results. We report the appearance of two new components that are ejected in a different direction than previously, confirming the wobbling of the jet. We find that the direction of ejection is consistent with that of a precessing jet.The derived period independently agrees with the values commonly found in the literature. Modeling of the spectral energy distribution further confirm that the differences between flares can be attributed to geometrical effects.","sentences":["Context.","The blazar AO 0235+164, located at redshift $z=0.94$, has displayed interesting and repeating flaring activity in the past, the latest episodes occurring in 2008 and 2015.","In 2020, the source brightened again, starting a new flaring episode that peaked in 2021.","Aims.","We study the origin and properties of the 2021 flare in relation to previous studies and the historical behavior of the source, in particular to the 2008 and 2015 flaring episodes.","Methods.","We analyze the multi-wavelength photo-polarimetric evolution of the source.","From Very Long Baseline Array images, we derive the kinematic parameters of new components associated with the 2021 flare.","We use this information to constrain a model for the spectral energy distribution of the emission during the flaring period.","We propose an analytical geometric model to test whether the observed wobbling of the jet is consistent with precession.","Results.","We report the appearance of two new components that are ejected in a different direction than previously, confirming the wobbling of the jet.","We find that the direction of ejection is consistent with that of a precessing jet.","The derived period independently agrees with the values commonly found in the literature.","Modeling of the spectral energy distribution further confirm that the differences between flares can be attributed to geometrical effects."],"url":"http://arxiv.org/abs/2405.10141v1","category":"astro-ph.HE"}
{"created":"2024-05-16 14:34:04","title":"Mental Well-being Opportunities in Interacting and Reflecting with Personal Data Sculptures of EEG","abstract":"Data physicalization is a research area in quick expansion whose necessity and popularity are motivated by the pervasiveness of data in our everyday. While the reflective ability of personal data physicalization has been vastly documented, their mental health and emotional well-being benefits remain largely unexplored. We present a qualitative study where we create personal data sculptures of electroencephalograms (EEG) and mental activity, observe users' interactions with them, and analyze their reflections for hints of self-discovery and intended behavioral change. We argue that there is a ground for using personal data sculptures as prompts for reflection on mental well-being and motivators for self-caring, and that data sculptures for mental well-being are a finalized use of data physicalization worth exploring further.","sentences":["Data physicalization is a research area in quick expansion whose necessity and popularity are motivated by the pervasiveness of data in our everyday.","While the reflective ability of personal data physicalization has been vastly documented, their mental health and emotional well-being benefits remain largely unexplored.","We present a qualitative study where we create personal data sculptures of electroencephalograms (EEG) and mental activity, observe users' interactions with them, and analyze their reflections for hints of self-discovery and intended behavioral change.","We argue that there is a ground for using personal data sculptures as prompts for reflection on mental well-being and motivators for self-caring, and that data sculptures for mental well-being are a finalized use of data physicalization worth exploring further."],"url":"http://arxiv.org/abs/2405.10139v1","category":"cs.HC"}
{"created":"2024-05-16 14:05:47","title":"Isospinning ${\\mathbb C}P^2$ solitons","abstract":"We study stationary rotating topological solitons in (2+1)-dimensional ${\\mathbb C}P^2$ non-linear sigma model with a stabilizing potential term. We find families of $U(1)\\times U(1)$ symmetric solutions with topological degrees larger than 2, which have two angular frequencies and are labelled by two (one topological and the other non-topological) winding numbers $k_1>k_2$. We discuss properties of these solitons and investigate the domains of their existence.","sentences":["We study stationary rotating topological solitons in (2+1)-dimensional ${\\mathbb C}P^2$ non-linear sigma model with a stabilizing potential term.","We find families of $U(1)\\times U(1)$ symmetric solutions with topological degrees larger than 2, which have two angular frequencies and are labelled by two (one topological and the other non-topological) winding numbers $k_1>k_2$.","We discuss properties of these solitons and investigate the domains of their existence."],"url":"http://arxiv.org/abs/2405.10114v1","category":"hep-th"}
{"created":"2024-05-16 13:24:32","title":"The Tracking Tapered Gridded Estimator for the 21-cm power spectrum from MWA drift scan observations I: Validation and preliminary results","abstract":"Drift scan observations provide the broad sky coverage and instrumental stability needed to measure the Epoch of Reionization (EoR) 21-cm signal. In such observations, the telescope's pointing center (PC) moves continuously on the sky. The Tracking Tapered Gridded Estimator (TTGE) combines observations from different PC to estimate $P(k_{\\perp}, k_{\\parallel})$ the 21-cm power spectrum, centered on a tracking center (TC) which remains fixed on the sky. The tapering further restricts the sky response to a small angular region around TC, thereby mitigating wide-field foregrounds. Here we consider $154.2 \\, {\\rm MHz}$ ($z = 8.2$) Murchison Widefield Array (MWA) drift scan observations. The periodic pattern of flagged channels, present in MWA data, is known to introduce artefacts which pose a challenge for estimating $P(k_{\\perp}, k_{\\parallel})$. We demonstrate that the TTGE is able to recover $P(k_{\\perp}, k_{\\parallel})$ without any artefacts, and estimate $P(k)$ within $5 \\%$ accuracy over a large $k$-range. We also present preliminary results for a single PC, combining 9 nights of observation $(17 \\, {\\rm min}$ total). We find that $P(k_{\\perp}, k_{\\parallel})$ exhibits streaks at a fixed interval of $k_{\\parallel}=0.29 \\, {\\rm Mpc}^{-1}$, which matches $\\Delta \\nu_{\\rm per}=1.28 \\, {\\rm MHz}$ that is the period of the flagged channels. The streaks are not as pronounced at larger $k_{\\parallel}$, and in some cases they do not appear to extend across the entire $k_{\\perp}$ range. The rectangular region $0.05 \\leq k_{\\perp} \\leq 0.16 \\, {\\rm Mpc^{-1}}$ and $0.9 \\leq k_{\\parallel} \\leq 4.6 \\, {\\rm Mpc^{-1}}$ is found to be relatively free of foreground contamination and artefacts, and we have used this to place the $2\\sigma$ upper limit $\\Delta^2(k) < (1.85 \\times 10^4)^2\\, {\\rm mK^2}$ on the EoR 21-cm mean squared brightness temperature fluctuations at $k=1 \\,{\\rm Mpc}^{-1}$.","sentences":["Drift scan observations provide the broad sky coverage and instrumental stability needed to measure the Epoch of Reionization (EoR) 21-cm signal.","In such observations, the telescope's pointing center (PC) moves continuously on the sky.","The Tracking Tapered Gridded Estimator (TTGE) combines observations from different PC to estimate $P(k_{\\perp}, k_{\\parallel})$ the 21-cm power spectrum, centered on a tracking center (TC) which remains fixed on the sky.","The tapering further restricts the sky response to a small angular region around TC, thereby mitigating wide-field foregrounds.","Here we consider $154.2 \\, {\\rm MHz}$ ($z = 8.2$) Murchison Widefield Array (MWA) drift scan observations.","The periodic pattern of flagged channels, present in MWA data, is known to introduce artefacts which pose a challenge for estimating $P(k_{\\perp}, k_{\\parallel})$. We demonstrate that the TTGE is able to recover $P(k_{\\perp}, k_{\\parallel})$ without any artefacts, and estimate $P(k)$ within $5 \\%$ accuracy over a large $k$-range.","We also present preliminary results for a single PC, combining 9 nights of observation $(17 \\, {\\rm min}$ total).","We find that $P(k_{\\perp}, k_{\\parallel})$ exhibits streaks at a fixed interval of $k_{\\parallel}=0.29 \\, {\\rm Mpc}^{-1}$, which matches $\\Delta \\nu_{\\rm per}=1.28 \\, {\\rm MHz}$ that is the period of the flagged channels.","The streaks are not as pronounced at larger $k_{\\parallel}$, and in some cases they do not appear to extend across the entire $k_{\\perp}$ range.","The rectangular region $0.05 \\leq k_{\\perp} \\leq 0.16 \\, {\\rm Mpc^{-1}}$ and $0.9 \\leq k_{\\parallel} \\leq 4.6 \\, {\\rm Mpc^{-1}}$ is found to be relatively free of foreground contamination and artefacts, and we have used this to place the $2\\sigma$ upper limit $\\Delta^2(k) <","(1.85 \\times 10^4)^2\\, {\\rm mK^2}$ on the EoR 21-cm mean squared brightness temperature fluctuations at $k=1 \\,{\\rm Mpc}^{-1}$."],"url":"http://arxiv.org/abs/2405.10080v1","category":"astro-ph.CO"}
{"created":"2024-05-16 12:49:52","title":"Discussing Risks and Benefits in the Future of Hybrid Rehabilitation and Fitness in Mixed Reality","abstract":"In a world where in-person context transitions more into remote and hybrid concepts, we should consider new concepts of interaction in health and rehabilitation and what advantages and disadvantages they bring. One of the rising topics is mixed reality, where we can use the advantages of immersive 3D, 360-degree environments. Meanwhile, physical activity is further decreasing and with it negative effects increase through sedentary behaviour or wrong and untrained movements. In this position paper, we discuss these new risks and potential benefits of mixed reality technology when used for rehabilitation and fitness. We conclude with suggesting better feedback and guidance for physical movement and tasks at home. Improving feedback and guidance for participants could be achieved through using new technologies like virtual reality and motion tracking.","sentences":["In a world where in-person context transitions more into remote and hybrid concepts, we should consider new concepts of interaction in health and rehabilitation and what advantages and disadvantages they bring.","One of the rising topics is mixed reality, where we can use the advantages of immersive 3D, 360-degree environments.","Meanwhile, physical activity is further decreasing and with it negative effects increase through sedentary behaviour or wrong and untrained movements.","In this position paper, we discuss these new risks and potential benefits of mixed reality technology when used for rehabilitation and fitness.","We conclude with suggesting better feedback and guidance for physical movement and tasks at home.","Improving feedback and guidance for participants could be achieved through using new technologies like virtual reality and motion tracking."],"url":"http://arxiv.org/abs/2405.10059v1","category":"cs.HC"}
{"created":"2024-05-16 12:46:12","title":"Shedding Light on Hadronization by Quarkonium Energy Correlator","abstract":"We propose to measure the energy correlator in quarkonium production, which tracks the energy deposited in the calorimeter $\\chi$-angular distance away from the identified quarkonium. The observable eliminates the need for jets while sustaining the perturbative predictive power. Analyzing the power correction to the energy correlator, we demonstrate the novel observable supplies a unique gateway to probing the hadronization, especially when $\\cos\\chi\\gtrsim 0$ in the quarkonium rest frame where the perturbative emissions are depleted due to the dead-cone effects. We expect the quarkonium energy correlator to add a new dimension to quarkonium studies.","sentences":["We propose to measure the energy correlator in quarkonium production, which tracks the energy deposited in the calorimeter $\\chi$-angular distance away from the identified quarkonium.","The observable eliminates the need for jets while sustaining the perturbative predictive power.","Analyzing the power correction to the energy correlator, we demonstrate the novel observable supplies a unique gateway to probing the hadronization, especially when $\\cos\\chi\\gtrsim 0$ in the quarkonium rest frame where the perturbative emissions are depleted due to the dead-cone effects.","We expect the quarkonium energy correlator to add a new dimension to quarkonium studies."],"url":"http://arxiv.org/abs/2405.10056v1","category":"hep-ph"}
{"created":"2024-05-16 12:33:30","title":"Stellar Chromospheric Activity Database of Solar-like Stars Based on the LAMOST Low-Resolution Spectroscopic Survey: II. the bolometric and photospheric calibration","abstract":"The dependence of stellar magnetic activity on stellar parameters would be inspired by the chromospheric activity studies based on the large-scale spectroscopic surveys. The Ca II H and K lines are employed to construct indicators for assessing and studying the chromospheric activity of solar-like stars. We investigate the widely used bolometric and photospheric calibrated chromospheric activity index $R'_{\\rm HK}$, derived from the method in the classic literature ($R'_{\\rm HK,classic}$) and the method based on the PHOENIX model ($R'_{\\rm HK,PHOENIX}$). Since the detailed stellar atmospheric parameters, effective temperature ($T_{\\rm eff}$), surface gravity ($\\log\\,g$), and metallic","sentences":["The dependence of stellar magnetic activity on stellar parameters would be inspired by the chromospheric activity studies based on the large-scale spectroscopic surveys.","The Ca II H and K lines are employed to construct indicators for assessing and studying the chromospheric activity of solar-like stars.","We investigate the widely used bolometric and photospheric calibrated chromospheric activity index $R'_{\\rm HK}$, derived from the method in the classic literature ($R'_{\\rm HK,classic}$) and the method based on the PHOENIX model ($R'_{\\rm HK,PHOENIX}$).","Since the detailed stellar atmospheric parameters, effective temperature ($T_{\\rm eff}$), surface gravity ($\\log\\,g$), and metallic"],"url":"http://arxiv.org/abs/2405.10047v1","category":"astro-ph.SR"}
{"created":"2024-05-16 12:18:45","title":"High-Scale SUSY from Sgoldstino Inflation","abstract":"We review a number of unimodular no-scale supergravity models with F-term SUSY breaking which support technically natural de Sitter vacua. A variant of these models develops a stage of inflection-point inflation which can be realized for subplanckian field values consistently with the observational data. For central value of the spectral index ns, the necessary tuning is of the order of 10^-6, the tensor-to-scalar ratio is tiny whereas the running of ns is around -3x10^-3. Our proposal is compatible with high-scale SUSY and the results of LHC on the Higgs boson mass.","sentences":["We review a number of unimodular no-scale supergravity models with F-term SUSY breaking which support technically natural de Sitter vacua.","A variant of these models develops a stage of inflection-point inflation which can be realized for subplanckian field values consistently with the observational data.","For central value of the spectral index ns, the necessary tuning is of the order of 10^-6, the tensor-to-scalar ratio is tiny whereas the running of ns is around -3x10^-3.","Our proposal is compatible with high-scale SUSY and the results of LHC on the Higgs boson mass."],"url":"http://arxiv.org/abs/2405.10039v1","category":"hep-ph"}
{"created":"2024-05-16 12:10:43","title":"The case for specifying the \"ideal\" target trial","abstract":"The target trial is an increasingly popular conceptual device for guiding the design and analysis of observational studies that seek to perform causal inference. As tends to occur with concepts like this, there is variability in how certain aspects of the approach are understood, which may lead to potentially consequential differences in how the approach is taught, implemented, and interpreted in practice. In this commentary, we provide a perspective on two of these aspects: how the target trial should be specified, and relatedly, how the target trial fits within a formal causal inference framework.","sentences":["The target trial is an increasingly popular conceptual device for guiding the design and analysis of observational studies that seek to perform causal inference.","As tends to occur with concepts like this, there is variability in how certain aspects of the approach are understood, which may lead to potentially consequential differences in how the approach is taught, implemented, and interpreted in practice.","In this commentary, we provide a perspective on two of these aspects: how the target trial should be specified, and relatedly, how the target trial fits within a formal causal inference framework."],"url":"http://arxiv.org/abs/2405.10026v1","category":"stat.ME"}
{"created":"2024-05-16 11:45:51","title":"Probing the role of self-gravity in clouds impacted by AGN-driven winds","abstract":"The impact of winds and jet-inflated bubbles driven by active galactic nuclei (AGN) are believed to significantly affect the host galaxy's interstellar medium (ISM) and regulate star formation. To explore this scenario, we perform a suite of hydrodynamic simulations to model the interaction between turbulent star-forming clouds and highly pressurised AGN-driven outflows, focusing on the effects of self-gravity. Our results demonstrate that the cloudlets fragmented by the wind can become gravitationally bound, significantly increasing their survival time. While external pressurisation leads to a global collapse of the clouds in cases of weaker winds ($10^{42}-10^{43}~{\\rm erg~s^{-1}}$), higher-power winds ($10^{44}-10^{45}~{\\rm erg~s^{-1}}$) disperse the gas and cause localised collapse of the cloudlets. We also demonstrate that a kinetic energy-dominated wind is more efficient in accelerating and dispersing the gas than a thermal wind with the same power. The interaction can give rise to multi-phase outflows with velocities ranging from a few 100 to several 1000~${\\rm km\\,s^{-1}}$. The mass outflow rates are tightly correlated with the wind power, which we explain by an ablation-based mass-loss model. Moreover, the velocity dispersion and the virial parameter of the cloud material can increase by up to one order of magnitude through the effect of the wind. Even though the wind can suppress or quench star formation for about 1 Myr during the initial interaction, a substantial number of gravitationally bound dense cloudlets manage to shield themselves from the wind's influence and subsequently undergo rapid gravitational collapse, leading to an enhanced star formation rate (SFR).","sentences":["The impact of winds and jet-inflated bubbles driven by active galactic nuclei (AGN) are believed to significantly affect the host galaxy's interstellar medium (ISM) and regulate star formation.","To explore this scenario, we perform a suite of hydrodynamic simulations to model the interaction between turbulent star-forming clouds and highly pressurised AGN-driven outflows, focusing on the effects of self-gravity.","Our results demonstrate that the cloudlets fragmented by the wind can become gravitationally bound, significantly increasing their survival time.","While external pressurisation leads to a global collapse of the clouds in cases of weaker winds ($10^{42}-10^{43}~{\\rm erg~s^{-1}}$), higher-power winds ($10^{44}-10^{45}~{\\rm erg~s^{-1}}$) disperse the gas and cause localised collapse of the cloudlets.","We also demonstrate that a kinetic energy-dominated wind is more efficient in accelerating and dispersing the gas than a thermal wind with the same power.","The interaction can give rise to multi-phase outflows with velocities ranging from a few 100 to several 1000~${\\rm km\\,s^{-1}}$. The mass outflow rates are tightly correlated with the wind power, which we explain by an ablation-based mass-loss model.","Moreover, the velocity dispersion and the virial parameter of the cloud material can increase by up to one order of magnitude through the effect of the wind.","Even though the wind can suppress or quench star formation for about 1 Myr during the initial interaction, a substantial number of gravitationally bound dense cloudlets manage to shield themselves from the wind's influence and subsequently undergo rapid gravitational collapse, leading to an enhanced star formation rate (SFR)."],"url":"http://arxiv.org/abs/2405.10005v1","category":"astro-ph.GA"}
{"created":"2024-05-16 10:32:09","title":"A study of the fine-structure constant dependence of radiative capture in Halo-EFT","abstract":"We study the fine-structure constant dependence of the rates of some selected radiative capture reactions within the framework of so-called Halo Effective Field Theory in order to assess the adequacy of some assumptions made on the Coulomb penetrability. We find that this dependence deviates from that implied by a parameterization of the cross sections of this effect via a simple penetration factor. Some features of this fine-structure dependence are discussed, in particular its potential impact on the abundances of the light elements in primordial nucleosynthesis.","sentences":["We study the fine-structure constant dependence of the rates of some selected radiative capture reactions within the framework of so-called Halo Effective Field Theory in order to assess the adequacy of some assumptions made on the Coulomb penetrability.","We find that this dependence deviates from that implied by a parameterization of the cross sections of this effect via a simple penetration factor.","Some features of this fine-structure dependence are discussed, in particular its potential impact on the abundances of the light elements in primordial nucleosynthesis."],"url":"http://arxiv.org/abs/2405.09971v1","category":"nucl-th"}
{"created":"2024-05-16 09:29:41","title":"Twenty years of $\u0398^+$","abstract":"Twenty years ago, in 2003, two experimental groups, LEPS and DIANA, announced the discovery of a light, narrow, exotic baryon with mass within the range of 1540 MeV, which was later dubbed as $\\Theta^+$. In this talk we recall the history of this discovery and its theoretical foundations. We also discuss possible future experiments that could determine the existence of $\\Theta^+$.","sentences":["Twenty years ago, in 2003, two experimental groups, LEPS and DIANA, announced the discovery of a light, narrow, exotic baryon with mass within the range of 1540 MeV, which was later dubbed as $\\Theta^+$. In this talk we recall the history of this discovery and its theoretical foundations.","We also discuss possible future experiments that could determine the existence of $\\Theta^+$."],"url":"http://arxiv.org/abs/2405.09926v1","category":"hep-ph"}
{"created":"2024-05-16 08:42:55","title":"NH3 gas sensing over 2D Phosphorene sheet: A First-Principles Study","abstract":"First-principles based calculations were executed to investigate the sensing properties of ammonia gas molecules on two-dimensional pristine black phosphorene towards its application as a gas sensor and related applications. We discuss in detail, the interaction of ammonia gas molecules on the phosphorene single sheet through the structural change analysis, electronic band gap, Bader charge transfer, and density-of-states calculations. Our calculations indicate that the phosphorene could be used as a detector of ammonia, where good sensitivity and very short recovery time at room temperature have confirmed the potential use of phosphorene in the detection of ammonia.","sentences":["First-principles based calculations were executed to investigate the sensing properties of ammonia gas molecules on two-dimensional pristine black phosphorene towards its application as a gas sensor and related applications.","We discuss in detail, the interaction of ammonia gas molecules on the phosphorene single sheet through the structural change analysis, electronic band gap, Bader charge transfer, and density-of-states calculations.","Our calculations indicate that the phosphorene could be used as a detector of ammonia, where good sensitivity and very short recovery time at room temperature have confirmed the potential use of phosphorene in the detection of ammonia."],"url":"http://arxiv.org/abs/2405.09898v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 08:07:21","title":"A 20 kiloparsec bipolar Lyman $\u03b1$ outflow from a radio galaxy at z=2.95","abstract":"The study of ionized gas kinematics in high-z active galaxies plays a key part in our understanding of galactic evolution, in an age where nuclear activity was widespread and star formation close to its peak. We present a study of TXS 0952-217, a radio galaxy at z=2.95, using VLT/MUSE integral field optical spectroscopy as part of a project aimed studying of the properties of ionized gas in high redshift radio galaxies (HzRGs). The Lyman $\\alpha$ line profile of this object presents various emission and absorption components. By utilizing Voronoi binning, we obtained a comprehensive map of the kinematic properties of these components. These observations revealed the presence of a redshifted, high velocity (v $\\sim 500$ km s$^-1$) bipolar structure of Lyman $\\alpha$ emission, most likely corresponding to an outflow of ionized gas. The outflow extends beyond the compact radio source on both sides, with a total size of $\\sim$ 21 kpc. Its kinetic power ($10^{42.1}$ erg s$^{-1}$) is about five orders of magnitude smaller than its radio power. Additional ionized lines, including HeII$\\lambda$1640, CIV$\\lambda$1550 and CIII]$\\lambda$1908 were detected and their line flux ratios determined. The presence of HeII allowed for a precise redshift measurement (z=2.945$\\pm$0.002). Along with the recent discovery of a similar structure in TN J1049-1258, another HzRG, it displays the feasibility of using Lyman $\\alpha$ as a tracer of outflowing gas in high redshift sources, and particularly so when supported by non-resonant ionized lines such as HeII, which allow for accurate redshift and velocity measurements.","sentences":["The study of ionized gas kinematics in high-z active galaxies plays a key part in our understanding of galactic evolution, in an age where nuclear activity was widespread and star formation close to its peak.","We present a study of TXS 0952-217, a radio galaxy at z=2.95, using VLT/MUSE integral field optical spectroscopy as part of a project aimed studying of the properties of ionized gas in high redshift radio galaxies (HzRGs).","The Lyman $\\alpha$ line profile of this object presents various emission and absorption components.","By utilizing Voronoi binning, we obtained a comprehensive map of the kinematic properties of these components.","These observations revealed the presence of a redshifted, high velocity (v $\\sim 500$ km s$^-1$) bipolar structure of Lyman $\\alpha$ emission, most likely corresponding to an outflow of ionized gas.","The outflow extends beyond the compact radio source on both sides, with a total size of $\\sim$ 21 kpc.","Its kinetic power ($10^{42.1}$ erg s$^{-1}$) is about five orders of magnitude smaller than its radio power.","Additional ionized lines, including HeII$\\lambda$1640, CIV$\\lambda$1550 and CIII]$\\lambda$1908 were detected and their line flux ratios determined.","The presence of HeII allowed for a precise redshift measurement (z=2.945$\\pm$0.002).","Along with the recent discovery of a similar structure in TN J1049-1258, another HzRG, it displays the feasibility of using Lyman $\\alpha$ as a tracer of outflowing gas in high redshift sources, and particularly so when supported by non-resonant ionized lines such as HeII, which allow for accurate redshift and velocity measurements."],"url":"http://arxiv.org/abs/2405.09884v1","category":"astro-ph.GA"}
{"created":"2024-05-16 07:48:47","title":"NNLO+PS predictions for Higgs production via bottom annihilation","abstract":"We report on the implementation of a new NNLO+PS event generator for the Higgs production via bottom annihilation using the MiNNLOPS method in the POWHEG framework. The calculation has been carried out in the five flavour scheme (5FS), where the bottom mass is neglected. We compare our results against fixed-order predictions at NNLO as well as resummed predictions at next-to-next-to-leading-logarithmic (NNLL) accuracy. We also present a preliminary study in the four flavour scheme (4FS) setup, achieving a new level of precision in the massive scheme.","sentences":["We report on the implementation of a new NNLO+PS event generator for the Higgs production via bottom annihilation using the MiNNLOPS method in the POWHEG framework.","The calculation has been carried out in the five flavour scheme (5FS), where the bottom mass is neglected.","We compare our results against fixed-order predictions at NNLO as well as resummed predictions at next-to-next-to-leading-logarithmic (NNLL) accuracy.","We also present a preliminary study in the four flavour scheme (4FS) setup, achieving a new level of precision in the massive scheme."],"url":"http://arxiv.org/abs/2405.09870v1","category":"hep-ph"}
{"created":"2024-05-16 07:42:54","title":"Assessing course difficulty and the effect of weather in amateur cross country running races","abstract":"Cross country running races are different to track and road races in that the courses are not typically accurately measured and the condition of the course can have a strong effect on the finish times of the participants. In this paper we investigate these effects by modelling the finish times of all participants in 28 cross country running races over 5 seasons in the North East of England. We model the natural logarithm of the finish times using linear mixed effects models for both the senior men's and senior women's races. We investigate the effects of weather and underfoot conditions using windspeed and rainfall as covariates, fit distance as a covariate, and investigate the effect of time via the season of the race, in particular investigating any evidence of a pre- to post-Covid effect. We use random athlete effects to model the participant to participant variability and identify the most difficult courses using random course effects. The statistical inference is Bayesian. We assess model adequacy by comparing samples from the posterior predictive distribution of finish times to the observed distribution of finish times in each race. We find strong differences between the difficulty of the courses, effects of rainfall in the month of the race and the previous month to increase finish times and an effect of increasing distance increasing finish times. We find no evidence that windspeed affects finish times.","sentences":["Cross country running races are different to track and road races in that the courses are not typically accurately measured and the condition of the course can have a strong effect on the finish times of the participants.","In this paper we investigate these effects by modelling the finish times of all participants in 28 cross country running races over 5 seasons in the North East of England.","We model the natural logarithm of the finish times using linear mixed effects models for both the senior men's and senior women's races.","We investigate the effects of weather and underfoot conditions using windspeed and rainfall as covariates, fit distance as a covariate, and investigate the effect of time via the season of the race, in particular investigating any evidence of a pre- to post-Covid effect.","We use random athlete effects to model the participant to participant variability and identify the most difficult courses using random course effects.","The statistical inference is Bayesian.","We assess model adequacy by comparing samples from the posterior predictive distribution of finish times to the observed distribution of finish times in each race.","We find strong differences between the difficulty of the courses, effects of rainfall in the month of the race and the previous month to increase finish times and an effect of increasing distance increasing finish times.","We find no evidence that windspeed affects finish times."],"url":"http://arxiv.org/abs/2405.09865v1","category":"stat.AP"}
{"created":"2024-05-16 06:49:44","title":"On Infinitesimally $\u03c4$-Isospectrality of Locally Symmetric Spaces","abstract":"Let $(\\tau, V_{\\tau})$ be a finite dimensional representation of $K$, a maximal compact subgroup of a connected non-compact semisimple Lie group $G$ and let $\\Gamma_i (i=1,2)$ be uniform torsion free lattices in $G$. We prove that if all but finitely many irreducible unitary $\\tau$-spherical representations of $G$ occur with equal multiplicities in $L^2(\\Gamma_1 \\backslash G)$ and $L^2(\\Gamma_2 \\backslash G)$, then $L^2(\\Gamma_1 \\backslash G) \\cong L^2(\\Gamma_2 \\backslash G)$ as representations of $G$.","sentences":["Let $(\\tau, V_{\\tau})$ be a finite dimensional representation of $K$, a maximal compact subgroup of a connected non-compact semisimple Lie group $G$ and let $\\Gamma_i (i=1,2)$ be uniform torsion free lattices in $G$. We prove that if all but finitely many irreducible unitary $\\tau$-spherical representations of $G$ occur with equal multiplicities in $L^2(\\Gamma_1 \\backslash G)$ and $L^2(\\Gamma_2 \\backslash G)$, then $L^2(\\Gamma_1 \\backslash G) \\cong L^2(\\Gamma_2 \\backslash G)$ as representations of $G$."],"url":"http://arxiv.org/abs/2405.09847v1","category":"math.RT"}
{"created":"2024-05-16 06:45:28","title":"Electrically switchable $2^N$-channel wave-front control with N cascaded polarization-dependent metasurfaces","abstract":"To increase the operating channels of polarization-dependent metasurfaces, we proposed a structure of $N$ cascaded dual-channel metasurfaces to achieve $2^N$ electrically switchable functional channels without intrinsic noise or cross-talk. As proof of principles, we have implemented a 3-layer setup to achieve 8 switchable channels. In success, we have demonstrated two typical applications of vortex beam generation with switchable topological charge of l=-3 ~ +4 or l=-1~ -8, and beam steering with the deflecting direction switchable in an 8*1 line or a 4*2 grid. We believe that our proposal would provide a practical way to extend the functionality of polarization-multiplexed metasurfaces, which are potential for the applications of LiDAR, glasses-free 3D display, OAM (de)multiplexing, and varifocal meta-lens.","sentences":["To increase the operating channels of polarization-dependent metasurfaces, we proposed a structure of $N$ cascaded dual-channel metasurfaces to achieve $2^N$ electrically switchable functional channels without intrinsic noise or cross-talk.","As proof of principles, we have implemented a 3-layer setup to achieve 8 switchable channels.","In success, we have demonstrated two typical applications of vortex beam generation with switchable topological charge of l=-3 ~ +4 or l=-1~ -8, and beam steering with the deflecting direction switchable in an 8*1 line or a 4*2 grid.","We believe that our proposal would provide a practical way to extend the functionality of polarization-multiplexed metasurfaces, which are potential for the applications of LiDAR, glasses-free 3D display, OAM (de)multiplexing, and varifocal meta-lens."],"url":"http://arxiv.org/abs/2405.09844v1","category":"physics.optics"}
{"created":"2024-05-16 04:25:47","title":"Quantum $\\mathfrak{hs}$-Yang-Mills from the IKKT matrix model","abstract":"We study the one-loop effective action of the higher-spin gauge theory induced by the IKKT matrix model on a $\\mathcal{M}^{1,3}\\times \\mathcal{K}$ background, where $\\mathcal{M}^{1,3}$ is an FLRW cosmological spacetime brane and $\\mathcal{K}$ are compact fuzzy extra dimensions. In particular, we show that all non-abelian ($\\mathfrak{hs}$-valued) gauge fields in this model acquire mass via quantum effects, thus avoiding no-go theorems. This leads to a massive non-abelian quantum $\\mathfrak{hs}$-Yang-Mills theory, whose detailed structure depends on $\\mathcal{K}$. The stabilization of $\\mathcal{K}$ at one loop is understood as a result of the coupling between $\\mathcal{K}$ and the $U(1)$-flux bundle on space-time. This flux stabilization induces the KK scale into the $\\mathcal{N} = 4$ SYM sector of the model, which break superconformal symmetry.","sentences":["We study the one-loop effective action of the higher-spin gauge theory induced by the IKKT matrix model on a $\\mathcal{M}^{1,3}\\times \\mathcal{K}$ background, where $\\mathcal{M}^{1,3}$ is an FLRW cosmological spacetime brane and $\\mathcal{K}$ are compact fuzzy extra dimensions.","In particular, we show that all non-abelian ($\\mathfrak{hs}$-valued) gauge fields in this model acquire mass via quantum effects, thus avoiding no-go theorems.","This leads to a massive non-abelian quantum $\\mathfrak{hs}$-Yang-Mills theory, whose detailed structure depends on $\\mathcal{K}$. The stabilization of $\\mathcal{K}$ at one loop is understood as a result of the coupling between $\\mathcal{K}$ and the $U(1)$-flux bundle on space-time.","This flux stabilization induces the KK scale into the $\\mathcal{N} = 4$ SYM sector of the model, which break superconformal symmetry."],"url":"http://arxiv.org/abs/2405.09804v1","category":"hep-th"}
{"created":"2024-05-16 04:10:38","title":"Direct ab initio calculation of the $^{4}$He nuclear electric dipole polarizability","abstract":"The calculation of nuclear electromagnetic sum rules by directly diagonalizing the nuclear Hamiltonian in a large basis is numerically challenging and has not been performed for $A>2$ nuclei. With the significant progress of high performance computing, we show that calculating sum rules using numerous discretized continuum states obtained by directly diagonalizing the ab initio no-core shell model Hamiltonian is achievable numerically. Specifically, we calculate the $^{4}$He electric dipole ($E1$) polarizability, that is an inverse energy weighted sum rule, employing the Daejeon16 $NN$ interaction. We demonstrate that the calculations are numerically tractable as the dimension of the basis increases and are convergent. Our results for the $^{4}$He electric dipole polarizability are consistent with the most recent experimental data and are compared with those of other theoretical studies employing different techniques and various interactions.","sentences":["The calculation of nuclear electromagnetic sum rules by directly diagonalizing the nuclear Hamiltonian in a large basis is numerically challenging and has not been performed for $A>2$ nuclei.","With the significant progress of high performance computing, we show that calculating sum rules using numerous discretized continuum states obtained by directly diagonalizing the ab initio no-core shell model Hamiltonian is achievable numerically.","Specifically, we calculate the $^{4}$He electric dipole ($E1$) polarizability, that is an inverse energy weighted sum rule, employing the Daejeon16 $NN$ interaction.","We demonstrate that the calculations are numerically tractable as the dimension of the basis increases and are convergent.","Our results for the $^{4}$He electric dipole polarizability are consistent with the most recent experimental data and are compared with those of other theoretical studies employing different techniques and various interactions."],"url":"http://arxiv.org/abs/2405.09799v1","category":"nucl-th"}
{"created":"2024-05-16 03:30:58","title":"Supermassive black hole formation from Affleck-Dine mechanism with suppressed clustering on large scales","abstract":"We study a primordial black hole (PBH) formation model based on the framework of the inhomogeneous Affleck-Dine (AD) mechanism, which can explain the seeds of supermassive black holes (SMBHs). This model, however, predicts strong clustering of SMBHs that is inconsistent with the observation of angular correlation of quasars. In this paper, we propose a modified model that can significantly reduce the PBH clustering on large scales by considering a time-dependent Hubble-induced mass during inflation. The quasar angular correlation is suppressed by the large Hubble-induced mass in the early stage of inflation while the small Hubble-induced mass in the late stage leads to the AD field fluctuations large enough for PBH formation as in the original model. As a result, the modified scenario can successfully explain the seeds of SMBHs.","sentences":["We study a primordial black hole (PBH) formation model based on the framework of the inhomogeneous Affleck-Dine (AD) mechanism, which can explain the seeds of supermassive black holes (SMBHs).","This model, however, predicts strong clustering of SMBHs that is inconsistent with the observation of angular correlation of quasars.","In this paper, we propose a modified model that can significantly reduce the PBH clustering on large scales by considering a time-dependent Hubble-induced mass during inflation.","The quasar angular correlation is suppressed by the large Hubble-induced mass in the early stage of inflation while the small Hubble-induced mass in the late stage leads to the AD field fluctuations large enough for PBH formation as in the original model.","As a result, the modified scenario can successfully explain the seeds of SMBHs."],"url":"http://arxiv.org/abs/2405.09790v1","category":"astro-ph.CO"}
{"created":"2024-05-16 02:27:25","title":"The effect of temperature oscillations on energy storage rectification in harmonic systems","abstract":"Rectification, the preferential transport of a current in one direction through a system, has garnered significant attention in molecules because of its importance for controlling thermal and electronic currents at the nanoscale. Here, we report the presence of energy storage rectification effects in a molecular chain. This phenomenon is generated by subjecting a harmonic molecular chain to an oscillating temperature gradient and showing that the energy absorption rate of the system depends on the direction of the gradient. We examine how the energy storage rectification ratios in the chain are affected by the oscillating gradient, asymmetry in the chain, and the system parameters. We find that energy storage rectification can be observed in harmonic lattice structures with time-dependent temperatures and that, correspondingly, anharmonicity is not required to generate this rectification mechanism in such systems.","sentences":["Rectification, the preferential transport of a current in one direction through a system, has garnered significant attention in molecules because of its importance for controlling thermal and electronic currents at the nanoscale.","Here, we report the presence of energy storage rectification effects in a molecular chain.","This phenomenon is generated by subjecting a harmonic molecular chain to an oscillating temperature gradient and showing that the energy absorption rate of the system depends on the direction of the gradient.","We examine how the energy storage rectification ratios in the chain are affected by the oscillating gradient, asymmetry in the chain, and the system parameters.","We find that energy storage rectification can be observed in harmonic lattice structures with time-dependent temperatures and that, correspondingly, anharmonicity is not required to generate this rectification mechanism in such systems."],"url":"http://arxiv.org/abs/2405.09774v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 02:23:02","title":"Advanced Tokamak: The Strongly Reversed Central Magnetic Shear Profile","abstract":"This review article will offer a qualitative overview of the strongly reversed shear profile for steady-state operation in tokamaks. For a steady-state reactor to be commercially viable, it is necessary to have a large bootstrap fraction. Currently, there appears great potential in an Advanced Tokamak (AT) regime, namely the hollow current profile (strongly reversed shear). This mode is characterized by high poloidal beta, broad current profiles, strong internal and edge pressure gradients, and relatively good magnetohydrodynamic (MHD) stability against Neoclassical Tearing Modes (NTMs) and ballooning modes. The n=1 and n=2 kink modes, resistive wall modes, and double tearing modes are of concern in the reversed shear profile, and avoidance and/or suppression of these modes is necessary. Although there is a relatively low net plasma current in the reversed shear, the regime appears to have excellent energy confinement properties due to the naturally occurring Internal Transport Barriers (ITBs) caused by the substantial bootstrap currents, and Edge Transport Barriers (ETBs), which can form from ELM-free H-Mode (QH-Mode), to form the Quiescent Double Barrier (QDBs). The reversed shear can be generated by freezing the current profile, through MHD effects or substantial heating and/or current drive during the current ramp up phase, and is sustained by off-axis non-inductive current drive sources, such as the Neutral Beam Current Drive (NBCD), Lower Hybrid Current Drive (LHCD), and Helicon Current Drive (HCD). Experimental results by DIII-D, JT-60U, ASDEX Upgrade, JET, PBX-M, COMPASS-D, and K-STAR, simulation models and codes, such as Lower Hybrid Simulation and STELION, and theoretical reactors, such as ARIES-RS, ARIES-AT and SSTR are referenced.","sentences":["This review article will offer a qualitative overview of the strongly reversed shear profile for steady-state operation in tokamaks.","For a steady-state reactor to be commercially viable, it is necessary to have a large bootstrap fraction.","Currently, there appears great potential in an Advanced Tokamak (AT) regime, namely the hollow current profile (strongly reversed shear).","This mode is characterized by high poloidal beta, broad current profiles, strong internal and edge pressure gradients, and relatively good magnetohydrodynamic (MHD) stability against Neoclassical Tearing Modes (NTMs) and ballooning modes.","The n=1 and n=2 kink modes, resistive wall modes, and double tearing modes are of concern in the reversed shear profile, and avoidance and/or suppression of these modes is necessary.","Although there is a relatively low net plasma current in the reversed shear, the regime appears to have excellent energy confinement properties due to the naturally occurring Internal Transport Barriers (ITBs) caused by the substantial bootstrap currents, and Edge Transport Barriers (ETBs), which can form from ELM-free H-Mode (QH-Mode), to form the Quiescent Double Barrier (QDBs).","The reversed shear can be generated by freezing the current profile, through MHD effects or substantial heating and/or current drive during the current ramp up phase, and is sustained by off-axis non-inductive current drive sources, such as the Neutral Beam Current Drive (NBCD), Lower Hybrid Current Drive (LHCD), and Helicon Current Drive (HCD).","Experimental results by DIII-D, JT-60U, ASDEX Upgrade, JET, PBX-M, COMPASS-D, and K-STAR, simulation models and codes, such as Lower Hybrid Simulation and STELION, and theoretical reactors, such as ARIES-RS, ARIES-AT and SSTR are referenced."],"url":"http://arxiv.org/abs/2405.09773v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 02:22:27","title":"Energy dependence of the low-frequency quasi-periodic oscillations in Swift J1727.8-1613","abstract":"Based on observations from the Insight-Hard X-ray Modulation Telescope (Insight-HXMT), an analysis of Type-C quasi-periodic oscillations (QPOs) observed during the outburst of the new black hole candidate Swift J1727.8-1613 in 2023 was conducted. This analysis scrutinized the QPO's evolution throughout the outburst, particularly noting its rapid frequency escalation during two flare events. Utilizing the energy range covered by Insight-HXMT, a dependency of the QPO frequency on energy was observed. Below approximately 3 Hz, minimal variations in frequency with energy were noted, whereas clear variations with photon energy were observed when it exceeded approximately 3 Hz. Additionally, a sharp drop in the rate of change was observed when the frequency exceeded approximately 8 Hz. This behavior, similar to several previously reported sources, suggests the presence of a common underlying physical mechanism. Moreover, the QPO rms-frequency relationship can be explained by the Lense-Thirring precession model. The relationship between rms-energy and phase lag with frequency suggests the black hole system as a high-inclination source.","sentences":["Based on observations from the Insight-Hard X-ray Modulation Telescope (Insight-HXMT), an analysis of Type-C quasi-periodic oscillations (QPOs) observed during the outburst of the new black hole candidate Swift J1727.8-1613 in 2023 was conducted.","This analysis scrutinized the QPO's evolution throughout the outburst, particularly noting its rapid frequency escalation during two flare events.","Utilizing the energy range covered by Insight-HXMT, a dependency of the QPO frequency on energy was observed.","Below approximately 3 Hz, minimal variations in frequency with energy were noted, whereas clear variations with photon energy were observed when it exceeded approximately 3 Hz.","Additionally, a sharp drop in the rate of change was observed when the frequency exceeded approximately 8 Hz.","This behavior, similar to several previously reported sources, suggests the presence of a common underlying physical mechanism.","Moreover, the QPO rms-frequency relationship can be explained by the Lense-Thirring precession model.","The relationship between rms-energy and phase lag with frequency suggests the black hole system as a high-inclination source."],"url":"http://arxiv.org/abs/2405.09772v1","category":"astro-ph.HE"}
{"created":"2024-05-16 02:20:36","title":"The metallicity and carbon-to-oxygen ratio of the ultra-hot Jupiter WASP-76b from Gemini-S/IGRINS","abstract":"Measurements of the carbon-to-oxygen (C/O) ratios of exoplanet atmospheres can reveal details about their formation and evolution. Recently, high-resolution cross-correlation analysis has emerged as a method of precisely constraining the C/O ratios of hot Jupiter atmospheres. We present two transits of the ultra-hot Jupiter WASP-76b observed between 1.4-2.4 $\\mu$m with Gemini-S/IGRINS. We detected the presence of H$_{2}$O, CO, and OH at signal-to-noise rations of 6.93, 6.47, and 3.90, respectively. We performed two retrievals on this data set. A free retrieval for abundances of these three species retrieved a volatile metallicity of $\\left[\\frac{\\mathrm{C}+\\mathrm{O}} {\\mathrm{H}}\\right]=-0.70^{+1.27}_{-0.93}$, consistent with the stellar value, and a super-solar carbon-to-oxygen ratio of C/O$=0.80^{+0.07}_{-0.11}$. We also ran a chemically self-consistent grid retrieval, which agreed with the free retrieval within $1\\sigma$ but favored a slightly more sub-stellar metallicity and solar C/O ratio ($\\left[\\frac{\\mathrm{C}+\\mathrm{O}} {\\mathrm{H}}\\right]=-0.74^{+0.23}_{-0.17}$ and C/O$=0.59^{+0.13}_{-0.14}$). A variety of formation pathways may explain the composition of WASP-76b. Additionally, we found systemic ($V_{sys}$) and Keplerian ($K_{p}$) velocity offsets which were broadly consistent with expectations from 3D general circulation models of WASP-76b, with the exception of a redshifted $V_{sys}$ for H$_{2}$O. Future observations to measure the phase-dependent velocity offsets and limb differences at high resolution on WASP-76b will be necessary to understand the H$_{2}$O velocity shift. Finally, we find that the population of exoplanets with precisely constrained C/O ratios generally trends toward super-solar C/O ratios. More results from high-resolution observations or JWST will serve to further elucidate any population-level trends.","sentences":["Measurements of the carbon-to-oxygen (C/O) ratios of exoplanet atmospheres can reveal details about their formation and evolution.","Recently, high-resolution cross-correlation analysis has emerged as a method of precisely constraining the C/O ratios of hot Jupiter atmospheres.","We present two transits of the ultra-hot Jupiter WASP-76b observed between 1.4-2.4 $\\mu$m with Gemini-S/IGRINS.","We detected the presence of H$_{2}$O, CO, and OH at signal-to-noise rations of 6.93, 6.47, and 3.90, respectively.","We performed two retrievals on this data set.","A free retrieval for abundances of these three species retrieved a volatile metallicity of $\\left[\\frac{\\mathrm{C}+\\mathrm{O}} {\\mathrm{H}}\\right]=-0.70^{+1.27}_{-0.93}$, consistent with the stellar value, and a super-solar carbon-to-oxygen ratio of C/O$=0.80^{+0.07}_{-0.11}$.","We also ran a chemically self-consistent grid retrieval, which agreed with the free retrieval within $1\\sigma$ but favored a slightly more sub-stellar metallicity and solar C/O ratio ($\\left[\\frac{\\mathrm{C}+\\mathrm{O}} {\\mathrm{H}}\\right]=-0.74^{+0.23}_{-0.17}$ and C/O$=0.59^{+0.13}_{-0.14}$).","A variety of formation pathways may explain the composition of WASP-76b.","Additionally, we found systemic ($V_{sys}$) and Keplerian ($K_{p}$) velocity offsets which were broadly consistent with expectations from 3D general circulation models of WASP-76b, with the exception of a redshifted $V_{sys}$ for H$_{2}$O. Future observations to measure the phase-dependent velocity offsets and limb differences at high resolution on WASP-76b will be necessary to understand the H$_{2}$O velocity shift.","Finally, we find that the population of exoplanets with precisely constrained C/O ratios generally trends toward super-solar C/O ratios.","More results from high-resolution observations or JWST will serve to further elucidate any population-level trends."],"url":"http://arxiv.org/abs/2405.09769v1","category":"astro-ph.EP"}
{"created":"2024-05-16 01:36:31","title":"Fermionic Non-Invertible Symmetries in (1+1)d: Gapped and Gapless Phases, Transitions, and Symmetry TFTs","abstract":"We study fermionic non-invertible symmetries in (1+1)d, which are generalized global symmetries that mix fermion parity symmetry with other invertible and non-invertible internal symmetries. Such symmetries are described by fermionic fusion supercategories, which are fusion $\\pi$-supercategories with a choice of fermion parity. The aim of this paper is to flesh out the categorical Landau paradigm for fermionic symmetries. We use the formalism of Symmetry Topological Field Theory (SymTFT) to study possible gapped and gapless phases for such symmetries, along with possible deformations between these phases, which are organized into a Hasse phase diagram. The phases can be characterized in terms of sets of condensed, confined and deconfined generalized symmetry charges, reminiscent of notions familiar from superconductivity. Many of the gapless phases also serve as phase transitions between gapped phases. The associated fermionic conformal field theories (CFTs) can be obtained by performing generalized fermionic Kennedy-Tasaki (KT) transformations on bosonic CFTs describing simpler transitions. The fermionic non-invertible symmetries along with their charges and phases discussed here can be obtained from those of bosonic non-invertible symmetries via fermionization or Jordan-Wigner transformation, which is discussed in detail.","sentences":["We study fermionic non-invertible symmetries in (1+1)d, which are generalized global symmetries that mix fermion parity symmetry with other invertible and non-invertible internal symmetries.","Such symmetries are described by fermionic fusion supercategories, which are fusion $\\pi$-supercategories with a choice of fermion parity.","The aim of this paper is to flesh out the categorical Landau paradigm for fermionic symmetries.","We use the formalism of Symmetry Topological Field Theory (SymTFT) to study possible gapped and gapless phases for such symmetries, along with possible deformations between these phases, which are organized into a Hasse phase diagram.","The phases can be characterized in terms of sets of condensed, confined and deconfined generalized symmetry charges, reminiscent of notions familiar from superconductivity.","Many of the gapless phases also serve as phase transitions between gapped phases.","The associated fermionic conformal field theories (CFTs) can be obtained by performing generalized fermionic Kennedy-Tasaki (KT) transformations on bosonic CFTs describing simpler transitions.","The fermionic non-invertible symmetries along with their charges and phases discussed here can be obtained from those of bosonic non-invertible symmetries via fermionization or Jordan-Wigner transformation, which is discussed in detail."],"url":"http://arxiv.org/abs/2405.09754v1","category":"hep-th"}
{"created":"2024-05-16 01:24:56","title":"Gravitational Chern-Simons form and Chiral Gravitational Anomaly in Fluid Mechanics","abstract":"We show that the hydrodynamics of the perfect fluid admit a deformation that includes a chiral gravitational (or mixed) anomaly alongside the chiral anomaly. The deformation features a Wess- Zumino functional which involves the gravitational Chern-Simons term.","sentences":["We show that the hydrodynamics of the perfect fluid admit a deformation that includes a chiral gravitational (or mixed) anomaly alongside the chiral anomaly.","The deformation features a Wess- Zumino functional which involves the gravitational Chern-Simons term."],"url":"http://arxiv.org/abs/2405.09751v1","category":"hep-th"}
{"created":"2024-05-16 01:15:28","title":"Low-frequency, wideband study of an active repeater, FRB 20240114A, with the GMRT","abstract":"We report the detection of a total of 135 bursts from a recently discovered active, repeating fast radio burst, FRB 20240114A with the GMRT over a frequency range of 300$-$750 MHz. The bursts were detected with intrinsic widths ranging from 0.308 to 39.364 ms, a median scattering timescale of 2.059 ms at 400 MHz and 1.372 ms at 650 MHz. The fluences of the detected bursts range from 36.81 mJy ms to 7.47 Jy ms. Both the energy and waiting time distributions of the bursts can be fitted with broken power laws, indicating the presence of two distinct populations of bursts. The energy distributions were modeled via broken power law with $\\alpha_{1} = -0.62 \\pm 0.01$ and $\\alpha_{2} = -1.98 \\pm 0.1$, while the waiting time distribution was modeled via a broken power law with $\\alpha_{1} = -0.71 \\pm 0.01$ and $\\alpha_{2} = -2.09 \\pm 0.09$. Both the energy and waiting time distributions of FRB 20240114A are comparable to high-energy bursts from magnetars, and giant radio pulses from pulsars, indicating that such objects could be likely progenitors.","sentences":["We report the detection of a total of 135 bursts from a recently discovered active, repeating fast radio burst, FRB 20240114A with the GMRT over a frequency range of 300$-$750 MHz.","The bursts were detected with intrinsic widths ranging from 0.308 to 39.364 ms, a median scattering timescale of 2.059 ms at 400 MHz and 1.372 ms at 650 MHz.","The fluences of the detected bursts range from 36.81 mJy ms to 7.47 Jy ms.","Both the energy and waiting time distributions of the bursts can be fitted with broken power laws, indicating the presence of two distinct populations of bursts.","The energy distributions were modeled via broken power law with $\\alpha_{1} = -0.62 \\pm 0.01$ and $\\alpha_{2} = -1.98 \\pm 0.1$, while the waiting time distribution was modeled via a broken power law with $\\alpha_{1} = -0.71 \\pm 0.01$ and $\\alpha_{2} = -2.09 \\pm 0.09$. Both the energy and waiting time distributions of FRB 20240114A are comparable to high-energy bursts from magnetars, and giant radio pulses from pulsars, indicating that such objects could be likely progenitors."],"url":"http://arxiv.org/abs/2405.09749v1","category":"astro-ph.HE"}
{"created":"2024-05-16 01:03:21","title":"Pseudoentropy sum rule by analytical continuation of the superposition parameter","abstract":"In this paper, we establish a sum rule that connects the pseudoentropy and entanglement entropy of a superposition state. Through analytical continuation of the superposition parameter, we demonstrate that the transition matrix and density matrix of the superposition state can be treated in a unified manner. Within this framework, we naturally derive sum rules for the (reduced) transition matrix, pseudo R\\'enyi entropy, and pseudoentropy. Furthermore, we demonstrate the close relationship between the sum rule for pseudoentropy and the singularity structure of the entropy function for the superposition state after analytical continuation. We also explore potential applications of the sum rule, including its relevance to understanding the gravity dual of non-Hermitian transition matrices and establishing upper bounds for the absolute value of pseudoentropy.","sentences":["In this paper, we establish a sum rule that connects the pseudoentropy and entanglement entropy of a superposition state.","Through analytical continuation of the superposition parameter, we demonstrate that the transition matrix and density matrix of the superposition state can be treated in a unified manner.","Within this framework, we naturally derive sum rules for the (reduced) transition matrix, pseudo R\\'enyi entropy, and pseudoentropy.","Furthermore, we demonstrate the close relationship between the sum rule for pseudoentropy and the singularity structure of the entropy function for the superposition state after analytical continuation.","We also explore potential applications of the sum rule, including its relevance to understanding the gravity dual of non-Hermitian transition matrices and establishing upper bounds for the absolute value of pseudoentropy."],"url":"http://arxiv.org/abs/2405.09745v1","category":"hep-th"}
{"created":"2024-05-16 00:31:31","title":"Are all models wrong? Falsifying binary formation models in gravitational-wave astronomy","abstract":"As the catalogue of gravitational-wave transients grows, several entries appear \"exceptional\" within the population. Tipping the scales with a total mass of $\\approx 150 M_\\odot$, GW190521 likely contained black holes in the pair-instability mass gap. The event GW190814, meanwhile, is unusual for its extreme mass ratio and the mass of its secondary component. A growing model-building industry has emerged to provide explanations for such exceptional events, and Bayesian model selection is frequently used to determine the most informative model. However, Bayesian methods can only take us so far. They provide no answer to the question: does our model provide an adequate explanation for the data? If none of the models we are testing provide an adequate explanation, then it is not enough to simply rank our existing models - we need new ones. In this paper, we introduce a method to answer this question with a frequentist $p$-value. We apply the method to different models that have been suggested to explain GW190521: hierarchical mergers in active galactic nuclei and globular clusters. We show that some (but not all) of these models provide adequate explanations for exceptionally massive events like GW190521.","sentences":["As the catalogue of gravitational-wave transients grows, several entries appear \"exceptional\" within the population.","Tipping the scales with a total mass of $\\approx 150 M_\\odot$, GW190521 likely contained black holes in the pair-instability mass gap.","The event GW190814, meanwhile, is unusual for its extreme mass ratio and the mass of its secondary component.","A growing model-building industry has emerged to provide explanations for such exceptional events, and Bayesian model selection is frequently used to determine the most informative model.","However, Bayesian methods can only take us so far.","They provide no answer to the question: does our model provide an adequate explanation for the data?","If none of the models we are testing provide an adequate explanation, then it is not enough to simply rank our existing models - we need new ones.","In this paper, we introduce a method to answer this question with a frequentist $p$-value.","We apply the method to different models that have been suggested to explain GW190521: hierarchical mergers in active galactic nuclei and globular clusters.","We show that some (but not all) of these models provide adequate explanations for exceptionally massive events like GW190521."],"url":"http://arxiv.org/abs/2405.09739v1","category":"astro-ph.HE"}
{"created":"2024-05-16 00:28:45","title":"X-ray Cool Core Remnants Heated by Strong Radio AGN Feedback","abstract":"Strong AGN heating provides an alternative means for the disruption of cluster cool cores (CCs) to cluster mergers. In this work we present a systematic Chandra study of a sample of 108 nearby ($z<0.1$) galaxy clusters, to investigate the effect of AGN heating on CCs. About 40% of clusters with small offsets between the BCG and the X-ray centre ($\\le50$ kpc) have small CCs. For comparison, 14 of 17 clusters with large offsets have small CCs, which suggests that mergers or sloshing can be efficient in reducing the CC size. Relaxed, small CC clusters generally have weak radio AGNs ($P_{1.4\\rm GHz}<10^{23}$ W Hz$^{-1}$), and they show a lack of systems hosting a radio AGN with intermediate radio power ($2\\times10^{23}<P_{1.4\\rm GHz}<2\\times10^{24}$ W Hz$^{-1}$). We found that the strongest circumnuclear ($<1$ kpc) X-ray emission only exists in clusters with strong radio AGN. The duty cycle of relaxed, small CC clusters is less than half of that for large CC clusters. It suggests that the radio activity of BCGs is affected by the properties of the surrounding gas beyond the central $\\sim10$ kpc, and strong radio AGNs in small X-ray CCs fade more rapidly than those embedded in large X-ray CCs. A scenario is also presented for the transition of large CCs and coronae due to radio AGN feedback. We also present a detailed analysis of galaxy cluster 3C 129.1 as an example of a CC remnant possibly disrupted by radio AGN.","sentences":["Strong AGN heating provides an alternative means for the disruption of cluster cool cores (CCs) to cluster mergers.","In this work we present a systematic Chandra study of a sample of 108 nearby ($z<0.1$) galaxy clusters, to investigate the effect of AGN heating on CCs.","About 40% of clusters with small offsets between the BCG and the X-ray centre ($\\le50$ kpc) have small CCs.","For comparison, 14 of 17 clusters with large offsets have small CCs, which suggests that mergers or sloshing can be efficient in reducing the CC size.","Relaxed, small CC clusters generally have weak radio AGNs ($P_{1.4\\rm GHz}<10^{23}$ W Hz$^{-1}$), and they show a lack of systems hosting a radio AGN with intermediate radio power ($2\\times10^{23}<P_{1.4\\rm GHz}<2\\times10^{24}$ W Hz$^{-1}$).","We found that the strongest circumnuclear ($<1$ kpc) X-ray emission only exists in clusters with strong radio AGN.","The duty cycle of relaxed, small CC clusters is less than half of that for large CC clusters.","It suggests that the radio activity of BCGs is affected by the properties of the surrounding gas beyond the central $\\sim10$ kpc, and strong radio AGNs in small X-ray CCs fade more rapidly than those embedded in large X-ray CCs.","A scenario is also presented for the transition of large CCs and coronae due to radio AGN feedback.","We also present a detailed analysis of galaxy cluster 3C 129.1 as an example of a CC remnant possibly disrupted by radio AGN."],"url":"http://arxiv.org/abs/2405.09738v1","category":"astro-ph.GA"}
{"created":"2024-05-16 00:15:04","title":"Validation of the DESI 2024 Lyman Alpha Forest BAL Masking Strategy","abstract":"Broad absorption line quasars (BALs) exhibit blueshifted absorption relative to a number of their prominent broad emission features. These absorption features can contribute to quasar redshift errors and add absorption to the Lyman-alpha (LyA) forest that is unrelated to large-scale structure. We present a detailed analysis of the impact of BALs on the Baryon Acoustic Oscillation (BAO) results with the LyA forest from the first year of data from the Dark Energy Spectroscopic Instrument (DESI). The baseline strategy for the first year analysis is to mask all pixels associated with all BAL absorption features that fall within the wavelength region used to measure the forest. We explore a range of alternate masking strategies and demonstrate that these changes have minimal impact on the BAO measurements with both DESI data and synthetic data. This includes when we mask the BAL features associated with emission lines outside of the forest region to minimize their contribution to redshift errors. We identify differences in the properties of BALs in the synthetic datasets relative to the observational data, as well as use the synthetic observations to characterize the completeness of the BAL identification algorithm, and demonstrate that incompleteness and differences in the BALs between real and synthetic data also do not impact the BAO results for the LyA forest.","sentences":["Broad absorption line quasars (BALs) exhibit blueshifted absorption relative to a number of their prominent broad emission features.","These absorption features can contribute to quasar redshift errors and add absorption to the Lyman-alpha (LyA) forest that is unrelated to large-scale structure.","We present a detailed analysis of the impact of BALs on the Baryon Acoustic Oscillation (BAO) results with the LyA forest from the first year of data from the Dark Energy Spectroscopic Instrument (DESI).","The baseline strategy for the first year analysis is to mask all pixels associated with all BAL absorption features that fall within the wavelength region used to measure the forest.","We explore a range of alternate masking strategies and demonstrate that these changes have minimal impact on the BAO measurements with both DESI data and synthetic data.","This includes when we mask the BAL features associated with emission lines outside of the forest region to minimize their contribution to redshift errors.","We identify differences in the properties of BALs in the synthetic datasets relative to the observational data, as well as use the synthetic observations to characterize the completeness of the BAL identification algorithm, and demonstrate that incompleteness and differences in the BALs between real and synthetic data also do not impact the BAO results for the LyA forest."],"url":"http://arxiv.org/abs/2405.09737v1","category":"astro-ph.CO"}
{"created":"2024-05-15 23:50:47","title":"Impacts of Hot Electron Diffusion, Electron-Phonon Coupling, and Surface Atoms on Metal Surface Dynamics Revealed by Reflection Ultrafast Electron Diffraction","abstract":"Metals exhibit nonequilibrium electron and lattice subsystems at transient times following femtosecond laser excitation. In the past four decades, various optical spectroscopy and time-resolved diffraction methods have been used to study electron-phonon coupling and the effects of underlying dynamical processes. Here, we take advantage of the surface specificity of reflection ultrafast electron diffraction (UED) to examine the structural dynamics of photoexcited metal surfaces, which are apparently slower in recovery than predicted by thermal diffusion from the profile of absorbed energy. Fast diffusion of hot electrons is found to critically reduce surface excitation and affect the temporal dependence of the increased atomic motions on not only the ultrashort but sub-nanosecond times. Whereas the two-temperature model with the accepted physical constants of platinum can reproduce the observed surface lattice dynamics, gold is found to exhibit appreciably larger-than-expected dynamic vibrational amplitudes of surface atoms while keeping the commonly used electron-phonon coupling constant. Such surface behavioral difference at transient times can be understood in the context of the different strengths of binding to surface atoms for the two metals. In addition, with the quantitative agreements between diffraction and theoretical results, we provide convincing evidence that surface structural dynamics can be reliably obtained by reflection UED even in the presence of laser-induced transient electric fields.","sentences":["Metals exhibit nonequilibrium electron and lattice subsystems at transient times following femtosecond laser excitation.","In the past four decades, various optical spectroscopy and time-resolved diffraction methods have been used to study electron-phonon coupling and the effects of underlying dynamical processes.","Here, we take advantage of the surface specificity of reflection ultrafast electron diffraction (UED) to examine the structural dynamics of photoexcited metal surfaces, which are apparently slower in recovery than predicted by thermal diffusion from the profile of absorbed energy.","Fast diffusion of hot electrons is found to critically reduce surface excitation and affect the temporal dependence of the increased atomic motions on not only the ultrashort but sub-nanosecond times.","Whereas the two-temperature model with the accepted physical constants of platinum can reproduce the observed surface lattice dynamics, gold is found to exhibit appreciably larger-than-expected dynamic vibrational amplitudes of surface atoms while keeping the commonly used electron-phonon coupling constant.","Such surface behavioral difference at transient times can be understood in the context of the different strengths of binding to surface atoms for the two metals.","In addition, with the quantitative agreements between diffraction and theoretical results, we provide convincing evidence that surface structural dynamics can be reliably obtained by reflection UED even in the presence of laser-induced transient electric fields."],"url":"http://arxiv.org/abs/2405.09731v1","category":"physics.chem-ph"}
{"created":"2024-05-15 23:28:59","title":"Hidden zero modes and topology of multiband non-Hermitian systems","abstract":"In a finite non-Hermitian system, the number of zero modes does not necessarily reflect the topology of the system. This is known as the breakdown of the bulk-boundary correspondence and has lead to misconceptions about the topological protection of edge modes in such systems. Here we show why this breakdown does occur and that it typically results in hidden zero modes, extremely long-lived zero energy excitations, which are only revealed when considering the singular value instead of the eigenvalue spectrum. We point out, furthermore, that in a finite multiband non-Hermitian system with Hamiltonian $H$, one needs to consider also the reflected Hamiltonian $\\tilde H$, which is in general distinct from the adjoint $H^\\dagger$, to properly relate the number of protected zeroes to the winding number of $H$.","sentences":["In a finite non-Hermitian system, the number of zero modes does not necessarily reflect the topology of the system.","This is known as the breakdown of the bulk-boundary correspondence and has lead to misconceptions about the topological protection of edge modes in such systems.","Here we show why this breakdown does occur and that it typically results in hidden zero modes, extremely long-lived zero energy excitations, which are only revealed when considering the singular value instead of the eigenvalue spectrum.","We point out, furthermore, that in a finite multiband non-Hermitian system with Hamiltonian $H$, one needs to consider also the reflected Hamiltonian $\\tilde H$, which is in general distinct from the adjoint $H^\\dagger$, to properly relate the number of protected zeroes to the winding number of $H$."],"url":"http://arxiv.org/abs/2405.09728v1","category":"cond-mat.stat-mech"}
