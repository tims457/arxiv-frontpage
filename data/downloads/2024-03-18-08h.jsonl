{"created":"2024-03-15 17:59:53","title":"P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors","abstract":"Autonomous vehicles are gradually entering city roads today, with the help of high-definition maps (HDMaps). However, the reliance on HDMaps prevents autonomous vehicles from stepping into regions without this expensive digital infrastructure. This fact drives many researchers to study online HDMap generation algorithms, but the performance of these algorithms at far regions is still unsatisfying. We present P-MapNet, in which the letter P highlights the fact that we focus on incorporating map priors to improve model performance. Specifically, we exploit priors in both SDMap and HDMap. On one hand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as an additional conditioning branch. Despite the misalignment challenge, our attention-based architecture adaptively attends to relevant SDMap skeletons and significantly improves performance. On the other hand, we exploit a masked autoencoder to capture the prior distribution of HDMap, which can serve as a refinement module to mitigate occlusions and artifacts. We benchmark on the nuScenes and Argoverse2 datasets. Through comprehensive experiments, we show that: (1) our SDMap prior can improve online map generation performance, using both rasterized (by up to $+18.73$ $\\rm mIoU$) and vectorized (by up to $+8.50$ $\\rm mAP$) output representations. (2) our HDMap prior can improve map perceptual metrics by up to $6.34\\%$. (3) P-MapNet can be switched into different inference modes that covers different regions of the accuracy-efficiency trade-off landscape. (4) P-MapNet is a far-seeing solution that brings larger improvements on longer ranges. Codes and models are publicly available at https://jike5.github.io/P-MapNet.","sentences":["Autonomous vehicles are gradually entering city roads today, with the help of high-definition maps (HDMaps).","However, the reliance on HDMaps prevents autonomous vehicles from stepping into regions without this expensive digital infrastructure.","This fact drives many researchers to study online HDMap generation algorithms, but the performance of these algorithms at far regions is still unsatisfying.","We present P-MapNet, in which the letter P highlights the fact that we focus on incorporating map priors to improve model performance.","Specifically, we exploit priors in both SDMap and HDMap.","On one hand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as an additional conditioning branch.","Despite the misalignment challenge, our attention-based architecture adaptively attends to relevant SDMap skeletons and significantly improves performance.","On the other hand, we exploit a masked autoencoder to capture the prior distribution of HDMap, which can serve as a refinement module to mitigate occlusions and artifacts.","We benchmark on the nuScenes and Argoverse2 datasets.","Through comprehensive experiments, we show that: (1) our SDMap prior can improve online map generation performance, using both rasterized (by up to $+18.73$ $\\rm mIoU$) and vectorized (by up to $+8.50$ $\\rm mAP$) output representations.","(2) our HDMap prior can improve map perceptual metrics by up to $6.34\\%$. (3) P-MapNet can be switched into different inference modes that covers different regions of the accuracy-efficiency trade-off landscape.","(4) P-MapNet is a far-seeing solution that brings larger improvements on longer ranges.","Codes and models are publicly available at https://jike5.github.io/P-MapNet."],"url":"http://arxiv.org/abs/2403.10521v1","category":"cs.CV"}
{"created":"2024-03-15 17:59:33","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives","abstract":"We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models. The first stage is global diffusion, which focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives. In contrast, the second-stage is the local diffusion, which parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules. In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion. Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness. Extensive experiments validate the efficacy of our method.","sentences":["We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music.","We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models.","The first stage is global diffusion, which focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives.","In contrast, the second-stage is the local diffusion, which parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules.","In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion.","Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness.","Extensive experiments validate the efficacy of our method."],"url":"http://arxiv.org/abs/2403.10518v1","category":"cs.CV"}
{"created":"2024-03-15 17:57:52","title":"VideoAgent: Long-form Video Understanding with Large Language Model as Agent","abstract":"Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.","sentences":["Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences.","Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs.","We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information.","Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average.","These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding."],"url":"http://arxiv.org/abs/2403.10517v1","category":"cs.CV"}
{"created":"2024-03-15 17:57:06","title":"FeatUp: A Model-Agnostic Framework for Features at Any Resolution","abstract":"Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training. We show that FeatUp significantly outperforms other feature upsampling and image super-resolution approaches in class activation map generation, transfer learning for segmentation and depth prediction, and end-to-end training for semantic segmentation.","sentences":["Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime.","However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas.","In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features.","We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution.","Both approaches use a multi-view consistency loss with deep analogies to NeRFs.","Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training.","We show that FeatUp significantly outperforms other feature upsampling and image super-resolution approaches in class activation map generation, transfer learning for segmentation and depth prediction, and end-to-end training for semantic segmentation."],"url":"http://arxiv.org/abs/2403.10516v1","category":"cs.CV"}
{"created":"2024-03-15 17:53:26","title":"Inflationary non-Gaussianities in alpha vacua and consistency with conformal symmetries","abstract":"We study the conformal invariance of inflationary non-Gaussianities associated with scalar fluctuations in a non-Bunch-Davies initial state, known as the $\\alpha$-vacuum, in single-field slow-roll inflation. The $\\alpha$-vacuum is a one-parameter family of states, including the Bunch-Davies one, that preserves the conformal symmetry of inflationary dynamics in a nearly de-Sitter space-time. Working within the leading slow-roll approximation, we compute the four-point scalar correlator (the trispectrum) in $\\alpha$-vacuum using the in-in formalism. We check that the conformal Ward identities are met between the three and four-point scalar $\\alpha$-vacua correlators. Surprisingly, this contrasts the previously reported negative result of the Ward identities being violated between the two and the three-point correlators. We have also extended the wave-functional method, previously used for correlators with Bunch-Davies initial condition, to compute the three and four-point scalar correlators in $\\alpha$-vacua. The results obtained from the wave-function method match the corresponding in-in results, adding further justification to our check of Ward identities with $\\alpha$-vacua correlators.","sentences":["We study the conformal invariance of inflationary non-Gaussianities associated with scalar fluctuations in a non-Bunch-Davies initial state, known as the $\\alpha$-vacuum, in single-field slow-roll inflation.","The $\\alpha$-vacuum is a one-parameter family of states, including the Bunch-Davies one, that preserves the conformal symmetry of inflationary dynamics in a nearly de-Sitter space-time.","Working within the leading slow-roll approximation, we compute the four-point scalar correlator (the trispectrum) in $\\alpha$-vacuum using the in-in formalism.","We check that the conformal Ward identities are met between the three and four-point scalar $\\alpha$-vacua correlators.","Surprisingly, this contrasts the previously reported negative result of the Ward identities being violated between the two and the three-point correlators.","We have also extended the wave-functional method, previously used for correlators with Bunch-Davies initial condition, to compute the three and four-point scalar correlators in $\\alpha$-vacua.","The results obtained from the wave-function method match the corresponding in-in results, adding further justification to our check of Ward identities with $\\alpha$-vacua correlators."],"url":"http://arxiv.org/abs/2403.10513v1","category":"hep-th"}
{"created":"2024-03-15 17:50:45","title":"A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction","abstract":"Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions. Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task. Furthermore, the vast majority of gaze following approaches have proposed static models that can handle only one person at a time, therefore failing to take advantage of social interactions and temporal dynamics. In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. The framework comprises of: (i) a temporal, transformer-based architecture that, in addition to image tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, that unifies annotation types across multiple gaze following and social gaze datasets. We show that our model trained on VSGaze can address all tasks jointly, and achieves state-of-the-art results for multi-person gaze following and social gaze prediction.","sentences":["Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions.","Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task.","Furthermore, the vast majority of gaze following approaches have proposed static models that can handle only one person at a time, therefore failing to take advantage of social interactions and temporal dynamics.","In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene.","The framework comprises of: (i) a temporal, transformer-based architecture that, in addition to image tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, that unifies annotation types across multiple gaze following and social gaze datasets.","We show that our model trained on VSGaze can address all tasks jointly, and achieves state-of-the-art results for multi-person gaze following and social gaze prediction."],"url":"http://arxiv.org/abs/2403.10511v1","category":"cs.CV"}
{"created":"2024-03-15 17:47:20","title":"Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization","abstract":"Fault localization is a critical process that involves identifying specific program elements responsible for program failures. Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming. To overcome this challenge, various fault localization tools have been developed. These tools typically generate a ranked list of suspicious program elements. However, this information alone is insufficient. A prior study emphasized that automated fault localization should offer a rationale.   In this study, we investigate the step-by-step reasoning for explainable fault localization. We explore the potential of Large Language Models (LLM) in assisting developers in reasoning about code. We proposed FuseFL that utilizes several combinations of information to enhance the LLM results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do). We conducted our investigation using faulty code from Refactory dataset. First, we evaluate the performance of the automated fault localization. Our results demonstrate a more than 30% increase in the number of successfully localized faults at Top-1 compared to the baseline. To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step reasoning as to why specific lines of code are considered faulty. This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines. Furthermore, we also conducted human studies to evaluate the explanations. We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations.","sentences":["Fault localization is a critical process that involves identifying specific program elements responsible for program failures.","Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming.","To overcome this challenge, various fault localization tools have been developed.","These tools typically generate a ranked list of suspicious program elements.","However, this information alone is insufficient.","A prior study emphasized that automated fault localization should offer a rationale.   ","In this study, we investigate the step-by-step reasoning for explainable fault localization.","We explore the potential of Large Language Models (LLM) in assisting developers in reasoning about code.","We proposed FuseFL that utilizes several combinations of information to enhance the LLM results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do).","We conducted our investigation using faulty code from Refactory dataset.","First, we evaluate the performance of the automated fault localization.","Our results demonstrate a more than 30% increase in the number of successfully localized faults at Top-1 compared to the baseline.","To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step reasoning as to why specific lines of code are considered faulty.","This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines.","Furthermore, we also conducted human studies to evaluate the explanations.","We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations."],"url":"http://arxiv.org/abs/2403.10507v1","category":"cs.SE"}
{"created":"2024-03-15 17:45:44","title":"HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation","abstract":"Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. The open-source code is available at https://sferrazza.cc/humanoidbench_site.","sentences":["Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology.","However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups.","To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks.","Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching.","With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas.","The open-source code is available at https://sferrazza.cc/humanoidbench_site."],"url":"http://arxiv.org/abs/2403.10506v1","category":"cs.RO"}
{"created":"2024-03-15 17:40:11","title":"Belief Change based on Knowledge Measures","abstract":"Knowledge Measures (KMs) aim at quantifying the amount of knowledge/information that a knowledge base carries. On the other hand, Belief Change (BC) is the process of changing beliefs (in our case, in terms of contraction, expansion and revision) taking into account a new piece of knowledge, which possibly may be in contradiction with the current belief. We propose a new quantitative BC framework that is based on KMs by defining belief change operators that try to minimise, from an information-theoretic point of view, the surprise that the changed belief carries. To this end, we introduce the principle of minimal surprise. In particular, our contributions are (i) a general information-theoretic approach to KMs for which [1] is a special case; (ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii) a characterisation of any BC operator that satisfies the AGM postulates as a KM-based BC operator, i.e., any BC operator satisfying the AGM postulates can be encoded within our quantitative BC framework. We also introduce quantitative measures that account for the information loss of contraction, information gain of expansion and information change of revision. We also give a succinct look into the problem of iterated revision, which deals with the application of a sequence of revision operations in our framework, and also illustrate how one may build from our KM-based contraction operator also one not satisfying the (in)famous recovery postulate, by focusing on the so-called severe withdrawal model as an illustrative example.","sentences":["Knowledge Measures (KMs) aim at quantifying the amount of knowledge/information that a knowledge base carries.","On the other hand, Belief Change (BC) is the process of changing beliefs (in our case, in terms of contraction, expansion and revision) taking into account a new piece of knowledge, which possibly may be in contradiction with the current belief.","We propose a new quantitative BC framework that is based on KMs by defining belief change operators that try to minimise, from an information-theoretic point of view, the surprise that the changed belief carries.","To this end, we introduce the principle of minimal surprise.","In particular, our contributions are (i) a general information-theoretic approach to KMs for which [1] is a special case; (ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii) a characterisation of any BC operator that satisfies the AGM postulates as a KM-based BC operator, i.e., any BC operator satisfying the AGM postulates can be encoded within our quantitative BC framework.","We also introduce quantitative measures that account for the information loss of contraction, information gain of expansion and information change of revision.","We also give a succinct look into the problem of iterated revision, which deals with the application of a sequence of revision operations in our framework, and also illustrate how one may build from our KM-based contraction operator also one not satisfying the (in)famous recovery postulate, by focusing on the so-called severe withdrawal model as an illustrative example."],"url":"http://arxiv.org/abs/2403.10502v1","category":"cs.AI"}
{"created":"2024-03-15 17:39:12","title":"Quantum States Seen by a Probe: Partial Trace Over a Region of Space","abstract":"The partial trace operation is usually considered in composite quantum systems, to reduce the state on a single subsystem. This operation has a key role in the decoherence effect and quantum measurements. However, partial trace operations can be defined in more generic situations. In particular, it can be used to restrict a quantum state (for a single or several quantum entities) on a specific region of space, the rest of the universe being treated as an environment. The reduced state is then interpreted as the state that can be detected by an ideal probe with a limited spatial extent. In this paper, such an operation is investigated for systems defined on a Fock Hilbert space. A generic expression of the reduced density matrix is computed, and it is applied to several case studies: eigenstates of the number operator, coherent states, and thermal states. These states admit very different behaviors. In particular, (i) a decoherence effect happens on eigenstates of the number operator (ii) coherent or thermal states remain coherent or thermal, but with an amplitude/temperature reduced non-trivially by the overlap between the field and the region of interest.","sentences":["The partial trace operation is usually considered in composite quantum systems, to reduce the state on a single subsystem.","This operation has a key role in the decoherence effect and quantum measurements.","However, partial trace operations can be defined in more generic situations.","In particular, it can be used to restrict a quantum state (for a single or several quantum entities) on a specific region of space, the rest of the universe being treated as an environment.","The reduced state is then interpreted as the state that can be detected by an ideal probe with a limited spatial extent.","In this paper, such an operation is investigated for systems defined on a Fock Hilbert space.","A generic expression of the reduced density matrix is computed, and it is applied to several case studies: eigenstates of the number operator, coherent states, and thermal states.","These states admit very different behaviors.","In particular, (i) a decoherence effect happens on eigenstates of the number operator (ii) coherent or thermal states remain coherent or thermal, but with an amplitude/temperature reduced non-trivially by the overlap between the field and the region of interest."],"url":"http://arxiv.org/abs/2403.10501v1","category":"quant-ph"}
{"created":"2024-03-15 17:35:02","title":"A lozenge triangulation of the plane with integers","abstract":"We introduce and study a three-folded linear operator depending on three parameters that has associated a triangular number tilling of the plane. As a result the set of all triples of integers is decomposed in classes of equivalence organized in four towers of two-dimensional triangulations. We provide the full characterization of the represented integers belonging to each network as families of certain quadratic forms. We note that one of the towers is generated by a germ that produces a covering of the plane with {L\\\"oschian} numbers.","sentences":["We introduce and study a three-folded linear operator depending on three parameters that has associated a triangular number tilling of the plane.","As a result the set of all triples of integers is decomposed in classes of equivalence organized in four towers of two-dimensional triangulations.","We provide the full characterization of the represented integers belonging to each network as families of certain quadratic forms.","We note that one of the towers is generated by a germ that produces a covering of the plane with {L\\\"oschian} numbers."],"url":"http://arxiv.org/abs/2403.10500v1","category":"math.NT"}
{"created":"2024-03-15 17:33:49","title":"Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study","abstract":"Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a significant robustness drop compared to supervised ImageNet models on our benchmark, especially under synthetic distribution shift and adversarial attacks. Furthermore, data overlap analysis suggests that the observed robustness under natural distribution shifts could be attributed, at least in part, to data overlap. In summary, our evaluation shows a comprehensive evaluation of robustness is necessary; and there is a significant need to improve the robustness of zero-shot multimodal models.","sentences":["Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks.","Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training.","Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift.","Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks.","We use CLIP as a pilot study.","We show that CLIP leads to a significant robustness drop compared to supervised ImageNet models on our benchmark, especially under synthetic distribution shift and adversarial attacks.","Furthermore, data overlap analysis suggests that the observed robustness under natural distribution shifts could be attributed, at least in part, to data overlap.","In summary, our evaluation shows a comprehensive evaluation of robustness is necessary; and there is a significant need to improve the robustness of zero-shot multimodal models."],"url":"http://arxiv.org/abs/2403.10499v1","category":"cs.LG"}
{"created":"2024-03-15 17:30:46","title":"PnP Restoration with Domain Adaptation for SANS","abstract":"Small Angle Neutron Scattering (SANS) is a non-destructive technique utilized to probe the nano- to mesoscale structure of materials by analyzing the scattering pattern of neutrons. Accelerating SANS acquisition for in-situ analysis is essential, but it often reduces the signal-to-noise ratio (SNR), highlighting the need for methods to enhance SNR even with short acquisition times. While deep learning (DL) can be used for enhancing SNR of low quality SANS, the amount of experimental data available for training is usually severely limited. We address this issue by proposing a Plug-and-play Restoration for SANS (PR-SANS) that uses domain-adapted priors. The prior in PR-SANS is initially trained on a set of generic images and subsequently fine-tuned using a limited amount of experimental SANS data. We present a theoretical convergence analysis of PR-SANS by focusing on the error resulting from using inexact domain-adapted priors instead of the ideal ones. We demonstrate with experimentally collected SANS data that PR-SANS can recover high-SNR 2D SANS detector images from low-SNR detector images, effectively increasing the SNR. This advancement enables a reduction in acquisition times by a factor of 12 while maintaining the original signal quality.","sentences":["Small Angle Neutron Scattering (SANS) is a non-destructive technique utilized to probe the nano- to mesoscale structure of materials by analyzing the scattering pattern of neutrons.","Accelerating SANS acquisition for in-situ analysis is essential, but it often reduces the signal-to-noise ratio (SNR), highlighting the need for methods to enhance SNR even with short acquisition times.","While deep learning (DL) can be used for enhancing SNR of low quality SANS, the amount of experimental data available for training is usually severely limited.","We address this issue by proposing a Plug-and-play Restoration for SANS (PR-SANS) that uses domain-adapted priors.","The prior in PR-SANS is initially trained on a set of generic images and subsequently fine-tuned using a limited amount of experimental SANS data.","We present a theoretical convergence analysis of PR-SANS by focusing on the error resulting from using inexact domain-adapted priors instead of the ideal ones.","We demonstrate with experimentally collected SANS data that PR-SANS can recover high-SNR 2D SANS detector images from low-SNR detector images, effectively increasing the SNR.","This advancement enables a reduction in acquisition times by a factor of 12 while maintaining the original signal quality."],"url":"http://arxiv.org/abs/2403.10495v1","category":"eess.SP"}
{"created":"2024-03-15 17:27:42","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","abstract":"Diffusion-based audio and music generation models commonly generate music by constructing an image representation of audio (e.g., a mel-spectrogram) and then converting it to audio using a phase reconstruction model or vocoder. Typical vocoders, however, produce monophonic audio at lower resolutions (e.g., 16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an efficient high-fidelity stereophonic vocoder. Our method employs a cascade of three generative adversarial networks (GANs) that convert low-resolution mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth expansion, and upmixes to stereophonic audio. Compared to previous work, we propose 1) a unified GAN-based generator and discriminator architecture and training procedure for each stage of our cascade, 2) a new fast, near downsampling-compatible bandwidth extension module, and 3) a new fast downmix-compatible mono-to-stereo upmixer that ensures the preservation of monophonic content in the output. We evaluate our approach using both objective and subjective listening tests and find our approach yields comparable or better audio quality, better spatialization control, and significantly faster inference speed compared to past work. Sound examples are at https://MusicHiFi.github.io/web/.","sentences":["Diffusion-based audio and music generation models commonly generate music by constructing an image representation of audio (e.g., a mel-spectrogram) and then converting it to audio using a phase reconstruction model or vocoder.","Typical vocoders, however, produce monophonic audio at lower resolutions (e.g., 16-24 kHz), which limits their effectiveness.","We propose MusicHiFi -- an efficient high-fidelity stereophonic vocoder.","Our method employs a cascade of three generative adversarial networks (GANs) that convert low-resolution mel-spectrograms to audio, upsamples to high-resolution audio via bandwidth expansion, and upmixes to stereophonic audio.","Compared to previous work, we propose 1) a unified GAN-based generator and discriminator architecture and training procedure for each stage of our cascade, 2) a new fast, near downsampling-compatible bandwidth extension module, and 3) a new fast downmix-compatible mono-to-stereo upmixer that ensures the preservation of monophonic content in the output.","We evaluate our approach using both objective and subjective listening tests and find our approach yields comparable or better audio quality, better spatialization control, and significantly faster inference speed compared to past work.","Sound examples are at https://MusicHiFi.github.io/web/."],"url":"http://arxiv.org/abs/2403.10493v1","category":"cs.SD"}
{"created":"2024-03-15 17:27:12","title":"Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning","abstract":"Mitigating hallucinations of Large Multi-modal Models(LMMs) is crucial to enhance their reliability for general-purpose assistants. This paper shows that such hallucinations of LMMs can be significantly exacerbated by preceding user-system dialogues. To precisely measure this, we first present an evaluation benchmark by extending popular multi-modal benchmark datasets with prepended hallucinatory dialogues generated by our novel Adversarial Question Generator, which can automatically generate image-related yet adversarial dialogues by adopting adversarial attacks on LMMs. On our benchmark, the zero-shot performance of state-of-the-art LMMs dropped significantly for both the VQA and Captioning tasks. Next, we further reveal this hallucination is mainly due to the prediction bias toward preceding dialogues rather than visual content. To reduce this bias, we propose Adversarial Instruction Tuning that robustly fine-tunes LMMs on augmented multi-modal instruction-following datasets with hallucinatory dialogues. Extensive experiments show that our proposed approach successfully reduces dialogue hallucination while maintaining or even improving performance.","sentences":["Mitigating hallucinations of Large Multi-modal Models(LMMs) is crucial to enhance their reliability for general-purpose assistants.","This paper shows that such hallucinations of LMMs can be significantly exacerbated by preceding user-system dialogues.","To precisely measure this, we first present an evaluation benchmark by extending popular multi-modal benchmark datasets with prepended hallucinatory dialogues generated by our novel Adversarial Question Generator, which can automatically generate image-related yet adversarial dialogues by adopting adversarial attacks on LMMs.","On our benchmark, the zero-shot performance of state-of-the-art LMMs dropped significantly for both the VQA and Captioning tasks.","Next, we further reveal this hallucination is mainly due to the prediction bias toward preceding dialogues rather than visual content.","To reduce this bias, we propose Adversarial Instruction Tuning that robustly fine-tunes LMMs on augmented multi-modal instruction-following datasets with hallucinatory dialogues.","Extensive experiments show that our proposed approach successfully reduces dialogue hallucination while maintaining or even improving performance."],"url":"http://arxiv.org/abs/2403.10492v1","category":"cs.CV"}
{"created":"2024-03-15 17:24:01","title":"Comparative study of the kinetic properties of proton and alpha beams in the Alfv\u00e9nic wind observed by SWA-PAS onboard Solar Orbiter","abstract":"The problems of heating and acceleration of solar wind particles are of significant and enduring interest in astrophysics. The interactions between waves and particles are crucial in determining the distributions of proton and alpha particles, resulting in non-Maxwellian characteristics including temperature anisotropies and particle beams. These processes can be better understood as long as the beam can be separated from the core for the two major components of the solar wind. We utilized an alternative numerical approach that leverages the clustering technique employed in Machine Learning to differentiate the primary populations within the velocity distribution, rather than employing the conventional biMaxwellian fitting method. Separation of the core and beam revealed new features for protons and alphas. We estimated that the total temperature of the two beams was slightly higher than that of their respective cores, and the temperature anisotropy for the cores and beams was larger than 1. We concluded that the temperature ratio between alphas and protons largely over 4 is due to the presence of a massive alpha beam, which is approximately 50% of the alpha core. We provided evidence that the alpha core and beam populations are sensitive to Alfv\\'enic fluctuations and the surfing effect found in the literature can be recovered only when considering the core and beam as a single population. Several similarities between proton and alpha beams would suggest a common and local generation mechanism not shared with the alpha core, which may not have necessarily been accelerated and heated locally.","sentences":["The problems of heating and acceleration of solar wind particles are of significant and enduring interest in astrophysics.","The interactions between waves and particles are crucial in determining the distributions of proton and alpha particles, resulting in non-Maxwellian characteristics including temperature anisotropies and particle beams.","These processes can be better understood as long as the beam can be separated from the core for the two major components of the solar wind.","We utilized an alternative numerical approach that leverages the clustering technique employed in Machine Learning to differentiate the primary populations within the velocity distribution, rather than employing the conventional biMaxwellian fitting method.","Separation of the core and beam revealed new features for protons and alphas.","We estimated that the total temperature of the two beams was slightly higher than that of their respective cores, and the temperature anisotropy for the cores and beams was larger than 1.","We concluded that the temperature ratio between alphas and protons largely over 4 is due to the presence of a massive alpha beam, which is approximately 50% of the alpha core.","We provided evidence that the alpha core and beam populations are sensitive to Alfv\\'enic fluctuations and the surfing effect found in the literature can be recovered only when considering the core and beam as a single population.","Several similarities between proton and alpha beams would suggest a common and local generation mechanism not shared with the alpha core, which may not have necessarily been accelerated and heated locally."],"url":"http://arxiv.org/abs/2403.10489v1","category":"astro-ph.SR"}
{"created":"2024-03-15 17:21:39","title":"Stimulate the Potential of Robots via Competition","abstract":"It is common for us to feel pressure in a competition environment, which arises from the desire to obtain success comparing with other individuals or opponents. Although we might get anxious under the pressure, it could also be a drive for us to stimulate our potentials to the best in order to keep up with others. Inspired by this, we propose a competitive learning framework which is able to help individual robot to acquire knowledge from the competition, fully stimulating its dynamics potential in the race. Specifically, the competition information among competitors is introduced as the additional auxiliary signal to learn advantaged actions. We further build a Multiagent-Race environment, and extensive experiments are conducted, demonstrating that robots trained in competitive environments outperform ones that are trained with SoTA algorithms in single robot environment.","sentences":["It is common for us to feel pressure in a competition environment, which arises from the desire to obtain success comparing with other individuals or opponents.","Although we might get anxious under the pressure, it could also be a drive for us to stimulate our potentials to the best in order to keep up with others.","Inspired by this, we propose a competitive learning framework which is able to help individual robot to acquire knowledge from the competition, fully stimulating its dynamics potential in the race.","Specifically, the competition information among competitors is introduced as the additional auxiliary signal to learn advantaged actions.","We further build a Multiagent-Race environment, and extensive experiments are conducted, demonstrating that robots trained in competitive environments outperform ones that are trained with SoTA algorithms in single robot environment."],"url":"http://arxiv.org/abs/2403.10487v1","category":"cs.RO"}
{"created":"2024-03-15 17:12:57","title":"Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?","abstract":"Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a benchmark, stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry. Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books. The integration of large language models (LLMs) and AI agents marks a groundbreaking development in this field. These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against benchmarks. In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis of performance drivers and utilizing LLMs as calculation engine for multi-level attribution analysis and question-answer (QA) exercises. Leveraging advanced prompt engineering techniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and employing a standard agent framework from LangChain, the research achieves promising results: it achieves accuracy rates exceeding 93% in analyzing performance drivers, attains 100% in multi-level attribution calculations, and surpasses 84% accuracy in QA exercises that simulate official examination standards. These findings affirm the impactful role of AI agents, prompt engineering and evaluation in advancing portfolio management processes, highlighting a significant advancement in the practical application and evaluation of AI technologies within the domain.","sentences":["Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a benchmark, stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry.","Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books.","The integration of large language models (LLMs) and AI agents marks a groundbreaking development in this field.","These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against benchmarks.","In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis of performance drivers and utilizing LLMs as calculation engine for multi-level attribution analysis and question-answer (QA) exercises.","Leveraging advanced prompt engineering techniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and employing a standard agent framework from LangChain, the research achieves promising results: it achieves accuracy rates exceeding 93% in analyzing performance drivers, attains 100% in multi-level attribution calculations, and surpasses 84% accuracy in QA exercises that simulate official examination standards.","These findings affirm the impactful role of AI agents, prompt engineering and evaluation in advancing portfolio management processes, highlighting a significant advancement in the practical application and evaluation of AI technologies within the domain."],"url":"http://arxiv.org/abs/2403.10482v1","category":"q-fin.CP"}
{"created":"2024-03-15 17:09:24","title":"Complete equational theories for classical and quantum Gaussian relations","abstract":"We give generators and relations for the hypergraph props of Gaussian relations and positive affine Lagrangian relations. The former extends Gaussian probabilistic processes by completely-uninformative priors, and the latter extends Gaussian quantum mechanics with infinitely-squeezed states. These presentations are given by adding a generator to the presentation of real affine relations and of real affine Lagrangian relations which freely codiscards effects, as well as certain rotations.   The presentation of positive affine Lagrangian relations provides a rigorous justification for many common yet informal calculations in the quantum physics literature involving infinite-squeezing. Our presentation naturally extends Menicucci et al.'s graph-theoretic representation of Gaussian quantum states with a representation for Gaussian transformations. We interpret the LOv-calculus, a diagrammatic calculus for reasoning about passive linear-optical quantum circuits in our presentation of positive affine Lagrangian relations. Moreover, we show how our presentation allows for additional optical operations such as active squeezing.","sentences":["We give generators and relations for the hypergraph props of Gaussian relations and positive affine Lagrangian relations.","The former extends Gaussian probabilistic processes by completely-uninformative priors, and the latter extends Gaussian quantum mechanics with infinitely-squeezed states.","These presentations are given by adding a generator to the presentation of real affine relations and of real affine Lagrangian relations which freely codiscards effects, as well as certain rotations.   ","The presentation of positive affine Lagrangian relations provides a rigorous justification for many common yet informal calculations in the quantum physics literature involving infinite-squeezing.","Our presentation naturally extends Menicucci et al.'s graph-theoretic representation of Gaussian quantum states with a representation for Gaussian transformations.","We interpret the LOv-calculus, a diagrammatic calculus for reasoning about passive linear-optical quantum circuits in our presentation of positive affine Lagrangian relations.","Moreover, we show how our presentation allows for additional optical operations such as active squeezing."],"url":"http://arxiv.org/abs/2403.10479v1","category":"cs.LO"}
{"created":"2024-03-15 17:07:23","title":"Quantum Synchronization in Nonconservative Electrical Circuits with Kirchhoff-Heisenberg Equations","abstract":"We investigate quantum synchronization phenomena in electrical circuits that incorporate specifically designed nonconservative elements. A dissipative theory of classical and quantized electrical circuits is developed based on the Rayleigh dissipation function. The introduction of this framework enables the formulation of a generalized version of classical Poisson brackets, which are termed Poisson-Rayleigh brackets. By using these brackets, we are able to derive the equations of motion for a given circuit. Remarkably, these equations are found to correspond to Kirchhoff's current laws when Kirchhoff's voltage laws are employed to impose topological constraints, and vice versa. In the quantum setting, the equations of motion are referred to as the Kirchhoff-Heisenberg equations, as they represent Kirchhoff's laws within the Heisenberg picture. These Kirchhoff-Heisenberg equations, serving as the native equations for an electrical circuit, can be used in place of the more abstract master equations in Lindblad form. To validate our theoretical framework, we examine three distinct circuits. The first circuit consists of two resonators coupled via a nonconservative element. The second circuit extends the first to incorporate weakly nonlinear resonators, such as transmons. Lastly, we investigate a circuit involving two resonators connected through an inductor in series with a resistor. This last circuit, which incidentally represents a realistic implementation, allows for the study of a singular system, where the absence of a coordinate leads to an ill-defined system of Hamilton's equations. To analyze such a pathological circuit, we introduce the concept of auxiliary circuit element. After resolving the singularity, we demonstrate that this element can be effectively eliminated at the conclusion of the analysis, recuperating the original circuit.","sentences":["We investigate quantum synchronization phenomena in electrical circuits that incorporate specifically designed nonconservative elements.","A dissipative theory of classical and quantized electrical circuits is developed based on the Rayleigh dissipation function.","The introduction of this framework enables the formulation of a generalized version of classical Poisson brackets, which are termed Poisson-Rayleigh brackets.","By using these brackets, we are able to derive the equations of motion for a given circuit.","Remarkably, these equations are found to correspond to Kirchhoff's current laws when Kirchhoff's voltage laws are employed to impose topological constraints, and vice versa.","In the quantum setting, the equations of motion are referred to as the Kirchhoff-Heisenberg equations, as they represent Kirchhoff's laws within the Heisenberg picture.","These Kirchhoff-Heisenberg equations, serving as the native equations for an electrical circuit, can be used in place of the more abstract master equations in Lindblad form.","To validate our theoretical framework, we examine three distinct circuits.","The first circuit consists of two resonators coupled via a nonconservative element.","The second circuit extends the first to incorporate weakly nonlinear resonators, such as transmons.","Lastly, we investigate a circuit involving two resonators connected through an inductor in series with a resistor.","This last circuit, which incidentally represents a realistic implementation, allows for the study of a singular system, where the absence of a coordinate leads to an ill-defined system of Hamilton's equations.","To analyze such a pathological circuit, we introduce the concept of auxiliary circuit element.","After resolving the singularity, we demonstrate that this element can be effectively eliminated at the conclusion of the analysis, recuperating the original circuit."],"url":"http://arxiv.org/abs/2403.10474v1","category":"quant-ph"}
{"created":"2024-03-15 17:06:12","title":"Self-gravitating Higgs field of scalar charge. II. Asymmetric scalar doublet","abstract":"The self-gravitating Higgs field of a scalar charge is studied for the case of an asymmetric scalar doublet containing, along with a canonical and a phantom component. It is shown that in the zero and first approximation of the smallness of the canonical and phantom scalar charges, the gravitational field of the scalar charge is described by the Schwarzschild-de Sitter metric with a cosmological constant determined by the stable equilibrium point - the vacuum potential of the canonical Higgs field and the zero value of the scalar potential. An equation for the perturbation of the stable value of the potential is obtained and studied, and the asymptotic behavior in the near and far zones is found. The averaging of microscopic oscillations of the scalar field is carried out and it is shown that the sign of the contribution of microscopic oscillations to the macroscopic energy of the scalar field is completely determined by the values of the fundamental constants of the Higgs potential of the asymmetric scalar doublet. Particular attention is paid to the case when the contribution of oscillations to the macroscopic energy and pressure densities is strictly equal to zero. Possible applications of the obtained solutions are discussed.   Keywords: scalarly charged black hole, asymmetric scalar doublet, scalar Higgs field, asymptotic behavior, macroscopic characteristics, doublet orientation.","sentences":["The self-gravitating Higgs field of a scalar charge is studied for the case of an asymmetric scalar doublet containing, along with a canonical and a phantom component.","It is shown that in the zero and first approximation of the smallness of the canonical and phantom scalar charges, the gravitational field of the scalar charge is described by the Schwarzschild-de Sitter metric with a cosmological constant determined by the stable equilibrium point - the vacuum potential of the canonical Higgs field and the zero value of the scalar potential.","An equation for the perturbation of the stable value of the potential is obtained and studied, and the asymptotic behavior in the near and far zones is found.","The averaging of microscopic oscillations of the scalar field is carried out and it is shown that the sign of the contribution of microscopic oscillations to the macroscopic energy of the scalar field is completely determined by the values of the fundamental constants of the Higgs potential of the asymmetric scalar doublet.","Particular attention is paid to the case when the contribution of oscillations to the macroscopic energy and pressure densities is strictly equal to zero.","Possible applications of the obtained solutions are discussed.   ","Keywords: scalarly charged black hole, asymmetric scalar doublet, scalar Higgs field, asymptotic behavior, macroscopic characteristics, doublet orientation."],"url":"http://arxiv.org/abs/2403.10472v1","category":"gr-qc"}
{"created":"2024-03-15 17:06:11","title":"Adding stubs to quantum string field theories","abstract":"Generalizing recent work by Schnabl-Stettinger and Erbin-Firat, we outline a universal algebraic procedure for `adding stubs' to string field theories obeying the BV quantum master equation. We apply our results to classical and quantum closed string field theory as well as to open-closed string field theory. We also clarify several aspects of the integration-out process in the co-algebraic formulation of string field theory at the quantum level.","sentences":["Generalizing recent work by Schnabl-Stettinger and Erbin-Firat, we outline a universal algebraic procedure for `adding stubs' to string field theories obeying the BV quantum master equation.","We apply our results to classical and quantum closed string field theory as well as to open-closed string field theory.","We also clarify several aspects of the integration-out process in the co-algebraic formulation of string field theory at the quantum level."],"url":"http://arxiv.org/abs/2403.10471v1","category":"hep-th"}
{"created":"2024-03-15 16:59:03","title":"The Goldilocks Principle of Learning Unitaries by Interlacing Fixed Operators with Programmable Phase Shifters on a Photonic Chip","abstract":"Programmable photonic integrated circuits represent an emerging technology that amalgamates photonics and electronics, paving the way for light-based information processing at high speeds and low power consumption. Programmable photonics provides a flexible platform that can be reconfigured to perform multiple tasks, thereby holding great promise for revolutionizing future optical networks and quantum computing systems. Over the past decade, there has been constant progress in developing several different architectures for realizing programmable photonic circuits that allow for realizing arbitrary discrete unitary operations with light. Here, we systematically investigate a general family of photonic circuits for realizing arbitrary unitaries based on a simple architecture that interlaces a fixed intervening layer with programmable phase shifter layers. We introduce a criterion for the intervening operator that guarantees the universality of this architecture for representing arbitrary $N \\times N$ unitary operators with $N+1$ phase layers. We explore this criterion for different photonic components, including photonic waveguide lattices and meshes of directional couplers, which allows the identification of several families of photonic components that can serve as the intervening layers in the interlacing architecture. Our findings pave the way for efficiently designing and realizing novel families of programmable photonic integrated circuits for multipurpose analog information processing.","sentences":["Programmable photonic integrated circuits represent an emerging technology that amalgamates photonics and electronics, paving the way for light-based information processing at high speeds and low power consumption.","Programmable photonics provides a flexible platform that can be reconfigured to perform multiple tasks, thereby holding great promise for revolutionizing future optical networks and quantum computing systems.","Over the past decade, there has been constant progress in developing several different architectures for realizing programmable photonic circuits that allow for realizing arbitrary discrete unitary operations with light.","Here, we systematically investigate a general family of photonic circuits for realizing arbitrary unitaries based on a simple architecture that interlaces a fixed intervening layer with programmable phase shifter layers.","We introduce a criterion for the intervening operator that guarantees the universality of this architecture for representing arbitrary $N \\times N$ unitary operators with $N+1$ phase layers.","We explore this criterion for different photonic components, including photonic waveguide lattices and meshes of directional couplers, which allows the identification of several families of photonic components that can serve as the intervening layers in the interlacing architecture.","Our findings pave the way for efficiently designing and realizing novel families of programmable photonic integrated circuits for multipurpose analog information processing."],"url":"http://arxiv.org/abs/2403.10469v1","category":"physics.optics"}
{"created":"2024-03-15 16:58:37","title":"An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues","abstract":"ChatGPT has significantly impacted software development practices, providing substantial assistance to developers in a variety of tasks, including coding, testing, and debugging. Despite its widespread adoption, the impact of ChatGPT as an assistant in collaborative coding remains largely unexplored. In this paper, we analyze a dataset of 210 and 370 developers shared conversations with ChatGPT in GitHub pull requests (PRs) and issues. We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. Our main observations are: (1) Developers seek ChatGPT assistance across 16 types of software engineering inquiries. In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review. (2) Developers frequently engage with ChatGPT via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement. Multi-turn conversations account for 33.2% of the conversations shared in PRs and 36.9% in issues. (3) In collaborative coding, developers leverage shared conversations with ChatGPT to facilitate their role-specific contributions, whether as authors of PRs or issues, code reviewers, or collaborators on issues. Our work serves as the first step towards understanding the dynamics between developers and ChatGPT in collaborative software development and opens up new directions for future research on the topic.","sentences":["ChatGPT has significantly impacted software development practices, providing substantial assistance to developers in a variety of tasks, including coding, testing, and debugging.","Despite its widespread adoption, the impact of ChatGPT as an assistant in collaborative coding remains largely unexplored.","In this paper, we analyze a dataset of 210 and 370 developers shared conversations with ChatGPT in GitHub pull requests (PRs) and issues.","We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them.","Our main observations are: (1) Developers seek ChatGPT assistance across 16 types of software engineering inquiries.","In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review.","(2) Developers frequently engage with ChatGPT via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement.","Multi-turn conversations account for 33.2% of the conversations shared in PRs and 36.9% in issues.","(3) In collaborative coding, developers leverage shared conversations with ChatGPT to facilitate their role-specific contributions, whether as authors of PRs or issues, code reviewers, or collaborators on issues.","Our work serves as the first step towards understanding the dynamics between developers and ChatGPT in collaborative software development and opens up new directions for future research on the topic."],"url":"http://arxiv.org/abs/2403.10468v1","category":"cs.SE"}
{"created":"2024-03-15 16:54:28","title":"Verification of Quantum Computations without Trusted Preparations or Measurements","abstract":"With the advent of delegated quantum computing as a service, verifying quantum computations is becoming a question of great importance. Existing information theoretically Secure Delegated Quantum Computing (SDQC) protocols require the client to possess the ability to perform either trusted state preparations or measurements. Whether it is possible to verify universal quantum computations with information-theoretic security without trusted preparations or measurements was an open question so far. In this paper, we settle this question in the affirmative by presenting a modular, composable, and efficient way to turn known verification schemes into protocols that rely only on trusted gates.   Our first contribution is an extremely lightweight reduction of the problem of quantum verification for BQP to the trusted application of single-qubit rotations around the Z axis and bit flips. The second construction presented in this work shows that it is generally possible to information-theoretically verify arbitrary quantum computations with quantum output without trusted preparations or measurements. However, this second protocol requires the verifier to perform multi-qubit gates on a register whose size is independent of the size of the delegated computation.","sentences":["With the advent of delegated quantum computing as a service, verifying quantum computations is becoming a question of great importance.","Existing information theoretically Secure Delegated Quantum Computing (SDQC) protocols require the client to possess the ability to perform either trusted state preparations or measurements.","Whether it is possible to verify universal quantum computations with information-theoretic security without trusted preparations or measurements was an open question so far.","In this paper, we settle this question in the affirmative by presenting a modular, composable, and efficient way to turn known verification schemes into protocols that rely only on trusted gates.   ","Our first contribution is an extremely lightweight reduction of the problem of quantum verification for BQP to the trusted application of single-qubit rotations around the Z axis and bit flips.","The second construction presented in this work shows that it is generally possible to information-theoretically verify arbitrary quantum computations with quantum output without trusted preparations or measurements.","However, this second protocol requires the verifier to perform multi-qubit gates on a register whose size is independent of the size of the delegated computation."],"url":"http://arxiv.org/abs/2403.10464v1","category":"quant-ph"}
{"created":"2024-03-15 16:54:24","title":"Vacuum zero point energy of self-interacting quantum fields in dS background","abstract":"We consider self-interacting scalar fields with a conformal coupling in the dS background and study the quantum corrections from bubble loop diagrams. Incorporating the perturbative in-in formalism, we calculate the quantum corrections in the vacuum zero point energy and pressure of self-interacting fields with the potential $V \\propto \\Phi^n $ for even values of $n$. We calculate the equation of state corresponding to these quantum corrections and examine the scaling of the divergent terms in the vacuum zero point energy and pressure associated to the dimensional regularization scheme. In particular, we show that for quartic self-interacting scalar field the conformal invariance is respected at two-loop order at the conformal point.","sentences":["We consider self-interacting scalar fields with a conformal coupling in the dS background and study the quantum corrections from bubble loop diagrams.","Incorporating the perturbative in-in formalism, we calculate the quantum corrections in the vacuum zero point energy and pressure of self-interacting fields with the potential $V \\propto \\Phi^n $ for even values of $n$. We calculate the equation of state corresponding to these quantum corrections and examine the scaling of the divergent terms in the vacuum zero point energy and pressure associated to the dimensional regularization scheme.","In particular, we show that for quartic self-interacting scalar field the conformal invariance is respected at two-loop order at the conformal point."],"url":"http://arxiv.org/abs/2403.10463v1","category":"gr-qc"}
{"created":"2024-03-15 16:53:13","title":"Safety Cases: Justifying the Safety of Advanced AI Systems","abstract":"As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.","sentences":["As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them.","To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe.","We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and deference to credible AI advisors.","We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy."],"url":"http://arxiv.org/abs/2403.10462v1","category":"cs.CY"}
{"created":"2024-03-15 16:51:30","title":"Online Concurrent Multi-Robot Coverage Path Planning","abstract":"Recently, centralized receding horizon online multi-robot coverage path planning algorithms have shown remarkable scalability in thoroughly exploring large, complex, unknown workspaces with many robots. In a horizon, the path planning and the path execution interleave, meaning when the path planning occurs for robots with no paths, the robots with outstanding paths do not execute, and subsequently, when the robots with new or outstanding paths execute to reach respective goals, path planning does not occur for those robots yet to get new paths, leading to wastage of both the robotic and the computation resources. As a remedy, we propose a centralized algorithm that is not horizon-based. It plans paths at any time for a subset of robots with no paths, i.e., who have reached their previously assigned goals, while the rest execute their outstanding paths, thereby enabling concurrent planning and execution. We formally prove that the proposed algorithm ensures complete coverage of an unknown workspace and analyze its time complexity. To demonstrate scalability, we evaluate our algorithm to cover eight large $2$D grid benchmark workspaces with up to 512 aerial and ground robots, respectively. A comparison with a state-of-the-art horizon-based algorithm shows its superiority in completing the coverage with up to 1.6x speedup. For validation, we perform ROS + Gazebo simulations in six 2D grid benchmark workspaces with 10 quadcopters and TurtleBots, respectively. We also successfully conducted one outdoor experiment with three quadcopters and one indoor with two TurtleBots.","sentences":["Recently, centralized receding horizon online multi-robot coverage path planning algorithms have shown remarkable scalability in thoroughly exploring large, complex, unknown workspaces with many robots.","In a horizon, the path planning and the path execution interleave, meaning when the path planning occurs for robots with no paths, the robots with outstanding paths do not execute, and subsequently, when the robots with new or outstanding paths execute to reach respective goals, path planning does not occur for those robots yet to get new paths, leading to wastage of both the robotic and the computation resources.","As a remedy, we propose a centralized algorithm that is not horizon-based.","It plans paths at any time for a subset of robots with no paths, i.e., who have reached their previously assigned goals, while the rest execute their outstanding paths, thereby enabling concurrent planning and execution.","We formally prove that the proposed algorithm ensures complete coverage of an unknown workspace and analyze its time complexity.","To demonstrate scalability, we evaluate our algorithm to cover eight large $2$D grid benchmark workspaces with up to 512 aerial and ground robots, respectively.","A comparison with a state-of-the-art horizon-based algorithm shows its superiority in completing the coverage with up to 1.6x speedup.","For validation, we perform ROS + Gazebo simulations in six 2D grid benchmark workspaces with 10 quadcopters and TurtleBots, respectively.","We also successfully conducted one outdoor experiment with three quadcopters and one indoor with two TurtleBots."],"url":"http://arxiv.org/abs/2403.10460v1","category":"cs.RO"}
{"created":"2024-03-15 16:51:24","title":"Understanding the Double Descent Phenomenon in Deep Learning","abstract":"Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.   In this tutorial, we explain the concept of double descent and its mechanisms. The first section sets the classical statistical learning framework and introduces the double descent phenomenon. By looking at a number of examples, section 2 introduces inductive biases that appear to have a key role in double descent by selecting, among the multiple interpolating solutions, a smooth empirical risk minimizer. Finally, section 3 explores the double descent with two linear models, and gives other points of view from recent related works.","sentences":["Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger.","Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance.","Past the interpolation point, increasing model complexity seems to actually lower the test error.   ","In this tutorial, we explain the concept of double descent and its mechanisms.","The first section sets the classical statistical learning framework and introduces the double descent phenomenon.","By looking at a number of examples, section 2 introduces inductive biases that appear to have a key role in double descent by selecting, among the multiple interpolating solutions, a smooth empirical risk minimizer.","Finally, section 3 explores the double descent with two linear models, and gives other points of view from recent related works."],"url":"http://arxiv.org/abs/2403.10459v1","category":"cs.LG"}
{"created":"2024-03-15 16:45:47","title":"Exact second-order spatio-temporal structure-function relationships in non-stationary incompressible turbulent flows with Reynolds decomposition and phase averaging","abstract":"The Karman-Howarth-Monin-Hill (KHMH) equation has been widely applied to scale-by-scale turbulent energy cascade studies in recent years, however, the forms and interpretations are not consistent. The present work generalizes to considering two different spatio-temporal points to reformulate the KHMH equation based on Reynolds decomposition and phase averaging. The unaveraged form and phase averaged form are detailed and interpreted. Then the assumptions of homogeneity and isotropy are included in the KHMH equation to obtain the special form for homogeneous flows and isotropic flows.","sentences":["The Karman-Howarth-Monin-Hill (KHMH) equation has been widely applied to scale-by-scale turbulent energy cascade studies in recent years, however, the forms and interpretations are not consistent.","The present work generalizes to considering two different spatio-temporal points to reformulate the KHMH equation based on Reynolds decomposition and phase averaging.","The unaveraged form and phase averaged form are detailed and interpreted.","Then the assumptions of homogeneity and isotropy are included in the KHMH equation to obtain the special form for homogeneous flows and isotropic flows."],"url":"http://arxiv.org/abs/2403.10457v1","category":"physics.flu-dyn"}
{"created":"2024-03-15 16:42:14","title":"Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness","abstract":"Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, we learn a high-level abstraction that can be solved efficiently and then refined to continuous actions for execution. We demonstrate our approach on several robotics problems where uncertainty is a crucial factor and show that reasoning under uncertainty in these problems outperforms previously proposed determinized planning, direct search, and reinforcement learning strategies. Lastly, we demonstrate our planner on two real-world robotics problems using recent advancements in probabilistic perception.","sentences":["Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems.","However, the typical TAMP problem formulation assumes full observability and deterministic action effects.","These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware.","We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes.","Our planner reasons under uncertainty at both the abstract task level and continuous controller level.","Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, we learn a high-level abstraction that can be solved efficiently and then refined to continuous actions for execution.","We demonstrate our approach on several robotics problems where uncertainty is a crucial factor and show that reasoning under uncertainty in these problems outperforms previously proposed determinized planning, direct search, and reinforcement learning strategies.","Lastly, we demonstrate our planner on two real-world robotics problems using recent advancements in probabilistic perception."],"url":"http://arxiv.org/abs/2403.10454v1","category":"cs.RO"}
{"created":"2024-03-15 16:37:43","title":"Robust Shape Fitting for 3D Scene Abstraction","abstract":"Humans perceive and construct the world as an arrangement of simple parametric models. In particular, we can often describe man-made environments using volumetric primitives such as cuboids or cylinders. Inferring these primitives is important for attaining high-level, abstract scene descriptions. Previous approaches for primitive-based abstraction estimate shape parameters directly and are only able to reproduce simple objects. In contrast, we propose a robust estimator for primitive fitting, which meaningfully abstracts complex real-world environments using cuboids. A RANSAC estimator guided by a neural network fits these primitives to a depth map. We condition the network on previously detected parts of the scene, parsing it one-by-one. To obtain cuboids from single RGB images, we additionally optimise a depth estimation CNN end-to-end. Naively minimising point-to-primitive distances leads to large or spurious cuboids occluding parts of the scene. We thus propose an improved occlusion-aware distance metric correctly handling opaque scenes. Furthermore, we present a neural network based cuboid solver which provides more parsimonious scene abstractions while also reducing inference time. The proposed algorithm does not require labour-intensive labels, such as cuboid annotations, for training. Results on the NYU Depth v2 dataset demonstrate that the proposed algorithm successfully abstracts cluttered real-world 3D scene layouts.","sentences":["Humans perceive and construct the world as an arrangement of simple parametric models.","In particular, we can often describe man-made environments using volumetric primitives such as cuboids or cylinders.","Inferring these primitives is important for attaining high-level, abstract scene descriptions.","Previous approaches for primitive-based abstraction estimate shape parameters directly and are only able to reproduce simple objects.","In contrast, we propose a robust estimator for primitive fitting, which meaningfully abstracts complex real-world environments using cuboids.","A RANSAC estimator guided by a neural network fits these primitives to a depth map.","We condition the network on previously detected parts of the scene, parsing it one-by-one.","To obtain cuboids from single RGB images, we additionally optimise a depth estimation CNN end-to-end.","Naively minimising point-to-primitive distances leads to large or spurious cuboids occluding parts of the scene.","We thus propose an improved occlusion-aware distance metric correctly handling opaque scenes.","Furthermore, we present a neural network based cuboid solver which provides more parsimonious scene abstractions while also reducing inference time.","The proposed algorithm does not require labour-intensive labels, such as cuboid annotations, for training.","Results on the NYU Depth v2 dataset demonstrate that the proposed algorithm successfully abstracts cluttered real-world 3D scene layouts."],"url":"http://arxiv.org/abs/2403.10452v1","category":"cs.CV"}
{"created":"2024-03-15 16:36:15","title":"Subgame Optimal and Prior-Independent Online Algorithms","abstract":"This paper takes a game theoretic approach to the design and analysis of online algorithms and illustrates the approach on the finite-horizon ski-rental problem. This approach allows beyond worst-case analysis of online algorithms. First, we define \"subgame optimality\" which is stronger than worst case optimality in that it requires the algorithm to take advantage of an adversary not playing a worst case input. Algorithms only focusing on the worst case can be far from subgame optimal. Second, we consider prior-independent design and analysis of online algorithms, where rather than choosing a worst case input, the adversary chooses a worst case independent and identical distribution over inputs. Prior-independent online algorithms are generally analytically intractable; instead we give a fully polynomial time approximation scheme to compute them. Highlighting the potential improvement from these paradigms for the finite-horizon ski-rental problem, we empirically compare worst-case, subgame optimal, and prior-independent algorithms in the prior-independent framework.","sentences":["This paper takes a game theoretic approach to the design and analysis of online algorithms and illustrates the approach on the finite-horizon ski-rental problem.","This approach allows beyond worst-case analysis of online algorithms.","First, we define \"subgame optimality\" which is stronger than worst case optimality in that it requires the algorithm to take advantage of an adversary not playing a worst case input.","Algorithms only focusing on the worst case can be far from subgame optimal.","Second, we consider prior-independent design and analysis of online algorithms, where rather than choosing a worst case input, the adversary chooses a worst case independent and identical distribution over inputs.","Prior-independent online algorithms are generally analytically intractable; instead we give a fully polynomial time approximation scheme to compute them.","Highlighting the potential improvement from these paradigms for the finite-horizon ski-rental problem, we empirically compare worst-case, subgame optimal, and prior-independent algorithms in the prior-independent framework."],"url":"http://arxiv.org/abs/2403.10451v1","category":"cs.GT"}
{"created":"2024-03-15 16:30:14","title":"Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases","abstract":"We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.","sentences":["We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases.","Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation.","Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model.","Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries.","The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets.","This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks.","Our code and models are available on Github."],"url":"http://arxiv.org/abs/2403.10446v1","category":"cs.CL"}
{"created":"2024-03-15 16:27:44","title":"Variance sum rule: proofs and solvable models","abstract":"We derive, in more general conditions, a recently introduced variance sum rule (VSR) [I. Di Terlizzi et al., 2024 Science 383 971] involving variances of displacement and force impulse for overdamped Langevin systems in a nonequilibrium steady state (NESS). This formula allows visualising the effect of nonequilibrium as a deviation of the sum of variances from normal diffusion $2Dt$, with $D$ the diffusion constant and $t$ the time. From the VSR, we also derive formulas for the entropy production rate $\\sigma$ that, differently from previous results, involve second-order time derivatives of position correlation functions. This novel feature gives a criterion for discriminating strong nonequilibrium regimes without measuring forces. We then apply and discuss our results to three analytically solved models: a stochastic switching trap, a Brownian vortex, and a Brownian gyrator. Finally, we compare the advantages and limitations of known and novel formulas for $\\sigma$ in an overdamped NESS.","sentences":["We derive, in more general conditions, a recently introduced variance sum rule (VSR)","[I. Di Terlizzi et al., 2024 Science 383 971] involving variances of displacement and force impulse for overdamped Langevin systems in a nonequilibrium steady state (NESS).","This formula allows visualising the effect of nonequilibrium as a deviation of the sum of variances from normal diffusion $2Dt$, with $D$ the diffusion constant and $t$ the time.","From the VSR, we also derive formulas for the entropy production rate $\\sigma$ that, differently from previous results, involve second-order time derivatives of position correlation functions.","This novel feature gives a criterion for discriminating strong nonequilibrium regimes without measuring forces.","We then apply and discuss our results to three analytically solved models: a stochastic switching trap, a Brownian vortex, and a Brownian gyrator.","Finally, we compare the advantages and limitations of known and novel formulas for $\\sigma$ in an overdamped NESS."],"url":"http://arxiv.org/abs/2403.10442v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-15 16:24:53","title":"Optimized Search for a Binary Black Hole Merger Population in LIGO-Virgo O3 Data","abstract":"Maximizing the number of detections in matched filter searches for compact binary coalescence (CBC) gravitational wave (GW) signals requires a model of the source population distribution. In previous searches using the PyCBC framework, sensitivity to the population of binary black hole (BBH) mergers was improved by restricting the range of filter template mass ratios and use of a simple one-dimensional population model. However, this approach does not make use of our full knowledge of the population and cannot be extended to a full parameter space search. Here, we introduce a new ranking method, based on kernel density estimation (KDE) with adaptive bandwidth, to accurately model the probability distributions of binary source parameters over a template bank, both for signals and for noise events. We demonstrate this ranking method by conducting a search over LIGO-Virgo O3 data for BBH with unrestricted mass ratio, using a signal model derived from previous significant detected events. We achieve over 10% increase in sensitive volume for a simple power-law simulated signal population, compared to the previous BBH search. Correspondingly, with the new ranking, 8 additional candidate events above an inverse false alarm rate (IFAR) threshold 0.5 yr are identified.","sentences":["Maximizing the number of detections in matched filter searches for compact binary coalescence (CBC) gravitational wave (GW) signals requires a model of the source population distribution.","In previous searches using the PyCBC framework, sensitivity to the population of binary black hole (BBH) mergers was improved by restricting the range of filter template mass ratios and use of a simple one-dimensional population model.","However, this approach does not make use of our full knowledge of the population and cannot be extended to a full parameter space search.","Here, we introduce a new ranking method, based on kernel density estimation (KDE) with adaptive bandwidth, to accurately model the probability distributions of binary source parameters over a template bank, both for signals and for noise events.","We demonstrate this ranking method by conducting a search over LIGO-Virgo O3 data for BBH with unrestricted mass ratio, using a signal model derived from previous significant detected events.","We achieve over 10% increase in sensitive volume for a simple power-law simulated signal population, compared to the previous BBH search.","Correspondingly, with the new ranking, 8 additional candidate events above an inverse false alarm rate (IFAR) threshold 0.5 yr are identified."],"url":"http://arxiv.org/abs/2403.10439v1","category":"gr-qc"}
{"created":"2024-03-15 16:20:51","title":"Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for Industry Teams","abstract":"Researchers urge technology practitioners such as data scientists to consider the impacts and ethical implications of algorithmic decisions. However, unlike programming, statistics, and data management, discussion of ethical implications is rarely included in standard data science training. To begin to address this gap, we designed and tested a toolbox called the data ethics emergency drill (DEED) to help data science teams discuss and reflect on the ethical implications of their work. The DEED is a roleplay of a fictional ethical emergency scenario that is contextually situated in the team's specific workplace and applications. This paper outlines the DEED toolbox and describes three studies carried out with two different data science teams that iteratively shaped its design. Our findings show that practitioners can apply lessons learnt from the roleplay to real-life situations, and how the DEED opened up conversations around ethics and values.","sentences":["Researchers urge technology practitioners such as data scientists to consider the impacts and ethical implications of algorithmic decisions.","However, unlike programming, statistics, and data management, discussion of ethical implications is rarely included in standard data science training.","To begin to address this gap, we designed and tested a toolbox called the data ethics emergency drill (DEED) to help data science teams discuss and reflect on the ethical implications of their work.","The DEED is a roleplay of a fictional ethical emergency scenario that is contextually situated in the team's specific workplace and applications.","This paper outlines the DEED toolbox and describes three studies carried out with two different data science teams that iteratively shaped its design.","Our findings show that practitioners can apply lessons learnt from the roleplay to real-life situations, and how the DEED opened up conversations around ethics and values."],"url":"http://arxiv.org/abs/2403.10438v1","category":"cs.HC"}
{"created":"2024-03-15 16:16:51","title":"On the Use of Cramer-Rao Lower Bound for Least-Variance Circuit Parameters Identification of Li-ion Cells","abstract":"Electrochemical Impedance Spectroscopy (EIS) and Equivalent Circuit Models (ECMs) are widely used to characterize the impedance and estimate parameters of electrochemical systems such as batteries. We use a generic ECM with ten parameters grouped to model different frequency regions of the Li-ion cell's impedance spectrum. We derive a noise covariance matrix from the measurement model and use it to assign weights for the fitting technique. The paper presents two formulations of the parameters identification problem. Using the properties of the ECM EIS spectra, we propose a method to initialize ECM parameters for the Complex Non-linear Least Squares (CNLS) technique. The paper proposes a novel algorithm for designing the EIS experiments by applying the theory on Cramer-Rao Lower Bound (CRLB) and Fisher Information Matrix (FIM) to the identification problem. We show that contributions to the FIM elements strongly depend on the frequencies at which EIS is performed. Hence, the algorithm aims to adjust frequencies such that the most information about parameters is collected. This is done by minimizing the highest variance of ECM parameters defined by CRLB. Results of a numerical experiment show that the estimator is efficient, and frequency adjustment leads to more accurate ECM parameters' identification.","sentences":["Electrochemical Impedance Spectroscopy (EIS) and Equivalent Circuit Models (ECMs) are widely used to characterize the impedance and estimate parameters of electrochemical systems such as batteries.","We use a generic ECM with ten parameters grouped to model different frequency regions of the Li-ion cell's impedance spectrum.","We derive a noise covariance matrix from the measurement model and use it to assign weights for the fitting technique.","The paper presents two formulations of the parameters identification problem.","Using the properties of the ECM EIS spectra, we propose a method to initialize ECM parameters for the Complex Non-linear Least Squares (CNLS) technique.","The paper proposes a novel algorithm for designing the EIS experiments by applying the theory on Cramer-Rao Lower Bound (CRLB) and Fisher Information Matrix (FIM) to the identification problem.","We show that contributions to the FIM elements strongly depend on the frequencies at which EIS is performed.","Hence, the algorithm aims to adjust frequencies such that the most information about parameters is collected.","This is done by minimizing the highest variance of ECM parameters defined by CRLB.","Results of a numerical experiment show that the estimator is efficient, and frequency adjustment leads to more accurate ECM parameters' identification."],"url":"http://arxiv.org/abs/2403.10435v1","category":"eess.SY"}
{"created":"2024-03-15 16:14:34","title":"Using an LLM to Turn Sign Spottings into Spoken Language Sentences","abstract":"Sign Language Translation (SLT) is a challenging task that aims to generate spoken language sentences from sign language videos. In this paper, we introduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and a pretrained large language model to improve SLT performance. Our method builds upon the strengths of both components. The videos are first processed by the spotter, which is trained on a linguistic sign language dataset, to identify individual signs. These spotted signs are then passed to the powerful language model, which transforms them into coherent and contextually appropriate spoken language sentences.","sentences":["Sign Language Translation (SLT) is a challenging task that aims to generate spoken language sentences from sign language videos.","In this paper, we introduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and a pretrained large language model to improve SLT performance.","Our method builds upon the strengths of both components.","The videos are first processed by the spotter, which is trained on a linguistic sign language dataset, to identify individual signs.","These spotted signs are then passed to the powerful language model, which transforms them into coherent and contextually appropriate spoken language sentences."],"url":"http://arxiv.org/abs/2403.10434v1","category":"cs.CV"}
{"created":"2024-03-15 16:11:15","title":"AI-enhanced Collective Intelligence: The State of the Art and Prospects","abstract":"The current societal challenges exceed the capacity of human individual or collective effort alone. As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. The interplay among these agents shapes the overall structure and dynamics of the system. We explore how agents' diversity and interactions influence the system's collective intelligence. Furthermore, we present an analysis of real-world instances of AI-enhanced collective intelligence. We conclude by addressing the potential challenges in AI-enhanced collective intelligence and offer perspectives on future developments in this field.","sentences":["The current societal challenges exceed the capacity of human individual or collective effort alone.","As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member.","Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation.","However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies.","This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer.","Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism.","The interplay among these agents shapes the overall structure and dynamics of the system.","We explore how agents' diversity and interactions influence the system's collective intelligence.","Furthermore, we present an analysis of real-world instances of AI-enhanced collective intelligence.","We conclude by addressing the potential challenges in AI-enhanced collective intelligence and offer perspectives on future developments in this field."],"url":"http://arxiv.org/abs/2403.10433v1","category":"cs.CY"}
{"created":"2024-03-15 16:08:48","title":"Spatial characterization of debris ejection from the interaction of a tightly focused PW-laser pulse with metal targets","abstract":"We present a novel scheme for rapid quantitative analysis of debris generated during experiments with solid targets following relativistic laser-plasma interaction at high-power laser facilities. Experimental data indicates that predictions by available modelling for non-mass-limited targets are reasonable, with debris on the order of hundreds ug-per-shot. We detect for the first time that several % of the debris is ejected directional following the target normal (rear- and interaction side); and confirm previous work that found the debris ejection in direction of the interaction side to be larger than on the side of the target rear.","sentences":["We present a novel scheme for rapid quantitative analysis of debris generated during experiments with solid targets following relativistic laser-plasma interaction at high-power laser facilities.","Experimental data indicates that predictions by available modelling for non-mass-limited targets are reasonable, with debris on the order of hundreds ug-per-shot.","We detect for the first time that several % of the debris is ejected directional following the target normal (rear- and interaction side); and confirm previous work that found the debris ejection in direction of the interaction side to be larger than on the side of the target rear."],"url":"http://arxiv.org/abs/2403.10431v1","category":"physics.plasm-ph"}
{"created":"2024-03-15 15:59:03","title":"The Advection Boundary Law in absence of mean flow: passivity, nonreciprocity and enhanced noise transmission attenuation","abstract":"Sound attenuation along a waveguide is intensively studied for applications ranging from heating and air-conditioning ventilation systems, to aircraft turbofan engines. In particular, the new generation of Ultra-High-By-Pass-Ratio turbofan requires higher attenuation at low frequencies, in less space for liner treatment. This demands to go beyond the classical acoustic liner concepts and overcome their limitations. In this paper, we discuss an unconventional boundary operator, called Advection Boundary Law, which can be artificially synthesized by electroactive means, such as Electroacoustic Resonators. This boundary condition entails nonreciprocal propagation, meanwhile enhancing noise transmission attenuation with respect to purely locally-reacting boundaries, along one sense of propagation. Because of its artificial nature though, its acoustical passivity limits are yet to be defined. In this paper, we provide a thorough numerical study to assess the performances of the Advection Boundary Law, in absence of mean flow. An experimental test-bench validates the numerical outcomes in terms of passivity limits, non-reciprocal propagation and enhanced isolation with respect to local impedance operators. This work provides the guidelines to properly implement the Advection Boundary Law for optimal noise transmission attenuation. Moreover, the tools and criteria provided here can also be employed for the design and characterization of other innovative liners.","sentences":["Sound attenuation along a waveguide is intensively studied for applications ranging from heating and air-conditioning ventilation systems, to aircraft turbofan engines.","In particular, the new generation of Ultra-High-By-Pass-Ratio turbofan requires higher attenuation at low frequencies, in less space for liner treatment.","This demands to go beyond the classical acoustic liner concepts and overcome their limitations.","In this paper, we discuss an unconventional boundary operator, called Advection Boundary Law, which can be artificially synthesized by electroactive means, such as Electroacoustic Resonators.","This boundary condition entails nonreciprocal propagation, meanwhile enhancing noise transmission attenuation with respect to purely locally-reacting boundaries, along one sense of propagation.","Because of its artificial nature though, its acoustical passivity limits are yet to be defined.","In this paper, we provide a thorough numerical study to assess the performances of the Advection Boundary Law, in absence of mean flow.","An experimental test-bench validates the numerical outcomes in terms of passivity limits, non-reciprocal propagation and enhanced isolation with respect to local impedance operators.","This work provides the guidelines to properly implement the Advection Boundary Law for optimal noise transmission attenuation.","Moreover, the tools and criteria provided here can also be employed for the design and characterization of other innovative liners."],"url":"http://arxiv.org/abs/2403.10426v1","category":"physics.app-ph"}
{"created":"2024-03-15 15:58:51","title":"NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices","abstract":"Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10x-80x speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow.","sentences":["Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision.","While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs.","In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns.","The architecture follows a global-to-local scheme.","Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy.","We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms.","We achieve a notable 10x-80x speedup compared to several state-of-the-art methods, while maintaining comparable accuracy.","Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones.","The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow."],"url":"http://arxiv.org/abs/2403.10425v1","category":"cs.CV"}
{"created":"2024-03-15 15:58:37","title":"Structured Evaluation of Synthetic Tabular Data","abstract":"Tabular data is common yet typically incomplete, small in volume, and access-restricted due to privacy concerns. Synthetic data generation offers potential solutions. Many metrics exist for evaluating the quality of synthetic tabular data; however, we lack an objective, coherent interpretation of the many metrics. To address this issue, we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data. Through various structural decomposition of the objective, this framework allows us to reason for the first time the completeness of any set of metrics, as well as unifies existing metrics, including those that stem from fidelity considerations, downstream application, and model-based approaches. Moreover, the framework motivates model-free baselines and a new spectrum of metrics. We evaluate structurally informed synthesizers and synthesizers powered by deep learning. Using our structured framework, we show that synthetic data generators that explicitly represent tabular structure outperform other methods, especially on smaller datasets.","sentences":["Tabular data is common yet typically incomplete, small in volume, and access-restricted due to privacy concerns.","Synthetic data generation offers potential solutions.","Many metrics exist for evaluating the quality of synthetic tabular data; however, we lack an objective, coherent interpretation of the many metrics.","To address this issue, we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data.","Through various structural decomposition of the objective, this framework allows us to reason for the first time the completeness of any set of metrics, as well as unifies existing metrics, including those that stem from fidelity considerations, downstream application, and model-based approaches.","Moreover, the framework motivates model-free baselines and a new spectrum of metrics.","We evaluate structurally informed synthesizers and synthesizers powered by deep learning.","Using our structured framework, we show that synthetic data generators that explicitly represent tabular structure outperform other methods, especially on smaller datasets."],"url":"http://arxiv.org/abs/2403.10424v1","category":"cs.LG"}
{"created":"2024-03-15 15:56:35","title":"Quantifying nonuniversal corner free-energy contributions in weakly-anisotropic two-dimensional critical systems","abstract":"We derive an exact formula for the corner free-energy contribution of weakly-anisotropic two-dimensional critical systems in the Ising universality class on rectangular domains, expressed in terms of quantities that specify the anisotropic fluctuations. The resulting expression agrees with numerical exact calculations that we perform for the anisotropic triangular Ising model and quantifies the nonuniversality of the corner term for anisotropic critical two-dimensional systems. Our generic formula is expected to apply also to other weakly-anisotropic critical two-dimensional systems that allow for a conformal field theory description in the isotropic limit. We consider the 3-states and 4-states Potts models as further specific examples.","sentences":["We derive an exact formula for the corner free-energy contribution of weakly-anisotropic two-dimensional critical systems in the Ising universality class on rectangular domains, expressed in terms of quantities that specify the anisotropic fluctuations.","The resulting expression agrees with numerical exact calculations that we perform for the anisotropic triangular Ising model and quantifies the nonuniversality of the corner term for anisotropic critical two-dimensional systems.","Our generic formula is expected to apply also to other weakly-anisotropic critical two-dimensional systems that allow for a conformal field theory description in the isotropic limit.","We consider the 3-states and 4-states Potts models as further specific examples."],"url":"http://arxiv.org/abs/2403.10422v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-15 15:55:19","title":"Neural Networks Hear You Loud And Clear: Hearing Loss Compensation Using Deep Neural Networks","abstract":"This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation. Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation. DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification. In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals. First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway. We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation. Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy. Finally, we evaluate the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners. The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies. Our proposed approach was shown to provide an increase in speech intelligibility and was found to outperform a conventional approach in terms of perceived speech quality.","sentences":["This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation.","Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation.","DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification.","In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals.","First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway.","We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation.","Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy.","Finally, we evaluate the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners.","The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies.","Our proposed approach was shown to provide an increase in speech intelligibility and was found to outperform a conventional approach in terms of perceived speech quality."],"url":"http://arxiv.org/abs/2403.10420v1","category":"eess.AS"}
{"created":"2024-03-15 15:49:31","title":"Gradient based Feature Attribution in Explainable AI: A Technical Review","abstract":"The surge in black-box AI models has prompted the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving. Due to the lack of a rigorous definition of explainable AI (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives. Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects. Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models. In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes. Then, we present the essence of technique details in chronological order and underscore the evolution of algorithms. Next, we introduce both human and quantitative evaluations to measure algorithm performance. More importantly, we demonstrate the general challenges in XAI and specific challenges in gradient based explanations. We hope that this survey can help researchers understand state-of-the-art progress and their corresponding disadvantages, which could spark their interest in addressing these issues in future work.","sentences":["The surge in black-box AI models has prompted the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving.","Due to the lack of a rigorous definition of explainable AI (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives.","Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects.","Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models.","In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes.","Then, we present the essence of technique details in chronological order and underscore the evolution of algorithms.","Next, we introduce both human and quantitative evaluations to measure algorithm performance.","More importantly, we demonstrate the general challenges in XAI and specific challenges in gradient based explanations.","We hope that this survey can help researchers understand state-of-the-art progress and their corresponding disadvantages, which could spark their interest in addressing these issues in future work."],"url":"http://arxiv.org/abs/2403.10415v1","category":"cs.AI"}
{"created":"2024-03-15 15:48:36","title":"Diamond Micro-Chip for Quantum Microscopy","abstract":"The nitrogen vacancy (NV) center in diamond is an increasingly popular quantum sensor for microscopy of electrical current, magnetization, and spins. However, efficient NV-sample integration with a robust, high-quality interface remains an outstanding challenge to realize scalable, high-throughput microscopy. In this work, we characterize a diamond micro-chip (DMC) containing a (111)-oriented NV ensemble; and demonstrate its utility for high-resolution quantum microscopy. We perform strain imaging of the DMC and find minimal detrimental strain variation across a field-of-view of tens of micrometer. We find good ensemble NV spin coherence and optical properties in the DMC, suitable for sensitive magnetometry. We then use the DMC to demonstrate wide-field microscopy of electrical current, and show that diffraction-limited quantum microscopy can be achieved. We also demonstrate the deterministic transfer of DMCs with multiple materials of interest for next-generation electronics and spintronics. Lastly, we develop a polymer-based technique for DMC placement. This work establishes the DMC's potential to expand the application of NV quantum microscopy in materials, device, geological, biomedical, and chemical sciences.","sentences":["The nitrogen vacancy (NV) center in diamond is an increasingly popular quantum sensor for microscopy of electrical current, magnetization, and spins.","However, efficient NV-sample integration with a robust, high-quality interface remains an outstanding challenge to realize scalable, high-throughput microscopy.","In this work, we characterize a diamond micro-chip (DMC) containing a (111)-oriented NV ensemble; and demonstrate its utility for high-resolution quantum microscopy.","We perform strain imaging of the DMC and find minimal detrimental strain variation across a field-of-view of tens of micrometer.","We find good ensemble NV spin coherence and optical properties in the DMC, suitable for sensitive magnetometry.","We then use the DMC to demonstrate wide-field microscopy of electrical current, and show that diffraction-limited quantum microscopy can be achieved.","We also demonstrate the deterministic transfer of DMCs with multiple materials of interest for next-generation electronics and spintronics.","Lastly, we develop a polymer-based technique for DMC placement.","This work establishes the DMC's potential to expand the application of NV quantum microscopy in materials, device, geological, biomedical, and chemical sciences."],"url":"http://arxiv.org/abs/2403.10414v1","category":"physics.app-ph"}
{"created":"2024-03-15 15:47:33","title":"RIS-Assisted Physical Layer Security in Emerging RF and Optical Wireless Communication Systems: A Comprehensive Survey","abstract":"Physical layer security (PLS) has received a growing interest from the research community for its ability to safeguard data confidentiality without relying on key distribution or encryption/decryption. However, the evolution towards the 5G technology and beyond poses new security challenges that must be addressed in order to fulfill the unprecedented performance requirements of future wireless networks. Among the potential enabling technologies, RIS has attracted extensive attention due to its ability to proactively and intelligently reconfigure the wireless propagation environment to combat dynamic wireless channel impairments. Consequently, the RIS technology can be adopted to improve the information-theoretic security of both RF and OWC systems. This survey paper provides a comprehensive overview of the information-theoretic security of RIS-based RF and optical systems. The article first discusses the fundamental concepts of PLS and RIS technologies, followed by their combination in both RF and OWC systems. Subsequently, some optimization techniques are presented in the context of the underlying system model, followed by an assessment of the impact of RIS-assisted PLS through a comprehensive performance analysis. Given that the computational complexity of future communication systems that adopt RIS-assisted PLS is likely to increase rapidly as the number of interactions between the users and infrastructure grows, ML is seen as a promising approach to address this complexity issue while sustaining or improving the network performance. A discussion of recent research studies on RIS-assisted PLS-based systems embedded with ML is presented. Furthermore, some important open research challenges are proposed and discussed to provide insightful future research directions, with the aim of moving a step closer towards the development and implementation of the forthcoming 6G wireless technology.","sentences":["Physical layer security (PLS) has received a growing interest from the research community for its ability to safeguard data confidentiality without relying on key distribution or encryption/decryption.","However, the evolution towards the 5G technology and beyond poses new security challenges that must be addressed in order to fulfill the unprecedented performance requirements of future wireless networks.","Among the potential enabling technologies, RIS has attracted extensive attention due to its ability to proactively and intelligently reconfigure the wireless propagation environment to combat dynamic wireless channel impairments.","Consequently, the RIS technology can be adopted to improve the information-theoretic security of both RF and OWC systems.","This survey paper provides a comprehensive overview of the information-theoretic security of RIS-based RF and optical systems.","The article first discusses the fundamental concepts of PLS and RIS technologies, followed by their combination in both RF and OWC systems.","Subsequently, some optimization techniques are presented in the context of the underlying system model, followed by an assessment of the impact of RIS-assisted PLS through a comprehensive performance analysis.","Given that the computational complexity of future communication systems that adopt RIS-assisted PLS is likely to increase rapidly as the number of interactions between the users and infrastructure grows, ML is seen as a promising approach to address this complexity issue while sustaining or improving the network performance.","A discussion of recent research studies on RIS-assisted PLS-based systems embedded with ML is presented.","Furthermore, some important open research challenges are proposed and discussed to provide insightful future research directions, with the aim of moving a step closer towards the development and implementation of the forthcoming 6G wireless technology."],"url":"http://arxiv.org/abs/2403.10412v1","category":"cs.IT"}
{"created":"2024-03-15 15:43:02","title":"SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores","abstract":"We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid -- a decentralised Web specification -- to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/.","sentences":["We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications.","Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid -- a decentralised Web specification -- to decouple user data from generative AI applications.","We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly.","SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods.","SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers.","Besides better privacy controls, this approach also enables portability across different services and applications.","Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address.","Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/."],"url":"http://arxiv.org/abs/2403.10408v1","category":"cs.CR"}
{"created":"2024-03-15 15:37:04","title":"Energy Correction Model in the Feature Space for Out-of-Distribution Detection","abstract":"In this work, we study the out-of-distribution (OOD) detection problem through the use of the feature space of a pre-trained deep classifier. We show that learning the density of in-distribution (ID) features with an energy-based models (EBM) leads to competitive detection results. However, we found that the non-mixing of MCMC sampling during the EBM's training undermines its detection performance. To overcome this an energy-based correction of a mixture of class-conditional Gaussian distributions. We obtains favorable results when compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100 OOD detection benchmarks.","sentences":["In this work, we study the out-of-distribution (OOD) detection problem through the use of the feature space of a pre-trained deep classifier.","We show that learning the density of in-distribution (ID) features with an energy-based models (EBM) leads to competitive detection results.","However, we found that the non-mixing of MCMC sampling during the EBM's training undermines its detection performance.","To overcome this an energy-based correction of a mixture of class-conditional Gaussian distributions.","We obtains favorable results when compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100 OOD detection benchmarks."],"url":"http://arxiv.org/abs/2403.10403v1","category":"cs.CV"}
{"created":"2024-03-15 15:34:59","title":"SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal Conditioned Diffusion Policy","abstract":"Manipulating deformable objects remains a challenge within robotics due to the difficulties of state estimation, long-horizon planning, and predicting how the object will deform given an interaction. These challenges are the most pronounced with 3D deformable objects. We propose SculptDiff, a goal-conditioned diffusion-based imitation learning framework that works with point cloud state observations to directly learn clay sculpting policies for a variety of target shapes. To the best of our knowledge this is the first real-world method that successfully learns manipulation policies for 3D deformable objects. For sculpting videos and access to our dataset and hardware CAD models, see the project website: https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home","sentences":["Manipulating deformable objects remains a challenge within robotics due to the difficulties of state estimation, long-horizon planning, and predicting how the object will deform given an interaction.","These challenges are the most pronounced with 3D deformable objects.","We propose SculptDiff, a goal-conditioned diffusion-based imitation learning framework that works with point cloud state observations to directly learn clay sculpting policies for a variety of target shapes.","To the best of our knowledge this is the first real-world method that successfully learns manipulation policies for 3D deformable objects.","For sculpting videos and access to our dataset and hardware CAD models, see the project website: https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home"],"url":"http://arxiv.org/abs/2403.10401v1","category":"cs.RO"}
{"created":"2024-03-15 15:32:56","title":"Learning of Nash Equilibria in Risk-Averse Games","abstract":"This paper considers risk-averse learning in convex games involving multiple agents that aim to minimize their individual risk of incurring significantly high costs. Specifically, the agents adopt the conditional value at risk (CVaR) as a risk measure with possibly different risk levels. To solve this problem, we propose a first-order risk-averse leaning algorithm, in which the CVaR gradient estimate depends on an estimate of the Value at Risk (VaR) value combined with the gradient of the stochastic cost function. Although estimation of the CVaR gradients using finitely many samples is generally biased, we show that the accumulated error of the CVaR gradient estimates is bounded with high probability. Moreover, assuming that the risk-averse game is strongly monotone, we show that the proposed algorithm converges to the risk-averse Nash equilibrium. We present numerical experiments on a Cournot game example to illustrate the performance of the proposed method.","sentences":["This paper considers risk-averse learning in convex games involving multiple agents that aim to minimize their individual risk of incurring significantly high costs.","Specifically, the agents adopt the conditional value at risk (CVaR) as a risk measure with possibly different risk levels.","To solve this problem, we propose a first-order risk-averse leaning algorithm, in which the CVaR gradient estimate depends on an estimate of the Value at Risk (VaR) value combined with the gradient of the stochastic cost function.","Although estimation of the CVaR gradients using finitely many samples is generally biased, we show that the accumulated error of the CVaR gradient estimates is bounded with high probability.","Moreover, assuming that the risk-averse game is strongly monotone, we show that the proposed algorithm converges to the risk-averse Nash equilibrium.","We present numerical experiments on a Cournot game example to illustrate the performance of the proposed method."],"url":"http://arxiv.org/abs/2403.10399v1","category":"math.OC"}
{"created":"2024-03-15 15:27:58","title":"Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding","abstract":"Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Secondly, we perform fine-tuning using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning. As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned geometry, rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent. The project page is available at https://isotropic3d.github.io/. The code and models are available at https://github.com/pkunliu/Isotropic3D.","sentences":["Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress.","Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view.","Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently.","In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input.","Isotropic3D allows the optimization to be isotropic w.r.t.","the azimuth angle by solely resting on the SDS loss.","The core of our framework lies in a two-stage diffusion model fine-tuning.","Firstly, we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities.","Secondly, we perform fine-tuning using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition.","CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning.","As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned geometry, rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent.","The project page is available at https://isotropic3d.github.io/. The code and models are available at https://github.com/pkunliu/Isotropic3D."],"url":"http://arxiv.org/abs/2403.10395v1","category":"cs.CV"}
{"created":"2024-03-15 15:24:14","title":"Fano fibrations and DK conjecture for relative Grassmann flips","abstract":"Given a vector bundle $\\mathcal E$ on a smooth projective variety $B$, the flag bundle $\\mathcal F l(1,2,\\mathcal E)$ admits two projective bundle structures over the Grassmann bundles $\\mathcal G r(1, \\mathcal E)$ and $G r(2, \\mathcal E)$. The data of a general section of a suitably defined line bundle on $\\mathcal F l(1,2,\\mathcal E)$ defines two varieties: a cover $X_1$ of $B$ and a fibration $X_2$ on $B$ with general fiber isomorphic to a smooth Fano variety. We construct a semiorthogonal decomposition of the derived category of $X_2$ which consists of a list of exceptional objects and a subcategory equivalent to the derived category of $X_1$. As a byproduct, we obtain a new full exceptional collection for the Fano fourfold of degree $12$ and genus $7$. Any birational map of smooth projective varieties which is resolved by blowups with exceptional divisor $\\mathcal F l(1, 2, \\mathcal E)$ is an instance of a so-called Grassmann flip: we prove that the DK conjecture of Bondal-Orlov and Kawamata holds for such flips. This generalizes a previous result of Leung and Xie to a relative setting.","sentences":["Given a vector bundle $\\mathcal E$ on a smooth projective variety $B$, the flag bundle $\\mathcal F l(1,2,\\mathcal E)$ admits two projective bundle structures over the Grassmann bundles $\\mathcal G r(1, \\mathcal E)$ and $G r(2, \\mathcal E)$.","The data of a general section of a suitably defined line bundle on $\\mathcal F l(1,2,\\mathcal E)$ defines two varieties: a cover $X_1$ of $B$ and a fibration $X_2$ on $B$ with general fiber isomorphic to a smooth Fano variety.","We construct a semiorthogonal decomposition of the derived category of $X_2$ which consists of a list of exceptional objects and a subcategory equivalent to the derived category of $X_1$. As a byproduct, we obtain a new full exceptional collection for the Fano fourfold of degree $12$ and genus $7$. Any birational map of smooth projective varieties which is resolved by blowups with exceptional divisor $\\mathcal F l(1, 2, \\mathcal E)$ is an instance of a so-called Grassmann flip: we prove that the DK conjecture of Bondal-Orlov and Kawamata holds for such flips.","This generalizes a previous result of Leung and Xie to a relative setting."],"url":"http://arxiv.org/abs/2403.10393v1","category":"math.AG"}
{"created":"2024-03-15 15:22:40","title":"Statistical investigation of wave propagation in the quiet-Sun using IRIS spectroscopic observations","abstract":"In the current analysis, we use spectroscopic observations of the quiet-Sun made by IRIS instrument, and investigate wave propagation. We analyze various spectral lines formed in different atmospheric layers such as the photosphere, chromosphere, and transition region. We examine Doppler velocity time-series at various locations in the quiet-Sun to determine the dominant oscillation periods. Our results executing statistical analysis resemble those of the classical physical scenario, indicating that the photosphere is mainly characterized by the dominant 5-minute period, while the chromosphere is primarily associated with the 3-minute oscillation period. In the transition region, we observe a variety of oscillation periods, with dominant periods of 3, 8, and 12 minutes. We estimate the cut-off frequency by deducing phase difference between two Doppler velocity time-series obtained from spectral line pairs in different atmospheric layers formed at different temperatures. It reveals a significant correlation between 3-minute periods in TR and photospheric oscillations, suggesting that these oscillations in the TR might propagate from the photosphere. Additionally, we analyze the phase difference between chromospheric oscillations and photospheric oscillations, demonstrating that only the 3-minute oscillations propagate upwards. Based on the statistical analyses, we suggest the presence of magnetoacoustic waves in the solar atmosphere in which some are propagating from the lower solar atmosphere upward, while some others are propagating downward. TR carries both long-period oscillations generated in situ, and some photospheric oscillations which are also able to reach there from below.","sentences":["In the current analysis, we use spectroscopic observations of the quiet-Sun made by IRIS instrument, and investigate wave propagation.","We analyze various spectral lines formed in different atmospheric layers such as the photosphere, chromosphere, and transition region.","We examine Doppler velocity time-series at various locations in the quiet-Sun to determine the dominant oscillation periods.","Our results executing statistical analysis resemble those of the classical physical scenario, indicating that the photosphere is mainly characterized by the dominant 5-minute period, while the chromosphere is primarily associated with the 3-minute oscillation period.","In the transition region, we observe a variety of oscillation periods, with dominant periods of 3, 8, and 12 minutes.","We estimate the cut-off frequency by deducing phase difference between two Doppler velocity time-series obtained from spectral line pairs in different atmospheric layers formed at different temperatures.","It reveals a significant correlation between 3-minute periods in TR and photospheric oscillations, suggesting that these oscillations in the TR might propagate from the photosphere.","Additionally, we analyze the phase difference between chromospheric oscillations and photospheric oscillations, demonstrating that only the 3-minute oscillations propagate upwards.","Based on the statistical analyses, we suggest the presence of magnetoacoustic waves in the solar atmosphere in which some are propagating from the lower solar atmosphere upward, while some others are propagating downward.","TR carries both long-period oscillations generated in situ, and some photospheric oscillations which are also able to reach there from below."],"url":"http://arxiv.org/abs/2403.10392v1","category":"astro-ph.SR"}
{"created":"2024-03-15 15:17:21","title":"Consistent extinction model for type Ia supernovae in Cepheid-based calibration galaxies and its impact on $H_{0}$","abstract":"The most recent SH0ES measurement of the Hubble constant, based on type Ia supernovae from the Pantheon+ compilation, employs corrections of supernova peak magnitudes which effectively accounts for extinction in the supernova host galaxies. These corrections are estimated using a probabilistic model which is trained on Hubble flow (z>0.03) supernovae and extrapolated to the calibration galaxies (those with observed Cepheid distances). By comparing the corrected peak magnitudes to distance moduli from Cepheids, we show that this standard approach underestimates the brightness of reddened supernovae in the high stellar-mass ($M_{\\star}>10^{10}M_{\\odot}$) calibration galaxies. This can be traced back to the fact that for these galaxies, a low total-to-selective extinction coefficient (R_B~3) is assumed, while for the low stellar-mass analogues a more standard R_B~4 is assumed. We propose a minimalistic modification of the Pantheon+ extinction model in order to alleviate this systematic effect. The modification is twofold and it involves: (i) the same, Milky Way-like distribution of R_B in all calibration galaxies (with mean R_B of 4.3 -- consistent with the extinction curve used for colour corrections of the Cepheids -- and scatter 0.4) and (ii) a modified shape of the E(B-V) reddening distribution while keeping the same effective slope of the supernova peak magnitude-colour relation and the same mean E(B-V) reddening as measured for supernovae in the Hubble flow. We show that this new approach yields a significantly better fit ($\\Delta$BIC=-11) to the calibration data and results in a lower value of the derived Hubble constant through a stronger extinction correction of supernovae in the calibration galaxies. Our result is $H_{0}=70.5\\pm1$ km/s/Mpc implying a reduction of the tension with the Planck $H_{0}$ measurement assuming a flat LCDM cosmology from $5.2\\sigma$ to $2.8\\sigma$.","sentences":["The most recent SH0ES measurement of the Hubble constant, based on type Ia supernovae from the Pantheon+ compilation, employs corrections of supernova peak magnitudes which effectively accounts for extinction in the supernova host galaxies.","These corrections are estimated using a probabilistic model which is trained on Hubble flow (z>0.03) supernovae and extrapolated to the calibration galaxies (those with observed Cepheid distances).","By comparing the corrected peak magnitudes to distance moduli from Cepheids, we show that this standard approach underestimates the brightness of reddened supernovae in the high stellar-mass ($M_{\\star}>10^{10}M_{\\odot}$) calibration galaxies.","This can be traced back to the fact that for these galaxies, a low total-to-selective extinction coefficient (R_B~3) is assumed, while for the low stellar-mass analogues a more standard R_B~4 is assumed.","We propose a minimalistic modification of the Pantheon+ extinction model in order to alleviate this systematic effect.","The modification is twofold and it involves: (i) the same, Milky Way-like distribution of R_B in all calibration galaxies (with mean R_B of 4.3 -- consistent with the extinction curve used for colour corrections of the Cepheids -- and scatter 0.4) and (ii) a modified shape of the E(B-V) reddening distribution while keeping the same effective slope of the supernova peak magnitude-colour relation and the same mean E(B-V) reddening as measured for supernovae in the Hubble flow.","We show that this new approach yields a significantly better fit ($\\Delta$BIC=-11) to the calibration data and results in a lower value of the derived Hubble constant through a stronger extinction correction of supernovae in the calibration galaxies.","Our result is $H_{0}=70.5\\pm1$ km/s/Mpc implying a reduction of the tension with the Planck $H_{0}$ measurement assuming a flat LCDM cosmology from $5.2\\sigma$ to $2.8\\sigma$."],"url":"http://arxiv.org/abs/2403.10388v1","category":"astro-ph.CO"}
{"created":"2024-03-15 15:14:01","title":"Scattering between a vortex dipole and a point vortex: insights from a new phase plane","abstract":"We study the scattering of a vortex dipole by a point vortex in the Helmholtz model of point-vortex interactions. We reduce a system of three vortices by stages to a one-degree-of-freedom conservative system. We relate properties of the scattering dynamics to features in the phase space of the reduced system. We first change variables to the Jacobi coordinates, followed by a Nambu reduction. This shows that the natural phase space of this problem is the upper sheet of a two-sheeted hyperboloid, which degenerates to an upper half-cone in a critical case. Gr\\\"obli (1877) derived the standard reduction of this system in which the system's state is represented by the lengths of the triangle's sides. The new coordinate system overcomes two related shortcomings of Gr\\\"obli's reduction that have made understanding the dynamics difficult: the lack of a phase plane and the singularity at all configurations in which the vortices are collinear. We then generalize the problem to consider when the circulation of the third, initially stationary, vortex differs from the circulations of the two that form the propagating dipole.","sentences":["We study the scattering of a vortex dipole by a point vortex in the Helmholtz model of point-vortex interactions.","We reduce a system of three vortices by stages to a one-degree-of-freedom conservative system.","We relate properties of the scattering dynamics to features in the phase space of the reduced system.","We first change variables to the Jacobi coordinates, followed by a Nambu reduction.","This shows that the natural phase space of this problem is the upper sheet of a two-sheeted hyperboloid, which degenerates to an upper half-cone in a critical case.","Gr\\\"obli (1877) derived the standard reduction of this system in which the system's state is represented by the lengths of the triangle's sides.","The new coordinate system overcomes two related shortcomings of Gr\\\"obli's reduction that have made understanding the dynamics difficult: the lack of a phase plane and the singularity at all configurations in which the vortices are collinear.","We then generalize the problem to consider when the circulation of the third, initially stationary, vortex differs from the circulations of the two that form the propagating dipole."],"url":"http://arxiv.org/abs/2403.10383v1","category":"math.DS"}
{"created":"2024-03-15 15:11:15","title":"One-dimensional Lieb superlattices: from the discrete to the continuum limit","abstract":"The Lieb lattice is one of the simplest lattices that exhibits both linear Dirac-like and flat topological electronic bands. We propose to further tailor its electronic properties through periodic 1D electrostatic superlattices (SLs), which, in the long wavelength limit, were predicted to give rise to novel transport signatures, such as the omnidirectional super-Klein tunnelling (SKT). By numerically modelling the electronic structure at tight-binding level, we uncover the evolution of the Lieb SL band structure from the discrete all the way to the continuum regime and build a comprehensive picture of the Lieb lattice under 1D potentials. This approach allows us to also take into consideration the discrete lattice symmetry-breaking that occurs at the well/barrier interfaces created by the 1D SL, whose consequences cannot be explored using the previous low energy and long wavelength approaches. We find novel features in the band structure, among which are intersections of quadratic and flat bands, tilted Dirac cones, or series of additional anisotropic Dirac cones at energies where the SKT is predicted. Such features are relevant to experimental realizations of electronic transport in Lieb 1D SL realized in artificial lattices or in real material systems like 2D covalent organic/metal-organic frameworks and inorganic 2D solids.","sentences":["The Lieb lattice is one of the simplest lattices that exhibits both linear Dirac-like and flat topological electronic bands.","We propose to further tailor its electronic properties through periodic 1D electrostatic superlattices (SLs), which, in the long wavelength limit, were predicted to give rise to novel transport signatures, such as the omnidirectional super-Klein tunnelling (SKT).","By numerically modelling the electronic structure at tight-binding level, we uncover the evolution of the Lieb SL band structure from the discrete all the way to the continuum regime and build a comprehensive picture of the Lieb lattice under 1D potentials.","This approach allows us to also take into consideration the discrete lattice symmetry-breaking that occurs at the well/barrier interfaces created by the 1D SL, whose consequences cannot be explored using the previous low energy and long wavelength approaches.","We find novel features in the band structure, among which are intersections of quadratic and flat bands, tilted Dirac cones, or series of additional anisotropic Dirac cones at energies where the SKT is predicted.","Such features are relevant to experimental realizations of electronic transport in Lieb 1D SL realized in artificial lattices or in real material systems like 2D covalent organic/metal-organic frameworks and inorganic 2D solids."],"url":"http://arxiv.org/abs/2403.10382v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 15:10:40","title":"BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics","abstract":"Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to diagnose environmental health and biodiversity. However, inconsistencies in research pose notable challenges hindering progress in this domain. Reliable DL models need to analyze bird calls flexibly across various species and environments to fully harness the potential of bioacoustics in a cost-effective passive acoustic monitoring scenario. Data fragmentation and opacity across studies complicate a comprehensive evaluation of general model performance. To overcome these challenges, we present the BirdSet benchmark, a unified framework consolidating research efforts with a holistic approach for classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes open-source bird recordings into a curated dataset collection. This unified approach provides an in-depth understanding of model performance and identifies potential shortcomings across different tasks. By establishing baseline results of current models, BirdSet aims to facilitate comparability, guide subsequent data collection, and increase accessibility for newcomers to avian bioacoustics.","sentences":["Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to diagnose environmental health and biodiversity.","However, inconsistencies in research pose notable challenges hindering progress in this domain.","Reliable DL models need to analyze bird calls flexibly across various species and environments to fully harness the potential of bioacoustics in a cost-effective passive acoustic monitoring scenario.","Data fragmentation and opacity across studies complicate a comprehensive evaluation of general model performance.","To overcome these challenges, we present the BirdSet benchmark, a unified framework consolidating research efforts with a holistic approach for classifying bird vocalizations in avian bioacoustics.","BirdSet harmonizes open-source bird recordings into a curated dataset collection.","This unified approach provides an in-depth understanding of model performance and identifies potential shortcomings across different tasks.","By establishing baseline results of current models, BirdSet aims to facilitate comparability, guide subsequent data collection, and increase accessibility for newcomers to avian bioacoustics."],"url":"http://arxiv.org/abs/2403.10380v1","category":"cs.SD"}
{"created":"2024-03-15 15:07:18","title":"Tilt in quadratic gravity","abstract":"In this work, non perfect fluid, tilted source solutions in both Einstein-Hilbert General Relativity (GR) and Quadratic Gravity (QG) for the anisotropic Bianchi V model are addressed. Since the excellent CMBR match of Starobinsky's inflation with Planck's team measurements data, QG has acquired a prominent status in the effective sense, for sufficiently strong gravity fields. The main interest is in the numeric time evolution to the past towards the singularity and the behavior of the kinematic variables, vorticity, acceleration, and the expansion of this source substance. In QG we found that for universes with higher and smaller matter densities fall into the Kasner or isotropic singularity attractors to the past, respectively. We also found that the Kasner singularity attractor to the past has always zero vorticity, for both GR and QG theories. While for QG the isotropic singularity attractor may have divergent vorticity. For the set of assumptions and conditions supposed in this work, the isotropic singularity attractor, favors QG as compared to GR. Only in QG we were able to find a geometric singularity with divergences in all of the kinematic variables of the substance, decreasing to finite values to the future, upon time reversing. That is, we obtained an initial kinematic singularity substance, that approaches a perfect fluid source.","sentences":["In this work, non perfect fluid, tilted source solutions in both Einstein-Hilbert General Relativity (GR) and Quadratic Gravity (QG) for the anisotropic Bianchi V model are addressed.","Since the excellent CMBR match of Starobinsky's inflation with Planck's team measurements data, QG has acquired a prominent status in the effective sense, for sufficiently strong gravity fields.","The main interest is in the numeric time evolution to the past towards the singularity and the behavior of the kinematic variables, vorticity, acceleration, and the expansion of this source substance.","In QG we found that for universes with higher and smaller matter densities fall into the Kasner or isotropic singularity attractors to the past, respectively.","We also found that the Kasner singularity attractor to the past has always zero vorticity, for both GR and QG theories.","While for QG the isotropic singularity attractor may have divergent vorticity.","For the set of assumptions and conditions supposed in this work, the isotropic singularity attractor, favors QG as compared to GR.","Only in QG we were able to find a geometric singularity with divergences in all of the kinematic variables of the substance, decreasing to finite values to the future, upon time reversing.","That is, we obtained an initial kinematic singularity substance, that approaches a perfect fluid source."],"url":"http://arxiv.org/abs/2403.10377v1","category":"gr-qc"}
{"created":"2024-03-15 15:04:30","title":"Overcoming Distribution Shifts in Plug-and-Play Methods with Test-Time Training","abstract":"Plug-and-Play Priors (PnP) is a well-known class of methods for solving inverse problems in computational imaging. PnP methods combine physical forward models with learned prior models specified as image denoisers. A common issue with the learned models is that of a performance drop when there is a distribution shift between the training and testing data. Test-time training (TTT) was recently proposed as a general strategy for improving the performance of learned models when training and testing data come from different distributions. In this paper, we propose PnP-TTT as a new method for overcoming distribution shifts in PnP. PnP-TTT uses deep equilibrium learning (DEQ) for optimizing a self-supervised loss at the fixed points of PnP iterations. PnP-TTT can be directly applied on a single test sample to improve the generalization of PnP. We show through simulations that given a sufficient number of measurements, PnP-TTT enables the use of image priors trained on natural images for image reconstruction in magnetic resonance imaging (MRI).","sentences":["Plug-and-Play Priors (PnP) is a well-known class of methods for solving inverse problems in computational imaging.","PnP methods combine physical forward models with learned prior models specified as image denoisers.","A common issue with the learned models is that of a performance drop when there is a distribution shift between the training and testing data.","Test-time training (TTT) was recently proposed as a general strategy for improving the performance of learned models when training and testing data come from different distributions.","In this paper, we propose PnP-TTT as a new method for overcoming distribution shifts in PnP. PnP-TTT uses deep equilibrium learning (DEQ) for optimizing a self-supervised loss at the fixed points of PnP iterations.","PnP-TTT can be directly applied on a single test sample to improve the generalization of PnP. We show through simulations that given a sufficient number of measurements, PnP-TTT enables the use of image priors trained on natural images for image reconstruction in magnetic resonance imaging (MRI)."],"url":"http://arxiv.org/abs/2403.10374v1","category":"eess.IV"}
{"created":"2024-03-15 15:04:20","title":"Towards a general framework for improving the performance of classifiers using XAI methods","abstract":"Modern Artificial Intelligence (AI) systems, especially Deep Learning (DL) models, poses challenges in understanding their inner workings by AI researchers. eXplainable Artificial Intelligence (XAI) inspects internal mechanisms of AI models providing explanations about their decisions. While current XAI research predominantly concentrates on explaining AI systems, there is a growing interest in using XAI techniques to automatically improve the performance of AI systems themselves. This paper proposes a general framework for automatically improving the performance of pre-trained DL classifiers using XAI methods, avoiding the computational overhead associated with retraining complex models from scratch. In particular, we outline the possibility of two different learning strategies for implementing this architecture, which we will call auto-encoder-based and encoder-decoder-based, and discuss their key aspects.","sentences":["Modern Artificial Intelligence (AI) systems, especially Deep Learning (DL) models, poses challenges in understanding their inner workings by AI researchers.","eXplainable Artificial Intelligence (XAI) inspects internal mechanisms of AI models providing explanations about their decisions.","While current XAI research predominantly concentrates on explaining AI systems, there is a growing interest in using XAI techniques to automatically improve the performance of AI systems themselves.","This paper proposes a general framework for automatically improving the performance of pre-trained DL classifiers using XAI methods, avoiding the computational overhead associated with retraining complex models from scratch.","In particular, we outline the possibility of two different learning strategies for implementing this architecture, which we will call auto-encoder-based and encoder-decoder-based, and discuss their key aspects."],"url":"http://arxiv.org/abs/2403.10373v1","category":"cs.LG"}
{"created":"2024-03-15 15:03:02","title":"Construction of all MDS and involutory MDS matrices","abstract":"In this paper, we propose two algorithms for a hybrid construction of all $n\\times n$ MDS and involutory MDS matrices over a finite field $\\mathbb{F}_{p^m}$, respectively. The proposed algorithms effectively narrow down the search space to identify $(n-1) \\times (n-1)$ MDS matrices, facilitating the generation of all $n \\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. To the best of our knowledge, existing literature lacks methods for generating all $n\\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. In our approach, we introduce a representative matrix form for generating all $n\\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. The determination of these representative MDS matrices involves searching through all $(n-1)\\times (n-1)$ MDS matrices over $\\mathbb{F}_{p^m}$. Our contributions extend to proving that the count of all $3\\times 3$ MDS matrices over $\\mathbb{F}_{2^m}$ is precisely $(2^m-1)^5(2^m-2)(2^m-3)(2^{2m}-9\\cdot 2^m+21)$. Furthermore, we explicitly provide the count of all $4\\times 4$ MDS and involutory MDS matrices over $\\mathbb{F}_{2^m}$ for $m=2, 3, 4$.","sentences":["In this paper, we propose two algorithms for a hybrid construction of all $n\\times n$ MDS and involutory MDS matrices over a finite field $\\mathbb{F}_{p^m}$, respectively.","The proposed algorithms effectively narrow down the search space to identify $(n-1)","\\times (n-1)$ MDS matrices, facilitating the generation of all $n \\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. To the best of our knowledge, existing literature lacks methods for generating all $n\\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. In our approach, we introduce a representative matrix form for generating all $n\\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. The determination of these representative MDS matrices involves searching through all $(n-1)\\times (n-1)$ MDS matrices over $\\mathbb{F}_{p^m}$. Our contributions extend to proving that the count of all $3\\times 3$ MDS matrices over $\\mathbb{F}_{2^m}$ is precisely $(2^m-1)^5(2^m-2)(2^m-3)(2^{2m}-9\\cdot 2^m+21)$.","Furthermore, we explicitly provide the count of all $4\\times 4$ MDS and involutory MDS matrices over $\\mathbb{F}_{2^m}$ for $m=2, 3, 4$."],"url":"http://arxiv.org/abs/2403.10372v1","category":"cs.CR"}
{"created":"2024-03-15 15:01:48","title":"An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness in IoT Applications","abstract":"Machine Learning (ML) is becoming increasingly important for IoT-based applications. However, the dynamic and ad-hoc nature of many IoT ecosystems poses unique challenges to the efficacy of ML algorithms. One such challenge is data incompleteness, which is manifested as missing sensor readings. Many factors, including sensor failures and/or network disruption, can cause data incompleteness. Furthermore, most IoT systems are severely power-constrained. It is important that we build IoT-based ML systems that are robust against data incompleteness while simultaneously being energy efficient. This paper presents an empirical study of SECOE - a recent technique for alleviating data incompleteness in IoT - with respect to its energy bottlenecks. Towards addressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive, energy-aware technique for mitigating the impact of concurrent missing data. ENAMLE is unique in the sense that it builds an energy-aware ensemble of sub-models, each trained with a subset of sensors chosen carefully based on their correlations. Furthermore, at inference time, ENAMLE adaptively alters the number of the ensemble of models based on the amount of missing data rate and the energy-accuracy trade-off. ENAMLE's design includes several novel mechanisms for minimizing energy consumption while maintaining accuracy. We present extensive experimental studies on two distinct datasets that demonstrate the energy efficiency of ENAMLE and its ability to alleviate sensor failures.","sentences":["Machine Learning (ML) is becoming increasingly important for IoT-based applications.","However, the dynamic and ad-hoc nature of many IoT ecosystems poses unique challenges to the efficacy of ML algorithms.","One such challenge is data incompleteness, which is manifested as missing sensor readings.","Many factors, including sensor failures and/or network disruption, can cause data incompleteness.","Furthermore, most IoT systems are severely power-constrained.","It is important that we build IoT-based ML systems that are robust against data incompleteness while simultaneously being energy efficient.","This paper presents an empirical study of SECOE - a recent technique for alleviating data incompleteness in IoT - with respect to its energy bottlenecks.","Towards addressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive, energy-aware technique for mitigating the impact of concurrent missing data.","ENAMLE is unique in the sense that it builds an energy-aware ensemble of sub-models, each trained with a subset of sensors chosen carefully based on their correlations.","Furthermore, at inference time, ENAMLE adaptively alters the number of the ensemble of models based on the amount of missing data rate and the energy-accuracy trade-off.","ENAMLE's design includes several novel mechanisms for minimizing energy consumption while maintaining accuracy.","We present extensive experimental studies on two distinct datasets that demonstrate the energy efficiency of ENAMLE and its ability to alleviate sensor failures."],"url":"http://arxiv.org/abs/2403.10371v1","category":"cs.LG"}
{"created":"2024-03-15 15:00:42","title":"Open Stamped Parts Dataset","abstract":"We present the Open Stamped Parts Dataset (OSPD), featuring synthetic and real images of stamped metal sheets for auto manufacturing. The real part images, captured from 7 cameras, consist of 7,980 unlabeled images and 1,680 labeled images. In addition, we have compiled a defect dataset by overlaying synthetically generated masks on 10% of the holes. The synthetic dataset replicates the real manufacturing environment in terms of lighting and part placement relative to the cameras. The synthetic data includes 7,980 training images, 1,680 validation images and 1,680 test images, each with bounding box and segmentation mask annotations around all holes. 10% of the holes in the synthetic data mimic defects generated in the real image dataset. We trained a hole-detection model on the synthetic-OSPD, achieving a modified recall score of 67.2% and a precision of 94.4% . We anticipate researchers in the auto manufacturing and broader machine learning and computer vision communities using OSPD to advance the state of the art in defect detection of stamped holes in the metalsheet stamping process. The dataset is available for download at: https://tinyurl.com/hm6xatd7","sentences":["We present the Open Stamped Parts Dataset (OSPD), featuring synthetic and real images of stamped metal sheets for auto manufacturing.","The real part images, captured from 7 cameras, consist of 7,980 unlabeled images and 1,680 labeled images.","In addition, we have compiled a defect dataset by overlaying synthetically generated masks on 10% of the holes.","The synthetic dataset replicates the real manufacturing environment in terms of lighting and part placement relative to the cameras.","The synthetic data includes 7,980 training images, 1,680 validation images and 1,680 test images, each with bounding box and segmentation mask annotations around all holes.","10% of the holes in the synthetic data mimic defects generated in the real image dataset.","We trained a hole-detection model on the synthetic-OSPD, achieving a modified recall score of 67.2% and a precision of 94.4% .","We anticipate researchers in the auto manufacturing and broader machine learning and computer vision communities using OSPD to advance the state of the art in defect detection of stamped holes in the metalsheet stamping process.","The dataset is available for download at: https://tinyurl.com/hm6xatd7"],"url":"http://arxiv.org/abs/2403.10369v1","category":"cs.CV"}
{"created":"2024-03-15 14:59:24","title":"Conformal Predictions for Probabilistically Robust Scalable Machine Learning Classification","abstract":"Conformal predictions make it possible to define reliable and robust learning algorithms. But they are essentially a method for evaluating whether an algorithm is good enough to be used in practice. To define a reliable learning framework for classification from the very beginning of its design, the concept of scalable classifier was introduced to generalize the concept of classical classifier by linking it to statistical order theory and probabilistic learning theory. In this paper, we analyze the similarities between scalable classifiers and conformal predictions by introducing a new definition of a score function and defining a special set of input variables, the conformal safety set, which can identify patterns in the input space that satisfy the error coverage guarantee, i.e., that the probability of observing the wrong (possibly unsafe) label for points belonging to this set is bounded by a predefined $\\varepsilon$ error level. We demonstrate the practical implications of this framework through an application in cybersecurity for identifying DNS tunneling attacks. Our work contributes to the development of probabilistically robust and reliable machine learning models.","sentences":["Conformal predictions make it possible to define reliable and robust learning algorithms.","But they are essentially a method for evaluating whether an algorithm is good enough to be used in practice.","To define a reliable learning framework for classification from the very beginning of its design, the concept of scalable classifier was introduced to generalize the concept of classical classifier by linking it to statistical order theory and probabilistic learning theory.","In this paper, we analyze the similarities between scalable classifiers and conformal predictions by introducing a new definition of a score function and defining a special set of input variables, the conformal safety set, which can identify patterns in the input space that satisfy the error coverage guarantee, i.e., that the probability of observing the wrong (possibly unsafe) label for points belonging to this set is bounded by a predefined $\\varepsilon$ error level.","We demonstrate the practical implications of this framework through an application in cybersecurity for identifying DNS tunneling attacks.","Our work contributes to the development of probabilistically robust and reliable machine learning models."],"url":"http://arxiv.org/abs/2403.10368v1","category":"stat.ML"}
{"created":"2024-03-15 14:58:31","title":"A Graded Schur Lemma and a graded-monoidal structure for induced modules over graded-commutative algebras","abstract":"We consider algebras and Frobenius algebras, internal to a monoidal category, that are graded over a finite abelian group. For the case that A is a twisted group algebra in a linear abelian monoidal category we obtain a graded generalization of the Schur Lemma for the category of induced A-modules. We further show that if the monoidal category is braided and A is commutative up to a bicharacter of the grading group, then the category of induced A-modules can be endowed with a graded-monoidal structure that is twisted by the bicharacter. In the particular case that the grading group is Z/2Z, these findings reproduce known results about superalgebras and super-monoidal structures.","sentences":["We consider algebras and Frobenius algebras, internal to a monoidal category, that are graded over a finite abelian group.","For the case that A is a twisted group algebra in a linear abelian monoidal category we obtain a graded generalization of the Schur Lemma for the category of induced A-modules.","We further show that if the monoidal category is braided and A is commutative up to a bicharacter of the grading group, then the category of induced A-modules can be endowed with a graded-monoidal structure that is twisted by the bicharacter.","In the particular case that the grading group is Z/2Z, these findings reproduce known results about superalgebras and super-monoidal structures."],"url":"http://arxiv.org/abs/2403.10366v1","category":"math.QA"}
{"created":"2024-03-15 14:58:27","title":"Scalable Algorithms for Individual Preference Stable Clustering","abstract":"In this paper, we study the individual preference (IP) stability, which is an notion capturing individual fairness and stability in clustering. Within this setting, a clustering is $\\alpha$-IP stable when each data point's average distance to its cluster is no more than $\\alpha$ times its average distance to any other cluster. In this paper, we study the natural local search algorithm for IP stable clustering. Our analysis confirms a $O(\\log n)$-IP stability guarantee for this algorithm, where $n$ denotes the number of points in the input. Furthermore, by refining the local search approach, we show it runs in an almost linear time, $\\tilde{O}(nk)$.","sentences":["In this paper, we study the individual preference (IP) stability, which is an notion capturing individual fairness and stability in clustering.","Within this setting, a clustering is $\\alpha$-IP stable when each data point's average distance to its cluster is no more than $\\alpha$ times its average distance to any other cluster.","In this paper, we study the natural local search algorithm for IP stable clustering.","Our analysis confirms a $O(\\log n)$-IP stability guarantee for this algorithm, where $n$ denotes the number of points in the input.","Furthermore, by refining the local search approach, we show it runs in an almost linear time, $\\tilde{O}(nk)$."],"url":"http://arxiv.org/abs/2403.10365v1","category":"cs.DS"}
{"created":"2024-03-15 14:55:37","title":"Scattering amplitude in QCD: summing large Pomeron loops","abstract":"In this paper we show that the sum of enhanced BFKL Pomeron loop diagrams generates the scattering amplitude, which turns out to be much smaller, than in the case of deep inelastic scattering. We use the simplified BFKL kernel in the leading twist approximation, which reproduces the main features of the scattering amplitude in the deep inelastic scattering(DIS). For such kernel the results are highly unexpected and they contradict (i) the solution to the Balitsky- Kovchegov(BK) equation for the scattering amplitude; (ii) the idea that the scattering amplitude stems from rare fluctuation and it has the same form as in DIS ; and (iii) the numerical simulations. We sincerely hope, that we made a mistake, which we failed to note, and which our reader will find. If not , we need to reconsider our view on the sum of the BFKL Pomeron loops and accept that their summing will lead to large contribution of the rare configurations in CGC approach to the scattering amplitude.","sentences":["In this paper we show that the sum of enhanced BFKL Pomeron loop diagrams generates the scattering amplitude, which turns out to be much smaller, than in the case of deep inelastic scattering.","We use the simplified BFKL kernel in the leading twist approximation, which reproduces the main features of the scattering amplitude in the deep inelastic scattering(DIS).","For such kernel the results are highly unexpected and they contradict (i) the solution to the Balitsky- Kovchegov(BK) equation for the scattering amplitude; (ii) the idea that the scattering amplitude stems from rare fluctuation and it has the same form as in DIS ; and (iii) the numerical simulations.","We sincerely hope, that we made a mistake, which we failed to note, and which our reader will find.","If not , we need to reconsider our view on the sum of the BFKL Pomeron loops and accept that their summing will lead to large contribution of the rare configurations in CGC approach to the scattering amplitude."],"url":"http://arxiv.org/abs/2403.10364v1","category":"hep-ph"}
{"created":"2024-03-15 14:49:13","title":"Eigenstate Thermalization Hypothesis for Wigner-type Matrices","abstract":"We prove the Eigenstate Thermalization Hypothesis for general Wigner-type matrices in the bulk of the self-consistent spectrum, with optimal control on the fluctuations for observables of arbitrary rank. As the main technical ingredient, we prove rank-uniform optimal local laws for one and two resolvents of a Wigner-type matrix with regular observables.","sentences":["We prove the Eigenstate Thermalization Hypothesis for general Wigner-type matrices in the bulk of the self-consistent spectrum, with optimal control on the fluctuations for observables of arbitrary rank.","As the main technical ingredient, we prove rank-uniform optimal local laws for one and two resolvents of a Wigner-type matrix with regular observables."],"url":"http://arxiv.org/abs/2403.10359v1","category":"math.PR"}
{"created":"2024-03-15 14:45:38","title":"ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image","abstract":"Recent progress in human shape learning, shows that neural implicit models are effective in generating 3D human surfaces from limited number of views, and even from a single RGB image. However, existing monocular approaches still struggle to recover fine geometric details such as face, hands or cloth wrinkles. They are also easily prone to depth ambiguities that result in distorted geometries along the camera optical axis. In this paper, we explore the benefits of incorporating depth observations in the reconstruction process by introducing ANIM, a novel method that reconstructs arbitrary 3D human shapes from single-view RGB-D images with an unprecedented level of accuracy. Our model learns geometric details from both multi-resolution pixel-aligned and voxel-aligned features to leverage depth information and enable spatial relationships, mitigating depth ambiguities. We further enhance the quality of the reconstructed shape by introducing a depth-supervision strategy, which improves the accuracy of the signed distance field estimation of points that lie on the reconstructed surface. Experiments demonstrate that ANIM outperforms state-of-the-art works that use RGB, surface normals, point cloud or RGB-D data as input. In addition, we introduce ANIM-Real, a new multi-modal dataset comprising high-quality scans paired with consumer-grade RGB-D camera, and our protocol to fine-tune ANIM, enabling high-quality reconstruction from real-world human capture.","sentences":["Recent progress in human shape learning, shows that neural implicit models are effective in generating 3D human surfaces from limited number of views, and even from a single RGB image.","However, existing monocular approaches still struggle to recover fine geometric details such as face, hands or cloth wrinkles.","They are also easily prone to depth ambiguities that result in distorted geometries along the camera optical axis.","In this paper, we explore the benefits of incorporating depth observations in the reconstruction process by introducing ANIM, a novel method that reconstructs arbitrary 3D human shapes from single-view RGB-D images with an unprecedented level of accuracy.","Our model learns geometric details from both multi-resolution pixel-aligned and voxel-aligned features to leverage depth information and enable spatial relationships, mitigating depth ambiguities.","We further enhance the quality of the reconstructed shape by introducing a depth-supervision strategy, which improves the accuracy of the signed distance field estimation of points that lie on the reconstructed surface.","Experiments demonstrate that ANIM outperforms state-of-the-art works that use RGB, surface normals, point cloud or RGB-D data as input.","In addition, we introduce ANIM-Real, a new multi-modal dataset comprising high-quality scans paired with consumer-grade RGB-D camera, and our protocol to fine-tune ANIM, enabling high-quality reconstruction from real-world human capture."],"url":"http://arxiv.org/abs/2403.10357v1","category":"cs.CV"}
{"created":"2024-03-15 14:41:43","title":"Optimising finite-time photon extraction from emitter-cavity systems","abstract":"We develop methods to find the limits to finite-time single photon extraction from emitter-cavity systems. We first establish analytic upper and lower bounds on the maximum extraction probability from a canonical $\\Lambda$-system before developing a numeric method to optimise generic output probabilities from $\\Lambda$-systems generalised to multiple ground states. We use these methods to study the limits to finite-time photon extraction and the wavepackets that satisfy them, finding that using an optimised wavepacket ranging between a sinusoidal and exponentially decaying profile can considerably reduce photon duration for a given extraction efficiency. We further optimise the rates of quantum protocols requiring emitter-photon correlation to obtain driving-independent conclusions about the effect of system parameters on success probability. We believe that these results and methods will provide valuable tools and insights for the development of cavity-based single photon sources combining high efficiency and high rate.","sentences":["We develop methods to find the limits to finite-time single photon extraction from emitter-cavity systems.","We first establish analytic upper and lower bounds on the maximum extraction probability from a canonical $\\Lambda$-system before developing a numeric method to optimise generic output probabilities from $\\Lambda$-systems generalised to multiple ground states.","We use these methods to study the limits to finite-time photon extraction and the wavepackets that satisfy them, finding that using an optimised wavepacket ranging between a sinusoidal and exponentially decaying profile can considerably reduce photon duration for a given extraction efficiency.","We further optimise the rates of quantum protocols requiring emitter-photon correlation to obtain driving-independent conclusions about the effect of system parameters on success probability.","We believe that these results and methods will provide valuable tools and insights for the development of cavity-based single photon sources combining high efficiency and high rate."],"url":"http://arxiv.org/abs/2403.10355v1","category":"quant-ph"}
{"created":"2024-03-15 14:34:34","title":"Denoising Task Difficulty-based Curriculum for Training Diffusion Models","abstract":"Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learning, to enhance the training process of diffusion models. By organizing timesteps or noise levels into clusters and training models with descending orders of difficulty, we facilitate an order-aware training regime, progressing from easier to harder denoising tasks, thereby deviating from the conventional approach of training diffusion models simultaneously across all timesteps. Our approach leads to improved performance and faster convergence by leveraging the benefits of curriculum learning, while maintaining orthogonality with existing improvements in diffusion training techniques. We validate these advantages through comprehensive experiments in image generation tasks, including unconditional, class-conditional, and text-to-image generation.","sentences":["Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling.","Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks.","While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult.","To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps.","Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps.","Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learning, to enhance the training process of diffusion models.","By organizing timesteps or noise levels into clusters and training models with descending orders of difficulty, we facilitate an order-aware training regime, progressing from easier to harder denoising tasks, thereby deviating from the conventional approach of training diffusion models simultaneously across all timesteps.","Our approach leads to improved performance and faster convergence by leveraging the benefits of curriculum learning, while maintaining orthogonality with existing improvements in diffusion training techniques.","We validate these advantages through comprehensive experiments in image generation tasks, including unconditional, class-conditional, and text-to-image generation."],"url":"http://arxiv.org/abs/2403.10348v1","category":"cs.CV"}
{"created":"2024-03-15 14:31:35","title":"End-to-end Adaptive Dynamic Subsampling and Reconstruction for Cardiac MRI","abstract":"Accelerating dynamic MRI is essential for enhancing clinical applications, such as adaptive radiotherapy, and improving patient comfort. Traditional deep learning (DL) approaches for accelerated dynamic MRI reconstruction typically rely on predefined or random subsampling patterns, applied uniformly across all temporal phases. This standard practice overlooks the potential benefits of leveraging temporal correlations and lacks the adaptability required for case-specific subsampling optimization, which holds the potential for maximizing reconstruction quality. Addressing this gap, we present a novel end-to-end framework for adaptive dynamic MRI subsampling and reconstruction. Our pipeline integrates a DL-based adaptive sampler, generating case-specific dynamic subsampling patterns, trained end-to-end with a state-of-the-art 2D dynamic reconstruction network, namely vSHARP, which effectively reconstructs the adaptive dynamic subsampled data into a moving image. Our method is assessed using dynamic cine cardiac MRI data, comparing its performance against vSHARP models that employ common subsampling trajectories, and pipelines trained to optimize dataset-specific sampling schemes alongside vSHARP reconstruction. Our results indicate superior reconstruction quality, particularly at high accelerations.","sentences":["Accelerating dynamic MRI is essential for enhancing clinical applications, such as adaptive radiotherapy, and improving patient comfort.","Traditional deep learning (DL) approaches for accelerated dynamic MRI reconstruction typically rely on predefined or random subsampling patterns, applied uniformly across all temporal phases.","This standard practice overlooks the potential benefits of leveraging temporal correlations and lacks the adaptability required for case-specific subsampling optimization, which holds the potential for maximizing reconstruction quality.","Addressing this gap, we present a novel end-to-end framework for adaptive dynamic MRI subsampling and reconstruction.","Our pipeline integrates a DL-based adaptive sampler, generating case-specific dynamic subsampling patterns, trained end-to-end with a state-of-the-art 2D dynamic reconstruction network, namely vSHARP, which effectively reconstructs the adaptive dynamic subsampled data into a moving image.","Our method is assessed using dynamic cine cardiac MRI data, comparing its performance against vSHARP models that employ common subsampling trajectories, and pipelines trained to optimize dataset-specific sampling schemes alongside vSHARP reconstruction.","Our results indicate superior reconstruction quality, particularly at high accelerations."],"url":"http://arxiv.org/abs/2403.10346v1","category":"eess.IV"}
{"created":"2024-03-15 14:26:53","title":"Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection","abstract":"Graph-based anomaly detection is currently an important research topic in the field of graph neural networks (GNNs). We find that in graph anomaly detection, the homophily distribution differences between different classes are significantly greater than those in homophilic and heterophilic graphs. For the first time, we introduce a new metric called Class Homophily Variance, which quantitatively describes this phenomenon. To mitigate its impact, we propose a novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe). Previous works typically focused on pruning, selecting or connecting on original relationships, and we refer to these methods as modifications. Different from these works, our method emphasizes generating new relationships with low class homophily variance, using the original relationships as an auxiliary. HedGe samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph. Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HedGe achieved the best performance across multiple benchmark datasets, including anomaly detection and edgeless node classification. The proposed model also improves the robustness under the novel Heterophily Attack with increased class homophily variance on other graph classification tasks.","sentences":["Graph-based anomaly detection is currently an important research topic in the field of graph neural networks (GNNs).","We find that in graph anomaly detection, the homophily distribution differences between different classes are significantly greater than those in homophilic and heterophilic graphs.","For the first time, we introduce a new metric called Class Homophily Variance, which quantitatively describes this phenomenon.","To mitigate its impact, we propose a novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe).","Previous works typically focused on pruning, selecting or connecting on original relationships, and we refer to these methods as modifications.","Different from these works, our method emphasizes generating new relationships with low class homophily variance, using the original relationships as an auxiliary.","HedGe samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph.","Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model.","Extensive comparison experiments demonstrate that HedGe achieved the best performance across multiple benchmark datasets, including anomaly detection and edgeless node classification.","The proposed model also improves the robustness under the novel Heterophily Attack with increased class homophily variance on other graph classification tasks."],"url":"http://arxiv.org/abs/2403.10339v1","category":"cs.LG"}
{"created":"2024-03-15 14:19:58","title":"A Conjecture for ATP Research","abstract":"This note generalizes factorization for formulas with multiplicities and conjectures that the connection method along with this feature is computationally as powerful as resolution, also seen from a complexity point of view.","sentences":["This note generalizes factorization for formulas with multiplicities and conjectures that the connection method along with this feature is computationally as powerful as resolution, also seen from a complexity point of view."],"url":"http://arxiv.org/abs/2403.10334v1","category":"cs.LO"}
{"created":"2024-03-15 14:19:09","title":"GreedyML: A Parallel Algorithm for Maximizing Submodular Functions","abstract":"We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data summarization, machine learning, and graph sparsification. Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck.   Here, we propose a generalization of the RandGreedI algorithm that employs multiple accumulation steps to reduce the memory required. We analyze the approximation ratio and the time complexity of the algorithm (in the BSP model). We also evaluate the new GreedyML algorithm on three classes of problems, and report results from massive data sets with millions of elements. The results show that the GreedyML algorithm can solve problems where the sequential Greedy and distributed RandGreedI algorithms fail due to memory constraints. For certain computationally intensive problems, the GreedyML algorithm can be faster than the RandGreedI algorithm. The observed approximation quality of the solutions computed by the GreedyML algorithm closely matches those obtained by the RandGreedI algorithm on these problems.","sentences":["We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors.","Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data summarization, machine learning, and graph sparsification.","Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015).","This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor.","However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck.   ","Here, we propose a generalization of the RandGreedI algorithm that employs multiple accumulation steps to reduce the memory required.","We analyze the approximation ratio and the time complexity of the algorithm (in the BSP model).","We also evaluate the new GreedyML algorithm on three classes of problems, and report results from massive data sets with millions of elements.","The results show that the GreedyML algorithm can solve problems where the sequential Greedy and distributed RandGreedI algorithms fail due to memory constraints.","For certain computationally intensive problems, the GreedyML algorithm can be faster than the RandGreedI algorithm.","The observed approximation quality of the solutions computed by the GreedyML algorithm closely matches those obtained by the RandGreedI algorithm on these problems."],"url":"http://arxiv.org/abs/2403.10332v1","category":"cs.DC"}
{"created":"2024-03-15 14:18:44","title":"Bias Control and Linearization of the Transfer Function of Electro-optic and Acousto-optic Modulators","abstract":"In several types of quantum computers light is one of the main tools to control both the position and the quantum state of the atoms used for computing. In practical systems laser light is applied to manipulate quantum states of qubits in the desired way. Beside physical effects like decoherence and quantum noise the precision of qubit manipulation has a significant impact on the achievable quantum computing error rate. One of the key optical components beside the laser is the optical modulator, which modulates or switches a constant power laser light in order to provide light pulses or pulse sequences with a desired envelope. Acousto-optic (AOM) and electro-optic (EOM) modulators can be applied, which are both voltage controlled. However, there is neither a simple linear relationship between their control signal and the precise modulator output, nor can they be considered to have time-invariant characteristics. The aim of this paper is to describe techniques to generate AOM and EOM control signals in such a way that almost arbitrary target output waveforms (i. e. optical power versus time) are achieved with high accuracy.","sentences":["In several types of quantum computers light is one of the main tools to control both the position and the quantum state of the atoms used for computing.","In practical systems laser light is applied to manipulate quantum states of qubits in the desired way.","Beside physical effects like decoherence and quantum noise the precision of qubit manipulation has a significant impact on the achievable quantum computing error rate.","One of the key optical components beside the laser is the optical modulator, which modulates or switches a constant power laser light in order to provide light pulses or pulse sequences with a desired envelope.","Acousto-optic (AOM) and electro-optic (EOM) modulators can be applied, which are both voltage controlled.","However, there is neither a simple linear relationship between their control signal and the precise modulator output, nor can they be considered to have time-invariant characteristics.","The aim of this paper is to describe techniques to generate AOM and EOM control signals in such a way that almost arbitrary target output waveforms (i. e. optical power versus time) are achieved with high accuracy."],"url":"http://arxiv.org/abs/2403.10331v1","category":"eess.SP"}
{"created":"2024-03-15 14:18:21","title":"Towards Non-Adversarial Algorithmic Recourse","abstract":"The streams of research on adversarial examples and counterfactual explanations have largely been growing independently. This has led to several recent works trying to elucidate their similarities and differences. Most prominently, it has been argued that adversarial examples, as opposed to counterfactual explanations, have a unique characteristic in that they lead to a misclassification compared to the ground truth. However, the computational goals and methodologies employed in existing counterfactual explanation and adversarial example generation methods often lack alignment with this requirement. Using formal definitions of adversarial examples and counterfactual explanations, we introduce non-adversarial algorithmic recourse and outline why in high-stakes situations, it is imperative to obtain counterfactual explanations that do not exhibit adversarial characteristics. We subsequently investigate how different components in the objective functions, e.g., the machine learning model or cost function used to measure distance, determine whether the outcome can be considered an adversarial example or not. Our experiments on common datasets highlight that these design choices are often more critical in deciding whether recourse is non-adversarial than whether recourse or attack algorithms are used. Furthermore, we show that choosing a robust and accurate machine learning model results in less adversarial recourse desired in practice.","sentences":["The streams of research on adversarial examples and counterfactual explanations have largely been growing independently.","This has led to several recent works trying to elucidate their similarities and differences.","Most prominently, it has been argued that adversarial examples, as opposed to counterfactual explanations, have a unique characteristic in that they lead to a misclassification compared to the ground truth.","However, the computational goals and methodologies employed in existing counterfactual explanation and adversarial example generation methods often lack alignment with this requirement.","Using formal definitions of adversarial examples and counterfactual explanations, we introduce non-adversarial algorithmic recourse and outline why in high-stakes situations, it is imperative to obtain counterfactual explanations that do not exhibit adversarial characteristics.","We subsequently investigate how different components in the objective functions, e.g., the machine learning model or cost function used to measure distance, determine whether the outcome can be considered an adversarial example or not.","Our experiments on common datasets highlight that these design choices are often more critical in deciding whether recourse is non-adversarial than whether recourse or attack algorithms are used.","Furthermore, we show that choosing a robust and accurate machine learning model results in less adversarial recourse desired in practice."],"url":"http://arxiv.org/abs/2403.10330v1","category":"cs.LG"}
{"created":"2024-03-15 14:16:10","title":"Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time (CBoTT)","abstract":"Threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. It can be performed in several ways, one of which is based on detecting anomalies. We propose an unsupervised framework, called continuous bag-of-terms-and-time (CBoTT), and publish its application programming interface (API) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among SIEM logs geared toward process auditing on endpoint devices. Analyses show that our framework consistently outperforms benchmark approaches. When logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between 1.82-6.46) while benchmark approaches identify the same anomalies at lower percentiles (between 3.25-80.92). This framework can be used by other researchers to conduct benchmark analyses and cybersecurity analysts to find anomalies in SIEM logs.","sentences":["Threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures.","It can be performed in several ways, one of which is based on detecting anomalies.","We propose an unsupervised framework, called continuous bag-of-terms-and-time (CBoTT), and publish its application programming interface (API) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among SIEM logs geared toward process auditing on endpoint devices.","Analyses show that our framework consistently outperforms benchmark approaches.","When logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between 1.82-6.46) while benchmark approaches identify the same anomalies at lower percentiles (between 3.25-80.92).","This framework can be used by other researchers to conduct benchmark analyses and cybersecurity analysts to find anomalies in SIEM logs."],"url":"http://arxiv.org/abs/2403.10327v1","category":"cs.CR"}
{"created":"2024-03-15 14:14:26","title":"CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model","abstract":"Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.","sentences":["Manually designing cloze test consumes enormous time and efforts.","The major challenge lies in wrong option (distractor) selection.","Having carefully-design distractors improves the effectiveness of learner ability assessment.","As a result, the idea of automatically generating cloze distractor is motivated.","In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation.","Experiments show that the PLM-enhanced model brings a substantial performance improvement.","Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score).","Our code and dataset is available at https://github.com/AndyChiangSH/CDGP."],"url":"http://arxiv.org/abs/2403.10326v1","category":"cs.CL"}
{"created":"2024-03-15 14:13:49","title":"Data-driven cold starting of good reservoirs","abstract":"Using short histories of observations from a dynamical system, a workflow for the post-training initialization of reservoir computing systems is described. This strategy is called cold-starting, and it is based on a map called the starting map, which is determined by an appropriately short history of observations that maps to a unique initial condition in the reservoir space. The time series generated by the reservoir system using that initial state can be used to run the system in autonomous mode, to produce accurate forecasts of the time series under consideration immediately. By utilizing this map, the lengthy \"washouts\" that are necessary to initialize reservoir systems can be eliminated, enabling the generation of forecasts using any selection of appropriately short histories of the observations.","sentences":["Using short histories of observations from a dynamical system, a workflow for the post-training initialization of reservoir computing systems is described.","This strategy is called cold-starting, and it is based on a map called the starting map, which is determined by an appropriately short history of observations that maps to a unique initial condition in the reservoir space.","The time series generated by the reservoir system using that initial state can be used to run the system in autonomous mode, to produce accurate forecasts of the time series under consideration immediately.","By utilizing this map, the lengthy \"washouts\" that are necessary to initialize reservoir systems can be eliminated, enabling the generation of forecasts using any selection of appropriately short histories of the observations."],"url":"http://arxiv.org/abs/2403.10325v1","category":"math.DS"}
{"created":"2024-03-15 14:13:31","title":"Joint Optimization for Achieving Covertness in MIMO Over-the-Air Computation Networks","abstract":"This paper investigates covert data transmission within a multiple-input multiple-output (MIMO) over-the-air computation (AirComp) network, where sensors transmit data to the access point (AP) while guaranteeing covertness to the warden (Willie). Simultaneously, the AP introduces artificial noise (AN) to confuse Willie, meeting the covert requirement. We address the challenge of minimizing mean-square-error (MSE) of the AP, while considering transmit power constraints at both the AP and the sensors, as well as ensuring the covert transmission to Willie with a low detection error probability (DEP). However, obtaining globally optimal solutions for the investigated non-convex problem is challenging due to the interdependence of optimization variables. To tackle this problem, we introduce an exact penalty algorithm and transform the optimization problem into a difference-of-convex (DC) form problem to find a locally optimal solution. Simulation results showcase the superior performance in terms of our proposed scheme in comparison to the benchmark schemes.","sentences":["This paper investigates covert data transmission within a multiple-input multiple-output (MIMO) over-the-air computation (AirComp) network, where sensors transmit data to the access point (AP) while guaranteeing covertness to the warden (Willie).","Simultaneously, the AP introduces artificial noise (AN) to confuse Willie, meeting the covert requirement.","We address the challenge of minimizing mean-square-error (MSE) of the AP, while considering transmit power constraints at both the AP and the sensors, as well as ensuring the covert transmission to Willie with a low detection error probability (DEP).","However, obtaining globally optimal solutions for the investigated non-convex problem is challenging due to the interdependence of optimization variables.","To tackle this problem, we introduce an exact penalty algorithm and transform the optimization problem into a difference-of-convex (DC) form problem to find a locally optimal solution.","Simulation results showcase the superior performance in terms of our proposed scheme in comparison to the benchmark schemes."],"url":"http://arxiv.org/abs/2403.10323v1","category":"eess.SP"}
{"created":"2024-03-15 14:11:21","title":"Asteroid reflectance spectra from Gaia DR3: Near-UV in primitive asteroids","abstract":"In the context of charge-coupled devices (CCDs), the ultraviolet (UV) region has mostly remained unexplored after the 1990s. Gaia DR3 offers the community a unique opportunity to explore tens of thousands of asteroids in the near-UV as a proxy of the UV absorption. This absorption has been proposed in previous works as a diagnostic of hydration, organics, and space weathering. Aims. In this work, we aim to explore the potential of the NUV as a diagnostic region for primitive asteroids using Gaia DR3. We used a corrective factor over the blue part of Gaia spectra to erase the solar analog selection effect. We identified an artificial relation between the band noise and slope and applied a signal-to-noise ratio (S/N) threshold for Gaia bands. Meeting the quality standards, we employed a Markov chain Monte Carlo (MCMC) algorithm to compute the albedo threshold, maximizing primitive asteroid inclusion. Utilizing one- and two-dimensional (1D and 2D) projections, along with dimensionality-reduction methods (such as PCA and UMAP), we identified primitive asteroid populations. We uncovered: (a) the first observational evidence linking UV absorption to the 0.7 {\\mu}m band, tied to hydrated iron-rich phyllosilicates; and (b) a 2D space revealing a split in C-type asteroids based on spectral features, including UV absorption. The computed average depth (3.5 +- 1.0 %) and center (0.70 +- 0.03 {\\mu}m) of the 0.7 {\\mu}m absorption band for primitive asteroids observed with Gaia is in agreement with the literature values. In this paper, we shed light on the importance of the UV absorption feature to discriminate among different mineralogies (i.e., iron-rich phyllosilicates vs. iron-poor) or to identify taxonomies that are conflated in the visible (i.e., F-types vs. B-types). We have shown that this is a promising region for diagnostic studies of the composition of primitive asteroids.","sentences":["In the context of charge-coupled devices (CCDs), the ultraviolet (UV) region has mostly remained unexplored after the 1990s.","Gaia DR3 offers the community a unique opportunity to explore tens of thousands of asteroids in the near-UV as a proxy of the UV absorption.","This absorption has been proposed in previous works as a diagnostic of hydration, organics, and space weathering.","Aims.","In this work, we aim to explore the potential of the NUV as a diagnostic region for primitive asteroids using Gaia DR3.","We used a corrective factor over the blue part of Gaia spectra to erase the solar analog selection effect.","We identified an artificial relation between the band noise and slope and applied a signal-to-noise ratio (S/N) threshold for Gaia bands.","Meeting the quality standards, we employed a Markov chain Monte Carlo (MCMC) algorithm to compute the albedo threshold, maximizing primitive asteroid inclusion.","Utilizing one-","and two-dimensional (1D and 2D) projections, along with dimensionality-reduction methods (such as PCA and UMAP), we identified primitive asteroid populations.","We uncovered: (a) the first observational evidence linking UV absorption to the 0.7 {\\mu}m band, tied to hydrated iron-rich phyllosilicates; and (b) a 2D space revealing a split in C-type asteroids based on spectral features, including UV absorption.","The computed average depth (3.5 +- 1.0 %) and center (0.70 +- 0.03 {\\mu}m) of the 0.7 {\\mu}m absorption band for primitive asteroids observed with Gaia is in agreement with the literature values.","In this paper, we shed light on the importance of the UV absorption feature to discriminate among different mineralogies (i.e., iron-rich phyllosilicates vs. iron-poor) or to identify taxonomies that are conflated in the visible (i.e., F-types vs. B-types).","We have shown that this is a promising region for diagnostic studies of the composition of primitive asteroids."],"url":"http://arxiv.org/abs/2403.10321v1","category":"astro-ph.EP"}
{"created":"2024-03-15 14:09:54","title":"NetBench: A Large-Scale and Comprehensive Network Traffic Benchmark Dataset for Foundation Models","abstract":"In computer networking, network traffic refers to the amount of data transmitted in the form of packets between internetworked computers or systems. Monitoring and analyzing network traffic is crucial for ensuring the performance, security, and reliability of a network. However, a significant challenge in network traffic analysis is to process diverse data packets including both ciphertext and plaintext. While many methods have been adopted to analyze network traffic, they often rely on different datasets for performance evaluation. This inconsistency results in substantial manual data processing efforts and unfair comparisons. Moreover, some data processing methods may cause data leakage due to improper separation of training and test data. To address these issues, we introduce NetBench, a large-scale and comprehensive benchmark dataset for assessing machine learning models, especially foundation models, in both traffic classification and generation tasks. NetBench is built upon seven publicly available datasets and encompasses a broad spectrum of 20 tasks, including 15 classification tasks and 5 generation tasks. Furthermore, we evaluate eight State-Of-The-Art (SOTA) classification models and two generative models using our benchmark. The results show that foundation models significantly outperform the traditional deep learning methods in traffic classification. We believe NetBench will facilitate fair comparisons among various approaches and advance the development of foundation models for network traffic. Our benchmark is available at https://github.com/WM-JayLab/NetBench.","sentences":["In computer networking, network traffic refers to the amount of data transmitted in the form of packets between internetworked computers or systems.","Monitoring and analyzing network traffic is crucial for ensuring the performance, security, and reliability of a network.","However, a significant challenge in network traffic analysis is to process diverse data packets including both ciphertext and plaintext.","While many methods have been adopted to analyze network traffic, they often rely on different datasets for performance evaluation.","This inconsistency results in substantial manual data processing efforts and unfair comparisons.","Moreover, some data processing methods may cause data leakage due to improper separation of training and test data.","To address these issues, we introduce NetBench, a large-scale and comprehensive benchmark dataset for assessing machine learning models, especially foundation models, in both traffic classification and generation tasks.","NetBench is built upon seven publicly available datasets and encompasses a broad spectrum of 20 tasks, including 15 classification tasks and 5 generation tasks.","Furthermore, we evaluate eight State-Of-The-Art (SOTA) classification models and two generative models using our benchmark.","The results show that foundation models significantly outperform the traditional deep learning methods in traffic classification.","We believe NetBench will facilitate fair comparisons among various approaches and advance the development of foundation models for network traffic.","Our benchmark is available at https://github.com/WM-JayLab/NetBench."],"url":"http://arxiv.org/abs/2403.10319v1","category":"cs.NI"}
{"created":"2024-03-15 14:07:46","title":"Application of machine learning to experimental design in quantum mechanics","abstract":"The recent advances in machine learning hold great promise for the fields of quantum sensing and metrology. With the help of reinforcement learning, we can tame the complexity of quantum systems and solve the problem of optimal experimental design. Reinforcement learning is a powerful model-free technique that allows an agent, typically a neural network, to learn the best strategy to reach a certain goal in a completely a priori unknown environment. However, in general, we know something about the quantum system with which the agent is interacting, at least that it follows the rules of quantum mechanics. In quantum metrology, we typically have a model for the system, and only some parameters of the evolution or the initial state are unknown. We present here a general machine learning technique that can optimize the precision of quantum sensors, exploiting the knowledge we have on the system through model-aware reinforcement learning. This framework has been implemented in the Python package qsensoropt, which is able to optimize a broad class of problems found in quantum metrology and quantum parameter estimation. The agent learns an optimal adaptive strategy that, based on previous outcomes, decides the next measurements to perform. We have explored some applications of this technique to NV centers and photonic circuits. So far, we have been able to certify better results than the current state-of-the-art controls for many cases. The machine learning technique developed here can be applied in all scenarios where the quantum system is well-characterized and relatively simple and small. In these cases, we can extract every last bit of information from a quantum sensor by appropriately controlling it with a trained neural network. The qsensoropt software is available on PyPI and can be installed with pip.","sentences":["The recent advances in machine learning hold great promise for the fields of quantum sensing and metrology.","With the help of reinforcement learning, we can tame the complexity of quantum systems and solve the problem of optimal experimental design.","Reinforcement learning is a powerful model-free technique that allows an agent, typically a neural network, to learn the best strategy to reach a certain goal in a completely a priori unknown environment.","However, in general, we know something about the quantum system with which the agent is interacting, at least that it follows the rules of quantum mechanics.","In quantum metrology, we typically have a model for the system, and only some parameters of the evolution or the initial state are unknown.","We present here a general machine learning technique that can optimize the precision of quantum sensors, exploiting the knowledge we have on the system through model-aware reinforcement learning.","This framework has been implemented in the Python package qsensoropt, which is able to optimize a broad class of problems found in quantum metrology and quantum parameter estimation.","The agent learns an optimal adaptive strategy that, based on previous outcomes, decides the next measurements to perform.","We have explored some applications of this technique to NV centers and photonic circuits.","So far, we have been able to certify better results than the current state-of-the-art controls for many cases.","The machine learning technique developed here can be applied in all scenarios where the quantum system is well-characterized and relatively simple and small.","In these cases, we can extract every last bit of information from a quantum sensor by appropriately controlling it with a trained neural network.","The qsensoropt software is available on PyPI and can be installed with pip."],"url":"http://arxiv.org/abs/2403.10317v1","category":"quant-ph"}
{"created":"2024-03-15 13:59:55","title":"Hunter's positivity theorem and random vector norms","abstract":"A theorem of Hunter ensures that the complete homogeneous symmetric polynomials of even degree are positive definite functions. A probabilistic interpretation of Hunter's theorem suggests a broad generalization: the construction of so-called random vector norms on square complex matrices. This paper surveys these ideas, starting from the fundamental notions and developing the theory to its present state. We study numerous examples and present a host of open problems.","sentences":["A theorem of Hunter ensures that the complete homogeneous symmetric polynomials of even degree are positive definite functions.","A probabilistic interpretation of Hunter's theorem suggests a broad generalization: the construction of so-called random vector norms on square complex matrices.","This paper surveys these ideas, starting from the fundamental notions and developing the theory to its present state.","We study numerous examples and present a host of open problems."],"url":"http://arxiv.org/abs/2403.10314v1","category":"math.FA"}
{"created":"2024-03-15 13:53:50","title":"Revolutionizing Packaging: A Robotic Bagging Pipeline with Constraint-aware Structure-of-Interest Planning","abstract":"Bagging operations, common in packaging and assisted living applications, are challenging due to a bag's complex deformable properties. To address this, we develop a robotic system for automated bagging tasks using an adaptive structure-of-interest (SOI) manipulation approach. Our method relies on real-time visual feedback to dynamically adjust manipulation without requiring prior knowledge of bag materials or dynamics. We present a robust pipeline featuring state estimation for SOIs using Gaussian Mixture Models (GMM), SOI generation via optimization-based bagging techniques, SOI motion planning with Constrained Bidirectional Rapidly-exploring Random Trees (CBiRRT), and dual-arm manipulation coordinated by Model Predictive Control (MPC). Experiments demonstrate the system's ability to achieve precise, stable bagging of various objects using adaptive coordination of the manipulators. The proposed framework advances the capability of dual-arm robots to perform more sophisticated automation of common tasks involving interactions with deformable objects.","sentences":["Bagging operations, common in packaging and assisted living applications, are challenging due to a bag's complex deformable properties.","To address this, we develop a robotic system for automated bagging tasks using an adaptive structure-of-interest (SOI) manipulation approach.","Our method relies on real-time visual feedback to dynamically adjust manipulation without requiring prior knowledge of bag materials or dynamics.","We present a robust pipeline featuring state estimation for SOIs using Gaussian Mixture Models (GMM), SOI generation via optimization-based bagging techniques, SOI motion planning with Constrained Bidirectional Rapidly-exploring Random Trees (CBiRRT), and dual-arm manipulation coordinated by Model Predictive Control (MPC).","Experiments demonstrate the system's ability to achieve precise, stable bagging of various objects using adaptive coordination of the manipulators.","The proposed framework advances the capability of dual-arm robots to perform more sophisticated automation of common tasks involving interactions with deformable objects."],"url":"http://arxiv.org/abs/2403.10309v1","category":"cs.RO"}
{"created":"2024-03-15 13:48:30","title":"Supplement Matrix and a Practical Method for Computing Eigenvalues of a Dual Hermitian Matrix","abstract":"We study dual number symmetric matrices, dual complex Hermitian matrices and dual quaternion Hermitian matrices in a unified frame of dual Hermitian matrices. Suppose we have a ring, which can be the real field, the complex field, or the quaternion ring. Then an $n \\times n$ dual Hermitian matrix has $n$ dual number eigenvalues.   We define supplement matrices for a dual Hermitian matrix. Supplement matrices are Hermitian matrices in the original ring. The standard parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of the standard part Hermitian matrix in the original ring, while the dual parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of those {supplement} matrices. Hence, by apply any practical method for computing eigenvalues of Hermitian matrices in the original ring, we have a practical method for computing eigenvalues of a dual Hermitian matrix. We call this method the supplement matrix method.   Applications to low rank approximation and generalized inverses of dual matrices, dual least squares problem and formation control are discussed.   Numerical experiments are reported.","sentences":["We study dual number symmetric matrices, dual complex Hermitian matrices and dual quaternion Hermitian matrices in a unified frame of dual Hermitian matrices.","Suppose we have a ring, which can be the real field, the complex field, or the quaternion ring.","Then an $n \\times n$ dual Hermitian matrix has $n$ dual number eigenvalues.   ","We define supplement matrices for a dual Hermitian matrix.","Supplement matrices are Hermitian matrices in the original ring.","The standard parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of the standard part Hermitian matrix in the original ring, while the dual parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of those {supplement} matrices.","Hence, by apply any practical method for computing eigenvalues of Hermitian matrices in the original ring, we have a practical method for computing eigenvalues of a dual Hermitian matrix.","We call this method the supplement matrix method.   ","Applications to low rank approximation and generalized inverses of dual matrices, dual least squares problem and formation control are discussed.   ","Numerical experiments are reported."],"url":"http://arxiv.org/abs/2403.10308v1","category":"math.NA"}
{"created":"2024-03-15 13:47:15","title":"A General Non-Strict Finsler's Lemma","abstract":"In this paper, we present a general non-strict Finsler's lemma. This result is general in the sense that it does not impose any restrictions on the involved matrices and, thereby, is more broadly applicable than existing non-strict versions of Finsler's lemma that do impose such restrictions. In fact, we show that this new non-strict formulation generalizes both the original strict Finsler's lemma as well as an existing non-strict version. To further illustrate its usefulness, we showcase applications of the non-strict Finsler's lemma in deriving a closed-form solution to the non-strict projection lemma, and a matrix Finsler's lemma, which is useful for data-driven control.","sentences":["In this paper, we present a general non-strict Finsler's lemma.","This result is general in the sense that it does not impose any restrictions on the involved matrices and, thereby, is more broadly applicable than existing non-strict versions of Finsler's lemma that do impose such restrictions.","In fact, we show that this new non-strict formulation generalizes both the original strict Finsler's lemma as well as an existing non-strict version.","To further illustrate its usefulness, we showcase applications of the non-strict Finsler's lemma in deriving a closed-form solution to the non-strict projection lemma, and a matrix Finsler's lemma, which is useful for data-driven control."],"url":"http://arxiv.org/abs/2403.10306v1","category":"math.OC"}
{"created":"2024-03-15 13:46:36","title":"KIF: A Framework for Virtual Integration of Heterogeneous Knowledge Bases using Wikidata","abstract":"We present a knowledge integration framework (called KIF) that uses Wikidata as a lingua franca to integrate heterogeneous knowledge bases. These can be triplestores, relational databases, CSV files, etc., which may or may not use the Wikidata dialect of RDF. KIF leverages Wikidata's data model and vocabulary plus user-defined mappings to expose a unified view of the integrated bases while keeping track of the context and provenance of their statements. The result is a virtual knowledge base which behaves like an \"extended Wikidata\" and which can be queried either through an efficient filter interface or using SPARQL. We present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF.","sentences":["We present a knowledge integration framework (called KIF) that uses Wikidata as a lingua franca to integrate heterogeneous knowledge bases.","These can be triplestores, relational databases, CSV files, etc., which may or may not use the Wikidata dialect of RDF.","KIF leverages Wikidata's data model and vocabulary plus user-defined mappings to expose a unified view of the integrated bases while keeping track of the context and provenance of their statements.","The result is a virtual knowledge base which behaves like an \"extended Wikidata\" and which can be queried either through an efficient filter interface or using SPARQL.","We present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF."],"url":"http://arxiv.org/abs/2403.10304v1","category":"cs.AI"}
{"created":"2024-03-15 13:44:04","title":"Probabilistic Models of Profiles for Voting by Evaluation","abstract":"Considering voting rules based on evaluation inputs rather than preference rankings modifies the paradigm of probabilistic studies of voting procedures. This article proposes several simulation models for generating evaluation-based voting inputs. These models can cope with dependent and non identical marginal distributions of the evaluations received by the candidates. A last part is devoted to fitting these models to real data sets.","sentences":["Considering voting rules based on evaluation inputs rather than preference rankings modifies the paradigm of probabilistic studies of voting procedures.","This article proposes several simulation models for generating evaluation-based voting inputs.","These models can cope with dependent and non identical marginal distributions of the evaluations received by the candidates.","A last part is devoted to fitting these models to real data sets."],"url":"http://arxiv.org/abs/2403.10302v1","category":"stat.AP"}
{"created":"2024-03-15 13:42:10","title":"The reliability of the gender Implicit Association Test (gIAT) for high-ability careers","abstract":"Males outnumber females in many high ability careers, for example, in the fields of science, technology, engineering, and mathematics and professors in academic medicine. These differences are often attributed to implicit, subconscious, bias. One objective of this study was to use statistical p value plots to independently test the ability to support the claim of implicit bias made in a meta analysis of gender bias studies.   The meta analysis examined correlations between implicit bias measures based on the gender Implicit Association Test, g IAT, and measures of intergroup, female and male, behavior. A second objective was to investigate general intelligence g and vocational, things people, interests as explanatory factors for gender differences in high ability careers.   The p value plots constructed using data sets from the meta analysis did not support real associations between the tested variables. These findings reinforce the lack of correlation between g IAT, implicit bias, measures and real world gender behaviors in high ability careers.   More demanding careers, attorneys, engineers, scientists, corporate executives, are recruited from people with higher g. One is dealing with gender groups and the group of high g females is smaller than high g males. Regarding vocational interests, females prefer working with people and males prefer working with things. STEM fields are typically things oriented. One is dealing with gender groups and the group of females who prefer working with things is smaller than the group of males.   These facts make it predictable that there are more males in high complexity, things careers, STEM, academic medicine positions, than females. Implicit bias gIAT measures have little or no explanatory power for gender differences in high ability careers relative to g and interests in working with things.","sentences":["Males outnumber females in many high ability careers, for example, in the fields of science, technology, engineering, and mathematics and professors in academic medicine.","These differences are often attributed to implicit, subconscious, bias.","One objective of this study was to use statistical p value plots to independently test the ability to support the claim of implicit bias made in a meta analysis of gender bias studies.   ","The meta analysis examined correlations between implicit bias measures based on the gender Implicit Association Test, g IAT, and measures of intergroup, female and male, behavior.","A second objective was to investigate general intelligence g and vocational, things people, interests as explanatory factors for gender differences in high ability careers.   ","The p value plots constructed using data sets from the meta analysis did not support real associations between the tested variables.","These findings reinforce the lack of correlation between g IAT, implicit bias, measures and real world gender behaviors in high ability careers.   ","More demanding careers, attorneys, engineers, scientists, corporate executives, are recruited from people with higher g.","One is dealing with gender groups and the group of high g females is smaller than high g males.","Regarding vocational interests, females prefer working with people and males prefer working with things.","STEM fields are typically things oriented.","One is dealing with gender groups and the group of females who prefer working with things is smaller than the group of males.   ","These facts make it predictable that there are more males in high complexity, things careers, STEM, academic medicine positions, than females.","Implicit bias gIAT measures have little or no explanatory power for gender differences in high ability careers relative to g and interests in working with things."],"url":"http://arxiv.org/abs/2403.10300v1","category":"stat.AP"}
{"created":"2024-03-15 13:42:00","title":"A Multi-constraint and Multi-objective Allocation Model for Emergency Rescue in IoT Environment","abstract":"Emergency relief operations are essential in disaster aftermaths, necessitating effective resource allocation to minimize negative impacts and maximize benefits. In prolonged crises or extensive disasters, a systematic, multi-cycle approach is key for timely and informed decision-making. Leveraging advancements in IoT and spatio-temporal data analytics, we've developed the Multi-Objective Shuffled Gray-Wolf Frog Leaping Model (MSGW-FLM). This multi-constraint, multi-objective resource allocation model has been rigorously tested against 28 diverse challenges, showing superior performance in comparison to established models such as NSGA-II, IBEA, and MOEA/D. MSGW-FLM's effectiveness is particularly notable in complex, multi-cycle emergency rescue scenarios, which involve numerous constraints and objectives. This model represents a significant step forward in optimizing resource distribution in emergency response situations.","sentences":["Emergency relief operations are essential in disaster aftermaths, necessitating effective resource allocation to minimize negative impacts and maximize benefits.","In prolonged crises or extensive disasters, a systematic, multi-cycle approach is key for timely and informed decision-making.","Leveraging advancements in IoT and spatio-temporal data analytics, we've developed the Multi-Objective Shuffled Gray-Wolf Frog Leaping Model (MSGW-FLM).","This multi-constraint, multi-objective resource allocation model has been rigorously tested against 28 diverse challenges, showing superior performance in comparison to established models such as NSGA-II, IBEA, and MOEA/D. MSGW-FLM's effectiveness is particularly notable in complex, multi-cycle emergency rescue scenarios, which involve numerous constraints and objectives.","This model represents a significant step forward in optimizing resource distribution in emergency response situations."],"url":"http://arxiv.org/abs/2403.10299v1","category":"cs.AI"}
{"created":"2024-03-15 13:40:44","title":"Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization","abstract":"Exploring and mining subtle yet distinctive features between sub-categories with similar appearances is crucial for fine-grained visual categorization (FGVC). However, less effort has been devoted to assessing the quality of extracted visual representations. Intuitively, the network may struggle to capture discriminative features from low-quality samples, which leads to a significant decline in FGVC performance. To tackle this challenge, we propose a weakly supervised Context-Semantic Quality Awareness Network (CSQA-Net) for FGVC. In this network, to model the spatial contextual relationship between rich part descriptors and global semantics for capturing more discriminative details within the object, we design a novel multi-part and multi-scale cross-attention (MPMSCA) module. Before feeding to the MPMSCA module, the part navigator is developed to address the scale confusion problems and accurately identify the local distinctive regions. Furthermore, we propose a generic multi-level semantic quality evaluation module (MLSQE) to progressively supervise and enhance hierarchical semantics from different levels of the backbone network. Finally, context-aware features from MPMSCA and semantically enhanced features from MLSQE are fed into the corresponding quality probing classifiers to evaluate their quality in real-time, thus boosting the discriminability of feature representations. Comprehensive experiments on four popular and highly competitive FGVC datasets demonstrate the superiority of the proposed CSQA-Net in comparison with the state-of-the-art methods.","sentences":["Exploring and mining subtle yet distinctive features between sub-categories with similar appearances is crucial for fine-grained visual categorization (FGVC).","However, less effort has been devoted to assessing the quality of extracted visual representations.","Intuitively, the network may struggle to capture discriminative features from low-quality samples, which leads to a significant decline in FGVC performance.","To tackle this challenge, we propose a weakly supervised Context-Semantic Quality Awareness Network (CSQA-Net) for FGVC.","In this network, to model the spatial contextual relationship between rich part descriptors and global semantics for capturing more discriminative details within the object, we design a novel multi-part and multi-scale cross-attention (MPMSCA) module.","Before feeding to the MPMSCA module, the part navigator is developed to address the scale confusion problems and accurately identify the local distinctive regions.","Furthermore, we propose a generic multi-level semantic quality evaluation module (MLSQE) to progressively supervise and enhance hierarchical semantics from different levels of the backbone network.","Finally, context-aware features from MPMSCA and semantically enhanced features from MLSQE are fed into the corresponding quality probing classifiers to evaluate their quality in real-time, thus boosting the discriminability of feature representations.","Comprehensive experiments on four popular and highly competitive FGVC datasets demonstrate the superiority of the proposed CSQA-Net in comparison with the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.10298v1","category":"cs.CV"}
{"created":"2024-03-15 13:40:37","title":"Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression","abstract":"Classical structural-based visual localization methods offer high accuracy but face trade-offs in terms of storage, speed, and privacy. A recent innovation, keypoint scene coordinate regression (KSCR) named D2S addresses these issues by leveraging graph attention networks to enhance keypoint relationships and predict their 3D coordinates using a simple multilayer perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using established 2D-3D correspondences. While KSCR achieves competitive results, rivaling state-of-the-art image-retrieval methods like HLoc across multiple benchmarks, its performance is hindered when data samples are limited due to the deep learning model's reliance on extensive data. This paper proposes a solution to this challenge by introducing a pipeline for keypoint descriptor synthesis using Neural Radiance Field (NeRF). By generating novel poses and feeding them into a trained NeRF model to create new views, our approach enhances the KSCR's generalization capabilities in data-scarce environments. The proposed system could significantly improve localization accuracy by up to 50\\% and cost only a fraction of time for data synthesis. Furthermore, its modular design allows for the integration of multiple NeRFs, offering a versatile and efficient solution for visual localization. The implementation is publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.","sentences":["Classical structural-based visual localization methods offer high accuracy but face trade-offs in terms of storage, speed, and privacy.","A recent innovation, keypoint scene coordinate regression (KSCR) named D2S addresses these issues by leveraging graph attention networks to enhance keypoint relationships and predict their 3D coordinates using a simple multilayer perceptron (MLP).","Camera pose is then determined via PnP+RANSAC, using established 2D-3D correspondences.","While KSCR achieves competitive results, rivaling state-of-the-art image-retrieval methods like HLoc across multiple benchmarks, its performance is hindered when data samples are limited due to the deep learning model's reliance on extensive data.","This paper proposes a solution to this challenge by introducing a pipeline for keypoint descriptor synthesis using Neural Radiance Field (NeRF).","By generating novel poses and feeding them into a trained NeRF model to create new views, our approach enhances the KSCR's generalization capabilities in data-scarce environments.","The proposed system could significantly improve localization accuracy by up to 50\\% and cost only a fraction of time for data synthesis.","Furthermore, its modular design allows for the integration of multiple NeRFs, offering a versatile and efficient solution for visual localization.","The implementation is publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map."],"url":"http://arxiv.org/abs/2403.10297v1","category":"cs.CV"}
{"created":"2024-03-15 13:33:19","title":"Experimental demonstration of improved reference-frame-independent quantum key distribution over 175km","abstract":"Reference-frame-independent (RFI) quantum key distribution (QKD) presents promising advantages, especially for mobile-platform-based implementations, as it eliminates the need for active reference frame calibration. While RFI-QKD has been explored in various studies, limitations in key rate and distance persist due to finite data collection. In this study, we experimentally demonstrate an improved RFI-QKD protocol proposed by Zhu \\textit{et al.} [Opt. Lett. 47, 4219 (2022)], featuring a statistical quantity for bounding information leaked to Eve that exhibits more insensitivity to statistical fluctuations and more robustness to variations in the reference frame. Taking into account finite-size considerations and potential general attacks, RFI-QKD is implemented over a distance of 175 \\si{\\kilo\\meter} in this work. We believe that our study extends the communication distance achievable by RFI-QKD, thereby constituting a notable advancement for its practical application.","sentences":["Reference-frame-independent (RFI) quantum key distribution (QKD) presents promising advantages, especially for mobile-platform-based implementations, as it eliminates the need for active reference frame calibration.","While RFI-QKD has been explored in various studies, limitations in key rate and distance persist due to finite data collection.","In this study, we experimentally demonstrate an improved RFI-QKD protocol proposed by Zhu \\textit{et al.}","[Opt.","Lett.","47, 4219 (2022)], featuring a statistical quantity for bounding information leaked to Eve that exhibits more insensitivity to statistical fluctuations and more robustness to variations in the reference frame.","Taking into account finite-size considerations and potential general attacks, RFI-QKD is implemented over a distance of 175 \\si{\\kilo\\meter} in this work.","We believe that our study extends the communication distance achievable by RFI-QKD, thereby constituting a notable advancement for its practical application."],"url":"http://arxiv.org/abs/2403.10294v1","category":"quant-ph"}
{"created":"2024-03-15 13:31:33","title":"Deep Learning for Multi-Level Detection and Localization of Myocardial Scars Based on Regional Strain Validated on Virtual Patients","abstract":"How well the heart is functioning can be quantified through measurements of myocardial deformation via echocardiography. Clinical assessment of cardiac function is generally focused on global indices of relative shortening, however, territorial, and segmental strain indices have shown to be abnormal in regions of myocardial disease, such as scar. In this work, we propose a single framework to predict myocardial disease substrates at global, territorial, and segmental levels using regional myocardial strain traces as input to a convolutional neural network (CNN)-based classification algorithm. An anatomically meaningful representation of the input data from the clinically standard bullseye representation to a multi-channel 2D image is proposed, to formulate the task as an image classification problem, thus enabling the use of state-of-the-art neural network configurations. A Fully Convolutional Network (FCN) is trained to detect and localize myocardial scar from regional left ventricular (LV) strain patterns. Simulated regional strain data from a controlled dataset of virtual patients with varying degrees and locations of myocardial scar is used for training and validation. The proposed method successfully detects and localizes the scars on 98% of the 5490 left ventricle (LV) segments of the 305 patients in the test set using strain traces only. Due to the sparse existence of scar, only 10% of the LV segments in the virtual patient cohort have scar. Taking the imbalance into account, the class balanced accuracy is calculated as 95%. The performance is reported on global, territorial, and segmental levels. The proposed method proves successful on the strain traces of the virtual cohort and offers the potential to solve the regional myocardial scar detection problem on the strain traces of the real patient cohorts.","sentences":["How well the heart is functioning can be quantified through measurements of myocardial deformation via echocardiography.","Clinical assessment of cardiac function is generally focused on global indices of relative shortening, however, territorial, and segmental strain indices have shown to be abnormal in regions of myocardial disease, such as scar.","In this work, we propose a single framework to predict myocardial disease substrates at global, territorial, and segmental levels using regional myocardial strain traces as input to a convolutional neural network (CNN)-based classification algorithm.","An anatomically meaningful representation of the input data from the clinically standard bullseye representation to a multi-channel 2D image is proposed, to formulate the task as an image classification problem, thus enabling the use of state-of-the-art neural network configurations.","A Fully Convolutional Network (FCN) is trained to detect and localize myocardial scar from regional left ventricular (LV) strain patterns.","Simulated regional strain data from a controlled dataset of virtual patients with varying degrees and locations of myocardial scar is used for training and validation.","The proposed method successfully detects and localizes the scars on 98% of the 5490 left ventricle (LV) segments of the 305 patients in the test set using strain traces only.","Due to the sparse existence of scar, only 10% of the LV segments in the virtual patient cohort have scar.","Taking the imbalance into account, the class balanced accuracy is calculated as 95%.","The performance is reported on global, territorial, and segmental levels.","The proposed method proves successful on the strain traces of the virtual cohort and offers the potential to solve the regional myocardial scar detection problem on the strain traces of the real patient cohorts."],"url":"http://arxiv.org/abs/2403.10291v1","category":"cs.CV"}
{"created":"2024-03-15 13:31:27","title":"Offline Goal-Conditioned Reinforcement Learning for Shape Control of Deformable Linear Objects","abstract":"Deformable objects present several challenges to the field of robotic manipulation. One of the tasks that best encapsulates the difficulties arising due to non-rigid behavior is shape control, which requires driving an object to a desired shape. While shape-servoing methods have been shown successful in contexts with approximately linear behavior, they can fail in tasks with more complex dynamics. We investigate an alternative approach, using offline RL to solve a planar shape control problem of a Deformable Linear Object (DLO). To evaluate the effect of material properties, two DLOs are tested namely a soft rope and an elastic cord. We frame this task as a goal-conditioned offline RL problem, and aim to learn to generalize to unseen goal shapes. Data collection and augmentation procedures are proposed to limit the amount of experimental data which needs to be collected with the real robot. We evaluate the amount of augmentation needed to achieve the best results, and test the effect of regularization through behavior cloning on the TD3+BC algorithm. Finally, we show that the proposed approach is able to outperform a shape-servoing baseline in a curvature inversion experiment.","sentences":["Deformable objects present several challenges to the field of robotic manipulation.","One of the tasks that best encapsulates the difficulties arising due to non-rigid behavior is shape control, which requires driving an object to a desired shape.","While shape-servoing methods have been shown successful in contexts with approximately linear behavior, they can fail in tasks with more complex dynamics.","We investigate an alternative approach, using offline RL to solve a planar shape control problem of a Deformable Linear Object (DLO).","To evaluate the effect of material properties, two DLOs are tested namely a soft rope and an elastic cord.","We frame this task as a goal-conditioned offline RL problem, and aim to learn to generalize to unseen goal shapes.","Data collection and augmentation procedures are proposed to limit the amount of experimental data which needs to be collected with the real robot.","We evaluate the amount of augmentation needed to achieve the best results, and test the effect of regularization through behavior cloning on the TD3+BC algorithm.","Finally, we show that the proposed approach is able to outperform a shape-servoing baseline in a curvature inversion experiment."],"url":"http://arxiv.org/abs/2403.10290v1","category":"cs.RO"}
{"created":"2024-03-15 13:29:45","title":"Rough Transformers for Continuous and Efficient Time-Series Modelling","abstract":"Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global dependencies in input data, while remaining robust to changes in the sequence length and sampling frequency. We find that Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the benefits of Neural ODE-based models using a fraction of the computational time and memory resources on synthetic and real-world time-series tasks.","sentences":["Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals.","In such contexts, traditional sequence-based recurrent models struggle.","To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies.","Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater.","To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts.","In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global dependencies in input data, while remaining robust to changes in the sequence length and sampling frequency.","We find that Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the benefits of Neural ODE-based models using a fraction of the computational time and memory resources on synthetic and real-world time-series tasks."],"url":"http://arxiv.org/abs/2403.10288v1","category":"stat.ML"}
{"created":"2024-03-15 13:27:33","title":"RACH-less Handover with Early Timing Advance Acquisition for Outage Reduction","abstract":"For fifth-generation (5G) and 5G-Advanced networks, outage reduction within the context of reliability is a key objective since outage denotes the time period when a user equipment (UE) cannot communicate with the network. Earlier studies have shown that in the experimental high mobility scenario considered, outage is dominated by the interruption time that stems from the random access channel (RACH)-based handover process from the serving cell to the target cell. A handover by itself is a necessary mobility process to prevent mobility failures and their associated outage. This paper proposes a RACH-less handover signaling scheme for the 3rd Generation Partnership Project (3GPP) conditional handover (CHO) mechanism. The proposed scheme exploits the decoupling between the CHO preparation and execution phases to establish initial synchronization between the UE and the target cell through an early acquisition of the timing advance. This significantly curtails the RACH process and therefore the handover interruption time. Results based on a system-level simulation-based mobility study have shown that the proposed scheme significantly reduces the outage and its constituent handover interruption time relatively by 18.7% and 43.2%, respectively.","sentences":["For fifth-generation (5G) and 5G-Advanced networks, outage reduction within the context of reliability is a key objective since outage denotes the time period when a user equipment (UE) cannot communicate with the network.","Earlier studies have shown that in the experimental high mobility scenario considered, outage is dominated by the interruption time that stems from the random access channel (RACH)-based handover process from the serving cell to the target cell.","A handover by itself is a necessary mobility process to prevent mobility failures and their associated outage.","This paper proposes a RACH-less handover signaling scheme for the 3rd Generation Partnership Project (3GPP) conditional handover (CHO) mechanism.","The proposed scheme exploits the decoupling between the CHO preparation and execution phases to establish initial synchronization between the UE and the target cell through an early acquisition of the timing advance.","This significantly curtails the RACH process and therefore the handover interruption time.","Results based on a system-level simulation-based mobility study have shown that the proposed scheme significantly reduces the outage and its constituent handover interruption time relatively by 18.7% and 43.2%, respectively."],"url":"http://arxiv.org/abs/2403.10286v1","category":"cs.NI"}
{"created":"2024-03-15 13:27:03","title":"Boundary parameter matching for isogeometric analysis using Schwarz-Christoffel mapping","abstract":"Isogeometric analysis has brought a paradigm shift in integrating computational simulations with geometric designs across engineering disciplines. This technique necessitates analysis-suitable parameterization of physical domains to fully harness the synergy between Computer-Aided Design and Computer-Aided Engineering analyses. The existing methods often fix boundary parameters, leading to challenges in elongated geometries such as fluid channels and tubular reactors. This paper presents an innovative solution for the boundary parameter matching problem, specifically designed for analysis-suitable parameterizations. We employ a sophisticated Schwarz-Christoffel mapping technique, which is instrumental in computing boundary correspondences. A refined boundary curve reparameterization process complements this. Our dual-strategy approach maintains the geometric exactness and continuity of input physical domains, overcoming limitations often encountered with the existing reparameterization techniques. By employing our proposed boundary parameter method, we show that even a simple linear interpolation approach can effectively construct a satisfactory analysis-suitable parameterization. Our methodology offers significant improvements over traditional practices, enabling the generation of analysis-suitable and geometrically precise models, which is crucial for ensuring accurate simulation results. Numerical experiments show the capacity of the proposed method to enhance the quality and reliability of isogeometric analysis workflows.","sentences":["Isogeometric analysis has brought a paradigm shift in integrating computational simulations with geometric designs across engineering disciplines.","This technique necessitates analysis-suitable parameterization of physical domains to fully harness the synergy between Computer-Aided Design and Computer-Aided Engineering analyses.","The existing methods often fix boundary parameters, leading to challenges in elongated geometries such as fluid channels and tubular reactors.","This paper presents an innovative solution for the boundary parameter matching problem, specifically designed for analysis-suitable parameterizations.","We employ a sophisticated Schwarz-Christoffel mapping technique, which is instrumental in computing boundary correspondences.","A refined boundary curve reparameterization process complements this.","Our dual-strategy approach maintains the geometric exactness and continuity of input physical domains, overcoming limitations often encountered with the existing reparameterization techniques.","By employing our proposed boundary parameter method, we show that even a simple linear interpolation approach can effectively construct a satisfactory analysis-suitable parameterization.","Our methodology offers significant improvements over traditional practices, enabling the generation of analysis-suitable and geometrically precise models, which is crucial for ensuring accurate simulation results.","Numerical experiments show the capacity of the proposed method to enhance the quality and reliability of isogeometric analysis workflows."],"url":"http://arxiv.org/abs/2403.10284v1","category":"math.NA"}
{"created":"2024-03-15 13:24:28","title":"Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning","abstract":"In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.","sentences":["In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification.","Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification.","Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods.","Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor.","This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research."],"url":"http://arxiv.org/abs/2403.10281v1","category":"cs.CL"}
{"created":"2024-03-15 13:20:38","title":"Emotion-Aware Multimodal Fusion for Meme Emotion Detection","abstract":"The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public's opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding. In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We then present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism. Our investigation establishes ALFRED's superiority over existing baselines by 4.94% F1. Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task. We then discuss ALFRED's domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets - HarMeme and Dank Memes, over other baselines. Further, we analyze ALFRED's interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis.","sentences":["The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent.","Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public's opinion.","Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche.","However, the current approaches are yet to model the affective dimensions expressed in memes effectively.","They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding.","In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions.","We then present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism.","Our investigation establishes ALFRED's superiority over existing baselines by 4.94% F1.","Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task.","We then discuss ALFRED's domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets - HarMeme and Dank Memes, over other baselines.","Further, we analyze ALFRED's interpretability using attention maps.","Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis."],"url":"http://arxiv.org/abs/2403.10279v1","category":"cs.CY"}
{"created":"2024-03-15 13:19:57","title":"Optimizing post-Newtonian parameters and fixing the BMS frame for numerical-relativity waveform hybridizations","abstract":"Numerical relativity (NR) simulations of binary black holes provide precise waveforms, but are typically too computationally expensive to produce waveforms with enough orbits to cover the whole frequency band of gravitational-wave observatories. Accordingly, it is important to be able to hybridize NR waveforms with analytic, post-Newtonian (PN) waveforms, which are accurate during the early inspiral phase. We show that to build such hybrids, it is crucial to both fix the Bondi-Metzner-Sachs (BMS) frame of the NR waveforms to match that of PN theory, and optimize over the PN parameters. We test such a hybridization procedure including all spin-weighted spherical harmonic modes with $|m|\\leq \\ell$ for $\\ell\\leq 8$, using 29 NR waveforms with mass ratios $q\\leq 10$ and spin magnitudes $|\\chi_1|, |\\chi_2|\\leq 0.8$. We find that for spin-aligned systems, the PN and NR waveforms agree very well. The difference is limited by the small nonzero orbital eccentricity of the NR waveforms, or equivalently by the lack of eccentric terms in the PN waveforms. To maintain full accuracy of the simulations, the matching window for spin-aligned systems should be at least 5 orbits long and end at least 15 orbits before merger. For precessing systems, the errors are larger than for spin-aligned cases. The errors are likely limited by the absence of precession-related spin-spin PN terms. Using $10^5\\,M$ long NR waveforms, we find that there is no optimal choice of the matching window within this time span, because the hybridization result for precessing cases is always better if using earlier or longer matching windows. We provide the mean orbital frequency of the smallest acceptable matching window as a function of the target error between the PN and NR waveforms and the black hole spins.","sentences":["Numerical relativity (NR) simulations of binary black holes provide precise waveforms, but are typically too computationally expensive to produce waveforms with enough orbits to cover the whole frequency band of gravitational-wave observatories.","Accordingly, it is important to be able to hybridize NR waveforms with analytic, post-Newtonian (PN) waveforms, which are accurate during the early inspiral phase.","We show that to build such hybrids, it is crucial to both fix the Bondi-Metzner-Sachs (BMS) frame of the NR waveforms to match that of PN theory, and optimize over the PN parameters.","We test such a hybridization procedure including all spin-weighted spherical harmonic modes with $|m|\\leq \\ell$ for $\\ell\\leq 8$, using 29 NR waveforms with mass ratios $q\\leq 10$ and spin magnitudes $|\\chi_1|, |\\chi_2|\\leq 0.8$.","We find that for spin-aligned systems, the PN and NR waveforms agree very well.","The difference is limited by the small nonzero orbital eccentricity of the NR waveforms, or equivalently by the lack of eccentric terms in the PN waveforms.","To maintain full accuracy of the simulations, the matching window for spin-aligned systems should be at least 5 orbits long and end at least 15 orbits before merger.","For precessing systems, the errors are larger than for spin-aligned cases.","The errors are likely limited by the absence of precession-related spin-spin PN terms.","Using $10^5\\,M$ long NR waveforms, we find that there is no optimal choice of the matching window within this time span, because the hybridization result for precessing cases is always better if using earlier or longer matching windows.","We provide the mean orbital frequency of the smallest acceptable matching window as a function of the target error between the PN and NR waveforms and the black hole spins."],"url":"http://arxiv.org/abs/2403.10278v1","category":"gr-qc"}
{"created":"2024-03-15 13:18:50","title":"The long-term and disparate impact of job loss on individual mobility behaviour","abstract":"In today's interconnected world of widespread mobility, ubiquitous social interaction, and rapid information dissemination, the demand for individuals to swiftly adapt their behaviors has increased dramatically. Timely decision-making faces new challenges due to the necessity of using finely temporal-resolved anonymised individual data to keep up with fast-paced behavioural changes. To tackle this issue, we propose a general framework that leverages privacy-enhanced GPS data from mobile devices alongside census information to infer the employment status of individuals over time. By analysing the mobility patterns of employed and unemployed individuals, we unveil significant differences in behaviours between the two groups, showing a contraction in visited locations and a general decline in the exploratory behaviour of unemployed individuals. Remarkably, these differences intensify over time since job loss, particularly affecting individuals from more vulnerable demographic groups. These findings highlight the importance of early monitoring of unemployed individuals who may face enduring levels of distress. Overall, our findings shed light on the dynamics of employment-related behaviour, emphasizing the importance of implementing timely interventions to support the unemployed and vulnerable populations.","sentences":["In today's interconnected world of widespread mobility, ubiquitous social interaction, and rapid information dissemination, the demand for individuals to swiftly adapt their behaviors has increased dramatically.","Timely decision-making faces new challenges due to the necessity of using finely temporal-resolved anonymised individual data to keep up with fast-paced behavioural changes.","To tackle this issue, we propose a general framework that leverages privacy-enhanced GPS data from mobile devices alongside census information to infer the employment status of individuals over time.","By analysing the mobility patterns of employed and unemployed individuals, we unveil significant differences in behaviours between the two groups, showing a contraction in visited locations and a general decline in the exploratory behaviour of unemployed individuals.","Remarkably, these differences intensify over time since job loss, particularly affecting individuals from more vulnerable demographic groups.","These findings highlight the importance of early monitoring of unemployed individuals who may face enduring levels of distress.","Overall, our findings shed light on the dynamics of employment-related behaviour, emphasizing the importance of implementing timely interventions to support the unemployed and vulnerable populations."],"url":"http://arxiv.org/abs/2403.10276v1","category":"physics.soc-ph"}
{"created":"2024-03-15 13:15:23","title":"A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption","abstract":"The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.","sentences":["The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity.","In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models.","To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio.","We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones.","We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers."],"url":"http://arxiv.org/abs/2403.10275v1","category":"cs.CL"}
{"created":"2024-03-15 13:03:24","title":"SuperME: Supervised and Mixture-to-Mixture Co-Learning for Speech Enhancement and Robust ASR","abstract":"The current dominant approach for neural speech enhancement is based on supervised learning by using simulated training data. The trained models, however, often exhibit limited generalizability to real-recorded data. To address this, we investigate training models directly on real target-domain data, and propose two algorithms, mixture-to-mixture (M2M) training and a co-learning algorithm that improves M2M with the help of supervised algorithms. When paired close-talk and far-field mixtures are available for training, M2M realizes speech enhancement by training a deep neural network (DNN) to produce speech and noise estimates in a way such that they can be linearly filtered to reconstruct the close-talk and far-field mixtures. This way, the DNN can be trained directly on real mixtures, and can leverage close-talk mixtures as a weak supervision to enhance far-field mixtures. To improve M2M, we combine it with supervised approaches to co-train the DNN, where mini-batches of real close-talk and far-field mixture pairs and mini-batches of simulated mixture and clean speech pairs are alternately fed to the DNN, and the loss functions are respectively (a) the mixture reconstruction loss on the real close-talk and far-field mixtures and (b) the regular enhancement loss on the simulated clean speech and noise. We find that, this way, the DNN can learn from real and simulated data to achieve better generalization to real data. We name this algorithm SuperME, $\\underline{super}$vised and $\\underline{m}$ixture-to-mixtur$\\underline{e}$ co-learning. Evaluation results on the CHiME-4 dataset show its effectiveness and potential.","sentences":["The current dominant approach for neural speech enhancement is based on supervised learning by using simulated training data.","The trained models, however, often exhibit limited generalizability to real-recorded data.","To address this, we investigate training models directly on real target-domain data, and propose two algorithms, mixture-to-mixture (M2M) training and a co-learning algorithm that improves M2M with the help of supervised algorithms.","When paired close-talk and far-field mixtures are available for training, M2M realizes speech enhancement by training a deep neural network (DNN) to produce speech and noise estimates in a way such that they can be linearly filtered to reconstruct the close-talk and far-field mixtures.","This way, the DNN can be trained directly on real mixtures, and can leverage close-talk mixtures as a weak supervision to enhance far-field mixtures.","To improve M2M, we combine it with supervised approaches to co-train the DNN, where mini-batches of real close-talk and far-field mixture pairs and mini-batches of simulated mixture and clean speech pairs are alternately fed to the DNN, and the loss functions are respectively (a) the mixture reconstruction loss on the real close-talk and far-field mixtures and (b) the regular enhancement loss on the simulated clean speech and noise.","We find that, this way, the DNN can learn from real and simulated data to achieve better generalization to real data.","We name this algorithm SuperME, $\\underline{super}$vised and $\\underline{m}$ixture-to-mixtur$\\underline{e}$ co-learning.","Evaluation results on the CHiME-4 dataset show its effectiveness and potential."],"url":"http://arxiv.org/abs/2403.10271v1","category":"eess.AS"}
{"created":"2024-03-15 12:58:34","title":"Discrete functional inequalities on lattice graphs","abstract":"In this thesis, we study problems at the interface of analysis and discrete mathematics. We discuss analogues of well known Hardy-type inequalities and Rearrangement inequalities on the lattice graphs $\\mathbb{Z}^d$, with a particular focus on behaviour of sharp constants and optimizers.In the first half of the thesis, we analyse Hardy inequalities on $\\mathbb{Z}^d$, first for $d=1$ and then for $d \\geq 3$. We prove a sharp weighted Hardy inequality on integers with power weights of the form $n^\\alpha$. This is done via two different methods, namely super-solution and Fourier method. We also use Fourier method to prove a weighted Hardy type inequality for higher order operators. After discussing the one dimensional case, we study the Hardy inequality in higher dimensions ($d \\geq 3$). In particular, we compute the asymptotic behaviour of the sharp constant in the discrete Hardy inequality, as $d \\rightarrow \\infty$. This is done by converting the inequality into a continuous Hardy-type inequality on a torus for functions having zero average. These continuous inequalities are new and interesting in themselves.   In the second half, we focus our attention on analogues of Rearrangement inequalities on lattice graphs. We begin by analysing the situation in dimension one. We define various notions of rearrangements and prove the corresponding Polya-Szeg\\H{o} inequality. These inequalities are also applied to prove some weighted Hardy inequalities on integers. Finally, we study Rearrangement inequalities (Polya-Szeg\\H{o}) on general graphs, with a particular focus on lattice graphs $\\mathbb{Z}^d$, for $d \\geq 2$. We develop a framework to study these inequalities, using which we derive concrete results in dimension two. In particular, these results develop connections between Polya-Szeg\\H{o} inequality and various isoperimetric inequalities on graphs.","sentences":["In this thesis, we study problems at the interface of analysis and discrete mathematics.","We discuss analogues of well known Hardy-type inequalities and Rearrangement inequalities on the lattice graphs $\\mathbb{Z}^d$, with a particular focus on behaviour of sharp constants and optimizers.","In the first half of the thesis, we analyse Hardy inequalities on $\\mathbb{Z}^d$, first for $d=1$ and then for $d \\geq 3$.","We prove a sharp weighted Hardy inequality on integers with power weights of the form $n^\\alpha$.","This is done via two different methods, namely super-solution and Fourier method.","We also use Fourier method to prove a weighted Hardy type inequality for higher order operators.","After discussing the one dimensional case, we study the Hardy inequality in higher dimensions ($d \\geq 3$).","In particular, we compute the asymptotic behaviour of the sharp constant in the discrete Hardy inequality, as $d \\rightarrow \\infty$. This is done by converting the inequality into a continuous Hardy-type inequality on a torus for functions having zero average.","These continuous inequalities are new and interesting in themselves.   ","In the second half, we focus our attention on analogues of Rearrangement inequalities on lattice graphs.","We begin by analysing the situation in dimension one.","We define various notions of rearrangements and prove the corresponding Polya-Szeg\\H{o} inequality.","These inequalities are also applied to prove some weighted Hardy inequalities on integers.","Finally, we study Rearrangement inequalities (Polya-Szeg\\H{o}) on general graphs, with a particular focus on lattice graphs $\\mathbb{Z}^d$, for $d \\geq 2$.","We develop a framework to study these inequalities, using which we derive concrete results in dimension two.","In particular, these results develop connections between Polya-Szeg\\H{o} inequality and various isoperimetric inequalities on graphs."],"url":"http://arxiv.org/abs/2403.10270v1","category":"math.FA"}
{"created":"2024-03-15 12:56:38","title":"Low-density parity-check representation of fault-tolerant quantum circuits","abstract":"In fault-tolerant quantum computing, quantum algorithms are implemented through quantum circuits capable of error correction. These circuits are typically constructed based on specific quantum error correction codes, with consideration given to the characteristics of the underlying physical platforms. Optimising these circuits within the constraints of today's quantum computing technologies, particularly in terms of error rates, qubit counts, and network topologies, holds substantial implications for the feasibility of quantum applications in the near future. This paper presents a toolkit for designing and analysing fault-tolerant quantum circuits. We introduce a framework for representing stabiliser circuits using classical low-density parity-check (LDPC) codes. Each codeword in the representation corresponds to a quantum-mechanical equation regarding the circuit, formalising the correlations utilised in parity checks and delineating logical operations within the circuit. Consequently, the LDPC code provides a means of quantifying fault tolerance and verifying logical operations. We outline the procedure for generating LDPC codes from circuits using the Tanner graph notation, alongside proposing graph-theory tools for constructing fault-tolerant quantum circuits from classical LDPC codes. These findings offer a systematic approach to applying classical error correction techniques in optimising existing fault-tolerant protocols and developing new ones.","sentences":["In fault-tolerant quantum computing, quantum algorithms are implemented through quantum circuits capable of error correction.","These circuits are typically constructed based on specific quantum error correction codes, with consideration given to the characteristics of the underlying physical platforms.","Optimising these circuits within the constraints of today's quantum computing technologies, particularly in terms of error rates, qubit counts, and network topologies, holds substantial implications for the feasibility of quantum applications in the near future.","This paper presents a toolkit for designing and analysing fault-tolerant quantum circuits.","We introduce a framework for representing stabiliser circuits using classical low-density parity-check (LDPC) codes.","Each codeword in the representation corresponds to a quantum-mechanical equation regarding the circuit, formalising the correlations utilised in parity checks and delineating logical operations within the circuit.","Consequently, the LDPC code provides a means of quantifying fault tolerance and verifying logical operations.","We outline the procedure for generating LDPC codes from circuits using the Tanner graph notation, alongside proposing graph-theory tools for constructing fault-tolerant quantum circuits from classical LDPC codes.","These findings offer a systematic approach to applying classical error correction techniques in optimising existing fault-tolerant protocols and developing new ones."],"url":"http://arxiv.org/abs/2403.10268v1","category":"quant-ph"}
{"created":"2024-03-15 12:56:10","title":"A Vocabulary of Board Game Dynamics","abstract":"In recent years, significant advances have been made in the field of game research. However, there has been a noticeable dearth of scholarly research focused on the domain of dynamics, despite the widespread recognition among researchers of its existence and importance. The objective of this paper is to address this research gap by presenting a vocabulary dedicated to boardgame dynamics. To achieve this goal, we employ a focus group to generate a set of dynamic concepts that are subsequently subjected to validation and refinement through a survey. The resulting concepts are then organized into a vocabulary using a taxonomic structure, allowing the grouping of these concepts into broader and more general ideas.","sentences":["In recent years, significant advances have been made in the field of game research.","However, there has been a noticeable dearth of scholarly research focused on the domain of dynamics, despite the widespread recognition among researchers of its existence and importance.","The objective of this paper is to address this research gap by presenting a vocabulary dedicated to boardgame dynamics.","To achieve this goal, we employ a focus group to generate a set of dynamic concepts that are subsequently subjected to validation and refinement through a survey.","The resulting concepts are then organized into a vocabulary using a taxonomic structure, allowing the grouping of these concepts into broader and more general ideas."],"url":"http://arxiv.org/abs/2403.10267v1","category":"cs.SE"}
{"created":"2024-03-15 12:53:50","title":"DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers","abstract":"Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end throughput by 42.0% to 216.8% over prior sequence parallelism methods.","sentences":["Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism.","However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions.","This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models.","The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention.","This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models.","Experiments show DSP improves end-to-end throughput by 42.0% to 216.8% over prior sequence parallelism methods."],"url":"http://arxiv.org/abs/2403.10266v1","category":"cs.DC"}
{"created":"2024-03-15 12:48:44","title":"Towards Generalizable Deepfake Video Detection with Thumbnail Layout and Graph Reasoning","abstract":"The deepfake threats to society and cybersecurity have provoked significant public apprehension, driving intensified efforts within the realm of deepfake video detection. Current video-level methods are mostly based on {3D CNNs} resulting in high computational demands, although have achieved good performance. This paper introduces an elegantly simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. This transformation process involves sequentially masking frames at the same positions within each frame. These frames are then resized into sub-frames and reorganized into the predetermined layout, forming thumbnails. TALL is model-agnostic and has remarkable simplicity, necessitating only minimal code modifications. Furthermore, we introduce a graph reasoning block (GRB) and semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB enhances interactions between different semantic regions to capture semantic-level inconsistency clues. The semantic consistency loss imposes consistency constraints on semantic features to improve model generalization ability. Extensive experiments on intra-dataset, cross-dataset, diffusion-generated image detection, and deepfake generation method recognition show that TALL++ achieves results surpassing or comparable to the state-of-the-art methods, demonstrating the effectiveness of our approaches for various deepfake detection problems. The code is available at https://github.com/rainy-xu/TALL4Deepfake.","sentences":["The deepfake threats to society and cybersecurity have provoked significant public apprehension, driving intensified efforts within the realm of deepfake video detection.","Current video-level methods are mostly based on {3D CNNs} resulting in high computational demands, although have achieved good performance.","This paper introduces an elegantly simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies.","This transformation process involves sequentially masking frames at the same positions within each frame.","These frames are then resized into sub-frames and reorganized into the predetermined layout, forming thumbnails.","TALL is model-agnostic and has remarkable simplicity, necessitating only minimal code modifications.","Furthermore, we introduce a graph reasoning block (GRB) and semantic consistency (SC) loss to strengthen TALL, culminating in TALL++.","GRB enhances interactions between different semantic regions to capture semantic-level inconsistency clues.","The semantic consistency loss imposes consistency constraints on semantic features to improve model generalization ability.","Extensive experiments on intra-dataset, cross-dataset, diffusion-generated image detection, and deepfake generation method recognition show that TALL++ achieves results surpassing or comparable to the state-of-the-art methods, demonstrating the effectiveness of our approaches for various deepfake detection problems.","The code is available at https://github.com/rainy-xu/TALL4Deepfake."],"url":"http://arxiv.org/abs/2403.10261v1","category":"cs.CV"}
{"created":"2024-03-15 12:47:45","title":"Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model","abstract":"In today's technology-driven era, the imperative for predictive maintenance and advanced diagnostics extends beyond aviation to encompass the identification of damages, failures, and operational defects in rotating and moving machines. Implementing such services not only curtails maintenance costs but also extends machine lifespan, ensuring heightened operational efficiency. Moreover, it serves as a preventive measure against potential accidents or catastrophic events. The advent of Artificial Intelligence (AI) has revolutionized maintenance across industries, enabling more accurate and efficient prediction and analysis of machine failures, thereby conserving time and resources. Our proposed study aims to delve into various machine learning classification techniques, including Support Vector Machine (SVM), Random Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for predicting and analyzing machine performance. SVM classifies data into different categories based on their positions in a multidimensional space, while Random Forest employs ensemble learning to create multiple decision trees for classification. Logistic Regression predicts the probability of binary outcomes using input data. The primary objective of the study is to assess these algorithms' performance in predicting and analyzing machine performance, considering factors such as accuracy, precision, recall, and F1 score. The findings will aid maintenance experts in selecting the most suitable machine learning algorithm for effective prediction and analysis of machine performance.","sentences":["In today's technology-driven era, the imperative for predictive maintenance and advanced diagnostics extends beyond aviation to encompass the identification of damages, failures, and operational defects in rotating and moving machines.","Implementing such services not only curtails maintenance costs but also extends machine lifespan, ensuring heightened operational efficiency.","Moreover, it serves as a preventive measure against potential accidents or catastrophic events.","The advent of Artificial Intelligence (AI) has revolutionized maintenance across industries, enabling more accurate and efficient prediction and analysis of machine failures, thereby conserving time and resources.","Our proposed study aims to delve into various machine learning classification techniques, including Support Vector Machine (SVM), Random Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for predicting and analyzing machine performance.","SVM classifies data into different categories based on their positions in a multidimensional space, while Random Forest employs ensemble learning to create multiple decision trees for classification.","Logistic Regression predicts the probability of binary outcomes using input data.","The primary objective of the study is to assess these algorithms' performance in predicting and analyzing machine performance, considering factors such as accuracy, precision, recall, and F1 score.","The findings will aid maintenance experts in selecting the most suitable machine learning algorithm for effective prediction and analysis of machine performance."],"url":"http://arxiv.org/abs/2403.10259v1","category":"cs.LG"}
{"created":"2024-03-15 12:45:40","title":"Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder","abstract":"Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies. The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent diffusion process is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage.","sentences":["Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications.","Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts.","Additionally, they do not offer enough diversity of output images nor image consistency at different scales.","Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results.","Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency.","We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales.","The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies.","The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales.","More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series.","The latent diffusion process is learnt by the denoising and the alignment losses jointly.","Errors in output images are backpropagated via the fixed decoder, improving the quality of output images.","In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency.","It is significantly better than the relevant prior-art in the inference speed and memory usage."],"url":"http://arxiv.org/abs/2403.10255v1","category":"cs.CV"}
{"created":"2024-03-15 12:44:35","title":"Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification","abstract":"Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \\textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities. Finally, to further reduce the effect of backgrounds, we propose a Background Consistency Constraint (BCC) and an Object-Centric Feature Refinement (OCFR). They are formulated as two new loss functions, which improve the feature discrimination with background suppression. As a result, our framework can generate more discriminative features for multi-modal object ReID. Extensive experiments on three multi-modal ReID benchmarks verify the effectiveness of our methods. The code is available at https://github.com/924973292/EDITOR.","sentences":["Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios.","In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications.","However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps.","To address above issues, we propose a novel learning framework named \\textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID.","We begin with a shared vision Transformer to extract tokenized features from different input modalities.","Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information.","Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities.","Finally, to further reduce the effect of backgrounds, we propose a Background Consistency Constraint (BCC) and an Object-Centric Feature Refinement (OCFR).","They are formulated as two new loss functions, which improve the feature discrimination with background suppression.","As a result, our framework can generate more discriminative features for multi-modal object ReID.","Extensive experiments on three multi-modal ReID benchmarks verify the effectiveness of our methods.","The code is available at https://github.com/924973292/EDITOR."],"url":"http://arxiv.org/abs/2403.10254v1","category":"cs.CV"}
{"created":"2024-03-15 12:38:00","title":"Interpretable Machine Learning for Survival Analysis","abstract":"With the spread and rapid advancement of black box machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade. This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and fairness in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts. More specifically, explainability can uncover a survival model's potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors. However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine learning for predicting time-to-event data. We present a comprehensive review of the limited existing amount of work on IML methods for survival analysis within the context of the general IML taxonomy. In addition, we formally detail how commonly used IML methods, such as such as individual conditional expectation (ICE), partial dependence plots (PDP), accumulated local effects (ALE), different feature importance measures or Friedman's H-interaction statistics can be adapted to survival outcomes. An application of several IML methods to real data on data on under-5 year mortality of Ghanaian children from the Demographic and Health Surveys (DHS) Program serves as a tutorial or guide for researchers, on how to utilize the techniques in practice to facilitate understanding of model decisions or predictions.","sentences":["With the spread and rapid advancement of black box machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade.","This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and fairness in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts.","More specifically, explainability can uncover a survival model's potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors.","However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine learning for predicting time-to-event data.","We present a comprehensive review of the limited existing amount of work on IML methods for survival analysis within the context of the general IML taxonomy.","In addition, we formally detail how commonly used IML methods, such as such as individual conditional expectation (ICE), partial dependence plots (PDP), accumulated local effects (ALE), different feature importance measures or Friedman's H-interaction statistics can be adapted to survival outcomes.","An application of several IML methods to real data on data on under-5 year mortality of Ghanaian children from the Demographic and Health Surveys (DHS) Program serves as a tutorial or guide for researchers, on how to utilize the techniques in practice to facilitate understanding of model decisions or predictions."],"url":"http://arxiv.org/abs/2403.10250v1","category":"stat.ML"}
{"created":"2024-03-15 12:37:12","title":"A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges","abstract":"The swift evolution of Large-scale Models (LMs), either language-focused or multi-modal, has garnered extensive attention in both academy and industry. But despite the surge in interest in this rapidly evolving area, there are scarce systematic reviews on their capabilities and potential in distinct impactful scenarios. This paper endeavours to help bridge this gap, offering a thorough examination of the current landscape of LM usage in regards to complex game playing scenarios and the challenges still open. Here, we seek to systematically review the existing architectures of LM-based Agents (LMAs) for games and summarize their commonalities, challenges, and any other insights. Furthermore, we present our perspective on promising future research avenues for the advancement of LMs in games. We hope to assist researchers in gaining a clear understanding of the field and to generate more interest in this highly impactful research direction. A corresponding resource, continuously updated, can be found in our GitHub repository.","sentences":["The swift evolution of Large-scale Models (LMs), either language-focused or multi-modal, has garnered extensive attention in both academy and industry.","But despite the surge in interest in this rapidly evolving area, there are scarce systematic reviews on their capabilities and potential in distinct impactful scenarios.","This paper endeavours to help bridge this gap, offering a thorough examination of the current landscape of LM usage in regards to complex game playing scenarios and the challenges still open.","Here, we seek to systematically review the existing architectures of LM-based Agents (LMAs) for games and summarize their commonalities, challenges, and any other insights.","Furthermore, we present our perspective on promising future research avenues for the advancement of LMs in games.","We hope to assist researchers in gaining a clear understanding of the field and to generate more interest in this highly impactful research direction.","A corresponding resource, continuously updated, can be found in our GitHub repository."],"url":"http://arxiv.org/abs/2403.10249v1","category":"cs.AI"}
{"created":"2024-03-15 12:35:35","title":"Mutual Information Bounded by Fisher Information","abstract":"We derive a general upper bound to mutual information in terms of the Fisher information. The bound may be further used to derive a lower bound for Bayesian quadratic cost. These two provide alternatives to the Efroimovich and to the van Trees inequality that are useful also for classes of prior distributions where the latter ones give trivial bounds. We illustrate the usefulness of our bounds with a case study in quantum phase estimation. Here, they allow us to adapt to mutual information the known and highly nontrivial bounds for Fisher information in the presence of noise. This nicely complements quantum metrology, since Fisher information is useful to gauge local estimation strategies, whereas mutual information is useful for global strategies.","sentences":["We derive a general upper bound to mutual information in terms of the Fisher information.","The bound may be further used to derive a lower bound for Bayesian quadratic cost.","These two provide alternatives to the Efroimovich and to the van Trees inequality that are useful also for classes of prior distributions where the latter ones give trivial bounds.","We illustrate the usefulness of our bounds with a case study in quantum phase estimation.","Here, they allow us to adapt to mutual information the known and highly nontrivial bounds for Fisher information in the presence of noise.","This nicely complements quantum metrology, since Fisher information is useful to gauge local estimation strategies, whereas mutual information is useful for global strategies."],"url":"http://arxiv.org/abs/2403.10248v1","category":"quant-ph"}
{"created":"2024-03-15 12:34:35","title":"Proper splittings of Hilbert space operators","abstract":"Proper splittings of operators are commonly used to study the convergence of iterative processes. In order to approximate solutions of operator equations, in this article we deal with proper splittings of closed range bounded linear operators defined on Hilbert spaces. We study the convergence of general proper splittings of operators in the infinite dimensional context. We also propose some particular splittings for special classes of operators and we study different criteria of convergence and comparison for them. In some cases, these criteria are given under hypothesis of operator order relations. In addition, we relate these results with the concept of the symmetric approximation of a frame in a Hilbert space.","sentences":["Proper splittings of operators are commonly used to study the convergence of iterative processes.","In order to approximate solutions of operator equations, in this article we deal with proper splittings of closed range bounded linear operators defined on Hilbert spaces.","We study the convergence of general proper splittings of operators in the infinite dimensional context.","We also propose some particular splittings for special classes of operators and we study different criteria of convergence and comparison for them.","In some cases, these criteria are given under hypothesis of operator order relations.","In addition, we relate these results with the concept of the symmetric approximation of a frame in a Hilbert space."],"url":"http://arxiv.org/abs/2403.10247v1","category":"math.FA"}
{"created":"2024-03-15 12:24:36","title":"FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model","abstract":"Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website https://qjfeng.net/FDGaussian/.","sentences":["Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available.","In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction.","Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity.","To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images.","Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints.","We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively.","More examples can be found at our website https://qjfeng.net/FDGaussian/."],"url":"http://arxiv.org/abs/2403.10242v1","category":"cs.CV"}
{"created":"2024-03-15 12:21:16","title":"Spatial Dependence of the Growth Factor in Scalar-Tensor Cosmology","abstract":"Scalar-tensor theories have taken on a key role in attempts to confront the growing open questions in standard cosmology. It is important to understand entirely their dynamics at perturbative level including any possible spatial dependence in their growth of large scale structures. In this work, we investigate the spatial dependence of the growth rate of scalar-tensor theories through the M\\'{e}sz\\'{a}ros equation. We confirm that at subhorizon level this dependence does not play a major role for viable models. However, we establish conditions on which this criterion is met which may be important for developing new models. In our work, we consider three specific models that exhibit spatial dependence of the growth rate at superhorizon modes, which may also be important for early Universe models.","sentences":["Scalar-tensor theories have taken on a key role in attempts to confront the growing open questions in standard cosmology.","It is important to understand entirely their dynamics at perturbative level including any possible spatial dependence in their growth of large scale structures.","In this work, we investigate the spatial dependence of the growth rate of scalar-tensor theories through the M\\'{e}sz\\'{a}ros equation.","We confirm that at subhorizon level this dependence does not play a major role for viable models.","However, we establish conditions on which this criterion is met which may be important for developing new models.","In our work, we consider three specific models that exhibit spatial dependence of the growth rate at superhorizon modes, which may also be important for early Universe models."],"url":"http://arxiv.org/abs/2403.10240v1","category":"gr-qc"}
{"created":"2024-03-15 12:05:44","title":"A Fixed-Point Approach to Unified Prompt-Based Counting","abstract":"Existing class-agnostic counting models typically rely on a single type of prompt, e.g., box annotations. This paper aims to establish a comprehensive prompt-based counting framework capable of generating density maps for concerned objects indicated by various prompt types, such as box, point, and text. To achieve this goal, we begin by converting prompts from different modalities into prompt masks without requiring training. These masks are then integrated into a class-agnostic counting methodology for predicting density maps. Furthermore, we introduce a fixed-point inference along with an associated loss function to improve counting accuracy, all without introducing new parameters. The effectiveness of this method is substantiated both theoretically and experimentally. Additionally, a contrastive training scheme is implemented to mitigate dataset bias inherent in current class-agnostic counting datasets, a strategy whose effectiveness is confirmed by our ablation study. Our model excels in prominent class-agnostic datasets and exhibits superior performance in cross-dataset adaptation tasks.","sentences":["Existing class-agnostic counting models typically rely on a single type of prompt, e.g., box annotations.","This paper aims to establish a comprehensive prompt-based counting framework capable of generating density maps for concerned objects indicated by various prompt types, such as box, point, and text.","To achieve this goal, we begin by converting prompts from different modalities into prompt masks without requiring training.","These masks are then integrated into a class-agnostic counting methodology for predicting density maps.","Furthermore, we introduce a fixed-point inference along with an associated loss function to improve counting accuracy, all without introducing new parameters.","The effectiveness of this method is substantiated both theoretically and experimentally.","Additionally, a contrastive training scheme is implemented to mitigate dataset bias inherent in current class-agnostic counting datasets, a strategy whose effectiveness is confirmed by our ablation study.","Our model excels in prominent class-agnostic datasets and exhibits superior performance in cross-dataset adaptation tasks."],"url":"http://arxiv.org/abs/2403.10236v1","category":"cs.CV"}
{"created":"2024-03-15 12:02:38","title":"Autonomous Engulfment of Active Colloids by Giant Lipid Vesicles","abstract":"The ability to design artificial micro/nanomachines able to perform sophisticated tasks crucially depends on the understanding of their interaction with biosystems and their compatibility with the biological environment. Here, Janus colloids fuelled only by glucose and light were designed, which can autonomously interact with cell-like compartments and trigger endocytosis. The crucial role played by the far field hydrodynamic interaction arising from the puller/pusher swimming mode and adhesion is evidenced. It is shown that a large contact time between the active particle and the lipid membrane is required to observe the engulfment of a particle inside a floppy giant lipid vesicle. Active Janus colloids showing relatively small velocities and a puller type swimming mode are able to target giant vesicles, deform their membranes and subsequently get stably engulfed. An instability arising from the unbound membrane segment is responsible for the transition between partial and complete stable engulfment. These experiments shed light on the physical criteria required for autonomous active particle engulfment in giant vesicles, which can serve as general principles in disciplines ranging from drug delivery and microbial infection to nanomedecine.","sentences":["The ability to design artificial micro/nanomachines able to perform sophisticated tasks crucially depends on the understanding of their interaction with biosystems and their compatibility with the biological environment.","Here, Janus colloids fuelled only by glucose and light were designed, which can autonomously interact with cell-like compartments and trigger endocytosis.","The crucial role played by the far field hydrodynamic interaction arising from the puller/pusher swimming mode and adhesion is evidenced.","It is shown that a large contact time between the active particle and the lipid membrane is required to observe the engulfment of a particle inside a floppy giant lipid vesicle.","Active Janus colloids showing relatively small velocities and a puller type swimming mode are able to target giant vesicles, deform their membranes and subsequently get stably engulfed.","An instability arising from the unbound membrane segment is responsible for the transition between partial and complete stable engulfment.","These experiments shed light on the physical criteria required for autonomous active particle engulfment in giant vesicles, which can serve as general principles in disciplines ranging from drug delivery and microbial infection to nanomedecine."],"url":"http://arxiv.org/abs/2403.10234v1","category":"cond-mat.soft"}
{"created":"2024-03-15 12:00:12","title":"Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs","abstract":"To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders their promise on large scale KGs and cannot be directly addressed by vanilla sampling methods. In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole KG, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence. With efficient subgraph-based prediction, we further introduce the automated searching of the optimal configurations in both data and model spaces. Empirically, we achieve promoted efficiency and leading performances on five large-scale benchmarks. The code is publicly available at: https://github.com/tmlr-group/one-shot-subgraph.","sentences":["To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query.","However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders their promise on large scale KGs and cannot be directly addressed by vanilla sampling methods.","In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction.","The design principle is that, instead of directly acting on the whole KG, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph.","We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence.","With efficient subgraph-based prediction, we further introduce the automated searching of the optimal configurations in both data and model spaces.","Empirically, we achieve promoted efficiency and leading performances on five large-scale benchmarks.","The code is publicly available at: https://github.com/tmlr-group/one-shot-subgraph."],"url":"http://arxiv.org/abs/2403.10231v1","category":"cs.LG"}
{"created":"2024-03-15 11:59:50","title":"Fairness Optimization for Intelligent Reflecting Surface Aided Uplink Rate-Splitting Multiple Access","abstract":"This paper studies the fair transmission design for an intelligent reflecting surface (IRS) aided rate-splitting multiple access (RSMA). IRS is used to establish a good signal propagation environment and enhance the RSMA transmission performance. The fair rate adaption problem is constructed as a max-min optimization problem. To solve the optimization problem, we adopt an alternative optimization (AO) algorithm to optimize the power allocation, beamforming, and decoding order, respectively. A generalized power iteration (GPI) method is proposed to optimize the receive beamforming, which can improve the minimum rate of devices and reduce the optimization complexity. At the base station (BS), a successive group decoding (SGD) algorithm is proposed to tackle the uplink signal estimation, which trades off the fairness and complexity of decoding. At the same time, we also consider robust communication with imperfect channel state information at the transmitter (CSIT), which studies robust optimization by using lower bound expressions on the expected data rates. Extensive numerical results show that the proposed optimization algorithm can significantly improve the performance of fairness. It also provides reliable results for uplink communication with imperfect CSIT.","sentences":["This paper studies the fair transmission design for an intelligent reflecting surface (IRS) aided rate-splitting multiple access (RSMA).","IRS is used to establish a good signal propagation environment and enhance the RSMA transmission performance.","The fair rate adaption problem is constructed as a max-min optimization problem.","To solve the optimization problem, we adopt an alternative optimization (AO) algorithm to optimize the power allocation, beamforming, and decoding order, respectively.","A generalized power iteration (GPI) method is proposed to optimize the receive beamforming, which can improve the minimum rate of devices and reduce the optimization complexity.","At the base station (BS), a successive group decoding (SGD) algorithm is proposed to tackle the uplink signal estimation, which trades off the fairness and complexity of decoding.","At the same time, we also consider robust communication with imperfect channel state information at the transmitter (CSIT), which studies robust optimization by using lower bound expressions on the expected data rates.","Extensive numerical results show that the proposed optimization algorithm can significantly improve the performance of fairness.","It also provides reliable results for uplink communication with imperfect CSIT."],"url":"http://arxiv.org/abs/2403.10230v1","category":"cs.IT"}
{"created":"2024-03-15 11:58:18","title":"HawkEye: Training Video-Text LLMs for Grounding Text in Videos","abstract":"Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than other alternatives. Extensive experiments show that HawkEye is better at temporal video grounding and comparable on other video-text tasks with existing video-text LLMs, which verifies its superior video-text multi-modal understanding abilities.","sentences":["Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos.","However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images.","In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner.","To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs.","We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than other alternatives.","Extensive experiments show that HawkEye is better at temporal video grounding and comparable on other video-text tasks with existing video-text LLMs, which verifies its superior video-text multi-modal understanding abilities."],"url":"http://arxiv.org/abs/2403.10228v1","category":"cs.CV"}
{"created":"2024-03-15 11:41:14","title":"V2AIX: A Multi-Modal Real-World Dataset of ETSI ITS V2X Messages in Public Road Traffic","abstract":"Connectivity is a main driver for the ongoing megatrend of automated mobility: future Cooperative Intelligent Transport Systems (C-ITS) will connect road vehicles, traffic signals, roadside infrastructure, and even vulnerable road users, sharing data and compute for safer, more efficient, and more comfortable mobility. In terms of communication technology for realizing such vehicle-to-everything (V2X) communication, the WLAN-based peer-to-peer approach (IEEE 802.11p, ITS-G5 in Europe) competes with C-V2X based on cellular technologies (4G and beyond). Irrespective of the underlying communication standard, common message interfaces are crucial for a common understanding between vehicles, especially from different manufacturers. Targeting this issue, the European Telecommunications Standards Institute (ETSI) has been standardizing V2X message formats such as the Cooperative Awareness Message (CAM). In this work, we present V2AIX, a multi-modal real-world dataset of ETSI ITS messages gathered in public road traffic, the first of its kind. Collected in measurement drives and with stationary infrastructure, we have recorded more than 230 000 V2X messages from more than 1800 vehicles and roadside units in public road traffic. Alongside a first analysis of the dataset, we present a way of integrating ETSI ITS V2X messages into the Robot Operating System (ROS). This enables researchers to not only thoroughly analyze real-world V2X data, but to also study and implement standardized V2X messages in ROS-based automated driving applications. The full dataset is publicly available for noncommercial use at v2aix.ika.rwth-aachen.de.","sentences":["Connectivity is a main driver for the ongoing megatrend of automated mobility: future Cooperative Intelligent Transport Systems (C-ITS) will connect road vehicles, traffic signals, roadside infrastructure, and even vulnerable road users, sharing data and compute for safer, more efficient, and more comfortable mobility.","In terms of communication technology for realizing such vehicle-to-everything (V2X) communication, the WLAN-based peer-to-peer approach (IEEE 802.11p, ITS-G5 in Europe) competes with C-V2X based on cellular technologies (4G and beyond).","Irrespective of the underlying communication standard, common message interfaces are crucial for a common understanding between vehicles, especially from different manufacturers.","Targeting this issue, the European Telecommunications Standards Institute (ETSI) has been standardizing V2X message formats such as the Cooperative Awareness Message (CAM).","In this work, we present V2AIX, a multi-modal real-world dataset of ETSI ITS messages gathered in public road traffic, the first of its kind.","Collected in measurement drives and with stationary infrastructure, we have recorded more than 230 000 V2X messages from more than 1800 vehicles and roadside units in public road traffic.","Alongside a first analysis of the dataset, we present a way of integrating ETSI ITS V2X messages into the Robot Operating System (ROS).","This enables researchers to not only thoroughly analyze real-world V2X data, but to also study and implement standardized V2X messages in ROS-based automated driving applications.","The full dataset is publicly available for noncommercial use at v2aix.ika.rwth-aachen.de."],"url":"http://arxiv.org/abs/2403.10221v1","category":"cs.MA"}
{"created":"2024-03-15 11:39:12","title":"From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations","abstract":"With the development of astronomical facilities, large-scale time series data observed by these facilities is being collected. Analyzing anomalies in these astronomical observations is crucial for uncovering potential celestial events and physical phenomena, thus advancing the scientific research process. However, existing time series anomaly detection methods fall short in tackling the unique characteristics of astronomical observations where each star is inherently independent but interfered by random concurrent noise, resulting in a high rate of false alarms. To overcome the challenges, we propose AERO, a novel two-stage framework tailored for unsupervised anomaly detection in astronomical observations. In the first stage, we employ a Transformer-based encoder-decoder architecture to learn the normal temporal patterns on each variate (i.e., star) in alignment with the characteristic of variate independence. In the second stage, we enhance the graph neural network with a window-wise graph structure learning to tackle the occurrence of concurrent noise characterized by spatial and temporal randomness. In this way, AERO is not only capable of distinguishing normal temporal patterns from potential anomalies but also effectively differentiating concurrent noise, thus decreasing the number of false alarms. We conducted extensive experiments on three synthetic datasets and three real-world datasets. The results demonstrate that AERO outperforms the compared baselines. Notably, compared to the state-of-the-art model, AERO improves the F1-score by up to 8.76% and 2.63% on synthetic and real-world datasets respectively.","sentences":["With the development of astronomical facilities, large-scale time series data observed by these facilities is being collected.","Analyzing anomalies in these astronomical observations is crucial for uncovering potential celestial events and physical phenomena, thus advancing the scientific research process.","However, existing time series anomaly detection methods fall short in tackling the unique characteristics of astronomical observations where each star is inherently independent but interfered by random concurrent noise, resulting in a high rate of false alarms.","To overcome the challenges, we propose AERO, a novel two-stage framework tailored for unsupervised anomaly detection in astronomical observations.","In the first stage, we employ a Transformer-based encoder-decoder architecture to learn the normal temporal patterns on each variate (i.e., star) in alignment with the characteristic of variate independence.","In the second stage, we enhance the graph neural network with a window-wise graph structure learning to tackle the occurrence of concurrent noise characterized by spatial and temporal randomness.","In this way, AERO is not only capable of distinguishing normal temporal patterns from potential anomalies but also effectively differentiating concurrent noise, thus decreasing the number of false alarms.","We conducted extensive experiments on three synthetic datasets and three real-world datasets.","The results demonstrate that AERO outperforms the compared baselines.","Notably, compared to the state-of-the-art model, AERO improves the F1-score by up to 8.76% and 2.63% on synthetic and real-world datasets respectively."],"url":"http://arxiv.org/abs/2403.10220v1","category":"cs.LG"}
{"created":"2024-03-15 11:38:05","title":"Synthesizing impurity clustering in the edge plasma of tokamaks using neural networks","abstract":"This work investigates the behavior of impurities in edge plasma of tokamaks using high-resolution numerical simulations based on Hasegawa--Wakatani equations. Specifically, it focuses on the behavior of inertial particles, which has not been extensively studied in the field of plasma physics. Our simulations utilize one-way coupling of a large number of inertial point particles, which model plasma impurities. We observe that with Stokes number ($St$) which characterizes the inertia of particles being much less than one, such light impurities closely track the fluid flow without pronounced clustering. For intermediate $St$ values, distinct clustering appears, with larger Stokes values, {\\it i.e.} heavy impurities even generating more substantial clusters. When $St$ is significantly large, very heavy impurities tend to detach from the flow and maintain their trajectory, resulting in fewer observable clusters and corresponding to random motion. A core component of this work involves machine learning techniques. Applying three different neural networks - Autoencoder, U-Net, and Generative Adversarial Network (GAN) - to synthesize preferential concentration fields of impurities, we use vorticity as input and predict impurity number density fields. GAN outperforms the two others by aligning closely with direct numerical simulation data in terms of probability density functions of the particle distribution and energy spectra. This machine learning technique holds the potential to reduce computational costs by eliminating the need to track millions of particles modeling impurities in simulations.","sentences":["This work investigates the behavior of impurities in edge plasma of tokamaks using high-resolution numerical simulations based on Hasegawa--Wakatani equations.","Specifically, it focuses on the behavior of inertial particles, which has not been extensively studied in the field of plasma physics.","Our simulations utilize one-way coupling of a large number of inertial point particles, which model plasma impurities.","We observe that with Stokes number ($St$) which characterizes the inertia of particles being much less than one, such light impurities closely track the fluid flow without pronounced clustering.","For intermediate $St$ values, distinct clustering appears, with larger Stokes values, {\\it i.e.} heavy impurities even generating more substantial clusters.","When $St$ is significantly large, very heavy impurities tend to detach from the flow and maintain their trajectory, resulting in fewer observable clusters and corresponding to random motion.","A core component of this work involves machine learning techniques.","Applying three different neural networks - Autoencoder, U-Net, and Generative Adversarial Network (GAN) - to synthesize preferential concentration fields of impurities, we use vorticity as input and predict impurity number density fields.","GAN outperforms the two others by aligning closely with direct numerical simulation data in terms of probability density functions of the particle distribution and energy spectra.","This machine learning technique holds the potential to reduce computational costs by eliminating the need to track millions of particles modeling impurities in simulations."],"url":"http://arxiv.org/abs/2403.10219v1","category":"physics.plasm-ph"}
{"created":"2024-03-15 11:37:50","title":"Structure, control, and dynamics of altermagnetic textures","abstract":"We present a phenomenological theory of altermagnets, that captures their unique magnetization dynamics and allows modelling magnetic textures in this new magnetic phase. Focusing on the prototypical d-wave altermagnets, e.g. RuO$_2$, we can explain intuitively the characteristic lifted degeneracy of their magnon spectra, by the emergence of an effective sublattice-dependent anisotropic spin stiffness arising naturally from the phenomenological theory. We show that as a consequence the altermagnetic domain walls, in contrast to antiferromagnets, have a finite gradient of the magnetization, with its strength and gradient direction connected to the altermagnetic anisotropy, even for 180$^\\circ$ domain walls. This gradient generates a ponderomotive force in the domain wall in the presence of a strongly inhomogeneous external magnetic field, which may be achieved through magnetic force microscopy techniques. The motion of these altermagentic domain walls is also characterized by an anisotropic Walker breakdown, with much higher speed limits of propagation than ferromagnets but lower than antiferromagnets.","sentences":["We present a phenomenological theory of altermagnets, that captures their unique magnetization dynamics and allows modelling magnetic textures in this new magnetic phase.","Focusing on the prototypical d-wave altermagnets, e.g. RuO$_2$, we can explain intuitively the characteristic lifted degeneracy of their magnon spectra, by the emergence of an effective sublattice-dependent anisotropic spin stiffness arising naturally from the phenomenological theory.","We show that as a consequence the altermagnetic domain walls, in contrast to antiferromagnets, have a finite gradient of the magnetization, with its strength and gradient direction connected to the altermagnetic anisotropy, even for 180$^\\circ$ domain walls.","This gradient generates a ponderomotive force in the domain wall in the presence of a strongly inhomogeneous external magnetic field, which may be achieved through magnetic force microscopy techniques.","The motion of these altermagentic domain walls is also characterized by an anisotropic Walker breakdown, with much higher speed limits of propagation than ferromagnets but lower than antiferromagnets."],"url":"http://arxiv.org/abs/2403.10218v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 11:36:26","title":"Exploring Optical Flow Inclusion into nnU-Net Framework for Surgical Instrument Segmentation","abstract":"Surgical instrument segmentation in laparoscopy is essential for computer-assisted surgical systems. Despite the Deep Learning progress in recent years, the dynamic setting of laparoscopic surgery still presents challenges for precise segmentation. The nnU-Net framework excelled in semantic segmentation analyzing single frames without temporal information. The framework's ease of use, including its ability to be automatically configured, and its low expertise requirements, have made it a popular base framework for comparisons. Optical flow (OF) is a tool commonly used in video tasks to estimate motion and represent it in a single frame, containing temporal information. This work seeks to employ OF maps as an additional input to the nnU-Net architecture to improve its performance in the surgical instrument segmentation task, taking advantage of the fact that instruments are the main moving objects in the surgical field. With this new input, the temporal component would be indirectly added without modifying the architecture. Using CholecSeg8k dataset, three different representations of movement were estimated and used as new inputs, comparing them with a baseline model. Results showed that the use of OF maps improves the detection of classes with high movement, even when these are scarce in the dataset. To further improve performance, future work may focus on implementing other OF-preserving augmentations.","sentences":["Surgical instrument segmentation in laparoscopy is essential for computer-assisted surgical systems.","Despite the Deep Learning progress in recent years, the dynamic setting of laparoscopic surgery still presents challenges for precise segmentation.","The nnU-Net framework excelled in semantic segmentation analyzing single frames without temporal information.","The framework's ease of use, including its ability to be automatically configured, and its low expertise requirements, have made it a popular base framework for comparisons.","Optical flow (OF) is a tool commonly used in video tasks to estimate motion and represent it in a single frame, containing temporal information.","This work seeks to employ OF maps as an additional input to the nnU-Net architecture to improve its performance in the surgical instrument segmentation task, taking advantage of the fact that instruments are the main moving objects in the surgical field.","With this new input, the temporal component would be indirectly added without modifying the architecture.","Using CholecSeg8k dataset, three different representations of movement were estimated and used as new inputs, comparing them with a baseline model.","Results showed that the use of OF maps improves the detection of classes with high movement, even when these are scarce in the dataset.","To further improve performance, future work may focus on implementing other OF-preserving augmentations."],"url":"http://arxiv.org/abs/2403.10216v1","category":"cs.CV"}
{"created":"2024-03-15 11:35:58","title":"Non-adiabatic particle production scenario in algebraically coupled quintessence field with dark matter fluid","abstract":"We investigate the dynamics of an algebraically coupled quintessence field with a dark matter fluid, considering a scenario involving non-adiabatic particle production, through the action principle by modifying the interaction Lagrangian. The interaction parameter serves as the source of dark matter particle and entropy production. As particle creation occurs due to the interaction between the field and fluid sectors, the system manifests an additional pressure. Our analysis includes studying the system's dynamics by considering an exponential type of interaction corresponding to the field's exponential potential. We find that the system exhibits phantom behavior at the current epoch before stabilizing in the accelerating future epoch of the universe.","sentences":["We investigate the dynamics of an algebraically coupled quintessence field with a dark matter fluid, considering a scenario involving non-adiabatic particle production, through the action principle by modifying the interaction Lagrangian.","The interaction parameter serves as the source of dark matter particle and entropy production.","As particle creation occurs due to the interaction between the field and fluid sectors, the system manifests an additional pressure.","Our analysis includes studying the system's dynamics by considering an exponential type of interaction corresponding to the field's exponential potential.","We find that the system exhibits phantom behavior at the current epoch before stabilizing in the accelerating future epoch of the universe."],"url":"http://arxiv.org/abs/2403.10215v1","category":"gr-qc"}
{"created":"2024-03-15 11:15:31","title":"Genuine non-Gaussian entanglement of light and quantum coherence for an atom from noisy multiphoton spin-boson interactions","abstract":"Harnessing entanglement and quantum coherence plays a central role in advancing quantum technologies. In quantum optical light-atom platforms, these two fundamental resources are often associated with a Jaynes-Cummings model description describing the coherent exchange of a photon between an optical resonator mode and a two-level spin. In a generic nonlinear spin-boson system, more photons and more modes will take part in the interactions. Here we consider such a generalisation -- the two-mode multiphoton Jaynes-Cummings (MPJC) model. We show how entanglement and quantum coherence can be optimally generated and subsequently manipulated with it in experimentally accessible parameter regimes. A detailed comparative analysis of this model reveals that nonlinearities within the MPJC interactions produce genuinely non-Gaussian entanglement, devoid of Gaussian contributions, from noisy resources. More specifically, strong coherent sources may be replaced by weaker, incoherent ones, significantly reducing the resource overhead, though at the expense of reduced efficiency. At the same time, increasing the multiphoton order of the MPJC interactions expedites the entanglement generation process, thus rendering the whole generation scheme again more efficient and robust. We further explore the use of additional dispersive spin-boson interactions and Kerr nonlinearities in order to create spin coherence solely from incoherent sources and to enhance the quantum correlations, respectively. As for the latter, somewhat unexpectedly, there is not necessarily an increase in quantum correlations due to the augmented nonlinearity. Towards possible applications of the MPJC model, we demonstrate how to engineer arbitrary NOON states with appropriately chosen experimental parameters.","sentences":["Harnessing entanglement and quantum coherence plays a central role in advancing quantum technologies.","In quantum optical light-atom platforms, these two fundamental resources are often associated with a Jaynes-Cummings model description describing the coherent exchange of a photon between an optical resonator mode and a two-level spin.","In a generic nonlinear spin-boson system, more photons and more modes will take part in the interactions.","Here we consider such a generalisation -- the two-mode multiphoton Jaynes-Cummings (MPJC) model.","We show how entanglement and quantum coherence can be optimally generated and subsequently manipulated with it in experimentally accessible parameter regimes.","A detailed comparative analysis of this model reveals that nonlinearities within the MPJC interactions produce genuinely non-Gaussian entanglement, devoid of Gaussian contributions, from noisy resources.","More specifically, strong coherent sources may be replaced by weaker, incoherent ones, significantly reducing the resource overhead, though at the expense of reduced efficiency.","At the same time, increasing the multiphoton order of the MPJC interactions expedites the entanglement generation process, thus rendering the whole generation scheme again more efficient and robust.","We further explore the use of additional dispersive spin-boson interactions and Kerr nonlinearities in order to create spin coherence solely from incoherent sources and to enhance the quantum correlations, respectively.","As for the latter, somewhat unexpectedly, there is not necessarily an increase in quantum correlations due to the augmented nonlinearity.","Towards possible applications of the MPJC model, we demonstrate how to engineer arbitrary NOON states with appropriately chosen experimental parameters."],"url":"http://arxiv.org/abs/2403.10207v1","category":"quant-ph"}
{"created":"2024-03-15 11:11:57","title":"Read between the lines -- Functionality Extraction From READMEs","abstract":"While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively.","sentences":["While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files.","Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful.","The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc.","We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task.","Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard.","Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively."],"url":"http://arxiv.org/abs/2403.10205v1","category":"cs.CL"}
{"created":"2024-03-15 11:10:49","title":"Effective polygonal mesh generation and refinement for VEM","abstract":"In the present work we introduce a novel refinement algorithm for two-dimensional elliptic partial differential equations discretized with Virtual Element Method (VEM). The algorithm improves the numerical solution accuracy and the mesh quality through a controlled refinement strategy applied to the generic polygonal elements of the domain tessellation. The numerical results show that the outlined strategy proves to be versatile and applicable to any two-dimensional problem where polygonal meshes offer advantages. In particular, we focus on the simulation of flow in fractured media, specifically using the Discrete Fracture Network (DFN) model. A residual a-posteriori error estimator tailored for the DFN case is employed. We chose this particular application to emphasize the effectiveness of the algorithm in handling complex geometries. All the numerical tests demonstrate optimal convergence rates for all the tested VEM orders.","sentences":["In the present work we introduce a novel refinement algorithm for two-dimensional elliptic partial differential equations discretized with Virtual Element Method (VEM).","The algorithm improves the numerical solution accuracy and the mesh quality through a controlled refinement strategy applied to the generic polygonal elements of the domain tessellation.","The numerical results show that the outlined strategy proves to be versatile and applicable to any two-dimensional problem where polygonal meshes offer advantages.","In particular, we focus on the simulation of flow in fractured media, specifically using the Discrete Fracture Network (DFN) model.","A residual a-posteriori error estimator tailored for the DFN case is employed.","We chose this particular application to emphasize the effectiveness of the algorithm in handling complex geometries.","All the numerical tests demonstrate optimal convergence rates for all the tested VEM orders."],"url":"http://arxiv.org/abs/2403.10203v1","category":"math.NA"}
{"created":"2024-03-15 11:07:38","title":"Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes","abstract":"In goal-oriented communications, the objective of the receiver is often to apply a Deep-Learning model, rather than reconstructing the original data. In this context, direct learning over compressed data, without any prior decoding, holds promise for enhancing the time-efficient execution of inference models at the receiver. However, conventional entropic-coding methods like Huffman and Arithmetic break data structure, rendering them unsuitable for learning without decoding. In this paper, we propose an alternative approach in which entropic coding is realized with Low-Density Parity Check (LDPC) codes. We hypothesize that Deep Learning models can more effectively exploit the internal code structure of LDPC codes. At the receiver, we leverage a specific class of Recurrent Neural Networks (RNNs), specifically Gated Recurrent Unit (GRU), trained for image classification. Our numerical results indicate that classification based on LDPC-coded bit-planes surpasses Huffman and Arithmetic coding, while necessitating a significantly smaller learning model. This demonstrates the efficiency of classification directly from LDPC-coded data, eliminating the need for any form of decompression, even partial, prior to applying the learning model.","sentences":["In goal-oriented communications, the objective of the receiver is often to apply a Deep-Learning model, rather than reconstructing the original data.","In this context, direct learning over compressed data, without any prior decoding, holds promise for enhancing the time-efficient execution of inference models at the receiver.","However, conventional entropic-coding methods like Huffman and Arithmetic break data structure, rendering them unsuitable for learning without decoding.","In this paper, we propose an alternative approach in which entropic coding is realized with Low-Density Parity Check (LDPC) codes.","We hypothesize that Deep Learning models can more effectively exploit the internal code structure of LDPC codes.","At the receiver, we leverage a specific class of Recurrent Neural Networks (RNNs), specifically Gated Recurrent Unit (GRU), trained for image classification.","Our numerical results indicate that classification based on LDPC-coded bit-planes surpasses Huffman and Arithmetic coding, while necessitating a significantly smaller learning model.","This demonstrates the efficiency of classification directly from LDPC-coded data, eliminating the need for any form of decompression, even partial, prior to applying the learning model."],"url":"http://arxiv.org/abs/2403.10202v1","category":"eess.IV"}
{"created":"2024-03-15 10:52:39","title":"Generative Region-Language Pretraining for Open-Ended Object Detection","abstract":"In recent research, significant attention has been devoted to the open-vocabulary object detection task, aiming to generalize beyond the limited number of classes labeled during training and detect objects described by arbitrary category names at inference. Compared with conventional object detection, open vocabulary object detection largely extends the object detection categories. However, it relies on calculating the similarity between image regions and a set of arbitrary category names with a pretrained vision-and-language model. This implies that, despite its open-set nature, the task still needs the predefined object categories during the inference stage. This raises the question: What if we do not have exact knowledge of object categories during inference? In this paper, we call such a new setting as generative open-ended object detection, which is a more general and practical problem. To address it, we formulate object detection as a generative problem and propose a simple framework named GenerateU, which can detect dense objects and generate their names in a free-form way. Particularly, we employ Deformable DETR as a region proposal generator with a language model translating visual regions to object names. To assess the free-form object detection task, we introduce an evaluation method designed to quantitatively measure the performance of generative outcomes. Extensive experiments demonstrate strong zero-shot detection performance of our GenerateU. For example, on the LVIS dataset, our GenerateU achieves comparable results to the open-vocabulary object detection method GLIP, even though the category names are not seen by GenerateU during inference. Code is available at: https:// github.com/FoundationVision/GenerateU .","sentences":["In recent research, significant attention has been devoted to the open-vocabulary object detection task, aiming to generalize beyond the limited number of classes labeled during training and detect objects described by arbitrary category names at inference.","Compared with conventional object detection, open vocabulary object detection largely extends the object detection categories.","However, it relies on calculating the similarity between image regions and a set of arbitrary category names with a pretrained vision-and-language model.","This implies that, despite its open-set nature, the task still needs the predefined object categories during the inference stage.","This raises the question: What if we do not have exact knowledge of object categories during inference?","In this paper, we call such a new setting as generative open-ended object detection, which is a more general and practical problem.","To address it, we formulate object detection as a generative problem and propose a simple framework named GenerateU, which can detect dense objects and generate their names in a free-form way.","Particularly, we employ Deformable DETR as a region proposal generator with a language model translating visual regions to object names.","To assess the free-form object detection task, we introduce an evaluation method designed to quantitatively measure the performance of generative outcomes.","Extensive experiments demonstrate strong zero-shot detection performance of our GenerateU. For example, on the LVIS dataset, our GenerateU achieves comparable results to the open-vocabulary object detection method GLIP, even though the category names are not seen by GenerateU during inference.","Code is available at: https:// github.com/FoundationVision/GenerateU ."],"url":"http://arxiv.org/abs/2403.10191v1","category":"cs.CV"}
{"created":"2024-03-15 10:52:18","title":"Perceptual Quality-based Model Training under Annotator Label Uncertainty","abstract":"Annotators exhibit disagreement during data labeling, which can be termed as annotator label uncertainty. Annotator label uncertainty manifests in variations of labeling quality. Training with a single low-quality annotation per sample induces model reliability degradations. In this work, we first examine the effects of annotator label uncertainty in terms of the model's generalizability and prediction uncertainty. We observe that the model's generalizability and prediction uncertainty degrade with the presence of low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty estimation algorithms indicates their incapability in response to annotator label uncertainty. To mitigate performance degradation, prior methods show that training models with labels collected from multiple independent annotators can enhance generalizability. However, they require massive annotations. Hence, we introduce a novel perceptual quality-based model training framework to objectively generate multiple labels for model training to enhance reliability, while avoiding massive annotations. Specifically, we first select a subset of samples with low perceptual quality scores ranked by statistical regularities of visual signals. We then assign de-aggregated labels to each sample in this subset to obtain a training set with multiple labels. Our experiments and analysis demonstrate that training with the proposed framework alleviates the degradation of generalizability and prediction uncertainty caused by annotator label uncertainty.","sentences":["Annotators exhibit disagreement during data labeling, which can be termed as annotator label uncertainty.","Annotator label uncertainty manifests in variations of labeling quality.","Training with a single low-quality annotation per sample induces model reliability degradations.","In this work, we first examine the effects of annotator label uncertainty in terms of the model's generalizability and prediction uncertainty.","We observe that the model's generalizability and prediction uncertainty degrade with the presence of low-quality noisy labels.","Meanwhile, our evaluation of existing uncertainty estimation algorithms indicates their incapability in response to annotator label uncertainty.","To mitigate performance degradation, prior methods show that training models with labels collected from multiple independent annotators can enhance generalizability.","However, they require massive annotations.","Hence, we introduce a novel perceptual quality-based model training framework to objectively generate multiple labels for model training to enhance reliability, while avoiding massive annotations.","Specifically, we first select a subset of samples with low perceptual quality scores ranked by statistical regularities of visual signals.","We then assign de-aggregated labels to each sample in this subset to obtain a training set with multiple labels.","Our experiments and analysis demonstrate that training with the proposed framework alleviates the degradation of generalizability and prediction uncertainty caused by annotator label uncertainty."],"url":"http://arxiv.org/abs/2403.10190v1","category":"cs.CV"}
{"created":"2024-03-15 10:48:16","title":"Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects","abstract":"Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfer to novel objects. Videos of our experiments are available at \\url{https://maltemosbach.github.io/grasp_anything}.","sentences":["Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning.","Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two.","In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation.","After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation.","We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation.","Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts.","Furthermore, we show robust zero-shot transfer to novel objects.","Videos of our experiments are available at \\url{https://maltemosbach.github.io/grasp_anything}."],"url":"http://arxiv.org/abs/2403.10187v1","category":"cs.RO"}
{"created":"2024-03-15 10:46:00","title":"Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based Deception","abstract":"If a person firmly believes in a non-factual statement, such as \"The Earth is flat\", and argues in its favor, there is no inherent intention to deceive. As the argumentation stems from genuine belief, it may be unlikely to exhibit the linguistic properties associated with deception or lying. This interplay of factuality, personal belief, and intent to deceive remains an understudied area. Disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them. To study the relation between deception and factuality, based on belief, we present the DeFaBel corpus, a crowd-sourced resource of belief-based deception. To create this corpus, we devise a study in which participants are instructed to write arguments supporting statements like \"eating watermelon seeds can cause indigestion\", regardless of its factual accuracy or their personal beliefs about the statement. In addition to the generation task, we ask them to disclose their belief about the statement. The collected instances are labelled as deceptive if the arguments are in contradiction to the participants' personal beliefs. Each instance in the corpus is thus annotated (or implicitly labelled) with personal beliefs of the author, factuality of the statement, and the intended deceptiveness. The DeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive and 388 are non-deceptive. It is the first publicly available corpus for studying deception in German. In our analysis, we find that people are more confident in the persuasiveness of their arguments when the statement is aligned with their belief, but surprisingly less confident when they are generating arguments in favor of facts. The DeFaBel corpus can be obtained from https://www.ims.uni-stuttgart.de/data/defabel","sentences":["If a person firmly believes in a non-factual statement, such as \"The Earth is flat\", and argues in its favor, there is no inherent intention to deceive.","As the argumentation stems from genuine belief, it may be unlikely to exhibit the linguistic properties associated with deception or lying.","This interplay of factuality, personal belief, and intent to deceive remains an understudied area.","Disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them.","To study the relation between deception and factuality, based on belief, we present the DeFaBel corpus, a crowd-sourced resource of belief-based deception.","To create this corpus, we devise a study in which participants are instructed to write arguments supporting statements like \"eating watermelon seeds can cause indigestion\", regardless of its factual accuracy or their personal beliefs about the statement.","In addition to the generation task, we ask them to disclose their belief about the statement.","The collected instances are labelled as deceptive if the arguments are in contradiction to the participants' personal beliefs.","Each instance in the corpus is thus annotated (or implicitly labelled) with personal beliefs of the author, factuality of the statement, and the intended deceptiveness.","The DeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive and 388 are non-deceptive.","It is the first publicly available corpus for studying deception in German.","In our analysis, we find that people are more confident in the persuasiveness of their arguments when the statement is aligned with their belief, but surprisingly less confident when they are generating arguments in favor of facts.","The DeFaBel corpus can be obtained from https://www.ims.uni-stuttgart.de/data/defabel"],"url":"http://arxiv.org/abs/2403.10185v1","category":"cs.CL"}
{"created":"2024-03-15 10:44:27","title":"Lifted Causal Inference in Relational Domains","abstract":"Lifted inference exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, thereby speeding up query answering while maintaining exact answers. Even though lifting is a well-established technique for the task of probabilistic inference in relational domains, it has not yet been applied to the task of causal inference. In this paper, we show how lifting can be applied to efficiently compute causal effects in relational domains. More specifically, we introduce parametric causal factor graphs as an extension of parametric factor graphs incorporating causal knowledge and give a formal semantics of interventions therein. We further present the lifted causal inference algorithm to compute causal effects on a lifted level, thereby drastically speeding up causal inference compared to propositional inference, e.g., in causal Bayesian networks. In our empirical evaluation, we demonstrate the effectiveness of our approach.","sentences":["Lifted inference exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, thereby speeding up query answering while maintaining exact answers.","Even though lifting is a well-established technique for the task of probabilistic inference in relational domains, it has not yet been applied to the task of causal inference.","In this paper, we show how lifting can be applied to efficiently compute causal effects in relational domains.","More specifically, we introduce parametric causal factor graphs as an extension of parametric factor graphs incorporating causal knowledge and give a formal semantics of interventions therein.","We further present the lifted causal inference algorithm to compute causal effects on a lifted level, thereby drastically speeding up causal inference compared to propositional inference, e.g., in causal Bayesian networks.","In our empirical evaluation, we demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.10184v1","category":"cs.AI"}
{"created":"2024-03-15 10:42:22","title":"Spectral CT Two-step and One-step Material Decomposition using Diffusion Posterior Sampling","abstract":"This paper proposes a novel approach to spectral computed tomography (CT) material decomposition that uses the recent advances in generative diffusion models (DMs) for inverse problems. Spectral CT and more particularly photon-counting CT (PCCT) can perform transmission measurements at different energy levels which can be used for material decomposition. It is an ill-posed inverse problem and therefore requires regularization. DMs are a class of generative model that can be used to solve inverse problems via diffusion posterior sampling (DPS). In this paper we adapt DPS for material decomposition in a PCCT setting. We propose two approaches, namely Two-step Diffusion Posterior Sampling (TDPS) and One-step Diffusion Posterior Sampling (ODPS). Early results from an experiment with simulated low-dose PCCT suggest that DPSs have the potential to outperform state-of-the-art model-based iterative reconstruction (MBIR). Moreover, our results indicate that TDPS produces material images with better peak signal-to-noise ratio (PSNR) than images produced with ODPS with similar structural similarity (SSIM).","sentences":["This paper proposes a novel approach to spectral computed tomography (CT) material decomposition that uses the recent advances in generative diffusion models (DMs) for inverse problems.","Spectral CT and more particularly photon-counting","CT (PCCT) can perform transmission measurements at different energy levels which can be used for material decomposition.","It is an ill-posed inverse problem and therefore requires regularization.","DMs are a class of generative model that can be used to solve inverse problems via diffusion posterior sampling (DPS).","In this paper we adapt DPS for material decomposition in a PCCT setting.","We propose two approaches, namely Two-step Diffusion Posterior Sampling (TDPS) and One-step Diffusion Posterior Sampling (ODPS).","Early results from an experiment with simulated low-dose PCCT suggest that DPSs have the potential to outperform state-of-the-art model-based iterative reconstruction (MBIR).","Moreover, our results indicate that TDPS produces material images with better peak signal-to-noise ratio (PSNR) than images produced with ODPS with similar structural similarity (SSIM)."],"url":"http://arxiv.org/abs/2403.10183v1","category":"physics.med-ph"}
{"created":"2024-03-15 10:36:31","title":"An efficient asymptotic DC method for sparse and low-rank matrix recovery","abstract":"The optimization problem of sparse and low-rank matrix recovery is considered, which involves a least squares problem with a rank constraint and a cardinality constraint. To overcome the challenges posed by these constraints, an asymptotic difference-of-convex (ADC) method that employs a Moreau smoothing approach and an exact penalty approach is proposed to transform this problem into a DC programming format gradually. To solve the gained DC programming, by making full use of its DC structure, an efficient inexact DC algorithm with sieving strategy (siDCA) is introduced. The subproblem of siDCA is solved by an efficient dual-based semismooth Newton method. The convergence of the solution sequence generated by siDCA is proved. To illustrate the effectiveness of ADC-siDCA, matrix recovery experiments on nonnegative and positive semidefinite matrices. The numerical results are compared with those obtained using a successive DC approximation minimization method and a penalty proximal alternating linearized minimization approach. The outcome of the comparison indicates that ADC-siDCA surpasses the other two methods in terms of efficiency and recovery error. Additionally, numerical experiments on sparse phase retrieval demonstrate that ADC-siDCA is a valuable tool for recovering sparse and low-rank Hermitian matrices.","sentences":["The optimization problem of sparse and low-rank matrix recovery is considered, which involves a least squares problem with a rank constraint and a cardinality constraint.","To overcome the challenges posed by these constraints, an asymptotic difference-of-convex (ADC) method that employs a Moreau smoothing approach and an exact penalty approach is proposed to transform this problem into a DC programming format gradually.","To solve the gained DC programming, by making full use of its DC structure, an efficient inexact DC algorithm with sieving strategy (siDCA) is introduced.","The subproblem of siDCA is solved by an efficient dual-based semismooth Newton method.","The convergence of the solution sequence generated by siDCA is proved.","To illustrate the effectiveness of ADC-siDCA, matrix recovery experiments on nonnegative and positive semidefinite matrices.","The numerical results are compared with those obtained using a successive DC approximation minimization method and a penalty proximal alternating linearized minimization approach.","The outcome of the comparison indicates that ADC-siDCA surpasses the other two methods in terms of efficiency and recovery error.","Additionally, numerical experiments on sparse phase retrieval demonstrate that ADC-siDCA is a valuable tool for recovering sparse and low-rank Hermitian matrices."],"url":"http://arxiv.org/abs/2403.10180v1","category":"math.OC"}
{"created":"2024-03-15 10:36:24","title":"Animate Your Motion: Turning Still Images into Dynamic Videos","abstract":"In recent years, diffusion models have made remarkable strides in text-to-video generation, sparking a quest for enhanced control over video outputs to more accurately reflect user intentions. Traditional efforts predominantly focus on employing either semantic cues, like images or depth maps, or motion-based conditions, like moving sketches or object bounding boxes. Semantic inputs offer a rich scene context but lack detailed motion specificity; conversely, motion inputs provide precise trajectory information but miss the broader semantic narrative. For the first time, we integrate both semantic and motion cues within a diffusion model for video generation, as demonstrated in Fig 1. To this end, we introduce the Scene and Motion Conditional Diffusion (SMCD), a novel methodology for managing multimodal inputs. It incorporates a recognized motion conditioning module and investigates various approaches to integrate scene conditions, promoting synergy between different modalities. For model training, we separate the conditions for the two modalities, introducing a two-stage training pipeline. Experimental results demonstrate that our design significantly enhances video quality, motion precision, and semantic coherence.","sentences":["In recent years, diffusion models have made remarkable strides in text-to-video generation, sparking a quest for enhanced control over video outputs to more accurately reflect user intentions.","Traditional efforts predominantly focus on employing either semantic cues, like images or depth maps, or motion-based conditions, like moving sketches or object bounding boxes.","Semantic inputs offer a rich scene context but lack detailed motion specificity; conversely, motion inputs provide precise trajectory information but miss the broader semantic narrative.","For the first time, we integrate both semantic and motion cues within a diffusion model for video generation, as demonstrated in Fig 1.","To this end, we introduce the Scene and Motion Conditional Diffusion (SMCD), a novel methodology for managing multimodal inputs.","It incorporates a recognized motion conditioning module and investigates various approaches to integrate scene conditions, promoting synergy between different modalities.","For model training, we separate the conditions for the two modalities, introducing a two-stage training pipeline.","Experimental results demonstrate that our design significantly enhances video quality, motion precision, and semantic coherence."],"url":"http://arxiv.org/abs/2403.10179v1","category":"cs.CV"}
{"created":"2024-03-15 10:32:06","title":"Expected performance of the Pyramid wavefront sensor with a laser guide star for 40 m class telescopes","abstract":"The use of artificial Laser Guide Stars (LGS) is planned for the new generation of giant segmented mirror telescopes, to extend the sky coverage of their adaptive optics systems. The LGS, being a 3D object at a finite distance will have a large elongation that will affect its use with the Shack-Hartmann (SH) wavefront sensor. In this paper, we compute the expected performance for a Pyramid WaveFront Sensor (PWFS) using a LGS for a 40 m telescope affected by photon noise, and also extend the analysis to a flat 2D object as reference. We developed a new way to discretize the LGS, and a new, faster method of propagating the light for any Fourier Filtering wavefront sensors (FFWFS) when using extended objects. We present the use of a sensitivity model to predict the performance of a closed-loop adaptive optic system. We optimized a point source calibrated interaction matrix to accommodate the signal of an extended object, by means of computing optical gains using a convolutional model. We found that the sensitivity drop, given the size of the extended laser source, is large enough to make the system operate in a low-performance regime given the expected return flux of the LGS. The width of the laser beam, rather than the thickness of the sodium layer was identified as the limiting factor. Even an ideal, flat LGS will have a drop in performance due to the flux of the LGS, and small variations in the return flux will result in large variations in performance. We conclude that knife-edge-like wavefront sensors, such as the PWFS, are not recommended for their use with LGS for a 40 m telescope, as they will operate in a low-performance regime, given the size of the extended object.","sentences":["The use of artificial Laser Guide Stars (LGS) is planned for the new generation of giant segmented mirror telescopes, to extend the sky coverage of their adaptive optics systems.","The LGS, being a 3D object at a finite distance will have a large elongation that will affect its use with the Shack-Hartmann (SH) wavefront sensor.","In this paper, we compute the expected performance for a Pyramid WaveFront Sensor (PWFS) using a LGS for a 40 m telescope affected by photon noise, and also extend the analysis to a flat 2D object as reference.","We developed a new way to discretize the LGS, and a new, faster method of propagating the light for any Fourier Filtering wavefront sensors (FFWFS) when using extended objects.","We present the use of a sensitivity model to predict the performance of a closed-loop adaptive optic system.","We optimized a point source calibrated interaction matrix to accommodate the signal of an extended object, by means of computing optical gains using a convolutional model.","We found that the sensitivity drop, given the size of the extended laser source, is large enough to make the system operate in a low-performance regime given the expected return flux of the LGS.","The width of the laser beam, rather than the thickness of the sodium layer was identified as the limiting factor.","Even an ideal, flat LGS will have a drop in performance due to the flux of the LGS, and small variations in the return flux will result in large variations in performance.","We conclude that knife-edge-like wavefront sensors, such as the PWFS, are not recommended for their use with LGS for a 40 m telescope, as they will operate in a low-performance regime, given the size of the extended object."],"url":"http://arxiv.org/abs/2403.10177v1","category":"astro-ph.IM"}
{"created":"2024-03-15 10:31:54","title":"Two-step aging dynamics in enzymatic milk gels","abstract":"Colloidal gels undergo a phenomenon known as physical aging, i.e., a continuous change of their physical properties with time after the gel point. To date, most of the research effort on aging in gels has been focused on suspensions of hard colloidal particles. In this letter, we tackle the case of soft colloidal \"micelles\" comprised of proteins, where gelation is induced by the addition of an enzyme. Using time-resolved mechanical spectroscopy, we monitor the viscoelastic properties of a suspension of colloidal micelles through the sol-gel transition and its subsequent aging. We show that the microscopic scenario underpinning the macroscopic aging dynamics comprises two sequential steps. First, the gel microstructure undergoes rapid coarsening, as observed by optical microscopy, followed by arrest. Second, aging occurs solely through a contact-driven mechanism, as evidenced by the square-root dependence of the yield stress with the elastic modulus measured at different ages of the gel. These results provide a comprehensive understanding of aging in enzymatic milk gels, which is crucial not only for a broad range of dairy products, but also for soft colloids in general.","sentences":["Colloidal gels undergo a phenomenon known as physical aging, i.e., a continuous change of their physical properties with time after the gel point.","To date, most of the research effort on aging in gels has been focused on suspensions of hard colloidal particles.","In this letter, we tackle the case of soft colloidal \"micelles\" comprised of proteins, where gelation is induced by the addition of an enzyme.","Using time-resolved mechanical spectroscopy, we monitor the viscoelastic properties of a suspension of colloidal micelles through the sol-gel transition and its subsequent aging.","We show that the microscopic scenario underpinning the macroscopic aging dynamics comprises two sequential steps.","First, the gel microstructure undergoes rapid coarsening, as observed by optical microscopy, followed by arrest.","Second, aging occurs solely through a contact-driven mechanism, as evidenced by the square-root dependence of the yield stress with the elastic modulus measured at different ages of the gel.","These results provide a comprehensive understanding of aging in enzymatic milk gels, which is crucial not only for a broad range of dairy products, but also for soft colloids in general."],"url":"http://arxiv.org/abs/2403.10176v1","category":"cond-mat.soft"}
{"created":"2024-03-15 10:31:46","title":"A Short Survey on Importance Weighting for Machine Learning","abstract":"Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.","sentences":["Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense.","The simplicity and usefulness of the idea has led to many applications of importance weighting.","For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio.","This survey summarizes the broad applications of importance weighting in machine learning and related research."],"url":"http://arxiv.org/abs/2403.10175v1","category":"cs.LG"}
{"created":"2024-03-15 10:28:31","title":"A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention","abstract":"Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed method surpasses baseline hybrid and SNN-based approaches by significant margins, with results comparable to existing ANN-based methods. Extensive ablation studies confirm the effectiveness of our proposed modules and architectural choices. These results pave the way toward a hybrid SNN-ANN architecture that achieves ANN like performance at a drastically reduced parameter budget. We implemented the SNN blocks on digital neuromorphic hardware to investigate latency and power consumption and demonstrate the feasibility of our approach.","sentences":["Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks.","While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance.","Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures.","In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras.","We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone.","Experimental results demonstrate that our proposed method surpasses baseline hybrid and SNN-based approaches by significant margins, with results comparable to existing ANN-based methods.","Extensive ablation studies confirm the effectiveness of our proposed modules and architectural choices.","These results pave the way toward a hybrid SNN-ANN architecture that achieves ANN like performance at a drastically reduced parameter budget.","We implemented the SNN blocks on digital neuromorphic hardware to investigate latency and power consumption and demonstrate the feasibility of our approach."],"url":"http://arxiv.org/abs/2403.10173v1","category":"cs.CV"}
{"created":"2024-03-15 10:27:17","title":"AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation","abstract":"In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing.","sentences":["In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning.","This development heralds a new era of scalability and human-like adaptability in goal attainment.","In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration).","AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention.","Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency.","Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience.","We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision.","The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing."],"url":"http://arxiv.org/abs/2403.10171v1","category":"cs.AI"}
{"created":"2024-03-15 10:26:52","title":"Computer User Interface Understanding. A New Dataset and a Learning Framework","abstract":"User Interface (UI) understanding has been an increasingly popular topic over the last few years. So far, there has been a vast focus solely on web and mobile applications. In this paper, we introduce the harder task of computer UI understanding. With the goal of enabling research in this field, we have generated a dataset with a set of videos where a user is performing a sequence of actions and each image shows the desktop contents at that time point. We also present a framework that is composed of a synthetic sample generation pipeline to augment the dataset with relevant characteristics, and a contrastive learning method to classify images in the videos. We take advantage of the natural conditional, tree-like, relationship of the images' characteristics to regularize the learning of the representations by dealing with multiple partial tasks simultaneously. Experimental results show that the proposed framework outperforms previously proposed hierarchical multi-label contrastive losses in fine-grain UI classification.","sentences":["User Interface (UI) understanding has been an increasingly popular topic over the last few years.","So far, there has been a vast focus solely on web and mobile applications.","In this paper, we introduce the harder task of computer UI understanding.","With the goal of enabling research in this field, we have generated a dataset with a set of videos where a user is performing a sequence of actions and each image shows the desktop contents at that time point.","We also present a framework that is composed of a synthetic sample generation pipeline to augment the dataset with relevant characteristics, and a contrastive learning method to classify images in the videos.","We take advantage of the natural conditional, tree-like, relationship of the images' characteristics to regularize the learning of the representations by dealing with multiple partial tasks simultaneously.","Experimental results show that the proposed framework outperforms previously proposed hierarchical multi-label contrastive losses in fine-grain UI classification."],"url":"http://arxiv.org/abs/2403.10170v1","category":"cs.CV"}
{"created":"2024-03-15 10:25:17","title":"Analytical perturbations of relativistic images in Kerr space-time","abstract":"Light rays passing very close to black holes may wind several times before escaping. For any given electromagnetic source around the black hole, a distant observer would thus observe two infinite sequences of images on either side of the black hole. These images are generated by light rays performing an increasing numbers of loops. The strong deflection limit provides a simple analytic formalism to describe such higher order images for spherically symmetric metrics, while for axially symmetric black holes one typically resorts to numerical approaches. Here we present the leading order perturbation to higher order images when the black hole spin is turned on. We show that the images slide around the black hole shadow as an effect of space-time dragging. We derive analytical formulae for their shifts and the perturbation of their time delays. We also discuss how such simple analytical formulae for images by Kerr black holes can be of great help in many applications.","sentences":["Light rays passing very close to black holes may wind several times before escaping.","For any given electromagnetic source around the black hole, a distant observer would thus observe two infinite sequences of images on either side of the black hole.","These images are generated by light rays performing an increasing numbers of loops.","The strong deflection limit provides a simple analytic formalism to describe such higher order images for spherically symmetric metrics, while for axially symmetric black holes one typically resorts to numerical approaches.","Here we present the leading order perturbation to higher order images when the black hole spin is turned on.","We show that the images slide around the black hole shadow as an effect of space-time dragging.","We derive analytical formulae for their shifts and the perturbation of their time delays.","We also discuss how such simple analytical formulae for images by Kerr black holes can be of great help in many applications."],"url":"http://arxiv.org/abs/2403.10169v1","category":"gr-qc"}
{"created":"2024-03-15 10:22:48","title":"Explainability through uncertainty: Trustworthy decision-making with neural networks","abstract":"Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident. This overconfidence is worrying under distribution shifts, where the model performance silently degrades as the data distribution diverges from the training data distribution. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI). Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider distribution shifts. This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific explanations; (ii) classification with rejection is used to reduce misclassifications by bringing a human expert in the loop for uncertain observations; (iii) the framework is applied to a case study on neural networks in educational data mining subject to distribution shifts. Uncertainty as XAI improves the model's trustworthiness in downstream decision-making tasks, giving rise to more actionable and robust machine learning systems in operations research.","sentences":["Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident.","This overconfidence is worrying under distribution shifts, where the model performance silently degrades as the data distribution diverges from the training data distribution.","Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted.","Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI).","Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider distribution shifts.","This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific explanations; (ii) classification with rejection is used to reduce misclassifications by bringing a human expert in the loop for uncertain observations; (iii) the framework is applied to a case study on neural networks in educational data mining subject to distribution shifts.","Uncertainty as XAI improves the model's trustworthiness in downstream decision-making tasks, giving rise to more actionable and robust machine learning systems in operations research."],"url":"http://arxiv.org/abs/2403.10168v1","category":"cs.LG"}
{"created":"2024-03-15 10:20:56","title":"Efficient Detection of Exchangeable Factors in Factor Graphs","abstract":"To allow for tractable probabilistic inference with respect to domain sizes, lifted probabilistic inference exploits symmetries in probabilistic graphical models. However, checking whether two factors encode equivalent semantics and hence are exchangeable is computationally expensive. In this paper, we efficiently solve the problem of detecting exchangeable factors in a factor graph. In particular, we introduce the detection of exchangeable factors (DEFT) algorithm, which allows us to drastically reduce the computational effort for checking whether two factors are exchangeable in practice. While previous approaches iterate all $O(n!)$ permutations of a factor's argument list in the worst case (where $n$ is the number of arguments of the factor), we prove that DEFT efficiently identifies restrictions to drastically reduce the number of permutations and validate the efficiency of DEFT in our empirical evaluation.","sentences":["To allow for tractable probabilistic inference with respect to domain sizes, lifted probabilistic inference exploits symmetries in probabilistic graphical models.","However, checking whether two factors encode equivalent semantics and hence are exchangeable is computationally expensive.","In this paper, we efficiently solve the problem of detecting exchangeable factors in a factor graph.","In particular, we introduce the detection of exchangeable factors (DEFT) algorithm, which allows us to drastically reduce the computational effort for checking whether two factors are exchangeable in practice.","While previous approaches iterate all $O(n!)$ permutations of a factor's argument list in the worst case (where $n$ is the number of arguments of the factor), we prove that DEFT efficiently identifies restrictions to drastically reduce the number of permutations and validate the efficiency of DEFT in our empirical evaluation."],"url":"http://arxiv.org/abs/2403.10167v1","category":"cs.AI"}
{"created":"2024-03-15 10:18:56","title":"SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation","abstract":"With the development of neural radiance fields and generative models, numerous methods have been proposed for learning 3D human generation from 2D images. These methods allow control over the pose of the generated 3D human and enable rendering from different viewpoints. However, none of these methods explore semantic disentanglement in human image synthesis, i.e., they can not disentangle the generation of different semantic parts, such as the body, tops, and bottoms. Furthermore, existing methods are limited to synthesize images at $512^2$ resolution due to the high computational cost of neural radiance fields. To address these limitations, we introduce SemanticHuman-HD, the first method to achieve semantic disentangled human image synthesis. Notably, SemanticHuman-HD is also the first method to achieve 3D-aware image synthesis at $1024^2$ resolution, benefiting from our proposed 3D-aware super-resolution module. By leveraging the depth maps and semantic masks as guidance for the 3D-aware super-resolution, we significantly reduce the number of sampling points during volume rendering, thereby reducing the computational cost. Our comparative experiments demonstrate the superiority of our method. The effectiveness of each proposed component is also verified through ablation studies. Moreover, our method opens up exciting possibilities for various applications, including 3D garment generation, semantic-aware image synthesis, controllable image synthesis, and out-of-domain image synthesis.","sentences":["With the development of neural radiance fields and generative models, numerous methods have been proposed for learning 3D human generation from 2D images.","These methods allow control over the pose of the generated 3D human and enable rendering from different viewpoints.","However, none of these methods explore semantic disentanglement in human image synthesis, i.e., they can not disentangle the generation of different semantic parts, such as the body, tops, and bottoms.","Furthermore, existing methods are limited to synthesize images at $512^2$ resolution due to the high computational cost of neural radiance fields.","To address these limitations, we introduce SemanticHuman-HD, the first method to achieve semantic disentangled human image synthesis.","Notably, SemanticHuman-HD is also the first method to achieve 3D-aware image synthesis at $1024^2$ resolution, benefiting from our proposed 3D-aware super-resolution module.","By leveraging the depth maps and semantic masks as guidance for the 3D-aware super-resolution, we significantly reduce the number of sampling points during volume rendering, thereby reducing the computational cost.","Our comparative experiments demonstrate the superiority of our method.","The effectiveness of each proposed component is also verified through ablation studies.","Moreover, our method opens up exciting possibilities for various applications, including 3D garment generation, semantic-aware image synthesis, controllable image synthesis, and out-of-domain image synthesis."],"url":"http://arxiv.org/abs/2403.10166v1","category":"cs.CV"}
{"created":"2024-03-15 10:18:06","title":"CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis","abstract":"Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and generalizable features that transfer more effectively in related downstream tasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.","sentences":["Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline.","This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance.","However, the end-to-end training pipeline makes the learned representations less explainable.","The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization.","To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression.","Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and generalizable features that transfer more effectively in related downstream tasks.","The code is publicly available at https://github.com/fadamsyah/CoReEcho."],"url":"http://arxiv.org/abs/2403.10164v1","category":"cs.CV"}
{"created":"2024-03-15 10:13:54","title":"Generalized zero-divisor graph of $*$-rings","abstract":"Let $R$ be a ring with involution $*$ and $Z^*(R)$ denotes the set of all non-zero zero-divisors of $R$.   We associate a simple (undirected) graph $\\Gamma'(R)$ with vertex set $Z^*(R)$ and two distinct vertices $x$ and $y$ are adjacent in $\\Gamma'(R)$ if and only if $x^ny^*=0$ or $y^nx^*=0$, for some positive integer $n$. We find the diameter and girth of $\\Gamma'(R)$. The characterizations are obtained for $*$-rings having $\\Gamma'(R)$ a connected graph, a complete graph, and a star graph. Further, we have shown that for a ring $R$, there is an involution on $R\\times R$ such that $\\Gamma'(R\\times R)$ is disconnected if and only if $R$ is an integral domain.","sentences":["Let $R$ be a ring with involution $*$ and $Z^*(R)$ denotes the set of all non-zero zero-divisors of $R$.   We associate a simple (undirected) graph $\\Gamma'(R)$ with vertex set $Z^*(R)$ and two distinct vertices $x$ and $y$ are adjacent in $\\Gamma'(R)$ if and only if $x^ny^*=0$ or $y^nx^*=0$, for some positive integer $n$. We find the diameter and girth of $\\Gamma'(R)$. The characterizations are obtained for $*$-rings having $\\Gamma'(R)$ a connected graph, a complete graph, and a star graph.","Further, we have shown that for a ring $R$, there is an involution on $R\\times R$ such that $\\Gamma'(R\\times R)$ is disconnected if and only if $R$ is an integral domain."],"url":"http://arxiv.org/abs/2403.10161v1","category":"math.CO"}
{"created":"2024-03-15 10:01:19","title":"Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights","abstract":"This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.","sentences":["This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies.","With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes.","Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation.","The efficacy of funGCN is validated through simulation experiments and a real-data application."],"url":"http://arxiv.org/abs/2403.10158v1","category":"cs.LG"}
{"created":"2024-03-15 10:00:00","title":"Complete integrability of subriemannian geodesic flows on $\\mathbb{S}^7$","abstract":"Four subriemannian (SR) structures over the Euclidean sphere $\\mathbb{S}^7$ are considered in accordance to the previous literature. The defining bracket generating distribution is chosen as the horizontal space in the Hopf fibration, the quaternionic Hopf fibration or spanned by a suitable number of canonical vector fields. In all cases the induced SR geodesic flow on $T^*\\mathbb{S}^7$ is studied. Adapting a method by A. Thimm, a maximal set of functionally independent and Poisson commuting first integrals are constructed, including the corresponding SR Hamiltonian. As a result, the complete integrability in the sense of Liouville is proved for the SR geodesic flow. It is observed that these first integrals arise as the symbols of commuting second order differential operators one of them being a (not necessarily intrinsic) sublaplacian. On the way one explicitly derives the Lie algebras of all SR isometry groups intersected with $O(8)$.","sentences":["Four subriemannian (SR) structures over the Euclidean sphere $\\mathbb{S}^7$ are considered in accordance to the previous literature.","The defining bracket generating distribution is chosen as the horizontal space in the Hopf fibration, the quaternionic Hopf fibration or spanned by a suitable number of canonical vector fields.","In all cases the induced SR geodesic flow on $T^*\\mathbb{S}^7$ is studied.","Adapting a method by A. Thimm, a maximal set of functionally independent and Poisson commuting first integrals are constructed, including the corresponding SR Hamiltonian.","As a result, the complete integrability in the sense of Liouville is proved for the SR geodesic flow.","It is observed that these first integrals arise as the symbols of commuting second order differential operators one of them being a (not necessarily intrinsic) sublaplacian.","On the way one explicitly derives the Lie algebras of all SR isometry groups intersected with $O(8)$."],"url":"http://arxiv.org/abs/2403.10157v1","category":"math.DG"}
{"created":"2024-03-15 09:54:04","title":"Improving Medical Multi-modal Contrastive Learning with Expert Annotations","abstract":"We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the \"modality gap\" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showcases consistent improvements in embedding quality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP's capability to harness high-quality annotations for enriched multi-modal analysis in the medical imaging domain.","sentences":["We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps.","It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the \"modality gap\" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability.","eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness.","eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture.","Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showcases consistent improvements in embedding quality.","The outcomes reveal enhanced alignment and uniformity, affirming eCLIP's capability to harness high-quality annotations for enriched multi-modal analysis in the medical imaging domain."],"url":"http://arxiv.org/abs/2403.10153v1","category":"cs.CV"}
{"created":"2024-03-15 09:49:40","title":"Spectral flow of fermions in the $\\CP^2$ (anti-)instanton, and the sphaleron with vanishing topological charge","abstract":"The spectral flow is ubiquitous in the physics of soliton-fermion interacting systems. We study the spectral flows related to a continuous deformation of background soliton solutions, which enable us to develop insight into the emergence of fermionic zero modes and the localization mechanism of fermion densities. We investigate a $\\CP^2$ nonlinear sigma model in which there are the (anti-) instantons and also the sphalerons with vanishing topological charge. The standard Yukawa coupling of the fermion successfully generates infinite towers of the spectra and the spectral flow is observed when increasing the size of such solitons. At that moment, the localization of the fermions on the solitons emerges. The avoided crossings are also observed in several stages of the exchange of the flows, they are indicating a manifestation of the fermion exchange of the localizing nature.","sentences":["The spectral flow is ubiquitous in the physics of soliton-fermion interacting systems.","We study the spectral flows related to a continuous deformation of background soliton solutions, which enable us to develop insight into the emergence of fermionic zero modes and the localization mechanism of fermion densities.","We investigate a $\\CP^2$ nonlinear sigma model in which there are the (anti-) instantons and also the sphalerons with vanishing topological charge.","The standard Yukawa coupling of the fermion successfully generates infinite towers of the spectra and the spectral flow is observed when increasing the size of such solitons.","At that moment, the localization of the fermions on the solitons emerges.","The avoided crossings are also observed in several stages of the exchange of the flows, they are indicating a manifestation of the fermion exchange of the localizing nature."],"url":"http://arxiv.org/abs/2403.10149v1","category":"hep-th"}
{"created":"2024-03-15 09:43:52","title":"NLP Verification: Towards a General Methodology for Certifying Robustness","abstract":"Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it as a standard metric in the NLP verification pipelines (alongside with the standard metrics of model accuracy and model verifiability). Secondly, we propose a general methodology to analyse the effect of the embedding gap, a problem that refers to the discrepancy between verification of geometric subpspaces on the one hand, and semantic meaning of sentences which the geometric subspaces are supposed to represent, on the other hand. In extreme cases, poor choices in embedding of sentences may invalidate verification results. We propose a number of practical NLP methods that can help to identify the effects of the embedding gap; and in particular we propose the metric of falsifiability of semantic subpspaces as another fundamental metric to be reported as part of the NLP verification pipeline. We believe that together these general principles pave the way towards a more consolidated and effective development of this new domain.","sentences":["Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output.","Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification.","In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date.","Our contributions are two-fold.","Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces.","We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it as a standard metric in the NLP verification pipelines (alongside with the standard metrics of model accuracy and model verifiability).","Secondly, we propose a general methodology to analyse the effect of the embedding gap, a problem that refers to the discrepancy between verification of geometric subpspaces on the one hand, and semantic meaning of sentences which the geometric subspaces are supposed to represent, on the other hand.","In extreme cases, poor choices in embedding of sentences may invalidate verification results.","We propose a number of practical NLP methods that can help to identify the effects of the embedding gap; and in particular we propose the metric of falsifiability of semantic subpspaces as another fundamental metric to be reported as part of the NLP verification pipeline.","We believe that together these general principles pave the way towards a more consolidated and effective development of this new domain."],"url":"http://arxiv.org/abs/2403.10144v1","category":"cs.CL"}
{"created":"2024-03-15 09:35:02","title":"Being heterogeneous is disadvantageous: Brownian non-Gaussian searches","abstract":"Diffusing diffusivity models, polymers in the grand canonical ensemble and polydisperse, and continuous-time random walks all exhibit stages of non-Gaussian diffusion. Is non-Gaussian targeting more efficient than Gaussian? We address this question, central to, e.g., diffusion-limited reactions and some biological processes, through a general approach that makes use of Jensen's inequality and that encompasses all these systems. In terms of customary mean first-passage time, we show that Gaussian searches are more effective than non-Gaussian ones. A companion paper argues that non-Gaussianity becomes instead highly more efficient in applications where only a small fraction of tracers is required to reach the target.","sentences":["Diffusing diffusivity models, polymers in the grand canonical ensemble and polydisperse, and continuous-time random walks all exhibit stages of non-Gaussian diffusion.","Is non-Gaussian targeting more efficient than Gaussian?","We address this question, central to, e.g., diffusion-limited reactions and some biological processes, through a general approach that makes use of Jensen's inequality and that encompasses all these systems.","In terms of customary mean first-passage time, we show that Gaussian searches are more effective than non-Gaussian ones.","A companion paper argues that non-Gaussianity becomes instead highly more efficient in applications where only a small fraction of tracers is required to reach the target."],"url":"http://arxiv.org/abs/2403.10138v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-15 09:34:02","title":"Device-independent quantum secret sharing with noise pre-processing and post-selection","abstract":"Quantum secret sharing (QSS) is a fundamental quantum secure communication primitive, which enables a dealer to distribute secret keys to a set of players. Device-independent (DI) QSS can relax the security assumptions about the devices' internal working, and effectively enhance QSS's security under practical experimental conditions. Here, we propose a DI-QSS protocol based on Greenberger-Horne-Zeilinger state, which guarantees the security of keys by the observation of the data conclusively violating the Svetlichny inequality. We estimate the performance of our DI-QSS protocol in practical noisy communication scenarios by simulating its key generation rate, noise tolerance threshold, and detection efficiency threshold. Moreover, some active improvement strategies, such as the noise pre-processing strategy and post-selection strategy are introduced into the DI-QSS protocol, which can increase its noise tolerance threshold from 7.148% to 8.072%, and reduce the detection efficiency threshold from 96.32% to 94.30%. It indicates that the adoption of the active improvement strategies can enhance DI-QSS's robustness against the noise and photon loss, which can reduce the experimental difficulty and promote DI-QSS's experimental realization. Our work may be a promising guidance for future DI-QSS's experiments.","sentences":["Quantum secret sharing (QSS) is a fundamental quantum secure communication primitive, which enables a dealer to distribute secret keys to a set of players.","Device-independent (DI) QSS can relax the security assumptions about the devices' internal working, and effectively enhance QSS's security under practical experimental conditions.","Here, we propose a DI-QSS protocol based on Greenberger-Horne-Zeilinger state, which guarantees the security of keys by the observation of the data conclusively violating the Svetlichny inequality.","We estimate the performance of our DI-QSS protocol in practical noisy communication scenarios by simulating its key generation rate, noise tolerance threshold, and detection efficiency threshold.","Moreover, some active improvement strategies, such as the noise pre-processing strategy and post-selection strategy are introduced into the DI-QSS protocol, which can increase its noise tolerance threshold from 7.148% to 8.072%, and reduce the detection efficiency threshold from 96.32% to 94.30%.","It indicates that the adoption of the active improvement strategies can enhance DI-QSS's robustness against the noise and photon loss, which can reduce the experimental difficulty and promote DI-QSS's experimental realization.","Our work may be a promising guidance for future DI-QSS's experiments."],"url":"http://arxiv.org/abs/2403.10137v1","category":"quant-ph"}
{"created":"2024-03-15 09:33:10","title":"Response Style Characterization for Repeated Measures Using the Visual Analogue Scale","abstract":"Self-report measures (e.g., Likert scales) are widely used to evaluate subjective health perceptions. Recently, the visual analog scale (VAS), a slider-based scale, has become popular owing to its ability to precisely and easily assess how people feel. These data can be influenced by the response style (RS), a user-dependent systematic tendency that occurs regardless of questionnaire instructions. Despite its importance, especially in between-individual analysis, little attention has been paid to handling the RS in the VAS (denoted as response profile (RP)), as it is mainly used for within-individual monitoring and is less affected by RP. However, VAS measurements often require repeated self-reports of the same questionnaire items, making it difficult to apply conventional methods on a Likert scale. In this study, we developed a novel RP characterization method for various types of repeatedly measured VAS data. This approach involves the modeling of RP as distributional parameters ${\\theta}$ through a mixture of RS-like distributions, and addressing the issue of unbalanced data through bootstrap sampling for treating repeated measures. We assessed the effectiveness of the proposed method using simulated pseudo-data and an actual dataset from an empirical study. The assessment of parameter recovery showed that our method accurately estimated the RP parameter ${\\theta}$, demonstrating its robustness. Moreover, applying our method to an actual VAS dataset revealed the presence of individual RP heterogeneity, even in repeated VAS measurements, similar to the findings of the Likert scale. Our proposed method enables RP heterogeneity-aware VAS data analysis, similar to Likert-scale data analysis.","sentences":["Self-report measures (e.g., Likert scales) are widely used to evaluate subjective health perceptions.","Recently, the visual analog scale (VAS), a slider-based scale, has become popular owing to its ability to precisely and easily assess how people feel.","These data can be influenced by the response style (RS), a user-dependent systematic tendency that occurs regardless of questionnaire instructions.","Despite its importance, especially in between-individual analysis, little attention has been paid to handling the RS in the VAS (denoted as response profile (RP)), as it is mainly used for within-individual monitoring and is less affected by RP.","However, VAS measurements often require repeated self-reports of the same questionnaire items, making it difficult to apply conventional methods on a Likert scale.","In this study, we developed a novel RP characterization method for various types of repeatedly measured VAS data.","This approach involves the modeling of RP as distributional parameters ${\\theta}$ through a mixture of RS-like distributions, and addressing the issue of unbalanced data through bootstrap sampling for treating repeated measures.","We assessed the effectiveness of the proposed method using simulated pseudo-data and an actual dataset from an empirical study.","The assessment of parameter recovery showed that our method accurately estimated the RP parameter ${\\theta}$, demonstrating its robustness.","Moreover, applying our method to an actual VAS dataset revealed the presence of individual RP heterogeneity, even in repeated VAS measurements, similar to the findings of the Likert scale.","Our proposed method enables RP heterogeneity-aware VAS data analysis, similar to Likert-scale data analysis."],"url":"http://arxiv.org/abs/2403.10136v1","category":"stat.ME"}
{"created":"2024-03-15 09:28:19","title":"The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation","abstract":"Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.","sentences":["Large language models (LLMs) have shown excellent performance on various NLP tasks.","To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation.","We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations.","As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration.","Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods.","In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods.","Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn."],"url":"http://arxiv.org/abs/2403.10135v1","category":"cs.IR"}
{"created":"2024-03-15 09:26:48","title":"E4C: Enhance Editability for Text-Based Image Editing by Harnessing Efficient CLIP Guidance","abstract":"Diffusion-based image editing is a composite process of preserving the source image content and generating new content or applying modifications. While current editing approaches have made improvements under text guidance, most of them have only focused on preserving the information of the input image, disregarding the importance of editability and alignment to the target prompt. In this paper, we prioritize the editability by proposing a zero-shot image editing method, named \\textbf{E}nhance \\textbf{E}ditability for text-based image \\textbf{E}diting via \\textbf{E}fficient \\textbf{C}LIP guidance (\\textbf{E4C}), which only requires inference-stage optimization to explicitly enhance the edibility and text alignment. Specifically, we develop a unified dual-branch feature-sharing pipeline that enables the preservation of the structure or texture of the source image while allowing the other to be adapted based on the editing task. We further integrate CLIP guidance into our pipeline by utilizing our novel random-gateway optimization mechanism to efficiently enhance the semantic alignment with the target prompt. Comprehensive quantitative and qualitative experiments demonstrate that our method effectively resolves the text alignment issues prevalent in existing methods while maintaining the fidelity to the source image, and performs well across a wide range of editing tasks.","sentences":["Diffusion-based image editing is a composite process of preserving the source image content and generating new content or applying modifications.","While current editing approaches have made improvements under text guidance, most of them have only focused on preserving the information of the input image, disregarding the importance of editability and alignment to the target prompt.","In this paper, we prioritize the editability by proposing a zero-shot image editing method, named \\textbf{E}nhance \\textbf{E}ditability for text-based image \\textbf{E}diting via \\textbf{E}fficient \\textbf{C}LIP guidance (\\textbf{E4C}), which only requires inference-stage optimization to explicitly enhance the edibility and text alignment.","Specifically, we develop a unified dual-branch feature-sharing pipeline that enables the preservation of the structure or texture of the source image while allowing the other to be adapted based on the editing task.","We further integrate CLIP guidance into our pipeline by utilizing our novel random-gateway optimization mechanism to efficiently enhance the semantic alignment with the target prompt.","Comprehensive quantitative and qualitative experiments demonstrate that our method effectively resolves the text alignment issues prevalent in existing methods while maintaining the fidelity to the source image, and performs well across a wide range of editing tasks."],"url":"http://arxiv.org/abs/2403.10133v1","category":"cs.CV"}
{"created":"2024-03-15 09:26:02","title":"RAFT: Adapting Language Model to Domain Specific RAG","abstract":"Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a \"open-book\" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.","sentences":["Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm.","When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning.","However, the optimal methodology for the model to gain such new knowledge remains an open question.","In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a \"open-book\" in-domain settings.","In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents.","RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question.","This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason.","In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG.","RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla."],"url":"http://arxiv.org/abs/2403.10131v1","category":"cs.CL"}
{"created":"2024-03-15 09:24:19","title":"Polymer Concentration Regimes from Fractional Microrheology","abstract":"In this work, a framework for deriving theoretical equations for mean squared displacement (MSD) and fractional Fokker-Planck (FFP) is developed for any arbitrary rheological model. The obtained general results are then specified for two fractional rheological models. To test our framework and bridge the gap between microrheology and fractional rheological models, the microrheology of polystyrene (PS) in tetrahydrofuran (THF) solutions at several polymer concentrations is measured. By comparing the experimental and theoretical MSDs, we find the fractional rheological parameters and demonstrate that the polymer concentration regimes can be distinguished using the fractional exponent and relaxation time data because of the existence of a distinct behavior in each regime. We suggest simple approximations for the critical overlap concentration and the shear viscosity of weakly subdiffusive solutions with small relaxation times. This work provides an alternative and more sensitive approach for distinguishing different polymer concentration regimes and measuring the critical overlap concentration and shear viscosity of polymeric solutions, which is useful when conventional rheological characterization methods are unreliable due to the volatility and low viscosity of the samples.","sentences":["In this work, a framework for deriving theoretical equations for mean squared displacement (MSD) and fractional Fokker-Planck (FFP) is developed for any arbitrary rheological model.","The obtained general results are then specified for two fractional rheological models.","To test our framework and bridge the gap between microrheology and fractional rheological models, the microrheology of polystyrene (PS) in tetrahydrofuran (THF) solutions at several polymer concentrations is measured.","By comparing the experimental and theoretical MSDs, we find the fractional rheological parameters and demonstrate that the polymer concentration regimes can be distinguished using the fractional exponent and relaxation time data because of the existence of a distinct behavior in each regime.","We suggest simple approximations for the critical overlap concentration and the shear viscosity of weakly subdiffusive solutions with small relaxation times.","This work provides an alternative and more sensitive approach for distinguishing different polymer concentration regimes and measuring the critical overlap concentration and shear viscosity of polymeric solutions, which is useful when conventional rheological characterization methods are unreliable due to the volatility and low viscosity of the samples."],"url":"http://arxiv.org/abs/2403.10130v1","category":"cond-mat.soft"}
{"created":"2024-03-15 09:19:53","title":"General-order open-shell coupled-cluster method with partial spin adaptation I: formulations","abstract":"A general-order open-shell coupled-cluster method based on spatial orbitals is formulated. The method is an extension of the partial-spin adaptation (PSA) scheme from Janssen and Schaefer (Theor. Chim. Acta, 79, 1-42, 1991). By increasing the order of excitation operator and spin adaptation, the full configuration interaction (CI) limit is expected to be achieved. In the meanwhile, the advantages of spin-free approach in closed-shell systems: fourth-order truncation of the Baker-Campbell-Hausdorff (BCH) expansion and only $HT$-type connected terms are conserved.","sentences":["A general-order open-shell coupled-cluster method based on spatial orbitals is formulated.","The method is an extension of the partial-spin adaptation (PSA) scheme from Janssen and Schaefer (Theor.","Chim.","Acta, 79, 1-42, 1991).","By increasing the order of excitation operator and spin adaptation, the full configuration interaction (CI) limit is expected to be achieved.","In the meanwhile, the advantages of spin-free approach in closed-shell systems: fourth-order truncation of the Baker-Campbell-Hausdorff (BCH) expansion and only $HT$-type connected terms are conserved."],"url":"http://arxiv.org/abs/2403.10128v1","category":"physics.chem-ph"}
{"created":"2024-03-15 09:06:50","title":"Do Visual-Language Maps Capture Latent Semantics?","abstract":"Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map. The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding. While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking. We investigate two critical properties of map quality: queryability and consistency. The evaluation of queryability addresses the ability to retrieve information from the embeddings. We investigate two aspects of consistency: intra-map consistency and inter-map consistency. Intra-map consistency captures the ability of the embeddings to represent abstract semantic classes, and inter-map consistency captures the generalization properties of the representation. In this paper, we propose a way to analyze the quality of maps created using VLMs, which forms an open-source benchmark to be used when proposing new open-vocabulary map representations. We demonstrate the benchmark by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set. We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods.","sentences":["Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map.","The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding.","While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking.","We investigate two critical properties of map quality: queryability and consistency.","The evaluation of queryability addresses the ability to retrieve information from the embeddings.","We investigate two aspects of consistency: intra-map consistency and inter-map consistency.","Intra-map consistency captures the ability of the embeddings to represent abstract semantic classes, and inter-map consistency captures the generalization properties of the representation.","In this paper, we propose a way to analyze the quality of maps created using VLMs, which forms an open-source benchmark to be used when proposing new open-vocabulary map representations.","We demonstrate the benchmark by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set.","We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods."],"url":"http://arxiv.org/abs/2403.10117v1","category":"cs.RO"}
{"created":"2024-03-15 09:03:13","title":"Fast Generation of Feasible Trajectories in Direct Optimal Control","abstract":"This paper examines the question of finding feasible points to discrete-time optimal control problems. The optimization problem of finding a feasible trajectory is transcribed to an unconstrained optimal control problem. An efficient algorithm, called FP-DDP, is proposed that solves the resulting problem using Differential Dynamic Programming preserving feasibility with respect to the system dynamics in every iteration. Notably, FP-DDP admits global and rapid local convergence properties induced by a combination of a Levenberg-Marquardt method and an Armijo-type line search. The efficiency of FP-DDP is demonstrated against established methods such as Direct Multiple Shooting, Direct Single Shooting, and state-of-the-art solvers.","sentences":["This paper examines the question of finding feasible points to discrete-time optimal control problems.","The optimization problem of finding a feasible trajectory is transcribed to an unconstrained optimal control problem.","An efficient algorithm, called FP-DDP, is proposed that solves the resulting problem using Differential Dynamic Programming preserving feasibility with respect to the system dynamics in every iteration.","Notably, FP-DDP admits global and rapid local convergence properties induced by a combination of a Levenberg-Marquardt method and an Armijo-type line search.","The efficiency of FP-DDP is demonstrated against established methods such as Direct Multiple Shooting, Direct Single Shooting, and state-of-the-art solvers."],"url":"http://arxiv.org/abs/2403.10115v1","category":"math.OC"}
{"created":"2024-03-15 08:59:12","title":"Reconstruction of Poloidal Magnetic Fluxes on EAST based on Neural Networks with Measured Signals","abstract":"The accurate construction of tokamak equilibria, which is critical for the effective control and optimization of plasma configurations, depends on the precise distribution of magnetic fields and magnetic fluxes. Equilibrium fitting codes, such as EFIT relying on traditional equilibrium algorithms, require solving the GS equation by iterations based on the least square method constrained with measured magnetic signals. The iterative methods face numerous challenges and complexities in the pursuit of equilibrium optimization. Furthermore, these methodologies heavily depend on the expertise and practical experience, demanding substantial resource allocation in personnel and time. This paper reconstructs magnetic equilibria for the EAST tokamak based on artificial neural networks through a supervised learning method. We use a fully connected neural network to replace the GS equation and reconstruct the poloidal magnetic flux distribution by training the model based on EAST datasets. The training set, validation set, and testing set are partitioned randomly from the dataset of poloidal magnetic flux distributions of the EAST experiments in 2016 and 2017 years. The feasibility of the neural network model is verified by comparing it to the offline EFIT results. It is found that the neural network algorithm based on the supervised machine learning method can accurately predict the location of different closed magnetic flux surfaces at a high efficiency. The similarities of the predicted X-point position and last closed magnetic surface are both 98%. The Pearson coherence of the predicted q profiles is 92%. Compared with the target value, the model results show the potential of the neural network model for practical use in plasma modeling and real-time control of tokamak operations.","sentences":["The accurate construction of tokamak equilibria, which is critical for the effective control and optimization of plasma configurations, depends on the precise distribution of magnetic fields and magnetic fluxes.","Equilibrium fitting codes, such as EFIT relying on traditional equilibrium algorithms, require solving the GS equation by iterations based on the least square method constrained with measured magnetic signals.","The iterative methods face numerous challenges and complexities in the pursuit of equilibrium optimization.","Furthermore, these methodologies heavily depend on the expertise and practical experience, demanding substantial resource allocation in personnel and time.","This paper reconstructs magnetic equilibria for the EAST tokamak based on artificial neural networks through a supervised learning method.","We use a fully connected neural network to replace the GS equation and reconstruct the poloidal magnetic flux distribution by training the model based on EAST datasets.","The training set, validation set, and testing set are partitioned randomly from the dataset of poloidal magnetic flux distributions of the EAST experiments in 2016 and 2017 years.","The feasibility of the neural network model is verified by comparing it to the offline EFIT results.","It is found that the neural network algorithm based on the supervised machine learning method can accurately predict the location of different closed magnetic flux surfaces at a high efficiency.","The similarities of the predicted X-point position and last closed magnetic surface are both 98%.","The Pearson coherence of the predicted q profiles is 92%.","Compared with the target value, the model results show the potential of the neural network model for practical use in plasma modeling and real-time control of tokamak operations."],"url":"http://arxiv.org/abs/2403.10114v1","category":"physics.plasm-ph"}
{"created":"2024-03-15 08:55:56","title":"Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution Approach","abstract":"In this paper, we focus on one centralized and one decentralized problem of active hypothesis testing in the presence of an eavesdropper. For the centralized problem including a single legitimate agent, we present a new framework based on NeuroEvolution (NE), whereas, for the decentralized problem, we develop a novel NE-based method for solving collaborative multi-agent tasks, which interestingly maintains all computational benefits of single-agent NE. The superiority of the proposed EAHT approaches over conventional active hypothesis testing policies, as well as learning-based methods, is validated through numerical investigations in an example use case of anomaly detection over wireless sensor networks.","sentences":["In this paper, we focus on one centralized and one decentralized problem of active hypothesis testing in the presence of an eavesdropper.","For the centralized problem including a single legitimate agent, we present a new framework based on NeuroEvolution (NE), whereas, for the decentralized problem, we develop a novel NE-based method for solving collaborative multi-agent tasks, which interestingly maintains all computational benefits of single-agent NE.","The superiority of the proposed EAHT approaches over conventional active hypothesis testing policies, as well as learning-based methods, is validated through numerical investigations in an example use case of anomaly detection over wireless sensor networks."],"url":"http://arxiv.org/abs/2403.10112v1","category":"cs.AI"}
{"created":"2024-03-15 08:54:25","title":"Meta Operator for Complex Query Answering on Knowledge Graphs","abstract":"Knowledge graphs contain informative factual knowledge but are considered incomplete. To answer complex queries under incomplete knowledge, learning-based Complex Query Answering (CQA) models are proposed to directly learn from the query-answer samples to avoid the direct traversal of incomplete graph data. Existing works formulate the training of complex query answering models as multi-task learning and require a large number of training samples. In this work, we explore the compositional structure of complex queries and argue that the different logical operator types, rather than the different complex query types, are the key to improving generalizability. Accordingly, we propose a meta-learning algorithm to learn the meta-operators with limited data and adapt them to different instances of operators under various complex queries. Empirical results show that learning meta-operators is more effective than learning original CQA or meta-CQA models.","sentences":["Knowledge graphs contain informative factual knowledge but are considered incomplete.","To answer complex queries under incomplete knowledge, learning-based Complex Query Answering (CQA) models are proposed to directly learn from the query-answer samples to avoid the direct traversal of incomplete graph data.","Existing works formulate the training of complex query answering models as multi-task learning and require a large number of training samples.","In this work, we explore the compositional structure of complex queries and argue that the different logical operator types, rather than the different complex query types, are the key to improving generalizability.","Accordingly, we propose a meta-learning algorithm to learn the meta-operators with limited data and adapt them to different instances of operators under various complex queries.","Empirical results show that learning meta-operators is more effective than learning original CQA or meta-CQA models."],"url":"http://arxiv.org/abs/2403.10110v1","category":"cs.LG"}
{"created":"2024-03-15 08:51:38","title":"Autonomous Monitoring of Pharmaceutical R&D Laboratories with 6 Axis Arm Equipped Quadruped Robot and Generative AI: A Preliminary Study","abstract":"This paper presents a proof-of-concept study that examines the utilization of generative AI and mobile robotics for autonomous laboratory monitoring in the pharmaceutical R&D laboratory. The study investigates the potential advantages of anomaly detection and automated reporting by multi-modal model and Vision Foundation Model (VFM), which have the potential to enhance compliance and safety in laboratory environments. Additionally, the paper discusses the current limitations of the generative AI approach and proposes future directions for its application in lab monitoring.","sentences":["This paper presents a proof-of-concept study that examines the utilization of generative AI and mobile robotics for autonomous laboratory monitoring in the pharmaceutical R&D laboratory.","The study investigates the potential advantages of anomaly detection and automated reporting by multi-modal model and Vision Foundation Model (VFM), which have the potential to enhance compliance and safety in laboratory environments.","Additionally, the paper discusses the current limitations of the generative AI approach and proposes future directions for its application in lab monitoring."],"url":"http://arxiv.org/abs/2403.10108v1","category":"cs.RO"}
{"created":"2024-03-15 08:51:38","title":"Measurement of the 1-jettiness event shape observable in deep-inelastic electron-proton scattering at HERA","abstract":"The H1 Collaboration reports the first measurement of the 1-jettiness event shape observable $\\tau_1^b$ in neutral-current deep-inelastic electron-proton scattering (DIS). The observable $\\tau_1^b$ is equivalent to a thrust observable defined in the Breit frame. The data sample was collected at the HERA $ep$ collider in the years 2003-2007 with center-of-mass energy of $\\sqrt{s}=319\\,\\text{GeV}$, corresponding to an integrated luminosity of $351.1\\,\\text{pb}^{-1}$. Triple differential cross sections are provided as a function of $\\tau_1^b$, event virtuality $Q^2$, and inelasticity $y$, in the kinematic region $Q^2>150\\,\\text{GeV}^{2}$. Single differential cross section are provided as a function of $\\tau_1^b$ in a limited kinematic range. Double differential cross sections are measured, in contrast, integrated over $\\tau_1^b$ and represent the inclusive neutral-current DIS cross section measured as a function of $Q^2$ and $y$. The data are compared to a variety of predictions and include classical and modern Monte Carlo event generators, predictions in fixed-order perturbative QCD where calculations up to $\\mathcal{O}(\\alpha_s^3)$ are available for $\\tau_1^b$ or inclusive DIS, and resummed predictions at next-to-leading logarithmic accuracy matched to fixed order predictions at $\\mathcal{O}(\\alpha_s^2)$. These comparisons reveal sensitivity of the 1-jettiness observable to QCD parton shower and resummation effects, as well as the modeling of hadronization and fragmentation. Within their range of validity, the fixed-order predictions provide a good description of the data. Monte Carlo event generators are predictive over the full measured range and hence their underlying models and parameters can be constrained by comparing to the presented data.","sentences":["The H1 Collaboration reports the first measurement of the 1-jettiness event shape observable $\\tau_1^b$ in neutral-current deep-inelastic electron-proton scattering (DIS).","The observable $\\tau_1^b$ is equivalent to a thrust observable defined in the Breit frame.","The data sample was collected at the HERA $ep$ collider in the years 2003-2007 with center-of-mass energy of $\\sqrt{s}=319\\,\\text{GeV}$, corresponding to an integrated luminosity of $351.1\\,\\text{pb}^{-1}$. Triple differential cross sections are provided as a function of $\\tau_1^b$, event virtuality $Q^2$, and inelasticity $y$, in the kinematic region $Q^2>150\\,\\text{GeV}^{2}$. Single differential cross section are provided as a function of $\\tau_1^b$ in a limited kinematic range.","Double differential cross sections are measured, in contrast, integrated over $\\tau_1^b$ and represent the inclusive neutral-current DIS cross section measured as a function of $Q^2$ and $y$. The data are compared to a variety of predictions and include classical and modern Monte Carlo event generators, predictions in fixed-order perturbative QCD where calculations up to $\\mathcal{O}(\\alpha_s^3)$ are available for $\\tau_1^b$ or inclusive DIS, and resummed predictions at next-to-leading logarithmic accuracy matched to fixed order predictions at $\\mathcal{O}(\\alpha_s^2)$. These comparisons reveal sensitivity of the 1-jettiness observable to QCD parton shower and resummation effects, as well as the modeling of hadronization and fragmentation.","Within their range of validity, the fixed-order predictions provide a good description of the data.","Monte Carlo event generators are predictive over the full measured range and hence their underlying models and parameters can be constrained by comparing to the presented data."],"url":"http://arxiv.org/abs/2403.10109v1","category":"hep-ex"}
{"created":"2024-03-15 08:51:15","title":"Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning","abstract":"Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs). We design a two-stage collaboration system of different LLMs for the V-HOI task. Specifically, in the first stage, we design a Cross-Agents Reasoning scheme to leverage the LLM conduct reasoning from different aspects. In the second stage, we perform Multi-LLMs Debate to get the final reasoning answer based on the different knowledge in different LLMs. Additionally, we devise an auxiliary training strategy that utilizes CLIP, a large vision-language model to enhance the base V-HOI models' discriminative ability to better cooperate with LLMs. We validate the superiority of our design by demonstrating its effectiveness in improving the prediction accuracy of the base V-HOI model via reasoning from multiple perspectives.","sentences":["Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems.","Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships.","In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs).","We design a two-stage collaboration system of different LLMs for the V-HOI task.","Specifically, in the first stage, we design a Cross-Agents Reasoning scheme to leverage the LLM conduct reasoning from different aspects.","In the second stage, we perform Multi-LLMs Debate to get the final reasoning answer based on the different knowledge in different LLMs.","Additionally, we devise an auxiliary training strategy that utilizes CLIP, a large vision-language model to enhance the base V-HOI models' discriminative ability to better cooperate with LLMs.","We validate the superiority of our design by demonstrating its effectiveness in improving the prediction accuracy of the base V-HOI model via reasoning from multiple perspectives."],"url":"http://arxiv.org/abs/2403.10107v1","category":"cs.CV"}
{"created":"2024-03-15 08:50:39","title":"Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots","abstract":"Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model's ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles.","sentences":["Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments.","However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications.","In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty.","BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans.","It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation.","Through experiments in various risk laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots.","The model's ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles."],"url":"http://arxiv.org/abs/2403.10105v1","category":"cs.RO"}
{"created":"2024-03-15 08:48:37","title":"DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video","abstract":"Recent advancements in dynamic neural radiance field methods have yielded remarkable outcomes. However, these approaches rely on the assumption of sharp input images. When faced with motion blur, existing dynamic NeRF methods often struggle to generate high-quality novel views. In this paper, we propose DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views from a monocular video affected by motion blur. To account for motion blur in input images, we simultaneously capture the camera trajectory and object Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we employ a global cross-time rendering approach to ensure consistent temporal coherence across the entire scene. We curate a dataset comprising diverse dynamic scenes that are specifically tailored for our task. Experimental results on our dataset demonstrate that our method outperforms existing approaches in generating sharp novel views from motion-blurred inputs while maintaining spatial-temporal consistency of the scene.","sentences":["Recent advancements in dynamic neural radiance field methods have yielded remarkable outcomes.","However, these approaches rely on the assumption of sharp input images.","When faced with motion blur, existing dynamic NeRF methods often struggle to generate high-quality novel views.","In this paper, we propose DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views from a monocular video affected by motion blur.","To account for motion blur in input images, we simultaneously capture the camera trajectory and object Discrete Cosine Transform (DCT) trajectories within the scene.","Additionally, we employ a global cross-time rendering approach to ensure consistent temporal coherence across the entire scene.","We curate a dataset comprising diverse dynamic scenes that are specifically tailored for our task.","Experimental results on our dataset demonstrate that our method outperforms existing approaches in generating sharp novel views from motion-blurred inputs while maintaining spatial-temporal consistency of the scene."],"url":"http://arxiv.org/abs/2403.10103v1","category":"cs.CV"}
{"created":"2024-03-15 08:48:29","title":"Nonlinear-ancilla aided quantum algorithm for nonlinear Schr\u00f6dinger equations","abstract":"We present an algorithm that uses a single ancilla qubit that can evolve nonlinearly, and show how to use it to efficiently solve generic nonlinear Schr\\\"odinger equations, including nonlocal Hartree equations and the Navier-Stokes equation for an irrotational, non-viscous flow. We propose a realization of such nonlinear qubits via spin-spin coupling of neutral atom qubits to a Bose-Einstein condensate. The results suggest that the use of nonlinear ancillas can provide substantial speedups compared to exclusively linear qubit devices.","sentences":["We present an algorithm that uses a single ancilla qubit that can evolve nonlinearly, and show how to use it to efficiently solve generic nonlinear Schr\\\"odinger equations, including nonlocal Hartree equations and the Navier-Stokes equation for an irrotational, non-viscous flow.","We propose a realization of such nonlinear qubits via spin-spin coupling of neutral atom qubits to a Bose-Einstein condensate.","The results suggest that the use of nonlinear ancillas can provide substantial speedups compared to exclusively linear qubit devices."],"url":"http://arxiv.org/abs/2403.10102v1","category":"quant-ph"}
{"created":"2024-03-15 08:46:49","title":"Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness","abstract":"Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient. One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation. This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control. In simulation and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios. The implementation is realized as an open-source package at https://github.com/ZWT006/agile_navigation.","sentences":["Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient.","One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation.","This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control.","In simulation and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios.","The implementation is realized as an open-source package at https://github.com/ZWT006/agile_navigation."],"url":"http://arxiv.org/abs/2403.10101v1","category":"cs.RO"}
{"created":"2024-03-15 08:44:15","title":"DiffMAC: Diffusion Manifold Hallucination Correction for High Generalization Blind Face Restoration","abstract":"Blind face restoration (BFR) is a highly challenging problem due to the uncertainty of degradation patterns. Current methods have low generalization across photorealistic and heterogeneous domains. In this paper, we propose a Diffusion-Information-Diffusion (DID) framework to tackle diffusion manifold hallucination correction (DiffMAC), which achieves high-generalization face restoration in diverse degraded scenes and heterogeneous domains. Specifically, the first diffusion stage aligns the restored face with spatial feature embedding of the low-quality face based on AdaIN, which synthesizes degradation-removal results but with uncontrollable artifacts for some hard cases. Based on Stage I, Stage II considers information compression using manifold information bottleneck (MIB) and finetunes the first diffusion model to improve facial fidelity. DiffMAC effectively fights against blind degradation patterns and synthesizes high-quality faces with attribute and identity consistencies. Experimental results demonstrate the superiority of DiffMAC over state-of-the-art methods, with a high degree of generalization in real-world and heterogeneous settings. The source code and models will be public.","sentences":["Blind face restoration (BFR) is a highly challenging problem due to the uncertainty of degradation patterns.","Current methods have low generalization across photorealistic and heterogeneous domains.","In this paper, we propose a Diffusion-Information-Diffusion (DID) framework to tackle diffusion manifold hallucination correction (DiffMAC), which achieves high-generalization face restoration in diverse degraded scenes and heterogeneous domains.","Specifically, the first diffusion stage aligns the restored face with spatial feature embedding of the low-quality face based on AdaIN, which synthesizes degradation-removal results but with uncontrollable artifacts for some hard cases.","Based on Stage I, Stage II considers information compression using manifold information bottleneck (MIB) and finetunes the first diffusion model to improve facial fidelity.","DiffMAC effectively fights against blind degradation patterns and synthesizes high-quality faces with attribute and identity consistencies.","Experimental results demonstrate the superiority of DiffMAC over state-of-the-art methods, with a high degree of generalization in real-world and heterogeneous settings.","The source code and models will be public."],"url":"http://arxiv.org/abs/2403.10098v1","category":"cs.CV"}
{"created":"2024-03-15 08:26:59","title":"Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks","abstract":"While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamically updates the conditional distribution to follow the currently updated feature extractors and balance the distance between classes in feature spaces. Our experiments show that AdaRand outperforms the other fine-tuning regularization, which requires auxiliary source information and heavy computation costs.","sentences":["While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets.","Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss.","However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations.","In this paper, we propose a simple method called adaptive random feature regularization (AdaRand).","AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs.","To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions.","Furthermore, AdaRand dynamically updates the conditional distribution to follow the currently updated feature extractors and balance the distance between classes in feature spaces.","Our experiments show that AdaRand outperforms the other fine-tuning regularization, which requires auxiliary source information and heavy computation costs."],"url":"http://arxiv.org/abs/2403.10097v1","category":"cs.LG"}
{"created":"2024-03-15 08:19:57","title":"RangeLDM: Fast Realistic LiDAR Point Cloud Generation","abstract":"Autonomous driving demands high-quality LiDAR data, yet the cost of physical LiDAR sensors presents a significant scaling-up challenge. While recent efforts have explored deep generative models to address this issue, they often consume substantial computational resources with slow generation speeds while suffering from a lack of realism. To address these limitations, we introduce RangeLDM, a novel approach for rapidly generating high-quality range-view LiDAR point clouds via latent diffusion models. We achieve this by correcting range-view data distribution for accurate projection from point clouds to range images via Hough voting, which has a critical impact on generative learning. We then compress the range images into a latent space with a variational autoencoder, and leverage a diffusion model to enhance expressivity. Additionally, we instruct the model to preserve 3D structural fidelity by devising a range-guided discriminator. Experimental results on KITTI-360 and nuScenes datasets demonstrate both the robust expressiveness and fast speed of our LiDAR point cloud generation.","sentences":["Autonomous driving demands high-quality LiDAR data, yet the cost of physical LiDAR sensors presents a significant scaling-up challenge.","While recent efforts have explored deep generative models to address this issue, they often consume substantial computational resources with slow generation speeds while suffering from a lack of realism.","To address these limitations, we introduce RangeLDM, a novel approach for rapidly generating high-quality range-view LiDAR point clouds via latent diffusion models.","We achieve this by correcting range-view data distribution for accurate projection from point clouds to range images via Hough voting, which has a critical impact on generative learning.","We then compress the range images into a latent space with a variational autoencoder, and leverage a diffusion model to enhance expressivity.","Additionally, we instruct the model to preserve 3D structural fidelity by devising a range-guided discriminator.","Experimental results on KITTI-360 and nuScenes datasets demonstrate both the robust expressiveness and fast speed of our LiDAR point cloud generation."],"url":"http://arxiv.org/abs/2403.10094v1","category":"cs.CV"}
{"created":"2024-03-15 17:53:54","title":"Multilevel functional distributional models with application to continuous glucose monitoring in diabetes clinical trials","abstract":"Continuous glucose monitoring (CGM) is a minimally invasive technology that allows continuous monitoring of an individual's blood glucose. We focus on a large clinical trial that collected CGM data every few minutes for 26 weeks and assumes that the basic observation unit is the distribution of CGM observations in a four-week interval. The resulting data structure is multilevel (because each individual has multiple months of data) and distributional (because the data for each four-week interval is represented as a distribution). The scientific goals are to: (1) identify and quantify the effects of factors that affect glycemic control in type 1 diabetes (T1D) patients; and (2) identify and characterize the patients who respond to treatment. To address these goals, we propose a new multilevel functional model that treats the CGM distributions as a response. Methods are motivated by and applied to data collected by The Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Group. Reproducible code for the methods introduced here is available on GitHub.","sentences":["Continuous glucose monitoring (CGM) is a minimally invasive technology that allows continuous monitoring of an individual's blood glucose.","We focus on a large clinical trial that collected CGM data every few minutes for 26 weeks and assumes that the basic observation unit is the distribution of CGM observations in a four-week interval.","The resulting data structure is multilevel (because each individual has multiple months of data) and distributional (because the data for each four-week interval is represented as a distribution).","The scientific goals are to: (1) identify and quantify the effects of factors that affect glycemic control in type 1 diabetes (T1D) patients; and (2) identify and characterize the patients who respond to treatment.","To address these goals, we propose a new multilevel functional model that treats the CGM distributions as a response.","Methods are motivated by and applied to data collected by The Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Group.","Reproducible code for the methods introduced here is available on GitHub."],"url":"http://arxiv.org/abs/2403.10514v1","category":"stat.ME"}
{"created":"2024-03-15 16:25:39","title":"Multivariate Bayesian models with flexible shared interactions for analyzing spatio-temporal patterns of rare cancers","abstract":"Rare cancers affect millions of people worldwide each year. However, estimating incidence or mortality rates associated with rare cancers presents important difficulties and poses new statistical methodological challenges. In this paper, we expand the collection of multivariate spatio-temporal models by introducing adaptable shared interactions to enable a comprehensive analysis of both incidence and cancer mortality in rare cancer cases. These models allow the modulation of spatio-temporal interactions between incidence and mortality, allowing for changes in their relationship over time. The new models have been implemented in INLA using r-generic constructions. We conduct a simulation study to evaluate the performance of the new spatio-temporal models in terms of sensitivity and specificity. Results show that multivariate spatio-temporal models with flexible shared interaction outperform conventional multivariate spatio-temporal models with independent interactions. We use these models to analyze incidence and mortality data for pancreatic cancer and leukaemia among males across 142 administrative healthcare districts of Great Britain over a span of nine biennial periods (2002-2019).","sentences":["Rare cancers affect millions of people worldwide each year.","However, estimating incidence or mortality rates associated with rare cancers presents important difficulties and poses new statistical methodological challenges.","In this paper, we expand the collection of multivariate spatio-temporal models by introducing adaptable shared interactions to enable a comprehensive analysis of both incidence and cancer mortality in rare cancer cases.","These models allow the modulation of spatio-temporal interactions between incidence and mortality, allowing for changes in their relationship over time.","The new models have been implemented in INLA using r-generic constructions.","We conduct a simulation study to evaluate the performance of the new spatio-temporal models in terms of sensitivity and specificity.","Results show that multivariate spatio-temporal models with flexible shared interaction outperform conventional multivariate spatio-temporal models with independent interactions.","We use these models to analyze incidence and mortality data for pancreatic cancer and leukaemia among males across 142 administrative healthcare districts of Great Britain over a span of nine biennial periods (2002-2019)."],"url":"http://arxiv.org/abs/2403.10440v1","category":"stat.ME"}
{"created":"2024-03-15 16:00:04","title":"SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians","abstract":"Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency.","sentences":["Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering.","More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios.","Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data.","To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections.","We achieve this by modeling appearance to seize photometric variations in the rendered images.","Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner.","Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency."],"url":"http://arxiv.org/abs/2403.10427v1","category":"cs.CV"}
{"created":"2024-03-15 15:37:19","title":"A comparative study on machine learning approaches for rock mass classification using drilling data","abstract":"Current rock engineering design in drill and blast tunnelling primarily relies on engineers' observational assessments. Measure While Drilling (MWD) data, a high-resolution sensor dataset collected during tunnel excavation, is underutilised, mainly serving for geological visualisation. This study aims to automate the translation of MWD data into actionable metrics for rock engineering. It seeks to link data to specific engineering actions, thus providing critical decision support for geological challenges ahead of the tunnel face. Leveraging a large and geologically diverse dataset of 500,000 drillholes from 15 tunnels, the research introduces models for accurate rock mass quality classification in a real-world tunnelling context. Both conventional machine learning and image-based deep learning are explored to classify MWD data into Q-classes and Q-values, examples of metrics describing the stability of the rock mass, using both tabular and image data. The results indicate that the K-nearest neighbours algorithm in an ensemble with tree-based models using tabular data, effectively classifies rock mass quality. It achieves a cross-validated balanced accuracy of 0.86 in classifying rock mass into the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification with E versus the rest. Classification using a CNN with MWD-images for each blasting round resulted in a balanced accuracy of 0.82 for binary classification. Regressing the Q-value from tabular MWD-data achieved cross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model as in classification. High performance in regression and classification boosts confidence in automated rock mass assessment. Applying advanced modelling on a unique dataset demonstrates MWD data's value in improving rock mass classification accuracy and advancing data-driven rock engineering design, reducing manual intervention.","sentences":["Current rock engineering design in drill and blast tunnelling primarily relies on engineers' observational assessments.","Measure While Drilling (MWD) data, a high-resolution sensor dataset collected during tunnel excavation, is underutilised, mainly serving for geological visualisation.","This study aims to automate the translation of MWD data into actionable metrics for rock engineering.","It seeks to link data to specific engineering actions, thus providing critical decision support for geological challenges ahead of the tunnel face.","Leveraging a large and geologically diverse dataset of 500,000 drillholes from 15 tunnels, the research introduces models for accurate rock mass quality classification in a real-world tunnelling context.","Both conventional machine learning and image-based deep learning are explored to classify MWD data into Q-classes and Q-values, examples of metrics describing the stability of the rock mass, using both tabular and image data.","The results indicate that the K-nearest neighbours algorithm in an ensemble with tree-based models using tabular data, effectively classifies rock mass quality.","It achieves a cross-validated balanced accuracy of 0.86 in classifying rock mass into the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification with E versus the rest.","Classification using a CNN with MWD-images for each blasting round resulted in a balanced accuracy of 0.82 for binary classification.","Regressing the Q-value from tabular MWD-data achieved cross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model as in classification.","High performance in regression and classification boosts confidence in automated rock mass assessment.","Applying advanced modelling on a unique dataset demonstrates MWD data's value in improving rock mass classification accuracy and advancing data-driven rock engineering design, reducing manual intervention."],"url":"http://arxiv.org/abs/2403.10404v1","category":"cs.LG"}
{"created":"2024-03-15 15:21:04","title":"Evaluating Perceptual Distances by Fitting Binomial Distributions to Two-Alternative Forced Choice Data","abstract":"The two-alternative forced choice (2AFC) experimental setup is popular in the visual perception literature, where practitioners aim to understand how human observers perceive distances within triplets that consist of a reference image and two distorted versions of that image. In the past, this had been conducted in controlled environments, with a tournament-style algorithm dictating which images are shown to each participant to rank the distorted images. Recently, crowd-sourced perceptual datasets have emerged, with no images shared between triplets, making ranking impossible. Evaluating perceptual distances using this data is non-trivial, relying on reducing the collection of judgements on a triplet to a binary decision -- which is suboptimal and prone to misleading conclusions. Instead, we statistically model the underlying decision-making process during 2AFC experiments using a binomial distribution. We use maximum likelihood estimation to fit a distribution to the perceptual judgements, conditioned on the perceptual distance to test and impose consistency and smoothness between our empirical estimates of the density. This way, we can evaluate a different number of judgements per triplet, and can calculate metrics such as likelihoods of judgements according to a set of distances -- key ingredients that neural network counterparts lack.","sentences":["The two-alternative forced choice (2AFC) experimental setup is popular in the visual perception literature, where practitioners aim to understand how human observers perceive distances within triplets that consist of a reference image and two distorted versions of that image.","In the past, this had been conducted in controlled environments, with a tournament-style algorithm dictating which images are shown to each participant to rank the distorted images.","Recently, crowd-sourced perceptual datasets have emerged, with no images shared between triplets, making ranking impossible.","Evaluating perceptual distances using this data is non-trivial, relying on reducing the collection of judgements on a triplet to a binary decision -- which is suboptimal and prone to misleading conclusions.","Instead, we statistically model the underlying decision-making process during 2AFC experiments using a binomial distribution.","We use maximum likelihood estimation to fit a distribution to the perceptual judgements, conditioned on the perceptual distance to test and impose consistency and smoothness between our empirical estimates of the density.","This way, we can evaluate a different number of judgements per triplet, and can calculate metrics such as likelihoods of judgements according to a set of distances -- key ingredients that neural network counterparts lack."],"url":"http://arxiv.org/abs/2403.10390v1","category":"cs.CV"}
{"created":"2024-03-15 14:41:00","title":"Resolving Full-Wave Through-Wall Transmission Effects in Multi-Static Synthetic Aperture Radar","abstract":"Through-wall synthetic aperture radar (SAR) imaging is of significant interest for security purposes, in particular when using multi-static SAR systems consisting of multiple distributed radar transmitters and receivers to improve resolution and the ability to recognise objects. Yet there is a significant challenge in forming focused, useful images due to multiple scattering effects through walls, whereas standard SAR imaging has an inherent single scattering assumption. This may be exacerbated with multi-static collections, since different scattering events will be observed from each angle and the data may not coherently combine well in a naive manner. To overcome this, we propose an image formation method which resolves full-wave effects through an approximately known wall or other arbitrary obstacle, which itself has some unknown \"nuisance\" parameters that are determined as part of the reconstruction to provide well focused images. The method is more flexible and realistic than existing methods which treat a single wall as a flat layered medium, whilst being significantly computationally cheaper than full-wave methods, strongly motivated by practical considerations for through-wall SAR.","sentences":["Through-wall synthetic aperture radar (SAR) imaging is of significant interest for security purposes, in particular when using multi-static SAR systems consisting of multiple distributed radar transmitters and receivers to improve resolution and the ability to recognise objects.","Yet there is a significant challenge in forming focused, useful images due to multiple scattering effects through walls, whereas standard SAR imaging has an inherent single scattering assumption.","This may be exacerbated with multi-static collections, since different scattering events will be observed from each angle and the data may not coherently combine well in a naive manner.","To overcome this, we propose an image formation method which resolves full-wave effects through an approximately known wall or other arbitrary obstacle, which itself has some unknown \"nuisance\" parameters that are determined as part of the reconstruction to provide well focused images.","The method is more flexible and realistic than existing methods which treat a single wall as a flat layered medium, whilst being significantly computationally cheaper than full-wave methods, strongly motivated by practical considerations for through-wall SAR."],"url":"http://arxiv.org/abs/2403.10354v1","category":"math.NA"}
{"created":"2024-03-15 14:30:07","title":"Search for the decay of the Higgs boson to a pair of light pseudoscalar bosons in the final state with four bottom quarks in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is presented for the decay of the 125 GeV Higgs boson (H) to a pair of new light pseudoscalar bosons (a), followed by the prompt decay of each a boson to a bottom quark-antiquark pair, H $\\to$ aa $\\to$ $\\mathrm{b\\bar{b}b\\bar{b}}$. The analysis is performed using a data sample of proton-proton collisions collected with the CMS detector at a center-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 138 fb$^{-1}$. To reduce the background from standard model processes, the search requires the Higgs boson to be produced in association with a leptonically decaying W or Z boson. The analysis probes the production of new light bosons in a 15 $\\lt$ $m_\\mathrm{a}$ $\\lt$ 60 GeV mass range. Assuming the standard model predictions for the Higgs boson production cross sections for pp $\\to$ WH and ZH, model independent upper limits at 95% confidence level are derived for the branching fraction $\\mathcal{B}$(H $\\to$ aa $\\to$ $\\mathrm{b\\bar{b}b\\bar{b}}$). The combined WH and ZH observed upper limit on the branching fraction ranges from 1.10 for $m_\\mathrm{a} =$ 20 GeV to 0.36 for $m_\\mathrm{a} =$ 60 GeV, complementing other measurements in the $\\mu\\mu\\tau\\tau$, $\\tau\\tau\\tau\\tau$ and bb$\\ell\\ell$ ($\\ell=$ $\\mu$,$\\tau$) channels.","sentences":["A search is presented for the decay of the 125 GeV Higgs boson (H) to a pair of new light pseudoscalar bosons (a), followed by the prompt decay of each a boson to a bottom quark-antiquark pair, H $\\to$ aa $\\to$ $\\mathrm{b\\bar{b}b\\bar{b}}$.","The analysis is performed using a data sample of proton-proton collisions collected with the CMS detector at a center-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 138 fb$^{-1}$. To reduce the background from standard model processes, the search requires the Higgs boson to be produced in association with a leptonically decaying W or Z boson.","The analysis probes the production of new light bosons in a 15 $\\lt$ $m_\\mathrm{a}$ $\\lt$ 60 GeV mass range.","Assuming the standard model predictions for the Higgs boson production cross sections for pp $\\to$ WH and ZH, model independent upper limits at 95% confidence level are derived for the branching fraction $\\mathcal{B}$(H $\\to$ aa $\\to$ $\\mathrm{b\\bar{b}b\\bar{b}}$).","The combined WH and ZH observed upper limit on the branching fraction ranges from 1.10 for $m_\\mathrm{a} =$ 20 GeV to 0.36 for $m_\\mathrm{a} =$ 60 GeV, complementing other measurements in the $\\mu\\mu\\tau\\tau$, $\\tau\\tau\\tau\\tau$ and bb$\\ell\\ell$ ($\\ell=$ $\\mu$,$\\tau$) channels."],"url":"http://arxiv.org/abs/2403.10341v1","category":"hep-ex"}
{"created":"2024-03-15 14:11:27","title":"Experimental Measurements of the Granular Density of Modes via Impact","abstract":"The jamming transition is an important feature of granular materials, with prior work showing an excess of low frequency modes in the granular analog to the density of states, the granular density of modes. In this work, we present an experimental method for acoustically measuring the granular density of modes using a single impact event to excite vibrational modes in an experimental, three dimensional, granular material. We test three different granular materials, all of which are composed of spherical beads. The first two systems are monodisperse collections of either 6 mm or 8 mm diameter beads. The third system is a bidisperse mixture of the previous two bead sizes. During data collection, the particles are confined to a box; on top of this box, and resting on the granular material, is a light, rigid sheet onto which pressure can be applied to the system. To excite the material, a steel impactor ball is dropped on top of the system. The response of the granular material to the impact pulse is recorded by piezoelectric sensors buried throughout the material, and the density of modes is computed from the spectrum of the velocity autocorrelation of these sensors. Our measurements of the density of modes show more low frequency modes at low pressure, consistent with previous experimental and numerical results, as well as several low frequency peaks in the density of modes that shift with applied pressure. Finally, we also see that the density of modes falls off at a wavelength on the order of twice the particle diameter, which is reminiscent of the Debye frequency.","sentences":["The jamming transition is an important feature of granular materials, with prior work showing an excess of low frequency modes in the granular analog to the density of states, the granular density of modes.","In this work, we present an experimental method for acoustically measuring the granular density of modes using a single impact event to excite vibrational modes in an experimental, three dimensional, granular material.","We test three different granular materials, all of which are composed of spherical beads.","The first two systems are monodisperse collections of either 6 mm or 8 mm diameter beads.","The third system is a bidisperse mixture of the previous two bead sizes.","During data collection, the particles are confined to a box; on top of this box, and resting on the granular material, is a light, rigid sheet onto which pressure can be applied to the system.","To excite the material, a steel impactor ball is dropped on top of the system.","The response of the granular material to the impact pulse is recorded by piezoelectric sensors buried throughout the material, and the density of modes is computed from the spectrum of the velocity autocorrelation of these sensors.","Our measurements of the density of modes show more low frequency modes at low pressure, consistent with previous experimental and numerical results, as well as several low frequency peaks in the density of modes that shift with applied pressure.","Finally, we also see that the density of modes falls off at a wavelength on the order of twice the particle diameter, which is reminiscent of the Debye frequency."],"url":"http://arxiv.org/abs/2403.10322v1","category":"cond-mat.soft"}
{"created":"2024-03-15 13:59:05","title":"Interactive Trimming against Evasive Online Data Manipulation Attacks: A Game-Theoretic Approach","abstract":"With the exponential growth of data and its crucial impact on our lives and decision-making, the integrity of data has become a significant concern. Malicious data poisoning attacks, where false values are injected into the data, can disrupt machine learning processes and lead to severe consequences. To mitigate these attacks, distance-based defenses, such as trimming, have been proposed, but they can be easily evaded by white-box attackers. The evasiveness and effectiveness of poisoning attack strategies are two sides of the same coin, making game theory a promising approach. However, existing game-theoretical models often overlook the complexities of online data poisoning attacks, where strategies must adapt to the dynamic process of data collection.   In this paper, we present an interactive game-theoretical model to defend online data manipulation attacks using the trimming strategy. Our model accommodates a complete strategy space, making it applicable to strong evasive and colluding adversaries. Leveraging the principle of least action and the Euler-Lagrange equation from theoretical physics, we derive an analytical model for the game-theoretic process. To demonstrate its practical usage, we present a case study in a privacy-preserving data collection system under local differential privacy where a non-deterministic utility function is adopted. Two strategies are devised from this analytical model, namely, Tit-for-tat and Elastic. We conduct extensive experiments on real-world datasets, which showcase the effectiveness and accuracy of these two strategies.","sentences":["With the exponential growth of data and its crucial impact on our lives and decision-making, the integrity of data has become a significant concern.","Malicious data poisoning attacks, where false values are injected into the data, can disrupt machine learning processes and lead to severe consequences.","To mitigate these attacks, distance-based defenses, such as trimming, have been proposed, but they can be easily evaded by white-box attackers.","The evasiveness and effectiveness of poisoning attack strategies are two sides of the same coin, making game theory a promising approach.","However, existing game-theoretical models often overlook the complexities of online data poisoning attacks, where strategies must adapt to the dynamic process of data collection.   ","In this paper, we present an interactive game-theoretical model to defend online data manipulation attacks using the trimming strategy.","Our model accommodates a complete strategy space, making it applicable to strong evasive and colluding adversaries.","Leveraging the principle of least action and the Euler-Lagrange equation from theoretical physics, we derive an analytical model for the game-theoretic process.","To demonstrate its practical usage, we present a case study in a privacy-preserving data collection system under local differential privacy where a non-deterministic utility function is adopted.","Two strategies are devised from this analytical model, namely, Tit-for-tat and Elastic.","We conduct extensive experiments on real-world datasets, which showcase the effectiveness and accuracy of these two strategies."],"url":"http://arxiv.org/abs/2403.10313v1","category":"cs.CR"}
{"created":"2024-03-15 13:26:39","title":"Local positional graphs and attentive local features for a data and runtime-efficient hierarchical place recognition pipeline","abstract":"Large-scale applications of Visual Place Recognition (VPR) require computationally efficient approaches. Further, a well-balanced combination of data-based and training-free approaches can decrease the required amount of training data and effort and can reduce the influence of distribution shifts between the training and application phases. This paper proposes a runtime and data-efficient hierarchical VPR pipeline that extends existing approaches and presents novel ideas. There are three main contributions: First, we propose Local Positional Graphs (LPG), a training-free and runtime-efficient approach to encode spatial context information of local image features. LPG can be combined with existing local feature detectors and descriptors and considerably improves the image-matching quality compared to existing techniques in our experiments. Second, we present Attentive Local SPED (ATLAS), an extension of our previous local features approach with an attention module that improves the feature quality while maintaining high data efficiency. The influence of the proposed modifications is evaluated in an extensive ablation study. Third, we present a hierarchical pipeline that exploits hyperdimensional computing to use the same local features as holistic HDC-descriptors for fast candidate selection and for candidate reranking. We combine all contributions in a runtime and data-efficient VPR pipeline that shows benefits over the state-of-the-art method Patch-NetVLAD on a large collection of standard place recognition datasets with 15$\\%$ better performance in VPR accuracy, 54$\\times$ faster feature comparison speed, and 55$\\times$ less descriptor storage occupancy, making our method promising for real-world high-performance large-scale VPR in changing environments. Code will be made available with publication of this paper.","sentences":["Large-scale applications of Visual Place Recognition (VPR) require computationally efficient approaches.","Further, a well-balanced combination of data-based and training-free approaches can decrease the required amount of training data and effort and can reduce the influence of distribution shifts between the training and application phases.","This paper proposes a runtime and data-efficient hierarchical VPR pipeline that extends existing approaches and presents novel ideas.","There are three main contributions: First, we propose Local Positional Graphs (LPG), a training-free and runtime-efficient approach to encode spatial context information of local image features.","LPG can be combined with existing local feature detectors and descriptors and considerably improves the image-matching quality compared to existing techniques in our experiments.","Second, we present Attentive Local SPED (ATLAS), an extension of our previous local features approach with an attention module that improves the feature quality while maintaining high data efficiency.","The influence of the proposed modifications is evaluated in an extensive ablation study.","Third, we present a hierarchical pipeline that exploits hyperdimensional computing to use the same local features as holistic HDC-descriptors for fast candidate selection and for candidate reranking.","We combine all contributions in a runtime and data-efficient VPR pipeline that shows benefits over the state-of-the-art method Patch-NetVLAD on a large collection of standard place recognition datasets with 15$\\%$ better performance in VPR accuracy, 54$\\times$ faster feature comparison speed, and 55$\\times$ less descriptor storage occupancy, making our method promising for real-world high-performance large-scale VPR in changing environments.","Code will be made available with publication of this paper."],"url":"http://arxiv.org/abs/2403.10283v1","category":"cs.CV"}
{"created":"2024-03-15 12:46:26","title":"Kink Crystal","abstract":"We describe a one-dimensional kink crystal, which represents a collection of equal and equally localized kinks forming a lattice in the real axis. The results are analytical, original and may motivate other studies on localized structures in high energy physics.","sentences":["We describe a one-dimensional kink crystal, which represents a collection of equal and equally localized kinks forming a lattice in the real axis.","The results are analytical, original and may motivate other studies on localized structures in high energy physics."],"url":"http://arxiv.org/abs/2403.10257v1","category":"hep-th"}
{"created":"2024-03-15 12:01:28","title":"Virial Black Hole Masses for AGNs behind the Magellanic Clouds","abstract":"We use the spectroscopic data collected by the Magellanic Quasars Survey (MQS) as well as the photometric V- and I-band data from the Optical Gravitational Lensing Experiment (OGLE) to measure the physical parameters for active galactic nuclei (AGNs) located behind the Magellanic Clouds. The flux-uncalibrated MQS spectra were obtained with the 4-m Anglo-Australian Telescope and the AAOmega spectroscope (R=1300) in a typical ~1.5-hour visit. They span a spectral range of 3700-8500 Angstroms and have S/N ratios in a range of 3-300. We report the discovery and observational properties of 161 AGNs in this footprint, which expands the total number of spectroscopically confirmed AGNs by MQS to 919. After converting the OGLE mean magnitudes to the monochromatic luminosities at 5100 Angstroms, 3000 Angstroms, and 1350 Angstroms, we reliably measured the black hole masses for 165 out of 919 AGNs. The remaining physical parameters we provide are the bolometric luminosities and the Eddington ratios. A fraction of these AGNs have been observed by the OGLE survey since 1997 (all of them since 2001), enabling studies of correlations between their variability and physical parameters.","sentences":["We use the spectroscopic data collected by the Magellanic Quasars Survey (MQS) as well as the photometric V- and I-band data from the Optical Gravitational Lensing Experiment (OGLE) to measure the physical parameters for active galactic nuclei (AGNs) located behind the Magellanic Clouds.","The flux-uncalibrated MQS spectra were obtained with the 4-m Anglo-Australian Telescope and the AAOmega spectroscope (R=1300) in a typical ~1.5-hour visit.","They span a spectral range of 3700-8500 Angstroms and have S/N ratios in a range of 3-300.","We report the discovery and observational properties of 161 AGNs in this footprint, which expands the total number of spectroscopically confirmed AGNs by MQS to 919.","After converting the OGLE mean magnitudes to the monochromatic luminosities at 5100 Angstroms, 3000 Angstroms, and 1350 Angstroms, we reliably measured the black hole masses for 165 out of 919 AGNs.","The remaining physical parameters we provide are the bolometric luminosities and the Eddington ratios.","A fraction of these AGNs have been observed by the OGLE survey since 1997 (all of them since 2001), enabling studies of correlations between their variability and physical parameters."],"url":"http://arxiv.org/abs/2403.10233v1","category":"astro-ph.GA"}
{"created":"2024-03-15 10:11:26","title":"Online Policy Learning from Offline Preferences","abstract":"In preference-based reinforcement learning (PbRL), a reward function is learned from a type of human feedback called preference. To expedite preference collection, recent works have leveraged \\emph{offline preferences}, which are preferences collected for some offline data. In this scenario, the learned reward function is fitted on the offline data. If a learning agent exhibits behaviors that do not overlap with the offline data, the learned reward function may encounter generalizability issues. To address this problem, the present study introduces a framework that consolidates offline preferences and \\emph{virtual preferences} for PbRL, which are comparisons between the agent's behaviors and the offline data. Critically, the reward function can track the agent's behaviors using the virtual preferences, thereby offering well-aligned guidance to the agent. Through experiments on continuous control tasks, this study demonstrates the effectiveness of incorporating the virtual preferences in PbRL.","sentences":["In preference-based reinforcement learning (PbRL), a reward function is learned from a type of human feedback called preference.","To expedite preference collection, recent works have leveraged \\emph{offline preferences}, which are preferences collected for some offline data.","In this scenario, the learned reward function is fitted on the offline data.","If a learning agent exhibits behaviors that do not overlap with the offline data, the learned reward function may encounter generalizability issues.","To address this problem, the present study introduces a framework that consolidates offline preferences and \\emph{virtual preferences} for PbRL, which are comparisons between the agent's behaviors and the offline data.","Critically, the reward function can track the agent's behaviors using the virtual preferences, thereby offering well-aligned guidance to the agent.","Through experiments on continuous control tasks, this study demonstrates the effectiveness of incorporating the virtual preferences in PbRL."],"url":"http://arxiv.org/abs/2403.10160v1","category":"cs.LG"}
{"created":"2024-03-15 09:51:07","title":"Self-Aligning Polar Active Matter","abstract":"Self-alignment describes the property of a polar active unit to align or anti-align its orientation towards its velocity. In contrast to mutual alignment, where the headings of multiple active units tend to directly align to each other -- as in the celebrated Vicsek model --, self-alignment impacts the dynamics at the individual level by coupling the rotation and displacements of each active unit. This enriches the dynamics even without interactions or external forces, and allows, for example, a single self-propelled particle to orbit in a harmonic potential. At the collective level, self-alignment modifies the nature of the transition to collective motion already in the mean field description, and it can also lead to other forms of self-organization such as collective actuation in dense or solid elastic assemblies of active units. This has significant implications for the study of dense biological systems, metamaterials, and swarm robotics. Here, we review a number of models that were introduced independently to describe the previously overlooked property of self-alignment and identify some of its experimental realizations. Our aim is three-fold: (i)~underline the importance of self-alignment in active systems, especially in the context of dense populations of active units and active solids; (ii)~provide a unified mathematical and conceptual framework for the description of self-aligning systems; (iii)~discuss the common features and specific differences of the existing models of self-alignment. We conclude by discussing promising research avenues in which the concept of self-alignment could play a significant role.","sentences":["Self-alignment describes the property of a polar active unit to align or anti-align its orientation towards its velocity.","In contrast to mutual alignment, where the headings of multiple active units tend to directly align to each other -- as in the celebrated Vicsek model --, self-alignment impacts the dynamics at the individual level by coupling the rotation and displacements of each active unit.","This enriches the dynamics even without interactions or external forces, and allows, for example, a single self-propelled particle to orbit in a harmonic potential.","At the collective level, self-alignment modifies the nature of the transition to collective motion already in the mean field description, and it can also lead to other forms of self-organization such as collective actuation in dense or solid elastic assemblies of active units.","This has significant implications for the study of dense biological systems, metamaterials, and swarm robotics.","Here, we review a number of models that were introduced independently to describe the previously overlooked property of self-alignment and identify some of its experimental realizations.","Our aim is three-fold: (i)~underline the importance of self-alignment in active systems, especially in the context of dense populations of active units and active solids; (ii)~provide a unified mathematical and conceptual framework for the description of self-aligning systems; (iii)~discuss the common features and specific differences of the existing models of self-alignment.","We conclude by discussing promising research avenues in which the concept of self-alignment could play a significant role."],"url":"http://arxiv.org/abs/2403.10151v1","category":"cond-mat.soft"}
{"created":"2024-03-15 17:59:44","title":"Strong and Controllable Blind Image Decomposition","abstract":"Blind image decomposition aims to decompose all components present in an image, typically used to restore a multi-degraded input image. While fully recovering the clean image is appealing, in some scenarios, users might want to retain certain degradations, such as watermarks, for copyright protection. To address this need, we add controllability to the blind image decomposition process, allowing users to enter which types of degradation to remove or retain. We design an architecture named controllable blind image decomposition network. Inserted in the middle of U-Net structure, our method first decomposes the input feature maps and then recombines them according to user instructions. Advantageously, this functionality is implemented at minimal computational cost: decomposition and recombination are all parameter-free. Experimentally, our system excels in blind image decomposition tasks and can outputs partially or fully restored images that well reflect user intentions. Furthermore, we evaluate and configure different options for the network structure and loss functions. This, combined with the proposed decomposition-and-recombination method, yields an efficient and competitive system for blind image decomposition, compared with current state-of-the-art methods.","sentences":["Blind image decomposition aims to decompose all components present in an image, typically used to restore a multi-degraded input image.","While fully recovering the clean image is appealing, in some scenarios, users might want to retain certain degradations, such as watermarks, for copyright protection.","To address this need, we add controllability to the blind image decomposition process, allowing users to enter which types of degradation to remove or retain.","We design an architecture named controllable blind image decomposition network.","Inserted in the middle of U-Net structure, our method first decomposes the input feature maps and then recombines them according to user instructions.","Advantageously, this functionality is implemented at minimal computational cost: decomposition and recombination are all parameter-free.","Experimentally, our system excels in blind image decomposition tasks and can outputs partially or fully restored images that well reflect user intentions.","Furthermore, we evaluate and configure different options for the network structure and loss functions.","This, combined with the proposed decomposition-and-recombination method, yields an efficient and competitive system for blind image decomposition, compared with current state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.10520v1","category":"cs.CV"}
{"created":"2024-03-15 17:53:07","title":"Surveyor: Facilitating Discovery Within Video Games for Blind and Low Vision Players","abstract":"Video games are increasingly accessible to blind and low vision (BLV) players, yet many aspects remain inaccessible. One aspect is the joy players feel when they explore environments and make new discoveries, which is integral to many games. Sighted players experience discovery by surveying environments and identifying unexplored areas. Current accessibility tools, however, guide BLV players directly to items and places, robbing them of that experience. Thus, a crucial challenge is to develop navigation assistance tools that also foster exploration and discovery. To address this challenge, we propose the concept of exploration assistance in games and design Surveyor, an in-game exploration assistance tool that enhances discovery by tracking where BLV players look and highlighting unexplored areas. We designed Surveyor using insights from a formative study and compared Surveyor's effectiveness to approaches found in existing accessible games. Our findings reveal implications for facilitating richer play experiences for BLV users within games.","sentences":["Video games are increasingly accessible to blind and low vision (BLV) players, yet many aspects remain inaccessible.","One aspect is the joy players feel when they explore environments and make new discoveries, which is integral to many games.","Sighted players experience discovery by surveying environments and identifying unexplored areas.","Current accessibility tools, however, guide BLV players directly to items and places, robbing them of that experience.","Thus, a crucial challenge is to develop navigation assistance tools that also foster exploration and discovery.","To address this challenge, we propose the concept of exploration assistance in games and design Surveyor, an in-game exploration assistance tool that enhances discovery by tracking where BLV players look and highlighting unexplored areas.","We designed Surveyor using insights from a formative study and compared Surveyor's effectiveness to approaches found in existing accessible games.","Our findings reveal implications for facilitating richer play experiences for BLV users within games."],"url":"http://arxiv.org/abs/2403.10512v1","category":"cs.HC"}
{"created":"2024-03-15 17:32:02","title":"Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings","abstract":"Algorithmic verification of realistic systems to satisfy safety and other temporal requirements has suffered from poor scalability of the employed formal approaches. To design systems with rigorous guarantees, many approaches still rely on exact models of the underlying systems. Since this assumption can rarely be met in practice, models have to be inferred from measurement data or are bypassed completely. Whilst former usually requires the model structure to be known a-priori and immense amounts of data to be available, latter gives rise to a plethora of restrictive mathematical assumptions about the unknown dynamics. In a pursuit of developing scalable formal verification algorithms without shifting the problem to unrealistic assumptions, we employ the concept of barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a compact set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result w.r.t. a set of plausible transition kernels. We show how to solve the resulting program efficiently using sum-of-squares optimization and a Gaussian process envelope. Our approach lifts the need for restrictive assumptions on the system dynamics and uncertainty, and suggests an improvement in the sample complexity of verifying the safety of a system on a tested case study compared to a state-of-the-art approach.","sentences":["Algorithmic verification of realistic systems to satisfy safety and other temporal requirements has suffered from poor scalability of the employed formal approaches.","To design systems with rigorous guarantees, many approaches still rely on exact models of the underlying systems.","Since this assumption can rarely be met in practice, models have to be inferred from measurement data or are bypassed completely.","Whilst former usually requires the model structure to be known a-priori and immense amounts of data to be available, latter gives rise to a plethora of restrictive mathematical assumptions about the unknown dynamics.","In a pursuit of developing scalable formal verification algorithms without shifting the problem to unrealistic assumptions, we employ the concept of barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a compact set of system trajectories.","We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result w.r.t.","a set of plausible transition kernels.","We show how to solve the resulting program efficiently using sum-of-squares optimization and a Gaussian process envelope.","Our approach lifts the need for restrictive assumptions on the system dynamics and uncertainty, and suggests an improvement in the sample complexity of verifying the safety of a system on a tested case study compared to a state-of-the-art approach."],"url":"http://arxiv.org/abs/2403.10497v1","category":"eess.SY"}
{"created":"2024-03-15 17:31:42","title":"Reconfigurable Robot Identification from Motion Data","abstract":"Integrating Large Language Models (VLMs) and Vision-Language Models (VLMs) with robotic systems enables robots to process and understand complex natural language instructions and visual information. However, a fundamental challenge remains: for robots to fully capitalize on these advancements, they must have a deep understanding of their physical embodiment. The gap between AI models cognitive capabilities and the understanding of physical embodiment leads to the following question: Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment? This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure. Here, we propose a meta self modeling that can deduce robot morphology through proprioception (the internal sense of position and movement). Our study introduces a 12 DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology. Utilizing a deep neural network model comprising a robot signature encoder and a configuration decoder, we demonstrate the capability of our system to accurately predict robot configurations from proprioceptive signals. This research contributes to the field of robotic self-modeling, aiming to enhance understanding of their physical embodiment and adaptability in real world scenarios.","sentences":["Integrating Large Language Models (VLMs) and Vision-Language Models (VLMs) with robotic systems enables robots to process and understand complex natural language instructions and visual information.","However, a fundamental challenge remains: for robots to fully capitalize on these advancements, they must have a deep understanding of their physical embodiment.","The gap between AI models cognitive capabilities and the understanding of physical embodiment leads to the following question: Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment?","This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure.","Here, we propose a meta self modeling that can deduce robot morphology through proprioception (the internal sense of position and movement).","Our study introduces a 12 DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology.","Utilizing a deep neural network model comprising a robot signature encoder and a configuration decoder, we demonstrate the capability of our system to accurately predict robot configurations from proprioceptive signals.","This research contributes to the field of robotic self-modeling, aiming to enhance understanding of their physical embodiment and adaptability in real world scenarios."],"url":"http://arxiv.org/abs/2403.10496v1","category":"cs.RO"}
{"created":"2024-03-15 17:26:02","title":"Detection of ionized hydrogen and oxygen from a very luminous and young galaxy 13.4 billion years ago","abstract":"The James Webb Space Telescope (JWST) has discovered a surprising population of bright galaxies in the very early universe (< 500 Myrs after the Big Bang) that is hard to explain with conventional galaxy formation models and whose physical properties remain to be fully understood. Insight into the internal physics of galaxies is captured best via observations of excited-state atomic transitions of ionized gas, but beyond z~7-9, the brightest spectral signatures are redshifted into the mid-infrared regime, where observations are increasingly more difficult. Here, we present the first detection of a hydrogen recombination line (H{\\alpha}) and doubly-ionized oxygen ([OIII]4959,5007{\\AA}) at z>10 using the JWST Mid-Infrared Instrument, MIRI. These detections place the bright galaxy GHZ2/GLASS-z12 at z=12.33+/-0.02, making it the most distant astronomical object with direct spectroscopic detection of these lines and the brightest confirmed object at this epoch. These observations provide key insights into the conditions of this primeval galaxy, which shows hard ionizing conditions rarely seen in the local Universe and likely driven by compact, young (<30 Myr) star formation. Its oxygen-to-hydrogen abundance is close to a tenth of the solar value, indicating a rapid metal enrichment during the earliest phases of galaxy formation. This study confirms the unique conditions of the brightest and most distant galaxies recently discovered by JWST and the huge potential of mid-IR observations to characterize these systems, opening a range of new possibilities in the study of the very early Universe.","sentences":["The James Webb Space Telescope (JWST) has discovered a surprising population of bright galaxies in the very early universe (< 500 Myrs after the Big Bang) that is hard to explain with conventional galaxy formation models and whose physical properties remain to be fully understood.","Insight into the internal physics of galaxies is captured best via observations of excited-state atomic transitions of ionized gas, but beyond z~7-9, the brightest spectral signatures are redshifted into the mid-infrared regime, where observations are increasingly more difficult.","Here, we present the first detection of a hydrogen recombination line (H{\\alpha}) and doubly-ionized oxygen ([OIII]4959,5007{\\AA}) at z>10 using the JWST Mid-Infrared Instrument, MIRI.","These detections place the bright galaxy GHZ2/GLASS-z12 at z=12.33+/-0.02, making it the most distant astronomical object with direct spectroscopic detection of these lines and the brightest confirmed object at this epoch.","These observations provide key insights into the conditions of this primeval galaxy, which shows hard ionizing conditions rarely seen in the local Universe and likely driven by compact, young (<30 Myr) star formation.","Its oxygen-to-hydrogen abundance is close to a tenth of the solar value, indicating a rapid metal enrichment during the earliest phases of galaxy formation.","This study confirms the unique conditions of the brightest and most distant galaxies recently discovered by JWST and the huge potential of mid-IR observations to characterize these systems, opening a range of new possibilities in the study of the very early Universe."],"url":"http://arxiv.org/abs/2403.10491v1","category":"astro-ph.GA"}
{"created":"2024-03-15 17:23:38","title":"Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild","abstract":"Audiovisual emotion recognition (ER) in videos has immense potential over unimodal performance. It effectively leverages the inter- and intra-modal dependencies between visual and auditory modalities. This work proposes a novel audio-visual emotion recognition system utilizing a joint multimodal transformer architecture with key-based cross-attention. This framework aims to exploit the complementary nature of audio and visual cues (facial expressions and vocal patterns) in videos, leading to superior performance compared to solely relying on a single modality. The proposed model leverages separate backbones for capturing intra-modal temporal dependencies within each modality (audio and visual). Subsequently, a joint multimodal transformer architecture integrates the individual modality embeddings, enabling the model to effectively capture inter-modal (between audio and visual) and intra-modal (within each modality) relationships. Extensive evaluations on the challenging Affwild2 dataset demonstrate that the proposed model significantly outperforms baseline and state-of-the-art methods in ER tasks.","sentences":["Audiovisual emotion recognition (ER) in videos has immense potential over unimodal performance.","It effectively leverages the inter- and intra-modal dependencies between visual and auditory modalities.","This work proposes a novel audio-visual emotion recognition system utilizing a joint multimodal transformer architecture with key-based cross-attention.","This framework aims to exploit the complementary nature of audio and visual cues (facial expressions and vocal patterns) in videos, leading to superior performance compared to solely relying on a single modality.","The proposed model leverages separate backbones for capturing intra-modal temporal dependencies within each modality (audio and visual).","Subsequently, a joint multimodal transformer architecture integrates the individual modality embeddings, enabling the model to effectively capture inter-modal (between audio and visual) and intra-modal (within each modality) relationships.","Extensive evaluations on the challenging Affwild2 dataset demonstrate that the proposed model significantly outperforms baseline and state-of-the-art methods in ER tasks."],"url":"http://arxiv.org/abs/2403.10488v1","category":"cs.CV"}
{"created":"2024-03-15 17:20:39","title":"The inhomogeneous $t$-PushTASEP and Macdonald polynomials","abstract":"We study a multispecies $t$-PushTASEP system on a finite ring of $n$ sites with site-dependent rates $x_1,\\dots,x_n$. Let $\\lambda=(\\lambda_1,\\dots,\\lambda_n)$ be a partition whose parts represent the species of the $n$ particles on the ring. We show that for each composition $\\eta$ obtained by permuting the parts of $\\lambda$, the stationary probability of being in state $\\eta$ is proportional to the ASEP polynomial $F_{\\eta}(x_1,\\dots,x_n; q,t)$ at $q=1$; the normalizing constant (or partition function) is the Macdonald polynomial $P_{\\lambda}(x_1,\\dots,x_n;q,t)$ at $q=1$. Our approach involves new relations between the families of ASEP polynomials and of non-symmetric Macdonald polynomials at $q=1$. We also use multiline diagrams, showing that a single jump of the PushTASEP system is closely related to the operation of moving from one line to the next in a multiline diagram. We derive symmetry properties for the system under permutation of its jump rates, as well as a formula for the current of a single-species system.","sentences":["We study a multispecies $t$-PushTASEP system on a finite ring of $n$ sites with site-dependent rates $x_1,\\dots,x_n$. Let $\\lambda=(\\lambda_1,\\dots,\\lambda_n)$ be a partition whose parts represent the species of the $n$ particles on the ring.","We show that for each composition $\\eta$ obtained by permuting the parts of $\\lambda$, the stationary probability of being in state $\\eta$ is proportional to the ASEP polynomial $F_{\\eta}(x_1,\\dots,x_n; q,t)$ at $q=1$; the normalizing constant (or partition function) is the Macdonald polynomial $P_{\\lambda}(x_1,\\dots,x_n;q,t)$ at $q=1$. Our approach involves new relations between the families of ASEP polynomials and of non-symmetric Macdonald polynomials at $q=1$. We also use multiline diagrams, showing that a single jump of the PushTASEP system is closely related to the operation of moving from one line to the next in a multiline diagram.","We derive symmetry properties for the system under permutation of its jump rates, as well as a formula for the current of a single-species system."],"url":"http://arxiv.org/abs/2403.10485v1","category":"math.CO"}
{"created":"2024-03-15 17:12:50","title":"Tensor Star Decomposition","abstract":"A novel tensor decomposition framework, termed Tensor Star (TS) decomposition, is proposed which represents a new type of tensor network decomposition based on tensor contractions. This is achieved by connecting the core tensors in a ring shape, whereby the core tensors act as skip connections between the factor tensors and allow for direct correlation characterisation between any two arbitrary dimensions. Uniquely, this makes it possible to decompose an order-$N$ tensor into $N$ order-$3$ factor tensors $\\{\\mathcal{G}_{k}\\}_{k=1}^{N}$ and $N$ order-$4$ core tensors $\\{\\mathcal{C}_{k}\\}_{k=1}^{N}$, which are arranged in a star shape. Unlike the class of Tensor Train (TT) decompositions, these factor tensors are not directly connected to one another. The so obtained core tensors also enable consecutive factor tensors to have different latent ranks. In this way, the TS decomposition alleviates the \"curse of dimensionality\" and controls the \"curse of ranks\", exhibiting a storage complexity which scales linearly with the number of dimensions and as the fourth power of the ranks.","sentences":["A novel tensor decomposition framework, termed Tensor Star (TS) decomposition, is proposed which represents a new type of tensor network decomposition based on tensor contractions.","This is achieved by connecting the core tensors in a ring shape, whereby the core tensors act as skip connections between the factor tensors and allow for direct correlation characterisation between any two arbitrary dimensions.","Uniquely, this makes it possible to decompose an order-$N$ tensor into $N$ order-$3$ factor tensors $\\{\\mathcal{G}_{k}\\}_{k=1}^{N}$ and $N$ order-$4$ core tensors $\\{\\mathcal{C}_{k}\\}_{k=1}^{N}$, which are arranged in a star shape.","Unlike the class of Tensor Train (TT) decompositions, these factor tensors are not directly connected to one another.","The so obtained core tensors also enable consecutive factor tensors to have different latent ranks.","In this way, the TS decomposition alleviates the \"curse of dimensionality\" and controls the \"curse of ranks\", exhibiting a storage complexity which scales linearly with the number of dimensions and as the fourth power of the ranks."],"url":"http://arxiv.org/abs/2403.10481v1","category":"eess.IV"}
{"created":"2024-03-15 16:56:17","title":"Chiral-stress-energy-momentum tensor for covariant description of spin and torque densities of light","abstract":"The measurement of the spin angular momentum of circularly polarized light by Beth [Phys. Rev. 50, 115 (1936)] can be explained by using a microscopic torque density. However, the experiment does not resolve the space- and time-dependent evolution of the spin density of light and the wave plate and the covariant form of the microscopic torque density. Here we focus on the covariant description of the helicity, spin, and torque densities of light in materials using the chiral-stress-energy-momentum tensor. We also perform simulations of Gaussian light pulses in quarter-wave-plate geometries made of birefringent and dielectric materials.","sentences":["The measurement of the spin angular momentum of circularly polarized light by Beth [Phys. Rev. 50, 115 (1936)] can be explained by using a microscopic torque density.","However, the experiment does not resolve the space- and time-dependent evolution of the spin density of light and the wave plate and the covariant form of the microscopic torque density.","Here we focus on the covariant description of the helicity, spin, and torque densities of light in materials using the chiral-stress-energy-momentum tensor.","We also perform simulations of Gaussian light pulses in quarter-wave-plate geometries made of birefringent and dielectric materials."],"url":"http://arxiv.org/abs/2403.10466v1","category":"physics.optics"}
{"created":"2024-03-15 16:45:06","title":"Thermodynamic properties of a superconductor interfaced with an altermagnet","abstract":"Recently introduced magnetic materials called altermagnets (AM) feature zero net magnetization but a momentum dependent magnetic exchange field, which can have intriguing implications when combined with superconductivity. In our work, we use the quasiclassical framework to study the effects of such a material on a conventional superconductor (S) in an AM/S bilayer. We discuss the superconducting phase diagram and heat capacity of AM/S while making a comparison with a ferromagnet-superconductor bilayer. Furthermore, we examine the density of states and analyze the system's response to an external magnetic field. We illustrate the anisotropy of spin-susceptibility and magnetization of AM/S by considering an external field in the in-plane and out-of-plane direction, thereby facilitating the scope of experimental detection and characterization of an AM in an AM/S hybrid system.","sentences":["Recently introduced magnetic materials called altermagnets (AM) feature zero net magnetization but a momentum dependent magnetic exchange field, which can have intriguing implications when combined with superconductivity.","In our work, we use the quasiclassical framework to study the effects of such a material on a conventional superconductor (S) in an AM/S bilayer.","We discuss the superconducting phase diagram and heat capacity of AM/S while making a comparison with a ferromagnet-superconductor bilayer.","Furthermore, we examine the density of states and analyze the system's response to an external magnetic field.","We illustrate the anisotropy of spin-susceptibility and magnetization of AM/S by considering an external field in the in-plane and out-of-plane direction, thereby facilitating the scope of experimental detection and characterization of an AM in an AM/S hybrid system."],"url":"http://arxiv.org/abs/2403.10456v1","category":"cond-mat.supr-con"}
{"created":"2024-03-15 16:43:21","title":"Evaluation of Quantum and Hybrid Solvers for Combinatorial Optimization","abstract":"Academic and industrial sectors have been engaged in a fierce competition to develop quantum technologies, fueled by the explosive advancements in quantum hardware. While universal quantum computers have been shown to support up to hundreds of qubits, the scale of quantum annealers has reached three orders of magnitude (i.e., thousands of qubits). Therefore, quantum algorithms are becoming increasingly popular in a variety of fields, with optimization being one of the most prominent. This work aims to explore the topic of quantum optimization by comprehensively evaluating the technologies provided by D-Wave Systems. To do so, a model for the energy optimization of data centers is proposed as a benchmark. D-Wave quantum and hybrid solvers are compared, in order to identify the most suitable one for the considered application. To highlight its advantageous performance capabilities and associated solving potential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a highly efficient classical solver.","sentences":["Academic and industrial sectors have been engaged in a fierce competition to develop quantum technologies, fueled by the explosive advancements in quantum hardware.","While universal quantum computers have been shown to support up to hundreds of qubits, the scale of quantum annealers has reached three orders of magnitude (i.e., thousands of qubits).","Therefore, quantum algorithms are becoming increasingly popular in a variety of fields, with optimization being one of the most prominent.","This work aims to explore the topic of quantum optimization by comprehensively evaluating the technologies provided by D-Wave Systems.","To do so, a model for the energy optimization of data centers is proposed as a benchmark.","D-Wave quantum and hybrid solvers are compared, in order to identify the most suitable one for the considered application.","To highlight its advantageous performance capabilities and associated solving potential, the selected D-Wave hybrid solver is then contrasted with CPLEX, a highly efficient classical solver."],"url":"http://arxiv.org/abs/2403.10455v1","category":"quant-ph"}
{"created":"2024-03-15 16:33:57","title":"Stationary non-radial localized patterns in the planar Swift-Hohenberg PDE: constructive proofs of existence","abstract":"In this paper, we present a methodology for establishing constructive proofs of existence of smooth, stationary, non-radial localized patterns in the planar Swift-Hohenberg equation. Specifically, given an approximate solution $u_0$, we construct an approximate inverse for the linearization around $u_0$, enabling the development of a Newton-Kantorovich approach. Consequently, we derive a sufficient condition for the existence of a unique localized pattern in the vicinity of $u_0$. The verification of this condition is facilitated through a combination of analytic techniques and rigorous numerical computations. Moreover, an additional condition is derived, establishing that the localized pattern serves as the limit of a family of periodic solutions (in space) as the period tends to infinity. The integration of analytical tools and meticulous numerical analysis ensures a comprehensive validation of this condition. To illustrate the efficacy of the proposed methodology, we present computer-assisted proofs for the existence of three distinct unbounded branches of periodic solutions in the planar Swift-Hohenberg equation, all converging towards a localized planar pattern, whose existence is also proven constructively. All computer-assisted proofs, including the requisite codes, are accessible on GitHub at \\cite{julia_cadiot}.","sentences":["In this paper, we present a methodology for establishing constructive proofs of existence of smooth, stationary, non-radial localized patterns in the planar Swift-Hohenberg equation.","Specifically, given an approximate solution $u_0$, we construct an approximate inverse for the linearization around $u_0$, enabling the development of a Newton-Kantorovich approach.","Consequently, we derive a sufficient condition for the existence of a unique localized pattern in the vicinity of $u_0$. The verification of this condition is facilitated through a combination of analytic techniques and rigorous numerical computations.","Moreover, an additional condition is derived, establishing that the localized pattern serves as the limit of a family of periodic solutions (in space) as the period tends to infinity.","The integration of analytical tools and meticulous numerical analysis ensures a comprehensive validation of this condition.","To illustrate the efficacy of the proposed methodology, we present computer-assisted proofs for the existence of three distinct unbounded branches of periodic solutions in the planar Swift-Hohenberg equation, all converging towards a localized planar pattern, whose existence is also proven constructively.","All computer-assisted proofs, including the requisite codes, are accessible on GitHub at \\cite{julia_cadiot}."],"url":"http://arxiv.org/abs/2403.10450v1","category":"math.AP"}
{"created":"2024-03-15 16:20:10","title":"H-MaP: An Iterative and Hybrid Sequential Manipulation Planner","abstract":"This study introduces the Hybrid Sequential Manipulation Planner (H-MaP), a novel approach that iteratively does motion planning using contact points and waypoints for complex sequential manipulation tasks in robotics. Combining optimization-based methods for generalizability and sampling-based methods for robustness, H-MaP enhances manipulation planning through active contact mode switches and enables interactions with auxiliary objects and tools. This framework, validated by a series of diverse physical manipulation tasks and real-robot experiments, offers a scalable and adaptable solution for complex real-world applications in robotic manipulation.","sentences":["This study introduces the Hybrid Sequential Manipulation Planner (H-MaP), a novel approach that iteratively does motion planning using contact points and waypoints for complex sequential manipulation tasks in robotics.","Combining optimization-based methods for generalizability and sampling-based methods for robustness, H-MaP enhances manipulation planning through active contact mode switches and enables interactions with auxiliary objects and tools.","This framework, validated by a series of diverse physical manipulation tasks and real-robot experiments, offers a scalable and adaptable solution for complex real-world applications in robotic manipulation."],"url":"http://arxiv.org/abs/2403.10436v1","category":"cs.RO"}
{"created":"2024-03-15 15:58:20","title":"Quantization Avoids Saddle Points in Distributed Optimization","abstract":"Distributed nonconvex optimization underpins key functionalities of numerous distributed systems, ranging from power systems, smart buildings, cooperative robots, vehicle networks to sensor networks. Recently, it has also merged as a promising solution to handle the enormous growth in data and model sizes in deep learning. A fundamental problem in distributed nonconvex optimization is avoiding convergence to saddle points, which significantly degrade optimization accuracy. We discover that the process of quantization, which is necessary for all digital communications, can be exploited to enable saddle-point avoidance. More specifically, we propose a stochastic quantization scheme and prove that it can effectively escape saddle points and ensure convergence to a second-order stationary point in distributed nonconvex optimization. With an easily adjustable quantization granularity, the approach allows a user to control the number of bits sent per iteration and, hence, to aggressively reduce the communication overhead. Numerical experimental results using distributed optimization and learning problems on benchmark datasets confirm the effectiveness of the approach.","sentences":["Distributed nonconvex optimization underpins key functionalities of numerous distributed systems, ranging from power systems, smart buildings, cooperative robots, vehicle networks to sensor networks.","Recently, it has also merged as a promising solution to handle the enormous growth in data and model sizes in deep learning.","A fundamental problem in distributed nonconvex optimization is avoiding convergence to saddle points, which significantly degrade optimization accuracy.","We discover that the process of quantization, which is necessary for all digital communications, can be exploited to enable saddle-point avoidance.","More specifically, we propose a stochastic quantization scheme and prove that it can effectively escape saddle points and ensure convergence to a second-order stationary point in distributed nonconvex optimization.","With an easily adjustable quantization granularity, the approach allows a user to control the number of bits sent per iteration and, hence, to aggressively reduce the communication overhead.","Numerical experimental results using distributed optimization and learning problems on benchmark datasets confirm the effectiveness of the approach."],"url":"http://arxiv.org/abs/2403.10423v1","category":"math.OC"}
{"created":"2024-03-15 15:52:58","title":"Beam Pattern Modulation Embedded mmWave Hybrid Transceiver Design Towards ISAC","abstract":"Integrated Sensing and Communication (ISAC) emerges as a promising technology for B5G/6G, particularly in the millimeter-wave (mmWave) band. However, the widespread adoption of hybrid architecture in mmWave systems compromises multiplexing gain due to limited radio-frequency chains, resulting in mediocre performance when embedding sensing functionality. To avoid sacrificing the spectrum efficiency in hybrid structures while addressing performance bottlenecks in its extension to ISAC, we present an optimized beam pattern modulation-embedded ISAC (BPM-ISAC). BPM-ISAC applies index modulation over beamspace by selectively activating communication beams, aiming to minimize sensing beampattern mean squared error (MSE) under communication MSE constraints through dedicated hybrid transceiver design. Optimization involves the analog part through a min-MSE-based beam selection algorithm, followed by the digital part using an alternating optimization algorithm. Convergence and asymptotic pairwise error probability (APEP) analyses accompany numerical simulations, validating its overall enhanced ISAC performance over existing alternatives.","sentences":["Integrated Sensing and Communication (ISAC) emerges as a promising technology for B5G/6G, particularly in the millimeter-wave (mmWave) band.","However, the widespread adoption of hybrid architecture in mmWave systems compromises multiplexing gain due to limited radio-frequency chains, resulting in mediocre performance when embedding sensing functionality.","To avoid sacrificing the spectrum efficiency in hybrid structures while addressing performance bottlenecks in its extension to ISAC, we present an optimized beam pattern modulation-embedded ISAC (BPM-ISAC).","BPM-ISAC applies index modulation over beamspace by selectively activating communication beams, aiming to minimize sensing beampattern mean squared error (MSE) under communication MSE constraints through dedicated hybrid transceiver design.","Optimization involves the analog part through a min-MSE-based beam selection algorithm, followed by the digital part using an alternating optimization algorithm.","Convergence and asymptotic pairwise error probability (APEP) analyses accompany numerical simulations, validating its overall enhanced ISAC performance over existing alternatives."],"url":"http://arxiv.org/abs/2403.10417v1","category":"eess.SP"}
{"created":"2024-03-15 15:51:27","title":"Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination","abstract":"We study Gaussian sparse estimation tasks in Huber's contamination model with a focus on mean estimation, PCA, and linear regression. For each of these tasks, we give the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors. All prior efficient algorithms for these tasks incur quantitatively suboptimal error. Concretely, for Gaussian robust $k$-sparse mean estimation on $\\mathbb{R}^d$ with corruption rate $\\epsilon>0$, our algorithm has sample complexity $(k^2/\\epsilon^2)\\mathrm{polylog}(d/\\epsilon)$, runs in sample polynomial time, and approximates the target mean within $\\ell_2$-error $O(\\epsilon)$. Previous efficient algorithms inherently incur error $\\Omega(\\epsilon \\sqrt{\\log(1/\\epsilon)})$. At the technical level, we develop a novel multidimensional filtering method in the sparse regime that may find other applications.","sentences":["We study Gaussian sparse estimation tasks in Huber's contamination model with a focus on mean estimation, PCA, and linear regression.","For each of these tasks, we give the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors.","All prior efficient algorithms for these tasks incur quantitatively suboptimal error.","Concretely, for Gaussian robust $k$-sparse mean estimation on $\\mathbb{R}^d$ with corruption rate $\\epsilon>0$, our algorithm has sample complexity $(k^2/\\epsilon^2)\\mathrm{polylog}(d/\\epsilon)$, runs in sample polynomial time, and approximates the target mean within $\\ell_2$-error $O(\\epsilon)$. Previous efficient algorithms inherently incur error $\\Omega(\\epsilon \\sqrt{\\log(1/\\epsilon)})$. At the technical level, we develop a novel multidimensional filtering method in the sparse regime that may find other applications."],"url":"http://arxiv.org/abs/2403.10416v1","category":"cs.LG"}
{"created":"2024-03-15 15:46:57","title":"Nuclear compensatory evolution driven by mito-nuclear incompatibilities","abstract":"Mitochondrial function relies on the coordinated expression of mitochondrial and nuclear genes, exhibiting remarkable resilience regardless the susceptibility of mitochondrial DNA (mtDNA) to accumulate harmful mutations. A suggested mechanism for preserving this mito-nuclear compatibility is the nuclear compensation, where deleterious mitochondrial alleles drive compensatory changes in nuclear genes. However, prevalence and conditioning factors for this phenomenon remain debated, with empirical evidence supporting and refuting its existence. Here, we investigate how mito-nuclear incompatibilities impact nuclear and mitochondrial substitutions in a model for species radiation under selection for mito-nuclear compatibility, similar to the process of mtDNA introgression. Mating eligibility relies on genetic (nuclear DNA) and spatial proximity, with populations evolving from partially compatible mito-nuclear states. Mutations do not confer advantages nor disadvantages, with no optimal nuclear or mitochondrial types, but individual fitness decreases with increasing incompatibilities, driving the demand for mito-nuclear genetic coordination. We find that selection consistently promotes compensation on incompatible nuclear genes, resulting in more substitutions than compatible or non-interacting genes. Surprisingly, low mitochondrial mutation rates favor compensation, as do increased selective pressure or a higher number of mismatches. High mitochondrial mutation rates boost substitutions in initially compatible nuclear genes, relaxing the selection against mito-nuclear incompatibilities and mirroring the compensatory evolution. Moreover, the presence of incompatibilities accelerates species radiation, but richness at equilibrium is not directly correlated with substitutions' response, revealing the complex dynamics triggered by mitochondrial introgression and mito-nuclear co-evolution.","sentences":["Mitochondrial function relies on the coordinated expression of mitochondrial and nuclear genes, exhibiting remarkable resilience regardless the susceptibility of mitochondrial DNA (mtDNA) to accumulate harmful mutations.","A suggested mechanism for preserving this mito-nuclear compatibility is the nuclear compensation, where deleterious mitochondrial alleles drive compensatory changes in nuclear genes.","However, prevalence and conditioning factors for this phenomenon remain debated, with empirical evidence supporting and refuting its existence.","Here, we investigate how mito-nuclear incompatibilities impact nuclear and mitochondrial substitutions in a model for species radiation under selection for mito-nuclear compatibility, similar to the process of mtDNA introgression.","Mating eligibility relies on genetic (nuclear DNA) and spatial proximity, with populations evolving from partially compatible mito-nuclear states.","Mutations do not confer advantages nor disadvantages, with no optimal nuclear or mitochondrial types, but individual fitness decreases with increasing incompatibilities, driving the demand for mito-nuclear genetic coordination.","We find that selection consistently promotes compensation on incompatible nuclear genes, resulting in more substitutions than compatible or non-interacting genes.","Surprisingly, low mitochondrial mutation rates favor compensation, as do increased selective pressure or a higher number of mismatches.","High mitochondrial mutation rates boost substitutions in initially compatible nuclear genes, relaxing the selection against mito-nuclear incompatibilities and mirroring the compensatory evolution.","Moreover, the presence of incompatibilities accelerates species radiation, but richness at equilibrium is not directly correlated with substitutions' response, revealing the complex dynamics triggered by mitochondrial introgression and mito-nuclear co-evolution."],"url":"http://arxiv.org/abs/2403.10411v1","category":"q-bio.PE"}
{"created":"2024-03-15 15:46:32","title":"Gating single-molecule fluorescence with electrons","abstract":"Tip-enhanced photoluminescence (TEPL) measurements are performed with sub-nanometer spatial resolution on individual molecules decoupled from a metallic substrate by a thin NaCl layer. TEPL spectra reveal progressive fluorescence quenching with decreasing tip-molecule distance when electrons tunneling from the tip of a scanning tunneling microscope are injected at resonance with the molecular states. Rate equations based on a many-body model reveal that the luminescence quenching is due to a progressive population inversion between the ground neutral (S$_0$) and the ground charge ($D_0^-$) states of the molecule occurring when the current is raised. We demonstrate that both the bias voltage and the atomic-scale lateral position of the tip can be used to gate the molecular emission. Our approach can in principle be applied to any molecular system, providing unprecedented control over the fluorescence of a single molecule.","sentences":["Tip-enhanced photoluminescence (TEPL) measurements are performed with sub-nanometer spatial resolution on individual molecules decoupled from a metallic substrate by a thin NaCl layer.","TEPL spectra reveal progressive fluorescence quenching with decreasing tip-molecule distance when electrons tunneling from the tip of a scanning tunneling microscope are injected at resonance with the molecular states.","Rate equations based on a many-body model reveal that the luminescence quenching is due to a progressive population inversion between the ground neutral (S$_0$) and the ground charge ($D_0^-$) states of the molecule occurring when the current is raised.","We demonstrate that both the bias voltage and the atomic-scale lateral position of the tip can be used to gate the molecular emission.","Our approach can in principle be applied to any molecular system, providing unprecedented control over the fluorescence of a single molecule."],"url":"http://arxiv.org/abs/2403.10410v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 15:45:33","title":"High-speed Readout System of X-ray CMOS Image Sensor for Time Domain Astronomy","abstract":"We developed an FPGA-based high-speed readout system for a complementary metal-oxide-semiconductor (CMOS) image sensor to observe soft X-ray transients in future satellite missions, such as HiZ-GUNDAM. Our previous research revealed that the CMOS image sensor has low-energy X-ray detection capability (0.4-4 keV) and strong radiation tolerance, which satisfies the requirements of the HiZ-GUNDAM mission. However, CMOS sensors typically have small pixel sizes (e.g., $\\sim$10 ${\\rm \\mu m}$), resulting in large volumes of image data. GSENSE400BSI has 2048$\\times$2048 pixels, producing 6 Mbyte per frame. These large volumes of observed raw image data cannot be stored in a satellite bus system with a limited storage size. Therefore, only X-ray photon events must be extracted from the raw image data. Furthermore, the readout time of CMOS image sensors is approximately ten times faster than that of typical X-ray CCDs, requiring faster event extraction on a timescale of $\\sim$0.1 s. To address these issues, we have developed an FPGA-based image signal processing system capable of high-speed X-ray event extraction onboard without storing raw image data. The developed compact system enabled mounting on a CubeSat mission, facilitating early in-orbit operation demonstration. Here, we present the design and results of the performance evaluation tests of the proposed FPGA-based readout system. Utilizing X-ray irradiation experiments, the results of the X-ray event extraction with the onboard and offline processing methods were consistent, validating the functionality of the proposed system.","sentences":["We developed an FPGA-based high-speed readout system for a complementary metal-oxide-semiconductor (CMOS) image sensor to observe soft X-ray transients in future satellite missions, such as HiZ-GUNDAM.","Our previous research revealed that the CMOS image sensor has low-energy X-ray detection capability (0.4-4 keV) and strong radiation tolerance, which satisfies the requirements of the HiZ-GUNDAM mission.","However, CMOS sensors typically have small pixel sizes (e.g., $\\sim$10 ${\\rm \\mu m}$), resulting in large volumes of image data.","GSENSE400BSI has 2048$\\times$2048 pixels, producing 6 Mbyte per frame.","These large volumes of observed raw image data cannot be stored in a satellite bus system with a limited storage size.","Therefore, only X-ray photon events must be extracted from the raw image data.","Furthermore, the readout time of CMOS image sensors is approximately ten times faster than that of typical X-ray CCDs, requiring faster event extraction on a timescale of $\\sim$0.1 s. To address these issues, we have developed an FPGA-based image signal processing system capable of high-speed X-ray event extraction onboard without storing raw image data.","The developed compact system enabled mounting on a CubeSat mission, facilitating early in-orbit operation demonstration.","Here, we present the design and results of the performance evaluation tests of the proposed FPGA-based readout system.","Utilizing X-ray irradiation experiments, the results of the X-ray event extraction with the onboard and offline processing methods were consistent, validating the functionality of the proposed system."],"url":"http://arxiv.org/abs/2403.10409v1","category":"astro-ph.IM"}
{"created":"2024-03-15 15:39:56","title":"A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE","abstract":"We present a comparative study between cross-encoder and LLMs rerankers in the context of re-ranking effective SPLADE retrievers. We conduct a large evaluation on TREC Deep Learning datasets and out-of-domain datasets such as BEIR and LoTTE. In the first set of experiments, we show how cross-encoder rerankers are hard to distinguish when it comes to re-rerank SPLADE on MS MARCO. Observations shift in the out-of-domain scenario, where both the type of model and the number of documents to re-rank have an impact on effectiveness. Then, we focus on listwise rerankers based on Large Language Models -- especially GPT-4. While GPT-4 demonstrates impressive (zero-shot) performance, we show that traditional cross-encoders remain very competitive. Overall, our findings aim to to provide a more nuanced perspective on the recent excitement surrounding LLM-based re-rankers -- by positioning them as another factor to consider in balancing effectiveness and efficiency in search systems.","sentences":["We present a comparative study between cross-encoder and LLMs rerankers in the context of re-ranking effective SPLADE retrievers.","We conduct a large evaluation on TREC Deep Learning datasets and out-of-domain datasets such as BEIR and LoTTE.","In the first set of experiments, we show how cross-encoder rerankers are hard to distinguish when it comes to re-rerank SPLADE on MS MARCO.","Observations shift in the out-of-domain scenario, where both the type of model and the number of documents to re-rank have an impact on effectiveness.","Then, we focus on listwise rerankers based on Large Language Models -- especially GPT-4.","While GPT-4 demonstrates impressive (zero-shot) performance, we show that traditional cross-encoders remain very competitive.","Overall, our findings aim to to provide a more nuanced perspective on the recent excitement surrounding LLM-based re-rankers -- by positioning them as another factor to consider in balancing effectiveness and efficiency in search systems."],"url":"http://arxiv.org/abs/2403.10407v1","category":"cs.IR"}
{"created":"2024-03-15 15:38:30","title":"Deep Bi-directional Attention Network for Image Super-Resolution Quality Assessment","abstract":"There has emerged a growing interest in exploring efficient quality assessment algorithms for image super-resolution (SR). However, employing deep learning techniques, especially dual-branch algorithms, to automatically evaluate the visual quality of SR images remains challenging. Existing SR image quality assessment (IQA) metrics based on two-stream networks lack interactions between branches. To address this, we propose a novel full-reference IQA (FR-IQA) method for SR images. Specifically, producing SR images and evaluating how close the SR images are to the corresponding HR references are separate processes. Based on this consideration, we construct a deep Bi-directional Attention Network (BiAtten-Net) that dynamically deepens visual attention to distortions in both processes, which aligns well with the human visual system (HVS). Experiments on public SR quality databases demonstrate the superiority of our proposed BiAtten-Net over state-of-the-art quality assessment methods. In addition, the visualization results and ablation study show the effectiveness of bi-directional attention.","sentences":["There has emerged a growing interest in exploring efficient quality assessment algorithms for image super-resolution (SR).","However, employing deep learning techniques, especially dual-branch algorithms, to automatically evaluate the visual quality of SR images remains challenging.","Existing SR image quality assessment (IQA) metrics based on two-stream networks lack interactions between branches.","To address this, we propose a novel full-reference IQA (FR-IQA) method for SR images.","Specifically, producing SR images and evaluating how close the SR images are to the corresponding HR references are separate processes.","Based on this consideration, we construct a deep Bi-directional Attention Network (BiAtten-Net) that dynamically deepens visual attention to distortions in both processes, which aligns well with the human visual system (HVS).","Experiments on public SR quality databases demonstrate the superiority of our proposed BiAtten-Net over state-of-the-art quality assessment methods.","In addition, the visualization results and ablation study show the effectiveness of bi-directional attention."],"url":"http://arxiv.org/abs/2403.10406v1","category":"cs.MM"}
{"created":"2024-03-15 15:38:15","title":"Action Functional as Early Warning Indicator in the Space of Probability Measures","abstract":"In neuroscience, scientists have some hypotheses on the brain's critical dynamics, which means the brain may stay in a phase transition state between ordered and disordered activities. Some tipping points, as a past of no return, can be critical for neural diseases. Therefore, a key question is how the critical brain hypothesis relates to pathological conditions compared with normal brain functionality.   Action functional between two meta-stable states in stochastic dynamical systems is a good tool to study the critical transition and tipping. Here we extend the conventional Onsager-Machlup action functional of finding the most probable transition pathway to be looking for the evolutionary transition dynamics between two meta-stable invariant sets. Hence a rich theory from Schr\\\"odinger Bridge and Optimal Transport is brought in to solve this problem. Furthermore, different from various early warning indicators via statistics, bifurcation theory, information theory, statistical physics, topology, graph theory, we propose a novel viewpoint of early warning indicators in the space of probability measure, which facilitates us to build indicators based on action functional.   To validate our framework, we apply this methodology to a Morris-Lecar model introduced to analyze the transition dynamics between a meta-stable state and the homo-clinic orbit. Besides, the real Alzheimer's data from ADNI database is also investigated to study the early warning signals of transition from healthy to pre-AD states. This framework not only extends the transition path to be pathway measures between two given densities on invariant sets but also shows potential ability for early warning indicators or biomarkers in complex diseases.","sentences":["In neuroscience, scientists have some hypotheses on the brain's critical dynamics, which means the brain may stay in a phase transition state between ordered and disordered activities.","Some tipping points, as a past of no return, can be critical for neural diseases.","Therefore, a key question is how the critical brain hypothesis relates to pathological conditions compared with normal brain functionality.   ","Action functional between two meta-stable states in stochastic dynamical systems is a good tool to study the critical transition and tipping.","Here we extend the conventional Onsager-Machlup action functional of finding the most probable transition pathway to be looking for the evolutionary transition dynamics between two meta-stable invariant sets.","Hence a rich theory from Schr\\\"odinger Bridge and Optimal Transport is brought in to solve this problem.","Furthermore, different from various early warning indicators via statistics, bifurcation theory, information theory, statistical physics, topology, graph theory, we propose a novel viewpoint of early warning indicators in the space of probability measure, which facilitates us to build indicators based on action functional.   ","To validate our framework, we apply this methodology to a Morris-Lecar model introduced to analyze the transition dynamics between a meta-stable state and the homo-clinic orbit.","Besides, the real Alzheimer's data from ADNI database is also investigated to study the early warning signals of transition from healthy to pre-AD states.","This framework not only extends the transition path to be pathway measures between two given densities on invariant sets but also shows potential ability for early warning indicators or biomarkers in complex diseases."],"url":"http://arxiv.org/abs/2403.10405v1","category":"math.DS"}
{"created":"2024-03-15 15:31:13","title":"Collaborative Aquatic Positioning system Utilising Multi-beam Sonar and Depth Sensors","abstract":"Accurate positioning of underwater robots in confined environments is crucial for inspection and mapping tasks and is also a prerequisite for autonomous operations. Presently, there are no positioning systems available that are suited for real-world use in confined underwater environments, unconstrained by environmental lighting and water turbidity levels and have sufficient accuracy for reliable and repeatable navigation. This shortage presents a significant barrier to enhancing the capabilities of ROVs in such scenarios. This paper introduces an innovative positioning system for ROVs operating in confined, cluttered underwater settings, achieved through the collaboration of an omnidirectional surface vehicle and an ROV. A formulation is proposed and evaluated in the simulation against ground truth. The experimental results from the simulation form a proof of principle of the proposed system and also demonstrate its deployability. Unlike many previous approaches, the system does not rely on fixed infrastructure or tracking of features in the environment and can cover large enclosed areas without additional equipment.","sentences":["Accurate positioning of underwater robots in confined environments is crucial for inspection and mapping tasks and is also a prerequisite for autonomous operations.","Presently, there are no positioning systems available that are suited for real-world use in confined underwater environments, unconstrained by environmental lighting and water turbidity levels and have sufficient accuracy for reliable and repeatable navigation.","This shortage presents a significant barrier to enhancing the capabilities of ROVs in such scenarios.","This paper introduces an innovative positioning system for ROVs operating in confined, cluttered underwater settings, achieved through the collaboration of an omnidirectional surface vehicle and an ROV.","A formulation is proposed and evaluated in the simulation against ground truth.","The experimental results from the simulation form a proof of principle of the proposed system and also demonstrate its deployability.","Unlike many previous approaches, the system does not rely on fixed infrastructure or tracking of features in the environment and can cover large enclosed areas without additional equipment."],"url":"http://arxiv.org/abs/2403.10397v1","category":"cs.RO"}
{"created":"2024-03-15 15:31:03","title":"On well-posedness of the leak localization problem in parallel pipe networks","abstract":"With the advent of integrated sensor technology (smart flow meters and pressure sensors), various new numerical algorithms for leak localization (a core element of water distribution system operation) have been developed. However, there is a lack of theory regarding the limitations of leak localization. In this work, we contribute to the development of such a theory by introducing an example water network structure with parallel pipes that is tractable for analytical treatment. We define the leak localization problem for this structure and show how many sensors and what conditions are needed for the well-posedness of the problem. We present a formula for the leak position as a function of measurements from these sensors. However, we also highlight the risk of finding false but plausible leak positions in the multiple pipes. We try to answer the questions of how and when the leaking pipe can be isolated. In particular, we show that nonlinearities in the pipes' head loss functions are essential for the well-posedness of the isolation problem. We propose procedures to get around the pitfall of multiple plausible leak positions.","sentences":["With the advent of integrated sensor technology (smart flow meters and pressure sensors), various new numerical algorithms for leak localization (a core element of water distribution system operation) have been developed.","However, there is a lack of theory regarding the limitations of leak localization.","In this work, we contribute to the development of such a theory by introducing an example water network structure with parallel pipes that is tractable for analytical treatment.","We define the leak localization problem for this structure and show how many sensors and what conditions are needed for the well-posedness of the problem.","We present a formula for the leak position as a function of measurements from these sensors.","However, we also highlight the risk of finding false but plausible leak positions in the multiple pipes.","We try to answer the questions of how and when the leaking pipe can be isolated.","In particular, we show that nonlinearities in the pipes' head loss functions are essential for the well-posedness of the isolation problem.","We propose procedures to get around the pitfall of multiple plausible leak positions."],"url":"http://arxiv.org/abs/2403.10396v1","category":"eess.SY"}
{"created":"2024-03-15 15:27:57","title":"pyCEPS: A cross-platform Electroanatomic Mapping Data to Computational Model Conversion Platform for the Calibration of Digital Twin Models of Cardiac Electrophysiology","abstract":"Background and Objective: Data from electro-anatomical mapping (EAM) systems are playing an increasingly important role in computational modeling studies for the patient-specific calibration of digital twin models. However, data exported from commercial EAM systems are challenging to access and parse. Converting to data formats that are easily amenable to be viewed and analyzed with commonly used cardiac simulation software tools such as openCARP remains challenging. We therefore developed an open-source platform, pyCEPS, for parsing and converting clinical EAM data conveniently to standard formats widely adopted within the cardiac modeling community. Methods and Results: pyCEPS is an open-source Python-based platform providing the following functions: (i) access and interrogate the EAM data exported from clinical mapping systems; (ii) efficient browsing of EAM data to preview mapping procedures, electrograms (EGMs), and electro-cardiograms (ECGs); (iii) conversion to modeling formats according to the openCARP standard, to be amenable to analysis with standard tools and advanced workflows as used for in silico EAM data. Documentation and training material to facilitate access to this complementary research tool for new users is provided. We describe the technological underpinnings and demonstrate the capabilities of pyCEPS first, and showcase its use in an exemplary modeling application where we use clinical imaging data to build a patient-specific anatomical model. Conclusion: With pyCEPS we offer an open-source framework for accessing EAM data, and converting these to cardiac modeling standard formats. pyCEPS provides the core functionality needed to integrate EAM data in cardiac modeling research. We detail how pyCEPS could be integrated into model calibration workflows facilitating the calibration of a computational model based on EAM data.","sentences":["Background and Objective: Data from electro-anatomical mapping (EAM) systems are playing an increasingly important role in computational modeling studies for the patient-specific calibration of digital twin models.","However, data exported from commercial EAM systems are challenging to access and parse.","Converting to data formats that are easily amenable to be viewed and analyzed with commonly used cardiac simulation software tools such as openCARP remains challenging.","We therefore developed an open-source platform, pyCEPS, for parsing and converting clinical EAM data conveniently to standard formats widely adopted within the cardiac modeling community.","Methods and Results: pyCEPS is an open-source Python-based platform providing the following functions: (i) access and interrogate the EAM data exported from clinical mapping systems; (ii) efficient browsing of EAM data to preview mapping procedures, electrograms (EGMs), and electro-cardiograms (ECGs); (iii) conversion to modeling formats according to the openCARP standard, to be amenable to analysis with standard tools and advanced workflows as used for in silico EAM data.","Documentation and training material to facilitate access to this complementary research tool for new users is provided.","We describe the technological underpinnings and demonstrate the capabilities of pyCEPS first, and showcase its use in an exemplary modeling application where we use clinical imaging data to build a patient-specific anatomical model.","Conclusion: With pyCEPS we offer an open-source framework for accessing EAM data, and converting these to cardiac modeling standard formats.","pyCEPS provides the core functionality needed to integrate EAM data in cardiac modeling research.","We detail how pyCEPS could be integrated into model calibration workflows facilitating the calibration of a computational model based on EAM data."],"url":"http://arxiv.org/abs/2403.10394v1","category":"physics.med-ph"}
{"created":"2024-03-15 15:19:12","title":"Non-Hermitian systems with a real spectrum and selective skin effect","abstract":"In this work we first show a simple approach to constructing non-Hermitian Hamiltonians with a real spectrum, which are \\textit{not} obtained by a non-unitary transformation such as the imaginary gauge transformation. They are given, instead, by the product of a Hermitian Hamiltonian $H_0$ and a positive semi-definite matrix $A$. Depending on whether $A$ has zero eigenvalue(s), the resulting $H$ can possess an exceptional point at zero energy. When $A$ is only required to be Hermitian instead, the resulting $H$ is pseudo-Hermitian that can have real and complex conjugate energy levels. In the special case where $A$ is diagonal, we compare our approach to an imaginary gauge transformation, which reveals a selective non-Hermitian skin effect in our approach, i.e., only the zero mode is a skin mode and the non-zero modes reside in the bulk. We further show that this selective non-Hermitian skin mode has a much lower lasing threshold than its counterpart in the standard non-Hermitian skin effect with the same spatial profile, when we pump at the boundary where they are localized. The form of our construction can also be found, for example, in dynamical matrices describing coupled frictionless harmonic oscillators with different masses.","sentences":["In this work we first show a simple approach to constructing non-Hermitian Hamiltonians with a real spectrum, which are \\textit{not} obtained by a non-unitary transformation such as the imaginary gauge transformation.","They are given, instead, by the product of a Hermitian Hamiltonian $H_0$ and a positive semi-definite matrix $A$.","Depending on whether $A$ has zero eigenvalue(s), the resulting $H$ can possess an exceptional point at zero energy.","When $A$ is only required to be Hermitian instead, the resulting $H$ is pseudo-Hermitian that can have real and complex conjugate energy levels.","In the special case where $A$ is diagonal, we compare our approach to an imaginary gauge transformation, which reveals a selective non-Hermitian skin effect in our approach, i.e., only the zero mode is a skin mode and the non-zero modes reside in the bulk.","We further show that this selective non-Hermitian skin mode has a much lower lasing threshold than its counterpart in the standard non-Hermitian skin effect with the same spatial profile, when we pump at the boundary where they are localized.","The form of our construction can also be found, for example, in dynamical matrices describing coupled frictionless harmonic oscillators with different masses."],"url":"http://arxiv.org/abs/2403.10389v1","category":"quant-ph"}
{"created":"2024-03-15 15:15:38","title":"Transport of non-classical light mediated by topological domain walls in a SSH photonic lattice","abstract":"Advancements in photonics technologies have significantly enhanced their capability to facilitate experiments involving quantum light, even at room temperature. Nevertheless, fully integrating photonic chips that include quantum light sources, effective manipulation and transport of light minimizing losses, and appropriate detection systems remains an ongoing challenge. Topological photonic systems have emerged as promising platforms to protect quantum light properties during propagation, beyond merely preserving light intensity. In this work, we delve into the dynamics of non-classical light traversing a Su-Schrieffer-Heeger photonic lattice with topological domain walls. Our focus centers on how topology influences the quantum properties of light as it moves across the array. By precisely adjusting the spacing between waveguides, we achieve dynamic repositioning and interaction of domain walls, facilitating effective beam-splitting operations. Our findings demonstrate high-fidelity transport of non-classical light across the lattice, replicating known results that are now safeguarded by the topology of the system. This protection is especially beneficial for quantum communication protocols with continuous variable states. Our study enhances the understanding of light dynamics in topological photonic systems and paves the way for high-fidelity, topology-protected quantum communication.","sentences":["Advancements in photonics technologies have significantly enhanced their capability to facilitate experiments involving quantum light, even at room temperature.","Nevertheless, fully integrating photonic chips that include quantum light sources, effective manipulation and transport of light minimizing losses, and appropriate detection systems remains an ongoing challenge.","Topological photonic systems have emerged as promising platforms to protect quantum light properties during propagation, beyond merely preserving light intensity.","In this work, we delve into the dynamics of non-classical light traversing a Su-Schrieffer-Heeger photonic lattice with topological domain walls.","Our focus centers on how topology influences the quantum properties of light as it moves across the array.","By precisely adjusting the spacing between waveguides, we achieve dynamic repositioning and interaction of domain walls, facilitating effective beam-splitting operations.","Our findings demonstrate high-fidelity transport of non-classical light across the lattice, replicating known results that are now safeguarded by the topology of the system.","This protection is especially beneficial for quantum communication protocols with continuous variable states.","Our study enhances the understanding of light dynamics in topological photonic systems and paves the way for high-fidelity, topology-protected quantum communication."],"url":"http://arxiv.org/abs/2403.10387v1","category":"quant-ph"}
{"created":"2024-03-15 15:14:24","title":"Coordination in Noncooperative Multiplayer Matrix Games via Reduced Rank Correlated Equilibria","abstract":"Coordination in multiplayer games enables players to avoid the lose-lose outcome that often arises at Nash equilibria. However, designing a coordination mechanism typically requires the consideration of the joint actions of all players, which becomes intractable in large-scale games. We develop a novel coordination mechanism, termed reduced rank correlated equilibria, which reduces the number of joint actions to be considered and thereby mitigates computational complexity. The idea is to approximate the set of all joint actions with the actions used in a set of pre-computed Nash equilibria via a convex hull operation. In a game with n players and each player having m actions, the proposed mechanism reduces the number of joint actions considered from O(m^n) to O(mn). We demonstrate the application of the proposed mechanism to an air traffic queue management problem. Compared with the correlated equilibrium-a popular benchmark coordination mechanism-the proposed approach is capable of solving a queue management problem involving four thousand times more joint actions. In the meantime, it yields a solution that shows a 58.5% to 99.5% improvement in the fairness indicator and a 1.8% to 50.4% reduction in average delay cost compared to the Nash solution, which does not involve coordination.","sentences":["Coordination in multiplayer games enables players to avoid the lose-lose outcome that often arises at Nash equilibria.","However, designing a coordination mechanism typically requires the consideration of the joint actions of all players, which becomes intractable in large-scale games.","We develop a novel coordination mechanism, termed reduced rank correlated equilibria, which reduces the number of joint actions to be considered and thereby mitigates computational complexity.","The idea is to approximate the set of all joint actions with the actions used in a set of pre-computed Nash equilibria via a convex hull operation.","In a game with n players and each player having m actions, the proposed mechanism reduces the number of joint actions considered from O(m^n) to O(mn).","We demonstrate the application of the proposed mechanism to an air traffic queue management problem.","Compared with the correlated equilibrium-a popular benchmark coordination mechanism-the proposed approach is capable of solving a queue management problem involving four thousand times more joint actions.","In the meantime, it yields a solution that shows a 58.5% to 99.5% improvement in the fairness indicator and a 1.8% to 50.4% reduction in average delay cost compared to the Nash solution, which does not involve coordination."],"url":"http://arxiv.org/abs/2403.10384v1","category":"cs.GT"}
{"created":"2024-03-15 15:09:13","title":"Regret Minimization via Saddle Point Optimization","abstract":"A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We further point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples.","sentences":["A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs.","In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret.","The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning.","By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions (E2D) algorithm.","Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis.","Our formulation leads to a practical algorithm for finite model classes and linear feedback models.","We further point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples."],"url":"http://arxiv.org/abs/2403.10379v1","category":"cs.LG"}
{"created":"2024-03-15 15:08:39","title":"EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models","abstract":"We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision-text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.","sentences":["We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models.","It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations.","The questions come in 11 languages from 7 language families.","Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems.","This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge.","Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image.","Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision-text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark."],"url":"http://arxiv.org/abs/2403.10378v1","category":"cs.CL"}
{"created":"2024-03-15 15:04:56","title":"Further study of $c\\bar{c}c\\bar{c}$ system within a chiral quark model","abstract":"Inspired by the recent Altas and CMS experiments on the invariant mass spectrum of $J/\\psi J/\\psi$, we systematically study the $c\\bar{c}c\\bar{c}$ system of $J^{P}=0^{+}$. In the framework of chiral quark model, we have carried out bound-state calculation and resonance-state calculation respectively by using Real-scaling method. The results of bound-state calculation show that there are no bound states in the $c\\bar{c}c\\bar{c}$ with $0^{+}$ system. The resonance-state calculation shows that there are four possible stable resonances: $R(6920)$, $R(7000)$, $R(7080)$ and $R(7160)$. $R(6920)$ and $R(7160)$ are experimental candidates for $X(6900)$ and $X(7200)$, whose main decay channel is $J/\\psi J/\\psi$. It is important to note that the another major decay channel of $R(7160)$ is $\\chi_{c0} \\chi_{c0} $, and the $\\chi_{c0} \\chi_{c0} $ is also the main decay channel of $R(7000)$, $R(7080)$. Therefore, we propose to search experimentally for these two predicted resonances in the $\\chi_{c0} \\chi_{c0}$ invariant mass spectrum.","sentences":["Inspired by the recent Altas and CMS experiments on the invariant mass spectrum of $J/\\psi J/\\psi$, we systematically study the $c\\bar{c}c\\bar{c}$ system of $J^{P}=0^{+}$. In the framework of chiral quark model, we have carried out bound-state calculation and resonance-state calculation respectively by using Real-scaling method.","The results of bound-state calculation show that there are no bound states in the $c\\bar{c}c\\bar{c}$ with $0^{+}$ system.","The resonance-state calculation shows that there are four possible stable resonances: $R(6920)$, $R(7000)$, $R(7080)$ and $R(7160)$. $R(6920)$ and $R(7160)$ are experimental candidates for $X(6900)$ and $X(7200)$, whose main decay channel is $J/\\psi J/\\psi$.","It is important to note that the another major decay channel of $R(7160)$ is $\\chi_{c0} \\chi_{c0} $, and the $\\chi_{c0} \\chi_{c0} $ is also the main decay channel of $R(7000)$, $R(7080)$. Therefore, we propose to search experimentally for these two predicted resonances in the $\\chi_{c0} \\chi_{c0}$ invariant mass spectrum."],"url":"http://arxiv.org/abs/2403.10375v1","category":"hep-ph"}
{"created":"2024-03-15 15:01:39","title":"Hessian-free force-gradient integrators","abstract":"We propose a new framework of Hessian-free force-gradient integrators that do not require the analytical expression of the force-gradient term based on the Hessian of the potential. Due to that the new class of decomposition algorithms for separable Hamiltonian systems with quadratic kinetic energy may be particularly useful when applied to Hamiltonian systems where an evaluation of the Hessian is significantly more expensive than an evaluation of its gradient, e.g. in molecular dynamics simulations of classical systems. Numerical experiments of an N-body problem, as well as applications to the molecular dynamics step in the Hybrid Monte Carlo (HMC) algorithm for lattice simulations of the Schwinger model and Quantum Chromodynamics (QCD) verify these expectations.","sentences":["We propose a new framework of Hessian-free force-gradient integrators that do not require the analytical expression of the force-gradient term based on the Hessian of the potential.","Due to that the new class of decomposition algorithms for separable Hamiltonian systems with quadratic kinetic energy may be particularly useful when applied to Hamiltonian systems where an evaluation of the Hessian is significantly more expensive than an evaluation of its gradient, e.g. in molecular dynamics simulations of classical systems.","Numerical experiments of an N-body problem, as well as applications to the molecular dynamics step in the Hybrid Monte Carlo (HMC) algorithm for lattice simulations of the Schwinger model and Quantum Chromodynamics (QCD) verify these expectations."],"url":"http://arxiv.org/abs/2403.10370v1","category":"math.NA"}
{"created":"2024-03-15 14:52:11","title":"An inflated dynamic Laplacian to track the emergence and disappearance of semi-material coherent sets","abstract":"Lagrangian methods continue to stand at the forefront of the analysis of time-dependent dynamical systems. Most Lagrangian methods have criteria that must be fulfilled by trajectories as they are followed throughout a given finite flow duration. This key strength of Lagrangian methods can also be a limitation in more complex evolving environments. It places a high importance on selecting a time window that produces useful results, and these results may vary significantly with changes in the flow duration. We show how to overcome this drawback in the tracking of coherent flow features. Finite-time coherent sets (FTCS) are material objects that strongly resist mixing in complicated nonlinear flows. Like other materially coherent objects, by definition they must retain their coherence properties throughout the specified flow duration. Recent work [Froyland and Koltai, CPAM, 2023] introduced the notion of semi-material FTCS, whereby a balance is struck between the material nature and the coherence properties of FTCS. This balance provides the flexibility for FTCS to come and go, merge and separate, or undergo other changes as the governing unsteady flow experiences dramatic shifts. The purpose of this work is to illustrate the utility of the inflated dynamic Laplacian introduced in [Froyland and Koltai, CPAM, 2023] in a range of dynamical systems that are challenging to analyse by standard Lagrangian means, and to provide an efficient meshfree numerical approach for the discretisation of the inflated dynamic Laplacian.","sentences":["Lagrangian methods continue to stand at the forefront of the analysis of time-dependent dynamical systems.","Most Lagrangian methods have criteria that must be fulfilled by trajectories as they are followed throughout a given finite flow duration.","This key strength of Lagrangian methods can also be a limitation in more complex evolving environments.","It places a high importance on selecting a time window that produces useful results, and these results may vary significantly with changes in the flow duration.","We show how to overcome this drawback in the tracking of coherent flow features.","Finite-time coherent sets (FTCS) are material objects that strongly resist mixing in complicated nonlinear flows.","Like other materially coherent objects, by definition they must retain their coherence properties throughout the specified flow duration.","Recent work [Froyland and Koltai, CPAM, 2023] introduced the notion of semi-material FTCS, whereby a balance is struck between the material nature and the coherence properties of FTCS.","This balance provides the flexibility for FTCS to come and go, merge and separate, or undergo other changes as the governing unsteady flow experiences dramatic shifts.","The purpose of this work is to illustrate the utility of the inflated dynamic Laplacian introduced in [Froyland and Koltai, CPAM, 2023] in a range of dynamical systems that are challenging to analyse by standard Lagrangian means, and to provide an efficient meshfree numerical approach for the discretisation of the inflated dynamic Laplacian."],"url":"http://arxiv.org/abs/2403.10360v1","category":"physics.flu-dyn"}
{"created":"2024-03-15 14:46:44","title":"GradNav: Accelerated Exploration of Potential Energy Surfaces with Gradient-Based Navigation","abstract":"The exploration of molecular systems' potential energy surface is important for comprehending their complex behaviors, particularly through identifying various metastable states. However, the transition between these states is often hindered by substantial energy barriers, demanding prolonged molecular simulations that consume considerable computational efforts. Our study introduces the GradNav algorithm, which enhances the exploration of the energy surface, accelerating the reconstruction of the potential energy surface (PES). This algorithm employs a strategy of initiating short simulation runs from updated starting points, derived from prior observations, to effectively navigate across potential barriers and explore new regions. To evaluate GradNav's performance, we introduce two metrics: the deepest well escape frame (DWEF) and the search success initialization ratio (SSIR). Through applications on Langevin dynamics within Mueller-type potential energy surfaces and molecular dynamics simulations of the Fs-Peptide protein, these metrics demonstrate GradNav's enhanced ability to escape deep energy wells, as shown by reduced DWEF values, and its reduced reliance on initial conditions, highlighted by increased SSIR values. Consequently, this improved exploration capability enables more precise energy estimations from simulation trajectories.","sentences":["The exploration of molecular systems' potential energy surface is important for comprehending their complex behaviors, particularly through identifying various metastable states.","However, the transition between these states is often hindered by substantial energy barriers, demanding prolonged molecular simulations that consume considerable computational efforts.","Our study introduces the GradNav algorithm, which enhances the exploration of the energy surface, accelerating the reconstruction of the potential energy surface (PES).","This algorithm employs a strategy of initiating short simulation runs from updated starting points, derived from prior observations, to effectively navigate across potential barriers and explore new regions.","To evaluate GradNav's performance, we introduce two metrics: the deepest well escape frame (DWEF) and the search success initialization ratio (SSIR).","Through applications on Langevin dynamics within Mueller-type potential energy surfaces and molecular dynamics simulations of the Fs-Peptide protein, these metrics demonstrate GradNav's enhanced ability to escape deep energy wells, as shown by reduced DWEF values, and its reduced reliance on initial conditions, highlighted by increased SSIR values.","Consequently, this improved exploration capability enables more precise energy estimations from simulation trajectories."],"url":"http://arxiv.org/abs/2403.10358v1","category":"physics.chem-ph"}
{"created":"2024-03-15 14:43:49","title":"Understanding Stress: A Web Interface for Mental Arithmetic Tasks in a Trier Social Stress Test","abstract":"Stress is a dynamic process that reflects the responses of the brain. Traditional methods for measuring stress are often time-consuming and susceptible to recall bias. To address this, we investigated changes in heart rate (HR) during the Trier Social Stress Test (TSST). Our study incorporated varying levels of complexity in mental arithmetic problems. Participants' HR increased during the Mental Arithmetic Task phase compared to baseline and resting stages, indicating that stress is reflected in HR.","sentences":["Stress is a dynamic process that reflects the responses of the brain.","Traditional methods for measuring stress are often time-consuming and susceptible to recall bias.","To address this, we investigated changes in heart rate (HR) during the Trier Social Stress Test (TSST).","Our study incorporated varying levels of complexity in mental arithmetic problems.","Participants' HR increased during the Mental Arithmetic Task phase compared to baseline and resting stages, indicating that stress is reflected in HR."],"url":"http://arxiv.org/abs/2403.10356v1","category":"cs.CY"}
{"created":"2024-03-15 14:36:38","title":"TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale","abstract":"The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summarization rationale.","sentences":["The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization.","However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings.","To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model.","Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality.","Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks.","Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively.","It also improves interpretability by providing insights into the summarization rationale."],"url":"http://arxiv.org/abs/2403.10351v1","category":"cs.CL"}
{"created":"2024-03-15 14:31:17","title":"Efficient All-electron Hybrid Density Functionals for Atomistic Simulations Beyond 10,000 Atoms","abstract":"Hybrid density functional approximations (DFAs) offer compelling accuracy for ab initio electronic-structure simulations of molecules, nanosystems, and bulk materials, addressing some deficiencies of computationally cheaper, frequently used semilocal DFAs. However, the computational bottleneck of hybrid DFAs is the evaluation of the non-local exact exchange contribution, which is the limiting factor for the application of the method for large-scale simulations. In this work, we present a drastically optimized resolution-of-identity-based real-space implementation of the exact exchange evaluation for both non-periodic and periodic boundary conditions in the all-electron code FHI-aims, targeting high-performance CPU compute clusters. The introduction of several new refined Message Passing Interface (MPI) parallelization layers and shared memory arrays according to the MPI-3 standard were the key components of the optimization. We demonstrate significant improvements of memory and performance efficiency, scalability, and workload distribution, extending the reach of hybrid DFAs to simulation sizes beyond ten thousand atoms. As a necessary byproduct of this work, other code parts in FHI-aims have been optimized as well, e.g., the computation of the Hartree potential and the evaluation of the force and stress components. We benchmark the performance and scaling of the hybrid DFA based simulations for a broad range of chemical systems, including hybrid organic-inorganic perovskites, organic crystals and ice crystals with up to 30,576 atoms (101,920 electrons described by 244,608 basis functions).","sentences":["Hybrid density functional approximations (DFAs) offer compelling accuracy for ab initio electronic-structure simulations of molecules, nanosystems, and bulk materials, addressing some deficiencies of computationally cheaper, frequently used semilocal DFAs.","However, the computational bottleneck of hybrid DFAs is the evaluation of the non-local exact exchange contribution, which is the limiting factor for the application of the method for large-scale simulations.","In this work, we present a drastically optimized resolution-of-identity-based real-space implementation of the exact exchange evaluation for both non-periodic and periodic boundary conditions in the all-electron code FHI-aims, targeting high-performance CPU compute clusters.","The introduction of several new refined Message Passing Interface (MPI) parallelization layers and shared memory arrays according to the MPI-3 standard were the key components of the optimization.","We demonstrate significant improvements of memory and performance efficiency, scalability, and workload distribution, extending the reach of hybrid DFAs to simulation sizes beyond ten thousand atoms.","As a necessary byproduct of this work, other code parts in FHI-aims have been optimized as well, e.g., the computation of the Hartree potential and the evaluation of the force and stress components.","We benchmark the performance and scaling of the hybrid DFA based simulations for a broad range of chemical systems, including hybrid organic-inorganic perovskites, organic crystals and ice crystals with up to 30,576 atoms (101,920 electrons described by 244,608 basis functions)."],"url":"http://arxiv.org/abs/2403.10343v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 14:30:25","title":"Cooperative Jamming for Physical Layer Security Enhancement Using Deep Reinforcement Learning","abstract":"Wireless data communications are always facing the risk of eavesdropping and interception. Conventional protection solutions which are based on encryption may not always be practical as is the case for wireless IoT networks or may soon become ineffective against quantum computers. In this regard, Physical Layer Security (PLS) presents a promising approach to secure wireless communications through the exploitation of the physical properties of the wireless channel. Cooperative Friendly Jamming (CFJ) is among the PLS techniques that have received attention in recent years. However, finding an optimal transmit power allocation that results in the highest secrecy is a complex problem that becomes more difficult to address as the size of the wireless network increases. In this paper, we propose an optimization approach to achieve CFJ in large Wi-Fi networks by using a Reinforcement Learning Algorithm. Obtained results show that our optimization approach offers better secrecy results and becomes more effective as the network size and the density of Wi-Fi access points increase.","sentences":["Wireless data communications are always facing the risk of eavesdropping and interception.","Conventional protection solutions which are based on encryption may not always be practical as is the case for wireless IoT networks or may soon become ineffective against quantum computers.","In this regard, Physical Layer Security (PLS) presents a promising approach to secure wireless communications through the exploitation of the physical properties of the wireless channel.","Cooperative Friendly Jamming (CFJ) is among the PLS techniques that have received attention in recent years.","However, finding an optimal transmit power allocation that results in the highest secrecy is a complex problem that becomes more difficult to address as the size of the wireless network increases.","In this paper, we propose an optimization approach to achieve CFJ in large Wi-Fi networks by using a Reinforcement Learning Algorithm.","Obtained results show that our optimization approach offers better secrecy results and becomes more effective as the network size and the density of Wi-Fi access points increase."],"url":"http://arxiv.org/abs/2403.10342v1","category":"cs.NI"}
{"created":"2024-03-15 14:23:12","title":"How Powerful Potential of Attention on Image Restoration?","abstract":"Transformers have demonstrated their effectiveness in image restoration tasks. Existing Transformer architectures typically comprise two essential components: multi-head self-attention and feed-forward network (FFN). The former captures long-range pixel dependencies, while the latter enables the model to learn complex patterns and relationships in the data. Previous studies have demonstrated that FFNs are key-value memories \\cite{geva2020transformer}, which are vital in modern Transformer architectures. In this paper, we conduct an empirical study to explore the potential of attention mechanisms without using FFN and provide novel structures to demonstrate that removing FFN is flexible for image restoration. Specifically, we propose Continuous Scaling Attention (\\textbf{CSAttn}), a method that computes attention continuously in three stages without using FFN. To achieve competitive performance, we propose a series of key components within the attention. Our designs provide a closer look at the attention mechanism and reveal that some simple operations can significantly affect the model performance. We apply our \\textbf{CSAttn} to several image restoration tasks and show that our model can outperform CNN-based and Transformer-based image restoration approaches.","sentences":["Transformers have demonstrated their effectiveness in image restoration tasks.","Existing Transformer architectures typically comprise two essential components: multi-head self-attention and feed-forward network (FFN).","The former captures long-range pixel dependencies, while the latter enables the model to learn complex patterns and relationships in the data.","Previous studies have demonstrated that FFNs are key-value memories \\cite{geva2020transformer}, which are vital in modern Transformer architectures.","In this paper, we conduct an empirical study to explore the potential of attention mechanisms without using FFN and provide novel structures to demonstrate that removing FFN is flexible for image restoration.","Specifically, we propose Continuous Scaling Attention (\\textbf{CSAttn}), a method that computes attention continuously in three stages without using FFN.","To achieve competitive performance, we propose a series of key components within the attention.","Our designs provide a closer look at the attention mechanism and reveal that some simple operations can significantly affect the model performance.","We apply our \\textbf{CSAttn} to several image restoration tasks and show that our model can outperform CNN-based and Transformer-based image restoration approaches."],"url":"http://arxiv.org/abs/2403.10336v1","category":"cs.CV"}
{"created":"2024-03-15 14:00:27","title":"Hierarchical Provision of Distribution Grid Flexibility with Online Feedback Optimization","abstract":"Utilizing distribution grid flexibility for ancillary services requires the coordination and dispatch of requested active and reactive power to a large number of distributed energy resources in underlying grid layers. This paper presents an approach to hierarchically dispatch flexibility requests based on Online Feedback Optimization (OFO). We implement a framework of individual controllers coordinating actors, contributing to flexibility provision, to track a requested operating point at the interface between grid layers. The framework is evaluated in terms of performance during coordination and possible interaction between individual controllers, both central and distributed. Results show high reliability and robustness of the OFO controllers as well as an efficient dispatch of active and reactive power. Its computational efficiency and capabilities in set point tracking during online grid operation are making OFO a promising approach to the flexibility dispatch problem.","sentences":["Utilizing distribution grid flexibility for ancillary services requires the coordination and dispatch of requested active and reactive power to a large number of distributed energy resources in underlying grid layers.","This paper presents an approach to hierarchically dispatch flexibility requests based on Online Feedback Optimization (OFO).","We implement a framework of individual controllers coordinating actors, contributing to flexibility provision, to track a requested operating point at the interface between grid layers.","The framework is evaluated in terms of performance during coordination and possible interaction between individual controllers, both central and distributed.","Results show high reliability and robustness of the OFO controllers as well as an efficient dispatch of active and reactive power.","Its computational efficiency and capabilities in set point tracking during online grid operation are making OFO a promising approach to the flexibility dispatch problem."],"url":"http://arxiv.org/abs/2403.10315v1","category":"eess.SY"}
{"created":"2024-03-15 13:47:05","title":"Pattern selection and the route to turbulence in incompressible polar active fluids","abstract":"Active fluids, such as suspensions of microswimmers, are known to self-organize into complex spatio-temporal flow patterns. An intriguing example is mesoscale turbulence, a state of dynamic vortex structures exhibiting a characteristic length scale. Here, we employ a minimal model for the effective microswimmer velocity field to explore how the turbulent state develops from regular vortex patterns when the strength of activity resp. related parameters such as nonlinear advection or polar alignment strength - is increased. First, we demonstrate analytically that the system, without any spatial constraints, develops a stationary square vortex lattice in the absence of nonlinear advection. Subsequently, we perform an extended stability analysis of this nonuniform \"ground state\" and uncover a linear instability, which follows from the mutual excitement and simultaneous growth of multiple perturbative modes. This extended analysis is based on linearization around an approximation of the analytical vortex lattice solution and allows us to calculate critical activity parameters. Above these critical values, the vortex lattice develops into mesoscale turbulence in numerical simulations. Utilizing the numerical approach, we uncover an extended region of hysteresis where both patterns are possible depending on the initial condition. Here, we find that turbulence persists below the instability of the vortex lattice. We further determine the stability of square vortex patterns as a function of their wavenumber and represent the results analogous to the well-known Busse balloons known from classical pattern-forming systems. Here, the region of stable periodic patterns shrinks and eventually disappears with increasing activity parameters. Our results show that the strength of activity plays a similar role for active turbulence as the Reynolds number does in driven flow exhibiting inertial turbulence.","sentences":["Active fluids, such as suspensions of microswimmers, are known to self-organize into complex spatio-temporal flow patterns.","An intriguing example is mesoscale turbulence, a state of dynamic vortex structures exhibiting a characteristic length scale.","Here, we employ a minimal model for the effective microswimmer velocity field to explore how the turbulent state develops from regular vortex patterns when the strength of activity resp.","related parameters such as nonlinear advection or polar alignment strength - is increased.","First, we demonstrate analytically that the system, without any spatial constraints, develops a stationary square vortex lattice in the absence of nonlinear advection.","Subsequently, we perform an extended stability analysis of this nonuniform \"ground state\" and uncover a linear instability, which follows from the mutual excitement and simultaneous growth of multiple perturbative modes.","This extended analysis is based on linearization around an approximation of the analytical vortex lattice solution and allows us to calculate critical activity parameters.","Above these critical values, the vortex lattice develops into mesoscale turbulence in numerical simulations.","Utilizing the numerical approach, we uncover an extended region of hysteresis where both patterns are possible depending on the initial condition.","Here, we find that turbulence persists below the instability of the vortex lattice.","We further determine the stability of square vortex patterns as a function of their wavenumber and represent the results analogous to the well-known Busse balloons known from classical pattern-forming systems.","Here, the region of stable periodic patterns shrinks and eventually disappears with increasing activity parameters.","Our results show that the strength of activity plays a similar role for active turbulence as the Reynolds number does in driven flow exhibiting inertial turbulence."],"url":"http://arxiv.org/abs/2403.10305v1","category":"cond-mat.soft"}
{"created":"2024-03-15 13:45:27","title":"An Investigation of the Factors Influencing Evolutionary Dynamics in the Joint Evolution of Robot Body and Control","abstract":"In evolutionary robotics, jointly optimising the design and the controller of robots is a challenging task due to the huge complexity of the solution space formed by the possible combinations of body and controller. We focus on the evolution of robots that can be physically created rather than just simulated, in a rich morphological space that includes a voxel-based chassis, wheels, legs and sensors. On the one hand, this space offers a high degree of liberty in the range of robots that can be produced, while on the other hand introduces a complexity rarely dealt with in previous works relating to matching controllers to designs and in evolving closed-loop control. This is usually addressed by augmenting evolution with a learning algorithm to refine controllers. Although several frameworks exist, few have studied the role of the \\textit{evolutionary dynamics} of the intertwined `evolution+learning' processes in realising high-performing robots. We conduct an in-depth study of the factors that influence these dynamics, specifically: synchronous vs asynchronous evolution; the mechanism for replacing parents with offspring, and rewarding goal-based fitness vs novelty via selection. Results show that asynchronicity combined with goal-based selection and a `replace worst' strategy results in the highest performance.","sentences":["In evolutionary robotics, jointly optimising the design and the controller of robots is a challenging task due to the huge complexity of the solution space formed by the possible combinations of body and controller.","We focus on the evolution of robots that can be physically created rather than just simulated, in a rich morphological space that includes a voxel-based chassis, wheels, legs and sensors.","On the one hand, this space offers a high degree of liberty in the range of robots that can be produced, while on the other hand introduces a complexity rarely dealt with in previous works relating to matching controllers to designs and in evolving closed-loop control.","This is usually addressed by augmenting evolution with a learning algorithm to refine controllers.","Although several frameworks exist, few have studied the role of the \\textit{evolutionary dynamics} of the intertwined `evolution+learning' processes in realising high-performing robots.","We conduct an in-depth study of the factors that influence these dynamics, specifically: synchronous vs asynchronous evolution; the mechanism for replacing parents with offspring, and rewarding goal-based fitness vs novelty via selection.","Results show that asynchronicity combined with goal-based selection and a `replace worst' strategy results in the highest performance."],"url":"http://arxiv.org/abs/2403.10303v1","category":"cs.RO"}
{"created":"2024-03-15 13:35:53","title":"Gradient dynamics approach to reactive thin-film hydrodynamics","abstract":"Wetting and dewetting dynamics of simple and complex liquids is described by kinetic equations in gradient dynamics form that incorporates the various coupled dissipative processes in a fully thermodynamically consistent manner. After briefly reviewing this, we also review how chemical reactions can be captured by a related gradient dynamics description, assuming detailed balanced mass action type kinetics. Then, we bring both aspects together and discuss mesoscopic reactive thin-film hydrodynamics illustrated by two examples, namely, models for reactive wetting and reactive surfactants. These models can describe the approach to equilibrium but may also be employed to study out-of-equilibrium dynamics. In the latter case, one breaks the gradient dynamics form by chemostatting to obtain active systems. In this way, for reactive wetting we recover running drops that are driven by chemically sustained wettability gradients and for drops covered by autocatalytic reactive surfactants we find complex forms of self-propulsion and self-excited oscillations.","sentences":["Wetting and dewetting dynamics of simple and complex liquids is described by kinetic equations in gradient dynamics form that incorporates the various coupled dissipative processes in a fully thermodynamically consistent manner.","After briefly reviewing this, we also review how chemical reactions can be captured by a related gradient dynamics description, assuming detailed balanced mass action type kinetics.","Then, we bring both aspects together and discuss mesoscopic reactive thin-film hydrodynamics illustrated by two examples, namely, models for reactive wetting and reactive surfactants.","These models can describe the approach to equilibrium but may also be employed to study out-of-equilibrium dynamics.","In the latter case, one breaks the gradient dynamics form by chemostatting to obtain active systems.","In this way, for reactive wetting we recover running drops that are driven by chemically sustained wettability gradients and for drops covered by autocatalytic reactive surfactants we find complex forms of self-propulsion and self-excited oscillations."],"url":"http://arxiv.org/abs/2403.10295v1","category":"physics.flu-dyn"}
{"created":"2024-03-15 13:29:59","title":"Towards a power analysis for PLS-based methods","abstract":"In recent years, power analysis has become widely used in applied sciences, with the increasing importance of the replicability issue. When distribution-free methods, such as Partial Least Squares (PLS)-based approaches, are considered, formulating power analysis turns out to be challenging. In this study, we introduce the methodological framework of a new procedure for performing power analysis when PLS-based methods are used. Data are simulated by the Monte Carlo method, assuming the null hypothesis of no effect is false and exploiting the latent structure estimated by PLS in the pilot data. In this way, the complex correlation data structure is explicitly considered in power analysis and sample size estimation. The paper offers insights into selecting statistical tests for the power analysis procedure, comparing accuracy-based tests and those based on continuous parameters estimated by PLS. Simulated and real datasets are investigated to show how the method works in practice.","sentences":["In recent years, power analysis has become widely used in applied sciences, with the increasing importance of the replicability issue.","When distribution-free methods, such as Partial Least Squares (PLS)-based approaches, are considered, formulating power analysis turns out to be challenging.","In this study, we introduce the methodological framework of a new procedure for performing power analysis when PLS-based methods are used.","Data are simulated by the Monte Carlo method, assuming the null hypothesis of no effect is false and exploiting the latent structure estimated by PLS in the pilot data.","In this way, the complex correlation data structure is explicitly considered in power analysis and sample size estimation.","The paper offers insights into selecting statistical tests for the power analysis procedure, comparing accuracy-based tests and those based on continuous parameters estimated by PLS.","Simulated and real datasets are investigated to show how the method works in practice."],"url":"http://arxiv.org/abs/2403.10289v1","category":"stat.ME"}
{"created":"2024-03-15 13:27:11","title":"Thermal radiation forces on planar structures with asymmetric optical response","abstract":"Light carries momentum and, upon interaction with material structures, can exert forces on them. Here, we show that a planar structure with asymmetric optical response is spontaneously accelerated when placed in an environment at a different temperature. This phenomenon originates from the imbalance in the exchange rates of photons between both sides of the structure and the environment. Using a simple theoretical model, we calculate the force acting on the planar structure and its terminal velocity in vacuum, and analyze their dependence on the initial temperature and the geometrical properties of the system for different realistic materials. Our results unravel an alternative approach to manipulating objects in the nano and microscale that does not require an external source of radiation.","sentences":["Light carries momentum and, upon interaction with material structures, can exert forces on them.","Here, we show that a planar structure with asymmetric optical response is spontaneously accelerated when placed in an environment at a different temperature.","This phenomenon originates from the imbalance in the exchange rates of photons between both sides of the structure and the environment.","Using a simple theoretical model, we calculate the force acting on the planar structure and its terminal velocity in vacuum, and analyze their dependence on the initial temperature and the geometrical properties of the system for different realistic materials.","Our results unravel an alternative approach to manipulating objects in the nano and microscale that does not require an external source of radiation."],"url":"http://arxiv.org/abs/2403.10285v1","category":"physics.optics"}
{"created":"2024-03-15 13:05:03","title":"Optimal Portfolio Choice with Cross-Impact Propagators","abstract":"We consider a class of optimal portfolio choice problems in continuous time where the agent's transactions create both transient cross-impact driven by a matrix-valued Volterra propagator, as well as temporary price impact. We formulate this problem as the maximization of a revenue-risk functional, where the agent also exploits available information on a progressively measurable price predicting signal. We solve the maximization problem explicitly in terms of operator resolvents, by reducing the corresponding first order condition to a coupled system of stochastic Fredholm equations of the second kind and deriving its solution. We then give sufficient conditions on the matrix-valued propagator so that the model does not permit price manipulation. We also provide an implementation of the solutions to the optimal portfolio choice problem and to the associated optimal execution problem. Our solutions yield financial insights on the influence of cross-impact on the optimal strategies and its interplay with alpha decays.","sentences":["We consider a class of optimal portfolio choice problems in continuous time where the agent's transactions create both transient cross-impact driven by a matrix-valued Volterra propagator, as well as temporary price impact.","We formulate this problem as the maximization of a revenue-risk functional, where the agent also exploits available information on a progressively measurable price predicting signal.","We solve the maximization problem explicitly in terms of operator resolvents, by reducing the corresponding first order condition to a coupled system of stochastic Fredholm equations of the second kind and deriving its solution.","We then give sufficient conditions on the matrix-valued propagator so that the model does not permit price manipulation.","We also provide an implementation of the solutions to the optimal portfolio choice problem and to the associated optimal execution problem.","Our solutions yield financial insights on the influence of cross-impact on the optimal strategies and its interplay with alpha decays."],"url":"http://arxiv.org/abs/2403.10273v1","category":"q-fin.PM"}
{"created":"2024-03-15 12:58:18","title":"Analysis of a Two-degree-of-freedom Beam for Rotational Piezoelectric Energy Harvesting","abstract":"This study introduces a two-degree-of-freedom piezoelectric energy harvester designed to harness rotational motion. The harvester is built using a cut-out beam which enables the first two resonant fre-quencies to be located close to each other. A distributed continuous model is developed and validated with experimental results. As the beam undergoes significant displacement due to rotational excitations, the geometric nonlinearity arising from longitudinal displacement is considered in the model. Com-pared to previous literature, the simulation results are much more precise for rotational energy harvest-ing. It is observed that as the rotating speed increases, the increased centrifugal force causes the first resonant frequency to rise while the second resonant frequency decreases. This study explores the po-tential to expand the bandwidth of the harvester using two types of nonlinear external force, namely mechanical stoppers and magnetic force. The results indicate that the proposed harvester can broaden the bandwidth by 1.17 Hz and 0.33 Hz at the first and second resonance frequencies, respectively, by using a stopper on the main beam.","sentences":["This study introduces a two-degree-of-freedom piezoelectric energy harvester designed to harness rotational motion.","The harvester is built using a cut-out beam which enables the first two resonant fre-quencies to be located close to each other.","A distributed continuous model is developed and validated with experimental results.","As the beam undergoes significant displacement due to rotational excitations, the geometric nonlinearity arising from longitudinal displacement is considered in the model.","Com-pared to previous literature, the simulation results are much more precise for rotational energy harvest-ing.","It is observed that as the rotating speed increases, the increased centrifugal force causes the first resonant frequency to rise while the second resonant frequency decreases.","This study explores the po-tential to expand the bandwidth of the harvester using two types of nonlinear external force, namely mechanical stoppers and magnetic force.","The results indicate that the proposed harvester can broaden the bandwidth by 1.17 Hz and 0.33 Hz at the first and second resonance frequencies, respectively, by using a stopper on the main beam."],"url":"http://arxiv.org/abs/2403.10269v1","category":"eess.SY"}
{"created":"2024-03-15 12:49:27","title":"Attractive carbon black dispersions: structural and mechanical responses to shear","abstract":"The rheological behavior of colloidal dispersions is of paramount importance in a wide range of applications, including construction materials, energy storage systems and food industry products. These dispersions consistently exhibit non-Newtonian behaviors, a consequence of intricate interplays involving colloids morphology, volume fraction, and inter-particle forces. Understanding how colloids structure under flow remains a challenge, particularly in the presence of attractive forces leading to clusters formation. In this study, we adopt a synergistic approach, combining rheology with ultra small-angle X-ray scattering (USAXS), to probe the flow-induced structural transformations of attractive carbon black (CB) dispersions and their effects on the viscosity. Our key findings can be summarized as follow. First, testing different CB volume fractions, in the high shear rate hydrodynamic regime, CB particles aggregate to form fractal clusters. Their size conforms to a power law of the shear rate, $\\xi_c \\propto \\dot{\\gamma}^{-m}$, with $m\\simeq 0.5$. Second, drawing insights from the fractal structure of clusters, we compute an effective volume fraction $\\phi_{\\mathrm{eff}}$ and find that microstructural models adeptly account for the hydrodynamic stress contributions. We identify a critical shear rate $\\dot{\\gamma^*}$ and a critical volume fraction $\\phi_{\\mathrm{eff}}^{*}$, at which the clusters percolate to form a dynamical network.","sentences":["The rheological behavior of colloidal dispersions is of paramount importance in a wide range of applications, including construction materials, energy storage systems and food industry products.","These dispersions consistently exhibit non-Newtonian behaviors, a consequence of intricate interplays involving colloids morphology, volume fraction, and inter-particle forces.","Understanding how colloids structure under flow remains a challenge, particularly in the presence of attractive forces leading to clusters formation.","In this study, we adopt a synergistic approach, combining rheology with ultra small-angle X-ray scattering (USAXS), to probe the flow-induced structural transformations of attractive carbon black (CB) dispersions and their effects on the viscosity.","Our key findings can be summarized as follow.","First, testing different CB volume fractions, in the high shear rate hydrodynamic regime, CB particles aggregate to form fractal clusters.","Their size conforms to a power law of the shear rate, $\\xi_c \\propto \\dot{\\gamma}^{-m}$, with $m\\simeq 0.5$. Second, drawing insights from the fractal structure of clusters, we compute an effective volume fraction $\\phi_{\\mathrm{eff}}$ and find that microstructural models adeptly account for the hydrodynamic stress contributions.","We identify a critical shear rate $\\dot{\\gamma^*}$ and a critical volume fraction $\\phi_{\\mathrm{eff}}^{*}$, at which the clusters percolate to form a dynamical network."],"url":"http://arxiv.org/abs/2403.10262v1","category":"cond-mat.soft"}
{"created":"2024-03-15 12:48:41","title":"Structural Preprocessing Method for Nonlinear Differential-Algebraic Equations Using Linear Symbolic Matrices","abstract":"Differential-algebraic equations (DAEs) have been used in modeling various dynamical systems in science and engineering. Several preprocessing methods for DAEs, such as consistent initialization and index reduction, use structural information on DAEs. Unfortunately, these methods may fail when the system Jacobian, which is a functional matrix, derived from the DAE is singular.   To transform a DAE with a singular system Jacobian into a nonsingular system, several regularization methods have been proposed. Most of all existing regularization methods rely on symbolic computation to eliminate the system Jacobian for finding a certificate of singularity, resulting in much computational time. Iwata--Oki--Takamatsu (2019) proposed a method (IOT-method) to find a certificate without symbolic computations. The IOT method approximates the system Jacobian by a simpler symbolic matrix, called a layered mixed matrix, which admits a fast combinatorial algorithm for singularity testing. However, it often overlooks the singularity of the system Jacobian since the approximation largely discards algebraic relationships among entries in the original system Jacobian.   In this study, we propose a new regularization method extending the idea of the IOT method. Instead of layered mixed matrices, our method approximates the system Jacobian by more expressive symbolic matrices, called rank-1 coefficient mixed (1CM) matrices. This makes our method more widely applicable. We give a fast combinatorial algorithm for finding a singularity certificate of 1CM-matrices, which is free from symbolic elimination. Our method is also advantageous in that it globally preserves the solution set to the DAE. Through numerical experiments, we confirmed that our method runs fast for large-scale DAEs from real instances.","sentences":["Differential-algebraic equations (DAEs) have been used in modeling various dynamical systems in science and engineering.","Several preprocessing methods for DAEs, such as consistent initialization and index reduction, use structural information on DAEs.","Unfortunately, these methods may fail when the system Jacobian, which is a functional matrix, derived from the DAE is singular.   ","To transform a DAE with a singular system Jacobian into a nonsingular system, several regularization methods have been proposed.","Most of all existing regularization methods rely on symbolic computation to eliminate the system Jacobian for finding a certificate of singularity, resulting in much computational time.","Iwata--Oki--Takamatsu (2019) proposed a method (IOT-method) to find a certificate without symbolic computations.","The IOT method approximates the system Jacobian by a simpler symbolic matrix, called a layered mixed matrix, which admits a fast combinatorial algorithm for singularity testing.","However, it often overlooks the singularity of the system Jacobian since the approximation largely discards algebraic relationships among entries in the original system Jacobian.   ","In this study, we propose a new regularization method extending the idea of the IOT method.","Instead of layered mixed matrices, our method approximates the system Jacobian by more expressive symbolic matrices, called rank-1 coefficient mixed (1CM) matrices.","This makes our method more widely applicable.","We give a fast combinatorial algorithm for finding a singularity certificate of 1CM-matrices, which is free from symbolic elimination.","Our method is also advantageous in that it globally preserves the solution set to the DAE.","Through numerical experiments, we confirmed that our method runs fast for large-scale DAEs from real instances."],"url":"http://arxiv.org/abs/2403.10260v1","category":"cs.SC"}
{"created":"2024-03-15 12:41:30","title":"Region-aware Distribution Contrast: A Novel Approach to Multi-Task Partially Supervised Learning","abstract":"In this study, we address the intricate challenge of multi-task dense prediction, encompassing tasks such as semantic segmentation, depth estimation, and surface normal estimation, particularly when dealing with partially annotated data (MTPSL). The complexity arises from the absence of complete task labels for each training image. Given the inter-related nature of these pixel-wise dense tasks, our focus is on mining and capturing cross-task relationships. Existing solutions typically rely on learning global image representations for global cross-task image matching, imposing constraints that, unfortunately, sacrifice the finer structures within the images. Attempting local matching as a remedy faces hurdles due to the lack of precise region supervision, making local alignment a challenging endeavor. The introduction of Segment Anything Model (SAM) sheds light on addressing local alignment challenges by providing free and high-quality solutions for region detection. Leveraging SAM-detected regions, the subsequent challenge lies in aligning the representations within these regions. Diverging from conventional methods that directly learn a monolithic image representation, our proposal involves modeling region-wise representations using Gaussian Distributions. Aligning these distributions between corresponding regions from different tasks imparts higher flexibility and capacity to capture intra-region structures, accommodating a broader range of tasks. This innovative approach significantly enhances our ability to effectively capture cross-task relationships, resulting in improved overall performance in partially supervised multi-task dense prediction scenarios. Extensive experiments conducted on two widely used benchmarks underscore the superior effectiveness of our proposed method, showcasing state-of-the-art performance even when compared to fully supervised methods.","sentences":["In this study, we address the intricate challenge of multi-task dense prediction, encompassing tasks such as semantic segmentation, depth estimation, and surface normal estimation, particularly when dealing with partially annotated data (MTPSL).","The complexity arises from the absence of complete task labels for each training image.","Given the inter-related nature of these pixel-wise dense tasks, our focus is on mining and capturing cross-task relationships.","Existing solutions typically rely on learning global image representations for global cross-task image matching, imposing constraints that, unfortunately, sacrifice the finer structures within the images.","Attempting local matching as a remedy faces hurdles due to the lack of precise region supervision, making local alignment a challenging endeavor.","The introduction of Segment Anything Model (SAM) sheds light on addressing local alignment challenges by providing free and high-quality solutions for region detection.","Leveraging SAM-detected regions, the subsequent challenge lies in aligning the representations within these regions.","Diverging from conventional methods that directly learn a monolithic image representation, our proposal involves modeling region-wise representations using Gaussian Distributions.","Aligning these distributions between corresponding regions from different tasks imparts higher flexibility and capacity to capture intra-region structures, accommodating a broader range of tasks.","This innovative approach significantly enhances our ability to effectively capture cross-task relationships, resulting in improved overall performance in partially supervised multi-task dense prediction scenarios.","Extensive experiments conducted on two widely used benchmarks underscore the superior effectiveness of our proposed method, showcasing state-of-the-art performance even when compared to fully supervised methods."],"url":"http://arxiv.org/abs/2403.10252v1","category":"cs.CV"}
{"created":"2024-03-15 12:39:50","title":"Exact time-evolving scattering states in open quantum-dot systems with an interaction: Discovery of time-evolving resonant states","abstract":"We study exact time-evolving many-electron states of an open double quantum-dot system with an interdot Coulomb interaction. A systematic construction of the time-evolving states for arbitrary initial conditions is proposed. For any initial states of one- and two-electron plane waves on the electrical leads, we obtain exact solutions of the time-evolving scattering states, which converge to known stationary scattering eigenstates in the long-time limit. For any initial states of localized electrons on the quantum dots, we find exact time-evolving states of a new type, which we refer to as time-evolving resonant states. In contrast to stationary resonant states, whose wave functions spatially diverge and not normalizable, the time-evolving resonant states are normalizable since their wave functions are restricted to a finite space interval due to causality. The exact time-evolving resonant states enable us to calculate the time-dependence of the survival probability of electrons on the quantum dots for the system with the linearized dispersions. It decays exponentially in time on one side of an exponential point of resonance energies while, on the other side, it oscillates during the decay as a result of the interference of the two resonance energies.","sentences":["We study exact time-evolving many-electron states of an open double quantum-dot system with an interdot Coulomb interaction.","A systematic construction of the time-evolving states for arbitrary initial conditions is proposed.","For any initial states of one- and two-electron plane waves on the electrical leads, we obtain exact solutions of the time-evolving scattering states, which converge to known stationary scattering eigenstates in the long-time limit.","For any initial states of localized electrons on the quantum dots, we find exact time-evolving states of a new type, which we refer to as time-evolving resonant states.","In contrast to stationary resonant states, whose wave functions spatially diverge and not normalizable, the time-evolving resonant states are normalizable since their wave functions are restricted to a finite space interval due to causality.","The exact time-evolving resonant states enable us to calculate the time-dependence of the survival probability of electrons on the quantum dots for the system with the linearized dispersions.","It decays exponentially in time on one side of an exponential point of resonance energies while, on the other side, it oscillates during the decay as a result of the interference of the two resonance energies."],"url":"http://arxiv.org/abs/2403.10251v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 12:31:26","title":"Iterative Confinement of Ions via the Quantum Zeno Effect: Probing Paradoxical Energy Consequences","abstract":"Building upon our previously introduced mechanism for ion trapping based on the quantum Zeno effect (QZE), we propose a novel approach to systematically draw ions closer together, solely via quantum measurements. The proposed method involves repeated measurements of the electromagnetic force exerted by ions on an enclosure of conductor plates to confine the ions within an incrementally smaller spatial region, achieved by exploiting the behaviour of the wavefunction at its boundaries. Taking a two-proton system as a case study, we explore the dynamics between the energy gain of the system, attributed to successive QZE measurements, and the energy expended making such measurements. The results reveal a paradox wherein, under specific circumstances, protons appear to accumulate more energy than is seemingly introduced into the system. This peculiarity aligns with prior studies that highlight challenges in energy conservation within quantum mechanics. To verify these observations, we propose an iterative confinement setup that is feasible with current technological capabilities. Confirmation of these findings could offer new insights for applications in quantum physics, including fusion research. Therefore, the proposed novel method of manipulating ions not only harbours considerable potential for diverse applications but also furnishes an additional tool for probing fundamental questions in the field.","sentences":["Building upon our previously introduced mechanism for ion trapping based on the quantum Zeno effect (QZE), we propose a novel approach to systematically draw ions closer together, solely via quantum measurements.","The proposed method involves repeated measurements of the electromagnetic force exerted by ions on an enclosure of conductor plates to confine the ions within an incrementally smaller spatial region, achieved by exploiting the behaviour of the wavefunction at its boundaries.","Taking a two-proton system as a case study, we explore the dynamics between the energy gain of the system, attributed to successive QZE measurements, and the energy expended making such measurements.","The results reveal a paradox wherein, under specific circumstances, protons appear to accumulate more energy than is seemingly introduced into the system.","This peculiarity aligns with prior studies that highlight challenges in energy conservation within quantum mechanics.","To verify these observations, we propose an iterative confinement setup that is feasible with current technological capabilities.","Confirmation of these findings could offer new insights for applications in quantum physics, including fusion research.","Therefore, the proposed novel method of manipulating ions not only harbours considerable potential for diverse applications but also furnishes an additional tool for probing fundamental questions in the field."],"url":"http://arxiv.org/abs/2403.10246v1","category":"quant-ph"}
{"created":"2024-03-15 12:28:21","title":"CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning","abstract":"This paper explores the problem of continual learning (CL) of vision-language models (VLMs) in open domains, where the models need to perform continual updating and inference on a streaming of datasets from diverse seen and unseen domains with novel classes. Such a capability is crucial for various applications in open environments, e.g., AI assistants, autonomous driving systems, and robotics. Current CL studies mostly focus on closed-set scenarios in a single domain with known classes. Large pre-trained VLMs like CLIP have demonstrated superior zero-shot recognition ability, and a number of recent studies leverage this ability to mitigate catastrophic forgetting in CL, but they focus on closed-set CL in a single domain dataset. Open-domain CL of large VLMs is significantly more challenging due to 1) large class correlations and domain gaps across the datasets and 2) the forgetting of zero-shot knowledge in the pre-trained VLMs in addition to the knowledge learned from the newly adapted datasets. In this work we introduce a novel approach, termed CoLeCLIP, that learns an open-domain CL model based on CLIP. It addresses these challenges by a joint learning of a set of task prompts and a cross-domain class vocabulary. Extensive experiments on 11 domain datasets show that CoLeCLIP outperforms state-of-the-art methods for open-domain CL under both task- and class-incremental learning settings.","sentences":["This paper explores the problem of continual learning (CL) of vision-language models (VLMs) in open domains, where the models need to perform continual updating and inference on a streaming of datasets from diverse seen and unseen domains with novel classes.","Such a capability is crucial for various applications in open environments, e.g., AI assistants, autonomous driving systems, and robotics.","Current CL studies mostly focus on closed-set scenarios in a single domain with known classes.","Large pre-trained VLMs like CLIP have demonstrated superior zero-shot recognition ability, and a number of recent studies leverage this ability to mitigate catastrophic forgetting in CL, but they focus on closed-set CL in a single domain dataset.","Open-domain CL of large VLMs is significantly more challenging due to 1) large class correlations and domain gaps across the datasets and 2) the forgetting of zero-shot knowledge in the pre-trained VLMs in addition to the knowledge learned from the newly adapted datasets.","In this work we introduce a novel approach, termed CoLeCLIP, that learns an open-domain CL model based on CLIP.","It addresses these challenges by a joint learning of a set of task prompts and a cross-domain class vocabulary.","Extensive experiments on 11 domain datasets show that CoLeCLIP outperforms state-of-the-art methods for open-domain CL under both task- and class-incremental learning settings."],"url":"http://arxiv.org/abs/2403.10245v1","category":"cs.CV"}
{"created":"2024-03-15 12:28:01","title":"Exploring the Efimov effect in the $D^*D^*D^*$ system","abstract":"The emergence of the Efimov effect in the $D^*D^*D^*$ system is explored under the assumption that the heavy partner of the $T_{cc}^+$ exists as a $D^*D^*$ molecule with $(I)J^P=(0)1^+$. The three-to-three relativistic scattering amplitude is obtained from the ladder amplitude formalism, built from an energy-dependent contact two-body potential where the molecular component of the $T_{cc}^*$ state can be varied. We find that $(I)J^P=(\\tfrac{1}{2})0^-$ three-body bound states can be formed, with properties that suggest that the Efimov effect can be realised for reasonable values of the molecular probability and binding energy of the $T_{cc}^*$.","sentences":["The emergence of the Efimov effect in the $D^*D^*D^*$ system is explored under the assumption that the heavy partner of the $T_{cc}^+$ exists as a $D^*D^*$ molecule with $(I)J^P=(0)1^+$. The three-to-three relativistic scattering amplitude is obtained from the ladder amplitude formalism, built from an energy-dependent contact two-body potential where the molecular component of the $T_{cc}^*$ state can be varied.","We find that $(I)J^P=(\\tfrac{1}{2})0^-$ three-body bound states can be formed, with properties that suggest that the Efimov effect can be realised for reasonable values of the molecular probability and binding energy of the $T_{cc}^*$."],"url":"http://arxiv.org/abs/2403.10244v1","category":"hep-ph"}
{"created":"2024-03-15 12:21:58","title":"Effective medium theory for the electrical conductance of random resistor networks which mimic crack-template-based transparent conductive films","abstract":"We studied random resistor networks produced with regular structure and random distribution of edge conductances. These networks are intended to mimic crack-template-based transparent conductive films as well some random networks produced using nano-imprinting technology. Applying an effective medium theory, we found out that the electrical conductance of such networks is proportional to the square root of the number density of conductive edges. This dependence is in agreement with numerical calculations in Voronoi networks, although the effective conductances are about 15%.","sentences":["We studied random resistor networks produced with regular structure and random distribution of edge conductances.","These networks are intended to mimic crack-template-based transparent conductive films as well some random networks produced using nano-imprinting technology.","Applying an effective medium theory, we found out that the electrical conductance of such networks is proportional to the square root of the number density of conductive edges.","This dependence is in agreement with numerical calculations in Voronoi networks, although the effective conductances are about 15%."],"url":"http://arxiv.org/abs/2403.10241v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-15 12:08:58","title":"A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream","abstract":"Topic detection is a complex process and depends on language because it somehow needs to analyze text. There have been few studies on topic detection in Persian, and the existing algorithms are not remarkable. Therefore, we aimed to study topic detection in Persian. The objectives of this study are: 1) to conduct an extensive study on the best algorithms for topic detection, 2) to identify necessary adaptations to make these algorithms suitable for the Persian language, and 3) to evaluate their performance on Persian social network texts. To achieve these objectives, we have formulated two research questions: First, considering the lack of research in Persian, what modifications should be made to existing frameworks, especially those developed in English, to make them compatible with Persian? Second, how do these algorithms perform, and which one is superior? There are various topic detection methods that can be categorized into different categories. Frequent pattern and clustering are selected for this research, and a hybrid of both is proposed as a new category. Then, ten methods from these three categories are selected. All of them are re-implemented from scratch, changed, and adapted with Persian. These ten methods encompass different types of topic detection methods and have shown good performance in English. The text of Persian social network posts is used as the dataset. Additionally, a new multiclass evaluation criterion, called FS, is used in this paper for the first time in the field of topic detection. Approximately 1.4 billion tokens are processed during experiments. The results indicate that if we are searching for keyword-topics that are easily understandable by humans, the hybrid category is better. However, if the aim is to cluster posts for further analysis, the frequent pattern category is more suitable.","sentences":["Topic detection is a complex process and depends on language because it somehow needs to analyze text.","There have been few studies on topic detection in Persian, and the existing algorithms are not remarkable.","Therefore, we aimed to study topic detection in Persian.","The objectives of this study are: 1) to conduct an extensive study on the best algorithms for topic detection, 2) to identify necessary adaptations to make these algorithms suitable for the Persian language, and 3) to evaluate their performance on Persian social network texts.","To achieve these objectives, we have formulated two research questions:","First, considering the lack of research in Persian, what modifications should be made to existing frameworks, especially those developed in English, to make them compatible with Persian?","Second, how do these algorithms perform, and which one is superior?","There are various topic detection methods that can be categorized into different categories.","Frequent pattern and clustering are selected for this research, and a hybrid of both is proposed as a new category.","Then, ten methods from these three categories are selected.","All of them are re-implemented from scratch, changed, and adapted with Persian.","These ten methods encompass different types of topic detection methods and have shown good performance in English.","The text of Persian social network posts is used as the dataset.","Additionally, a new multiclass evaluation criterion, called FS, is used in this paper for the first time in the field of topic detection.","Approximately 1.4 billion tokens are processed during experiments.","The results indicate that if we are searching for keyword-topics that are easily understandable by humans, the hybrid category is better.","However, if the aim is to cluster posts for further analysis, the frequent pattern category is more suitable."],"url":"http://arxiv.org/abs/2403.10237v1","category":"cs.CL"}
{"created":"2024-03-15 11:41:16","title":"L-functional analysis","abstract":"Inspired by the theories of Kaplansky-Hilbert modules and probability theory in vector lattices, we generalise functional analysis by replacing the scalars $\\mathbb{R}$ or $\\mathbb{C}$ by a real or complex Dedekind complete unital $f$-algebra $\\mathbb{L}$; such an algebra can be represented as a suitable space of continuous functions. We set up the basic theory of $\\mathbb{L}$-normed and $\\mathbb{L}$-Banach spaces and bounded operators between them, we discuss the $\\mathbb{L}$-valued analogues of the classical $\\ell^p$-spaces, and we prove the analogue of the Hahn-Banach theorem. We also discuss the basics of the theory of $\\mathbb{L}$-Hilbert spaces, including projections onto convex subsets and the Riesz Representation theorem.","sentences":["Inspired by the theories of Kaplansky-Hilbert modules and probability theory in vector lattices, we generalise functional analysis by replacing the scalars $\\mathbb{R}$ or $\\mathbb{C}$ by a real or complex Dedekind complete unital $f$-algebra $\\mathbb{L}$; such an algebra can be represented as a suitable space of continuous functions.","We set up the basic theory of $\\mathbb{L}$-normed and $\\mathbb{L}$-Banach spaces and bounded operators between them, we discuss the $\\mathbb{L}$-valued analogues of the classical $\\ell^p$-spaces, and we prove the analogue of the Hahn-Banach theorem.","We also discuss the basics of the theory of $\\mathbb{L}$-Hilbert spaces, including projections onto convex subsets and the Riesz Representation theorem."],"url":"http://arxiv.org/abs/2403.10222v1","category":"math.FA"}
{"created":"2024-03-15 11:28:11","title":"Note on the second derivative of bounded analytic functions","abstract":"Assume $z_0$ lies in the open unit disk $\\mathbb{D}$ and $g$ is an analytic self-map of $\\mathbb{D}$. We will determine the region of values of $g''(z_0)$ in terms of $z_0$, $g(z_0)$ and the hyperbolic derivative of $g$ at $z_0$, and give the form of all the extremal functions. In particular, we obtain a smaller sharp upper bound for $|g''(z_0)|$ than Ruscheweyh's inequality for the case of the second derivative. Moreover, we use a different method to obtain Sz{\\'a}sz's inequality, which provides a sharp upper bound for $|g''(z_0)|$ depending only on $|z_0|$.","sentences":["Assume $z_0$ lies in the open unit disk $\\mathbb{D}$ and $g$ is an analytic self-map of $\\mathbb{D}$. We will determine the region of values of $g''(z_0)$ in terms of $z_0$, $g(z_0)$ and the hyperbolic derivative of $g$ at $z_0$, and give the form of all the extremal functions.","In particular, we obtain a smaller sharp upper bound for $|g''(z_0)|$ than Ruscheweyh's inequality for the case of the second derivative.","Moreover, we use a different method to obtain Sz{\\'a}sz's inequality, which provides a sharp upper bound for $|g''(z_0)|$ depending only on $|z_0|$."],"url":"http://arxiv.org/abs/2403.10213v1","category":"math.CV"}
{"created":"2024-03-15 11:21:34","title":"BlindDiff: Empowering Degradation Modelling in Diffusion Models for Blind Image Super-Resolution","abstract":"Diffusion models (DM) have achieved remarkable promise in image super-resolution (SR). However, most of them are tailored to solving non-blind inverse problems with fixed known degradation settings, limiting their adaptability to real-world applications that involve complex unknown degradations. In this work, we propose BlindDiff, a DM-based blind SR method to tackle the blind degradation settings in SISR. BlindDiff seamlessly integrates the MAP-based optimization into DMs, which constructs a joint distribution of the low-resolution (LR) observation, high-resolution (HR) data, and degradation kernels for the data and kernel priors, and solves the blind SR problem by unfolding MAP approach along with the reverse process. Unlike most DMs, BlindDiff firstly presents a modulated conditional transformer (MCFormer) that is pre-trained with noise and kernel constraints, further serving as a posterior sampler to provide both priors simultaneously. Then, we plug a simple yet effective kernel-aware gradient term between adjacent sampling iterations that guides the diffusion model to learn degradation consistency knowledge. This also enables to joint refine the degradation model as well as HR images by observing the previous denoised sample. With the MAP-based reverse diffusion process, we show that BlindDiff advocates alternate optimization for blur kernel estimation and HR image restoration in a mutual reinforcing manner. Experiments on both synthetic and real-world datasets show that BlindDiff achieves the state-of-the-art performance with significant model complexity reduction compared to recent DM-based methods. Code will be available at \\url{https://github.com/lifengcs/BlindDiff}","sentences":["Diffusion models (DM) have achieved remarkable promise in image super-resolution (SR).","However, most of them are tailored to solving non-blind inverse problems with fixed known degradation settings, limiting their adaptability to real-world applications that involve complex unknown degradations.","In this work, we propose BlindDiff, a DM-based blind SR method to tackle the blind degradation settings in SISR.","BlindDiff seamlessly integrates the MAP-based optimization into DMs, which constructs a joint distribution of the low-resolution (LR) observation, high-resolution (HR) data, and degradation kernels for the data and kernel priors, and solves the blind SR problem by unfolding MAP approach along with the reverse process.","Unlike most DMs, BlindDiff firstly presents a modulated conditional transformer (MCFormer) that is pre-trained with noise and kernel constraints, further serving as a posterior sampler to provide both priors simultaneously.","Then, we plug a simple yet effective kernel-aware gradient term between adjacent sampling iterations that guides the diffusion model to learn degradation consistency knowledge.","This also enables to joint refine the degradation model as well as HR images by observing the previous denoised sample.","With the MAP-based reverse diffusion process, we show that BlindDiff advocates alternate optimization for blur kernel estimation and HR image restoration in a mutual reinforcing manner.","Experiments on both synthetic and real-world datasets show that BlindDiff achieves the state-of-the-art performance with significant model complexity reduction compared to recent DM-based methods.","Code will be available at \\url{https://github.com/lifengcs/BlindDiff}"],"url":"http://arxiv.org/abs/2403.10211v1","category":"cs.CV"}
{"created":"2024-03-15 10:57:09","title":"Ultra-Wideband Positioning System Based on ESP32 and DWM3000 Modules","abstract":"In this paper, an Ultra-Wideband (UWB) positioning system is introduced, that leverages six identical custom-designed boards, each featuring an ESP32 microcontroller and a DWM3000 module from Quorvo. The system is capable of achieving localization with an accuracy of up to 10 cm, by utilizing Two-Way-Ranging (TWR) measurements between one designated tag and five anchor devices. The gathered distance measurements are subsequently processed by an Extended Kalman Filter (EKF) running locally on the tag board, enabling it to determine its own position, relying on fixed, a priori known positions of the anchor boards. This paper presents a comprehensive overview of the systems architecture, the key components, and the capabilities it offers for indoor positioning and tracking applications.","sentences":["In this paper, an Ultra-Wideband (UWB) positioning system is introduced, that leverages six identical custom-designed boards, each featuring an ESP32 microcontroller and a DWM3000 module from Quorvo.","The system is capable of achieving localization with an accuracy of up to 10 cm, by utilizing Two-Way-Ranging (TWR) measurements between one designated tag and five anchor devices.","The gathered distance measurements are subsequently processed by an Extended Kalman Filter (EKF) running locally on the tag board, enabling it to determine its own position, relying on fixed, a priori known positions of the anchor boards.","This paper presents a comprehensive overview of the systems architecture, the key components, and the capabilities it offers for indoor positioning and tracking applications."],"url":"http://arxiv.org/abs/2403.10194v1","category":"cs.RO"}
{"created":"2024-03-15 10:53:53","title":"Finite temperature detection of quantum critical points via internal quantum teleportation","abstract":"We show that the teleportation protocol can be efficiently used to detect quantum critical points using finite temperature data even if all resources needed to its implementation lie within the system under investigation. Contrary to a previous proposal, there is no need to use an external qubit as the input state to be teleported to one of the qubits within the system. Here, we use a pair of nearest neighbor spins from an infinite spin-1/2 chain in equilibrium with a heat bath as the entangled resource of the quantum teleportation protocol and a third adjacent qubit within the chain itself as the input state to be teleported. For several spin chain models subjected to an external magnetic field, we show that the efficiency of the teleportation protocol is severely affected as we cross the quantum critical points associated with those spin chains. This abrupt change in efficiency gives us a clear indication of a quantum phase transition.","sentences":["We show that the teleportation protocol can be efficiently used to detect quantum critical points using finite temperature data even if all resources needed to its implementation lie within the system under investigation.","Contrary to a previous proposal, there is no need to use an external qubit as the input state to be teleported to one of the qubits within the system.","Here, we use a pair of nearest neighbor spins from an infinite spin-1/2 chain in equilibrium with a heat bath as the entangled resource of the quantum teleportation protocol and a third adjacent qubit within the chain itself as the input state to be teleported.","For several spin chain models subjected to an external magnetic field, we show that the efficiency of the teleportation protocol is severely affected as we cross the quantum critical points associated with those spin chains.","This abrupt change in efficiency gives us a clear indication of a quantum phase transition."],"url":"http://arxiv.org/abs/2403.10193v1","category":"quant-ph"}
{"created":"2024-03-15 10:53:09","title":"Open quantum system simulation of time and frequency resolved spectroscopy","abstract":"The dynamics of excitonic energy transfer in molecular complexes triggered by interaction with laser pulses offers a unique window into the underlying physical processes. The absorbed energy moves through the network of interlinked pigments and in photosynthetic complexes reaches a reaction center. The efficiency and time-scale depend not only on the excitonic couplings, but are also affected by the dissipation of energy to vibrational modes of the molecules. An open quantum system description provides a suitable tool to describe the involved processes and connects the decoherence and relaxation dynamics to measurements of the time-dependent polarization.","sentences":["The dynamics of excitonic energy transfer in molecular complexes triggered by interaction with laser pulses offers a unique window into the underlying physical processes.","The absorbed energy moves through the network of interlinked pigments and in photosynthetic complexes reaches a reaction center.","The efficiency and time-scale depend not only on the excitonic couplings, but are also affected by the dissipation of energy to vibrational modes of the molecules.","An open quantum system description provides a suitable tool to describe the involved processes and connects the decoherence and relaxation dynamics to measurements of the time-dependent polarization."],"url":"http://arxiv.org/abs/2403.10192v1","category":"quant-ph"}
{"created":"2024-03-15 10:51:26","title":"Even-Odd partition identities of G\u00f6ellnitz-Gordon type","abstract":"In this paper, we prove a theorem which adds a new member to the famous G\\\"oellnitz-Gordon identities. We construct a \"new system of recurrence formulas\" in order to prove it.","sentences":["In this paper, we prove a theorem which adds a new member to the famous G\\\"oellnitz-Gordon identities.","We construct a \"new system of recurrence formulas\" in order to prove it."],"url":"http://arxiv.org/abs/2403.10189v1","category":"math.CO"}
{"created":"2024-03-15 10:38:14","title":"Effects of spin-orbit coupling in a valley chiral kagom\u00e9 network","abstract":"Valley chiral kagom\\'e networks can arise in various situations, like for example, in double-aligned graphene-hexagonal boron nitride and periodically strained graphene. Here, we construct a phenomenological scattering model based on the symmetries of the network to investigate the energy spectrum and magnetotransport in this system. Additionally, we consider the effects of a finite Rashba spin-orbit coupling on the transport properties of the kagom\\'e network. We identify conditions where the interplay of the Rashba spin-orbit coupling and the geometry of the lattice results in a reduction of the periodicity of the magnetoconductance and characteristic sharp resonances. Moreover, we find a finite spin-polarization of the conductance, which could be exploited in spintronic devices.","sentences":["Valley chiral kagom\\'e networks can arise in various situations, like for example, in double-aligned graphene-hexagonal boron nitride and periodically strained graphene.","Here, we construct a phenomenological scattering model based on the symmetries of the network to investigate the energy spectrum and magnetotransport in this system.","Additionally, we consider the effects of a finite Rashba spin-orbit coupling on the transport properties of the kagom\\'e network.","We identify conditions where the interplay of the Rashba spin-orbit coupling and the geometry of the lattice results in a reduction of the periodicity of the magnetoconductance and characteristic sharp resonances.","Moreover, we find a finite spin-polarization of the conductance, which could be exploited in spintronic devices."],"url":"http://arxiv.org/abs/2403.10181v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 10:18:31","title":"Finite mixture copulas for modeling dependence in longitudinal count data","abstract":"Dependence modeling of multivariate count data has been receiving a considerable attention in recent times. Multivariate elliptical copulas are typically preferred in statistical literature to analyze dependence between repeated measurements of longitudinal data since they allow for different choices of the correlation structure. But these copulas lack in flexibility to model dependence and inference is only feasible under parametric restrictions. In this article, we propose the use of finite mixture of elliptical copulas in order to capture complex and hidden temporal dependency of discrete longitudinal data. With guaranteed model identifiability, our approach permits to use different correlation matrices in each component of the mixture copula. We theoretically examine the dependence properties of finite mixture of copulas, before applying them for constructing regression models for count longitudinal data. The inference of the proposed class of models is based on composite likelihood approach and the finite sample performance of the parameter estimates are investigated through extensive simulation studies. For model validation, besides the standard techniques we extended the t-plot method to accommodate finite mixture of elliptical copulas. Finally, our models are applied to analyze the temporal dependency of two real world longitudinal data sets and shown to provide improvements if compared against standard elliptical copulas.","sentences":["Dependence modeling of multivariate count data has been receiving a considerable attention in recent times.","Multivariate elliptical copulas are typically preferred in statistical literature to analyze dependence between repeated measurements of longitudinal data since they allow for different choices of the correlation structure.","But these copulas lack in flexibility to model dependence and inference is only feasible under parametric restrictions.","In this article, we propose the use of finite mixture of elliptical copulas in order to capture complex and hidden temporal dependency of discrete longitudinal data.","With guaranteed model identifiability, our approach permits to use different correlation matrices in each component of the mixture copula.","We theoretically examine the dependence properties of finite mixture of copulas, before applying them for constructing regression models for count longitudinal data.","The inference of the proposed class of models is based on composite likelihood approach and the finite sample performance of the parameter estimates are investigated through extensive simulation studies.","For model validation, besides the standard techniques we extended the t-plot method to accommodate finite mixture of elliptical copulas.","Finally, our models are applied to analyze the temporal dependency of two real world longitudinal data sets and shown to provide improvements if compared against standard elliptical copulas."],"url":"http://arxiv.org/abs/2403.10165v1","category":"stat.ME"}
{"created":"2024-03-15 10:15:39","title":"Ultra-high endurance silicon photonic memory using vanadium dioxide","abstract":"Silicon photonics arises as a viable solution to address the stringent resource demands of emergent technologies, such as neural networks. Within this framework, photonic memories are fundamental building blocks of photonic integrated circuits that have not yet found a standardized solution due to several trade-off among different metrics such as energy consumption, speed, footprint, or fabrication complexity, to name a few. In particular, a photonic memory exhibiting ultra-high endurance performance (> 10^6 cycles) has been elusive to date. Here, we report an ultra-high endurance silicon photonic memory using vanadium dioxide (VO_2) exhibiting a record cyclability of up to 10^7 cycles without degradation. Moreover, our memory features an ultra-compact footprint below 5 {\\mu}m with potential for nanosecond and picojoule programming performance. Our silicon photonic memory could find application in emerging photonic applications demanding high number of memory updates such as photonic neural networks with in-situ training.","sentences":["Silicon photonics arises as a viable solution to address the stringent resource demands of emergent technologies, such as neural networks.","Within this framework, photonic memories are fundamental building blocks of photonic integrated circuits that have not yet found a standardized solution due to several trade-off among different metrics such as energy consumption, speed, footprint, or fabrication complexity, to name a few.","In particular, a photonic memory exhibiting ultra-high endurance performance (> 10^6 cycles) has been elusive to date.","Here, we report an ultra-high endurance silicon photonic memory using vanadium dioxide (VO_2) exhibiting a record cyclability of up to 10^7 cycles without degradation.","Moreover, our memory features an ultra-compact footprint below 5 {\\mu}m with potential for nanosecond and picojoule programming performance.","Our silicon photonic memory could find application in emerging photonic applications demanding high number of memory updates such as photonic neural networks with in-situ training."],"url":"http://arxiv.org/abs/2403.10162v1","category":"physics.optics"}
{"created":"2024-03-15 10:10:32","title":"Spontaneous spin chirality reversal and competing phases in the topological magnet EuAl$_4$","abstract":"We demonstrate the spontaneous reversal of spin chirality in a single crystal sample of the intermetallic magnet EuAl$_4$. We solve the nanoscopic nature of each of the four magnetically phases of EuAl$_4$ using resonant magnetic x-ray scattering, and demonstrate all four phases order with single-k incommensurate magnetic modulation vectors. Below 15.4 K the system forms a spin density modulated spin structure where the spins are orientated in the ab plane perpendicular to the orientation of the magnetic propagation vector. Below 13.2 K a second spin density wave orders with moments aligned parallel to the c-axis, such that the two spin density wave orders coexist. Below 12.2 K a magnetic helix of a single chirality is stabilised across the entire sample. Below 10 K the chirality of the magnetic helix reverses, and the sample remains a single chiral domain. Concomitant with the establishment of the helical magnetic ordering is the lowering of the crystal symmetry to monoclinic, as evidenced the formation of uniaxial charge and spin strip domains. A group theoretical analysis demonstrates that below 12.2 K the symmetry lowers to polar monoclinic, which is necessary to explain the observed asymmetry in the chiral states of the magnetic helix and the spin chiral reversal. We find that in every magnetically ordered phase of EuAl4 the in-plane moment is perpendicular to the orientation of the magnetic propagation vector, which we demonstrate is favoured by magnetic dipolar interactions.","sentences":["We demonstrate the spontaneous reversal of spin chirality in a single crystal sample of the intermetallic magnet EuAl$_4$. We solve the nanoscopic nature of each of the four magnetically phases of EuAl$_4$ using resonant magnetic x-ray scattering, and demonstrate all four phases order with single-k incommensurate magnetic modulation vectors.","Below 15.4 K the system forms a spin density modulated spin structure where the spins are orientated in the ab plane perpendicular to the orientation of the magnetic propagation vector.","Below 13.2 K a second spin density wave orders with moments aligned parallel to the c-axis, such that the two spin density wave orders coexist.","Below 12.2 K a magnetic helix of a single chirality is stabilised across the entire sample.","Below 10 K the chirality of the magnetic helix reverses, and the sample remains a single chiral domain.","Concomitant with the establishment of the helical magnetic ordering is the lowering of the crystal symmetry to monoclinic, as evidenced the formation of uniaxial charge and spin strip domains.","A group theoretical analysis demonstrates that below 12.2 K the symmetry lowers to polar monoclinic, which is necessary to explain the observed asymmetry in the chiral states of the magnetic helix and the spin chiral reversal.","We find that in every magnetically ordered phase of EuAl4 the in-plane moment is perpendicular to the orientation of the magnetic propagation vector, which we demonstrate is favoured by magnetic dipolar interactions."],"url":"http://arxiv.org/abs/2403.10159v1","category":"cond-mat.str-el"}
{"created":"2024-03-15 09:55:08","title":"Structure-property relations of silicon oxycarbides studied using a machine learning interatomic potential","abstract":"Silicon oxycarbides show outstanding versatility due to their highly tunable composition and microstructure. Consequently, a key challenge is a thorough knowledge of structure-property relations in the system. In this work, we fit an atomic cluster expansion potential to a set of actively learned DFT training data spanning a wide configurational space. We demonstrate the ability of the potential to produce realistic amorphous structures and rationalize the formation of different morphologies of the turbostratic free carbon phase. Finally, we relate the materials stiffness to its composition and microstructure, finding a delicate dependence on Si-C bonds that contradicts commonly assumed relations to the free carbon phase.","sentences":["Silicon oxycarbides show outstanding versatility due to their highly tunable composition and microstructure.","Consequently, a key challenge is a thorough knowledge of structure-property relations in the system.","In this work, we fit an atomic cluster expansion potential to a set of actively learned DFT training data spanning a wide configurational space.","We demonstrate the ability of the potential to produce realistic amorphous structures and rationalize the formation of different morphologies of the turbostratic free carbon phase.","Finally, we relate the materials stiffness to its composition and microstructure, finding a delicate dependence on Si-C bonds that contradicts commonly assumed relations to the free carbon phase."],"url":"http://arxiv.org/abs/2403.10154v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 09:52:54","title":"Sun-Jupiter-Saturn System may exist: A verified computation of quasiperiodic solutions for the planar three body problem","abstract":"In this paper, we present evidence of the stability of a simplified model of the Solar System, a flat (Newtonian) Sun-Jupiter-Saturn system with realistic data: masses of the Sun and the planets, their semi-axes, eccentricities and (apsidal) precessions of the planets close to the real ones. The evidence is based on convincing numerics that a KAM theorem can be applied to the Hamiltonian equations of the model to produce quasiperiodic motion (on an invariant torus) with the appropriate frequencies. To do so, we first use KAM numerical schemes to compute translated tori to continue from the Kepler approximation (two uncoupled two-body problems) up to the actual Hamiltonian of the system, for which the translated torus is an invariant torus. Second, we use KAM numerical schemes for invariant tori to refine the solution giving the desired torus. Lastly, the convergence of the KAM scheme for the invariant torus is (numerically) checked by applying several times a KAM iterative lemma, from which we obtain that the final torus (numerically) satisfies the existence conditions given by a KAM theorem.","sentences":["In this paper, we present evidence of the stability of a simplified model of the Solar System, a flat (Newtonian) Sun-Jupiter-Saturn system with realistic data: masses of the Sun and the planets, their semi-axes, eccentricities and (apsidal) precessions of the planets close to the real ones.","The evidence is based on convincing numerics that a KAM theorem can be applied to the Hamiltonian equations of the model to produce quasiperiodic motion (on an invariant torus) with the appropriate frequencies.","To do so, we first use KAM numerical schemes to compute translated tori to continue from the Kepler approximation (two uncoupled two-body problems) up to the actual Hamiltonian of the system, for which the translated torus is an invariant torus.","Second, we use KAM numerical schemes for invariant tori to refine the solution giving the desired torus.","Lastly, the convergence of the KAM scheme for the invariant torus is (numerically) checked by applying several times a KAM iterative lemma, from which we obtain that the final torus (numerically) satisfies the existence conditions given by a KAM theorem."],"url":"http://arxiv.org/abs/2403.10152v1","category":"math.DS"}
{"created":"2024-03-15 09:47:35","title":"GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time","abstract":"This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios. Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses. Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods. To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference. As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at $\\ge$ 5 FPS and real-time rendering at $\\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness. It can also approach the real pose-based 3D-GS methods. Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences.","sentences":["This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios.","Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model.","With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses.","Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods.","To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference.","As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at $\\ge$ 5 FPS and real-time rendering at $\\ge$ 100 FPS.","Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness.","It can also approach the real pose-based 3D-GS methods.","Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences."],"url":"http://arxiv.org/abs/2403.10147v1","category":"cs.CV"}
{"created":"2024-03-15 09:44:02","title":"RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception","abstract":"The value of roadside perception, which could extend the boundaries of autonomous driving and traffic management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a traffic area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted traffic areas. Rcooper has its own domain-specific challenges, but further exploration is hindered due to the lack of datasets. We hence release the first real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative traffic scenes (i.e., intersection and corridor). The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper.","sentences":["The value of roadside perception, which could extend the boundaries of autonomous driving and traffic management, has gradually become more prominent and acknowledged in recent years.","However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a traffic area because of the limited sensing range and blind spots.","Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted traffic areas.","Rcooper has its own domain-specific challenges, but further exploration is hindered due to the lack of datasets.","We hence release the first real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking.","The manually annotated dataset comprises 50k images and 30k point clouds, including two representative traffic scenes (i.e., intersection and corridor).","The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research.","Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper."],"url":"http://arxiv.org/abs/2403.10145v1","category":"cs.CV"}
{"created":"2024-03-15 09:41:40","title":"Being Heterogeneous Is Advantageous: Extreme Brownian Non-Gaussian Searches","abstract":"Redundancy in biology may be explained by the need to optimize extreme searching processes, where one or few among many particles are requested to reach the target like in human fertilization. We show that non-Gaussian rare fluctuations in Brownian diffusion dominates such searches, introducing drastic corrections to the known Gaussian behavior. Our demonstration entails different physical systems and pinpoints the relevance of diversity within redundancy to boost fast targeting. We sketch an experimental context to test our results: polydisperse systems.","sentences":["Redundancy in biology may be explained by the need to optimize extreme searching processes, where one or few among many particles are requested to reach the target like in human fertilization.","We show that non-Gaussian rare fluctuations in Brownian diffusion dominates such searches, introducing drastic corrections to the known Gaussian behavior.","Our demonstration entails different physical systems and pinpoints the relevance of diversity within redundancy to boost fast targeting.","We sketch an experimental context to test our results: polydisperse systems."],"url":"http://arxiv.org/abs/2403.10143v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-15 09:38:34","title":"Comparative Analysis of Programming by Demonstration Methods: Kinesthetic Teaching vs Human Demonstration","abstract":"Programming by demonstration (PbD) is a simple and efficient way to program robots without explicit robot programming. PbD enables unskilled operators to easily demonstrate and guide different robots to execute task. In this paper we present comparison of demonstration methods with comprehensive user study. Each participant had to demonstrate drawing simple pattern with human demonstration using virtual marker and kinesthetic teaching with robot manipulator. To evaluate differences between demonstration methods, we conducted user study with 24 participants which filled out NASA raw task load index (rTLX) and system usability scale (SUS). We also evaluated similarity of the executed trajectories to measure difference between demonstrated and ideal trajectory. We concluded study with finding that human demonstration using a virtual marker is on average 8 times faster, superior in terms of quality and imposes 2 times less overall workload than kinesthetic teaching.","sentences":["Programming by demonstration (PbD) is a simple and efficient way to program robots without explicit robot programming.","PbD enables unskilled operators to easily demonstrate and guide different robots to execute task.","In this paper we present comparison of demonstration methods with comprehensive user study.","Each participant had to demonstrate drawing simple pattern with human demonstration using virtual marker and kinesthetic teaching with robot manipulator.","To evaluate differences between demonstration methods, we conducted user study with 24 participants which filled out NASA raw task load index (rTLX) and system usability scale (SUS).","We also evaluated similarity of the executed trajectories to measure difference between demonstrated and ideal trajectory.","We concluded study with finding that human demonstration using a virtual marker is on average 8 times faster, superior in terms of quality and imposes 2 times less overall workload than kinesthetic teaching."],"url":"http://arxiv.org/abs/2403.10140v1","category":"cs.RO"}
{"created":"2024-03-15 09:38:08","title":"Modeling Multiday Extreme Precipitation Across Eastern Australia: A Dynamical Perspective","abstract":"The purpose of this paper is to illustrate new techniques for computing multiday extreme precipitation taken from recent theoretical advancements in extreme value theory in the framework of dynamical systems, using historical precipitation data along the eastern coast of Australia as a case study. We explore the numerical pitfalls of applying standard extreme value techniques to model multiday extremes. Then, we illustrate that our data conforms to the appropriate setting for the application of recently derived extreme value distributions for runs of extremes in the dynamical framework and adapt these to the non-stationary setting. Finally, we use these distributions to make more informed predictions on the return times and magnitudes of consecutive daily extreme precipitation and find changes in the dependence of increasing consecutive daily rainfall extremes on the Southern Oscillation Index. Although our case study is focused on extreme precipitation across eastern Australia, we emphasize that these techniques can be used to model expected returns and magnitudes of consecutive extreme precipitation events across many locations.","sentences":["The purpose of this paper is to illustrate new techniques for computing multiday extreme precipitation taken from recent theoretical advancements in extreme value theory in the framework of dynamical systems, using historical precipitation data along the eastern coast of Australia as a case study.","We explore the numerical pitfalls of applying standard extreme value techniques to model multiday extremes.","Then, we illustrate that our data conforms to the appropriate setting for the application of recently derived extreme value distributions for runs of extremes in the dynamical framework and adapt these to the non-stationary setting.","Finally, we use these distributions to make more informed predictions on the return times and magnitudes of consecutive daily extreme precipitation and find changes in the dependence of increasing consecutive daily rainfall extremes on the Southern Oscillation Index.","Although our case study is focused on extreme precipitation across eastern Australia, we emphasize that these techniques can be used to model expected returns and magnitudes of consecutive extreme precipitation events across many locations."],"url":"http://arxiv.org/abs/2403.10139v1","category":"math.DS"}
{"created":"2024-03-15 09:26:17","title":"Universal correlations in chaotic many-body quantum states: Fock-space formulation of Berrys random wave model","abstract":"The apparent randomness of chaotic eigenstates in interacting quantum systems hides subtle correlations dynamically imposed by their finite energy per particle. These correlations are revealed when Berrys approach for chaotic eigenfunctions in single-particle systems is lifted into many-body space. We achieve this by a many-body semiclassics analysis, appropriate for the mesoscopic regime of large but finite number of particles. We then identify the universality of both the cross-correlations and the Gaussian distribution of expansion coefficients as the signatures of chaotic eigenstates. Combined, these two aspects imprint a distinctive backbone to the morphology of eigenstates that we check against extensive quantum simulations. The universality of eigenstate correlations for fixed energy density is then a further signature of many-body quantum chaos that, while consistent with the eigenstate thermalization hypothesis, lies beyond random matrix theory.","sentences":["The apparent randomness of chaotic eigenstates in interacting quantum systems hides subtle correlations dynamically imposed by their finite energy per particle.","These correlations are revealed when Berrys approach for chaotic eigenfunctions in single-particle systems is lifted into many-body space.","We achieve this by a many-body semiclassics analysis, appropriate for the mesoscopic regime of large but finite number of particles.","We then identify the universality of both the cross-correlations and the Gaussian distribution of expansion coefficients as the signatures of chaotic eigenstates.","Combined, these two aspects imprint a distinctive backbone to the morphology of eigenstates that we check against extensive quantum simulations.","The universality of eigenstate correlations for fixed energy density is then a further signature of many-body quantum chaos that, while consistent with the eigenstate thermalization hypothesis, lies beyond random matrix theory."],"url":"http://arxiv.org/abs/2403.10132v1","category":"quant-ph"}
{"created":"2024-03-15 09:18:18","title":"$\\partial\\bar{\\partial}$-Lemma and $p$-K\u00e4hler structures on families of solvmanifolds","abstract":"We provide families of compact $(n + 1)$-dimensional complex non K\\\"ahler manifolds satisfying the $\\partial\\bar{\\partial}$-Lemma, with holomoprhically trivial canonical bundle, carrying a balanced metric and with no $p$-K\\\"ahler structures. Such a construction extends to the completely solvable case in any dimension Nakamura's construction of low-dimensional holomorphically parallelizable solvmanifolds.","sentences":["We provide families of compact $(n + 1)$-dimensional complex non K\\\"ahler manifolds satisfying the $\\partial\\bar{\\partial}$-Lemma, with holomoprhically trivial canonical bundle, carrying a balanced metric and with no $p$-K\\\"ahler structures.","Such a construction extends to the completely solvable case in any dimension Nakamura's construction of low-dimensional holomorphically parallelizable solvmanifolds."],"url":"http://arxiv.org/abs/2403.10126v1","category":"math.DG"}
{"created":"2024-03-15 09:14:18","title":"Regularization-Based Efficient Continual Learning in Deep State-Space Models","abstract":"Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistently outperform traditional DSSMs in terms of effectively addressing catastrophic forgetting, enabling swift and accurate parameter transfer to new tasks.","sentences":["Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems.","However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task.","To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting.","Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems.","We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets.","The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistently outperform traditional DSSMs in terms of effectively addressing catastrophic forgetting, enabling swift and accurate parameter transfer to new tasks."],"url":"http://arxiv.org/abs/2403.10123v1","category":"cs.LG"}
{"created":"2024-03-15 09:10:28","title":"A Novel Bioinspired Neuromorphic Vision-based Tactile Sensor for Fast Tactile Perception","abstract":"Tactile sensing represents a crucial technique that can enhance the performance of robotic manipulators in various tasks. This work presents a novel bioinspired neuromorphic vision-based tactile sensor that uses an event-based camera to quickly capture and convey information about the interactions between robotic manipulators and their environment. The camera in the sensor observes the deformation of a flexible skin manufactured from a cheap and accessible 3D printed material, whereas a 3D printed rigid casing houses the components of the sensor together. The sensor is tested in a grasping stage classification task involving several objects using a data-driven learning-based approach. The results show that the proposed approach enables the sensor to detect pressing and slip incidents within a speed of 2 ms. The fast tactile perception properties of the proposed sensor makes it an ideal candidate for safe grasping of different objects in industries that involve high-speed pick-and-place operations.","sentences":["Tactile sensing represents a crucial technique that can enhance the performance of robotic manipulators in various tasks.","This work presents a novel bioinspired neuromorphic vision-based tactile sensor that uses an event-based camera to quickly capture and convey information about the interactions between robotic manipulators and their environment.","The camera in the sensor observes the deformation of a flexible skin manufactured from a cheap and accessible 3D printed material, whereas a 3D printed rigid casing houses the components of the sensor together.","The sensor is tested in a grasping stage classification task involving several objects using a data-driven learning-based approach.","The results show that the proposed approach enables the sensor to detect pressing and slip incidents within a speed of 2 ms.","The fast tactile perception properties of the proposed sensor makes it an ideal candidate for safe grasping of different objects in industries that involve high-speed pick-and-place operations."],"url":"http://arxiv.org/abs/2403.10120v1","category":"cs.RO"}
{"created":"2024-03-15 08:50:48","title":"Motility-Induced Pinning in Flocking System with Discrete Symmetry","abstract":"We report a motility-induced pinning transition in the active Ising model for an active self-propelled particle system with discrete symmetry. This model was known to exhibit a liquid-gas type flocking phase transition, but a recent study reveals that the polar order is metastable due to droplet excitation. Using extensive Monte Carlo simulations, we demonstrate that, for an intermediate alignment interaction strength, the steady state is characterized by traveling local domains, which renders the polar order short-ranged in both space and time. We further demonstrate that interfaces between colliding domains become pinned as the alignment interaction strength increases. A resonating back-and-forth motion of individual self-propelled particles across interfaces is identified as a mechanism for the pinning. We present an analytic argument for the motility-induced pinning transition by incorporating the resonance mechanism into the hydrodynamic theory. The resulting steady state consists of a network of pinned interfaces interconnected by particle currents. While the polar order is still short-ranged in space, the particle currents mediated by the pinned interfaces can exhibit long-range temporal correlations.","sentences":["We report a motility-induced pinning transition in the active Ising model for an active self-propelled particle system with discrete symmetry.","This model was known to exhibit a liquid-gas type flocking phase transition, but a recent study reveals that the polar order is metastable due to droplet excitation.","Using extensive Monte Carlo simulations, we demonstrate that, for an intermediate alignment interaction strength, the steady state is characterized by traveling local domains, which renders the polar order short-ranged in both space and time.","We further demonstrate that interfaces between colliding domains become pinned as the alignment interaction strength increases.","A resonating back-and-forth motion of individual self-propelled particles across interfaces is identified as a mechanism for the pinning.","We present an analytic argument for the motility-induced pinning transition by incorporating the resonance mechanism into the hydrodynamic theory.","The resulting steady state consists of a network of pinned interfaces interconnected by particle currents.","While the polar order is still short-ranged in space, the particle currents mediated by the pinned interfaces can exhibit long-range temporal correlations."],"url":"http://arxiv.org/abs/2403.10106v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-15 08:45:32","title":"Efficient Multiplayer Battle Game Optimizer for Adversarial Robust Neural Architecture Search","abstract":"This paper introduces a novel metaheuristic algorithm, known as the efficient multiplayer battle game optimizer (EMBGO), specifically designed for addressing complex numerical optimization tasks. The motivation behind this research stems from the need to rectify identified shortcomings in the original MBGO, particularly in search operators during the movement phase, as revealed through ablation experiments. EMBGO mitigates these limitations by integrating the movement and battle phases to simplify the original optimization framework and improve search efficiency. Besides, two efficient search operators: differential mutation and L\\'evy flight are introduced to increase the diversity of the population. To evaluate the performance of EMBGO comprehensively and fairly, numerical experiments are conducted on benchmark functions such as CEC2017, CEC2020, and CEC2022, as well as engineering problems. Twelve well-established MA approaches serve as competitor algorithms for comparison. Furthermore, we apply the proposed EMBGO to the complex adversarial robust neural architecture search (ARNAS) tasks and explore its robustness and scalability. The experimental results and statistical analyses confirm the efficiency and effectiveness of EMBGO across various optimization tasks. As a potential optimization technique, EMBGO holds promise for diverse applications in real-world problems and deep learning scenarios. The source code of EMBGO is made available in \\url{https://github.com/RuiZhong961230/EMBGO}.","sentences":["This paper introduces a novel metaheuristic algorithm, known as the efficient multiplayer battle game optimizer (EMBGO), specifically designed for addressing complex numerical optimization tasks.","The motivation behind this research stems from the need to rectify identified shortcomings in the original MBGO, particularly in search operators during the movement phase, as revealed through ablation experiments.","EMBGO mitigates these limitations by integrating the movement and battle phases to simplify the original optimization framework and improve search efficiency.","Besides, two efficient search operators: differential mutation and L\\'evy flight are introduced to increase the diversity of the population.","To evaluate the performance of EMBGO comprehensively and fairly, numerical experiments are conducted on benchmark functions such as CEC2017, CEC2020, and CEC2022, as well as engineering problems.","Twelve well-established MA approaches serve as competitor algorithms for comparison.","Furthermore, we apply the proposed EMBGO to the complex adversarial robust neural architecture search (ARNAS) tasks and explore its robustness and scalability.","The experimental results and statistical analyses confirm the efficiency and effectiveness of EMBGO across various optimization tasks.","As a potential optimization technique, EMBGO holds promise for diverse applications in real-world problems and deep learning scenarios.","The source code of EMBGO is made available in \\url{https://github.com/RuiZhong961230/EMBGO}."],"url":"http://arxiv.org/abs/2403.10100v1","category":"cs.NE"}
{"created":"2024-03-15 08:24:04","title":"Analysis of a two phase flow model of biofilm spread","abstract":"Free boundaries of biofilms advancing on surfaces evolve according to conservation laws coupled with systems of partial differential equations for velocities, pressures and chemicals affecting cell behavior. Thin film approximations lead to complicated quasi-stationary systems coupling stationary transport equations and compressible Stokes systems with convection-reaction-diffusion equations.We establish existence, uniqueness and stability of solutions of the different submodels involved and then obtain well posedness results for the full system. Our analysis relies on the construction of weak solutions for the steady transport equations under sign assumptions and the reformulation of the compressible Stokes problem as an elliptic system with enhanced regularity properties on the pressure. We need to consider velocity fields whose divergence and normal boundary components satisfy sign conditions, instead of vanishing as classical results require. Applications include the study of cells, biofilms and tissues, where one phase is a liquid solution, whereas the other one is assorted biomass.","sentences":["Free boundaries of biofilms advancing on surfaces evolve according to conservation laws coupled with systems of partial differential equations for velocities, pressures and chemicals affecting cell behavior.","Thin film approximations lead to complicated quasi-stationary systems coupling stationary transport equations and compressible Stokes systems with convection-reaction-diffusion equations.","We establish existence, uniqueness and stability of solutions of the different submodels involved and then obtain well posedness results for the full system.","Our analysis relies on the construction of weak solutions for the steady transport equations under sign assumptions and the reformulation of the compressible Stokes problem as an elliptic system with enhanced regularity properties on the pressure.","We need to consider velocity fields whose divergence and normal boundary components satisfy sign conditions, instead of vanishing as classical results require.","Applications include the study of cells, biofilms and tissues, where one phase is a liquid solution, whereas the other one is assorted biomass."],"url":"http://arxiv.org/abs/2403.10096v1","category":"math.AP"}
{"created":"2024-03-15 17:49:01","title":"Tuneable band topology and optical conductivity in altermagnets","abstract":"We study two-dimensional $d$-wave altermagnetic metals taking into account the presence of substrate-induced Rashba spin-orbit coupling. We consider the altermagnet bandstructure using a 2D band Hamiltonian near the $\\Gamma$ point under external magnetic field. It is shown that time-reversal-symmetry breaking due to altermagnetism, together with Rashba coupling and external magnetic field, can result in non-trivial band topology. The topological phases can be tuned by magnetic field strength and directions, and are classified by their Chern numbers. Furthermore, we investigate the charge response by computing the full optical conductivity tensor with and without magnetic field. In particular, we focus on magneto-optical responses, which are the finite-frequency analog of the Berry curvature-induced anomalous Hall conductivity. Finally, using experimentally realistic parameters for RuO$_2$, we estimate the Faraday angle in the absence of magnetic fields.","sentences":["We study two-dimensional $d$-wave altermagnetic metals taking into account the presence of substrate-induced Rashba spin-orbit coupling.","We consider the altermagnet bandstructure using a 2D band Hamiltonian near the $\\Gamma$ point under external magnetic field.","It is shown that time-reversal-symmetry breaking due to altermagnetism, together with Rashba coupling and external magnetic field, can result in non-trivial band topology.","The topological phases can be tuned by magnetic field strength and directions, and are classified by their Chern numbers.","Furthermore, we investigate the charge response by computing the full optical conductivity tensor with and without magnetic field.","In particular, we focus on magneto-optical responses, which are the finite-frequency analog of the Berry curvature-induced anomalous Hall conductivity.","Finally, using experimentally realistic parameters for RuO$_2$, we estimate the Faraday angle in the absence of magnetic fields."],"url":"http://arxiv.org/abs/2403.10509v1","category":"cond-mat.str-el"}
{"created":"2024-03-15 17:43:43","title":"ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment","abstract":"The advent of the Transformer architecture has propelled the growth of natural language processing (NLP) models, leading to remarkable achievements in numerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU memory and high-speed interconnects poses challenges for training large-scale models. This makes it daunting for many users to experiment with pre-training and fine-tuning large language models (LLMs). In this study, we introduce \\atom, a resilient distributed training framework designed for asynchronous training of vast models in a decentralized setting using cost-effective hardware, including consumer-grade GPUs and Ethernet. Unlike conventional model partitioning methods that distribute sub-models across GPUs, \\atom aims to accommodate a complete LLM on one host (peer) through seamlessly model swapping and concurrently trains multiple copies across various peers to optimize training throughput. Through static analysis, \\atom identifies the best model partitioning strategy and flawlessly merges model execution with swapping. Key benefits of \\atom include: Avoiding the central point of failure found in pipeline parallelism methods. Demonstrating superior performance and scalability compared to closely-integrated pipeline parallelism in slower networks. Our experiments using different GPT-3 model configurations reveal that, in scenarios with suboptimal network connections, \\atom can enhance training efficiency up to $20 \\times$ when juxtaposed with the state-of-the-art decentralized pipeline parallelism approaches.","sentences":["The advent of the Transformer architecture has propelled the growth of natural language processing (NLP) models, leading to remarkable achievements in numerous NLP tasks.","Yet, the absence of specialized hardware like expansive GPU memory and high-speed interconnects poses challenges for training large-scale models.","This makes it daunting for many users to experiment with pre-training and fine-tuning large language models (LLMs).","In this study, we introduce \\atom, a resilient distributed training framework designed for asynchronous training of vast models in a decentralized setting using cost-effective hardware, including consumer-grade GPUs and Ethernet.","Unlike conventional model partitioning methods that distribute sub-models across GPUs, \\atom aims to accommodate a complete LLM on one host (peer) through seamlessly model swapping and concurrently trains multiple copies across various peers to optimize training throughput.","Through static analysis, \\atom identifies the best model partitioning strategy and flawlessly merges model execution with swapping.","Key benefits of \\atom include: Avoiding the central point of failure found in pipeline parallelism methods.","Demonstrating superior performance and scalability compared to closely-integrated pipeline parallelism in slower networks.","Our experiments using different GPT-3 model configurations reveal that, in scenarios with suboptimal network connections, \\atom can enhance training efficiency up to $20 \\times$ when juxtaposed with the state-of-the-art decentralized pipeline parallelism approaches."],"url":"http://arxiv.org/abs/2403.10504v1","category":"cs.DC"}
{"created":"2024-03-15 17:24:55","title":"Does the Correlation between 2MRS Galaxies and the CMB Indicate an Unmodeled CMB Foreground?","abstract":"We revisit the claimed detection of a new cosmic microwave background (CMB) foreground based on the correlation between low-redshift 2MASS Redshift Survey (2MRS) galaxies and CMB temperature maps from the Planck and WMAP missions. We reproduce the reported measurements but argue that the original analysis significantly underestimated the uncertainties. We cross-correlate the 2MRS galaxy positions with simulated CMB maps and show that the correlation measured with the real data for late-type spiral galaxies at angular scales $\\theta\\geq0.1^{\\circ}$ and redshift $cz<4500$ km s$^{-1}$ is consistent with zero at the $1.7\\sigma$ level or less, depending on the exact CMB map and simulation construction. This was the sample that formed the basis for the original detection claim. For smaller angular separations the results are not robust to galaxy type or CMB cleaning method, and we are unable to draw firm conclusions. The original analysis did not propose a specific, falsifiable physical correlation mechanism, and it is impossible to rule out any contribution from an underlying physical effect. However, given our calculations, the lack of signal from expanding the redshift range, and the lack of corroboration from other galaxy surveys, we do not find the evidence for a new CMB foreground signal compelling.","sentences":["We revisit the claimed detection of a new cosmic microwave background (CMB) foreground based on the correlation between low-redshift 2MASS Redshift Survey (2MRS) galaxies and CMB temperature maps from the Planck and WMAP missions.","We reproduce the reported measurements but argue that the original analysis significantly underestimated the uncertainties.","We cross-correlate the 2MRS galaxy positions with simulated CMB maps and show that the correlation measured with the real data for late-type spiral galaxies at angular scales $\\theta\\geq0.1^{\\circ}$ and redshift $cz<4500$ km s$^{-1}$ is consistent with zero at the $1.7\\sigma$ level or less, depending on the exact CMB map and simulation construction.","This was the sample that formed the basis for the original detection claim.","For smaller angular separations the results are not robust to galaxy type or CMB cleaning method, and we are unable to draw firm conclusions.","The original analysis did not propose a specific, falsifiable physical correlation mechanism, and it is impossible to rule out any contribution from an underlying physical effect.","However, given our calculations, the lack of signal from expanding the redshift range, and the lack of corroboration from other galaxy surveys, we do not find the evidence for a new CMB foreground signal compelling."],"url":"http://arxiv.org/abs/2403.10490v1","category":"astro-ph.CO"}
{"created":"2024-03-15 17:19:04","title":"Moodle Usability Assessment Methodology using the Universal Design for Learning perspective","abstract":"The application of the Universal Design for Learning framework favors the creation of virtual educational environments for all. It requires developing accessible content, having a usable platform, and the use of flexible didactics and evaluations that promote constant student motivation. The present study aims to design a methodology to evaluate the usability of the Moodle platform based on the principles of Universal Design for Learning, recognizing the importance of accessibility, usability and the availability of Assistive Technologies. We developed and applied a methodology to assess the usability level of Moodle platforms, taking into consideration that they integrate Assistive Technologies or are used for MOOC contexts. We provide the results of a use case that assesses two instances for the respective Moodle v.2.x and v.3.x family versions. We employed the framework of mixed design research in order to assess a MOOC-type educational program devised under the principles of Universal Design for Learning. As a result of the assessment of Moodle v.2.x and v.3.x, we conclude that the platforms must improve some key elements (e.g. contrasting colors, incorporation of alternative text and links) in order to comply with international accessibility standards. With respect to usability, we can confirm that the principles and guidelines of Universal Design for Learning are applicable to MOOC-type Virtual Learning Environments, are positively valued by students, and have a positive impact on certification rates.","sentences":["The application of the Universal Design for Learning framework favors the creation of virtual educational environments for all.","It requires developing accessible content, having a usable platform, and the use of flexible didactics and evaluations that promote constant student motivation.","The present study aims to design a methodology to evaluate the usability of the Moodle platform based on the principles of Universal Design for Learning, recognizing the importance of accessibility, usability and the availability of Assistive Technologies.","We developed and applied a methodology to assess the usability level of Moodle platforms, taking into consideration that they integrate Assistive Technologies or are used for MOOC contexts.","We provide the results of a use case that assesses two instances for the respective Moodle v.2.x and v.3.x family versions.","We employed the framework of mixed design research in order to assess a MOOC-type educational program devised under the principles of Universal Design for Learning.","As a result of the assessment of Moodle v.2.x and v.3.x, we conclude that the platforms must improve some key elements (e.g. contrasting colors, incorporation of alternative text and links) in order to comply with international accessibility standards.","With respect to usability, we can confirm that the principles and guidelines of Universal Design for Learning are applicable to MOOC-type Virtual Learning Environments, are positively valued by students, and have a positive impact on certification rates."],"url":"http://arxiv.org/abs/2403.10484v1","category":"cs.CY"}
{"created":"2024-03-15 16:56:49","title":"Defect design in ferroelectrics -- new insights on agglomeration","abstract":"Functional properties of ferroelectrics and their change with time depend crucially on the defect structure. In particular, point defects and bias fields induced by defect dipoles modify the field hysteresis and play an important role in fatigue and aging. However, a full understanding on how order, agglomeration and strength of defect dipoles affect phase stability and functional properties is still lacking. To close these gaps in knowledge, we screen these parameters by \\textit{ab\\ initio} based molecular dynamics simulations with the effective Hamiltonian method for the prototypical ferroelectric material (Ba,Sr)TiO$_3$. Our findings suggest that the {\\it{active surface area}} of the defects, rather than the defect concentration is the decisive factor. For a fixed defect concentration, clustering reduces the {\\it{active surface area}} and thus the defect-induced changes of phase stability and field hysteresis. Particularly planar agglomerates of defects appear as promising route for the material design as their impact on the field hysteresis can be controlled by the field direction and their impact on the phase stability shows a cross-over with the strength of the defect dipoles. For this agglomeration, we furthermore find that pinched field hysteresis, which is beneficial, e.g. for energy storage, can be achieved for a wide range of defect dipole strengths and thus is not too sensitive to the choice of dopants.","sentences":["Functional properties of ferroelectrics and their change with time depend crucially on the defect structure.","In particular, point defects and bias fields induced by defect dipoles modify the field hysteresis and play an important role in fatigue and aging.","However, a full understanding on how order, agglomeration and strength of defect dipoles affect phase stability and functional properties is still lacking.","To close these gaps in knowledge, we screen these parameters by \\textit{ab\\ initio} based molecular dynamics simulations with the effective Hamiltonian method for the prototypical ferroelectric material (Ba,Sr)TiO$_3$. Our findings suggest that the {\\it{active surface area}} of the defects, rather than the defect concentration is the decisive factor.","For a fixed defect concentration, clustering reduces the {\\it{active surface area}} and thus the defect-induced changes of phase stability and field hysteresis.","Particularly planar agglomerates of defects appear as promising route for the material design as their impact on the field hysteresis can be controlled by the field direction and their impact on the phase stability shows a cross-over with the strength of the defect dipoles.","For this agglomeration, we furthermore find that pinched field hysteresis, which is beneficial, e.g. for energy storage, can be achieved for a wide range of defect dipole strengths and thus is not too sensitive to the choice of dopants."],"url":"http://arxiv.org/abs/2403.10467v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 16:52:25","title":"Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness","abstract":"Machine Learning (ML) is susceptible to adversarial attacks that aim to trick ML models, making them produce faulty predictions. Adversarial training was found to increase the robustness of ML models against these attacks. However, in network and cybersecurity, obtaining labeled training and adversarial training data is challenging and costly. Furthermore, concept drift deepens the challenge, particularly in dynamic domains like network and cybersecurity, and requires various models to conduct periodic retraining. This letter introduces Adaptive Continuous Adversarial Training (ACAT) to continuously integrate adversarial training samples into the model during ongoing learning sessions, using real-world detected adversarial data, to enhance model resilience against evolving adversarial threats. ACAT is an adaptive defense mechanism that utilizes periodic retraining to effectively counter adversarial attacks while mitigating catastrophic forgetting. Our approach also reduces the total time required for adversarial sample detection, especially in environments such as network security where the rate of attacks could be very high. Traditional detection processes that involve two stages may result in lengthy procedures. Experimental results using a SPAM detection dataset demonstrate that with ACAT, the accuracy of the SPAM filter increased from 69% to over 88% after just three retraining sessions. Furthermore, ACAT outperforms conventional adversarial sample detectors, providing faster decision times, up to four times faster in some cases.","sentences":["Machine Learning (ML) is susceptible to adversarial attacks that aim to trick ML models, making them produce faulty predictions.","Adversarial training was found to increase the robustness of ML models against these attacks.","However, in network and cybersecurity, obtaining labeled training and adversarial training data is challenging and costly.","Furthermore, concept drift deepens the challenge, particularly in dynamic domains like network and cybersecurity, and requires various models to conduct periodic retraining.","This letter introduces Adaptive Continuous Adversarial Training (ACAT) to continuously integrate adversarial training samples into the model during ongoing learning sessions, using real-world detected adversarial data, to enhance model resilience against evolving adversarial threats.","ACAT is an adaptive defense mechanism that utilizes periodic retraining to effectively counter adversarial attacks while mitigating catastrophic forgetting.","Our approach also reduces the total time required for adversarial sample detection, especially in environments such as network security where the rate of attacks could be very high.","Traditional detection processes that involve two stages may result in lengthy procedures.","Experimental results using a SPAM detection dataset demonstrate that with ACAT, the accuracy of the SPAM filter increased from 69% to over 88% after just three retraining sessions.","Furthermore, ACAT outperforms conventional adversarial sample detectors, providing faster decision times, up to four times faster in some cases."],"url":"http://arxiv.org/abs/2403.10461v1","category":"cs.LG"}
{"created":"2024-03-15 16:27:30","title":"A Mean-Field Game of Market Entry: Portfolio Liquidation with Trading Constraints","abstract":"We consider both $N$-player and mean-field games of optimal portfolio liquidation in which the players are not allowed to change the direction of trading. Players with an initially short position of stocks are only allowed to buy while players with an initially long position are only allowed to sell the stock. Under suitable conditions on the model parameters we show that the games are equivalent to games of timing where the players need to determine the optimal times of market entry and exit. We identify the equilibrium entry and exit times and prove that equilibrium mean-trading rates can be characterized in terms of the solutions to a highly non-linear higher-order integral equation with endogenous terminal condition. We prove the existence of a unique solution to the integral equation from which we obtain the existence of a unique equilibrium both in the mean-field and the $N$-player game.","sentences":["We consider both $N$-player and mean-field games of optimal portfolio liquidation in which the players are not allowed to change the direction of trading.","Players with an initially short position of stocks are only allowed to buy while players with an initially long position are only allowed to sell the stock.","Under suitable conditions on the model parameters we show that the games are equivalent to games of timing where the players need to determine the optimal times of market entry and exit.","We identify the equilibrium entry and exit times and prove that equilibrium mean-trading rates can be characterized in terms of the solutions to a highly non-linear higher-order integral equation with endogenous terminal condition.","We prove the existence of a unique solution to the integral equation from which we obtain the existence of a unique equilibrium both in the mean-field and the $N$-player game."],"url":"http://arxiv.org/abs/2403.10441v1","category":"q-fin.MF"}
{"created":"2024-03-15 16:10:36","title":"Model free collision aggregation for the computation of escape distributions","abstract":"Motivated by a heat radiative transport equation, we consider a particle undergoing collisions in a space-time domain and propose a method to sample its escape time, space and direction from the domain. The first step of the procedure is an estimation of how many elementary collisions is safe to take before chances of exiting the domain are too high; then these collisions are aggregated into a single movement. The method does not use any model nor any particular regime of parameters. We give theoretical results both under the normal approximation and without it and test the method on some benchmarks from the literature. The results confirm the theoretical predictions and show that the proposal is an efficient method to sample the escape distribution of the particle.","sentences":["Motivated by a heat radiative transport equation, we consider a particle undergoing collisions in a space-time domain and propose a method to sample its escape time, space and direction from the domain.","The first step of the procedure is an estimation of how many elementary collisions is safe to take before chances of exiting the domain are too high; then these collisions are aggregated into a single movement.","The method does not use any model nor any particular regime of parameters.","We give theoretical results both under the normal approximation and without it and test the method on some benchmarks from the literature.","The results confirm the theoretical predictions and show that the proposal is an efficient method to sample the escape distribution of the particle."],"url":"http://arxiv.org/abs/2403.10432v1","category":"physics.comp-ph"}
{"created":"2024-03-15 16:00:27","title":"How to train your ears: Auditory-model emulation for large-dynamic-range inputs and mild-to-severe hearing losses","abstract":"Advanced auditory models are useful in designing signal-processing algorithms for hearing-loss compensation or speech enhancement. Such auditory models provide rich and detailed descriptions of the auditory pathway, and might allow for individualization of signal-processing strategies, based on physiological measurements. However, these auditory models are often computationally demanding, requiring significant time to compute. To address this issue, previous studies have explored the use of deep neural networks to emulate auditory models and reduce inference time. While these deep neural networks offer impressive efficiency gains in terms of computational time, they may suffer from uneven emulation performance as a function of auditory-model frequency-channels and input sound pressure level, making them unsuitable for many tasks. In this study, we demonstrate that the conventional machine-learning optimization objective used in existing state-of-the-art methods is the primary source of this limitation. Specifically, the optimization objective fails to account for the frequency- and level-dependencies of the auditory model, caused by a large input dynamic range and different types of hearing losses emulated by the auditory model. To overcome this limitation, we propose a new optimization objective that explicitly embeds the frequency- and level-dependencies of the auditory model. Our results show that this new optimization objective significantly improves the emulation performance of deep neural networks across relevant input sound levels and auditory-model frequency channels, without increasing the computational load during inference. Addressing these limitations is essential for advancing the application of auditory models in signal-processing tasks, ensuring their efficacy in diverse scenarios.","sentences":["Advanced auditory models are useful in designing signal-processing algorithms for hearing-loss compensation or speech enhancement.","Such auditory models provide rich and detailed descriptions of the auditory pathway, and might allow for individualization of signal-processing strategies, based on physiological measurements.","However, these auditory models are often computationally demanding, requiring significant time to compute.","To address this issue, previous studies have explored the use of deep neural networks to emulate auditory models and reduce inference time.","While these deep neural networks offer impressive efficiency gains in terms of computational time, they may suffer from uneven emulation performance as a function of auditory-model frequency-channels and input sound pressure level, making them unsuitable for many tasks.","In this study, we demonstrate that the conventional machine-learning optimization objective used in existing state-of-the-art methods is the primary source of this limitation.","Specifically, the optimization objective fails to account for the frequency- and level-dependencies of the auditory model, caused by a large input dynamic range and different types of hearing losses emulated by the auditory model.","To overcome this limitation, we propose a new optimization objective that explicitly embeds the frequency- and level-dependencies of the auditory model.","Our results show that this new optimization objective significantly improves the emulation performance of deep neural networks across relevant input sound levels and auditory-model frequency channels, without increasing the computational load during inference.","Addressing these limitations is essential for advancing the application of auditory models in signal-processing tasks, ensuring their efficacy in diverse scenarios."],"url":"http://arxiv.org/abs/2403.10428v1","category":"eess.AS"}
{"created":"2024-03-15 15:55:44","title":"Ferroelectric phases and phase transitions in CsGeBr$_3$ induced by mechanical load","abstract":"First-principles-based atomistic simulations are used to reveal ferroelectric phases and phase transitions induced in a semiconductor ferroelectric, CsGeBr$_3$, by external loads: hydrostatic pressure, uniaxial and biaxial stresses, and misfit strain. Hydrostatic pressure was found to suppress the Curie point at the rate -0.45$T_C(0)$ K/GPa, where $T_C(0)$ is the zero pressure Curie temperature. Stresses and misfit strains were found to induce additional ferroelectric phase transitions and phases not available under normal conditions. We find that tensile load significantly enhances both the Curie temperature and spontaneous polarization, while compressive load has the opposite effect but with the difference that the Curie temperature is only slightly suppressed. The isothermal dependencies of polarization on pressure and stresses are highly nonlinear, which could result in large nonlinear piezoelectric responses. The phase diagrams reveal the diversity of the phases accessible through mechanical load, which include tetragonal, orthorhombic and monoclinic symmetries in addition to the rhombohedral and cubic ones realizable under normal conditions. We believe that this work reveals the potential of Ge-based halide perovskites for applications in energy converting devices, which is especially significant in the current pursuit of environmental friendly lead-free technologies.","sentences":["First-principles-based atomistic simulations are used to reveal ferroelectric phases and phase transitions induced in a semiconductor ferroelectric, CsGeBr$_3$, by external loads: hydrostatic pressure, uniaxial and biaxial stresses, and misfit strain.","Hydrostatic pressure was found to suppress the Curie point at the rate -0.45$T_C(0)$ K/GPa, where $T_C(0)$ is the zero pressure Curie temperature.","Stresses and misfit strains were found to induce additional ferroelectric phase transitions and phases not available under normal conditions.","We find that tensile load significantly enhances both the Curie temperature and spontaneous polarization, while compressive load has the opposite effect but with the difference that the Curie temperature is only slightly suppressed.","The isothermal dependencies of polarization on pressure and stresses are highly nonlinear, which could result in large nonlinear piezoelectric responses.","The phase diagrams reveal the diversity of the phases accessible through mechanical load, which include tetragonal, orthorhombic and monoclinic symmetries in addition to the rhombohedral and cubic ones realizable under normal conditions.","We believe that this work reveals the potential of Ge-based halide perovskites for applications in energy converting devices, which is especially significant in the current pursuit of environmental friendly lead-free technologies."],"url":"http://arxiv.org/abs/2403.10421v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 15:54:58","title":"Fischer decompositions for entire functions of sufficiently low order","abstract":"The existence of decompositions of the form $f=P\\cdot q+r$ with $P_k^{\\ast}\\left( D\\right) r=0 $, where $f$ is entire, $P$ a polynomial and $P^{\\ast}_k$ the principal part of $P$ with its coefficients conjugated, was achieved in \\cite{AlRe23} under certain restrictions on the order of $f$. Here we prove uniqueness, thereby obtaining Fischer decompositions, under conditions that sometimes match those required for existence, and sometimes are more restrictive, depending on the parameters involved.","sentences":["The existence of decompositions of the form $f=P\\cdot q+r$ with $P_k^{\\ast}\\left( D\\right) r=0 $, where $f$ is entire, $P$ a polynomial and $P^{\\ast}_k$ the principal part of $P$ with its coefficients conjugated, was achieved in \\cite{AlRe23} under certain restrictions on the order of $f$. Here we prove uniqueness, thereby obtaining Fischer decompositions, under conditions that sometimes match those required for existence, and sometimes are more restrictive, depending on the parameters involved."],"url":"http://arxiv.org/abs/2403.10419v1","category":"math.AP"}
{"created":"2024-03-15 15:34:57","title":"A Fischer type decomposition theorem from the apolar inner product","abstract":"We continue the study initiated by H. S. Shapiro on Fischer decompositions of entire functions, showing that such decomposition exist in a weak sense (we do not prove uniqueness) under hypotheses regarding the order of the entire function $f$ to be expressed as $f= P\\cdot q+r$, the polynomial $P$, and bounds on the apolar norm of homogeneous polynomials of degree $m$.   These bounds, previously used by Khavinson and Shapiro, and by Ebenfelt and Shapiro, can be interpreted as a quantitative, asymptotic strengthening of Bombieri's inequality. In the special case where both the dimension of the space and the degree of $P$ are two, we characterize for which polynomials $P$ such bounds hold.","sentences":["We continue the study initiated by H. S. Shapiro on Fischer decompositions of entire functions, showing that such decomposition exist in a weak sense (we do not prove uniqueness) under hypotheses regarding the order of the entire function $f$ to be expressed as $f= P\\cdot q+r$, the polynomial $P$, and bounds on the apolar norm of homogeneous polynomials of degree $m$.   These bounds, previously used by Khavinson and Shapiro, and by Ebenfelt and Shapiro, can be interpreted as a quantitative, asymptotic strengthening of Bombieri's inequality.","In the special case where both the dimension of the space and the degree of $P$ are two, we characterize for which polynomials $P$ such bounds hold."],"url":"http://arxiv.org/abs/2403.10400v1","category":"math.AP"}
{"created":"2024-03-15 15:22:13","title":"CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning","abstract":"Pseudo-label-based semi-supervised learning (SSL) algorithms trained on a class-imbalanced set face two cascading challenges: 1) Classifiers tend to be biased towards majority classes, and 2) Biased pseudo-labels are used for training. It is difficult to appropriately re-balance the classifiers in SSL because the class distribution of an unlabeled set is often unknown and could be mismatched with that of a labeled set. We propose a novel class-imbalanced SSL algorithm called class-distribution-mismatch-aware debiasing (CDMAD). For each iteration of training, CDMAD first assesses the classifier's biased degree towards each class by calculating the logits on an image without any patterns (e.g., solid color image), which can be considered irrelevant to the training set. CDMAD then refines biased pseudo-labels of the base SSL algorithm by ensuring the classifier's neutrality. CDMAD uses these refined pseudo-labels during the training of the base SSL algorithm to improve the quality of the representations. In the test phase, CDMAD similarly refines biased class predictions on test samples. CDMAD can be seen as an extension of post-hoc logit adjustment to address a challenge of incorporating the unknown class distribution of the unlabeled set for re-balancing the biased classifier under class distribution mismatch. CDMAD ensures Fisher consistency for the balanced error. Extensive experiments verify the effectiveness of CDMAD.","sentences":["Pseudo-label-based semi-supervised learning (SSL) algorithms trained on a class-imbalanced set face two cascading challenges: 1) Classifiers tend to be biased towards majority classes, and 2) Biased pseudo-labels are used for training.","It is difficult to appropriately re-balance the classifiers in SSL because the class distribution of an unlabeled set is often unknown and could be mismatched with that of a labeled set.","We propose a novel class-imbalanced SSL algorithm called class-distribution-mismatch-aware debiasing (CDMAD).","For each iteration of training, CDMAD first assesses the classifier's biased degree towards each class by calculating the logits on an image without any patterns (e.g., solid color image), which can be considered irrelevant to the training set.","CDMAD then refines biased pseudo-labels of the base SSL algorithm by ensuring the classifier's neutrality.","CDMAD uses these refined pseudo-labels during the training of the base SSL algorithm to improve the quality of the representations.","In the test phase, CDMAD similarly refines biased class predictions on test samples.","CDMAD can be seen as an extension of post-hoc logit adjustment to address a challenge of incorporating the unknown class distribution of the unlabeled set for re-balancing the biased classifier under class distribution mismatch.","CDMAD ensures Fisher consistency for the balanced error.","Extensive experiments verify the effectiveness of CDMAD."],"url":"http://arxiv.org/abs/2403.10391v1","category":"cs.CV"}
{"created":"2024-03-15 15:10:41","title":"Monotonic Representation of Numeric Properties in Language Models","abstract":"Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year. Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion. When editing representations along directions in these subspaces, LM output changes accordingly. For example, by patching activations along a \"birthyear\" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic representation of numeric properties consistently emerges during LM pretraining. Code: https://github.com/bheinzerling/numeric-property-repr","sentences":["Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902.","However, how this information is encoded in the model's internal representations is not understood well.","Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year.","Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion.","When editing representations along directions in these subspaces, LM output changes accordingly.","For example, by patching activations along a \"birthyear\" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968.","Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic representation of numeric properties consistently emerges during LM pretraining.","Code: https://github.com/bheinzerling/numeric-property-repr"],"url":"http://arxiv.org/abs/2403.10381v1","category":"cs.CL"}
{"created":"2024-03-15 14:35:05","title":"ParaPoint: Learning Global Free-Boundary Surface Parameterization of 3D Point Clouds","abstract":"Surface parameterization is a fundamental geometry processing problem with rich downstream applications. Traditional approaches are designed to operate on well-behaved mesh models with high-quality triangulations that are laboriously produced by specialized 3D modelers, and thus unable to meet the processing demand for the current explosion of ordinary 3D data. In this paper, we seek to perform UV unwrapping on unstructured 3D point clouds. Technically, we propose ParaPoint, an unsupervised neural learning pipeline for achieving global free-boundary surface parameterization by building point-wise mappings between given 3D points and 2D UV coordinates with adaptively deformed boundaries. We ingeniously construct several geometrically meaningful sub-networks with specific functionalities, and assemble them into a bi-directional cycle mapping framework. We also design effective loss functions and auxiliary differential geometric constraints for the optimization of the neural mapping process. To the best of our knowledge, this work makes the first attempt to investigate neural point cloud parameterization that pursues both global mappings and free boundaries. Experiments demonstrate the effectiveness and inspiring potential of our proposed learning paradigm. The code will be publicly available.","sentences":["Surface parameterization is a fundamental geometry processing problem with rich downstream applications.","Traditional approaches are designed to operate on well-behaved mesh models with high-quality triangulations that are laboriously produced by specialized 3D modelers, and thus unable to meet the processing demand for the current explosion of ordinary 3D data.","In this paper, we seek to perform UV unwrapping on unstructured 3D point clouds.","Technically, we propose ParaPoint, an unsupervised neural learning pipeline for achieving global free-boundary surface parameterization by building point-wise mappings between given 3D points and 2D UV coordinates with adaptively deformed boundaries.","We ingeniously construct several geometrically meaningful sub-networks with specific functionalities, and assemble them into a bi-directional cycle mapping framework.","We also design effective loss functions and auxiliary differential geometric constraints for the optimization of the neural mapping process.","To the best of our knowledge, this work makes the first attempt to investigate neural point cloud parameterization that pursues both global mappings and free boundaries.","Experiments demonstrate the effectiveness and inspiring potential of our proposed learning paradigm.","The code will be publicly available."],"url":"http://arxiv.org/abs/2403.10349v1","category":"cs.CV"}
{"created":"2024-03-15 14:27:15","title":"Thermal-NeRF: Neural Radiance Fields from an Infrared Camera","abstract":"In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant potential in encoding highly-detailed 3D geometry and environmental appearance, positioning themselves as a promising alternative to traditional explicit representation for 3D scene reconstruction. However, the predominant reliance on RGB imaging presupposes ideal lighting conditions: a premise frequently unmet in robotic applications plagued by poor lighting or visual obstructions. This limitation overlooks the capabilities of infrared (IR) cameras, which excel in low-light detection and present a robust alternative under such adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first method that estimates a volumetric scene representation in the form of a NeRF solely from IR imaging. By leveraging a thermal mapping and structural thermal constraint derived from the thermal characteristics of IR imaging, our method showcasing unparalleled proficiency in recovering NeRFs in visually degraded scenes where RGB-based methods fall short. We conduct extensive experiments to demonstrate that Thermal-NeRF can achieve superior quality compared to existing methods. Furthermore, we contribute a dataset for IR-based NeRF applications, paving the way for future research in IR NeRF reconstruction.","sentences":["In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant potential in encoding highly-detailed 3D geometry and environmental appearance, positioning themselves as a promising alternative to traditional explicit representation for 3D scene reconstruction.","However, the predominant reliance on RGB imaging presupposes ideal lighting conditions: a premise frequently unmet in robotic applications plagued by poor lighting or visual obstructions.","This limitation overlooks the capabilities of infrared (IR) cameras, which excel in low-light detection and present a robust alternative under such adverse scenarios.","To tackle these issues, we introduce Thermal-NeRF, the first method that estimates a volumetric scene representation in the form of a NeRF solely from IR imaging.","By leveraging a thermal mapping and structural thermal constraint derived from the thermal characteristics of IR imaging, our method showcasing unparalleled proficiency in recovering NeRFs in visually degraded scenes where RGB-based methods fall short.","We conduct extensive experiments to demonstrate that Thermal-NeRF can achieve superior quality compared to existing methods.","Furthermore, we contribute a dataset for IR-based NeRF applications, paving the way for future research in IR NeRF reconstruction."],"url":"http://arxiv.org/abs/2403.10340v1","category":"cs.CV"}
{"created":"2024-03-15 14:16:21","title":"The cool and the cruel: separating hard parts of LWE secrets","abstract":"Sparse binary LWE secrets are under consideration for standardization for Homomorphic Encryption and its applications to private computation. Known attacks on sparse binary LWE secrets include the sparse dual attack and the hybrid sparse dual-meet in the middle attack which requires significant memory. In this paper, we provide a new statistical attack with low memory requirement. The attack relies on some initial lattice reduction. The key observation is that, after lattice reduction is applied to the rows of a q-ary-like embedded random matrix $\\mathbf A$, the entries with high variance are concentrated in the early columns of the extracted matrix. This allows us to separate out the \"hard part\" of the LWE secret. We can first solve the sub-problem of finding the \"cruel\" bits of the secret in the early columns, and then find the remaining \"cool\" bits in linear time. We use statistical techniques to distinguish distributions to identify both the cruel and the cool bits of the secret. We provide concrete attack timings for recovering secrets in dimensions $n=256$, $512$, and $768$. For the lattice reduction stage, we leverage recent improvements in lattice reduction (e.g. flatter) applied in parallel. We also apply our new attack in the RLWE setting for $2$-power cyclotomic rings, showing that these RLWE instances are much more vulnerable to this attack than LWE.","sentences":["Sparse binary LWE secrets are under consideration for standardization for Homomorphic Encryption and its applications to private computation.","Known attacks on sparse binary LWE secrets include the sparse dual attack and the hybrid sparse dual-meet in the middle attack which requires significant memory.","In this paper, we provide a new statistical attack with low memory requirement.","The attack relies on some initial lattice reduction.","The key observation is that, after lattice reduction is applied to the rows of a q-ary-like embedded random matrix $\\mathbf A$, the entries with high variance are concentrated in the early columns of the extracted matrix.","This allows us to separate out the \"hard part\" of the LWE secret.","We can first solve the sub-problem of finding the \"cruel\" bits of the secret in the early columns, and then find the remaining \"cool\" bits in linear time.","We use statistical techniques to distinguish distributions to identify both the cruel and the cool bits of the secret.","We provide concrete attack timings for recovering secrets in dimensions $n=256$, $512$, and $768$. For the lattice reduction stage, we leverage recent improvements in lattice reduction (e.g. flatter) applied in parallel.","We also apply our new attack in the RLWE setting for $2$-power cyclotomic rings, showing that these RLWE instances are much more vulnerable to this attack than LWE."],"url":"http://arxiv.org/abs/2403.10328v1","category":"cs.CR"}
{"created":"2024-03-15 14:10:17","title":"A universal crack tip correction algorithm discovered by physical deep symbolic regression","abstract":"Digital image correlation is a widely used technique in the field of experimental mechanics. In fracture mechanics, determining the precise location of the crack tip is crucial. In this paper, we introduce a universal crack tip detection algorithm based on displacement and strain fields obtained by digital image correlation. Iterative crack tip correction formulas are discovered by applying deep symbolic regression guided by physical unit constraints to a dataset of simulated cracks under mode I, II and mixed-mode conditions with variable T-stress. For the training dataset, we fit the Williams series expansion with super-singular terms to the simulated displacement fields at randomly chosen origins around the actual crack tip. We analyse the discovered formulas and apply the most promising one to digital image correlation data obtained from uniaxial and biaxial fatigue crack growth experiments of AA2024-T3 sheet material. Throughout the experiments, the crack tip positions are reliably detected leading to improved stability of the crack propagation curves.","sentences":["Digital image correlation is a widely used technique in the field of experimental mechanics.","In fracture mechanics, determining the precise location of the crack tip is crucial.","In this paper, we introduce a universal crack tip detection algorithm based on displacement and strain fields obtained by digital image correlation.","Iterative crack tip correction formulas are discovered by applying deep symbolic regression guided by physical unit constraints to a dataset of simulated cracks under mode I, II and mixed-mode conditions with variable T-stress.","For the training dataset, we fit the Williams series expansion with super-singular terms to the simulated displacement fields at randomly chosen origins around the actual crack tip.","We analyse the discovered formulas and apply the most promising one to digital image correlation data obtained from uniaxial and biaxial fatigue crack growth experiments of AA2024-T3 sheet material.","Throughout the experiments, the crack tip positions are reliably detected leading to improved stability of the crack propagation curves."],"url":"http://arxiv.org/abs/2403.10320v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 14:00:59","title":"A de Finetti theorem for quantum causal structures","abstract":"What does it mean for a causal structure to be `unknown'? Can we even talk about `repetitions' of an experiment without prior knowledge of causal relations? And under what conditions can we say that a set of processes with arbitrary, possibly indefinite, causal structure are independent and identically distributed? Similar questions for classical probabilities, quantum states, and quantum channels are beautifully answered by so-called \"de Finetti theorems\", which connect a simple and easy-to-justify condition -- symmetry under exchange -- with a very particular multipartite structure: a mixture of identical states/channels. Here we extend the result to processes with arbitrary causal structure, including indefinite causal order and multi-time, non-Markovian processes applicable to noisy quantum devices. The result also implies a new class of de Finetti theorems for quantum states subject to a large class of linear constraints, which can be of independent interest.","sentences":["What does it mean for a causal structure to be `unknown'?","Can we even talk about `repetitions' of an experiment without prior knowledge of causal relations?","And under what conditions can we say that a set of processes with arbitrary, possibly indefinite, causal structure are independent and identically distributed?","Similar questions for classical probabilities, quantum states, and quantum channels are beautifully answered by so-called \"de Finetti theorems\", which connect a simple and easy-to-justify condition -- symmetry under exchange -- with a very particular multipartite structure: a mixture of identical states/channels.","Here we extend the result to processes with arbitrary causal structure, including indefinite causal order and multi-time, non-Markovian processes applicable to noisy quantum devices.","The result also implies a new class of de Finetti theorems for quantum states subject to a large class of linear constraints, which can be of independent interest."],"url":"http://arxiv.org/abs/2403.10316v1","category":"quant-ph"}
{"created":"2024-03-15 13:57:05","title":"Designing User-Centered Simulations of Leadership Situations for Cave Automatic Virtual Environments: Development and Usability Study","abstract":"Given that experience is a pivotal dimension of learning processes in the field of leadership, the ongoing and unresolved issue is how such experiential moments could be provided when developing leadership skills and competencies. Role-plays and business simulations are widely used in this context as they are said to teach relevant social leadership skills, like those required by everyday communication to followers, by decision-making on compensation, evaluating performance, dealing with conflicts, or terminating contracts. However, the effectiveness of simulations can highly vary depending on the counterpart's ability to act in the given scenarios. In our project, we deal with how immersive media could create experiential learning based on simulations for leadership development. In recent years different variations of extended reality got significant technological improvements. Head-mounted displays are an easy and cost-efficient way to present high-resolution virtual environments. For groups of people that are part of an immersive experience, cave automatic virtual environments offer an excellent trade-off between actual exchange with other humans and interaction with virtual content simultaneously. The work presented is based on developing a user-centered simulation of leadership situations for cave automatic virtual environments and includes the results of a first usability study. In the future, the presented results can help to support the development and evaluation of simulated situations for cave automatic virtual environments with an emphasis on leadership-related scenarios.","sentences":["Given that experience is a pivotal dimension of learning processes in the field of leadership, the ongoing and unresolved issue is how such experiential moments could be provided when developing leadership skills and competencies.","Role-plays and business simulations are widely used in this context as they are said to teach relevant social leadership skills, like those required by everyday communication to followers, by decision-making on compensation, evaluating performance, dealing with conflicts, or terminating contracts.","However, the effectiveness of simulations can highly vary depending on the counterpart's ability to act in the given scenarios.","In our project, we deal with how immersive media could create experiential learning based on simulations for leadership development.","In recent years different variations of extended reality got significant technological improvements.","Head-mounted displays are an easy and cost-efficient way to present high-resolution virtual environments.","For groups of people that are part of an immersive experience, cave automatic virtual environments offer an excellent trade-off between actual exchange with other humans and interaction with virtual content simultaneously.","The work presented is based on developing a user-centered simulation of leadership situations for cave automatic virtual environments and includes the results of a first usability study.","In the future, the presented results can help to support the development and evaluation of simulated situations for cave automatic virtual environments with an emphasis on leadership-related scenarios."],"url":"http://arxiv.org/abs/2403.10312v1","category":"cs.HC"}
{"created":"2024-03-15 13:43:47","title":"Uni-SMART: Universal Science Multimodal Analysis and Research Transformer","abstract":"In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this demand, we present Uni-SMART (Universal Science Multimodal Analysis and Research Transformer), an innovative model designed for in-depth understanding of multimodal scientific literature. Through rigorous quantitative evaluation across several domains, Uni-SMART demonstrates superior performance over leading text-focused LLMs. Furthermore, our exploration extends to practical applications, including patent infringement detection and nuanced analysis of charts. These applications not only highlight Uni-SMART's adaptability but also its potential to revolutionize how we interact with scientific literature.","sentences":["In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others.","However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming.","The emergence of Large Language Models (LLMs) has offered a new way to address this challenge.","Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature.","However, existing LLMs have their own limits.","Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze.","This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature.","To answer this demand, we present Uni-SMART (Universal Science Multimodal Analysis and Research Transformer), an innovative model designed for in-depth understanding of multimodal scientific literature.","Through rigorous quantitative evaluation across several domains, Uni-SMART demonstrates superior performance over leading text-focused LLMs.","Furthermore, our exploration extends to practical applications, including patent infringement detection and nuanced analysis of charts.","These applications not only highlight Uni-SMART's adaptability but also its potential to revolutionize how we interact with scientific literature."],"url":"http://arxiv.org/abs/2403.10301v1","category":"cs.CL"}
{"created":"2024-03-15 13:33:10","title":"MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank","abstract":"Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in `within-language breadth': most treebanks focus on standard languages. Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian. To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction). We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers' orthographies. Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries. We provide baseline parsing and POS tagging results, which are lower than results obtained on German and vary substantially between different graph-based parsers. To support further research on Bavarian syntax, we make our dataset, language-specific guidelines and code publicly available.","sentences":["Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in `within-language breadth': most treebanks focus on standard languages.","Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian.","To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction).","We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers' orthographies.","Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries.","We provide baseline parsing and POS tagging results, which are lower than results obtained on German and vary substantially between different graph-based parsers.","To support further research on Bavarian syntax, we make our dataset, language-specific guidelines and code publicly available."],"url":"http://arxiv.org/abs/2403.10293v1","category":"cs.CL"}
{"created":"2024-03-15 13:29:41","title":"Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models","abstract":"The task of few-shot image classification and segmentation (FS-CS) involves classifying and segmenting target objects in a query image, given only a few examples of the target classes. We introduce the Vision-Instructed Segmentation and Evaluation (VISE) method that transforms the FS-CS problem into the Visual Question Answering (VQA) problem, utilising Vision-Language Models (VLMs), and addresses it in a training-free manner. By enabling a VLM to interact with off-the-shelf vision models as tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, chain-of-thought prompting and in-context learning guide the VLM to answer multiple-choice questions like a human; vision models such as YOLO and Segment Anything Model (SAM) assist the VLM in completing the task. The modular framework of the proposed method makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i and COCO-20i datasets.","sentences":["The task of few-shot image classification and segmentation (FS-CS) involves classifying and segmenting target objects in a query image, given only a few examples of the target classes.","We introduce the Vision-Instructed Segmentation and Evaluation (VISE) method that transforms the FS-CS problem into the Visual Question Answering (VQA) problem, utilising Vision-Language Models (VLMs), and addresses it in a training-free manner.","By enabling a VLM to interact with off-the-shelf vision models as tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels.","Specifically, chain-of-thought prompting and in-context learning guide the VLM to answer multiple-choice questions like a human; vision models such as YOLO and Segment Anything Model (SAM) assist the VLM in completing the task.","The modular framework of the proposed method makes it easily extendable.","Our approach achieves state-of-the-art performance on the Pascal-5i and COCO-20i datasets."],"url":"http://arxiv.org/abs/2403.10287v1","category":"cs.CV"}
{"created":"2024-03-15 13:26:36","title":"Optimal Control of Stationary Doubly Diffusive Flows on Two and Three Dimensional Bounded Lipschitz Domains: Numerical Analysis","abstract":"In this work, we propose fully nonconforming, locally exactly divergence-free discretizations based on lowest order Crouziex-Raviart finite element and piecewise constant spaces to study the optimal control of stationary double diffusion model presented in [B\\\"urger, M\\'endez, Ruiz-Baier, SINUM (2019), 57:1318-1343]. The well-posedness of the discrete uncontrolled state and adjoint equations are discussed using discrete lifting and fixed point arguments, and convergence results are derived rigorously under minimal regularity. Building upon our recent work [Tushar, Khan, Mohan arXiv (2023)], we prove the local optimality of a reference control using second-order sufficient optimality condition for the control problem, and use it along with an optimize-then-discretize approach to prove optimal order a priori error estimates for the control, state and adjoint variables upto the regularity of the solution. The optimal control is computed using a primal-dual active set strategy as a semi-smooth Newton method and computational tests validate the predicted error decay rates and illustrate the proposed scheme's applicability to optimal control of thermohaline circulation problems.","sentences":["In this work, we propose fully nonconforming, locally exactly divergence-free discretizations based on lowest order Crouziex-Raviart finite element and piecewise constant spaces to study the optimal control of stationary double diffusion model presented in [B\\\"urger, M\\'endez, Ruiz-Baier, SINUM (2019), 57:1318-1343].","The well-posedness of the discrete uncontrolled state and adjoint equations are discussed using discrete lifting and fixed point arguments, and convergence results are derived rigorously under minimal regularity.","Building upon our recent work [Tushar, Khan, Mohan arXiv (2023)], we prove the local optimality of a reference control using second-order sufficient optimality condition for the control problem, and use it along with an optimize-then-discretize approach to prove optimal order a priori error estimates for the control, state and adjoint variables upto the regularity of the solution.","The optimal control is computed using a primal-dual active set strategy as a semi-smooth Newton method and computational tests validate the predicted error decay rates and illustrate the proposed scheme's applicability to optimal control of thermohaline circulation problems."],"url":"http://arxiv.org/abs/2403.10282v1","category":"math.NA"}
{"created":"2024-03-15 13:22:34","title":"Almost sure OTM-realizability","abstract":"Combining the approaches made in works with Galeotti and Passmann, we define and study a notion of \"almost sure\" realizability with parameter-free ordinal Turing machines (OTMs). In particular, we show that, in contrast to the classical case, almost sure realizability differs from plain realizability, while closure under intuitionistic predicate logic and realizability of Kripke-Platek set theory continue to hold.","sentences":["Combining the approaches made in works with Galeotti and Passmann, we define and study a notion of \"almost sure\" realizability with parameter-free ordinal Turing machines (OTMs).","In particular, we show that, in contrast to the classical case, almost sure realizability differs from plain realizability, while closure under intuitionistic predicate logic and realizability of Kripke-Platek set theory continue to hold."],"url":"http://arxiv.org/abs/2403.10280v1","category":"math.LO"}
{"created":"2024-03-15 12:49:51","title":"Probing the anomalous triple $ZZ\u03b3$ and $Z\u03b3\u03b3$ couplings at the FCC-$\u03bcp$ and SPPC-$\u03bcp$","abstract":"In this study, 24.5 TeV CoM energy FCC-$\\mu p$ and 20.2 TeV CoM energy SPPC-$\\mu p$ muon-proton colliders have been utilized to explore the anomalous $ZZ\\gamma$ and $Z\\gamma\\gamma$ couplings corresponding to dim-8 operators through the process of $\\mu^- \\gamma \\to Z l^{-} \\to l^{-} \\tilde{\\nu_{l}} \\nu_{l}$. A cut-based method has been applied to enhance the signal background ratio in the analysis. Coupling limits, at a 95\\% Confidence Level (C.L.), under the systematic uncertainties of 0\\%, 3\\%, and 5\\%, were obtained with luminosities of ${\\cal L}_{int}=5$ and 42.8 fb$^{-1}$ for FCC-$\\mu p$ and SPPC-$\\mu p$ colliders, respectively. The limits for anomalous $C_{BB}/\\Lambda^{4}$, $C_{\\tilde{B}W}/\\Lambda^{4}$, $C_{WW}/\\Lambda^{4}$, $C_{BW}/\\Lambda^{4}$ couplings without systematic uncertainty for FCC-$\\mu p$ and SPPC-$\\mu p$ were found as follows, respectively: [0.06845; 0.070019967] TeV-4, [-0.18662; 0.18422] TeV-4, [-0.23623; 0.23815] TeV-4, [-0.61912; 0.62072] TeV-4, and [-0.06606; 0.05050] TeV-4, [-0.15791; 0.15126] TeV-4, [-0.20663; 0.19155] TeV-4, [-0.51383; 0.52211] TeV-4.","sentences":["In this study, 24.5 TeV CoM energy FCC-$\\mu p$ and 20.2 TeV CoM energy SPPC-$\\mu p$ muon-proton colliders have been utilized to explore the anomalous $ZZ\\gamma$ and $Z\\gamma\\gamma$ couplings corresponding to dim-8 operators through the process of $\\mu^- \\gamma \\to Z l^{-} \\to l^{-} \\tilde{\\nu_{l}} \\nu_{l}$. A cut-based method has been applied to enhance the signal background ratio in the analysis.","Coupling limits, at a 95\\% Confidence Level (C.L.), under the systematic uncertainties of 0\\%, 3\\%, and 5\\%, were obtained with luminosities of ${\\cal L}_{int}=5$ and 42.8 fb$^{-1}$ for FCC-$\\mu p$ and SPPC-$\\mu p$ colliders, respectively.","The limits for anomalous $C_{BB}/\\Lambda^{4}$, $C_{\\tilde{B}W}/\\Lambda^{4}$, $C_{WW}/\\Lambda^{4}$, $C_{BW}/\\Lambda^{4}$ couplings without systematic uncertainty for FCC-$\\mu p$ and SPPC-$\\mu p$ were found as follows, respectively: [0.06845; 0.070019967] TeV-4, [-0.18662; 0.18422]","TeV-4, [-0.23623; 0.23815] TeV-4, [-0.61912; 0.62072] TeV-4, and [-0.06606; 0.05050]","TeV-4, [-0.15791; 0.15126]","TeV-4, [-0.20663; 0.19155] TeV-4, [-0.51383; 0.52211] TeV-4."],"url":"http://arxiv.org/abs/2403.10263v1","category":"hep-ph"}
{"created":"2024-03-15 12:12:44","title":"JWST NIRSpec Spectroscopy of the Remarkable Bright Galaxy GHZ2/GLASS-z12 at Redshift 12.34","abstract":"We spectroscopically confirm the $M_{\\rm UV} = -20.5$ mag galaxy GHZ2/GLASS-z12 to be at redshift $z=12.34$. The source was selected via NIRCam photometry in GLASS-JWST Early Release Science data, providing the first evidence of a surprising abundance of bright galaxies at $z \\gtrsim 10$. The NIRSpec PRISM spectrum is remarkable and unlike any local analog. It shows significant detections of N IV, C IV, He II, O III, C III, O II, and Ne III lines, and the first detection in a high-redshift object of the O III Bowen fluorescence line at 3133 {\\AA} rest-frame. The prominent C IV line with rest-frame equivalent width (EW) $\\sim 46$ {\\AA} puts GHZ2 in the category of extreme C IV emitters characterised by hard radiation fields. GHZ2 displays UV lines with EWs that are only found in active galactic nuclei (AGNs) or composite objects at low/intermediate redshifts, and UV line-intensity ratios that are compatible both with AGNs and star formation in a low-metallicity environment. The nondetection of the very high-ionization lines [Ne IV] and [Ne V], and the remarkable similarity between GHZ2 and other known C IV emitters, favors a scenario in which the high ionizing output is due to very low metallicity, massive stars forming in a dense environment. We estimate a metallicity $\\lesssim 0.1 Z/{\\rm Z}_{\\odot}$, a high ionization parameter logU $>$ -2, a N/O abundance 4--5 times the solar value, and a subsolar C/O ratio similar to the recently discovered class of nitrogen-enhanced objects at high redshift. Considering its abundance patterns and the high stellar mass density ($10^4$ M$_{\\odot}$ pc$^{-2}$), GHZ2 is an ideal formation site for the progenitors of today's globular clusters. The remarkable brightness of GHZ2 makes it a \"Rosetta stone\" for understanding the physics of galaxy formation within just 360 Myr after the Big Bang.","sentences":["We spectroscopically confirm the $M_{\\rm UV} = -20.5$ mag galaxy GHZ2/GLASS-z12 to be at redshift $z=12.34$. The source was selected via NIRCam photometry in GLASS-JWST Early Release Science data, providing the first evidence of a surprising abundance of bright galaxies at $z \\gtrsim 10$.","The NIRSpec PRISM spectrum is remarkable and unlike any local analog.","It shows significant detections of N IV, C IV, He II, O III, C III, O II, and Ne III lines, and the first detection in a high-redshift object of the O III Bowen fluorescence line at 3133 {\\AA} rest-frame.","The prominent C IV line with rest-frame equivalent width (EW) $\\sim 46$ {\\AA} puts GHZ2 in the category of extreme C IV emitters characterised by hard radiation fields.","GHZ2 displays UV lines with EWs that are only found in active galactic nuclei (AGNs) or composite objects at low/intermediate redshifts, and UV line-intensity ratios that are compatible both with AGNs and star formation in a low-metallicity environment.","The nondetection of the very high-ionization lines [Ne IV] and [Ne V], and the remarkable similarity between GHZ2 and other known C IV emitters, favors a scenario in which the high ionizing output is due to very low metallicity, massive stars forming in a dense environment.","We estimate a metallicity $\\lesssim 0.1 Z/{\\rm Z}_{\\odot}$, a high ionization parameter logU $>$ -2, a N/O abundance 4--5 times the solar value, and a subsolar C/O ratio similar to the recently discovered class of nitrogen-enhanced objects at high redshift.","Considering its abundance patterns and the high stellar mass density ($10^4$ M$_{\\odot}$ pc$^{-2}$), GHZ2 is an ideal formation site for the progenitors of today's globular clusters.","The remarkable brightness of GHZ2 makes it a \"Rosetta stone\" for understanding the physics of galaxy formation within just 360 Myr after the Big Bang."],"url":"http://arxiv.org/abs/2403.10238v1","category":"astro-ph.GA"}
{"created":"2024-03-15 11:56:39","title":"Role of magnetic fields on the outer crust in a magnetar","abstract":"We explore the properties of 4110 nuclides from Z = 5 to Z = 82 with the Sky3D code and the composition of the outer crust in the magnetars under extreme magnetic fields. The effects of the variation of the nuclear masses due to the magnetic fields on the outer crust are comprehensively studied. The neutron-drip transition pressure, the equation of state and neutron fraction in the outer crust have also been discussed.","sentences":["We explore the properties of 4110 nuclides from Z = 5 to Z = 82 with the Sky3D code and the composition of the outer crust in the magnetars under extreme magnetic fields.","The effects of the variation of the nuclear masses due to the magnetic fields on the outer crust are comprehensively studied.","The neutron-drip transition pressure, the equation of state and neutron fraction in the outer crust have also been discussed."],"url":"http://arxiv.org/abs/2403.10227v1","category":"nucl-th"}
{"created":"2024-03-15 11:53:46","title":"Liquid Staking Tokens in Automated Market Makers","abstract":"This paper studies liquid staking tokens (LSTs) on automated market makers (AMMs), both theoretically and empirically. LSTs are tokenized representations of staked assets on proof-of-stake blockchains. First, we theoretically model LST-liquidity on AMMs. This includes categorizing suitable AMM types for LST liquidity, as well as deriving formulas for the necessary returns from trading fees to adequately compensate liquidity providers under the particular price trajectories of LSTs. Two relevant metrics are considered. Firstly, losses compared to holding the liquidity outside the AMM (loss-versus-holding, or impermanent loss). Secondly, the relative profitability compared to fully staking the capital (loss-versus-staking) which is a metric specifically introduced for the case of LST-liquidity. Subsequently, we empirically measure these metrics for Ethereum LSTs across the most relevant AMM pools. We find that, while trading fees often compensate for impermanent loss, fully staking is more profitable for many pools, putting the sustainability of current LST allocation to AMMs into question.","sentences":["This paper studies liquid staking tokens (LSTs) on automated market makers (AMMs), both theoretically and empirically.","LSTs are tokenized representations of staked assets on proof-of-stake blockchains.","First, we theoretically model LST-liquidity on AMMs.","This includes categorizing suitable AMM types for LST liquidity, as well as deriving formulas for the necessary returns from trading fees to adequately compensate liquidity providers under the particular price trajectories of LSTs.","Two relevant metrics are considered.","Firstly, losses compared to holding the liquidity outside the AMM (loss-versus-holding, or impermanent loss).","Secondly, the relative profitability compared to fully staking the capital (loss-versus-staking) which is a metric specifically introduced for the case of LST-liquidity.","Subsequently, we empirically measure these metrics for Ethereum LSTs across the most relevant AMM pools.","We find that, while trading fees often compensate for impermanent loss, fully staking is more profitable for many pools, putting the sustainability of current LST allocation to AMMs into question."],"url":"http://arxiv.org/abs/2403.10226v1","category":"cs.CR"}
{"created":"2024-03-15 11:52:14","title":"A single-photon large-momentum-transfer atom interferometry scheme for Sr/Yb atoms with application to determining the fine-structure constant","abstract":"The leading experimental determinations of the fine-structure constant, $\\alpha$, currently rely on atomic photon-recoil measurements from Ramsey-Bord\\'e atom interferometry with large momentum transfer to provide an absolute mass measurement. We propose an experimental scheme for an intermediate-scale differential atom interferometer to measure the photon-recoil of neutral atomic species with a single-photon optical clock transition. We calculate trajectories for our scheme that optimise the recoil phase while nullifying the undesired gravity gradient phase by considering independently launching two clouds of ultracold atoms with the appropriate initial conditions. For Sr and Yb, we find an atom interferometer of height 3m to be sufficient for an absolute mass measurement precision of $\\Delta m / m \\sim 1\\times 10^{-11}$ with current technology. Such a precise measurement (the first of its kind for Sr or Yb) would yield a factor of two reduction in the uncertainty of $\\alpha$ -- an uncertainty that would no longer be limited by an absolute mass measurement. The removal of this bound facilitates reducing the uncertainty in $\\alpha$ by a factor of 10 with improvements in relative mass measurements, thus paving the way for higher-precision tests of the Standard Model of particle physics.","sentences":["The leading experimental determinations of the fine-structure constant, $\\alpha$, currently rely on atomic photon-recoil measurements from Ramsey-Bord\\'e atom interferometry with large momentum transfer to provide an absolute mass measurement.","We propose an experimental scheme for an intermediate-scale differential atom interferometer to measure the photon-recoil of neutral atomic species with a single-photon optical clock transition.","We calculate trajectories for our scheme that optimise the recoil phase while nullifying the undesired gravity gradient phase by considering independently launching two clouds of ultracold atoms with the appropriate initial conditions.","For Sr and Yb, we find an atom interferometer of height 3m to be sufficient for an absolute mass measurement precision of $\\Delta m / m \\sim 1\\times 10^{-11}$ with current technology.","Such a precise measurement (the first of its kind for Sr or Yb) would yield a factor of two reduction in the uncertainty of $\\alpha$ -- an uncertainty that would no longer be limited by an absolute mass measurement.","The removal of this bound facilitates reducing the uncertainty in $\\alpha$ by a factor of 10 with improvements in relative mass measurements, thus paving the way for higher-precision tests of the Standard Model of particle physics."],"url":"http://arxiv.org/abs/2403.10225v1","category":"physics.atom-ph"}
{"created":"2024-03-15 11:32:44","title":"Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis","abstract":"Aspect-category-based sentiment analysis (ACSA), which aims to identify aspect categories and predict their sentiments has been intensively studied due to its wide range of NLP applications. Most approaches mainly utilize intrasentential features. However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review. Even in a sentence, there is more than one aspect category with its sentiments, and they are entangled intra-sentence, which makes the model fail to discriminately preserve all sentiment characteristics. In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and sentiment identification. To address the issue of multiple aspect categories and sentiment entanglement, we propose a hierarchical disentanglement module to extract distinct categories and sentiment features. Extensive experimental and visualization results show that our ECAN effectively decouples multiple categories and sentiments entangled in the coherence representations and achieves state-of-the-art (SOTA) performance. Our codes and data are available online: \\url{https://github.com/cuijin-23/ECAN}.","sentences":["Aspect-category-based sentiment analysis (ACSA), which aims to identify aspect categories and predict their sentiments has been intensively studied due to its wide range of NLP applications.","Most approaches mainly utilize intrasentential features.","However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review.","Even in a sentence, there is more than one aspect category with its sentiments, and they are entangled intra-sentence, which makes the model fail to discriminately preserve all sentiment characteristics.","In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks.","Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and sentiment identification.","To address the issue of multiple aspect categories and sentiment entanglement, we propose a hierarchical disentanglement module to extract distinct categories and sentiment features.","Extensive experimental and visualization results show that our ECAN effectively decouples multiple categories and sentiments entangled in the coherence representations and achieves state-of-the-art (SOTA) performance.","Our codes and data are available online: \\url{https://github.com/cuijin-23/ECAN}."],"url":"http://arxiv.org/abs/2403.10214v1","category":"cs.CL"}
{"created":"2024-03-15 11:16:47","title":"Irrational Random Utility Models","abstract":"We show that the set of aggregate choices of a population of rational decision-makers - random utility models (RUMs) - can be represented by a population of irrational ones if, and only if, their preferences are sufficiently uncorrelated. We call this representation: Irrational RUM. We then show that almost all RUMs can be represented by a population in which at least some decision-makers are irrational and that under specific conditions their irrational behavior is unconstrained.","sentences":["We show that the set of aggregate choices of a population of rational decision-makers - random utility models (RUMs) - can be represented by a population of irrational ones if, and only if, their preferences are sufficiently uncorrelated.","We call this representation: Irrational RUM.","We then show that almost all RUMs can be represented by a population in which at least some decision-makers are irrational and that under specific conditions their irrational behavior is unconstrained."],"url":"http://arxiv.org/abs/2403.10208v1","category":"econ.TH"}
{"created":"2024-03-15 10:58:24","title":"Half-metallic transport and spin-polarized tunneling through the van der Waals ferromagnet Fe${_4}$GeTe$_{2}$","abstract":"The recent emergence of van der Waals (vdW) ferromagnets has opened new opportunities for designing spintronic devices. We theoretically investigate the coherent spin-dependent transport properties of the vdW ferromagnet Fe$_4$GeTe$_2$, by using density functional theory combined with the non-equilibrium Green's functions method. We find that the conductance in the direction perpendicular to the layers is half-metallic, namely it is entirely spin-polarized, as a result of the material's electronic structure. This characteristic persists from bulk to single layer, even under significant bias voltages, and it is little affected by spin-orbit coupling and electron correlation. Motivated by this observation, we then investigate the tunnel magnetoresistance (TMR) effect in an magnetic tunnel junction, which comprises two Fe$_4$GeTe$_2$ layers separated by the vdW gap acting as insulating barrier. We predict a TMR ratio of almost 500\\%, which can be further boosted by increasing the number of Fe$_4$GeTe$_2$ layers in the junction.","sentences":["The recent emergence of van der Waals (vdW) ferromagnets has opened new opportunities for designing spintronic devices.","We theoretically investigate the coherent spin-dependent transport properties of the vdW ferromagnet Fe$_4$GeTe$_2$, by using density functional theory combined with the non-equilibrium Green's functions method.","We find that the conductance in the direction perpendicular to the layers is half-metallic, namely it is entirely spin-polarized, as a result of the material's electronic structure.","This characteristic persists from bulk to single layer, even under significant bias voltages, and it is little affected by spin-orbit coupling and electron correlation.","Motivated by this observation, we then investigate the tunnel magnetoresistance (TMR) effect in an magnetic tunnel junction, which comprises two Fe$_4$GeTe$_2$ layers separated by the vdW gap acting as insulating barrier.","We predict a TMR ratio of almost 500\\%, which can be further boosted by increasing the number of Fe$_4$GeTe$_2$ layers in the junction."],"url":"http://arxiv.org/abs/2403.10195v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 10:51:07","title":"Taiyi: A high-performance CKKS accelerator for Practical Fully Homomorphic Encryption","abstract":"Fully Homomorphic Encryption (FHE), a novel cryptographic theory enabling computation directly on ciphertext data, offers significant security benefits but is hampered by substantial performance overhead. In recent years, a series of accelerator designs have significantly enhanced the performance of FHE applications, bringing them closer to real-world applicability. However, these accelerators face challenges related to large on-chip memory and area. Additionally, FHE algorithms undergo rapid development, rendering the previous accelerator designs less perfectly adapted to the evolving landscape of optimized FHE applications. In this paper, we conducted a detailed analysis of existing applications with the new FHE method, making two key observations: 1) the bottleneck of FHE applications shifts from NTT to the inner-product operation, and 2) the optimal {\\alpha} of KeySwitch changes with the decrease in multiplicative level. Based on these observations, we designed an accelerator named Taiyi, which includes specific hardware for the inner-product operation and optimizes the NTT and BConv operations through algorithmic derivation. A comparative evaluation of Taiyi against previous state-of-the-art designs reveals an average performance improvement of 1.5x and reduces the area overhead by 15.7%.","sentences":["Fully Homomorphic Encryption (FHE), a novel cryptographic theory enabling computation directly on ciphertext data, offers significant security benefits but is hampered by substantial performance overhead.","In recent years, a series of accelerator designs have significantly enhanced the performance of FHE applications, bringing them closer to real-world applicability.","However, these accelerators face challenges related to large on-chip memory and area.","Additionally, FHE algorithms undergo rapid development, rendering the previous accelerator designs less perfectly adapted to the evolving landscape of optimized FHE applications.","In this paper, we conducted a detailed analysis of existing applications with the new FHE method, making two key observations: 1) the bottleneck of FHE applications shifts from NTT to the inner-product operation, and 2) the optimal {\\alpha} of KeySwitch changes with the decrease in multiplicative level.","Based on these observations, we designed an accelerator named Taiyi, which includes specific hardware for the inner-product operation and optimizes the NTT and BConv operations through algorithmic derivation.","A comparative evaluation of Taiyi against previous state-of-the-art designs reveals an average performance improvement of 1.5x and reduces the area overhead by 15.7%."],"url":"http://arxiv.org/abs/2403.10188v1","category":"cs.CR"}
{"created":"2024-03-15 10:38:48","title":"Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification","abstract":"In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the snapshot, batch, and multi-input multi-output ensemble. This study is the first to provide a comprehensive comparison of a single NN, a deep ensemble, and the three efficient NN ensembles. In addition, we propose a Diversity Quality metric to quantify the ensembles' performance on the in-distribution and OOD sets in one single metric. The OR case study discusses industrial parts classification to identify and manage spare parts, important for timely maintenance of industrial plants. The results highlight the batch ensemble as a cost-effective and competitive alternative to the deep ensemble. It outperforms the deep ensemble in both uncertainty and accuracy while exhibiting a training time speedup of 7x, a test time speedup of 8x, and 9x memory savings.","sentences":["In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution.","In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification.","However, NNs tend to make confident yet incorrect predictions when confronted with OOD data.","Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted.","Hence, reliable uncertainty quantification in NNs is crucial in the OR domain.","Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation.","However, their deployment is challenging due to substantial computational demands.","Recent fundamental research has proposed more efficient NN ensembles, namely the snapshot, batch, and multi-input multi-output ensemble.","This study is the first to provide a comprehensive comparison of a single NN, a deep ensemble, and the three efficient NN ensembles.","In addition, we propose a Diversity Quality metric to quantify the ensembles' performance on the in-distribution and OOD sets in one single metric.","The OR case study discusses industrial parts classification to identify and manage spare parts, important for timely maintenance of industrial plants.","The results highlight the batch ensemble as a cost-effective and competitive alternative to the deep ensemble.","It outperforms the deep ensemble in both uncertainty and accuracy while exhibiting a training time speedup of 7x, a test time speedup of 8x, and 9x memory savings."],"url":"http://arxiv.org/abs/2403.10182v1","category":"cs.LG"}
{"created":"2024-03-15 10:29:23","title":"Topology optimization of blazed gratings under conical incidence","abstract":"A topology optimization method is presented and applied to a blazed diffraction grating in reflection under conical incidence. This type of gratings is meant to disperse the incident light on one particular diffraction order and this property is fundamental in spectroscopy. Conventionally, a blazed metallic grating is made of a sawtooth profile designed to work with the +/-1st diffraction order in reflection. In this paper, we question this intuitive triangular pattern and look for optimal opto-geometric characteristics using topology optimization based on Finite Element modelling of Maxwell's equations. In practical contexts, the grating geometry is mono-periodic but it is enlightened by a 3D plane wave with a wavevector outside of the plane of invariance. Consequently, this study deals with the resolution of a direct and inverse problem using the Finite Element Method in this intermediate state between 2D and 3D: the so-called conical incidence. A multi-wavelength objective is used in order to obtain a broadband blazed effect. Finally, several numerical experiments are detailed. The results show that it is possible to reach a 98% diffraction efficiency on the -1st diffraction order if the optimization is performed on a single wavelength, and that the reflection integrated over the [400,1500]nm wavelength range can be 29% higher in absolute, 56% in relative, than that of the sawtooth blazed grating when using a multi-wavelength optimization criterion (from 52% to 81%).","sentences":["A topology optimization method is presented and applied to a blazed diffraction grating in reflection under conical incidence.","This type of gratings is meant to disperse the incident light on one particular diffraction order and this property is fundamental in spectroscopy.","Conventionally, a blazed metallic grating is made of a sawtooth profile designed to work with the +/-1st diffraction order in reflection.","In this paper, we question this intuitive triangular pattern and look for optimal opto-geometric characteristics using topology optimization based on Finite Element modelling of Maxwell's equations.","In practical contexts, the grating geometry is mono-periodic but it is enlightened by a 3D plane wave with a wavevector outside of the plane of invariance.","Consequently, this study deals with the resolution of a direct and inverse problem using the Finite Element Method in this intermediate state between 2D and 3D: the so-called conical incidence.","A multi-wavelength objective is used in order to obtain a broadband blazed effect.","Finally, several numerical experiments are detailed.","The results show that it is possible to reach a 98% diffraction efficiency on the -1st diffraction order if the optimization is performed on a single wavelength, and that the reflection integrated over the [400,1500]nm wavelength range can be 29% higher in absolute, 56% in relative, than that of the sawtooth blazed grating when using a multi-wavelength optimization criterion (from 52% to 81%)."],"url":"http://arxiv.org/abs/2403.10174v1","category":"physics.comp-ph"}
{"created":"2024-03-15 09:40:12","title":"On a globally convergent semismooth* Newton method in nonsmooth nonconvex optimzation","abstract":"In this paper we present GSSN, a globalized SCD semismooth* Newton method for solving nonsmooth nonconvex optimization problems. The global convergence properties of the method are ensured by the proximal gradient method, whereas locally superlinear convergence is established via the SCD semismooth* Newton method under quite weak assumptions. The Newton direction is based on the SC (subspace containing) derivative of the subdifferential mapping and can be computed by the (approximate) solution of an equality-constrained quadratic program. Special attention is given to the efficient numerical implementation of the overall method.","sentences":["In this paper we present GSSN, a globalized SCD semismooth* Newton method for solving nonsmooth nonconvex optimization problems.","The global convergence properties of the method are ensured by the proximal gradient method, whereas locally superlinear convergence is established via the SCD semismooth* Newton method under quite weak assumptions.","The Newton direction is based on the SC (subspace containing) derivative of the subdifferential mapping and can be computed by the (approximate) solution of an equality-constrained quadratic program.","Special attention is given to the efficient numerical implementation of the overall method."],"url":"http://arxiv.org/abs/2403.10142v1","category":"math.OC"}
{"created":"2024-03-15 09:38:52","title":"Anisotropic magneto-photothermal voltage in Sb2Te3 topological insulator thin films","abstract":"We studied longitudinal and Hall photothermal voltages under a planar magnetic field scan in epitaxial thin films of the Topological Insulator (TI) Sb2Te3, grown using pulsed laser deposition (PLD). Unlike prior research that utilised polarised light-induced photocurrent to investigate the TI, our study introduces advancements based on unpolarized light-induced local heating. This method yields a thermoelectric response exhibiting a direct signature of strong spin-orbit coupling. Our analysis reveals three distinct contributions when fitting the photothermal voltage data to the angular dependence of the planar magnetic field. The interaction between the applied magnetic field and the thermal gradient on the bulk band orbitals enables the differentiation between the ordinary Nernst effect from the out-of-plane thermal gradient and an extraordinary magneto-thermal contribution from the planar thermal gradient. The fitting of our data to theoretical models indicates that these effects primarily arise from the bulk states of the TI rather than the surface states. These findings highlight PLD-grown epitaxial topological insulator thin films as promising candidates for optoelectronic devices, including sensors and actuators. Such devices offer controllable responses through position-dependent, non-invasive local heating via focused incident light and variations in the applied magnetic field direction.","sentences":["We studied longitudinal and Hall photothermal voltages under a planar magnetic field scan in epitaxial thin films of the Topological Insulator (TI) Sb2Te3, grown using pulsed laser deposition (PLD).","Unlike prior research that utilised polarised light-induced photocurrent to investigate the TI, our study introduces advancements based on unpolarized light-induced local heating.","This method yields a thermoelectric response exhibiting a direct signature of strong spin-orbit coupling.","Our analysis reveals three distinct contributions when fitting the photothermal voltage data to the angular dependence of the planar magnetic field.","The interaction between the applied magnetic field and the thermal gradient on the bulk band orbitals enables the differentiation between the ordinary Nernst effect from the out-of-plane thermal gradient and an extraordinary magneto-thermal contribution from the planar thermal gradient.","The fitting of our data to theoretical models indicates that these effects primarily arise from the bulk states of the TI rather than the surface states.","These findings highlight PLD-grown epitaxial topological insulator thin films as promising candidates for optoelectronic devices, including sensors and actuators.","Such devices offer controllable responses through position-dependent, non-invasive local heating via focused incident light and variations in the applied magnetic field direction."],"url":"http://arxiv.org/abs/2403.10141v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 09:18:08","title":"Stochastic nanoswimmer: a multistate model for enzyme self-propulsion and enhanced diffusion","abstract":"Several enzymes show enhanced diffusion in the presence of substrate a feature that is explained by postulating that the enzyme in the bound state has a higher diffusion constant than in the unbound state. In a recent experiment [Jee et al. PNAS, 115, E10812 (2018)], it was observed that some of these enzymes perform a run-and-tumble motion, where the self-propulsion velocities can be of the order of mm/s. Theoretical models of nanoscale swimmers have not been able to explain the high self-propulsion speeds measured in experiments. Here we model the enzyme as a dimer with fluctuating mobility. We show that even a dimer can perform a run-and-tumble motion when the dimer switches between states of different mobility in the enzymatic cycle. Within a three-state enzymatic cycle, we investigate the conditions under which self-propulsion speeds consistent with experiments can be obtained.","sentences":["Several enzymes show enhanced diffusion in the presence of substrate a feature that is explained by postulating that the enzyme in the bound state has a higher diffusion constant than in the unbound state.","In a recent experiment [Jee et al. PNAS, 115, E10812 (2018)], it was observed that some of these enzymes perform a run-and-tumble motion, where the self-propulsion velocities can be of the order of mm/s. Theoretical models of nanoscale swimmers have not been able to explain the high self-propulsion speeds measured in experiments.","Here we model the enzyme as a dimer with fluctuating mobility.","We show that even a dimer can perform a run-and-tumble motion when the dimer switches between states of different mobility in the enzymatic cycle.","Within a three-state enzymatic cycle, we investigate the conditions under which self-propulsion speeds consistent with experiments can be obtained."],"url":"http://arxiv.org/abs/2403.10125v1","category":"cond-mat.soft"}
{"created":"2024-03-15 09:15:57","title":"Depth-induced Saliency Comparison Network for Diagnosis of Alzheimer's Disease via Jointly Analysis of Visual Stimuli and Eye Movements","abstract":"Early diagnosis of Alzheimer's Disease (AD) is very important for following medical treatments, and eye movements under special visual stimuli may serve as a potential non-invasive biomarker for detecting cognitive abnormalities of AD patients. In this paper, we propose an Depth-induced saliency comparison network (DISCN) for eye movement analysis, which may be used for diagnosis the Alzheimers disease. In DISCN, a salient attention module fuses normal eye movements with RGB and depth maps of visual stimuli using hierarchical salient attention (SAA) to evaluate comprehensive saliency maps, which contain information from both visual stimuli and normal eye movement behaviors. In addition, we introduce serial attention module (SEA) to emphasis the most abnormal eye movement behaviors to reduce personal bias for a more robust result. According to our experiments, the DISCN achieves consistent validity in classifying the eye movements between the AD patients and normal controls.","sentences":["Early diagnosis of Alzheimer's Disease (AD) is very important for following medical treatments, and eye movements under special visual stimuli may serve as a potential non-invasive biomarker for detecting cognitive abnormalities of AD patients.","In this paper, we propose an Depth-induced saliency comparison network (DISCN) for eye movement analysis, which may be used for diagnosis the Alzheimers disease.","In DISCN, a salient attention module fuses normal eye movements with RGB and depth maps of visual stimuli using hierarchical salient attention (SAA) to evaluate comprehensive saliency maps, which contain information from both visual stimuli and normal eye movement behaviors.","In addition, we introduce serial attention module (SEA) to emphasis the most abnormal eye movement behaviors to reduce personal bias for a more robust result.","According to our experiments, the DISCN achieves consistent validity in classifying the eye movements between the AD patients and normal controls."],"url":"http://arxiv.org/abs/2403.10124v1","category":"cs.CV"}
{"created":"2024-03-15 09:12:59","title":"Water-based Quantum Dots Liquid Scintillator for Particle Physics","abstract":"Liquid scintillators are typically composed from organic compounds dissolved in organic solvents. However, usage of such material is often restricted due to fire safety and environmental reasons. Because of this, R\\&D of water-based liquid scintillators is of extreme relevance; yet, no such scintillators have been made commercially available as yet. Here, we investigate an alternative, water-based quantum dots liquid scintillator. Pre-determined and controllable optical properties of the quantum dots, as well as the existence of large libraries of established protocols for their dispersion in aqueous solutions, make them an attractive option for nuclear and particle physics applications. We characterize the optical properties of water-based quantum dots liquid scintillator and find that most of its optical properties are preserved upon quantum dots' phase transfer into water, through the addition of an oleic acid hydrophilic layer. Using the developed scintillator, the time and charge responses from atmospheric muons are measured, highlighting the practical viability of water-based quantum dots liquid scintillators for nuclear and particle physics, special interest on neutrino physics.","sentences":["Liquid scintillators are typically composed from organic compounds dissolved in organic solvents.","However, usage of such material is often restricted due to fire safety and environmental reasons.","Because of this, R\\&D of water-based liquid scintillators is of extreme relevance; yet, no such scintillators have been made commercially available as yet.","Here, we investigate an alternative, water-based quantum dots liquid scintillator.","Pre-determined and controllable optical properties of the quantum dots, as well as the existence of large libraries of established protocols for their dispersion in aqueous solutions, make them an attractive option for nuclear and particle physics applications.","We characterize the optical properties of water-based quantum dots liquid scintillator and find that most of its optical properties are preserved upon quantum dots' phase transfer into water, through the addition of an oleic acid hydrophilic layer.","Using the developed scintillator, the time and charge responses from atmospheric muons are measured, highlighting the practical viability of water-based quantum dots liquid scintillators for nuclear and particle physics, special interest on neutrino physics."],"url":"http://arxiv.org/abs/2403.10122v1","category":"physics.ins-det"}
{"created":"2024-03-15 09:04:00","title":"Instance-optimal Clipping for Summation Problems in the Shuffle Model of Differential Privacy","abstract":"Differentially private mechanisms achieving worst-case optimal error bounds (e.g., the classical Laplace mechanism) are well-studied in the literature. However, when typical data are far from the worst case, \\emph{instance-specific} error bounds -- which depend on the largest value in the dataset -- are more meaningful. For example, consider the sum estimation problem, where each user has an integer $x_i$ from the domain $\\{0,1,\\dots,U\\}$ and we wish to estimate $\\sum_i x_i$. This has a worst-case optimal error of $O(U/\\varepsilon)$, while recent work has shown that the clipping mechanism can achieve an instance-optimal error of $O(\\max_i x_i \\cdot \\log\\log U /\\varepsilon)$. Under the shuffle model, known instance-optimal protocols are less communication-efficient. The clipping mechanism also works in the shuffle model, but requires two rounds: Round one finds the clipping threshold, and round two does the clipping and computes the noisy sum of the clipped data. In this paper, we show how these two seemingly sequential steps can be done simultaneously in one round using just $1+o(1)$ messages per user, while maintaining the instance-optimal error bound. We also extend our technique to the high-dimensional sum estimation problem and sparse vector aggregation (a.k.a. frequency estimation under user-level differential privacy). Our experiments show order-of-magnitude improvements of our protocols in terms of error compared with prior work.","sentences":["Differentially private mechanisms achieving worst-case optimal error bounds (e.g., the classical Laplace mechanism) are well-studied in the literature.","However, when typical data are far from the worst case, \\emph{instance-specific} error bounds -- which depend on the largest value in the dataset -- are more meaningful.","For example, consider the sum estimation problem, where each user has an integer $x_i$ from the domain $\\{0,1,\\dots,U\\}$","and we wish to estimate $\\sum_i x_i$.","This has a worst-case optimal error of $O(U/\\varepsilon)$, while recent work has shown that the clipping mechanism can achieve an instance-optimal error of $O(\\max_i x_i","\\cdot \\log\\log U /\\varepsilon)$.","Under the shuffle model, known instance-optimal protocols are less communication-efficient.","The clipping mechanism also works in the shuffle model, but requires two rounds: Round one finds the clipping threshold, and round two does the clipping and computes the noisy sum of the clipped data.","In this paper, we show how these two seemingly sequential steps can be done simultaneously in one round using just $1+o(1)$ messages per user, while maintaining the instance-optimal error bound.","We also extend our technique to the high-dimensional sum estimation problem and sparse vector aggregation (a.k.a. frequency estimation under user-level differential privacy).","Our experiments show order-of-magnitude improvements of our protocols in terms of error compared with prior work."],"url":"http://arxiv.org/abs/2403.10116v1","category":"cs.CR"}
{"created":"2024-03-15 08:55:44","title":"Long-time behavior for discretization schemes of Fokker-Planck equations via couplings","abstract":"Continuous-time Markov chains associated to finite-volume discretization schemes of Fokker-Planck equations are constructed. Sufficient conditions under which quantitative exponential decay in the $\\phi$-entropy and Wasserstein distance are established, implying modified logarithmic Sobolev, Poincar\\'e, and discrete Beckner inequalities. The results are not restricted to additive potentials and do not make use of discrete Bochner-type identities. The proof for the $\\phi$-decay relies on a coupling technique due to Conforti, while the proof for the Wasserstein distance uses the path coupling method. Furthermore, exponential equilibration for discrete-time Markov chains is proved, based on an abstract discrete Bakry-Emery method and a path coupling.","sentences":["Continuous-time Markov chains associated to finite-volume discretization schemes of Fokker-Planck equations are constructed.","Sufficient conditions under which quantitative exponential decay in the $\\phi$-entropy and Wasserstein distance are established, implying modified logarithmic Sobolev, Poincar\\'e, and discrete Beckner inequalities.","The results are not restricted to additive potentials and do not make use of discrete Bochner-type identities.","The proof for the $\\phi$-decay relies on a coupling technique due to Conforti, while the proof for the Wasserstein distance uses the path coupling method.","Furthermore, exponential equilibration for discrete-time Markov chains is proved, based on an abstract discrete Bakry-Emery method and a path coupling."],"url":"http://arxiv.org/abs/2403.10111v1","category":"math.PR"}
{"created":"2024-03-15 17:29:54","title":"Lifelong LERF: Local 3D Semantic Inventory Monitoring Using FogROS2","abstract":"Inventory monitoring in homes, factories, and retail stores relies on maintaining data despite objects being swapped, added, removed, or moved. We introduce Lifelong LERF, a method that allows a mobile robot with minimal compute to jointly optimize a dense language and geometric representation of its surroundings. Lifelong LERF maintains this representation over time by detecting semantic changes and selectively updating these regions of the environment, avoiding the need to exhaustively remap. Human users can query inventory by providing natural language queries and receiving a 3D heatmap of potential object locations. To manage the computational load, we use Fog-ROS2, a cloud robotics platform, to offload resource-intensive tasks. Lifelong LERF obtains poses from a monocular RGBD SLAM backend, and uses these poses to progressively optimize a Language Embedded Radiance Field (LERF) for semantic monitoring. Experiments with 3-5 objects arranged on a tabletop and a Turtlebot with a RealSense camera suggest that Lifelong LERF can persistently adapt to changes in objects with up to 91% accuracy.","sentences":["Inventory monitoring in homes, factories, and retail stores relies on maintaining data despite objects being swapped, added, removed, or moved.","We introduce Lifelong LERF, a method that allows a mobile robot with minimal compute to jointly optimize a dense language and geometric representation of its surroundings.","Lifelong LERF maintains this representation over time by detecting semantic changes and selectively updating these regions of the environment, avoiding the need to exhaustively remap.","Human users can query inventory by providing natural language queries and receiving a 3D heatmap of potential object locations.","To manage the computational load, we use Fog-ROS2, a cloud robotics platform, to offload resource-intensive tasks.","Lifelong LERF obtains poses from a monocular RGBD SLAM backend, and uses these poses to progressively optimize a Language Embedded Radiance Field (LERF) for semantic monitoring.","Experiments with 3-5 objects arranged on a tabletop and a Turtlebot with a RealSense camera suggest that Lifelong LERF can persistently adapt to changes in objects with up to 91% accuracy."],"url":"http://arxiv.org/abs/2403.10494v1","category":"cs.RO"}
{"created":"2024-03-15 17:12:33","title":"Lower Bounds for Kernel Density Estimation on Symmetric Spaces","abstract":"We prove that kernel density estimation on symmetric spaces of non-compact type, whose L2-risk was bounded above in previous work (Asta,2021), in fact achieves a minimax rate of convergence. With this result, the story for kernel density estimation on all symmetric spaces is completed. The idea in adapting the proof for Euclidean space is to suitably abstract vector space operations on Euclidean space to both actions of symmetric groups and reparametrizations of Helgason-Fourier transforms and to use the fact that the exponential map for symmetric spaces of non-compact type defines a diffeomorphism.","sentences":["We prove that kernel density estimation on symmetric spaces of non-compact type, whose L2-risk was bounded above in previous work (Asta,2021), in fact achieves a minimax rate of convergence.","With this result, the story for kernel density estimation on all symmetric spaces is completed.","The idea in adapting the proof for Euclidean space is to suitably abstract vector space operations on Euclidean space to both actions of symmetric groups and reparametrizations of Helgason-Fourier transforms and to use the fact that the exponential map for symmetric spaces of non-compact type defines a diffeomorphism."],"url":"http://arxiv.org/abs/2403.10480v1","category":"math.ST"}
{"created":"2024-03-15 16:32:15","title":"Non-reciprocal dynamics and non-Hermitian skin effect of repulsively bound pairs","abstract":"We study the dynamics of a Bose-Hubbard model coupled to an engineered environment which in the non-interacting limit is described by the celebrated Hatano-Nelson model. At strong interactions, two bosons occupying the same site form a so-called repulsively bound pair, or doublon. Using tensor-network simulations, we clearly identify a distinct doublon lightcone and show that the doublon inherits non-reciprocity from that of single particles. Applying the idea of reservoir engineering at the level of doublons, we introduce a new set of dissipators and we analytically show that then the doublon dynamics are governed by the Hatano-Nelson model. This brings about an interaction-induced non-Hermitian skin effect and non-reciprocal doublon motion. Combining features of the two models we study, we show that single particles and doublons can be made to spread with opposite directionality, opening intriguing possibilities for the study of dynamics in interacting non-reciprocal models.","sentences":["We study the dynamics of a Bose-Hubbard model coupled to an engineered environment which in the non-interacting limit is described by the celebrated Hatano-Nelson model.","At strong interactions, two bosons occupying the same site form a so-called repulsively bound pair, or doublon.","Using tensor-network simulations, we clearly identify a distinct doublon lightcone and show that the doublon inherits non-reciprocity from that of single particles.","Applying the idea of reservoir engineering at the level of doublons, we introduce a new set of dissipators and we analytically show that then the doublon dynamics are governed by the Hatano-Nelson model.","This brings about an interaction-induced non-Hermitian skin effect and non-reciprocal doublon motion.","Combining features of the two models we study, we show that single particles and doublons can be made to spread with opposite directionality, opening intriguing possibilities for the study of dynamics in interacting non-reciprocal models."],"url":"http://arxiv.org/abs/2403.10449v1","category":"quant-ph"}
{"created":"2024-03-15 14:39:39","title":"SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras","abstract":"The field of autonomous driving has attracted considerable interest in approaches that directly infer 3D objects in the Bird's Eye View (BEV) from multiple cameras. Some attempts have also explored utilizing 2D detectors from single images to enhance the performance of 3D detection. However, these approaches rely on a two-stage process with separate detectors, where the 2D detection results are utilized only once for token selection or query initialization. In this paper, we present a single model termed SimPB, which simultaneously detects 2D objects in the perspective view and 3D objects in the BEV space from multiple cameras. To achieve this, we introduce a hybrid decoder consisting of several multi-view 2D decoder layers and several 3D decoder layers, specifically designed for their respective detection tasks. A Dynamic Query Allocation module and an Adaptive Query Aggregation module are proposed to continuously update and refine the interaction between 2D and 3D results, in a cyclic 3D-2D-3D manner. Additionally, Query-group Attention is utilized to strengthen the interaction among 2D queries within each camera group. In the experiments, we evaluate our method on the nuScenes dataset and demonstrate promising results for both 2D and 3D detection tasks. Our code is available at: https://github.com/nullmax-vision/SimPB.","sentences":["The field of autonomous driving has attracted considerable interest in approaches that directly infer 3D objects in the Bird's Eye View (BEV) from multiple cameras.","Some attempts have also explored utilizing 2D detectors from single images to enhance the performance of 3D detection.","However, these approaches rely on a two-stage process with separate detectors, where the 2D detection results are utilized only once for token selection or query initialization.","In this paper, we present a single model termed SimPB, which simultaneously detects 2D objects in the perspective view and 3D objects in the BEV space from multiple cameras.","To achieve this, we introduce a hybrid decoder consisting of several multi-view 2D decoder layers and several 3D decoder layers, specifically designed for their respective detection tasks.","A Dynamic Query Allocation module and an Adaptive Query Aggregation module are proposed to continuously update and refine the interaction between 2D and 3D results, in a cyclic 3D-2D-3D manner.","Additionally, Query-group Attention is utilized to strengthen the interaction among 2D queries within each camera group.","In the experiments, we evaluate our method on the nuScenes dataset and demonstrate promising results for both 2D and 3D detection tasks.","Our code is available at: https://github.com/nullmax-vision/SimPB."],"url":"http://arxiv.org/abs/2403.10353v1","category":"cs.CV"}
{"created":"2024-03-15 13:56:14","title":"A canonical tree decomposition for order types, and some applications","abstract":"We introduce and study a notion of decomposition of planar point sets (or rather of their chirotopes) as trees decorated by smaller chirotopes. This decomposition is based on the concept of mutually avoiding sets (which we rephrase as \\emph{modules}), and adapts in some sense the modular decomposition of graphs in the world of chirotopes. The associated tree always exists and is unique up to some appropriate constraints. We also show how to compute the number of triangulations of a chirotope efficiently, starting from its tree and the (weighted) numbers of triangulations of its parts.","sentences":["We introduce and study a notion of decomposition of planar point sets (or rather of their chirotopes) as trees decorated by smaller chirotopes.","This decomposition is based on the concept of mutually avoiding sets (which we rephrase as \\emph{modules}), and adapts in some sense the modular decomposition of graphs in the world of chirotopes.","The associated tree always exists and is unique up to some appropriate constraints.","We also show how to compute the number of triangulations of a chirotope efficiently, starting from its tree and the (weighted) numbers of triangulations of its parts."],"url":"http://arxiv.org/abs/2403.10311v1","category":"cs.CG"}
{"created":"2024-03-15 11:05:03","title":"Proton Beam Based Production of Positron Emitters by Exploiting the 27Al(p,x)22Na Reaction","abstract":"Positron annihilation experiments on an laboratory scale depend on the supply and the availability of $\\beta^+$ emitters. Here we present the production of positron sources based on the $^{27}$Al(p,x)$^{22}$Na reaction by irradiation of Al with a 68\\,MeV proton beam. We simulated the energy loss, range and radial scattering of the protons in Al in order to design a simple target consisting of a stack of Al discs. Our approach allows (i) the direct use of the Al discs as positron emitters that inherently avoids wet chemical processes as usually applied in commercial production of carrier-free $^{22}$Na, (ii) the production of multiple positron sources at once, and (iii) the simple measurement of the depth and lateral distribution of $^{22}$Na. We precisely determined the cross section of the $^{27}$Al(p,x)$^{22}$Na reaction which was found to differ from literature values particularly for proton energies between 27 and 40\\,MeV. The activity of all nuclides produced (apart from $^{22}$Na) was shown to be negligible 15 days after irradiation. The production of radionuclides such as $^{48}$Sc, $^{54}$Mn and $^{56}$Co can be prevented by using Al of a higher purity. The concept presented here can easily be adapted for the production of stronger $^{22}$Na sources by increasing the proton current or/and the irradiation time.","sentences":["Positron annihilation experiments on an laboratory scale depend on the supply and the availability of $\\beta^+$ emitters.","Here we present the production of positron sources based on the $^{27}$Al(p,x)$^{22}$Na reaction by irradiation of Al with a 68\\,MeV proton beam.","We simulated the energy loss, range and radial scattering of the protons in Al in order to design a simple target consisting of a stack of Al discs.","Our approach allows (i) the direct use of the Al discs as positron emitters that inherently avoids wet chemical processes as usually applied in commercial production of carrier-free $^{22}$Na, (ii) the production of multiple positron sources at once, and (iii) the simple measurement of the depth and lateral distribution of $^{22}$Na.","We precisely determined the cross section of the $^{27}$Al(p,x)$^{22}$Na reaction which was found to differ from literature values particularly for proton energies between 27 and 40\\,MeV. The activity of all nuclides produced (apart from $^{22}$Na) was shown to be negligible 15 days after irradiation.","The production of radionuclides such as $^{48}$Sc, $^{54}$Mn and $^{56}$Co can be prevented by using Al of a higher purity.","The concept presented here can easily be adapted for the production of stronger $^{22}$Na sources by increasing the proton current or/and the irradiation time."],"url":"http://arxiv.org/abs/2403.10200v1","category":"nucl-ex"}
{"created":"2024-03-15 09:18:53","title":"TransLandSeg: A Transfer Learning Approach for Landslide Semantic Segmentation Based on Vision Foundation Model","abstract":"Landslides are one of the most destructive natural disasters in the world, posing a serious threat to human life and safety. The development of foundation models has provided a new research paradigm for large-scale landslide detection. The Segment Anything Model (SAM) has garnered widespread attention in the field of image segmentation. However, our experiment found that SAM performed poorly in the task of landslide segmentation. We propose TransLandSeg, which is a transfer learning approach for landslide semantic segmentation based on a vision foundation model (VFM). TransLandSeg outperforms traditional semantic segmentation models on both the Landslide4Sense dataset and the Bijie landslide dataset. Our proposed adaptive transfer learning (ATL) architecture enables the powerful segmentation capability of SAM to be transferred to landslide detection by training only 1.3% of the number of the parameters of SAM, which greatly improves the training efficiency of the model. Finally we also conducted ablation experiments on models with different ATL structures, concluded that the deployment location and residual connection of ATL play an important role in TransLandSeg accuracy improvement.","sentences":["Landslides are one of the most destructive natural disasters in the world, posing a serious threat to human life and safety.","The development of foundation models has provided a new research paradigm for large-scale landslide detection.","The Segment Anything Model (SAM) has garnered widespread attention in the field of image segmentation.","However, our experiment found that SAM performed poorly in the task of landslide segmentation.","We propose TransLandSeg, which is a transfer learning approach for landslide semantic segmentation based on a vision foundation model (VFM).","TransLandSeg outperforms traditional semantic segmentation models on both the Landslide4Sense dataset and the Bijie landslide dataset.","Our proposed adaptive transfer learning (ATL) architecture enables the powerful segmentation capability of SAM to be transferred to landslide detection by training only 1.3% of the number of the parameters of SAM, which greatly improves the training efficiency of the model.","Finally we also conducted ablation experiments on models with different ATL structures, concluded that the deployment location and residual connection of ATL play an important role in TransLandSeg accuracy improvement."],"url":"http://arxiv.org/abs/2403.10127v1","category":"cs.CV"}
{"created":"2024-03-15 08:23:34","title":"A Belief Propagation Algorithm for Multipath-based SLAM with Multiple Map Features: A mmWave MIMO Application","abstract":"In this paper, we present a multipath-based simultaneous localization and mapping (SLAM) algorithm that continuously adapts mulitiple map feature (MF) models describing specularly reflected multipath components (MPCs) from flat surfaces and point-scattered MPCs, respectively. We develop a Bayesian model for sequential detection and estimation of interacting MF model parameters, MF states and mobile agent's state including position and orientation. The Bayesian model is represented by a factor graph enabling the use of belief propagation (BP) for efficient computation of the marginal posterior distributions. The algorithm also exploits amplitude information enabling reliable detection of weak MFs associated with MPCs of very low signal-to-noise ratios (SNRs). The performance of the proposed algorithm is evaluated using real millimeter-wave (mmWave) multiple-input-multiple-output (MIMO) measurements with single base station setup. Results demonstrate the excellent localization and mapping performance of the proposed algorithm in challenging dynamic outdoor scenarios.","sentences":["In this paper, we present a multipath-based simultaneous localization and mapping (SLAM) algorithm that continuously adapts mulitiple map feature (MF) models describing specularly reflected multipath components (MPCs) from flat surfaces and point-scattered MPCs, respectively.","We develop a Bayesian model for sequential detection and estimation of interacting MF model parameters, MF states and mobile agent's state including position and orientation.","The Bayesian model is represented by a factor graph enabling the use of belief propagation (BP) for efficient computation of the marginal posterior distributions.","The algorithm also exploits amplitude information enabling reliable detection of weak MFs associated with MPCs of very low signal-to-noise ratios (SNRs).","The performance of the proposed algorithm is evaluated using real millimeter-wave (mmWave) multiple-input-multiple-output (MIMO) measurements with single base station setup.","Results demonstrate the excellent localization and mapping performance of the proposed algorithm in challenging dynamic outdoor scenarios."],"url":"http://arxiv.org/abs/2403.10095v1","category":"eess.SP"}
{"created":"2024-03-15 17:07:32","title":"The role of heating on the formation and the dynamics of YSO jets : I. A parametric study","abstract":"Theoretical arguments as well as observations of young stellar objects (YSO) support the presence of a diversified circumstellar environment. A stellar jet is thought to account for most of the stellar spin down and disk wind outflow for the observed high mass loss rate, thus playing a major role in the launching of powerful jets. RY Tau, for instance, is an extensively studied intermediate mass pre-main sequence star. Observational data reveal a small scale jet called microjet. Nevertheless, it is not clear how the microjet shapes the jet observed at a large scale. The goal is to investigate the spatial stability and structure of the central jet at a large scale by mixing the stellar and disk components. We mix two existing analytical self-similar models for the disk and the stellar winds to build the initial set-ups. Instead of using a polytropic equation of state, we map from the analytical solutions, the heating and cooling sources. The heating exchange rate is controlled by two parameters, its spatial extent and its intensity. The central jet and the surrounding disk are strongly affected by these two parameters. We separate the results in three categories, which show different emissivity, temperature, and velocity maps. We reached this categorization by looking at the opening angle of the stellar solution. For cylindrically, well collimated jets, we have opening angles as low as 10 degrees between 8 and 10 au, and for the wider jets, we can reach 30 degrees with a morphology closer to radial solar winds. Our parametric study shows that the less heated the outflow is, the more collimated it appears. We also show that recollimation shocks appear consistently with UV observations in terms of temperature but not density.","sentences":["Theoretical arguments as well as observations of young stellar objects (YSO) support the presence of a diversified circumstellar environment.","A stellar jet is thought to account for most of the stellar spin down and disk wind outflow for the observed high mass loss rate, thus playing a major role in the launching of powerful jets.","RY Tau, for instance, is an extensively studied intermediate mass pre-main sequence star.","Observational data reveal a small scale jet called microjet.","Nevertheless, it is not clear how the microjet shapes the jet observed at a large scale.","The goal is to investigate the spatial stability and structure of the central jet at a large scale by mixing the stellar and disk components.","We mix two existing analytical self-similar models for the disk and the stellar winds to build the initial set-ups.","Instead of using a polytropic equation of state, we map from the analytical solutions, the heating and cooling sources.","The heating exchange rate is controlled by two parameters, its spatial extent and its intensity.","The central jet and the surrounding disk are strongly affected by these two parameters.","We separate the results in three categories, which show different emissivity, temperature, and velocity maps.","We reached this categorization by looking at the opening angle of the stellar solution.","For cylindrically, well collimated jets, we have opening angles as low as 10 degrees between 8 and 10 au, and for the wider jets, we can reach 30 degrees with a morphology closer to radial solar winds.","Our parametric study shows that the less heated the outflow is, the more collimated it appears.","We also show that recollimation shocks appear consistently with UV observations in terms of temperature but not density."],"url":"http://arxiv.org/abs/2403.10475v1","category":"astro-ph.HE"}
{"created":"2024-03-15 16:49:31","title":"New functional inequalities with applications to the arctan-fast diffusion equation","abstract":"In this paper, we prove a couple of new nonlinear functional inequalities of Sobolev type akin to the logarithmic Sobolev inequality. In particular, one of the inequalities reads $$ \\int_{\\mathbb{S}^1}\\arctan\\left(\\frac{\\partial_x u}{u}\\right)\\partial_xu \\,dx\\geq \\arctan\\left(\\|u(t)\\|_{\\dot{W}^{1,1}(\\mathbb{S}^1)}\\right)\\|u(t)\\|_{\\dot{W}^{1,1}(\\mathbb{S}^1)}. $$ Then, these inequalities are used in the study of the nonlinear \\emph{arctan}-fast diffusion equation $$ \\partial_t u-\\partial_x\\arctan\\left(\\frac{\\partial_x u}{u}\\right)=0. $$ For this highly nonlinear PDE we establish a number of well-posedness results and qualitative properties.","sentences":["In this paper, we prove a couple of new nonlinear functional inequalities of Sobolev type akin to the logarithmic Sobolev inequality.","In particular, one of the inequalities reads $$ \\int_{\\mathbb{S}^1}\\arctan\\left(\\frac{\\partial_x u}{u}\\right)\\partial_xu \\,dx\\geq \\arctan\\left(\\|u(t)\\|_{\\dot{W}^{1,1}(\\mathbb{S}^1)}\\right)\\|u(t)\\|_{\\dot{W}^{1,1}(\\mathbb{S}^1)}.","$$ Then, these inequalities are used in the study of the nonlinear \\emph{arctan}-fast diffusion equation $$ \\partial_t u-\\partial_x\\arctan\\left(\\frac{\\partial_x u}{u}\\right)=0.","$$ For this highly nonlinear PDE we establish a number of well-posedness results and qualitative properties."],"url":"http://arxiv.org/abs/2403.10458v1","category":"math.AP"}
{"created":"2024-03-15 15:54:45","title":"Versatile Capillary Cells for Handling Concentrated Samples in Analytical Ultracentrifugation","abstract":"In concentrated macromolecular dispersions, far-from-ideal intermolecular interactions determine the dispersion behaviors including phase transition, crystallization, and liquid-liquid phase separation. Here, we present a novel versatile capillary-cell design for analytical ultracentrifugation-sedimentation equilibrium (AUC-SE), ideal for studying samples at high concentrations. Current setups for such studies are difficult and unreliable to handle, leading to a low experimental success rate. The design presented here is easy to use, robust, and reusable for samples in both aqueous and organic solvents while requiring no special tools or chemical modification of AUC cells. The key and unique feature is the fabrication of liquid reservoirs directly on the bottom window of AUC cells, which can be easily realized by laser ablation or mechanical drilling. The channel length and optical path length are therefore tunable. The success rate for assembling this new cell is close to 100%. We demonstrate the practicality of this cell by studying: 1) the equation of state and second virial coefficients of concentrated gold nanoparticle dispersions in water and bovine serum albumin (BSA) as well as lysozyme solution in aqueous buffers, 2) the gelation phase transition of DNA and BSA solutions, and 3) liquid-liquid phase separation of concentrated BSA/polyethylene glycol (PEG) droplets.","sentences":["In concentrated macromolecular dispersions, far-from-ideal intermolecular interactions determine the dispersion behaviors including phase transition, crystallization, and liquid-liquid phase separation.","Here, we present a novel versatile capillary-cell design for analytical ultracentrifugation-sedimentation equilibrium (AUC-SE), ideal for studying samples at high concentrations.","Current setups for such studies are difficult and unreliable to handle, leading to a low experimental success rate.","The design presented here is easy to use, robust, and reusable for samples in both aqueous and organic solvents while requiring no special tools or chemical modification of AUC cells.","The key and unique feature is the fabrication of liquid reservoirs directly on the bottom window of AUC cells, which can be easily realized by laser ablation or mechanical drilling.","The channel length and optical path length are therefore tunable.","The success rate for assembling this new cell is close to 100%.","We demonstrate the practicality of this cell by studying: 1) the equation of state and second virial coefficients of concentrated gold nanoparticle dispersions in water and bovine serum albumin (BSA) as well as lysozyme solution in aqueous buffers, 2) the gelation phase transition of DNA and BSA solutions, and 3) liquid-liquid phase separation of concentrated BSA/polyethylene glycol (PEG) droplets."],"url":"http://arxiv.org/abs/2403.10418v1","category":"cond-mat.soft"}
{"created":"2024-03-15 15:47:54","title":"Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search","abstract":"Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attentions due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require lots of trials by human experts. In this paper, we address the challenge of integrating multi-head self-attention into high resolution representation CNNs efficiently, by leveraging architecture search. Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features, but also finds the proper location for placing multi-head self-attention module. Our search algorithm is optimized towards multiple objective s (e.g., latency and mIoU) and capable of finding architectures on Pareto frontier with arbitrary number of branches in a single search. We further present a series of model via Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searched for the best hybrid combination of light-weight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuse to high resolution for both efficiency and effectiveness. Extensive experiments demonstrate that HyCTAS outperforms previous methods on semantic segmentation task. Code and models are available at \\url{https://github.com/MarvinYu1995/HyCTAS}.","sentences":["Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attentions due to its vast applications in image understanding and autonomous driving.","However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require lots of trials by human experts.","In this paper, we address the challenge of integrating multi-head self-attention into high resolution representation CNNs efficiently, by leveraging architecture search.","Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory to maintain high resolution.","By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features, but also finds the proper location for placing multi-head self-attention module.","Our search algorithm is optimized towards multiple objective s (e.g., latency and mIoU) and capable of finding architectures on Pareto frontier with arbitrary number of branches in a single search.","We further present a series of model via Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searched for the best hybrid combination of light-weight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuse to high resolution for both efficiency and effectiveness.","Extensive experiments demonstrate that HyCTAS outperforms previous methods on semantic segmentation task.","Code and models are available at \\url{https://github.com/MarvinYu1995/HyCTAS}."],"url":"http://arxiv.org/abs/2403.10413v1","category":"cs.CV"}
{"created":"2024-03-15 15:36:14","title":"Modeling the Spread of COVID-19 in University Communities","abstract":"Mathematical and simulation models are often used to predict the spread of a disease and estimate the impact of public health interventions, and many such models have been developed and used during the COVID-19 pandemic. This paper describes a study that systematically compared models for a university community, which has a much smaller but more connected population than a state or nation. We developed a stochastic agent-based model, a deterministic compartment model, and a model based on ordinary differential equations. All three models represented the disease progression with the same susceptible-exposed-infectious-recovered (SEIR) model. We created a baseline scenario for a population of 14,000 students and faculty and eleven other scenarios for combinations of interventions such as regular testing, contact tracing, quarantine, isolation, moving courses online, mask wearing, improving ventilation, and vaccination. We used parameter values from other epidemiological studies and incorporated data about COVID-19 testing in College Park, Maryland, but the study was designed to compare modeling approaches to each other using a synthetic population. For each scenario we used the models to estimate the number of persons who become infected over a semester of 119 days. We evaluated the models by comparing their predictions and evaluating their parsimony and computational effort. The agent-based model (ABM) and the deterministic compartment model (DCM) had similar results with cyclic flow of persons to and from quarantine, but the model based on ordinary differential equations failed to capture these dynamics. The ABM's computation time was much greater than the other two models' computation time. The DCM captured some of the dynamics that were present in the ABM's predictions and, like those from the ABM, clearly showed the importance of testing and moving classes on-line.","sentences":["Mathematical and simulation models are often used to predict the spread of a disease and estimate the impact of public health interventions, and many such models have been developed and used during the COVID-19 pandemic.","This paper describes a study that systematically compared models for a university community, which has a much smaller but more connected population than a state or nation.","We developed a stochastic agent-based model, a deterministic compartment model, and a model based on ordinary differential equations.","All three models represented the disease progression with the same susceptible-exposed-infectious-recovered (SEIR) model.","We created a baseline scenario for a population of 14,000 students and faculty and eleven other scenarios for combinations of interventions such as regular testing, contact tracing, quarantine, isolation, moving courses online, mask wearing, improving ventilation, and vaccination.","We used parameter values from other epidemiological studies and incorporated data about COVID-19 testing in College Park, Maryland, but the study was designed to compare modeling approaches to each other using a synthetic population.","For each scenario we used the models to estimate the number of persons who become infected over a semester of 119 days.","We evaluated the models by comparing their predictions and evaluating their parsimony and computational effort.","The agent-based model (ABM) and the deterministic compartment model (DCM) had similar results with cyclic flow of persons to and from quarantine, but the model based on ordinary differential equations failed to capture these dynamics.","The ABM's computation time was much greater than the other two models' computation time.","The DCM captured some of the dynamics that were present in the ABM's predictions and, like those from the ABM, clearly showed the importance of testing and moving classes on-line."],"url":"http://arxiv.org/abs/2403.10402v1","category":"q-bio.PE"}
{"created":"2024-03-15 14:31:21","title":"Bi-Lagrangian structures and the space of rays","abstract":"This paper focuses on local curvature invariants associated with bi-Lagrangian structures. We establish several geometric conditions that determine when the canonical connection is flat, building on our previous findings regarding divergence-free webs. Addressing questions raised by Tabachnikov, we provide complete solutions to two problems: the existence of flat bi-Lagrangian structures within the space of rays induced by a pair of hypersurfaces, and the existence of flat bi-Lagrangian structures induced by tangents to Lagrangian curves in the symplectic plane.","sentences":["This paper focuses on local curvature invariants associated with bi-Lagrangian structures.","We establish several geometric conditions that determine when the canonical connection is flat, building on our previous findings regarding divergence-free webs.","Addressing questions raised by Tabachnikov, we provide complete solutions to two problems: the existence of flat bi-Lagrangian structures within the space of rays induced by a pair of hypersurfaces, and the existence of flat bi-Lagrangian structures induced by tangents to Lagrangian curves in the symplectic plane."],"url":"http://arxiv.org/abs/2403.10345v1","category":"math.DG"}
{"created":"2024-03-15 14:31:17","title":"SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution","abstract":"Neural implicit surface representation methods have recently shown impressive 3D reconstruction results. However, existing solutions struggle to reconstruct urban outdoor scenes due to their large, unbounded, and highly detailed nature. Hence, to achieve accurate reconstructions, additional supervision data such as LiDAR, strong geometric priors, and long training times are required. To tackle such issues, we present SCILLA, a new hybrid implicit surface learning method to reconstruct large driving scenes from 2D images. SCILLA's hybrid architecture models two separate implicit fields: one for the volumetric density and another for the signed distance to the surface. To accurately represent urban outdoor scenarios, we introduce a novel volume-rendering strategy that relies on self-supervised probabilistic density estimation to sample points near the surface and transition progressively from volumetric to surface representation. Our solution permits a proper and fast initialization of the signed distance field without relying on any geometric prior on the scene, compared to concurrent methods. By conducting extensive experiments on four outdoor driving datasets, we show that SCILLA can learn an accurate and detailed 3D surface scene representation in various urban scenarios while being two times faster to train compared to previous state-of-the-art solutions.","sentences":["Neural implicit surface representation methods have recently shown impressive 3D reconstruction results.","However, existing solutions struggle to reconstruct urban outdoor scenes due to their large, unbounded, and highly detailed nature.","Hence, to achieve accurate reconstructions, additional supervision data such as LiDAR, strong geometric priors, and long training times are required.","To tackle such issues, we present SCILLA, a new hybrid implicit surface learning method to reconstruct large driving scenes from 2D images.","SCILLA's hybrid architecture models two separate implicit fields: one for the volumetric density and another for the signed distance to the surface.","To accurately represent urban outdoor scenarios, we introduce a novel volume-rendering strategy that relies on self-supervised probabilistic density estimation to sample points near the surface and transition progressively from volumetric to surface representation.","Our solution permits a proper and fast initialization of the signed distance field without relying on any geometric prior on the scene, compared to concurrent methods.","By conducting extensive experiments on four outdoor driving datasets, we show that SCILLA can learn an accurate and detailed 3D surface scene representation in various urban scenarios while being two times faster to train compared to previous state-of-the-art solutions."],"url":"http://arxiv.org/abs/2403.10344v1","category":"cs.CV"}
{"created":"2024-03-15 14:23:06","title":"NECA: Neural Customizable Human Avatar","abstract":"Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at https://github.com/iSEE-Laboratory/NECA.","sentences":["Human avatar has become a novel type of 3D asset with various applications.","Ideally, a human avatar should be fully customizable to accommodate different settings and environments.","In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture.","The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering.","Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting.","The code is available at https://github.com/iSEE-Laboratory/NECA."],"url":"http://arxiv.org/abs/2403.10335v1","category":"cs.CV"}
{"created":"2024-03-15 14:13:45","title":"Multifractal scaling and the Euler equations on R^3/Z^3","abstract":"We study the Euler equations describing the motion of an incompressible fluid on the cubic torus with real initial data. We construct solutions on the Fourier side which display a sudden loss of regularity within finite time even for highly regular initial data. Moreover, the solution may regain its initial regularity within finite time. This loss of regularity may coincide with the appearance of a certain type of multifractal scaling of the solutions.","sentences":["We study the Euler equations describing the motion of an incompressible fluid on the cubic torus with real initial data.","We construct solutions on the Fourier side which display a sudden loss of regularity within finite time even for highly regular initial data.","Moreover, the solution may regain its initial regularity within finite time.","This loss of regularity may coincide with the appearance of a certain type of multifractal scaling of the solutions."],"url":"http://arxiv.org/abs/2403.10324v1","category":"math.AP"}
{"created":"2024-03-15 14:09:46","title":"Anytime Neural Architecture Search on Tabular Data","abstract":"The increasing demand for tabular data analysis calls for transitioning from manual architecture design to Neural Architecture Search (NAS). This transition demands an efficient and responsive anytime NAS approach that is capable of returning current optimal architectures within any given time budget while progressively enhancing architecture quality with increased budget allocation. However, the area of research on Anytime NAS for tabular data remains unexplored. To this end, we introduce ATLAS, the first anytime NAS approach tailored for tabular data. ATLAS introduces a novel two-phase filtering-and-refinement optimization scheme with joint optimization, combining the strengths of both paradigms of training-free and training-based architecture evaluation. Specifically, in the filtering phase, ATLAS employs a new zero-cost proxy specifically designed for tabular data to efficiently estimate the performance of candidate architectures, thereby obtaining a set of promising architectures. Subsequently, in the refinement phase, ATLAS leverages a fixed-budget search algorithm to schedule the training of the promising candidates, so as to accurately identify the optimal architecture. To jointly optimize the two phases for anytime NAS, we also devise a budget-aware coordinator that delivers high NAS performance within constraints. Experimental evaluations demonstrate that our ATLAS can obtain a good-performing architecture within any predefined time budget and return better architectures as and when a new time budget is made available. Overall, it reduces the search time on tabular data by up to 82.75x compared to existing NAS approaches.","sentences":["The increasing demand for tabular data analysis calls for transitioning from manual architecture design to Neural Architecture Search (NAS).","This transition demands an efficient and responsive anytime NAS approach that is capable of returning current optimal architectures within any given time budget while progressively enhancing architecture quality with increased budget allocation.","However, the area of research on Anytime NAS for tabular data remains unexplored.","To this end, we introduce ATLAS, the first anytime NAS approach tailored for tabular data.","ATLAS introduces a novel two-phase filtering-and-refinement optimization scheme with joint optimization, combining the strengths of both paradigms of training-free and training-based architecture evaluation.","Specifically, in the filtering phase, ATLAS employs a new zero-cost proxy specifically designed for tabular data to efficiently estimate the performance of candidate architectures, thereby obtaining a set of promising architectures.","Subsequently, in the refinement phase, ATLAS leverages a fixed-budget search algorithm to schedule the training of the promising candidates, so as to accurately identify the optimal architecture.","To jointly optimize the two phases for anytime NAS, we also devise a budget-aware coordinator that delivers high NAS performance within constraints.","Experimental evaluations demonstrate that our ATLAS can obtain a good-performing architecture within any predefined time budget and return better architectures as and when a new time budget is made available.","Overall, it reduces the search time on tabular data by up to 82.75x compared to existing NAS approaches."],"url":"http://arxiv.org/abs/2403.10318v1","category":"cs.LG"}
{"created":"2024-03-15 13:47:44","title":"Chernoff Information as a Privacy Constraint for Adversarial Classification","abstract":"This work studies a privacy metric based on Chernoff information, \\textit{Chernoff differential privacy}, due to its significance in characterization of classifier performance. Adversarial classification, as any other classification problem is built around minimization of the (average or correct detection) probability of error in deciding on either of the classes in the case of binary classification. Unlike the classical hypothesis testing problem, where the false alarm and mis-detection probabilities are handled separately resulting in an asymmetric behavior of the best error exponent, in this work, we focus on the Bayesian setting and characterize the relationship between the best error exponent of the average error probability and $\\varepsilon-$differential privacy. Accordingly, we re-derive Chernoff differential privacy in terms of $\\varepsilon-$differential privacy using the Radon-Nikodym derivative and show that it satisfies the composition property. Subsequently, we present numerical evaluation results, which demonstrates that Chernoff information outperforms Kullback-Leibler divergence as a function of the privacy parameter $\\varepsilon$, the impact of the adversary's attack and global sensitivity for the problem of adversarial classification in Laplace mechanisms.","sentences":["This work studies a privacy metric based on Chernoff information, \\textit{Chernoff differential privacy}, due to its significance in characterization of classifier performance.","Adversarial classification, as any other classification problem is built around minimization of the (average or correct detection) probability of error in deciding on either of the classes in the case of binary classification.","Unlike the classical hypothesis testing problem, where the false alarm and mis-detection probabilities are handled separately resulting in an asymmetric behavior of the best error exponent, in this work, we focus on the Bayesian setting and characterize the relationship between the best error exponent of the average error probability and $\\varepsilon-$differential privacy.","Accordingly, we re-derive Chernoff differential privacy in terms of $\\varepsilon-$differential privacy using the Radon-Nikodym derivative and show that it satisfies the composition property.","Subsequently, we present numerical evaluation results, which demonstrates that Chernoff information outperforms Kullback-Leibler divergence as a function of the privacy parameter $\\varepsilon$, the impact of the adversary's attack and global sensitivity for the problem of adversarial classification in Laplace mechanisms."],"url":"http://arxiv.org/abs/2403.10307v1","category":"cs.IT"}
{"created":"2024-03-15 13:06:15","title":"Topological Noetherianity of the infinite half-spin representations","abstract":"We prove that the infinite half-spin representations are topologically Noetherian with respect to the infinite spin group. As a consequence we obtain that half-spin varieties, which we introduce, are defined by the pullback of equations at a finite level. The main example for such varieties is the infinite isotropic Grassmannian in its spinor embedding, for which we explicitly determine its defining equations.","sentences":["We prove that the infinite half-spin representations are topologically Noetherian with respect to the infinite spin group.","As a consequence we obtain that half-spin varieties, which we introduce, are defined by the pullback of equations at a finite level.","The main example for such varieties is the infinite isotropic Grassmannian in its spinor embedding, for which we explicitly determine its defining equations."],"url":"http://arxiv.org/abs/2403.10274v1","category":"math.AG"}
{"created":"2024-03-15 12:52:27","title":"Moduli spaces of quadratic differentials: Abel-Jacobi map and deformation","abstract":"We study the moduli space of quadratic differentials with prescribed singularities parameterized by a decorated marked surface with punctures (DMSp), where simple zeros, double poles and higher order poles respectively correspond to decorations, punctures and boundary components. We show that the fundamental group of this space equals the kernel of the Abel-Jacobi map from the mapping class group of DMSp to the first homology group of the marked surface (without decorations/punctures). Moreover, a universal cover of this space is given by the space of stability conditions on the associated 3-Calabi-Yau category.   Furthermore, when we partial compactify (and orbifold) the moduli space by allowing the collision of simple zeros and some of the double poles, the resulting moduli space is isomorphic to a quotient of the space of stability conditions on the deformed (with respect to those collidable double poles) 3-Calabi-Yau category.","sentences":["We study the moduli space of quadratic differentials with prescribed singularities parameterized by a decorated marked surface with punctures (DMSp), where simple zeros, double poles and higher order poles respectively correspond to decorations, punctures and boundary components.","We show that the fundamental group of this space equals the kernel of the Abel-Jacobi map from the mapping class group of DMSp to the first homology group of the marked surface (without decorations/punctures).","Moreover, a universal cover of this space is given by the space of stability conditions on the associated 3-Calabi-Yau category.   ","Furthermore, when we partial compactify (and orbifold) the moduli space by allowing the collision of simple zeros and some of the double poles, the resulting moduli space is isomorphic to a quotient of the space of stability conditions on the deformed (with respect to those collidable double poles) 3-Calabi-Yau category."],"url":"http://arxiv.org/abs/2403.10265v1","category":"math.GT"}
{"created":"2024-03-15 12:00:37","title":"Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks","abstract":"Conventional matrix completion methods approximate the missing values by assuming the matrix to be low-rank, which leads to a linear approximation of missing values. It has been shown that enhanced performance could be attained by using nonlinear estimators such as deep neural networks. Deep fully connected neural networks (FCNNs), one of the most suitable architectures for matrix completion, suffer from over-fitting due to their high capacity, which leads to low generalizability. In this paper, we control over-fitting by regularizing the FCNN model in terms of the $\\ell_{1}$ norm of intermediate representations and nuclear norm of weight matrices. As such, the resulting regularized objective function becomes nonsmooth and nonconvex, i.e., existing gradient-based methods cannot be applied to our model. We propose a variant of the proximal gradient method and investigate its convergence to a critical point. In the initial epochs of FCNN training, the regularization terms are ignored, and through epochs, the effect of that increases. The gradual addition of nonsmooth regularization terms is the main reason for the better performance of the deep neural network with nonsmooth regularization terms (DNN-NSR) algorithm. Our simulations indicate the superiority of the proposed algorithm in comparison with existing linear and nonlinear algorithms.","sentences":["Conventional matrix completion methods approximate the missing values by assuming the matrix to be low-rank, which leads to a linear approximation of missing values.","It has been shown that enhanced performance could be attained by using nonlinear estimators such as deep neural networks.","Deep fully connected neural networks (FCNNs), one of the most suitable architectures for matrix completion, suffer from over-fitting due to their high capacity, which leads to low generalizability.","In this paper, we control over-fitting by regularizing the FCNN model in terms of the $\\ell_{1}$ norm of intermediate representations and nuclear norm of weight matrices.","As such, the resulting regularized objective function becomes nonsmooth and nonconvex, i.e., existing gradient-based methods cannot be applied to our model.","We propose a variant of the proximal gradient method and investigate its convergence to a critical point.","In the initial epochs of FCNN training, the regularization terms are ignored, and through epochs, the effect of that increases.","The gradual addition of nonsmooth regularization terms is the main reason for the better performance of the deep neural network with nonsmooth regularization terms (DNN-NSR) algorithm.","Our simulations indicate the superiority of the proposed algorithm in comparison with existing linear and nonlinear algorithms."],"url":"http://arxiv.org/abs/2403.10232v1","category":"cs.IT"}
{"created":"2024-03-15 11:59:46","title":"Convergence Rates For Tikhonov Regularization of Coefficient Identification Problems in Robin-Boundary Equation","abstract":"This paper investigates the convergence rate for Tikhonov regularization of the problem of identifying the coefficient $a \\in L^{\\infty}(\\Omega)$ in the Robin-boundary equation $-\\mathrm{div}(a\\nabla u)-bu=f,~ x \\in \\Omega \\subset \\mathbb R^M,~ M \\geq 1$ and $u=0,~ x ~on~ \\partial\\Omega$, where $f(x)\\in L^{\\infty}(\\Omega)$. Assume we only know the imprecise values of $u$ in the subset $\\Omega_1 \\subset \\Omega$ given by $z^{\\delta} \\in {H}^1(\\Omega_1)$, satisfies $\\|u-z^{\\delta}\\|_{H^1(\\Omega_1)}\\leq \\delta$. We assume $u$ satisfy the following boundary conditions on $\\partial\\Omega_1$: \\begin{align*} \\nabla u \\cdot \\vec{n}+\\gamma u =0~on~\\partial\\Omega_1, \\end{align*} where $\\vec{n}$ is the normal vector of $\\partial\\Omega_1$ and $\\gamma>0$ is a constant. We regularize this problem by correspondingly minimizing the strictly convex functional:   \\begin{align*}   \\min \\limits_{a \\in \\mathbb A} &\\frac12 \\int_{\\Omega_1} a | {\\nabla(U(a)-z^\\delta)}|^2 +\\frac12\\int_{\\partial\\Omega_1} a\\gamma [U(a)-z^\\delta]^2-\\frac12 \\int_{\\Omega_1} b [U(a)-z^\\delta]^2\\\\ &+ \\rho \\| a-a^* \\|^2_{L^2(\\Omega)},   \\end{align*}   where $U(a)$ is a map for $a$ to the solution of the Robin-boundary problem, $\\rho > 0$ is the regularization parameter and $a^*$ is a priori estimate of $a$. We prove that the functional attain a unique global minimizer on the admissible set. Further, we give very simple source condition without the smallness requirement on the source function which provide the convergence rate $O(\\sqrt{\\delta})$ for the regularized solution.","sentences":["This paper investigates the convergence rate for Tikhonov regularization of the problem of identifying the coefficient $a \\in L^{\\infty}(\\Omega)$ in the Robin-boundary equation $-\\mathrm{div}(a\\nabla u)-bu=f,~ x \\in \\Omega \\subset \\mathbb R^M,~ M \\geq 1$ and $u=0,~ x ~on~ \\partial\\Omega$, where $f(x)\\in L^{\\infty}(\\Omega)$.","Assume we only know the imprecise values of $u$ in the subset $\\Omega_1 \\subset \\Omega$ given by $z^{\\delta} \\in {H}^1(\\Omega_1)$, satisfies $\\|u-z^{\\delta}\\|_{H^1(\\Omega_1)}\\leq \\delta$.","We assume $u$ satisfy the following boundary conditions on $\\partial\\Omega_1$: \\begin{align*} \\nabla u \\cdot \\vec{n}+\\gamma u =0~on~\\partial\\Omega_1, \\end{align*} where $\\vec{n}$ is the normal vector of $\\partial\\Omega_1$ and $\\gamma>0$ is a constant.","We regularize this problem by correspondingly minimizing the strictly convex functional:   ","\\begin{align*}   \\min \\limits_{a \\in \\mathbb A} &\\frac12 \\int_{\\Omega_1} a | {\\nabla(U(a)-z^\\delta)}|^2 +\\frac12\\int_{\\partial\\Omega_1} a\\gamma [U(a)-z^\\delta]^2-\\frac12 \\int_{\\Omega_1} b [U(a)-z^\\delta]^2\\\\ &+ \\rho \\| a-a^* \\|^2_{L^2(\\Omega)},   \\end{align*}   where $U(a)$ is a map for $a$ to the solution of the Robin-boundary problem, $\\rho > 0$ is the regularization parameter and $a^*$ is a priori estimate of $a$. We prove that the functional attain a unique global minimizer on the admissible set.","Further, we give very simple source condition without the smallness requirement on the source function which provide the convergence rate $O(\\sqrt{\\delta})$ for the regularized solution."],"url":"http://arxiv.org/abs/2403.10229v1","category":"math.AP"}
{"created":"2024-03-15 11:03:46","title":"Quasi-geostrophic projection of rotating shallow-water equations","abstract":"With a new unifying model for layered rotating shallow-water (RSW) and quasi-geostrophic (QG) equations, this paper sheds light on the relation between these two sets of equations. We propose here a new formulation of the quasi-geostrophic equations as a projection of the rotating shallow-water equations. This QG formulation uses the same prognostic variables as RSW, namely velocity and layer thickness, restoring the proximity of these two sets of equations. It allows direct access to the ageostrophic velocities hidden in geostrophic velocities resolved by the QG equations. It opens the path of studying the difference between QG and RSW using the same underlying numerical discretization. We illustrate this new possibility on a vortex shear instability and a double-gyre configuration.","sentences":["With a new unifying model for layered rotating shallow-water (RSW) and quasi-geostrophic (QG) equations, this paper sheds light on the relation between these two sets of equations.","We propose here a new formulation of the quasi-geostrophic equations as a projection of the rotating shallow-water equations.","This QG formulation uses the same prognostic variables as RSW, namely velocity and layer thickness, restoring the proximity of these two sets of equations.","It allows direct access to the ageostrophic velocities hidden in geostrophic velocities resolved by the QG equations.","It opens the path of studying the difference between QG and RSW using the same underlying numerical discretization.","We illustrate this new possibility on a vortex shear instability and a double-gyre configuration."],"url":"http://arxiv.org/abs/2403.10199v1","category":"physics.flu-dyn"}
{"created":"2024-03-15 09:56:42","title":"Gap results for biharmonic submanifolds in spheres","abstract":"In this paper we determine a larger gap of the mean curvature for a class of proper biharmonic submanifolds with parallel mean curvature vector field in Euclidean spheres. When the bounds of the gap are reached, we obtain splitting results of the submanifold.","sentences":["In this paper we determine a larger gap of the mean curvature for a class of proper biharmonic submanifolds with parallel mean curvature vector field in Euclidean spheres.","When the bounds of the gap are reached, we obtain splitting results of the submanifold."],"url":"http://arxiv.org/abs/2403.10155v1","category":"math.DG"}
{"created":"2024-03-15 09:10:40","title":"Invariant submanifolds for solutions to rough differential equations","abstract":"In this paper we provide necessary and sufficient conditions for invariance of finite dimensional submanifolds for rough differential equations (RDEs) with values in a Banach space. Furthermore, we apply our findings to the particular situation of random RDEs driven by $Q$-Wiener processes and random RDEs driven by $Q$-fractional Brownian motion.","sentences":["In this paper we provide necessary and sufficient conditions for invariance of finite dimensional submanifolds for rough differential equations (RDEs) with values in a Banach space.","Furthermore, we apply our findings to the particular situation of random RDEs driven by $Q$-Wiener processes and random RDEs driven by $Q$-fractional Brownian motion."],"url":"http://arxiv.org/abs/2403.10121v1","category":"math.PR"}
{"created":"2024-03-15 09:08:27","title":"URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields","abstract":"We propose a novel rolling shutter bundle adjustment method for neural radiance fields (NeRF), which utilizes the unordered rolling shutter (RS) images to obtain the implicit 3D representation. Existing NeRF methods suffer from low-quality images and inaccurate initial camera poses due to the RS effect in the image, whereas, the previous method that incorporates the RS into NeRF requires strict sequential data input, limiting its widespread applicability. In constant, our method recovers the physical formation of RS images by estimating camera poses and velocities, thereby removing the input constraints on sequential data. Moreover, we adopt a coarse-to-fine training strategy, in which the RS epipolar constraints of the pairwise frames in the scene graph are used to detect the camera poses that fall into local minima. The poses detected as outliers are corrected by the interpolation method with neighboring poses. The experimental results validate the effectiveness of our method over state-of-the-art works and demonstrate that the reconstruction of 3D representations is not constrained by the requirement of video sequence input.","sentences":["We propose a novel rolling shutter bundle adjustment method for neural radiance fields (NeRF), which utilizes the unordered rolling shutter (RS) images to obtain the implicit 3D representation.","Existing NeRF methods suffer from low-quality images and inaccurate initial camera poses due to the RS effect in the image, whereas, the previous method that incorporates the RS into NeRF requires strict sequential data input, limiting its widespread applicability.","In constant, our method recovers the physical formation of RS images by estimating camera poses and velocities, thereby removing the input constraints on sequential data.","Moreover, we adopt a coarse-to-fine training strategy, in which the RS epipolar constraints of the pairwise frames in the scene graph are used to detect the camera poses that fall into local minima.","The poses detected as outliers are corrected by the interpolation method with neighboring poses.","The experimental results validate the effectiveness of our method over state-of-the-art works and demonstrate that the reconstruction of 3D representations is not constrained by the requirement of video sequence input."],"url":"http://arxiv.org/abs/2403.10119v1","category":"cs.CV"}
{"created":"2024-03-15 08:44:56","title":"KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation","abstract":"In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and Deformation framework that takes object scans as input and jointly retrieves and deforms the most geometrically similar CAD models from a pre-processed database to tightly match the target. Unlike existing dense matching based methods that typically struggle with noisy partial scans, we propose to leverage category-consistent sparse keypoints to naturally handle both full and partial object scans. Specifically, we first employ a lightweight retrieval module to establish a keypoint-based embedding space, measuring the similarity among objects by dynamically aggregating deformation-aware local-global features around extracted keypoints. Objects that are close in the embedding space are considered similar in geometry. Then we introduce the neural cage-based deformation module that estimates the influence vector of each keypoint upon cage vertices inside its local support region to control the deformation of the retrieved shape. Extensive experiments on the synthetic dataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED surpasses existing state-of-the-art approaches by a large margin. Codes and trained models will be released in https://github.com/lolrudy/KP-RED.","sentences":["In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and Deformation framework that takes object scans as input and jointly retrieves and deforms the most geometrically similar CAD models from a pre-processed database to tightly match the target.","Unlike existing dense matching based methods that typically struggle with noisy partial scans, we propose to leverage category-consistent sparse keypoints to naturally handle both full and partial object scans.","Specifically, we first employ a lightweight retrieval module to establish a keypoint-based embedding space, measuring the similarity among objects by dynamically aggregating deformation-aware local-global features around extracted keypoints.","Objects that are close in the embedding space are considered similar in geometry.","Then we introduce the neural cage-based deformation module that estimates the influence vector of each keypoint upon cage vertices inside its local support region to control the deformation of the retrieved shape.","Extensive experiments on the synthetic dataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED surpasses existing state-of-the-art approaches by a large margin.","Codes and trained models will be released in https://github.com/lolrudy/KP-RED."],"url":"http://arxiv.org/abs/2403.10099v1","category":"cs.CV"}
{"created":"2024-03-15 17:09:02","title":"An Improved Metric and Benchmark for Assessing the Performance of Virtual Screening Models","abstract":"Structure-based virtual screening (SBVS) is a key workflow in computational drug discovery. SBVS models are assessed by measuring the enrichment of known active molecules over decoys in retrospective screens. However, the standard formula for enrichment cannot estimate model performance on very large libraries. Additionally, current screening benchmarks cannot easily be used with machine learning (ML) models due to data leakage. We propose an improved formula for calculating VS enrichment and introduce the BayesBind benchmarking set composed of protein targets that are structurally dissimilar to those in the BigBind training set. We assess current models on this benchmark and find that none perform appreciably better than a KNN baseline.","sentences":["Structure-based virtual screening (SBVS) is a key workflow in computational drug discovery.","SBVS models are assessed by measuring the enrichment of known active molecules over decoys in retrospective screens.","However, the standard formula for enrichment cannot estimate model performance on very large libraries.","Additionally, current screening benchmarks cannot easily be used with machine learning (ML) models due to data leakage.","We propose an improved formula for calculating VS enrichment and introduce the BayesBind benchmarking set composed of protein targets that are structurally dissimilar to those in the BigBind training set.","We assess current models on this benchmark and find that none perform appreciably better than a KNN baseline."],"url":"http://arxiv.org/abs/2403.10478v1","category":"q-bio.QM"}
{"created":"2024-03-15 17:07:39","title":"Approximate Nullspace Augmented Finetuning for Robust Vision Transformers","abstract":"Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment. In this work, we provide a finetuning approach to enhance the robustness of vision transformers inspired by the concept of nullspace from linear algebra. Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model's output when added to the input. Firstly, we show that for many pretrained ViTs, a non-trivial nullspace exists due to the presence of the patch embedding layer. Secondly, as nullspace is a concept associated with linear algebra, we demonstrate that it is possible to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing an optimisation strategy. Finally, we propose a fine-tuning strategy for ViTs wherein we augment the training data with synthesized approximate nullspace noise. After finetuning, we find that the model demonstrates robustness to adversarial and natural image perbutations alike.","sentences":["Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment.","In this work, we provide a finetuning approach to enhance the robustness of vision transformers inspired by the concept of nullspace from linear algebra.","Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model's output when added to the input.","Firstly, we show that for many pretrained ViTs, a non-trivial nullspace exists due to the presence of the patch embedding layer.","Secondly, as nullspace is a concept associated with linear algebra, we demonstrate that it is possible to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing an optimisation strategy.","Finally, we propose a fine-tuning strategy for ViTs wherein we augment the training data with synthesized approximate nullspace noise.","After finetuning, we find that the model demonstrates robustness to adversarial and natural image perbutations alike."],"url":"http://arxiv.org/abs/2403.10476v1","category":"cs.CV"}
{"created":"2024-03-15 17:02:28","title":"MADAS -- A Python framework for assessing similarity in materials-science data","abstract":"Computational materials science produces large quantities of data, both in terms of high-throughput calculations and individual studies. Extracting knowledge from this large and heterogeneous pool of data is challenging due to the wide variety of computational methods and approximations, resulting in significant veracity in the sheer amount of available data. Here, we present MADAS, a Python framework for computing similarity relations between material properties. It can be used to automate the download of data from various sources, compute descriptors and similarities between materials, analyze the relationship between materials through their properties, and can incorporate a variety of existing machine learning methods. We explain the design of the package and demonstrate its power with representative examples.","sentences":["Computational materials science produces large quantities of data, both in terms of high-throughput calculations and individual studies.","Extracting knowledge from this large and heterogeneous pool of data is challenging due to the wide variety of computational methods and approximations, resulting in significant veracity in the sheer amount of available data.","Here, we present MADAS, a Python framework for computing similarity relations between material properties.","It can be used to automate the download of data from various sources, compute descriptors and similarities between materials, analyze the relationship between materials through their properties, and can incorporate a variety of existing machine learning methods.","We explain the design of the package and demonstrate its power with representative examples."],"url":"http://arxiv.org/abs/2403.10470v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 16:28:22","title":"Optimal Block-Level Draft Verification for Accelerating Speculative Decoding","abstract":"Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft block. We propose a verification algorithm that achieves the optimal accepted length for the block-level transport problem. We empirically evaluate our proposed block-level verification algorithm in a wide range of tasks and datasets, and observe consistent improvements in wall-clock speedup when compared to token-level verification algorithm. To the best of our knowledge, our work is the first to establish improvement over speculative decoding through a better draft verification algorithm.","sentences":["Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference.","In each iteration, the algorithm first uses a smaller model to draft a block of tokens.","The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model.","In all of the prior speculative decoding works, the draft verification is performed token-by-token independently.","In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens.","We first formulate the draft verification step as a block-level optimal transport problem.","The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft block.","We propose a verification algorithm that achieves the optimal accepted length for the block-level transport problem.","We empirically evaluate our proposed block-level verification algorithm in a wide range of tasks and datasets, and observe consistent improvements in wall-clock speedup when compared to token-level verification algorithm.","To the best of our knowledge, our work is the first to establish improvement over speculative decoding through a better draft verification algorithm."],"url":"http://arxiv.org/abs/2403.10444v1","category":"cs.LG"}
{"created":"2024-03-15 14:59:21","title":"Testing MediaPipe Holistic for Linguistic Analysis of Nonmanual Markers in Sign Languages","abstract":"Advances in Deep Learning have made possible reliable landmark tracking of human bodies and faces that can be used for a variety of tasks. We test a recent Computer Vision solution, MediaPipe Holistic (MPH), to find out if its tracking of the facial features is reliable enough for a linguistic analysis of data from sign languages, and compare it to an older solution (OpenFace, OF). We use an existing data set of sentences in Kazakh-Russian Sign Language and a newly created small data set of videos with head tilts and eyebrow movements. We find that MPH does not perform well enough for linguistic analysis of eyebrow movement -- but in a different way from OF, which is also performing poorly without correction. We reiterate a previous proposal to train additional correction models to overcome these limitations.","sentences":["Advances in Deep Learning have made possible reliable landmark tracking of human bodies and faces that can be used for a variety of tasks.","We test a recent Computer Vision solution, MediaPipe Holistic (MPH), to find out if its tracking of the facial features is reliable enough for a linguistic analysis of data from sign languages, and compare it to an older solution (OpenFace, OF).","We use an existing data set of sentences in Kazakh-Russian Sign Language and a newly created small data set of videos with head tilts and eyebrow movements.","We find that MPH does not perform well enough for linguistic analysis of eyebrow movement -- but in a different way from OF, which is also performing poorly without correction.","We reiterate a previous proposal to train additional correction models to overcome these limitations."],"url":"http://arxiv.org/abs/2403.10367v1","category":"cs.CV"}
{"created":"2024-03-15 14:25:59","title":"Investigating grammatical abstraction in language models using few-shot learning of novel noun gender","abstract":"Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word embedding space. While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented. For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either.","sentences":["Humans can learn a new word and infer its grammatical properties from very few examples.","They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words.","Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French.","Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context.","We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category.","Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word embedding space.","While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented.","For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either."],"url":"http://arxiv.org/abs/2403.10338v1","category":"cs.CL"}
{"created":"2024-03-15 12:43:03","title":"Open Continual Feature Selection via Granular-Ball Knowledge Transfer","abstract":"This paper presents a novel framework for continual feature selection (CFS) in data preprocessing, particularly in the context of an open and dynamic environment where unknown classes may emerge. CFS encounters two primary challenges: the discovery of unknown knowledge and the transfer of known knowledge. To this end, the proposed CFS method combines the strengths of continual learning (CL) with granular-ball computing (GBC), which focuses on constructing a granular-ball knowledge base to detect unknown classes and facilitate the transfer of previously learned knowledge for further feature selection. CFS consists of two stages: initial learning and open learning. The former aims to establish an initial knowledge base through multi-granularity representation using granular-balls. The latter utilizes prior granular-ball knowledge to identify unknowns, updates the knowledge base for granular-ball knowledge transfer, reinforces old knowledge, and integrates new knowledge. Subsequently, we devise an optimal feature subset mechanism that incorporates minimal new features into the existing optimal subset, often yielding superior results during each period. Extensive experimental results on public benchmark datasets demonstrate our method's superiority in terms of both effectiveness and efficiency compared to state-of-the-art feature selection methods.","sentences":["This paper presents a novel framework for continual feature selection (CFS) in data preprocessing, particularly in the context of an open and dynamic environment where unknown classes may emerge.","CFS encounters two primary challenges: the discovery of unknown knowledge and the transfer of known knowledge.","To this end, the proposed CFS method combines the strengths of continual learning (CL) with granular-ball computing (GBC), which focuses on constructing a granular-ball knowledge base to detect unknown classes and facilitate the transfer of previously learned knowledge for further feature selection.","CFS consists of two stages: initial learning and open learning.","The former aims to establish an initial knowledge base through multi-granularity representation using granular-balls.","The latter utilizes prior granular-ball knowledge to identify unknowns, updates the knowledge base for granular-ball knowledge transfer, reinforces old knowledge, and integrates new knowledge.","Subsequently, we devise an optimal feature subset mechanism that incorporates minimal new features into the existing optimal subset, often yielding superior results during each period.","Extensive experimental results on public benchmark datasets demonstrate our method's superiority in terms of both effectiveness and efficiency compared to state-of-the-art feature selection methods."],"url":"http://arxiv.org/abs/2403.10253v1","category":"cs.LG"}
{"created":"2024-03-15 12:27:01","title":"Constraining Protoplanetary Disk Winds from Forbidden Line Profiles with Simulation-based Inference","abstract":"Protoplanetary disks are the sites of vigorous hydrodynamic processes, such as accretion and outflows, and ultimately establish the conditions for the formation of planets. The properties of disk outflows are often inferred through analysis of forbidden emission lines. These lines contain multiple overlapping components, tracing different emission regions with different processes that excite them: a high-velocity component (tracing a jet), a broad low-velocity component (tracing inner disk wind), and a narrow low-velocity component (tracing outer disk wind). They are also heavily contaminated by background spectral features. All of these challenges call into question the traditional approach of fitting Gaussian components to the line profiles, and cloud the physical interpretation of those components. We introduce a novel statistical technique to analyze emission lines in protoplanetary disks. Simulation-Based Inference is a computationally efficient machine learning technique that produces posterior distributions of the parameters (e.g. magnetic field, radiation sources, geometry) of a representative wind model when given a spectrum, without any prior assumption about line shapes (e.g.symmetry). In this pathfinder study, we demonstrate that this technique indeed accurately recovers the parameters from simulated spectra without noise and background. A following work will deal with the analysis of observed spectra.","sentences":["Protoplanetary disks are the sites of vigorous hydrodynamic processes, such as accretion and outflows, and ultimately establish the conditions for the formation of planets.","The properties of disk outflows are often inferred through analysis of forbidden emission lines.","These lines contain multiple overlapping components, tracing different emission regions with different processes that excite them: a high-velocity component (tracing a jet), a broad low-velocity component (tracing inner disk wind), and a narrow low-velocity component (tracing outer disk wind).","They are also heavily contaminated by background spectral features.","All of these challenges call into question the traditional approach of fitting Gaussian components to the line profiles, and cloud the physical interpretation of those components.","We introduce a novel statistical technique to analyze emission lines in protoplanetary disks.","Simulation-Based Inference is a computationally efficient machine learning technique that produces posterior distributions of the parameters (e.g. magnetic field, radiation sources, geometry) of a representative wind model when given a spectrum, without any prior assumption about line shapes (e.g.symmetry).","In this pathfinder study, we demonstrate that this technique indeed accurately recovers the parameters from simulated spectra without noise and background.","A following work will deal with the analysis of observed spectra."],"url":"http://arxiv.org/abs/2403.10243v1","category":"astro-ph.SR"}
{"created":"2024-03-15 09:58:49","title":"Cardiac valve event timing in echocardiography using deep learning and triplane recordings","abstract":"Cardiac valve event timing plays a crucial role when conducting clinical measurements using echocardiography. However, established automated approaches are limited by the need of external electrocardiogram sensors, and manual measurements often rely on timing from different cardiac cycles. Recent methods have applied deep learning to cardiac timing, but they have mainly been restricted to only detecting two key time points, namely end-diastole (ED) and end-systole (ES). In this work, we propose a deep learning approach that leverages triplane recordings to enhance detection of valve events in echocardiography. Our method demonstrates improved performance detecting six different events, including valve events conventionally associated with ED and ES. Of all events, we achieve an average absolute frame difference (aFD) of maximum 1.4 frames (29 ms) for start of diastasis, down to 0.6 frames (12 ms) for mitral valve opening when performing a ten-fold cross-validation with test splits on triplane data from 240 patients. On an external independent test consisting of apical long-axis data from 180 other patients, the worst performing event detection had an aFD of 1.8 (30 ms). The proposed approach has the potential to significantly impact clinical practice by enabling more accurate, rapid and comprehensive event detection, leading to improved clinical measurements.","sentences":["Cardiac valve event timing plays a crucial role when conducting clinical measurements using echocardiography.","However, established automated approaches are limited by the need of external electrocardiogram sensors, and manual measurements often rely on timing from different cardiac cycles.","Recent methods have applied deep learning to cardiac timing, but they have mainly been restricted to only detecting two key time points, namely end-diastole (ED) and end-systole (ES).","In this work, we propose a deep learning approach that leverages triplane recordings to enhance detection of valve events in echocardiography.","Our method demonstrates improved performance detecting six different events, including valve events conventionally associated with ED and ES.","Of all events, we achieve an average absolute frame difference (aFD) of maximum 1.4 frames (29 ms) for start of diastasis, down to 0.6 frames (12 ms) for mitral valve opening when performing a ten-fold cross-validation with test splits on triplane data from 240 patients.","On an external independent test consisting of apical long-axis data from 180 other patients, the worst performing event detection had an aFD of 1.8 (30 ms).","The proposed approach has the potential to significantly impact clinical practice by enabling more accurate, rapid and comprehensive event detection, leading to improved clinical measurements."],"url":"http://arxiv.org/abs/2403.10156v1","category":"eess.IV"}
{"created":"2024-03-15 09:47:17","title":"Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval","abstract":"Audio-text retrieval (ATR), which retrieves a relevant caption given an audio clip (A2T) and vice versa (T2A), has recently attracted much research attention. Existing methods typically aggregate information from each modality into a single vector for matching, but this sacrifices local details and can hardly capture intricate relationships within and between modalities. Furthermore, current ATR datasets lack comprehensive alignment information, and simple binary contrastive learning labels overlook the measurement of fine-grained semantic differences between samples. To counter these challenges, we present a novel ATR framework that comprehensively captures the matching relationships of multimodal information from different perspectives and finer granularities. Specifically, a fine-grained alignment method is introduced, achieving a more detail-oriented matching through a multiscale process from local to global levels to capture meticulous cross-modal relationships. In addition, we pioneer the application of cross-modal similarity consistency, leveraging intra-modal similarity relationships as soft supervision to boost more intricate alignment. Extensive experiments validate the effectiveness of our approach, outperforming previous methods by significant margins of at least 3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4% (A2T) R@1 on the Clotho dataset.","sentences":["Audio-text retrieval (ATR), which retrieves a relevant caption given an audio clip (A2T) and vice versa (T2A), has recently attracted much research attention.","Existing methods typically aggregate information from each modality into a single vector for matching, but this sacrifices local details and can hardly capture intricate relationships within and between modalities.","Furthermore, current ATR datasets lack comprehensive alignment information, and simple binary contrastive learning labels overlook the measurement of fine-grained semantic differences between samples.","To counter these challenges, we present a novel ATR framework that comprehensively captures the matching relationships of multimodal information from different perspectives and finer granularities.","Specifically, a fine-grained alignment method is introduced, achieving a more detail-oriented matching through a multiscale process from local to global levels to capture meticulous cross-modal relationships.","In addition, we pioneer the application of cross-modal similarity consistency, leveraging intra-modal similarity relationships as soft supervision to boost more intricate alignment.","Extensive experiments validate the effectiveness of our approach, outperforming previous methods by significant margins of at least 3.9% (T2A) / 6.9% (A2T) R@1 on the AudioCaps dataset and 2.9% (T2A) / 5.4% (A2T) R@1 on the Clotho dataset."],"url":"http://arxiv.org/abs/2403.10146v1","category":"cs.SD"}
{"created":"2024-03-15 17:48:43","title":"Active transport of a passive colloid in a bath of run-and-tumble particles","abstract":"The dispersion of a passive colloid immersed in a bath of non-interacting and non-Brownian run-and-tumble microswimmers in two dimensions is analyzed using stochastic simulations and an asymptotic theory, both based on a minimal model of swimmer-colloid collisions characterized solely by frictionless steric interactions. We estimate the effective long-time diffusivity $\\mathcal{D}$ of the suspended colloid resulting from its interaction with the active bath, and elucidate its dependence on the level of activity (persistence length of swimmer trajectories), the mobility ratio of the colloid to a swimmer, and the number density of swimmers in the bath. We also propose a semi-analytical model for the colloid diffusivity in terms of the variance and correlation time of the net fluctuating active force on the colloid resulting from swimmer collisions. Quantitative agreement is found between numerical simulations and analytical results in the experimentally-relevant regime of low swimmer density, low mobility ratios, and high activity.","sentences":["The dispersion of a passive colloid immersed in a bath of non-interacting and non-Brownian run-and-tumble microswimmers in two dimensions is analyzed using stochastic simulations and an asymptotic theory, both based on a minimal model of swimmer-colloid collisions characterized solely by frictionless steric interactions.","We estimate the effective long-time diffusivity $\\mathcal{D}$ of the suspended colloid resulting from its interaction with the active bath, and elucidate its dependence on the level of activity (persistence length of swimmer trajectories), the mobility ratio of the colloid to a swimmer, and the number density of swimmers in the bath.","We also propose a semi-analytical model for the colloid diffusivity in terms of the variance and correlation time of the net fluctuating active force on the colloid resulting from swimmer collisions.","Quantitative agreement is found between numerical simulations and analytical results in the experimentally-relevant regime of low swimmer density, low mobility ratios, and high activity."],"url":"http://arxiv.org/abs/2403.10508v1","category":"cond-mat.soft"}
{"created":"2024-03-15 16:41:47","title":"Stochastic integration with respect to cylindrical L\u00e9vy processes in Hilbert spaces","abstract":"In this work, we present a comprehensive theory of stochastic integration with respect to arbitrary cylindrical L\\'evy processes in Hilbert spaces. Since cylindrical L\\'evy processes do not enjoy a semi-martingale decomposition, our approach relies on an alternative approach to stochastic integration by decoupled tangent sequences. The space of deterministic integrands is identified as a modular space described in terms of the characteristics of the cylindrical L\\'evy process. The space of random integrands is described as the space of predictable processes whose trajectories are in the space of deterministic integrands almost surely. The derived space of random integrands is verified as the largest space of potential integrands, based on a classical definition of stochastic integrability. We apply the introduced theory of stochastic integration to establish a dominated convergence theorem.","sentences":["In this work, we present a comprehensive theory of stochastic integration with respect to arbitrary cylindrical L\\'evy processes in Hilbert spaces.","Since cylindrical L\\'evy processes do not enjoy a semi-martingale decomposition, our approach relies on an alternative approach to stochastic integration by decoupled tangent sequences.","The space of deterministic integrands is identified as a modular space described in terms of the characteristics of the cylindrical L\\'evy process.","The space of random integrands is described as the space of predictable processes whose trajectories are in the space of deterministic integrands almost surely.","The derived space of random integrands is verified as the largest space of potential integrands, based on a classical definition of stochastic integrability.","We apply the introduced theory of stochastic integration to establish a dominated convergence theorem."],"url":"http://arxiv.org/abs/2403.10453v1","category":"math.PR"}
{"created":"2024-03-15 16:06:40","title":"Convergence and Trade-Offs in Riemannian Gradient Descent and Riemannian Proximal Point","abstract":"In this work, we analyze two of the most fundamental algorithms in geodesically convex optimization: Riemannian gradient descent and (possibly inexact) Riemannian proximal point. We quantify their rates of convergence and produce different variants with several trade-offs. Crucially, we show the iterates naturally stay in a ball around an optimizer, of radius depending on the initial distance and, in some cases, on the curvature. In contrast, except for limited cases, previous works bounded the maximum distance between iterates and an optimizer only by assumption, leading to incomplete analyses and unquantified rates. We also provide an implementable inexact proximal point algorithm yielding new results on minmax problems, and we prove several new useful properties of Riemannian proximal methods: they work when positive curvature is present, the proximal operator does not move points away from any optimizer, and we quantify the smoothness of its induced Moreau envelope. Further, we explore beyond our theory with empirical tests.","sentences":["In this work, we analyze two of the most fundamental algorithms in geodesically convex optimization: Riemannian gradient descent and (possibly inexact)","Riemannian proximal point.","We quantify their rates of convergence and produce different variants with several trade-offs.","Crucially, we show the iterates naturally stay in a ball around an optimizer, of radius depending on the initial distance and, in some cases, on the curvature.","In contrast, except for limited cases, previous works bounded the maximum distance between iterates and an optimizer only by assumption, leading to incomplete analyses and unquantified rates.","We also provide an implementable inexact proximal point algorithm yielding new results on minmax problems, and we prove several new useful properties of Riemannian proximal methods: they work when positive curvature is present, the proximal operator does not move points away from any optimizer, and we quantify the smoothness of its induced Moreau envelope.","Further, we explore beyond our theory with empirical tests."],"url":"http://arxiv.org/abs/2403.10429v1","category":"math.OC"}
{"created":"2024-03-15 15:14:30","title":"Beyond structural stabilization of highly-textured AlN thin film: the role of chemical effects","abstract":"The crystalline quality and degree of c-axis orientation of hexagonal AlN thin films correlate directly with their functional properties. Therefore, achieving AlN thin films of high crystalline quality and texture is of extraordinary importance for many applications, but in particular in electronic devices. Here, we present a systematic study revealing that the growth of c-axis orientated AlN thin films can be governed by a chemical stabilization effect in addition to the conventionally known structural, i.e. strain-induced, stabilization mechanism. The promotion of in-plane growth of AlN grains with c-axis out-of-plane orientation is demonstrated on Y, W or Al seed layers with different thicknesses and crystallinity preliminary exposed to N2 at room temperature. We establish that the stabilization mechanism is chemical in nature: the formation of an N-rich surface layer on the metal seed layers upon exposure to N2 pre-determines the polarity of AlN islands at initial stages of thin film growth while the low energy barrier for the subsequent coalescence of islands of the same polarity contributes to grain growth. These results suggest that the growth of c-axis oriented AlN thin films can be optimized and controlled chemically thus opening more pathways for energy-efficient and controllable AlN thin-film growth processes.","sentences":["The crystalline quality and degree of c-axis orientation of hexagonal AlN thin films correlate directly with their functional properties.","Therefore, achieving AlN thin films of high crystalline quality and texture is of extraordinary importance for many applications, but in particular in electronic devices.","Here, we present a systematic study revealing that the growth of c-axis orientated AlN thin films can be governed by a chemical stabilization effect in addition to the conventionally known structural, i.e. strain-induced, stabilization mechanism.","The promotion of in-plane growth of AlN grains with c-axis out-of-plane orientation is demonstrated on Y, W or Al seed layers with different thicknesses and crystallinity preliminary exposed to N2 at room temperature.","We establish that the stabilization mechanism is chemical in nature: the formation of an N-rich surface layer on the metal seed layers upon exposure to N2 pre-determines the polarity of AlN islands at initial stages of thin film growth while the low energy barrier for the subsequent coalescence of islands of the same polarity contributes to grain growth.","These results suggest that the growth of c-axis oriented AlN thin films can be optimized and controlled chemically thus opening more pathways for energy-efficient and controllable AlN thin-film growth processes."],"url":"http://arxiv.org/abs/2403.10385v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 15:05:29","title":"PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively Aggregated Spatio-Temporal Aligment","abstract":"Leveraging Transformer attention has led to great advancements in HDR deghosting. However, the intricate nature of self-attention introduces practical challenges, as existing state-of-the-art methods often demand high-end GPUs or exhibit slow inference speeds, especially for high-resolution images like 2K. Striking an optimal balance between performance and latency remains a critical concern. In response, this work presents PASTA, a novel Progressively Aggregated Spatio-Temporal Alignment framework for HDR deghosting. Our approach achieves effectiveness and efficiency by harnessing hierarchical representation during feature distanglement. Through the utilization of diverse granularities within the hierarchical structure, our method substantially boosts computational speed and optimizes the HDR imaging workflow. In addition, we explore within-scale feature modeling with local and global attention, gradually merging and refining them in a coarse-to-fine fashion. Experimental results showcase PASTA's superiority over current SOTA methods in both visual quality and performance metrics, accompanied by a substantial 3-fold (x3) increase in inference speed.","sentences":["Leveraging Transformer attention has led to great advancements in HDR deghosting.","However, the intricate nature of self-attention introduces practical challenges, as existing state-of-the-art methods often demand high-end GPUs or exhibit slow inference speeds, especially for high-resolution images like 2K. Striking an optimal balance between performance and latency remains a critical concern.","In response, this work presents PASTA, a novel Progressively Aggregated Spatio-Temporal Alignment framework for HDR deghosting.","Our approach achieves effectiveness and efficiency by harnessing hierarchical representation during feature distanglement.","Through the utilization of diverse granularities within the hierarchical structure, our method substantially boosts computational speed and optimizes the HDR imaging workflow.","In addition, we explore within-scale feature modeling with local and global attention, gradually merging and refining them in a coarse-to-fine fashion.","Experimental results showcase PASTA's superiority over current SOTA methods in both visual quality and performance metrics, accompanied by a substantial 3-fold (x3) increase in inference speed."],"url":"http://arxiv.org/abs/2403.10376v1","category":"cs.CV"}
{"created":"2024-03-15 14:19:21","title":"Tomography of near-field radiative heat exchange between mesoscopic bodies immersed in a thermal bath","abstract":"A tomographic study of near-field radiative heat exchanges between a mesoscopic object and a substrate immersed in a thermal bath is carried out within the theoretical framework of fluctuational electrodynamics. By using the discrete-dipole-approximation method, we compute the power density distribution for radiative exchanges and highlight the major role played by many-body interactions in these transfers. Additionally, we emphasize the close relationship between power distribution and eigenmodes within the solid paving the way to applications for hot-spot targeting at deep sub-wavelength scale by shape optimization.","sentences":["A tomographic study of near-field radiative heat exchanges between a mesoscopic object and a substrate immersed in a thermal bath is carried out within the theoretical framework of fluctuational electrodynamics.","By using the discrete-dipole-approximation method, we compute the power density distribution for radiative exchanges and highlight the major role played by many-body interactions in these transfers.","Additionally, we emphasize the close relationship between power distribution and eigenmodes within the solid paving the way to applications for hot-spot targeting at deep sub-wavelength scale by shape optimization."],"url":"http://arxiv.org/abs/2403.10333v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 14:16:46","title":"Multi-Source Localization and Data Association for Time-Difference of Arrival Measurements","abstract":"In this work, we consider the problem of localizing multiple signal sources based on time-difference of arrival (TDOA) measurements. In the blind setting, in which the source signals are not known, the localization task is challenging due to the data association problem. That is, it is not known which of the TDOA measurements correspond to the same source. Herein, we propose to perform joint localization and data association by means of an optimal transport formulation. The method operates by finding optimal groupings of TDOA measurements and associating these with candidate source locations. To allow for computationally feasible localization in three-dimensional space, an efficient set of candidate locations is constructed using a minimal multilateration solver based on minimal sets of receiver pairs. In numerical simulations, we demonstrate that the proposed method is robust both to measurement noise and TDOA detection errors. Furthermore, it is shown that the data association provided by the proposed method allows for statistically efficient estimates of the source locations.","sentences":["In this work, we consider the problem of localizing multiple signal sources based on time-difference of arrival (TDOA) measurements.","In the blind setting, in which the source signals are not known, the localization task is challenging due to the data association problem.","That is, it is not known which of the TDOA measurements correspond to the same source.","Herein, we propose to perform joint localization and data association by means of an optimal transport formulation.","The method operates by finding optimal groupings of TDOA measurements and associating these with candidate source locations.","To allow for computationally feasible localization in three-dimensional space, an efficient set of candidate locations is constructed using a minimal multilateration solver based on minimal sets of receiver pairs.","In numerical simulations, we demonstrate that the proposed method is robust both to measurement noise and TDOA detection errors.","Furthermore, it is shown that the data association provided by the proposed method allows for statistically efficient estimates of the source locations."],"url":"http://arxiv.org/abs/2403.10329v1","category":"eess.SP"}
{"created":"2024-03-15 12:47:39","title":"Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models","abstract":"Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.","sentences":["Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora.","Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks.","In this work, we extend the evaluation from NLP tasks to real user queries.","We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios.","For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language.","Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs."],"url":"http://arxiv.org/abs/2403.10258v1","category":"cs.CL"}
{"created":"2024-03-15 11:19:31","title":"Comparison of Proximal First-Order Primal and Primal-Dual algorithms via Performance Estimation","abstract":"Selecting the fastest algorithm for a specific signal/image processing task is a challenging question. We propose an approach based on the Performance Estimation Problem framework that numerically and automatically computes the worst-case performance of a given optimization method on a class of functions. We first propose a computer-assisted analysis and comparison of several first-order primal optimization methods, namely, the gradient method, the forward-backward, Peaceman-Rachford, and Douglas-Rachford splittings. We tighten the existing convergence results of these algorithms and extend them to new classes of functions. Our analysis is then extended and evaluated in the context of the primal-dual Chambolle-Pock and Condat-V\\~u methods.","sentences":["Selecting the fastest algorithm for a specific signal/image processing task is a challenging question.","We propose an approach based on the Performance Estimation Problem framework that numerically and automatically computes the worst-case performance of a given optimization method on a class of functions.","We first propose a computer-assisted analysis and comparison of several first-order primal optimization methods, namely, the gradient method, the forward-backward, Peaceman-Rachford, and Douglas-Rachford splittings.","We tighten the existing convergence results of these algorithms and extend them to new classes of functions.","Our analysis is then extended and evaluated in the context of the primal-dual Chambolle-Pock and Condat-V\\~u methods."],"url":"http://arxiv.org/abs/2403.10209v1","category":"math.OC"}
{"created":"2024-03-15 11:07:29","title":"Steering internal and outgoing electron dynamics in bilayer graphene cavities by cavity design","abstract":"Ballistic, gate-defined devices in two-dimensional materials offer a platform for electron optics phenomena influenced by the material's properties and gate control. We study the ray trajectory dynamics of all-electronic, gate-defined cavities in bilayer graphene to establish how distinct regimes of the internal and outgoing charge carrier dynamics can be tuned and optimized by the cavity shape, symmetry, and parameter choice, e.g., the band gap and the cavity orientation. In particular, we compare the dynamics of two cavity shapes, o'nigiri, and Lima\\c{c}on cavities, which fall into different symmetry classes. We demonstrate that for stabilising regular, internal cavity modes, such as periodic and whispering gallery orbits, it is beneficial to match the cavity shape to the bilayer graphene Fermi line contour. Conversely, a cavity of a different symmetry than the material dispersion allows one to determine preferred emission directionalities in the emitted far-field.","sentences":["Ballistic, gate-defined devices in two-dimensional materials offer a platform for electron optics phenomena influenced by the material's properties and gate control.","We study the ray trajectory dynamics of all-electronic, gate-defined cavities in bilayer graphene to establish how distinct regimes of the internal and outgoing charge carrier dynamics can be tuned and optimized by the cavity shape, symmetry, and parameter choice, e.g., the band gap and the cavity orientation.","In particular, we compare the dynamics of two cavity shapes, o'nigiri, and Lima\\c{c}on cavities, which fall into different symmetry classes.","We demonstrate that for stabilising regular, internal cavity modes, such as periodic and whispering gallery orbits, it is beneficial to match the cavity shape to the bilayer graphene Fermi line contour.","Conversely, a cavity of a different symmetry than the material dispersion allows one to determine preferred emission directionalities in the emitted far-field."],"url":"http://arxiv.org/abs/2403.10201v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 11:00:30","title":"Dynamics, locality and weak measurements: trajectories and which-way information in the case of a simplified double-slit setup","abstract":"Understanding how the interference pattern produced by a quantum particle in Young's double-slit setup builds up -- the \"only mystery\" of quantum mechanics according to Feynman -- is still a matter of discussion and speculation. Recent works have revisited the possibility of acquiring which-way information based on weak measurements. Weak measurements preserve the interference pattern due to their minimally perturbing character while still leading to a final position detection. Here we investigate a simplified double-slit setup by including weakly coupled pointers. We examine how the information provided by the weak pointers can be interpreted to infer the dynamics within a local picture through \"weak trajectories\". We contrast our approach with non-local dynamical accounts, such as the modular momentum approach to weak values and the trajectories defined by the de Broglie-Bohm picture.","sentences":["Understanding how the interference pattern produced by a quantum particle in Young's double-slit setup builds up -- the \"only mystery\" of quantum mechanics according to Feynman -- is still a matter of discussion and speculation.","Recent works have revisited the possibility of acquiring which-way information based on weak measurements.","Weak measurements preserve the interference pattern due to their minimally perturbing character while still leading to a final position detection.","Here we investigate a simplified double-slit setup by including weakly coupled pointers.","We examine how the information provided by the weak pointers can be interpreted to infer the dynamics within a local picture through \"weak trajectories\".","We contrast our approach with non-local dynamical accounts, such as the modular momentum approach to weak values and the trajectories defined by the de Broglie-Bohm picture."],"url":"http://arxiv.org/abs/2403.10197v1","category":"quant-ph"}
{"created":"2024-03-15 10:59:11","title":"Euclidean rectifiability of sub-Finsler spheres in free-Carnot groups of step 2","abstract":"We consider 2-step free-Carnot groups equipped with sub-Finsler distances. We prove that the metric spheres are codimension-one rectifiable from the Euclidean viewpoint. The result is obtained by studying how the Lipschitz constant for the distance function behaves near abnormal geodesics.","sentences":["We consider 2-step free-Carnot groups equipped with sub-Finsler distances.","We prove that the metric spheres are codimension-one rectifiable from the Euclidean viewpoint.","The result is obtained by studying how the Lipschitz constant for the distance function behaves near abnormal geodesics."],"url":"http://arxiv.org/abs/2403.10196v1","category":"math.MG"}
{"created":"2024-03-15 08:49:33","title":"CSDNet: Detect Salient Object in Depth-Thermal via A Lightweight Cross Shallow and Deep Perception Network","abstract":"While we enjoy the richness and informativeness of multimodal data, it also introduces interference and redundancy of information. To achieve optimal domain interpretation with limited resources, we propose CSDNet, a lightweight \\textbf{C}ross \\textbf{S}hallow and \\textbf{D}eep Perception \\textbf{Net}work designed to integrate two modalities with less coherence, thereby discarding redundant information or even modality. We implement our CSDNet for Salient Object Detection (SOD) task in robotic perception. The proposed method capitalises on spatial information prescreening and implicit coherence navigation across shallow and deep layers of the depth-thermal (D-T) modality, prioritising integration over fusion to maximise the scene interpretation. To further refine the descriptive capabilities of the encoder for the less-known D-T modalities, we also propose SAMAEP to guide an effective feature mapping to the generalised feature space. Our approach is tested on the VDT-2048 dataset, leveraging the D-T modality outperforms those of SOTA methods using RGB-T or RGB-D modalities for the first time, achieves comparable performance with the RGB-D-T triple-modality benchmark method with 5.97 times faster at runtime and demanding 0.0036 times fewer FLOPs. Demonstrates the proposed CSDNet effectively integrates the information from the D-T modality. The code will be released upon acceptance.","sentences":["While we enjoy the richness and informativeness of multimodal data, it also introduces interference and redundancy of information.","To achieve optimal domain interpretation with limited resources, we propose CSDNet, a lightweight \\textbf{C}ross \\textbf{S}hallow and \\textbf{D}eep Perception \\textbf{Net}work designed to integrate two modalities with less coherence, thereby discarding redundant information or even modality.","We implement our CSDNet for Salient Object Detection (SOD) task in robotic perception.","The proposed method capitalises on spatial information prescreening and implicit coherence navigation across shallow and deep layers of the depth-thermal (D-T) modality, prioritising integration over fusion to maximise the scene interpretation.","To further refine the descriptive capabilities of the encoder for the less-known D-T modalities, we also propose SAMAEP to guide an effective feature mapping to the generalised feature space.","Our approach is tested on the VDT-2048 dataset, leveraging the D-T modality outperforms those of SOTA methods using RGB-T or RGB-D modalities for the first time, achieves comparable performance with the RGB-D-T triple-modality benchmark method with 5.97 times faster at runtime and demanding 0.0036 times fewer FLOPs.","Demonstrates the proposed CSDNet effectively integrates the information from the D-T modality.","The code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.10104v1","category":"cs.CV"}
{"created":"2024-03-15 17:49:13","title":"Comments on the no boundary wavefunction and slow roll inflation","abstract":"We review aspects of the Hartle-Hawking no boundary geometry in the context of slow roll inflation. We give an analytic approximation to the geometry and we explain the rationale for the proposal. We also explain why it gives a prediction for the curvature of the universe that is in disagreement with observations and give a quick review of proposed ways to resolve that disagreement.","sentences":["We review aspects of the Hartle-Hawking no boundary geometry in the context of slow roll inflation.","We give an analytic approximation to the geometry and we explain the rationale for the proposal.","We also explain why it gives a prediction for the curvature of the universe that is in disagreement with observations and give a quick review of proposed ways to resolve that disagreement."],"url":"http://arxiv.org/abs/2403.10510v1","category":"hep-th"}
{"created":"2024-03-15 17:32:38","title":"$DK/D\u03c0$ scattering and an exotic virtual bound state at the $SU(3)$ flavour symmetric point from lattice QCD","abstract":"Elastic $S-$wave scattering of a charm meson with a light pseudoscalar meson in $J^P =0^+$ is investigated in the flavour $\\bar{\\mathbf{3}}$, $\\mathbf{6}$ and $\\overline{\\mathbf{15}}$ sectors at the $SU(3)_f$ flavour point using lattice QCD, working on three volumes with $m_{\\pi} \\approx 700$ MeV. Large bases of interpolating operators are employed to extract finite-volume spectra, which are subsequently used with the L\\\"uscher method to provide constraints on infinite-volume scattering amplitudes. Examining the singularities of the amplitudes, the $S-$wave amplitude in the flavour $\\bar{\\mathbf{3}}$ sector is found to contain a deeply bound state, strongly coupled to elastic threshold, corresponding to the $J^P = 0^+$ $D_{s0}^*(2317)$. In the exotic flavour $\\mathbf{6}$ sector a virtual bound state is found at $\\sqrt{s_{\\rm{pole}}} = 2510 - 2610$ MeV, roughly $40-140$ MeV below threshold, whereas the $\\overline{\\mathbf{15}}$ channel shows weak repulsion.","sentences":["Elastic $S-$wave scattering of a charm meson with a light pseudoscalar meson in $J^P =0^+$ is investigated in the flavour $\\bar{\\mathbf{3}}$, $\\mathbf{6}$ and $\\overline{\\mathbf{15}}$ sectors at the $SU(3)_f$ flavour point using lattice QCD, working on three volumes with $m_{\\pi} \\approx 700$ MeV. Large bases of interpolating operators are employed to extract finite-volume spectra, which are subsequently used with the L\\\"uscher method to provide constraints on infinite-volume scattering amplitudes.","Examining the singularities of the amplitudes, the $S-$wave amplitude in the flavour $\\bar{\\mathbf{3}}$ sector is found to contain a deeply bound state, strongly coupled to elastic threshold, corresponding to the $J^P = 0^+$ $","D_{s0}^*(2317)$. In the exotic flavour $\\mathbf{6}$ sector a virtual bound state is found at $\\sqrt{s_{\\rm{pole}}} = 2510 - 2610$ MeV, roughly $40-140$ MeV below threshold, whereas the $\\overline{\\mathbf{15}}$ channel shows weak repulsion."],"url":"http://arxiv.org/abs/2403.10498v1","category":"hep-lat"}
{"created":"2024-03-15 17:21:23","title":"Unifying frequency metrology across microwave, optical, and free-electron domains","abstract":"Frequency metrology lies at the heart of precision measurement. Optical frequency combs provide a coherent link uniting the microwave and optical domains in the electromagnetic spectrum, with profound implications in timekeeping, sensing and spectroscopy, fundamental physics tests, exoplanet search, and light detection and ranging. Here, we extend this frequency link to free electrons by coherent modulation of the electron phase by a continuous-wave laser locked to a fully stabilized optical frequency comb. Microwave frequency standards are transferred to the optical domain via the frequency comb, and are further imprinted in the electron spectrum by optically modulating the electron phase with a photonic chip-based microresonator. As a proof-of-concept demonstration, we apply this frequency link in the calibration of an electron spectrometer, and use the electron spectrum to measure the optical frequency. Our work bridges frequency domains differed by a factor of $\\sim10^{13}$ and carried by different physical objects, establishes a spectroscopic connection between electromagnetic waves and free-electron matter waves, and has direct ramifications in ultrahigh-precision electron spectroscopy.","sentences":["Frequency metrology lies at the heart of precision measurement.","Optical frequency combs provide a coherent link uniting the microwave and optical domains in the electromagnetic spectrum, with profound implications in timekeeping, sensing and spectroscopy, fundamental physics tests, exoplanet search, and light detection and ranging.","Here, we extend this frequency link to free electrons by coherent modulation of the electron phase by a continuous-wave laser locked to a fully stabilized optical frequency comb.","Microwave frequency standards are transferred to the optical domain via the frequency comb, and are further imprinted in the electron spectrum by optically modulating the electron phase with a photonic chip-based microresonator.","As a proof-of-concept demonstration, we apply this frequency link in the calibration of an electron spectrometer, and use the electron spectrum to measure the optical frequency.","Our work bridges frequency domains differed by a factor of $\\sim10^{13}$ and carried by different physical objects, establishes a spectroscopic connection between electromagnetic waves and free-electron matter waves, and has direct ramifications in ultrahigh-precision electron spectroscopy."],"url":"http://arxiv.org/abs/2403.10486v1","category":"physics.optics"}
{"created":"2024-03-15 16:54:46","title":"Microscopic understanding of NMR signals by dynamic mean-field theory for spins","abstract":"A recently developed dynamic mean-field theory for disordered spins (spinDMFT) is shown to capture the spin dynamics of nuclear spins very well. The key quantities are the spin autocorrelations. In order to compute the free induction decay (FID), pair correlations are needed in addition. They can be computed on spin clusters of moderate size which are coupled to the dynamic mean fields determined in a first step by spinDMFT. We dub this versatile approach non-local spinDMFT (nl-spinDMFT). It is a particular asset of nl-spinDMFT that one knows from where the contributions to the FID stem. We illustrate the strengths of nl-spinDMFT in comparison to experimental data for CaF$_2$. Furthermore, spinDMFT provides the dynamic mean-fields explaining the FID of the nuclear spins in $^{13}$C in adamantane up to some static noise. The spin Hahn echo in adamantane is free from effects of static noise and agrees excellently with the spinDMFT results without further fitting.","sentences":["A recently developed dynamic mean-field theory for disordered spins (spinDMFT) is shown to capture the spin dynamics of nuclear spins very well.","The key quantities are the spin autocorrelations.","In order to compute the free induction decay (FID), pair correlations are needed in addition.","They can be computed on spin clusters of moderate size which are coupled to the dynamic mean fields determined in a first step by spinDMFT.","We dub this versatile approach non-local spinDMFT (nl-spinDMFT).","It is a particular asset of nl-spinDMFT that one knows from where the contributions to the FID stem.","We illustrate the strengths of nl-spinDMFT in comparison to experimental data for CaF$_2$. Furthermore, spinDMFT provides the dynamic mean-fields explaining the FID of the nuclear spins in $^{13}$C in adamantane up to some static noise.","The spin Hahn echo in adamantane is free from effects of static noise and agrees excellently with the spinDMFT results without further fitting."],"url":"http://arxiv.org/abs/2403.10465v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-15 16:30:41","title":"Free Doubly-Infinitary Distributive Categories are Cartesian Closed","abstract":"We delve into the concept of categories with products that distribute over coproducts, which we call doubly-infinitary distributive categories. We show various instances of doubly-infinitary distributive categories aiming for a comparative analysis with established notions such as extensivity, infinitary distributiveness, and cartesian closedness. Our exploration reveals that this condition represents a substantial extension beyond the classical understanding of infinitary distributive categories. Our main theorem establishes that free doubly-infinitary distributive categories are cartesian closed. We end the paper with remarks on non-canonical isomorphisms, open questions, and future work.","sentences":["We delve into the concept of categories with products that distribute over coproducts, which we call doubly-infinitary distributive categories.","We show various instances of doubly-infinitary distributive categories aiming for a comparative analysis with established notions such as extensivity, infinitary distributiveness, and cartesian closedness.","Our exploration reveals that this condition represents a substantial extension beyond the classical understanding of infinitary distributive categories.","Our main theorem establishes that free doubly-infinitary distributive categories are cartesian closed.","We end the paper with remarks on non-canonical isomorphisms, open questions, and future work."],"url":"http://arxiv.org/abs/2403.10447v1","category":"math.CT"}
{"created":"2024-03-15 16:27:53","title":"Celestial soft currents at one-loop and their OPEs","abstract":"Conformally soft operators and their associated soft theorems on the celestial sphere encode the low energy behaviour of bulk scattering amplitudes. They lead to an infinite dimensional symmetry algebra of the celestial CFT at tree-level. In this paper, we introduce new operators in the celestial CFT in order to extend the definition of conformally soft currents to include one-loop effects. We then compute their OPEs with other operators in the theory. We also examine new subtleties that arise in defining OPEs of two conformally soft operators. We elucidate the connection between the new operators and loop corrected soft theorems in the bulk. Finally, we conclude by demonstrating how these operators fit into the framework of a logarithmic CFT.","sentences":["Conformally soft operators and their associated soft theorems on the celestial sphere encode the low energy behaviour of bulk scattering amplitudes.","They lead to an infinite dimensional symmetry algebra of the celestial CFT at tree-level.","In this paper, we introduce new operators in the celestial CFT in order to extend the definition of conformally soft currents to include one-loop effects.","We then compute their OPEs with other operators in the theory.","We also examine new subtleties that arise in defining OPEs of two conformally soft operators.","We elucidate the connection between the new operators and loop corrected soft theorems in the bulk.","Finally, we conclude by demonstrating how these operators fit into the framework of a logarithmic CFT."],"url":"http://arxiv.org/abs/2403.10443v1","category":"hep-th"}
{"created":"2024-03-15 15:31:53","title":"Searching for precursor activity of Type IIn Supernovae","abstract":"We conducted a search for luminous outbursts prior to the explosion of Type IIn Supernovae (SNe IIn). We built a sample of 27 objects spectroscopically classified as SNe IIn, all located at $z<0.015$. Using deep archival SN fields images taken up to nearly 20 years prior from transient surveys (PTF, ZTF, DES, CHASE) and major astronomical observatories (ESO and NOAO), we found at least one outburst years to months before the explosion of seven SNe IIn, the earliest precursor being 10 years prior to the explosion of SN 2019bxq. The maximum absolute magnitudes of the outbursts range between -11.5 mag and -15 mag, and the eruptive phases last for a few weeks to a few years. The $g-r$ colour measured for three objects during their outburst is relatively red, with $g-r$ ranging between 0.5 and 1.0 mag. This is similar to the colour expected during the eruptions of Luminous Blue Variables. We noticed that the SNe with pre-SN outbursts have light curves with faster decline rates than those that do not show pre-SN outbursts. SN 2011fh is remarkable, as it is still visible 12 years after the luminous SN-like event, indicating that the progenitor possibly survived, or that the interaction is still on-going. We detect precursor activity in 29% of bona-fide SNe~IIn in our sample. However, a quantitative assessment of the observational biases affecting the sample suggests that this fraction underestimates the intrinsic precursor occurrence rate.","sentences":["We conducted a search for luminous outbursts prior to the explosion of Type IIn Supernovae (SNe IIn).","We built a sample of 27 objects spectroscopically classified as SNe IIn, all located at $z<0.015$. Using deep archival SN fields images taken up to nearly 20 years prior from transient surveys (PTF, ZTF, DES, CHASE) and major astronomical observatories (ESO and NOAO), we found at least one outburst years to months before the explosion of seven SNe IIn, the earliest precursor being 10 years prior to the explosion of SN 2019bxq.","The maximum absolute magnitudes of the outbursts range between -11.5 mag and -15 mag, and the eruptive phases last for a few weeks to a few years.","The $g-r$ colour measured for three objects during their outburst is relatively red, with $g-r$ ranging between 0.5 and 1.0 mag.","This is similar to the colour expected during the eruptions of Luminous Blue Variables.","We noticed that the SNe with pre-SN outbursts have light curves with faster decline rates than those that do not show pre-SN outbursts.","SN 2011fh is remarkable, as it is still visible 12 years after the luminous SN-like event, indicating that the progenitor possibly survived, or that the interaction is still on-going.","We detect precursor activity in 29% of bona-fide SNe~IIn in our sample.","However, a quantitative assessment of the observational biases affecting the sample suggests that this fraction underestimates the intrinsic precursor occurrence rate."],"url":"http://arxiv.org/abs/2403.10398v1","category":"astro-ph.HE"}
{"created":"2024-03-15 15:15:15","title":"Peak energy--Isotropic Luminosity Correlation and Jet Opening Angle Evolution in Swift-BAT Short GRBs with Soft Tail Emission","abstract":"Some short gamma-ray bursts (SGRBs) exhibit a short duration and spectral hard emission (referred to as a \"hard spike\") followed by a slightly longer soft emission (known as a \"soft tail\"). We identified nine SGRBs with known redshift in the \\textit{Swift}/BAT gamma-ray burst catalog by specifically searching for the soft tail. We found that spectra of these SGRBs can be described as a cutoff power-law model for both hard spike and soft tail, and both show time variation keeping the $E_{\\rm peak}$--$L_{\\rm iso}$ correlation. This suggests that the emission mechanism of both phenomena is identical. Furthermore, we found a trend of luminosity evolution as a function of redshift. This phenomenon suggests that these bursts originate from sources that have intrinsically bright and/or energy density concentrated within a narrower jet at higher redshift. We demonstrate that the average jet opening angle, derived from the jet break, can be explained by considering a model based on a strongly redshift-dependent jet opening angle.","sentences":["Some short gamma-ray bursts (SGRBs) exhibit a short duration and spectral hard emission (referred to as a \"hard spike\") followed by a slightly longer soft emission (known as a \"soft tail\").","We identified nine SGRBs with known redshift in the \\textit{Swift}/BAT gamma-ray burst catalog by specifically searching for the soft tail.","We found that spectra of these SGRBs can be described as a cutoff power-law model for both hard spike and soft tail, and both show time variation keeping the $E_{\\rm peak}$--$L_{\\rm iso}$ correlation.","This suggests that the emission mechanism of both phenomena is identical.","Furthermore, we found a trend of luminosity evolution as a function of redshift.","This phenomenon suggests that these bursts originate from sources that have intrinsically bright and/or energy density concentrated within a narrower jet at higher redshift.","We demonstrate that the average jet opening angle, derived from the jet break, can be explained by considering a model based on a strongly redshift-dependent jet opening angle."],"url":"http://arxiv.org/abs/2403.10386v1","category":"astro-ph.HE"}
{"created":"2024-03-15 14:53:42","title":"Spectroscopic Observations of the Solar Corona during the 2017 August 21 Total Solar Eclipse: Comparison of Spectral Line Widths and Doppler Shifts Between Open and Closed Magnetic Structures","abstract":"The spectroscopic observations presented here were acquired during the 2017 August 21 total solar eclipse with a three-channel partially multiplexed imaging spectrometer (3PAMIS) operating at extremely high orders ($>$ 50). The 4 $R_\\odot$ extent of the slit in the North-South direction scanned the corona starting from the central meridian out to approximately 1.0 $R_\\odot$ off the east limb throughout totality. The line widths and Doppler shifts of the Fe X (637.4 nm) and Fe XIV (530.3 nm) emission lines, characteristic of $1.1 \\times 10^6$ K and $1.8 \\times 10^6$ K electron temperatures respectively, varied across the different coronal structures intercepted by the slit. Fe XIV was the dominant emission in the closed fields of an active region and the base of a streamer, with relatively constant 20 - 30 km s$^{-1}$ line widths independent of the height. In contrast, Fe X emission exhibited broader ($>40 $km s$^{-1}$) line widths in open fields which increased with height, in particular in the polar coronal hole. Inferences of line widths and Doppler shifts were consistent with extreme ultraviolet (EUV) observations from Hinode/EIS, as well as with the near-infrared Fe XIII 1074 nm line observed by CoMP. The differences in the spectral line widths between distinct coronal structures are interpreted as an indication of the predominance of wave heating in open structures versus localized heating in closed structures. This study underscores the unparalleled advantages and the enormous potential of TSE spectroscopy in measuring line widths simultaneously in open and closed fields at high altitudes, with minimal exposure times, stray light levels, and instrumental widths.","sentences":["The spectroscopic observations presented here were acquired during the 2017 August 21 total solar eclipse with a three-channel partially multiplexed imaging spectrometer (3PAMIS) operating at extremely high orders ($>$ 50).","The 4 $R_\\odot$ extent of the slit in the North-South direction scanned the corona starting from the central meridian out to approximately 1.0 $R_\\odot$ off the east limb throughout totality.","The line widths and Doppler shifts of the Fe X (637.4 nm) and Fe XIV (530.3 nm) emission lines, characteristic of $1.1 \\times 10^6$ K and $1.8 \\times 10^6$ K electron temperatures respectively, varied across the different coronal structures intercepted by the slit.","Fe XIV was the dominant emission in the closed fields of an active region and the base of a streamer, with relatively constant 20 - 30 km s$^{-1}$ line widths independent of the height.","In contrast, Fe X emission exhibited broader ($>40 $km s$^{-1}$) line widths in open fields which increased with height, in particular in the polar coronal hole.","Inferences of line widths and Doppler shifts were consistent with extreme ultraviolet (EUV) observations from Hinode/EIS, as well as with the near-infrared Fe XIII 1074 nm line observed by CoMP.","The differences in the spectral line widths between distinct coronal structures are interpreted as an indication of the predominance of wave heating in open structures versus localized heating in closed structures.","This study underscores the unparalleled advantages and the enormous potential of TSE spectroscopy in measuring line widths simultaneously in open and closed fields at high altitudes, with minimal exposure times, stray light levels, and instrumental widths."],"url":"http://arxiv.org/abs/2403.10363v1","category":"astro-ph.SR"}
{"created":"2024-03-15 14:52:52","title":"Unveiling Wash Trading in Popular NFT Markets","abstract":"As emerging digital assets, NFTs are susceptible to anomalous trading behaviors due to the lack of stringent regulatory mechanisms, potentially causing economic losses. In this paper, we conduct the first systematic analysis of four non-fungible tokens (NFT) markets. Specifically, we analyze more than 25 million transactions within these markets, to explore the evolution of wash trade activities. Furthermore, we propose a heuristic algorithm that integrates the network characteristics of transactions with behavioral analysis, to detect wash trading activities in NFT markets. Our findings indicate that NFT markets with incentivized structures exhibit higher proportions of wash trading volume compared to those without incentives. Notably, the LooksRare and X2Y2 markets are detected with wash trading volume proportions as high as 94.5% and 84.2%, respectively.","sentences":["As emerging digital assets, NFTs are susceptible to anomalous trading behaviors due to the lack of stringent regulatory mechanisms, potentially causing economic losses.","In this paper, we conduct the first systematic analysis of four non-fungible tokens (NFT) markets.","Specifically, we analyze more than 25 million transactions within these markets, to explore the evolution of wash trade activities.","Furthermore, we propose a heuristic algorithm that integrates the network characteristics of transactions with behavioral analysis, to detect wash trading activities in NFT markets.","Our findings indicate that NFT markets with incentivized structures exhibit higher proportions of wash trading volume compared to those without incentives.","Notably, the LooksRare and X2Y2 markets are detected with wash trading volume proportions as high as 94.5% and 84.2%, respectively."],"url":"http://arxiv.org/abs/2403.10361v1","category":"cs.CR"}
{"created":"2024-03-15 13:32:05","title":"Low-energy theorems for neutron-proton scattering in $\u03c7$EFT using a perturbative power counting","abstract":"Low-energy theorems (LETs) for effective-range parameters in nucleon-nucleon scattering encode properties of the long-range part of the nuclear force. We compute LETs for \\textit{S}-wave neutron-proton scattering using chiral effective field theory ($\\chi$EFT) with a modified version of Weinberg power counting. Corrections to the leading order amplitude are included in distorted-wave perturbation theory and we include contributions up to the third order in the power counting. We find that LETs in the $^3S_1$ partial wave agree excellently with the empirical effective-range parameters while LETs in the $^1S_0$ partial wave show a fairly good agreement. At the same time, phase shifts up to laboratory scattering energies of about 100 MeV can be reproduced. The contributions from two-pion exchange do not improve the LETs in the $^1S_0$ partial wave, and a noticeable dependence on the employed momentum space cutoff emerges in the $^3S_1$ partial wave for cutoffs smaller than 750 MeV. The results show that empirical effective-range parameters and phase shifts can be reproduced simultaneously in the employed power counting and that LETs might be useful when inferring low-energy constants in power countings where corrections are added perturbatively.","sentences":["Low-energy theorems (LETs) for effective-range parameters in nucleon-nucleon scattering encode properties of the long-range part of the nuclear force.","We compute LETs for \\textit{S}-wave neutron-proton scattering using chiral effective field theory ($\\chi$EFT) with a modified version of Weinberg power counting.","Corrections to the leading order amplitude are included in distorted-wave perturbation theory and we include contributions up to the third order in the power counting.","We find that LETs in the $^3S_1$ partial wave agree excellently with the empirical effective-range parameters while LETs in the $^1S_0$ partial wave show a fairly good agreement.","At the same time, phase shifts up to laboratory scattering energies of about 100 MeV can be reproduced.","The contributions from two-pion exchange do not improve the LETs in the $^1S_0$ partial wave, and a noticeable dependence on the employed momentum space cutoff emerges in the $^3S_1$ partial wave for cutoffs smaller than 750 MeV.","The results show that empirical effective-range parameters and phase shifts can be reproduced simultaneously in the employed power counting and that LETs might be useful when inferring low-energy constants in power countings where corrections are added perturbatively."],"url":"http://arxiv.org/abs/2403.10292v1","category":"nucl-th"}
{"created":"2024-03-15 12:04:57","title":"Spin and Orbital Magnetism by Light in Rutile Altermagnets","abstract":"While the understanding of altermagnetism is still at a very early stage, it is expected to play a role in various fields of condensed matter research, for example spintronics, caloritronics and superconductivity. In the field of optical magnetism, it is still unclear to which extent altermagnets as a class can exhibit a distinct behavior. Here we choose RuO$_2$, a prototype metallic altermagnet with a giant spin splitting, and CoF$_2$, an experimentally known insulating altermagnet, to study the light-induced magnetism in rutile altermagnets from first-principles. We demonstrate that in the non-relativisic limit the allowed sublattice-resolved orbital response exhibits symmetries, imposed by altermagnetism, which lead to a drastic canting of light-induced moments. On the other hand, we find that inclusion of spin-orbit interaction enhances the overall effect drastically, introduces a significant anisotropy with respect to the light polarization and strongly suppresses the canting of induced moments. Remarkably, we observe that the moments induced by linearly-polarized laser pulses in light altermagnets can even exceed in magnitude those predicted for heavy ferromagnets exposed to circularly polarized light. By resorting to microscopic tools we interpret our results in terms of the altermagnetic spin splittings and of their reciprocal space distribution. Based on our findings, we speculate that optical excitations may provide a unique tool to switch and probe the magnetic state of rutile altermagnets.","sentences":["While the understanding of altermagnetism is still at a very early stage, it is expected to play a role in various fields of condensed matter research, for example spintronics, caloritronics and superconductivity.","In the field of optical magnetism, it is still unclear to which extent altermagnets as a class can exhibit a distinct behavior.","Here we choose RuO$_2$, a prototype metallic altermagnet with a giant spin splitting, and CoF$_2$, an experimentally known insulating altermagnet, to study the light-induced magnetism in rutile altermagnets from first-principles.","We demonstrate that in the non-relativisic limit the allowed sublattice-resolved orbital response exhibits symmetries, imposed by altermagnetism, which lead to a drastic canting of light-induced moments.","On the other hand, we find that inclusion of spin-orbit interaction enhances the overall effect drastically, introduces a significant anisotropy with respect to the light polarization and strongly suppresses the canting of induced moments.","Remarkably, we observe that the moments induced by linearly-polarized laser pulses in light altermagnets can even exceed in magnitude those predicted for heavy ferromagnets exposed to circularly polarized light.","By resorting to microscopic tools we interpret our results in terms of the altermagnetic spin splittings and of their reciprocal space distribution.","Based on our findings, we speculate that optical excitations may provide a unique tool to switch and probe the magnetic state of rutile altermagnets."],"url":"http://arxiv.org/abs/2403.10235v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-15 11:48:36","title":"The self-dual Lorentz violating model: quantization, scattering and dual equivalence","abstract":"In this paper, we analysis the dynamics, at the quantum level, of the self-dual field minimally coupled to bosons with Lorentz symmetry breaking. We quantize the model by applying the Dirac bracket canonical quantization procedure. In addition, we test the relativistic invariance of the model by computing the boson-boson elastic scattering amplitude. Therefore, we show that the Lorentz symmetry breaking has been restored at the quantum level. We finalize our analysis by computing the dual equivalence between the self-dual model with Lorentz symmetry breaking coupled with bosonic matter and the Maxwell-Chern-Simons with Lorentz invariance violation coupled with bosonic field.","sentences":["In this paper, we analysis the dynamics, at the quantum level, of the self-dual field minimally coupled to bosons with Lorentz symmetry breaking.","We quantize the model by applying the Dirac bracket canonical quantization procedure.","In addition, we test the relativistic invariance of the model by computing the boson-boson elastic scattering amplitude.","Therefore, we show that the Lorentz symmetry breaking has been restored at the quantum level.","We finalize our analysis by computing the dual equivalence between the self-dual model with Lorentz symmetry breaking coupled with bosonic matter and the Maxwell-Chern-Simons with Lorentz invariance violation coupled with bosonic field."],"url":"http://arxiv.org/abs/2403.10224v1","category":"hep-th"}
{"created":"2024-03-15 11:22:43","title":"Beam Dynamics Framework Incorporating Acceleration to Define the Minimum Aperture in Two Focusing Schemes for Proton Radiotherapy Linac","abstract":"In this paper, a self-consistent transverse beam dynamics framework is demonstrated, that incorporates acceleration into the transverse beam dynamics studies for a proton linac machine. Two focusing schemes are developed and discussed; the FODO-like scheme, and the minimum aperture scheme. The FODO-like scheme is a simple scheme, requiring only one quadrupole per cavity. The scheme is analytically solved to minimise the beam size at the cavity entrance/exit and ensures a constant beam size along the lattice, with respect to adiabatic damping due to longitudinally accelerating rf cavities. The minimum aperture scheme describes the regime that matches the beam ellipse to the acceptance ellipse of a cavity, allowing for the smallest possible aperture, for a given cavity length. A simple approximation of an rf cavity map is determined to allow changes in particle energy along a lattice, and acceleration is assumed only in the longitudinal direction.","sentences":["In this paper, a self-consistent transverse beam dynamics framework is demonstrated, that incorporates acceleration into the transverse beam dynamics studies for a proton linac machine.","Two focusing schemes are developed and discussed; the FODO-like scheme, and the minimum aperture scheme.","The FODO-like scheme is a simple scheme, requiring only one quadrupole per cavity.","The scheme is analytically solved to minimise the beam size at the cavity entrance/exit and ensures a constant beam size along the lattice, with respect to adiabatic damping due to longitudinally accelerating rf cavities.","The minimum aperture scheme describes the regime that matches the beam ellipse to the acceptance ellipse of a cavity, allowing for the smallest possible aperture, for a given cavity length.","A simple approximation of an rf cavity map is determined to allow changes in particle energy along a lattice, and acceleration is assumed only in the longitudinal direction."],"url":"http://arxiv.org/abs/2403.10212v1","category":"physics.acc-ph"}
{"created":"2024-03-15 11:01:33","title":"On the functoriality of refined unramified cohomology","abstract":"In this paper, we generalise the construction of the functorial pullback of refined unramified cohomology between smooth schemes, by following the ideas of Fulton's intersection theory and Rost's cycle modules. We also define standard actions of algebraic cycles on the refined unramified cohomology groups of smooth proper schemes avoiding Chow's moving lemma, which coincide with Schreieder's constructions for smooth projective schemes. As applications, we prove the projective bundle and blow-up formulas for refined unramified cohomology groups and we reduce the Rost nilpotence principle in characteristic zero to a statement concerning certain refined unramified cohomology groups. Moreover, we compute the refined unramified cohomology for smooth proper linear varieties and show that Rost's nilpotence principle holds for these varieties in characteristic zero.","sentences":["In this paper, we generalise the construction of the functorial pullback of refined unramified cohomology between smooth schemes, by following the ideas of Fulton's intersection theory and Rost's cycle modules.","We also define standard actions of algebraic cycles on the refined unramified cohomology groups of smooth proper schemes avoiding Chow's moving lemma, which coincide with Schreieder's constructions for smooth projective schemes.","As applications, we prove the projective bundle and blow-up formulas for refined unramified cohomology groups and we reduce the Rost nilpotence principle in characteristic zero to a statement concerning certain refined unramified cohomology groups.","Moreover, we compute the refined unramified cohomology for smooth proper linear varieties and show that Rost's nilpotence principle holds for these varieties in characteristic zero."],"url":"http://arxiv.org/abs/2403.10198v1","category":"math.AG"}
{"created":"2024-03-15 10:35:20","title":"Active nematic-isotropic interfaces on flat surfaces: effects of anchoring, ordering field and activity","abstract":"A surface in contact with the isotropic phase of a passive liquid crystal can induce nematic order over distances that range from microscopic to macroscopic when the nematic-isotropic interface undergoes an orientational-wetting transition. If the nematic is active, what happens to the interface? Does it propagate and, if it does, is its structure different from the passive one? In this paper, we address these questions. We investigate how the active nematic-isotropic interface is affected by the anchoring strength of the surface, the bulk ordering field and the activity. We find that while passive interfaces are one-dimensional the active ones exhibit two dynamical regimes: a passive-like regime and a propagating regime where the interfaces propagate until the entire domain is active nematic. Active interfaces break the translational symmetry within the interfacial plane above a threshold activity, where the active nematic fluctuations, which are ultimately responsible for the emergence of an active turbulent nematic phase, drive non-steady dynamical interfacial regimes.","sentences":["A surface in contact with the isotropic phase of a passive liquid crystal can induce nematic order over distances that range from microscopic to macroscopic when the nematic-isotropic interface undergoes an orientational-wetting transition.","If the nematic is active, what happens to the interface?","Does it propagate and, if it does, is its structure different from the passive one?","In this paper, we address these questions.","We investigate how the active nematic-isotropic interface is affected by the anchoring strength of the surface, the bulk ordering field and the activity.","We find that while passive interfaces are one-dimensional the active ones exhibit two dynamical regimes: a passive-like regime and a propagating regime where the interfaces propagate until the entire domain is active nematic.","Active interfaces break the translational symmetry within the interfacial plane above a threshold activity, where the active nematic fluctuations, which are ultimately responsible for the emergence of an active turbulent nematic phase, drive non-steady dynamical interfacial regimes."],"url":"http://arxiv.org/abs/2403.10178v1","category":"cond-mat.soft"}
{"created":"2024-03-15 09:50:00","title":"Is Abell 3667 an offset merger?","abstract":"Abell 3667 is a near-by (z=0.056) merging cluster with the most prominent cold front and a pair of two bright radio relics. Assuming a face-to-face merger scenario, the origin of the cold front is often considered to be a remnant core of the cluster stripped of its surrounding ICM. Sarazin et al. (2016) proposed an offset merger scenario in which the sub-cluster cores rotate after the first core-crossing. To distinguish between these scenarios, we revisited the ICM distribution and measured the line-of-sight bulk ICM velocity using a calibration technique proposed by Sanders et al. (2020). In the unsharp masked image, we identified several ICM features, some of which we detected for the first time. There is an enhancement of the X-ray surface brightness extending from the 1st BCG to the cold front, which is named the \"BCG-E tail\". The notable feature is the \"RG1 vortex\", which is a clockwise vortex-like enhancement with a radius of about 250 kpc connecting the 1st BCG to the radio galaxy (RG1). It is particularly enhanced near the north of the 1st BCG, which is named the \"BCG-N tail\". The thermodynamic map shows that the ICM in the RG1 vortex has a relatively high abundance of 0.5-0.6 solar compared to the surrounding regions. The ICM of the BCG-E tail also has high abundance, and low pseudo-entropy, and can be interpreted as the remnant of the ICM of the cluster core. Including its arc-like shape, the RG1 vortex supports the idea that the ICM around the cluster center is rotating, which is natural in an offset merger scenario. The results of the line-of-sight bulk ICM velocity measurement show that the ICM around the BCG-N tail is redshifted with a velocity difference of 940$\\pm$440 km s$^{-1}$ compared to the optical redshift of the 1st BCG. Other symptoms of diversity in the line-of-sight velocity of the ICM were also obtained and discussed in the context of the offset merger.","sentences":["Abell 3667 is a near-by (z=0.056) merging cluster with the most prominent cold front and a pair of two bright radio relics.","Assuming a face-to-face merger scenario, the origin of the cold front is often considered to be a remnant core of the cluster stripped of its surrounding ICM.","Sarazin et al. (2016) proposed an offset merger scenario in which the sub-cluster cores rotate after the first core-crossing.","To distinguish between these scenarios, we revisited the ICM distribution and measured the line-of-sight bulk ICM velocity using a calibration technique proposed by Sanders et al. (2020).","In the unsharp masked image, we identified several ICM features, some of which we detected for the first time.","There is an enhancement of the X-ray surface brightness extending from the 1st BCG to the cold front, which is named the \"BCG-E tail\".","The notable feature is the \"RG1 vortex\", which is a clockwise vortex-like enhancement with a radius of about 250 kpc connecting the 1st BCG to the radio galaxy (RG1).","It is particularly enhanced near the north of the 1st BCG, which is named the \"BCG-N tail\".","The thermodynamic map shows that the ICM in the RG1 vortex has a relatively high abundance of 0.5-0.6 solar compared to the surrounding regions.","The ICM of the BCG-E tail also has high abundance, and low pseudo-entropy, and can be interpreted as the remnant of the ICM of the cluster core.","Including its arc-like shape, the RG1 vortex supports the idea that the ICM around the cluster center is rotating, which is natural in an offset merger scenario.","The results of the line-of-sight bulk ICM velocity measurement show that the ICM around the BCG-N tail is redshifted with a velocity difference of 940$\\pm$440 km s$^{-1}$ compared to the optical redshift of the 1st BCG.","Other symptoms of diversity in the line-of-sight velocity of the ICM were also obtained and discussed in the context of the offset merger."],"url":"http://arxiv.org/abs/2403.10150v1","category":"astro-ph.HE"}
{"created":"2024-03-15 09:27:37","title":"Measurement of groomed event shape observables in deep-inelastic electron-proton scattering at HERA","abstract":"The H1 Collaboration at HERA reports the first measurement of groomed event shape observables in deep inelastic electron-proton scattering (DIS) at $\\sqrt{s}=319$ GeV, using data recorded between the years 2003 and 2007 with an integrated luminosity of $351$ pb$^{-1}$. Event shapes provide incisive probes of perturbative and non-perturbative QCD. Grooming techniques have been used for jet measurements in hadronic collisions; this paper presents the first application of grooming to DIS data. The analysis is carried out in the Breit frame, utilizing the novel Centauro jet clustering algorithm that is designed for DIS event topologies. Events are required to have squared momentum-transfer $Q^2 > 150$ GeV$^2$ and inelasticity $ 0.2 < y < 0.7$. We report measurements of the production cross section of groomed event 1-jettiness and groomed invariant mass for several choices of grooming parameter. Monte Carlo model calculations and analytic calculations based on Soft Collinear Effective Theory are compared to the measurements.","sentences":["The H1 Collaboration at HERA reports the first measurement of groomed event shape observables in deep inelastic electron-proton scattering (DIS) at $\\sqrt{s}=319$ GeV, using data recorded between the years 2003 and 2007 with an integrated luminosity of $351$ pb$^{-1}$. Event shapes provide incisive probes of perturbative and non-perturbative QCD.","Grooming techniques have been used for jet measurements in hadronic collisions; this paper presents the first application of grooming to DIS data.","The analysis is carried out in the Breit frame, utilizing the novel Centauro jet clustering algorithm that is designed for DIS event topologies.","Events are required to have squared momentum-transfer $Q^2 > 150$ GeV$^2$ and inelasticity $ 0.2 <","y < 0.7$. We report measurements of the production cross section of groomed event 1-jettiness and groomed invariant mass for several choices of grooming parameter.","Monte Carlo model calculations and analytic calculations based on Soft Collinear Effective Theory are compared to the measurements."],"url":"http://arxiv.org/abs/2403.10134v1","category":"hep-ex"}
{"created":"2024-03-15 09:22:48","title":"Optical read-out and control of antiferromagnetic Neel vector in altermagnets and beyond","abstract":"Finding methods for the most efficient and fastest detection and control of magnetic domains in antiferromagnets is presently among the main challenges of magnetic research at large. We analyse the problem of optical read-out and control of the antiferromagnetic Neel vector using symmetry analysis and the principles of equilibrium thermodynamics. Following the pioneering approach of Dzyaloshinksii, we divide all antiferromagnets in three classes. It is shown that, using the magneto-optical Faraday effect or other effects which scale linearly with the antiferromagnetic Neel vector, it is possible to distinguish antiferromagnetic domains with mutually opposite N\\'eel vectors in two of the three classes. Symmetry properties of one of these two classes are similar to those of altermagnets. The analysis also reveals multiple mechanisms to directly excite spins with light for practically every type of antiferromagnet.","sentences":["Finding methods for the most efficient and fastest detection and control of magnetic domains in antiferromagnets is presently among the main challenges of magnetic research at large.","We analyse the problem of optical read-out and control of the antiferromagnetic Neel vector using symmetry analysis and the principles of equilibrium thermodynamics.","Following the pioneering approach of Dzyaloshinksii, we divide all antiferromagnets in three classes.","It is shown that, using the magneto-optical Faraday effect or other effects which scale linearly with the antiferromagnetic Neel vector, it is possible to distinguish antiferromagnetic domains with mutually opposite N\\'eel vectors in two of the three classes.","Symmetry properties of one of these two classes are similar to those of altermagnets.","The analysis also reveals multiple mechanisms to directly excite spins with light for practically every type of antiferromagnet."],"url":"http://arxiv.org/abs/2403.10129v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-15 08:58:49","title":"Tokamak H-mode edge-SOL global turbulence simulations with an electromagnetic, transcollisional drift-fluid model","abstract":"The design of commercially feasible magnetic confinement fusion reactors strongly relies on the reduced turbulent transport in the plasma edge during operation in the high confinement mode (H-mode). We present first global turbulence simulations of the ASDEX Upgrade tokamak edge and scrape-off layer (SOL) in ITER baseline H-mode conditions. Reasonable agreement with the experiment is obtained for outboard mid-plane measurements of plasma density, electron and ion temperature, as well as the radial electric field. The radial heat transport is underpredicted by roughly a factor 2. These results were obtained with the GRILLIX code implementing a transcollisional, electromagnetic, global drift-fluid plasma model, coupled to diffusive neutrals. The transcollisional extensions include neoclassical corrections for the ion viscosity, as well as either a Landau-fluid or free-streaming limited model for the parallel heat conduction. Electromagnetic fluctuations are found to play a critical role in H-mode conditions. We investigate the structure of the significant $E \\times B$ flow shear, finding both neoclassical components as well as zonal flows. But unlike in L-mode, geodesic acoustic modes are not observed. The turbulence mode structure is mostly that of drift-Alfv\\'en waves. However, in the upper part of the pedestal, it is very weak and overshadowed by neoclassical transport. At the pedestal foot, on the other hand, we find instead the (electromagnetic) kinetic ballooning mode (KBM), most clearly just inside the separatrix. Our results pave the way towards predictive simulations of fusion reactors.","sentences":["The design of commercially feasible magnetic confinement fusion reactors strongly relies on the reduced turbulent transport in the plasma edge during operation in the high confinement mode (H-mode).","We present first global turbulence simulations of the ASDEX Upgrade tokamak edge and scrape-off layer (SOL) in ITER baseline H-mode conditions.","Reasonable agreement with the experiment is obtained for outboard mid-plane measurements of plasma density, electron and ion temperature, as well as the radial electric field.","The radial heat transport is underpredicted by roughly a factor 2.","These results were obtained with the GRILLIX code implementing a transcollisional, electromagnetic, global drift-fluid plasma model, coupled to diffusive neutrals.","The transcollisional extensions include neoclassical corrections for the ion viscosity, as well as either a Landau-fluid or free-streaming limited model for the parallel heat conduction.","Electromagnetic fluctuations are found to play a critical role in H-mode conditions.","We investigate the structure of the significant $E \\times B$ flow shear, finding both neoclassical components as well as zonal flows.","But unlike in L-mode, geodesic acoustic modes are not observed.","The turbulence mode structure is mostly that of drift-Alfv\\'en waves.","However, in the upper part of the pedestal, it is very weak and overshadowed by neoclassical transport.","At the pedestal foot, on the other hand, we find instead the (electromagnetic) kinetic ballooning mode (KBM), most clearly just inside the separatrix.","Our results pave the way towards predictive simulations of fusion reactors."],"url":"http://arxiv.org/abs/2403.10113v1","category":"physics.plasm-ph"}
