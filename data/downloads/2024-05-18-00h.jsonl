{"created":"2024-05-15 17:56:49","title":"Classifying geospatial objects from multiview aerial imagery using semantic meshes","abstract":"Aerial imagery is increasingly used in Earth science and natural resource management as a complement to labor-intensive ground-based surveys. Aerial systems can collect overlapping images that provide multiple views of each location from different perspectives. However, most prediction approaches (e.g. for tree species classification) use a single, synthesized top-down \"orthomosaic\" image as input that contains little to no information about the vertical aspects of objects and may include processing artifacts. We propose an alternate approach that generates predictions directly on the raw images and accurately maps these predictions into geospatial coordinates using semantic meshes. This method$\\unicode{x2013}$released as a user-friendly open-source toolkit$\\unicode{x2013}$enables analysts to use the highest quality data for predictions, capture information about the sides of objects, and leverage multiple viewpoints of each location for added robustness. We demonstrate the value of this approach on a new benchmark dataset of four forest sites in the western U.S. that consists of drone images, photogrammetry results, predicted tree locations, and species classification data derived from manual surveys. We show that our proposed multiview method improves classification accuracy from 53% to 75% relative to an orthomosaic baseline on a challenging cross-site tree species classification task.","sentences":["Aerial imagery is increasingly used in Earth science and natural resource management as a complement to labor-intensive ground-based surveys.","Aerial systems can collect overlapping images that provide multiple views of each location from different perspectives.","However, most prediction approaches (e.g. for tree species classification) use a single, synthesized top-down \"orthomosaic\" image as input that contains little to no information about the vertical aspects of objects and may include processing artifacts.","We propose an alternate approach that generates predictions directly on the raw images and accurately maps these predictions into geospatial coordinates using semantic meshes.","This method$\\unicode{x2013}$released as a user-friendly open-source toolkit$\\unicode{x2013}$enables analysts to use the highest quality data for predictions, capture information about the sides of objects, and leverage multiple viewpoints of each location for added robustness.","We demonstrate the value of this approach on a new benchmark dataset of four forest sites in the western U.S. that consists of drone images, photogrammetry results, predicted tree locations, and species classification data derived from manual surveys.","We show that our proposed multiview method improves classification accuracy from 53% to 75% relative to an orthomosaic baseline on a challenging cross-site tree species classification task."],"url":"http://arxiv.org/abs/2405.09544v1","category":"cs.CV"}
{"created":"2024-05-15 17:37:28","title":"Energy-Efficient Sleep Mode Optimization of 5G mmWave Networks Using Deep Contextual MAB","abstract":"Millimeter-wave (mmWave) networks, integral to 5G communication, offer a vast spectrum that addresses the issue of spectrum scarcity and enhances peak rate and capacity. However, their dense deployment, necessary to counteract propagation losses, leads to high power consumption. An effective strategy to reduce this energy consumption in mobile networks is the sleep mode optimization (SMO) of base stations (BSs). In this paper, we propose a novel SMO approach for mmWave BSs in a 3D urban environment. This approach, which incorporates a neural network (NN) based contextual multi-armed bandit (C-MAB) with an epsilon decay algorithm, accommodates the dynamic and diverse traffic of user equipment (UE) by clustering the UEs in their respective tracking areas (TAs). Our strategy includes beamforming, which helps reduce energy consumption from the UE side, while SMO minimizes energy use from the BS perspective. We extended our investigation to include Random, Epsilon Greedy, Upper Confidence Bound (UCB), and Load Based sleep mode (SM) strategies. We compared the performance of our proposed C-MAB based SM algorithm with those of All On and other alternative approaches. Simulation results show that our proposed method outperforms all other SM strategies in terms of the $10^{th}$ percentile of user rate and average throughput while demonstrating comparable average throughput to the All On approach. Importantly, it outperforms all approaches in terms of energy efficiency (EE).","sentences":["Millimeter-wave (mmWave) networks, integral to 5G communication, offer a vast spectrum that addresses the issue of spectrum scarcity and enhances peak rate and capacity.","However, their dense deployment, necessary to counteract propagation losses, leads to high power consumption.","An effective strategy to reduce this energy consumption in mobile networks is the sleep mode optimization (SMO) of base stations (BSs).","In this paper, we propose a novel SMO approach for mmWave BSs in a 3D urban environment.","This approach, which incorporates a neural network (NN) based contextual multi-armed bandit (C-MAB) with an epsilon decay algorithm, accommodates the dynamic and diverse traffic of user equipment (UE) by clustering the UEs in their respective tracking areas (TAs).","Our strategy includes beamforming, which helps reduce energy consumption from the UE side, while SMO minimizes energy use from the BS perspective.","We extended our investigation to include Random, Epsilon Greedy, Upper Confidence Bound (UCB), and Load Based sleep mode (SM) strategies.","We compared the performance of our proposed C-MAB based SM algorithm with those of All On and other alternative approaches.","Simulation results show that our proposed method outperforms all other SM strategies in terms of the $10^{th}$ percentile of user rate and average throughput while demonstrating comparable average throughput to the All On approach.","Importantly, it outperforms all approaches in terms of energy efficiency (EE)."],"url":"http://arxiv.org/abs/2405.09528v1","category":"eess.SP"}
{"created":"2024-05-15 17:24:34","title":"Towards a fully declarative neuro-symbolic language","abstract":"Neuro-symbolic systems (NeSy), which claim to combine the best of both learning and reasoning capabilities of artificial intelligence, are missing a core property of reasoning systems: Declarativeness. The lack of declarativeness is caused by the functional nature of neural predicates inherited from neural networks. We propose and implement a general framework for fully declarative neural predicates, which hence extends to fully declarative NeSy frameworks. We first show that the declarative extension preserves the learning and reasoning capabilities while being able to answer arbitrary queries while only being trained on a single query type.","sentences":["Neuro-symbolic systems (NeSy), which claim to combine the best of both learning and reasoning capabilities of artificial intelligence, are missing a core property of reasoning systems: Declarativeness.","The lack of declarativeness is caused by the functional nature of neural predicates inherited from neural networks.","We propose and implement a general framework for fully declarative neural predicates, which hence extends to fully declarative NeSy frameworks.","We first show that the declarative extension preserves the learning and reasoning capabilities while being able to answer arbitrary queries while only being trained on a single query type."],"url":"http://arxiv.org/abs/2405.09521v1","category":"cs.AI"}
{"created":"2024-05-15 16:58:35","title":"QueryNER: Segmentation of E-commerce Queries","abstract":"We present QueryNER, a manually-annotated dataset and accompanying model for e-commerce query segmentation. Prior work in sequence labeling for e-commerce has largely addressed aspect-value extraction which focuses on extracting portions of a product title or query for narrowly defined aspects. Our work instead focuses on the goal of dividing a query into meaningful chunks with broadly applicable types. We report baseline tagging results and conduct experiments comparing token and entity dropping for null and low recall query recovery. Challenging test sets are created using automatic transformations and show how simple data augmentation techniques can make the models more robust to noise. We make the QueryNER dataset publicly available.","sentences":["We present QueryNER, a manually-annotated dataset and accompanying model for e-commerce query segmentation.","Prior work in sequence labeling for e-commerce has largely addressed aspect-value extraction which focuses on extracting portions of a product title or query for narrowly defined aspects.","Our work instead focuses on the goal of dividing a query into meaningful chunks with broadly applicable types.","We report baseline tagging results and conduct experiments comparing token and entity dropping for null and low recall query recovery.","Challenging test sets are created using automatic transformations and show how simple data augmentation techniques can make the models more robust to noise.","We make the QueryNER dataset publicly available."],"url":"http://arxiv.org/abs/2405.09507v1","category":"cs.CL"}
{"created":"2024-05-15 16:44:54","title":"ParaNames 1.0: Creating an Entity Name Corpus for 400+ Languages using Wikidata","abstract":"We introduce ParaNames, a massively multilingual parallel name resource consisting of 140 million names spanning over 400 languages. Names are provided for 16.8 million entities, and each entity is mapped from a complex type hierarchy to a standard type (PER/LOC/ORG). Using Wikidata as a source, we create the largest resource of this type to date. We describe our approach to filtering and standardizing the data to provide the best quality possible. ParaNames is useful for multilingual language processing, both in defining tasks for name translation/transliteration and as supplementary data for tasks such as named entity recognition and linking. We demonstrate the usefulness of ParaNames on two tasks. First, we perform canonical name translation between English and 17 other languages. Second, we use it as a gazetteer for multilingual named entity recognition, obtaining performance improvements on all 10 languages evaluated.","sentences":["We introduce ParaNames, a massively multilingual parallel name resource consisting of 140 million names spanning over 400 languages.","Names are provided for 16.8 million entities, and each entity is mapped from a complex type hierarchy to a standard type (PER/LOC/ORG).","Using Wikidata as a source, we create the largest resource of this type to date.","We describe our approach to filtering and standardizing the data to provide the best quality possible.","ParaNames is useful for multilingual language processing, both in defining tasks for name translation/transliteration and as supplementary data for tasks such as named entity recognition and linking.","We demonstrate the usefulness of ParaNames on two tasks.","First, we perform canonical name translation between English and 17 other languages.","Second, we use it as a gazetteer for multilingual named entity recognition, obtaining performance improvements on all 10 languages evaluated."],"url":"http://arxiv.org/abs/2405.09496v1","category":"cs.CL"}
{"created":"2024-05-15 16:16:37","title":"Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task","abstract":"Knowledge graph embedding (KGE) has caught significant interest for its effectiveness in knowledge graph completion (KGC), specifically link prediction (LP), with recent KGE models cracking the LP benchmarks. Despite the rapidly growing literature, insufficient attention has been paid to the cooperation between humans and AI on KG. However, humans' capability to analyze graphs conceptually may further improve the efficacy of KGE models with semantic information. To this effect, we carefully designed a human-AI team (HAIT) system dubbed KG-HAIT, which harnesses the human insights on KG by leveraging fully human-designed ad-hoc dynamic programming (DP) on KG to produce human insightful feature (HIF) vectors that capture the subgraph structural feature and semantic similarities. By integrating HIF vectors into the training of KGE models, notable improvements are observed across various benchmarks and metrics, accompanied by accelerated model convergence. Our results underscore the effectiveness of human-designed DP in the task of LP, emphasizing the pivotal role of collaboration between humans and AI on KG. We open avenues for further exploration and innovation through KG-HAIT, paving the way towards more effective and insightful KG analysis techniques.","sentences":["Knowledge graph embedding (KGE) has caught significant interest for its effectiveness in knowledge graph completion (KGC), specifically link prediction (LP), with recent KGE models cracking the LP benchmarks.","Despite the rapidly growing literature, insufficient attention has been paid to the cooperation between humans and AI on KG.","However, humans' capability to analyze graphs conceptually may further improve the efficacy of KGE models with semantic information.","To this effect, we carefully designed a human-AI team (HAIT) system dubbed KG-HAIT, which harnesses the human insights on KG by leveraging fully human-designed ad-hoc dynamic programming (DP) on KG to produce human insightful feature (HIF) vectors that capture the subgraph structural feature and semantic similarities.","By integrating HIF vectors into the training of KGE models, notable improvements are observed across various benchmarks and metrics, accompanied by accelerated model convergence.","Our results underscore the effectiveness of human-designed DP in the task of LP, emphasizing the pivotal role of collaboration between humans and AI on KG.","We open avenues for further exploration and innovation through KG-HAIT, paving the way towards more effective and insightful KG analysis techniques."],"url":"http://arxiv.org/abs/2405.09477v1","category":"cs.LG"}
{"created":"2024-05-15 16:03:56","title":"Dominating Lengthscales of Zebrafish Collective Behaviour","abstract":"Collective behaviour in living systems is observed across many scales, from bacteria to insects, to fish shoals. Zebrafish have emerged as a model system amenable to laboratory study. Here we report a three-dimensional study of the collective dynamics of fifty zebrafish. We observed the emergence of collective behaviour changing between \\yy{ordered} to randomised, upon \\yy{adaptation} to new environmental conditions. We quantify the spatial and temporal correlation functions of the fish and identify two length scales, the persistence length and the nearest neighbour distance, that capture the essence of the behavioural changes. The ratio of the two length scales correlates robustly with the polarisation of collective motion that we explain with a reductionist model of self--propelled particles with alignment interactions.","sentences":["Collective behaviour in living systems is observed across many scales, from bacteria to insects, to fish shoals.","Zebrafish have emerged as a model system amenable to laboratory study.","Here we report a three-dimensional study of the collective dynamics of fifty zebrafish.","We observed the emergence of collective behaviour changing between \\yy{ordered} to randomised, upon \\yy{adaptation} to new environmental conditions.","We quantify the spatial and temporal correlation functions of the fish and identify two length scales, the persistence length and the nearest neighbour distance, that capture the essence of the behavioural changes.","The ratio of the two length scales correlates robustly with the polarisation of collective motion that we explain with a reductionist model of self--propelled particles with alignment interactions."],"url":"http://arxiv.org/abs/2405.09469v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 15:52:27","title":"Fourier Boundary Features Network with Wider Catchers for Glass Segmentation","abstract":"Glass largely blurs the boundary between the real world and the reflection. The special transmittance and reflectance quality have confused the semantic tasks related to machine vision. Therefore, how to clear the boundary built by glass, and avoid over-capturing features as false positive information in deep structure, matters for constraining the segmentation of reflection surface and penetrating glass. We proposed the Fourier Boundary Features Network with Wider Catchers (FBWC), which might be the first attempt to utilize sufficiently wide horizontal shallow branches without vertical deepening for guiding the fine granularity segmentation boundary through primary glass semantic information. Specifically, we designed the Wider Coarse-Catchers (WCC) for anchoring large area segmentation and reducing excessive extraction from a structural perspective. We embed fine-grained features by Cross Transpose Attention (CTA), which is introduced to avoid the incomplete area within the boundary caused by reflection noise. For excavating glass features and balancing high-low layers context, a learnable Fourier Convolution Controller (FCC) is proposed to regulate information integration robustly. The proposed method has been validated on three different public glass segmentation datasets. Experimental results reveal that the proposed method yields better segmentation performance compared with the state-of-the-art (SOTA) methods in glass image segmentation.","sentences":["Glass largely blurs the boundary between the real world and the reflection.","The special transmittance and reflectance quality have confused the semantic tasks related to machine vision.","Therefore, how to clear the boundary built by glass, and avoid over-capturing features as false positive information in deep structure, matters for constraining the segmentation of reflection surface and penetrating glass.","We proposed the Fourier Boundary Features Network with Wider Catchers (FBWC), which might be the first attempt to utilize sufficiently wide horizontal shallow branches without vertical deepening for guiding the fine granularity segmentation boundary through primary glass semantic information.","Specifically, we designed the Wider Coarse-Catchers (WCC) for anchoring large area segmentation and reducing excessive extraction from a structural perspective.","We embed fine-grained features by Cross Transpose Attention (CTA), which is introduced to avoid the incomplete area within the boundary caused by reflection noise.","For excavating glass features and balancing high-low layers context, a learnable Fourier Convolution Controller (FCC) is proposed to regulate information integration robustly.","The proposed method has been validated on three different public glass segmentation datasets.","Experimental results reveal that the proposed method yields better segmentation performance compared with the state-of-the-art (SOTA) methods in glass image segmentation."],"url":"http://arxiv.org/abs/2405.09459v1","category":"cs.CV"}
{"created":"2024-05-15 15:50:43","title":"Efficient pooling designs and screening performance in group testing for two type defectives","abstract":"Group testing is utilized in the case when we want to find a few defectives among large amount of items. Testing n items one by one requires n tests, but if the ratio of defectives is small, group testing is an efficient way to reduce the number of tests. Many research have been developed for group testing for a single type of defectives. In this paper, we consider the case where two types of defective A and B exist. For two types of defectives, we develop a belief propagation algorithm to compute marginal posterior probability of defectives. Furthermore, we construct several kinds of collections of pools in order to test for A and B. And by utilizing our belief propagation algorithm, we evaluate the performance of group testing by conducting simulations.","sentences":["Group testing is utilized in the case when we want to find a few defectives among large amount of items.","Testing n items one by one requires n tests, but if the ratio of defectives is small, group testing is an efficient way to reduce the number of tests.","Many research have been developed for group testing for a single type of defectives.","In this paper, we consider the case where two types of defective A and B exist.","For two types of defectives, we develop a belief propagation algorithm to compute marginal posterior probability of defectives.","Furthermore, we construct several kinds of collections of pools in order to test for A and B. And by utilizing our belief propagation algorithm, we evaluate the performance of group testing by conducting simulations."],"url":"http://arxiv.org/abs/2405.09455v1","category":"stat.CO"}
{"created":"2024-05-15 15:48:11","title":"Kuramoto Oscillators and Swarms on Manifolds for Geometry Informed Machine Learning","abstract":"We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets. These models are systems of matrix ODE's describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups. Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory. They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries. In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups). Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning. We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles. The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups.","sentences":["We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets.","These models are systems of matrix ODE's describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups.","Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory.","They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries.","In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups).","Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning.","We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles.","The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups."],"url":"http://arxiv.org/abs/2405.09453v1","category":"cs.LG"}
{"created":"2024-05-15 15:42:41","title":"M$^4$oE: A Foundation Model for Medical Multimodal Image Segmentation with Mixture of Experts","abstract":"Medical imaging data is inherently heterogeneous across different modalities and clinical centers, posing unique challenges for developing generalizable foundation models. Conventional entails training distinct models per dataset or using a shared encoder with modality-specific decoders. However, these approaches incur heavy computational overheads and suffer from poor scalability. To address these limitations, we propose the Medical Multimodal Mixture of Experts (M$^4$oE) framework, leveraging the SwinUNet architecture. Specifically, M$^4$oE comprises modality-specific experts; each separately initialized to learn features encoding domain knowledge. Subsequently, a gating network is integrated during fine-tuning to modulate each expert's contribution to the collective predictions dynamically. This enhances model interpretability and generalization ability while retaining expertise specialization. Simultaneously, the M$^4$oE architecture amplifies the model's parallel processing capabilities, and it also ensures the model's adaptation to new modalities with ease. Experiments across three modalities reveal that M$^4$oE can achieve 3.45% over STU-Net-L, 5.11% over MED3D, and 11.93% over SAM-Med2D across the MICCAI FLARE22, AMOS2022, and ATLAS2023 datasets. Moreover, M$^4$oE showcases a significant reduction in training duration with 7 hours less while maintaining a parameter count that is only 30% of its compared methods. The code is available at https://github.com/JefferyJiang-YF/M4oE.","sentences":["Medical imaging data is inherently heterogeneous across different modalities and clinical centers, posing unique challenges for developing generalizable foundation models.","Conventional entails training distinct models per dataset or using a shared encoder with modality-specific decoders.","However, these approaches incur heavy computational overheads and suffer from poor scalability.","To address these limitations, we propose the Medical Multimodal Mixture of Experts (M$^4$oE) framework, leveraging the SwinUNet architecture.","Specifically, M$^4$oE comprises modality-specific experts; each separately initialized to learn features encoding domain knowledge.","Subsequently, a gating network is integrated during fine-tuning to modulate each expert's contribution to the collective predictions dynamically.","This enhances model interpretability and generalization ability while retaining expertise specialization.","Simultaneously, the M$^4$oE architecture amplifies the model's parallel processing capabilities, and it also ensures the model's adaptation to new modalities with ease.","Experiments across three modalities reveal that M$^4$oE can achieve 3.45% over STU-Net-L, 5.11% over MED3D, and 11.93% over SAM-Med2D across the MICCAI FLARE22, AMOS2022, and ATLAS2023 datasets.","Moreover, M$^4$oE showcases a significant reduction in training duration with 7 hours less while maintaining a parameter count that is only 30% of its compared methods.","The code is available at https://github.com/JefferyJiang-YF/M4oE."],"url":"http://arxiv.org/abs/2405.09446v1","category":"eess.IV"}
{"created":"2024-05-15 15:39:35","title":"Desk-AId: Humanitarian Aid Desk Assessment with Geospatial AI for Predicting Landmine Areas","abstract":"The process of clearing areas, namely demining, starts by assessing and prioritizing potential hazardous areas (i.e., desk assessment) to go under thorough investigation of experts, who confirm the risk and proceed with the mines clearance operations. This paper presents Desk-AId that supports the desk assessment phase by estimating landmine risks using geospatial data and socioeconomic information. Desk-AId uses a Geospatial AI approach specialized to landmines. The approach includes mixed data sampling strategies and context-enrichment by historical conflicts and key multi-domain facilities (e.g., buildings, roads, health sites). The proposed system addresses the issue of having only ground-truth for confirmed hazardous areas by implementing a new hard-negative data sampling strategy, where negative points are sampled in the vicinity of hazardous areas. Experiments validate Desk-Aid in two domains for landmine risk assessment: 1) country-wide, and 2) uncharted study areas). The proposed approach increases the estimation accuracies up to 92%, for different classification models such as RandomForest (RF), Feedforward Neural Networks (FNN), and Graph Neural Networks (GNN).","sentences":["The process of clearing areas, namely demining, starts by assessing and prioritizing potential hazardous areas (i.e., desk assessment) to go under thorough investigation of experts, who confirm the risk and proceed with the mines clearance operations.","This paper presents Desk-AId that supports the desk assessment phase by estimating landmine risks using geospatial data and socioeconomic information.","Desk-AId uses a Geospatial AI approach specialized to landmines.","The approach includes mixed data sampling strategies and context-enrichment by historical conflicts and key multi-domain facilities (e.g., buildings, roads, health sites).","The proposed system addresses the issue of having only ground-truth for confirmed hazardous areas by implementing a new hard-negative data sampling strategy, where negative points are sampled in the vicinity of hazardous areas.","Experiments validate Desk-Aid in two domains for landmine risk assessment: 1) country-wide, and 2) uncharted study areas).","The proposed approach increases the estimation accuracies up to 92%, for different classification models such as RandomForest (RF), Feedforward Neural Networks (FNN), and Graph Neural Networks (GNN)."],"url":"http://arxiv.org/abs/2405.09444v1","category":"cs.CY"}
{"created":"2024-05-15 15:30:17","title":"Facilitating Opinion Diversity through Hybrid NLP Approaches","abstract":"Modern democracies face a critical issue of declining citizen participation in decision-making. Online discussion forums are an important avenue for enhancing citizen participation. This thesis proposal 1) identifies the challenges involved in facilitating large-scale online discussions with Natural Language Processing (NLP), 2) suggests solutions to these challenges by incorporating hybrid human-AI technologies, and 3) investigates what these technologies can reveal about individual perspectives in online discussions. We propose a three-layered hierarchy for representing perspectives that can be obtained by a mixture of human intelligence and large language models. We illustrate how these representations can draw insights into the diversity of perspectives and allow us to investigate interactions in online discussions.","sentences":["Modern democracies face a critical issue of declining citizen participation in decision-making.","Online discussion forums are an important avenue for enhancing citizen participation.","This thesis proposal 1) identifies the challenges involved in facilitating large-scale online discussions with Natural Language Processing (NLP), 2) suggests solutions to these challenges by incorporating hybrid human-AI technologies, and 3) investigates what these technologies can reveal about individual perspectives in online discussions.","We propose a three-layered hierarchy for representing perspectives that can be obtained by a mixture of human intelligence and large language models.","We illustrate how these representations can draw insights into the diversity of perspectives and allow us to investigate interactions in online discussions."],"url":"http://arxiv.org/abs/2405.09439v1","category":"cs.CL"}
{"created":"2024-05-15 15:17:04","title":"MicroPython Testbed for Federated Learning Algorithms","abstract":"Recently, Python Testbed for Federated Learning Algorithms emerged as a low code and generative large language models amenable framework for developing decentralized and distributed applications, primarily targeting edge systems, by nonprofessional programmers with the help of emerging artificial intelligence tools. This light framework is written in pure Python to be easy to install and to fit into a small IoT memory. It supports formally verified generic centralized and decentralized federated learning algorithms, as well as the peer-to-peer data exchange used in time division multiplexing communication, and its current main limitation is that all the application instances can run only on a single PC. This paper presents the MicroPyton Testbed for Federated Learning Algorithms, the new framework that overcomes its predecessor's limitation such that individual application instances may run on different network nodes like PCs and IoTs, primarily in edge systems. The new framework carries on the pure Python ideal, is based on asynchronous I/O abstractions, and runs on MicroPython, and therefore is a great match for IoTs and devices in edge systems. The new framework was experimentally validated on a wireless network comprising PCs and Raspberry Pi Pico W boards, by using application examples originally developed for the predecessor framework.","sentences":["Recently, Python Testbed for Federated Learning Algorithms emerged as a low code and generative large language models amenable framework for developing decentralized and distributed applications, primarily targeting edge systems, by nonprofessional programmers with the help of emerging artificial intelligence tools.","This light framework is written in pure Python to be easy to install and to fit into a small IoT memory.","It supports formally verified generic centralized and decentralized federated learning algorithms, as well as the peer-to-peer data exchange used in time division multiplexing communication, and its current main limitation is that all the application instances can run only on a single PC.","This paper presents the MicroPyton Testbed for Federated Learning Algorithms, the new framework that overcomes its predecessor's limitation such that individual application instances may run on different network nodes like PCs and IoTs, primarily in edge systems.","The new framework carries on the pure Python ideal, is based on asynchronous I/O abstractions, and runs on MicroPython, and therefore is a great match for IoTs and devices in edge systems.","The new framework was experimentally validated on a wireless network comprising PCs and Raspberry Pi Pico W boards, by using application examples originally developed for the predecessor framework."],"url":"http://arxiv.org/abs/2405.09423v1","category":"cs.DC"}
{"created":"2024-05-15 15:10:03","title":"On the Correspondence of Non-flat Assumption-based Argumentation and Logic Programming with Negation as Failure in the Head","abstract":"The relation between (a fragment of) assumption-based argumentation (ABA) and logic programs (LPs) under stable model semantics is well-studied. However, for obtaining this relation, the ABA framework needs to be restricted to being flat, i.e., a fragment where the (defeasible) assumptions can never be entailed, only assumed to be true or false. Here, we remove this restriction and show a correspondence between non-flat ABA and LPs with negation as failure in their head. We then extend this result to so-called set-stable ABA semantics, originally defined for the fragment of non-flat ABA called bipolar ABA. We showcase how to define set-stable semantics for LPs with negation as failure in their head and show the correspondence to set-stable ABA semantics.","sentences":["The relation between (a fragment of) assumption-based argumentation (ABA) and logic programs (LPs) under stable model semantics is well-studied.","However, for obtaining this relation, the ABA framework needs to be restricted to being flat, i.e., a fragment where the (defeasible) assumptions can never be entailed, only assumed to be true or false.","Here, we remove this restriction and show a correspondence between non-flat ABA and LPs with negation as failure in their head.","We then extend this result to so-called set-stable ABA semantics, originally defined for the fragment of non-flat ABA called bipolar ABA.","We showcase how to define set-stable semantics for LPs with negation as failure in their head and show the correspondence to set-stable ABA semantics."],"url":"http://arxiv.org/abs/2405.09415v1","category":"cs.AI"}
{"created":"2024-05-15 14:51:11","title":"$O_2$ is a multiple context-free grammar: an implementation-, formalisation-friendly proof","abstract":"Classifying formal languages according to the expressiveness of grammars able to generate them is a fundamental problem in computational linguistics and, therefore, in the theory of computation. Furthermore, such kind of analysis can give insight into the classification of abstract algebraic structure such as groups, for example through the correspondence given by the word problem. While many such classification problems remain open, others have been settled. Recently, it was proved that $n$-balanced languages (i.e., whose strings contain the same occurrences of letters $a_i$ and $A_i$ with $1\\leq i \\leq n$) can be generated by multiple context-free grammars (MCFGs), which are one of the several slight extensions of context free grammars added to the classical Chomsky hierarchy to make the mentioned classification more precise. This paper analyses the existing proofs from the computational and the proof-theoretical point of views, systematically studying whether each proof can lead to a verified (i.e., checked by a proof assistant) algorithm parsing balanced languages via MCFGs. We conclude that none of the existing proofs is realistically suitable against this practical goal, and proceed to provide a radically new, elementary, extremely short proof for the crucial case $n \\leq 2$. A comparative analysis with respect to the existing proofs is finally performed to justify why the proposed proof is a substantial step towards concretely obtaining a verified parsing algorithm for $O_2$.","sentences":["Classifying formal languages according to the expressiveness of grammars able to generate them is a fundamental problem in computational linguistics and, therefore, in the theory of computation.","Furthermore, such kind of analysis can give insight into the classification of abstract algebraic structure such as groups, for example through the correspondence given by the word problem.","While many such classification problems remain open, others have been settled.","Recently, it was proved that $n$-balanced languages (i.e., whose strings contain the same occurrences of letters $a_i$ and $A_i$ with $1\\leq i \\leq n$) can be generated by multiple context-free grammars (MCFGs), which are one of the several slight extensions of context free grammars added to the classical Chomsky hierarchy to make the mentioned classification more precise.","This paper analyses the existing proofs from the computational and the proof-theoretical point of views, systematically studying whether each proof can lead to a verified (i.e., checked by a proof assistant) algorithm parsing balanced languages via MCFGs.","We conclude that none of the existing proofs is realistically suitable against this practical goal, and proceed to provide a radically new, elementary, extremely short proof for the crucial case $n \\leq 2$.","A comparative analysis with respect to the existing proofs is finally performed to justify why the proposed proof is a substantial step towards concretely obtaining a verified parsing algorithm for $O_2$."],"url":"http://arxiv.org/abs/2405.09396v1","category":"cs.FL"}
{"created":"2024-05-15 14:50:51","title":"Matching domain experts by training from scratch on domain knowledge","abstract":"Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024). What is the basis for this performance? One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance. To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge. Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results. Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2. Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches.","sentences":["Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024).","What is the basis for this performance?","One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance.","To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.","Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results.","Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2.","Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches."],"url":"http://arxiv.org/abs/2405.09395v1","category":"q-bio.NC"}
{"created":"2024-05-15 14:09:11","title":"Vision-Based Neurosurgical Guidance: Unsupervised Localization and Camera-Pose Prediction","abstract":"Localizing oneself during endoscopic procedures can be problematic due to the lack of distinguishable textures and landmarks, as well as difficulties due to the endoscopic device such as a limited field of view and challenging lighting conditions. Expert knowledge shaped by years of experience is required for localization within the human body during endoscopic procedures. In this work, we present a deep learning method based on anatomy recognition, that constructs a surgical path in an unsupervised manner from surgical videos, modelling relative location and variations due to different viewing angles. At inference time, the model can map an unseen video's frames on the path and estimate the viewing angle, aiming to provide guidance, for instance, to reach a particular destination. We test the method on a dataset consisting of surgical videos of transsphenoidal adenomectomies, as well as on a synthetic dataset. An online tool that lets researchers upload their surgical videos to obtain anatomy detections and the weights of the trained YOLOv7 model are available at: https://surgicalvision.bmic.ethz.ch.","sentences":["Localizing oneself during endoscopic procedures can be problematic due to the lack of distinguishable textures and landmarks, as well as difficulties due to the endoscopic device such as a limited field of view and challenging lighting conditions.","Expert knowledge shaped by years of experience is required for localization within the human body during endoscopic procedures.","In this work, we present a deep learning method based on anatomy recognition, that constructs a surgical path in an unsupervised manner from surgical videos, modelling relative location and variations due to different viewing angles.","At inference time, the model can map an unseen video's frames on the path and estimate the viewing angle, aiming to provide guidance, for instance, to reach a particular destination.","We test the method on a dataset consisting of surgical videos of transsphenoidal adenomectomies, as well as on a synthetic dataset.","An online tool that lets researchers upload their surgical videos to obtain anatomy detections and the weights of the trained YOLOv7 model are available at: https://surgicalvision.bmic.ethz.ch."],"url":"http://arxiv.org/abs/2405.09355v1","category":"cs.CV"}
{"created":"2024-05-15 13:53:29","title":"Full-wave EM simulation analysis of human body blockage by dense 2D antenna arrays","abstract":"Recently, proposals of human-sensing-based services for cellular and local area networks have brought indoor localization to the attention of several research groups. In response to these stimuli, various Device-Free Localization (DFL) techniques, also known as Passive Localization methods, have emerged by exploiting ambient signals to locate and track individuals that do not carry any electronic device. This study delves into human passive indoor localization through full-wave electromagnetic simulations. For the scope, we exploit simulations from the commercial tool FEKO software that employs the Method of Moments (MoM). In particular, we collect and analyze the electric field values in a scenario constituted by a dense 2D/3D deployment of receivers in the presence of an anthropomorphic mobile target. The paper describes in detail the collected dataset and provides a first analysis based on a statistical approach. Possible use cases are also investigated through examples in the context of passive localization, sensing, and imaging.","sentences":["Recently, proposals of human-sensing-based services for cellular and local area networks have brought indoor localization to the attention of several research groups.","In response to these stimuli, various Device-Free Localization (DFL) techniques, also known as Passive Localization methods, have emerged by exploiting ambient signals to locate and track individuals that do not carry any electronic device.","This study delves into human passive indoor localization through full-wave electromagnetic simulations.","For the scope, we exploit simulations from the commercial tool FEKO software that employs the Method of Moments (MoM).","In particular, we collect and analyze the electric field values in a scenario constituted by a dense 2D/3D deployment of receivers in the presence of an anthropomorphic mobile target.","The paper describes in detail the collected dataset and provides a first analysis based on a statistical approach.","Possible use cases are also investigated through examples in the context of passive localization, sensing, and imaging."],"url":"http://arxiv.org/abs/2405.09346v1","category":"eess.SP"}
{"created":"2024-05-15 13:44:13","title":"Large Language Model Bias Mitigation from the Perspective of Knowledge Editing","abstract":"Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge. In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization. Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs.","sentences":["Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge.","In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization.","Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge.","Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs."],"url":"http://arxiv.org/abs/2405.09341v1","category":"cs.CL"}
{"created":"2024-05-15 13:34:07","title":"Content-Based Image Retrieval for Multi-Class Volumetric Radiology Images: A Benchmark Study","abstract":"While content-based image retrieval (CBIR) has been extensively studied in natural image retrieval, its application to medical images presents ongoing challenges, primarily due to the 3D nature of medical images. Recent studies have shown the potential use of pre-trained vision embeddings for CBIR in the context of radiology image retrieval. However, a benchmark for the retrieval of 3D volumetric medical images is still lacking, hindering the ability to objectively evaluate and compare the efficiency of proposed CBIR approaches in medical imaging. In this study, we extend previous work and establish a benchmark for region-based and multi-organ retrieval using the TotalSegmentator dataset (TS) with detailed multi-organ annotations. We benchmark embeddings derived from pre-trained supervised models on medical images against embeddings derived from pre-trained unsupervised models on non-medical images for 29 coarse and 104 detailed anatomical structures in volume and region levels. We adopt a late interaction re-ranking method inspired by text matching for image retrieval, comparing it against the original method proposed for volume and region retrieval achieving retrieval recall of 1.0 for diverse anatomical regions with a wide size range. The findings and methodologies presented in this paper provide essential insights and benchmarks for the development and evaluation of CBIR approaches in the context of medical imaging.","sentences":["While content-based image retrieval (CBIR) has been extensively studied in natural image retrieval, its application to medical images presents ongoing challenges, primarily due to the 3D nature of medical images.","Recent studies have shown the potential use of pre-trained vision embeddings for CBIR in the context of radiology image retrieval.","However, a benchmark for the retrieval of 3D volumetric medical images is still lacking, hindering the ability to objectively evaluate and compare the efficiency of proposed CBIR approaches in medical imaging.","In this study, we extend previous work and establish a benchmark for region-based and multi-organ retrieval using the TotalSegmentator dataset (TS) with detailed multi-organ annotations.","We benchmark embeddings derived from pre-trained supervised models on medical images against embeddings derived from pre-trained unsupervised models on non-medical images for 29 coarse and 104 detailed anatomical structures in volume and region levels.","We adopt a late interaction re-ranking method inspired by text matching for image retrieval, comparing it against the original method proposed for volume and region retrieval achieving retrieval recall of 1.0 for diverse anatomical regions with a wide size range.","The findings and methodologies presented in this paper provide essential insights and benchmarks for the development and evaluation of CBIR approaches in the context of medical imaging."],"url":"http://arxiv.org/abs/2405.09334v1","category":"cs.CV"}
{"created":"2024-05-15 13:22:39","title":"ReconBoost: Boosting Can Achieve Modality Reconcilement","abstract":"This paper explores a novel multi-modal alternating learning paradigm pursuing a reconciliation between the exploitation of uni-modal features and the exploration of cross-modal interactions. This is motivated by the fact that current paradigms of multi-modal learning tend to explore multi-modal features simultaneously. The resulting gradient prohibits further exploitation of the features in the weak modality, leading to modality competition, where the dominant modality overpowers the learning process. To address this issue, we study the modality-alternating learning paradigm to achieve reconcilement. Specifically, we propose a new method called ReconBoost to update a fixed modality each time. Herein, the learning objective is dynamically adjusted with a reconcilement regularization against competition with the historical models. By choosing a KL-based reconcilement, we show that the proposed method resembles Friedman's Gradient-Boosting (GB) algorithm, where the updated learner can correct errors made by others and help enhance the overall performance. The major difference with the classic GB is that we only preserve the newest model for each modality to avoid overfitting caused by ensembling strong learners. Furthermore, we propose a memory consolidation scheme and a global rectification scheme to make this strategy more effective. Experiments over six multi-modal benchmarks speak to the efficacy of the method. We release the code at https://github.com/huacong/ReconBoost.","sentences":["This paper explores a novel multi-modal alternating learning paradigm pursuing a reconciliation between the exploitation of uni-modal features and the exploration of cross-modal interactions.","This is motivated by the fact that current paradigms of multi-modal learning tend to explore multi-modal features simultaneously.","The resulting gradient prohibits further exploitation of the features in the weak modality, leading to modality competition, where the dominant modality overpowers the learning process.","To address this issue, we study the modality-alternating learning paradigm to achieve reconcilement.","Specifically, we propose a new method called ReconBoost to update a fixed modality each time.","Herein, the learning objective is dynamically adjusted with a reconcilement regularization against competition with the historical models.","By choosing a KL-based reconcilement, we show that the proposed method resembles Friedman's Gradient-Boosting (GB) algorithm, where the updated learner can correct errors made by others and help enhance the overall performance.","The major difference with the classic GB is that we only preserve the newest model for each modality to avoid overfitting caused by ensembling strong learners.","Furthermore, we propose a memory consolidation scheme and a global rectification scheme to make this strategy more effective.","Experiments over six multi-modal benchmarks speak to the efficacy of the method.","We release the code at https://github.com/huacong/ReconBoost."],"url":"http://arxiv.org/abs/2405.09321v1","category":"cs.CV"}
{"created":"2024-05-15 13:21:30","title":"Search for new physics in high-mass diphoton events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"Results are presented from a search for new physics in high-mass diphoton events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV. The data set was collected in 2016-2018 with the CMS detector at the LHC and corresponds to an integrated luminosity of 138 fb$^{-1}$. Events with a diphoton invariant mass greater than 500\\GeV are considered. Two different techniques are used to predict the standard model backgrounds: parametric fits to the smoothly-falling background and a first-principles calculation of the standard model diphoton spectrum at next-to-next-to-leading order in perturbative quantum chromodynamics calculations. The first technique is sensitive to resonant excesses while the second technique can identify broad differences in the invariant mass shape. The data are used to constrain the production of heavy Higgs bosons, Randall-Sundrum gravitons, the large extra dimensions model of Arkani-Hamed, Dimopoulos, and Dvali (ADD), and the continuum clockwork mechanism. No statistically significant excess is observed. The present results are the strongest limits to date on ADD extra dimensions and RS gravitons with a coupling parameter greater than 0.1.","sentences":["Results are presented from a search for new physics in high-mass diphoton events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV.","The data set was collected in 2016-2018 with the CMS detector at the LHC and corresponds to an integrated luminosity of 138 fb$^{-1}$. Events with a diphoton invariant mass greater than 500\\GeV are considered.","Two different techniques are used to predict the standard model backgrounds: parametric fits to the smoothly-falling background and a first-principles calculation of the standard model diphoton spectrum at next-to-next-to-leading order in perturbative quantum chromodynamics calculations.","The first technique is sensitive to resonant excesses while the second technique can identify broad differences in the invariant mass shape.","The data are used to constrain the production of heavy Higgs bosons, Randall-Sundrum gravitons, the large extra dimensions model of Arkani-Hamed, Dimopoulos, and Dvali (ADD), and the continuum clockwork mechanism.","No statistically significant excess is observed.","The present results are the strongest limits to date on ADD extra dimensions and RS gravitons with a coupling parameter greater than 0.1."],"url":"http://arxiv.org/abs/2405.09320v1","category":"hep-ex"}
{"created":"2024-05-15 13:11:29","title":"Vortex-comb spectroscopy","abstract":"We propose a new Fourier-transform spectroscopy technique based on the rotational Doppler effect. The technique offers an application for optical vortex frequency combs, where each frequency component carries a unique amount of orbital angular momentum (OAM). Here, we emulate a vortex comb using a tunable single frequency laser and a collection of spiral phase plates, generating up to eleven distinct OAM modes. Unlike in traditional Fourier-transform spectroscopy based on the Michelson interferometer (linear Doppler effect), the spectral resolution of vortex-comb spectroscopy is not limited by the mechanical scan distance of the instrument but only by the measurement time. Although the spectrometer requires just one free-running frequency comb, the down-conversion scheme resembles dual-comb spectroscopy, leading to fast mode-resolved measurements.","sentences":["We propose a new Fourier-transform spectroscopy technique based on the rotational Doppler effect.","The technique offers an application for optical vortex frequency combs, where each frequency component carries a unique amount of orbital angular momentum (OAM).","Here, we emulate a vortex comb using a tunable single frequency laser and a collection of spiral phase plates, generating up to eleven distinct OAM modes.","Unlike in traditional Fourier-transform spectroscopy based on the Michelson interferometer (linear Doppler effect), the spectral resolution of vortex-comb spectroscopy is not limited by the mechanical scan distance of the instrument but only by the measurement time.","Although the spectrometer requires just one free-running frequency comb, the down-conversion scheme resembles dual-comb spectroscopy, leading to fast mode-resolved measurements."],"url":"http://arxiv.org/abs/2405.09313v1","category":"physics.optics"}
{"created":"2024-05-15 13:11:28","title":"Agnostic Active Learning of Single Index Models with Linear Sample Complexity","abstract":"We study active learning methods for single index models of the form $F({\\mathbf x}) = f(\\langle {\\mathbf w}, {\\mathbf x}\\rangle)$, where $f:\\mathbb{R} \\to \\mathbb{R}$ and ${\\mathbf x,\\mathbf w} \\in \\mathbb{R}^d$. In addition to their theoretical interest as simple examples of non-linear neural networks, single index models have received significant recent attention due to applications in scientific machine learning like surrogate modeling for partial differential equations (PDEs). Such applications require sample-efficient active learning methods that are robust to adversarial noise. I.e., that work even in the challenging agnostic learning setting.   We provide two main results on agnostic active learning of single index models. First, when $f$ is known and Lipschitz, we show that $\\tilde{O}(d)$ samples collected via {statistical leverage score sampling} are sufficient to learn a near-optimal single index model. Leverage score sampling is simple to implement, efficient, and already widely used for actively learning linear models. Our result requires no assumptions on the data distribution, is optimal up to log factors, and improves quadratically on a recent ${O}(d^{2})$ bound of \\cite{gajjar2023active}. Second, we show that $\\tilde{O}(d)$ samples suffice even in the more difficult setting when $f$ is \\emph{unknown}. Our results leverage tools from high dimensional probability, including Dudley's inequality and dual Sudakov minoration, as well as a novel, distribution-aware discretization of the class of Lipschitz functions.","sentences":["We study active learning methods for single index models of the form $F({\\mathbf x})","= f(\\langle {\\mathbf w}, {\\mathbf x}\\rangle)$, where $f:\\mathbb{R} \\to \\mathbb{R}$ and ${\\mathbf x,\\mathbf w} \\in \\mathbb{R}^d$.","In addition to their theoretical interest as simple examples of non-linear neural networks, single index models have received significant recent attention due to applications in scientific machine learning like surrogate modeling for partial differential equations (PDEs).","Such applications require sample-efficient active learning methods that are robust to adversarial noise.","I.e., that work even in the challenging agnostic learning setting.   ","We provide two main results on agnostic active learning of single index models.","First, when $f$ is known and Lipschitz, we show that $\\tilde{O}(d)$ samples collected via {statistical leverage score sampling} are sufficient to learn a near-optimal single index model.","Leverage score sampling is simple to implement, efficient, and already widely used for actively learning linear models.","Our result requires no assumptions on the data distribution, is optimal up to log factors, and improves quadratically on a recent ${O}(d^{2})$ bound of \\cite{gajjar2023active}.","Second, we show that $\\tilde{O}(d)$ samples suffice even in the more difficult setting when $f$ is \\emph{unknown}.","Our results leverage tools from high dimensional probability, including Dudley's inequality and dual Sudakov minoration, as well as a novel, distribution-aware discretization of the class of Lipschitz functions."],"url":"http://arxiv.org/abs/2405.09312v1","category":"cs.LG"}
{"created":"2024-05-15 13:03:41","title":"TimeX++: Learning Time-Series Explanations with Information Bottleneck","abstract":"Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals. In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues. To address these issues, we introduce a simple yet practical objective function for time series explainable learning. The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues. We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving. We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application. Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}.","sentences":["Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals.","In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues.","To address these issues, we introduce a simple yet practical objective function for time series explainable learning.","The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues.","We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving.","We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application.","Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data.","The source code is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}."],"url":"http://arxiv.org/abs/2405.09308v1","category":"cs.LG"}
{"created":"2024-05-15 12:44:54","title":"Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A Blind Assessment of Large Language Models for Psychological Support","abstract":"Background: Rapid advancements in natural language processing have led to the development of large language models with the potential to revolutionize mental health care. These models have shown promise in assisting clinicians and providing support to individuals experiencing various psychological challenges.   Objective: This study aims to compare the performance of two large language models, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts, to assess their potential applicability in mental health care settings.   Methods: A blind methodology was employed, with a clinical psychologist evaluating the models' responses without knowledge of their origins. The prompts encompassed a diverse range of mental health topics, including depression, anxiety, and trauma, to ensure a comprehensive assessment.   Results: The results demonstrated a significant difference in performance between the two models (p > 0.05). GPT-4 achieved an average rating of 8.29 out of 10, while Chat-GPT received an average rating of 6.52. The clinical psychologist's evaluation suggested that GPT-4 was more effective at generating clinically relevant and empathetic responses, thereby providing better support and guidance to potential users.   Conclusions: This study contributes to the growing body of literature on the applicability of large language models in mental health care settings. The findings underscore the importance of continued research and development in the field to optimize these models for clinical use. Further investigation is necessary to understand the specific factors underlying the performance differences between the two models and to explore their generalizability across various populations and mental health conditions.","sentences":["Background: Rapid advancements in natural language processing have led to the development of large language models with the potential to revolutionize mental health care.","These models have shown promise in assisting clinicians and providing support to individuals experiencing various psychological challenges.   ","Objective: This study aims to compare the performance of two large language models, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts, to assess their potential applicability in mental health care settings.   ","Methods: A blind methodology was employed, with a clinical psychologist evaluating the models' responses without knowledge of their origins.","The prompts encompassed a diverse range of mental health topics, including depression, anxiety, and trauma, to ensure a comprehensive assessment.   ","Results:","The results demonstrated a significant difference in performance between the two models (p > 0.05).","GPT-4 achieved an average rating of 8.29 out of 10, while Chat-GPT received an average rating of 6.52.","The clinical psychologist's evaluation suggested that GPT-4 was more effective at generating clinically relevant and empathetic responses, thereby providing better support and guidance to potential users.   ","Conclusions: This study contributes to the growing body of literature on the applicability of large language models in mental health care settings.","The findings underscore the importance of continued research and development in the field to optimize these models for clinical use.","Further investigation is necessary to understand the specific factors underlying the performance differences between the two models and to explore their generalizability across various populations and mental health conditions."],"url":"http://arxiv.org/abs/2405.09300v1","category":"cs.CL"}
{"created":"2024-05-15 12:34:40","title":"Do language models capture implied discourse meanings? An investigation with exhaustivity implicatures of Korean morphology","abstract":"Markedness in natural language is often associated with non-literal meanings in discourse. Differential Object Marking (DOM) in Korean is one instance of this phenomenon, where post-positional markers are selected based on both the semantic features of the noun phrases and the discourse features that are orthogonal to the semantic features. Previous work has shown that distributional models of language recover certain semantic features of words -- do these models capture implied discourse-level meanings as well? We evaluate whether a set of large language models are capable of associating discourse meanings with different object markings in Korean. Results suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker.","sentences":["Markedness in natural language is often associated with non-literal meanings in discourse.","Differential Object Marking (DOM) in Korean is one instance of this phenomenon, where post-positional markers are selected based on both the semantic features of the noun phrases and the discourse features that are orthogonal to the semantic features.","Previous work has shown that distributional models of language recover certain semantic features of words -- do these models capture implied discourse-level meanings as well?","We evaluate whether a set of large language models are capable of associating discourse meanings with different object markings in Korean.","Results suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker."],"url":"http://arxiv.org/abs/2405.09293v1","category":"cs.CL"}
{"created":"2024-05-15 12:30:19","title":"Attribute reduction algorithm of rough sets based on spatial optimization","abstract":"Rough set is one of the important methods for rule acquisition and attribute reduction. The current goal of rough set attribute reduction focuses more on minimizing the number of reduced attributes, but ignores the spatial similarity between reduced and decision attributes, which may lead to problems such as increased number of rules and limited generality. In this paper, a rough set attribute reduction algorithm based on spatial optimization is proposed. By introducing the concept of spatial similarity, to find the reduction with the highest spatial similarity, so that the spatial similarity between reduction and decision attributes is higher, and more concise and widespread rules are obtained. In addition, a comparative experiment with the traditional rough set attribute reduction algorithms is designed to prove the effectiveness of the rough set attribute reduction algorithm based on spatial optimization, which has made significant improvements on many datasets.","sentences":["Rough set is one of the important methods for rule acquisition and attribute reduction.","The current goal of rough set attribute reduction focuses more on minimizing the number of reduced attributes, but ignores the spatial similarity between reduced and decision attributes, which may lead to problems such as increased number of rules and limited generality.","In this paper, a rough set attribute reduction algorithm based on spatial optimization is proposed.","By introducing the concept of spatial similarity, to find the reduction with the highest spatial similarity, so that the spatial similarity between reduction and decision attributes is higher, and more concise and widespread rules are obtained.","In addition, a comparative experiment with the traditional rough set attribute reduction algorithms is designed to prove the effectiveness of the rough set attribute reduction algorithm based on spatial optimization, which has made significant improvements on many datasets."],"url":"http://arxiv.org/abs/2405.09292v1","category":"cs.AI"}
{"created":"2024-05-15 12:29:35","title":"Sensitivity Decouple Learning for Image Compression Artifacts Reduction","abstract":"With the benefit of deep learning techniques, recent researches have made significant progress in image compression artifacts reduction. Despite their improved performances, prevailing methods only focus on learning a mapping from the compressed image to the original one but ignore the intrinsic attributes of the given compressed images, which greatly harms the performance of downstream parsing tasks. Different from these methods, we propose to decouple the intrinsic attributes into two complementary features for artifacts reduction,ie, the compression-insensitive features to regularize the high-level semantic representations during training and the compression-sensitive features to be aware of the compression degree. To achieve this, we first employ adversarial training to regularize the compressed and original encoded features for retaining high-level semantics, and we then develop the compression quality-aware feature encoder for compression-sensitive features. Based on these dual complementary features, we propose a Dual Awareness Guidance Network (DAGN) to utilize these awareness features as transformation guidance during the decoding phase. In our proposed DAGN, we develop a cross-feature fusion module to maintain the consistency of compression-insensitive features by fusing compression-insensitive features into the artifacts reduction baseline. Our method achieves an average 2.06 dB PSNR gains on BSD500, outperforming state-of-the-art methods, and only requires 29.7 ms to process one image on BSD500. Besides, the experimental results on LIVE1 and LIU4K also demonstrate the efficiency, effectiveness, and superiority of the proposed method in terms of quantitative metrics, visual quality, and downstream machine vision tasks.","sentences":["With the benefit of deep learning techniques, recent researches have made significant progress in image compression artifacts reduction.","Despite their improved performances, prevailing methods only focus on learning a mapping from the compressed image to the original one but ignore the intrinsic attributes of the given compressed images, which greatly harms the performance of downstream parsing tasks.","Different from these methods, we propose to decouple the intrinsic attributes into two complementary features for artifacts reduction,ie, the compression-insensitive features to regularize the high-level semantic representations during training and the compression-sensitive features to be aware of the compression degree.","To achieve this, we first employ adversarial training to regularize the compressed and original encoded features for retaining high-level semantics, and we then develop the compression quality-aware feature encoder for compression-sensitive features.","Based on these dual complementary features, we propose a Dual Awareness Guidance Network (DAGN) to utilize these awareness features as transformation guidance during the decoding phase.","In our proposed DAGN, we develop a cross-feature fusion module to maintain the consistency of compression-insensitive features by fusing compression-insensitive features into the artifacts reduction baseline.","Our method achieves an average 2.06 dB PSNR gains on BSD500, outperforming state-of-the-art methods, and only requires 29.7 ms to process one image on BSD500.","Besides, the experimental results on LIVE1 and LIU4K also demonstrate the efficiency, effectiveness, and superiority of the proposed method in terms of quantitative metrics, visual quality, and downstream machine vision tasks."],"url":"http://arxiv.org/abs/2405.09291v1","category":"cs.CV"}
{"created":"2024-05-15 11:55:14","title":"Sign of the Times: Evaluating the use of Large Language Models for Idiomaticity Detection","abstract":"Despite the recent ubiquity of large language models and their high zero-shot prompted performance across a wide range of tasks, it is still not known how well they perform on tasks which require processing of potentially idiomatic language. In particular, how well do such models perform in comparison to encoder-only models fine-tuned specifically for idiomaticity tasks? In this work, we attempt to answer this question by looking at the performance of a range of LLMs (both local and software-as-a-service models) on three idiomaticity datasets: SemEval 2022 Task 2a, FLUTE, and MAGPIE. Overall, we find that whilst these models do give competitive performance, they do not match the results of fine-tuned task-specific models, even at the largest scales (e.g. for GPT-4). Nevertheless, we do see consistent performance improvements across model scale. Additionally, we investigate prompting approaches to improve performance, and discuss the practicalities of using LLMs for these tasks.","sentences":["Despite the recent ubiquity of large language models and their high zero-shot prompted performance across a wide range of tasks, it is still not known how well they perform on tasks which require processing of potentially idiomatic language.","In particular, how well do such models perform in comparison to encoder-only models fine-tuned specifically for idiomaticity tasks?","In this work, we attempt to answer this question by looking at the performance of a range of LLMs (both local and software-as-a-service models) on three idiomaticity datasets:","SemEval 2022 Task 2a, FLUTE, and MAGPIE.","Overall, we find that whilst these models do give competitive performance, they do not match the results of fine-tuned task-specific models, even at the largest scales (e.g. for GPT-4).","Nevertheless, we do see consistent performance improvements across model scale.","Additionally, we investigate prompting approaches to improve performance, and discuss the practicalities of using LLMs for these tasks."],"url":"http://arxiv.org/abs/2405.09279v1","category":"cs.CL"}
{"created":"2024-05-15 11:46:47","title":"Dual-Segment Clustering Strategy for Federated Learning in Heterogeneous Environments","abstract":"Federated learning (FL) is a distributed machine learning paradigm with high efficiency and low communication load, only transmitting parameters or gradients of network. However, the non-independent and identically distributed (Non-IID) data characteristic has a negative impact on this paradigm. Furthermore, the heterogeneity of communication quality will significantly affect the accuracy of parameter transmission, causing a degradation in the performance of the FL system or even preventing its convergence. This letter proposes a dual-segment clustering (DSC) strategy, which first clusters the clients according to the heterogeneous communication conditions and then performs a second clustering by the sample size and label distribution, so as to solve the problem of data and communication heterogeneity. Experimental results show that the DSC strategy proposed in this letter can improve the convergence rate of FL, and has superiority on accuracy in a heterogeneous environment compared with the classical algorithm of cluster.","sentences":["Federated learning (FL) is a distributed machine learning paradigm with high efficiency and low communication load, only transmitting parameters or gradients of network.","However, the non-independent and identically distributed (Non-IID) data characteristic has a negative impact on this paradigm.","Furthermore, the heterogeneity of communication quality will significantly affect the accuracy of parameter transmission, causing a degradation in the performance of the FL system or even preventing its convergence.","This letter proposes a dual-segment clustering (DSC) strategy, which first clusters the clients according to the heterogeneous communication conditions and then performs a second clustering by the sample size and label distribution, so as to solve the problem of data and communication heterogeneity.","Experimental results show that the DSC strategy proposed in this letter can improve the convergence rate of FL, and has superiority on accuracy in a heterogeneous environment compared with the classical algorithm of cluster."],"url":"http://arxiv.org/abs/2405.09276v1","category":"cs.LG"}
{"created":"2024-05-15 11:42:41","title":"Fair Generalized Linear Mixed Models","abstract":"When using machine learning for automated prediction, it is important to account for fairness in the prediction. Fairness in machine learning aims to ensure that biases in the data and model inaccuracies do not lead to discriminatory decisions. E.g., predictions from fair machine learning models should not discriminate against sensitive variables such as sexual orientation and ethnicity. The training data often in obtained from social surveys. In social surveys, oftentimes the data collection process is a strata sampling, e.g. due to cost restrictions. In strata samples, the assumption of independence between the observation is not fulfilled. Hence, if the machine learning models do not account for the strata correlations, the results may be biased. Especially high is the bias in cases where the strata assignment is correlated to the variable of interest. We present in this paper an algorithm that can handle both problems simultaneously, and we demonstrate the impact of stratified sampling on the quality of fair machine learning predictions in a reproducible simulation study.","sentences":["When using machine learning for automated prediction, it is important to account for fairness in the prediction.","Fairness in machine learning aims to ensure that biases in the data and model inaccuracies do not lead to discriminatory decisions.","E.g., predictions from fair machine learning models should not discriminate against sensitive variables such as sexual orientation and ethnicity.","The training data often in obtained from social surveys.","In social surveys, oftentimes the data collection process is a strata sampling, e.g. due to cost restrictions.","In strata samples, the assumption of independence between the observation is not fulfilled.","Hence, if the machine learning models do not account for the strata correlations, the results may be biased.","Especially high is the bias in cases where the strata assignment is correlated to the variable of interest.","We present in this paper an algorithm that can handle both problems simultaneously, and we demonstrate the impact of stratified sampling on the quality of fair machine learning predictions in a reproducible simulation study."],"url":"http://arxiv.org/abs/2405.09273v1","category":"cs.LG"}
{"created":"2024-05-15 11:38:10","title":"Preconceptual Modeling in Software Engineering: Metaphysics of Diagrammatic Representations","abstract":"According to many researchers, conceptual model (CM) development is a hard task, and system requirements are difficult to collect, causing many miscommunication problems. CMs require more than modeling ability alone - they first require an understanding of the targeted domain that the model attempts to represent. Accordingly, a preconceptual modeling (pre-CM) stage is intended to address ontological issues before typical CM development is initiated. It involves defining a portion of reality when entities and processes are differentiated and integrated as unified wholes. This pre-CM phase forms the focus of research in this paper. The purpose is not show how to model; rather, it is to demonstrate how to establish a metaphysical basis of the involved portion of reality. To demonstrate such a venture, we employ the so-called thinging machine (TM) modeling that has been proposed as a high-level CM. A TM model integrates staticity and dynamism grounded in a fundamental construct called a thimac (things/machine). It involves two modes of reality, existence (events) and subsistence (regions - roughly, specifications of things and processes). Currently, the dominant approach in CM has evolved to limit its scope of application to develop ontological categorization (types of things). In the TM approach, pre-CM metaphysics is viewed as a part and parcel of CM itself. The general research problem is how to map TM constructs to what is out there in the targeted domain. Discussions involve the nature of thimacs (things and processes) and subsistence and existence as they are superimposed over each other in reality. Specifically, we make two claims, (a) the perceptibility of regions as a phenomenon and (b) the distinctiveness of existence as a construct for events. The results contribute to further the understanding of TM modeling in addition to introducing some metaphysical insights.","sentences":["According to many researchers, conceptual model (CM) development is a hard task, and system requirements are difficult to collect, causing many miscommunication problems.","CMs require more than modeling ability alone - they first require an understanding of the targeted domain that the model attempts to represent.","Accordingly, a preconceptual modeling (pre-CM) stage is intended to address ontological issues before typical CM development is initiated.","It involves defining a portion of reality when entities and processes are differentiated and integrated as unified wholes.","This pre-CM phase forms the focus of research in this paper.","The purpose is not show how to model; rather, it is to demonstrate how to establish a metaphysical basis of the involved portion of reality.","To demonstrate such a venture, we employ the so-called thinging machine (TM) modeling that has been proposed as a high-level CM.","A TM model integrates staticity and dynamism grounded in a fundamental construct called a thimac (things/machine).","It involves two modes of reality, existence (events) and subsistence (regions - roughly, specifications of things and processes).","Currently, the dominant approach in CM has evolved to limit its scope of application to develop ontological categorization (types of things).","In the TM approach, pre-CM metaphysics is viewed as a part and parcel of CM itself.","The general research problem is how to map TM constructs to what is out there in the targeted domain.","Discussions involve the nature of thimacs (things and processes) and subsistence and existence as they are superimposed over each other in reality.","Specifically, we make two claims, (a) the perceptibility of regions as a phenomenon and (b) the distinctiveness of existence as a construct for events.","The results contribute to further the understanding of TM modeling in addition to introducing some metaphysical insights."],"url":"http://arxiv.org/abs/2405.09269v1","category":"cs.SE"}
{"created":"2024-05-15 11:33:07","title":"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation","abstract":"The task of generating dance from music is crucial, yet current methods, which mainly produce joint sequences, lead to outputs that lack intuitiveness and complicate data collection due to the necessity for precise joint annotations. We introduce a Dance Any Beat Diffusion model, namely DabFusion, that employs music as a conditional input to directly create dance videos from still images, utilizing conditional image-to-video generation principles. This approach pioneers the use of music as a conditioning factor in image-to-video synthesis. Our method unfolds in two stages: training an auto-encoder to predict latent optical flow between reference and driving frames, eliminating the need for joint annotation, and training a U-Net-based diffusion model to produce these latent optical flows guided by music rhythm encoded by CLAP. Although capable of producing high-quality dance videos, the baseline model struggles with rhythm alignment. We enhance the model by adding beat information, improving synchronization. We introduce a 2D motion-music alignment score (2D-MM Align) for quantitative assessment. Evaluated on the AIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align score and established metrics. Video results can be found on our project page: https://DabFusion.github.io.","sentences":["The task of generating dance from music is crucial, yet current methods, which mainly produce joint sequences, lead to outputs that lack intuitiveness and complicate data collection due to the necessity for precise joint annotations.","We introduce a Dance Any Beat Diffusion model, namely DabFusion, that employs music as a conditional input to directly create dance videos from still images, utilizing conditional image-to-video generation principles.","This approach pioneers the use of music as a conditioning factor in image-to-video synthesis.","Our method unfolds in two stages: training an auto-encoder to predict latent optical flow between reference and driving frames, eliminating the need for joint annotation, and training a U-Net-based diffusion model to produce these latent optical flows guided by music rhythm encoded by CLAP.","Although capable of producing high-quality dance videos, the baseline model struggles with rhythm alignment.","We enhance the model by adding beat information, improving synchronization.","We introduce a 2D motion-music alignment score (2D-MM Align) for quantitative assessment.","Evaluated on the AIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align score and established metrics.","Video results can be found on our project page: https://DabFusion.github.io."],"url":"http://arxiv.org/abs/2405.09266v1","category":"cs.CV"}
{"created":"2024-05-15 11:14:33","title":"Reinforcement Learning-Based Framework for the Intelligent Adaptation of User Interfaces","abstract":"Adapting the user interface (UI) of software systems to meet the needs and preferences of users is a complex task. The main challenge is to provide the appropriate adaptations at the appropriate time to offer value to end-users. Recent advances in Machine Learning (ML) techniques may provide effective means to support the adaptation process. In this paper, we instantiate a reference framework for Intelligent User Interface Adaptation by using Reinforcement Learning (RL) as the ML component to adapt user interfaces and ultimately improving the overall User Experience (UX). By using RL, the system is able to learn from past adaptations to improve the decision-making capabilities. Moreover, assessing the success of such adaptations remains a challenge. To overcome this issue, we propose to use predictive Human-Computer Interaction (HCI) models to evaluate the outcome of each action (ie adaptations) performed by the RL agent. In addition, we present an implementation of the instantiated framework, which is an extension of OpenAI Gym, that serves as a toolkit for developing and comparing RL algorithms. This Gym environment is highly configurable and extensible to other UI adaptation contexts. The evaluation results show that our RL-based framework can successfully train RL agents able to learn how to adapt UIs in a specific context to maximize the user engagement by using an HCI model as rewards predictor.","sentences":["Adapting the user interface (UI) of software systems to meet the needs and preferences of users is a complex task.","The main challenge is to provide the appropriate adaptations at the appropriate time to offer value to end-users.","Recent advances in Machine Learning (ML) techniques may provide effective means to support the adaptation process.","In this paper, we instantiate a reference framework for Intelligent User Interface Adaptation by using Reinforcement Learning (RL) as the ML component to adapt user interfaces and ultimately improving the overall User Experience (UX).","By using RL, the system is able to learn from past adaptations to improve the decision-making capabilities.","Moreover, assessing the success of such adaptations remains a challenge.","To overcome this issue, we propose to use predictive Human-Computer Interaction (HCI) models to evaluate the outcome of each action (ie adaptations) performed by the RL agent.","In addition, we present an implementation of the instantiated framework, which is an extension of OpenAI Gym, that serves as a toolkit for developing and comparing RL algorithms.","This Gym environment is highly configurable and extensible to other UI adaptation contexts.","The evaluation results show that our RL-based framework can successfully train RL agents able to learn how to adapt UIs in a specific context to maximize the user engagement by using an HCI model as rewards predictor."],"url":"http://arxiv.org/abs/2405.09255v1","category":"cs.HC"}
{"created":"2024-05-15 10:30:53","title":"Design and commissioning of a high-level control system for a medical isochronous cyclotron","abstract":"MEDICYC (MEDical CYClotron) is an isochronous cyclotron dedicated to radiotherapy which was built and commissioned in Nice, France, in 1990 by a local team aided by experts from CERN. The cyclotron accelerates negative H to a maximum energy of 65 MeV and uses stripping to extract a proton beam. Its primary purpose is treating ocular melanoma by protontherapy but a significant research activity is also present on beam-lines dedicated for this purpose. An extensive refurbishment program of the cyclotron has been started to cope with the end-of-life and/or the obsolescence of several sub-systems. In this context, a new high-level cyclotron control system has been developed and commissioned in 2021-2024. The primary responsibility of the system is the high-level coordination of the source, the RF system, the beam-line and cyclotron magnets, to produce and deliver a beam with a given set of characteristics. A secondary responsibility is the collection, visualization and analysis of sub-system and beam data for monitoring and pre-emptive fault detection. In this contribution, the control system software architecture is presented and the infrastructure on which the systems are deployed is laid out.","sentences":["MEDICYC (MEDical CYClotron) is an isochronous cyclotron dedicated to radiotherapy which was built and commissioned in Nice, France, in 1990 by a local team aided by experts from CERN.","The cyclotron accelerates negative H to a maximum energy of 65 MeV and uses stripping to extract a proton beam.","Its primary purpose is treating ocular melanoma by protontherapy but a significant research activity is also present on beam-lines dedicated for this purpose.","An extensive refurbishment program of the cyclotron has been started to cope with the end-of-life and/or the obsolescence of several sub-systems.","In this context, a new high-level cyclotron control system has been developed and commissioned in 2021-2024.","The primary responsibility of the system is the high-level coordination of the source, the RF system, the beam-line and cyclotron magnets, to produce and deliver a beam with a given set of characteristics.","A secondary responsibility is the collection, visualization and analysis of sub-system and beam data for monitoring and pre-emptive fault detection.","In this contribution, the control system software architecture is presented and the infrastructure on which the systems are deployed is laid out."],"url":"http://arxiv.org/abs/2405.09235v1","category":"physics.acc-ph"}
{"created":"2024-05-15 10:04:44","title":"Perception-Inspired Graph Convolution for Music Understanding Tasks","abstract":"We propose a new graph convolutional block, called MusGConv, specifically designed for the efficient processing of musical score data and motivated by general perceptual principles. It focuses on two fundamental dimensions of music, pitch and rhythm, and considers both relative and absolute representations of these components. We evaluate our approach on four different musical understanding problems: monophonic voice separation, harmonic analysis, cadence detection, and composer identification which, in abstract terms, translate to different graph learning problems, namely, node classification, link prediction, and graph classification. Our experiments demonstrate that MusGConv improves the performance on three of the aforementioned tasks while being conceptually very simple and efficient. We interpret this as evidence that it is beneficial to include perception-informed processing of fundamental musical concepts when developing graph network applications on musical score data.","sentences":["We propose a new graph convolutional block, called MusGConv, specifically designed for the efficient processing of musical score data and motivated by general perceptual principles.","It focuses on two fundamental dimensions of music, pitch and rhythm, and considers both relative and absolute representations of these components.","We evaluate our approach on four different musical understanding problems: monophonic voice separation, harmonic analysis, cadence detection, and composer identification which, in abstract terms, translate to different graph learning problems, namely, node classification, link prediction, and graph classification.","Our experiments demonstrate that MusGConv improves the performance on three of the aforementioned tasks while being conceptually very simple and efficient.","We interpret this as evidence that it is beneficial to include perception-informed processing of fundamental musical concepts when developing graph network applications on musical score data."],"url":"http://arxiv.org/abs/2405.09224v1","category":"cs.SD"}
{"created":"2024-05-15 10:04:19","title":"Word Alignment as Preference for Machine Translation","abstract":"The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission.","sentences":["The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena.","In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment.","We first study the correlation between word alignment and the phenomena of hallucination and omission in MT.","Then we propose to utilize word alignment as preference to optimize the LLM-based MT model.","The preference data are constructed by selecting chosen and rejected translations from multiple MT tools.","Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal.","Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues.","We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission."],"url":"http://arxiv.org/abs/2405.09223v1","category":"cs.CL"}
{"created":"2024-05-15 10:03:22","title":"Anchor Layout Optimization for Ultrasonic Indoor Positioning Using Swarm Intelligence","abstract":"Indoor positioning applications are craving for ever higher precision and accuracy across the entire coverage zone. Optimal anchor placement and the deployment of multiple distributed anchor nodes could have a major impact in this regard. This paper examines the influences of these two difficult to approach hypotheses by means of a straightforward ultrasonic 3D indoor positioning system deployed in a real-life scenario via a geometric based simulation framework. To obtain an optimal anchor placement, a particle swarm optimization (PSO) algorithm is introduced and consequently performed for setups ranging from 4 to 10 anchors. In this way, besides the optimal anchor placement layout, the influence of deploying several distributed anchor nodes is investigated. In order to theoretically compare the optimization progress, a system model and Cram\\'er-Rao lower bound (CRLB) are established and the results are quantified based on the simulation data. With limited anchors, the placement is crucial to obtain a high precision high reliability (HPHR) indoor positioning system (IPS), while the addition of anchors, to a lesser extent, gives a supplementary improvement.","sentences":["Indoor positioning applications are craving for ever higher precision and accuracy across the entire coverage zone.","Optimal anchor placement and the deployment of multiple distributed anchor nodes could have a major impact in this regard.","This paper examines the influences of these two difficult to approach hypotheses by means of a straightforward ultrasonic 3D indoor positioning system deployed in a real-life scenario via a geometric based simulation framework.","To obtain an optimal anchor placement, a particle swarm optimization (PSO) algorithm is introduced and consequently performed for setups ranging from 4 to 10 anchors.","In this way, besides the optimal anchor placement layout, the influence of deploying several distributed anchor nodes is investigated.","In order to theoretically compare the optimization progress, a system model and Cram\\'er-Rao lower bound (CRLB) are established and the results are quantified based on the simulation data.","With limited anchors, the placement is crucial to obtain a high precision high reliability (HPHR) indoor positioning system (IPS), while the addition of anchors, to a lesser extent, gives a supplementary improvement."],"url":"http://arxiv.org/abs/2405.09222v1","category":"eess.SP"}
{"created":"2024-05-15 10:02:47","title":"Bridging the gap in online hate speech detection: a comparative analysis of BERT and traditional models for homophobic content identification on X/Twitter","abstract":"Our study addresses a significant gap in online hate speech detection research by focusing on homophobia, an area often neglected in sentiment analysis research. Utilising advanced sentiment analysis models, particularly BERT, and traditional machine learning methods, we developed a nuanced approach to identify homophobic content on X/Twitter. This research is pivotal due to the persistent underrepresentation of homophobia in detection models. Our findings reveal that while BERT outperforms traditional methods, the choice of validation technique can impact model performance. This underscores the importance of contextual understanding in detecting nuanced hate speech. By releasing the largest open-source labelled English dataset for homophobia detection known to us, an analysis of various models' performance and our strongest BERT-based model, we aim to enhance online safety and inclusivity. Future work will extend to broader LGBTQIA+ hate speech detection, addressing the challenges of sourcing diverse datasets. Through this endeavour, we contribute to the larger effort against online hate, advocating for a more inclusive digital landscape. Our study not only offers insights into the effective detection of homophobic content by improving on previous research results, but it also lays groundwork for future advancements in hate speech analysis.","sentences":["Our study addresses a significant gap in online hate speech detection research by focusing on homophobia, an area often neglected in sentiment analysis research.","Utilising advanced sentiment analysis models, particularly BERT, and traditional machine learning methods, we developed a nuanced approach to identify homophobic content on X/Twitter.","This research is pivotal due to the persistent underrepresentation of homophobia in detection models.","Our findings reveal that while BERT outperforms traditional methods, the choice of validation technique can impact model performance.","This underscores the importance of contextual understanding in detecting nuanced hate speech.","By releasing the largest open-source labelled English dataset for homophobia detection known to us, an analysis of various models' performance and our strongest BERT-based model, we aim to enhance online safety and inclusivity.","Future work will extend to broader LGBTQIA+ hate speech detection, addressing the challenges of sourcing diverse datasets.","Through this endeavour, we contribute to the larger effort against online hate, advocating for a more inclusive digital landscape.","Our study not only offers insights into the effective detection of homophobic content by improving on previous research results, but it also lays groundwork for future advancements in hate speech analysis."],"url":"http://arxiv.org/abs/2405.09221v1","category":"cs.CL"}
{"created":"2024-05-15 09:59:37","title":"ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models","abstract":"In this paper, we present the findings of our Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks.\" Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis. Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent. Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.","sentences":["In this paper, we present the findings of our Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks.\"","Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities.","We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node.","In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights.","Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix.","These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis.","Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent.","Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path.","In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks.","This study may contribute to our understanding of the general planning capabilities in other related domains."],"url":"http://arxiv.org/abs/2405.09220v1","category":"cs.LG"}
{"created":"2024-05-15 09:47:59","title":"Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model","abstract":"We introduce Xmodel-VLM, a cutting-edge multimodal vision language model. It is designed for efficient deployment on consumer GPU servers. Our work directly confronts a pivotal industry issue by grappling with the prohibitive service costs that hinder the broad adoption of large-scale multimodal systems. Through rigorous training, we have developed a 1B-scale language model from the ground up, employing the LLaVA paradigm for modal alignment. The result, which we call Xmodel-VLM, is a lightweight yet powerful multimodal vision language model. Extensive testing across numerous classic multimodal benchmarks has revealed that despite its smaller size and faster execution, Xmodel-VLM delivers performance comparable to that of larger models. Our model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM.","sentences":["We introduce Xmodel-VLM, a cutting-edge multimodal vision language model.","It is designed for efficient deployment on consumer GPU servers.","Our work directly confronts a pivotal industry issue by grappling with the prohibitive service costs that hinder the broad adoption of large-scale multimodal systems.","Through rigorous training, we have developed a 1B-scale language model from the ground up, employing the LLaVA paradigm for modal alignment.","The result, which we call Xmodel-VLM, is a lightweight yet powerful multimodal vision language model.","Extensive testing across numerous classic multimodal benchmarks has revealed that despite its smaller size and faster execution, Xmodel-VLM delivers performance comparable to that of larger models.","Our model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM."],"url":"http://arxiv.org/abs/2405.09215v1","category":"cs.CV"}
{"created":"2024-05-15 09:15:48","title":"Performance Analysis of RIS-aided MISO Systems with EMI and Channel Aging","abstract":"In this paper, we investigate a reconfigurable intelligent surface (RIS)-aided multiple-input single-output (MISO) system in the presence of electromagnetic interference (EMI) and channel aging with a Rician fading channel model between the base station (BS) and user equipment (UE). Specifically, we derive the closed-form expression for downlink spectral efficiency (SE) with maximum ratio transmission (MRT) precoding. The Monte-Carlo simulation supports the theoretical results, demonstrating that amplifying the weight of the line-of-sight (LoS) component in Rician fading channels can boost SE, while EMI has a detrimental impact. Furthermore, continuously increasing the number of RIS elements is not an optimal choice when EMI exists. Nonetheless, RIS can be deployed to compensate for SE degradation caused by channel aging effects. Finally, enlarging the RIS elements size can significantly improve system performance.","sentences":["In this paper, we investigate a reconfigurable intelligent surface (RIS)-aided multiple-input single-output (MISO) system in the presence of electromagnetic interference (EMI) and channel aging with a Rician fading channel model between the base station (BS) and user equipment (UE).","Specifically, we derive the closed-form expression for downlink spectral efficiency (SE) with maximum ratio transmission (MRT) precoding.","The Monte-Carlo simulation supports the theoretical results, demonstrating that amplifying the weight of the line-of-sight (LoS) component in Rician fading channels can boost SE, while EMI has a detrimental impact.","Furthermore, continuously increasing the number of RIS elements is not an optimal choice when EMI exists.","Nonetheless, RIS can be deployed to compensate for SE degradation caused by channel aging effects.","Finally, enlarging the RIS elements size can significantly improve system performance."],"url":"http://arxiv.org/abs/2405.09200v1","category":"cs.IT"}
{"created":"2024-05-15 09:01:54","title":"Autonomous Cooperative Levels of Multiple-Heterogeneous Unmanned Vehicle Systems","abstract":"As multiple and heterogenous unmanned vehicle systems continue to play an increasingly important role in addressing complex missions in the real world, the need for effective cooperation among unmanned vehicles becomes paramount. The concept of autonomous cooperation, wherein unmanned vehicles cooperate without human intervention or human control, offers promising avenues for enhancing the efficiency and adaptability of intelligence of multiple-heterogeneous unmanned vehicle systems. Despite the growing interests in this domain, as far as the authors are concerned, there exists a notable lack of comprehensive literature on defining explicit concept and classifying levels of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems. In this aspect, this article aims to define the explicit concept of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems. Furthermore, we provide a novel criterion to assess the technical maturity of the developed unmanned vehicle systems by classifying the autonomous cooperative levels of multiple-heterogeneous unmanned vehicle systems.","sentences":["As multiple and heterogenous unmanned vehicle systems continue to play an increasingly important role in addressing complex missions in the real world, the need for effective cooperation among unmanned vehicles becomes paramount.","The concept of autonomous cooperation, wherein unmanned vehicles cooperate without human intervention or human control, offers promising avenues for enhancing the efficiency and adaptability of intelligence of multiple-heterogeneous unmanned vehicle systems.","Despite the growing interests in this domain, as far as the authors are concerned, there exists a notable lack of comprehensive literature on defining explicit concept and classifying levels of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems.","In this aspect, this article aims to define the explicit concept of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems.","Furthermore, we provide a novel criterion to assess the technical maturity of the developed unmanned vehicle systems by classifying the autonomous cooperative levels of multiple-heterogeneous unmanned vehicle systems."],"url":"http://arxiv.org/abs/2405.09193v1","category":"eess.SY"}
{"created":"2024-05-15 08:53:47","title":"Advancing Explainable AI with Causal Analysis in Large-Scale Fuzzy Cognitive Maps","abstract":"In the quest for accurate and interpretable AI models, eXplainable AI (XAI) has become crucial. Fuzzy Cognitive Maps (FCMs) stand out as an advanced XAI method because of their ability to synergistically combine and exploit both expert knowledge and data-driven insights, providing transparency and intrinsic interpretability. This letter introduces and investigates the \"Total Causal Effect Calculation for FCMs\" (TCEC-FCM) algorithm, an innovative approach that, for the first time, enables the efficient calculation of total causal effects among concepts in large-scale FCMs by leveraging binary search and graph traversal techniques, thereby overcoming the challenge of exhaustive causal path exploration that hinder existing methods. We evaluate the proposed method across various synthetic FCMs that demonstrate TCEC-FCM's superior performance over exhaustive methods, marking a significant advancement in causal effect analysis within FCMs, thus broadening their usability for modern complex XAI applications.","sentences":["In the quest for accurate and interpretable AI models, eXplainable AI (XAI) has become crucial.","Fuzzy Cognitive Maps (FCMs) stand out as an advanced XAI method because of their ability to synergistically combine and exploit both expert knowledge and data-driven insights, providing transparency and intrinsic interpretability.","This letter introduces and investigates the \"Total Causal Effect Calculation for FCMs\" (TCEC-FCM) algorithm, an innovative approach that, for the first time, enables the efficient calculation of total causal effects among concepts in large-scale FCMs by leveraging binary search and graph traversal techniques, thereby overcoming the challenge of exhaustive causal path exploration that hinder existing methods.","We evaluate the proposed method across various synthetic FCMs that demonstrate TCEC-FCM's superior performance over exhaustive methods, marking a significant advancement in causal effect analysis within FCMs, thus broadening their usability for modern complex XAI applications."],"url":"http://arxiv.org/abs/2405.09190v1","category":"cs.AI"}
{"created":"2024-05-15 08:46:33","title":"Influence Maximization in Hypergraphs Using A Genetic Algorithm with New Initialization and Evaluation Methods","abstract":"Influence maximization (IM) is a crucial optimization task related to analyzing complex networks in the real world, such as social networks, disease propagation networks, and marketing networks. Publications to date about the IM problem focus mainly on graphs, which fail to capture high-order interaction relationships from the real world. Therefore, the use of hypergraphs for addressing the IM problem has been receiving increasing attention. However, identifying the most influential nodes in hypergraphs remains challenging, mainly because nodes and hyperedges are often strongly coupled and correlated. In this paper, to effectively identify the most influential nodes, we first propose a novel hypergraph-independent cascade model that integrates the influences of both node and hyperedge failures. Afterward, we introduce genetic algorithms (GA) to identify the most influential nodes that leverage hypergraph collective influences. In the GA-based method, the hypergraph collective influence is effectively used to initialize the population, thereby enhancing the quality of initial candidate solutions. The designed fitness function considers the joint influences of both nodes and hyperedges. This ensures the optimal set of nodes with the best influence on both nodes and hyperedges to be evaluated accurately. Moreover, a new mutation operator is designed by introducing factors, i.e., the collective influence and overlapping effects of nodes in hypergraphs, to breed high-quality offspring. In the experiments, several simulations on both synthetic and real hypergraphs have been conducted, and the results demonstrate that the proposed method outperforms the compared methods.","sentences":["Influence maximization (IM) is a crucial optimization task related to analyzing complex networks in the real world, such as social networks, disease propagation networks, and marketing networks.","Publications to date about the IM problem focus mainly on graphs, which fail to capture high-order interaction relationships from the real world.","Therefore, the use of hypergraphs for addressing the IM problem has been receiving increasing attention.","However, identifying the most influential nodes in hypergraphs remains challenging, mainly because nodes and hyperedges are often strongly coupled and correlated.","In this paper, to effectively identify the most influential nodes, we first propose a novel hypergraph-independent cascade model that integrates the influences of both node and hyperedge failures.","Afterward, we introduce genetic algorithms (GA) to identify the most influential nodes that leverage hypergraph collective influences.","In the GA-based method, the hypergraph collective influence is effectively used to initialize the population, thereby enhancing the quality of initial candidate solutions.","The designed fitness function considers the joint influences of both nodes and hyperedges.","This ensures the optimal set of nodes with the best influence on both nodes and hyperedges to be evaluated accurately.","Moreover, a new mutation operator is designed by introducing factors, i.e., the collective influence and overlapping effects of nodes in hypergraphs, to breed high-quality offspring.","In the experiments, several simulations on both synthetic and real hypergraphs have been conducted, and the results demonstrate that the proposed method outperforms the compared methods."],"url":"http://arxiv.org/abs/2405.09185v1","category":"cs.SI"}
