{"created":"2024-02-26 11:42:29","title":"Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision","abstract":"Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation.","sentences":["Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language.","Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages.","In this paper, we show that CLQA can be addressed using a single encoder-decoder model.","To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia.","We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation.","Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation."],"url":"http://arxiv.org/abs/2402.16508v1","category":"cs.CL"}
{"created":"2024-02-26 11:08:26","title":"Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification","abstract":"Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.","sentences":["Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification.","This research addresses this problem with a novel, scalable, and AI-driven solution.","The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types.","Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes.","Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft.","It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification.","To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner.","Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936).","The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality.","The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition."],"url":"http://arxiv.org/abs/2402.16486v1","category":"cs.CV"}
{"created":"2024-02-26 09:59:04","title":"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering","abstract":"Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at \\url{https://github.com/hyintell/RetrievalQA}","sentences":["Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information.","However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied.","This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge.","The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly.","This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods.","We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions.","Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.","The dataset and code will be available at \\url{https://github.com/hyintell/RetrievalQA}"],"url":"http://arxiv.org/abs/2402.16457v1","category":"cs.CL"}
{"created":"2024-02-26 09:50:34","title":"Distortion-Controlled Dithering with Reduced Recompression Rate","abstract":"Dithering is a technique that can improve human perception of low-resolution data by reducing quantization artifacts. In this work we formalize and analytically justify two metrics for quantization artifact prominence, using them to design a novel dithering method for distortion-controlled data compression. We present theoretical entropy calculations for this dither and experimentally validate its performance on a low-rate image compression task. The result is a drastic improvement in the perceptual quality of quantized images with a lower recompression entropy than any state-of-the-art dither technique, achieving 45 points lower PIQUE at the same rate or 40% lower rate at the same PIQUE. The proposed dither is an adaptable tool applicable for use in any lossy compression system, permitting precise control of rate-distortion characteristics for both compression and recompression.","sentences":["Dithering is a technique that can improve human perception of low-resolution data by reducing quantization artifacts.","In this work we formalize and analytically justify two metrics for quantization artifact prominence, using them to design a novel dithering method for distortion-controlled data compression.","We present theoretical entropy calculations for this dither and experimentally validate its performance on a low-rate image compression task.","The result is a drastic improvement in the perceptual quality of quantized images with a lower recompression entropy than any state-of-the-art dither technique, achieving 45 points lower PIQUE at the same rate or 40% lower rate at the same PIQUE.","The proposed dither is an adaptable tool applicable for use in any lossy compression system, permitting precise control of rate-distortion characteristics for both compression and recompression."],"url":"http://arxiv.org/abs/2402.16447v1","category":"eess.SP"}
{"created":"2024-02-26 09:43:52","title":"ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing","abstract":"Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation. However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field. This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks. Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability. Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. In the protein property prediction task, ProLLaMA achieves nearly 100\\% accuracy across many categories. The latter two tasks are beyond the reach of other ProLLMs. Code is available at \\url{https://github.com/Lyu6PosHao/ProLLaMA}.","sentences":["Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks.","Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation.","However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field.","This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands.","To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks.","Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability.","Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously.","Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task.","In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities.","In the protein property prediction task, ProLLaMA achieves nearly 100\\% accuracy across many categories.","The latter two tasks are beyond the reach of other ProLLMs.","Code is available at \\url{https://github.com/Lyu6PosHao/ProLLaMA}."],"url":"http://arxiv.org/abs/2402.16445v1","category":"cs.CE"}
{"created":"2024-02-26 09:19:46","title":"Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models","abstract":"We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.","sentences":["We present our work on predicting United Nations sustainable development goals (SDG) for university courses.","We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input.","We use this data to train several different smaller language models to predict SDGs for university courses.","This work contributes to better university level adaptation of SDGs.","The best performing model in our experiments was BART with an F1-score of 0.786."],"url":"http://arxiv.org/abs/2402.16420v1","category":"cs.CL"}
{"created":"2024-02-26 09:09:24","title":"Direct excitation of Kelvin waves on quantized vortices","abstract":"Helices and spirals, prevalent across various systems, play a crucial role in characterizing symmetry, describing dynamics, and imparting unique functionalities, attributed to their inherent simplicity and chiral nature. A helical excitation on a quantized vortex, an example of a one-dimensional topological defect, emerges as a Nambu-Goldstone mode following spontaneous symmetry breaking, known as a Kelvin wave. Kelvin waves play a vital role in energy dissipation within inviscid quantum fluids. However, deliberately exciting Kelvin waves has proven to be challenging. Here, we introduce a controlled method for exciting Kelvin waves on a quantized vortex in superfluid helium-4. We used a charged nanoparticle, oscillated by a time-varying electric field, to stimulate Kelvin waves on the vortex. A major breakthrough in our research is the confirmation of the helical nature of Kelvin waves through three-dimensional image reconstruction, providing visual evidence of their complex dynamics. Additionally, we determined the dispersion relation and the phase velocity of the Kelvin wave and identified the vorticity direction, enhancing our understanding of quantum fluid behavior. This work elucidates the dynamics of Kelvin waves and pioneers a novel approach for manipulating and observing quantized vortices in three dimensions, thereby opening new avenues for exploring quantum fluidic systems.","sentences":["Helices and spirals, prevalent across various systems, play a crucial role in characterizing symmetry, describing dynamics, and imparting unique functionalities, attributed to their inherent simplicity and chiral nature.","A helical excitation on a quantized vortex, an example of a one-dimensional topological defect, emerges as a Nambu-Goldstone mode following spontaneous symmetry breaking, known as a Kelvin wave.","Kelvin waves play a vital role in energy dissipation within inviscid quantum fluids.","However, deliberately exciting Kelvin waves has proven to be challenging.","Here, we introduce a controlled method for exciting Kelvin waves on a quantized vortex in superfluid helium-4.","We used a charged nanoparticle, oscillated by a time-varying electric field, to stimulate Kelvin waves on the vortex.","A major breakthrough in our research is the confirmation of the helical nature of Kelvin waves through three-dimensional image reconstruction, providing visual evidence of their complex dynamics.","Additionally, we determined the dispersion relation and the phase velocity of the Kelvin wave and identified the vorticity direction, enhancing our understanding of quantum fluid behavior.","This work elucidates the dynamics of Kelvin waves and pioneers a novel approach for manipulating and observing quantized vortices in three dimensions, thereby opening new avenues for exploring quantum fluidic systems."],"url":"http://arxiv.org/abs/2402.16411v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-26 08:47:35","title":"Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations","abstract":"Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes. The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state. Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras. However, most of the adopted methods have poor real-time performance. To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations. Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3). Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression. Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art. Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively.","sentences":["Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes.","The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state.","Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras.","However, most of the adopted methods have poor real-time performance.","To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations.","Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3).","Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression.","Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art.","Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively."],"url":"http://arxiv.org/abs/2402.16398v1","category":"cs.RO"}
{"created":"2024-02-26 07:52:40","title":"Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning","abstract":"Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions in the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing graph learning methods into several essential scenarios, including graph domain adaptation learning, graph out-of-distribution learning, and graph continual learning. For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in distribution-shifted graph learning. Additionally, we discuss the potential applications and future directions for graph learning under distribution shifts with a systematic analysis of the current state in this field. The survey is positioned to provide general guidance for the development of effective graph learning algorithms in handling graph distribution shifts, and to stimulate future research and advancements in this area.","sentences":["Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data.","In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue.","This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness.","In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning.","Concretely, according to the observability of distributions in the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing graph learning methods into several essential scenarios, including graph domain adaptation learning, graph out-of-distribution learning, and graph continual learning.","For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in distribution-shifted graph learning.","Additionally, we discuss the potential applications and future directions for graph learning under distribution shifts with a systematic analysis of the current state in this field.","The survey is positioned to provide general guidance for the development of effective graph learning algorithms in handling graph distribution shifts, and to stimulate future research and advancements in this area."],"url":"http://arxiv.org/abs/2402.16374v1","category":"cs.LG"}
{"created":"2024-02-26 07:00:58","title":"CodeS: Towards Building Open-source Language Models for Text-to-SQL","abstract":"Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.","sentences":["Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL).","However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads.","To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task.","CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes.","This paper studies the research challenges in building CodeS.","To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus.","Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique.","We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications.","The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks."],"url":"http://arxiv.org/abs/2402.16347v1","category":"cs.CL"}
{"created":"2024-02-26 06:08:25","title":"Achieving $\\tilde{O}(1/\u03b5)$ Sample Complexity for Constrained Markov Decision Process","abstract":"We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$ sample complexity bound, with $\\kappa$ being a problem-dependent parameter, yet independent of $\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \\textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i). an eliminating procedure that characterizes one optimal basis of the primal LP, and; ii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.","sentences":["We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making.","In this problem, we are given finite resources and a MDP with unknown transition probabilities.","At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time.","In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems.","We derive a logarithmic regret bound, which translates into a $O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$ sample complexity bound, with $\\kappa$ being a problem-dependent parameter, yet independent of $\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems.","To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \\textit{adaptive} remaining resource capacities.","The key elements of our algorithm are: i).","an eliminating procedure that characterizes one optimal basis of the primal LP, and; ii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis."],"url":"http://arxiv.org/abs/2402.16324v1","category":"cs.LG"}
{"created":"2024-02-26 05:30:48","title":"Cross-domain Chinese Sentence Pattern Parsing","abstract":"Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.","sentences":["Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.","Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.","To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework.","Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.","Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics."],"url":"http://arxiv.org/abs/2402.16311v2","category":"cs.CL"}
{"created":"2024-02-26 05:28:36","title":"REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories","abstract":"Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.9\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.","sentences":["Location prediction forecasts a user's location based on historical user mobility traces.","To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful.","Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction.","However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances.","Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction.","Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps.","Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets.","Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.9\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities."],"url":"http://arxiv.org/abs/2402.16310v1","category":"cs.LG"}
{"created":"2024-02-26 03:49:18","title":"Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation","abstract":"Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances. As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc. In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL). Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation. To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task. First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS). Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework. Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations.","sentences":["Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances.","As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc.","In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL).","Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation.","To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task.","First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS).","Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework.","Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations."],"url":"http://arxiv.org/abs/2402.16280v1","category":"cs.CV"}
{"created":"2024-02-26 02:48:43","title":"UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval","abstract":"Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives. Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously.","sentences":["Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue.","However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency.","Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection.","To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product.","Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives.","Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously."],"url":"http://arxiv.org/abs/2402.16261v1","category":"cs.CL"}
{"created":"2024-02-26 02:14:42","title":"Topic-to-essay generation with knowledge-based content selection","abstract":"The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.","sentences":["The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words.","Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity.","In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder.","Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities.","In addition, we have contributed a new Chinese dataset for TEG tasks.","Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency."],"url":"http://arxiv.org/abs/2402.16248v1","category":"cs.CL"}
{"created":"2024-02-26 01:31:50","title":"Phase-Space Delaunay Tessellation Field Estimator","abstract":"The reconstruction of density and velocity fields is of central importance to the interpretation of $N$-body simulations. We propose a phase-space extension of the Delaunay tessellation field estimator (DTFE) that tracks the dark matter fluid in phase-space. The new reconstruction scheme removes several artifacts from the conventional DTFE in multi-stream regions, while preserving the adaptive resolution in high-density regions and yielding continuous fields. The estimator also removes tessellation artifacts of a previously proposed phase-space reconstruction scheme.","sentences":["The reconstruction of density and velocity fields is of central importance to the interpretation of $N$-body simulations.","We propose a phase-space extension of the Delaunay tessellation field estimator (DTFE) that tracks the dark matter fluid in phase-space.","The new reconstruction scheme removes several artifacts from the conventional DTFE in multi-stream regions, while preserving the adaptive resolution in high-density regions and yielding continuous fields.","The estimator also removes tessellation artifacts of a previously proposed phase-space reconstruction scheme."],"url":"http://arxiv.org/abs/2402.16234v1","category":"astro-ph.CO"}
{"created":"2024-02-25 21:35:00","title":"Probing the physics of star formation (ProPStar): I. First resolved maps of the electron fraction and cosmic-ray ionization rate in NGC 1333","abstract":"Electron fraction and cosmic-ray ionization rates (CRIR) in star-forming regions are important quantities in astrochemical modeling and are critical to the degree of coupling between neutrals, ions, and electrons, which regulates the dynamics of the magnetic field. However, these are difficult quantities to estimate. We aim to derive the electron fraction and CRIR maps of an active star-forming region. We combined observations of the nearby NGC 1333 star-forming region carried out with the NOEMA interferometer and IRAM 30-m single dish to generate high spatial dynamic range maps of different molecular transitions. We used the DCO$^+$ and H$^{13}$CO$^+$ ratio (in addition to complementary data) to estimate the electron fraction and produce cosmic-ray ionization rate maps. We derived the first large-area electron fraction and CRIR resolved maps in a star-forming region, with typical values of $10^{-6.5}$ and $10^{-16.5}$ s$^{-1}$, respectively. The maps present clear evidence of enhanced values around embedded young stellar objects (YSOs). This provides strong evidence for locally accelerated cosmic rays. We also found a strong enhancement toward the northwest region in the map that might be related either to an interaction with a bubble or to locally generated cosmic rays by YSOs. We used the typical electron fraction and derived a MHD turbulence dissipation scale of 0.054 pc, which could be tested with future observations. We found a higher cosmic-ray ionization rate compared to the canonical value for $N({\\rm H_2})=10^{21}-10^{23}$ cm$^{-2}$ of $10^{-17}$ s$^{-1}$ in the region, and it is likely generated by the accreting YSOs. The high value of the electron fraction suggests that new disks will form from gas in the ideal-MHD limit. This indicates that local enhancements of $\\zeta({\\rm H_2})$, due to YSOs, should be taken into account in the analysis of clustered star formation.","sentences":["Electron fraction and cosmic-ray ionization rates (CRIR) in star-forming regions are important quantities in astrochemical modeling and are critical to the degree of coupling between neutrals, ions, and electrons, which regulates the dynamics of the magnetic field.","However, these are difficult quantities to estimate.","We aim to derive the electron fraction and CRIR maps of an active star-forming region.","We combined observations of the nearby NGC 1333 star-forming region carried out with the NOEMA interferometer and IRAM 30-m single dish to generate high spatial dynamic range maps of different molecular transitions.","We used the DCO$^+$ and H$^{13}$CO$^+$ ratio (in addition to complementary data) to estimate the electron fraction and produce cosmic-ray ionization rate maps.","We derived the first large-area electron fraction and CRIR resolved maps in a star-forming region, with typical values of $10^{-6.5}$ and $10^{-16.5}$ s$^{-1}$, respectively.","The maps present clear evidence of enhanced values around embedded young stellar objects (YSOs).","This provides strong evidence for locally accelerated cosmic rays.","We also found a strong enhancement toward the northwest region in the map that might be related either to an interaction with a bubble or to locally generated cosmic rays by YSOs.","We used the typical electron fraction and derived a MHD turbulence dissipation scale of 0.054 pc, which could be tested with future observations.","We found a higher cosmic-ray ionization rate compared to the canonical value for $N({\\rm H_2})=10^{21}-10^{23}$ cm$^{-2}$ of $10^{-17}$ s$^{-1}$ in the region, and it is likely generated by the accreting YSOs.","The high value of the electron fraction suggests that new disks will form from gas in the ideal-MHD limit.","This indicates that local enhancements of $\\zeta({\\rm H_2})$, due to YSOs, should be taken into account in the analysis of clustered star formation."],"url":"http://arxiv.org/abs/2402.16202v1","category":"astro-ph.GA"}
{"created":"2024-02-25 17:37:53","title":"Distribution-Free Fair Federated Learning with Small Samples","abstract":"As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method.","sentences":["As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important.","However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees.","To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples.","Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes.","We provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method."],"url":"http://arxiv.org/abs/2402.16158v1","category":"stat.ML"}
{"created":"2024-02-26 11:35:23","title":"Trajectory Prediction for Autonomous Driving Using a Transformer Network","abstract":"Predicting the trajectories of surrounding agents is still considered one of the most challenging tasks for autonomous driving. In this paper, we introduce a multi-modal trajectory prediction framework based on the transformer network. The semantic maps of each agent are used as inputs to convolutional networks to automatically derive relevant contextual information. A novel auxiliary loss that penalizes unfeasible off-road predictions is also proposed in this study. Experiments on the Lyft l5kit dataset show that the proposed model achieves state-of-the-art performance, substantially improving the accuracy and feasibility of the prediction outcomes.","sentences":["Predicting the trajectories of surrounding agents is still considered one of the most challenging tasks for autonomous driving.","In this paper, we introduce a multi-modal trajectory prediction framework based on the transformer network.","The semantic maps of each agent are used as inputs to convolutional networks to automatically derive relevant contextual information.","A novel auxiliary loss that penalizes unfeasible off-road predictions is also proposed in this study.","Experiments on the Lyft l5kit dataset show that the proposed model achieves state-of-the-art performance, substantially improving the accuracy and feasibility of the prediction outcomes."],"url":"http://arxiv.org/abs/2402.16501v1","category":"cs.RO"}
