{"created":"2024-04-04 17:59:40","title":"The More You See in 2D, the More You Perceive in 3D","abstract":"Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.","sentences":["Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images.","Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images.","Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning.","The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis.","We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods.","We demonstrate our system on real images as well as standard synthetic benchmarks.","Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding."],"url":"http://arxiv.org/abs/2404.03652v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:22","title":"Multipartite edge modes and tensor networks","abstract":"Holographic tensor networks model AdS/CFT, but so far they have been limited by involving only systems that are very different from gravity. Unfortunately, we cannot straightforwardly discretize gravity to incorporate it, because that would break diffeomorphism invariance. In this note, we explore a resolution. In low dimensions gravity can be written as a topological gauge theory, which can be discretized without breaking gauge-invariance. However, new problems arise. Foremost, we now need a qualitatively new kind of \"area operator,\" which has no relation to the number of links along the cut and is instead topological. Secondly, the inclusion of matter becomes trickier. We successfully construct a tensor network both including matter and with this new type of area. Notably, while this area is still related to the entanglement in \"edge mode\" degrees of freedom, the edge modes are no longer bipartite entangled pairs. Instead they are highly multipartite. Along the way, we calculate the entropy of novel subalgebras in a particular topological gauge theory. We also show that the multipartite nature of the edge modes gives rise to non-commuting area operators, a property that other tensor networks do not exhibit.","sentences":["Holographic tensor networks model AdS/CFT, but so far they have been limited by involving only systems that are very different from gravity.","Unfortunately, we cannot straightforwardly discretize gravity to incorporate it, because that would break diffeomorphism invariance.","In this note, we explore a resolution.","In low dimensions gravity can be written as a topological gauge theory, which can be discretized without breaking gauge-invariance.","However, new problems arise.","Foremost, we now need a qualitatively new kind of \"area operator,\" which has no relation to the number of links along the cut and is instead topological.","Secondly, the inclusion of matter becomes trickier.","We successfully construct a tensor network both including matter and with this new type of area.","Notably, while this area is still related to the entanglement in \"edge mode\" degrees of freedom, the edge modes are no longer bipartite entangled pairs.","Instead they are highly multipartite.","Along the way, we calculate the entropy of novel subalgebras in a particular topological gauge theory.","We also show that the multipartite nature of the edge modes gives rise to non-commuting area operators, a property that other tensor networks do not exhibit."],"url":"http://arxiv.org/abs/2404.03651v1","category":"hep-th"}
{"created":"2024-04-04 17:59:08","title":"OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views","abstract":"Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.","sentences":["Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner.","This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set.","More recently, first works on open-set segmentation in 3D scenes have appeared in the literature.","These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes.","However, these 3D scene representations do not align well with the image-based nature of the visual-language models.","Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features.","To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF.","This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization.","Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images.","For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU."],"url":"http://arxiv.org/abs/2404.03650v1","category":"cs.CV"}
{"created":"2024-04-04 17:55:38","title":"Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention","abstract":"In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication","sentences":["In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards.","Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation.","Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate.","In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention.","The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process.","To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks.","Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance.","Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks.","The source code is accessible online to facilitate replication"],"url":"http://arxiv.org/abs/2404.03637v1","category":"cs.IR"}
{"created":"2024-04-04 17:49:38","title":"ROBUST: 221 Bugs in the Robot Operating System","abstract":"As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS). We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes. Finally, we reflect on the implications of our findings and outline future research directions for the community.","sentences":["As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.","To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS).","We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes.","Finally, we reflect on the implications of our findings and outline future research directions for the community."],"url":"http://arxiv.org/abs/2404.03629v1","category":"cs.SE"}
{"created":"2024-04-04 17:39:41","title":"On the Efficiency of Convolutional Neural Networks","abstract":"Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models. Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago. Yet computer scientists make computational efficiency their primary objective. Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements. Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency. Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency. However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity. Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency. This insight enabled us to co-optimize the separate factors that determine latency. We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity. Therefore, kernels that implement these layers use significant memory resources. We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size. Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task. Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost.","sentences":["Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models.","Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago.","Yet computer scientists make computational efficiency their primary objective.","Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements.","Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency.","Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency.","However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity.","Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency.","This insight enabled us to co-optimize the separate factors that determine latency.","We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity.","Therefore, kernels that implement these layers use significant memory resources.","We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size.","Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task.","Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost."],"url":"http://arxiv.org/abs/2404.03617v1","category":"cs.LG"}
{"created":"2024-04-04 17:25:25","title":"A Unified Algorithmic Framework for Dynamic Assortment Optimization under MNL Choice","abstract":"We consider assortment and inventory planning problems with dynamic stockout-based substitution effects and no replenishment. We consider two settings: 1. Customers can see all available products when they arrive, which is commonly seen in physical stores. 2. The seller can choose to offer a subset of available products to each customer, which is typical on online platforms. Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different. We develop a unified algorithm framework under the MNL choice model for both settings. Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee, runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints. In the process, we establish various novel properties of dynamic assortment planning (under the MNL choice) that may be useful more broadly.","sentences":["We consider assortment and inventory planning problems with dynamic stockout-based substitution effects and no replenishment.","We consider two settings: 1. Customers can see all available products when they arrive, which is commonly seen in physical stores.","2.","The seller can choose to offer a subset of available products to each customer, which is typical on online platforms.","Both settings are known to be computationally challenging, and the current approximation algorithms for the two settings are quite different.","We develop a unified algorithm framework under the MNL choice model for both settings.","Our algorithms improve on the state-of-the-art algorithms in terms of approximation guarantee, runtime, and the ability to manage uncertainty in the total number of customers and handle more complex constraints.","In the process, we establish various novel properties of dynamic assortment planning (under the MNL choice) that may be useful more broadly."],"url":"http://arxiv.org/abs/2404.03604v1","category":"math.OC"}
{"created":"2024-04-04 17:23:28","title":"Analysis of second-order temporal schemes for modeling flow-solute transport in unsaturated porous media","abstract":"In this study, second-order temporal discretizations are analyzed for solving the coupled system of infiltration and solute transport in unsaturated porous media. The Richards equation is used to describe unsaturated flow, while the advection-dispersion equation (ADE) is used for modeling solute transport. The standard finite element discretization in space is utilized and four time-stepping methods are studied. Three of these methods require an iterative resolution to solve the Richards equation in its mixed form. In the remaining method, a novel technique is proposed to linearize the system of equations in time, and the iterative processes are avoided. In this method, a free stabilized parameter is introduced. Numerical tests are conducted to analyze the accuracy and efficiency of methods. The developed linear scheme based on the optimal free parameter is accurate and performs better in terms of efficiency since it offers a considerable gain in computational time compared to the other methods. The reliability and effectiveness of the developed semi-implicit scheme are investigated using numerical experiments for modeling water flow and solute transport in unsaturated soils.","sentences":["In this study, second-order temporal discretizations are analyzed for solving the coupled system of infiltration and solute transport in unsaturated porous media.","The Richards equation is used to describe unsaturated flow, while the advection-dispersion equation (ADE) is used for modeling solute transport.","The standard finite element discretization in space is utilized and four time-stepping methods are studied.","Three of these methods require an iterative resolution to solve the Richards equation in its mixed form.","In the remaining method, a novel technique is proposed to linearize the system of equations in time, and the iterative processes are avoided.","In this method, a free stabilized parameter is introduced.","Numerical tests are conducted to analyze the accuracy and efficiency of methods.","The developed linear scheme based on the optimal free parameter is accurate and performs better in terms of efficiency since it offers a considerable gain in computational time compared to the other methods.","The reliability and effectiveness of the developed semi-implicit scheme are investigated using numerical experiments for modeling water flow and solute transport in unsaturated soils."],"url":"http://arxiv.org/abs/2404.03603v1","category":"math.NA"}
{"created":"2024-04-04 17:00:37","title":"Beyond the blur: using experimental point spread functions to help scanning Kelvin probe microscopy reach its full potential","abstract":"Scanning Kelvin probe microscopy (SKPM) is a powerful technique for investigating the electrostatic properties of material surfaces, enabling the imaging of variations in work function, topology, surface charge density, or combinations thereof. Regardless of the underlying signal source, SKPM results in a voltage image which is spatially distorted due to the finite size of the probe, long-range electrostatic interactions, mechanical and electrical noise, and the finite response time of the electronics. In order to recover the underlying signal, it is necessary to deconvolve the measurement with an appropriate point spread function (PSF) that accounts the aforementioned distortions, but determining this PSF is difficult. Here we describe how such PSFs can be determined experimentally, and show how they can be used to recover the underlying information of interest. We first consider the physical principles that enable SKPM, and discuss how these affect the system PSF. We then show how one can experimentally measure PSFs by looking at well defined features, and that these compare well to simulated PSFs, provided scans are performed extremely slowly and carefully. Next, we work at realistic scan speeds, and show that the idealised PSFs fail to capture temporal distortions in the scan direction. While simulating PSFs for these situations would be quite challenging, we show that measuring PSFs with similar scan parameters works well. Our approach clarifies the basic principles of and inherent challenges to SKPM measurements, and gives practical methods to improve results.","sentences":["Scanning Kelvin probe microscopy (SKPM) is a powerful technique for investigating the electrostatic properties of material surfaces, enabling the imaging of variations in work function, topology, surface charge density, or combinations thereof.","Regardless of the underlying signal source, SKPM results in a voltage image which is spatially distorted due to the finite size of the probe, long-range electrostatic interactions, mechanical and electrical noise, and the finite response time of the electronics.","In order to recover the underlying signal, it is necessary to deconvolve the measurement with an appropriate point spread function (PSF) that accounts the aforementioned distortions, but determining this PSF is difficult.","Here we describe how such PSFs can be determined experimentally, and show how they can be used to recover the underlying information of interest.","We first consider the physical principles that enable SKPM, and discuss how these affect the system PSF.","We then show how one can experimentally measure PSFs by looking at well defined features, and that these compare well to simulated PSFs, provided scans are performed extremely slowly and carefully.","Next, we work at realistic scan speeds, and show that the idealised PSFs fail to capture temporal distortions in the scan direction.","While simulating PSFs for these situations would be quite challenging, we show that measuring PSFs with similar scan parameters works well.","Our approach clarifies the basic principles of and inherent challenges to SKPM measurements, and gives practical methods to improve results."],"url":"http://arxiv.org/abs/2404.03593v1","category":"physics.app-ph"}
{"created":"2024-04-04 16:59:13","title":"Wilkins: HPC In Situ Workflows Made Easy","abstract":"In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at simulation time. Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks. In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks. Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism. Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications. We demonstrate the above features using both synthetic benchmarks and two science use cases in materials science and cosmology.","sentences":["In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at simulation time.","Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks.","In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks.","Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism.","Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications.","We demonstrate the above features using both synthetic benchmarks and two science use cases in materials science and cosmology."],"url":"http://arxiv.org/abs/2404.03591v1","category":"cs.DC"}
{"created":"2024-04-04 16:58:02","title":"Homotopy types of diagrams of chain complexes","abstract":"We study the homotopy theory of diagrams of chain complexes over a field indexed by a finite poset, and show that it can be completely described in terms of appropriate diagrams of graded vector spaces.","sentences":["We study the homotopy theory of diagrams of chain complexes over a field indexed by a finite poset, and show that it can be completely described in terms of appropriate diagrams of graded vector spaces."],"url":"http://arxiv.org/abs/2404.03589v1","category":"math.AT"}
{"created":"2024-04-04 16:42:41","title":"The CCube reconstruction algorithm for the SoLid experiment","abstract":"The SoLid experiment is a very-short-baseline experiment aimed at searching for nuclear reactor-produced active to sterile antineutrino oscillations. The detection principle is based on the pairing of two types of solid scintillators: polyvinyl toluene and $^6$LiF:ZnS(Ag), which is a new technology used in this field of Physics. In addition to good neutron-gamma discrimination, this setup allows the detector to be highly segmented (the basic detection unit is a 5~cm side cube). High segmentation provides numerous advantages, including the precise location of Inverse Beta Decay (IBD) products, the derivation of the considerate antineutrino energy estimator, and a powerful background reduction tool based on the topological signature of the signal. Finally, the system is read out by a network of wavelength-shifting fibres coupled to a photodetector (MPPC). This paper describes the design of the reconstruction algorithm that allows maximum use of the granularity of the detector. The goal of the algorithm is to convert the output of the optical-fibre readout to the list of the detection units from which it originated. This paper provides a performance comparison for three methods and concludes with a choice of the baseline approach for the experiment.","sentences":["The SoLid experiment is a very-short-baseline experiment aimed at searching for nuclear reactor-produced active to sterile antineutrino oscillations.","The detection principle is based on the pairing of two types of solid scintillators: polyvinyl toluene and $^6$LiF:ZnS(Ag), which is a new technology used in this field of Physics.","In addition to good neutron-gamma discrimination, this setup allows the detector to be highly segmented (the basic detection unit is a 5~cm side cube).","High segmentation provides numerous advantages, including the precise location of Inverse Beta Decay (IBD) products, the derivation of the considerate antineutrino energy estimator, and a powerful background reduction tool based on the topological signature of the signal.","Finally, the system is read out by a network of wavelength-shifting fibres coupled to a photodetector (MPPC).","This paper describes the design of the reconstruction algorithm that allows maximum use of the granularity of the detector.","The goal of the algorithm is to convert the output of the optical-fibre readout to the list of the detection units from which it originated.","This paper provides a performance comparison for three methods and concludes with a choice of the baseline approach for the experiment."],"url":"http://arxiv.org/abs/2404.03580v1","category":"physics.ins-det"}
{"created":"2024-04-04 16:40:11","title":"Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models","abstract":"Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning. To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering. KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions. (2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question. (3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions. We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT .","sentences":["Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters.","However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters.","This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory.","While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge.","Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning.","To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering.","KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions.","(2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question.","(3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions.","We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances.","Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT ."],"url":"http://arxiv.org/abs/2404.03577v1","category":"cs.CL"}
{"created":"2024-04-04 16:37:42","title":"Terrain Point Cloud Inpainting via Signal Decomposition","abstract":"The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains. However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data. Inpainting algorithms are widely used to patch these holes. However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined. On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling. Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds. This representation can help to repair the holes without clear boundaries. Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively. In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem. By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details. The experimental results also demonstrate the effectiveness of our method.","sentences":["The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains.","However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data.","Inpainting algorithms are widely used to patch these holes.","However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined.","On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling.","Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds.","This representation can help to repair the holes without clear boundaries.","Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively.","In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem.","By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details.","The experimental results also demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.03572v1","category":"cs.CV"}
{"created":"2024-04-04 16:30:20","title":"Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity","abstract":"We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.","sentences":["We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace.","Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping.","With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity.","We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks.","These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.","Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities.","One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies."],"url":"http://arxiv.org/abs/2404.03570v1","category":"cs.RO"}
{"created":"2024-04-04 16:15:27","title":"A characterization of zero entropy loosely Bernoulli flows via FK-pseudometric","abstract":"We introduce the Feldman-Katok pseudometric (FK-pseudometric for short) for flows. We then provide a characterization of zero entropy loosely Bernoulli measures for continuous flows via the FK-pseudometric extending the result known for discrete-time dynamical systems. We also provide a purely topological characterization of uniquely ergodic continuous flows whose unique invariant measure is zero entropy loosely Bernoulli.","sentences":["We introduce the Feldman-Katok pseudometric (FK-pseudometric for short) for flows.","We then provide a characterization of zero entropy loosely Bernoulli measures for continuous flows via the FK-pseudometric extending the result known for discrete-time dynamical systems.","We also provide a purely topological characterization of uniquely ergodic continuous flows whose unique invariant measure is zero entropy loosely Bernoulli."],"url":"http://arxiv.org/abs/2404.03559v1","category":"math.DS"}
{"created":"2024-04-04 16:10:45","title":"Signal-preserving CMB component separation with machine learning","abstract":"Analysis of microwave sky signals, such as the cosmic microwave background, often requires component separation with multi-frequency methods, where different signals are isolated by their frequency behaviors. Many so-called \"blind\" methods, such as the internal linear combination (ILC), make minimal assumptions about the spatial distribution of the signal or contaminants, and only assume knowledge of the frequency dependence of the signal. The ILC is a minimum-variance linear combination of the measured frequency maps. In the case of Gaussian, statistically isotropic fields, this is the optimal linear combination, as the variance is the only statistic of interest. However, in many cases the signal we wish to isolate, or the foregrounds we wish to remove, are non-Gaussian and/or statistically anisotropic (in particular for Galactic foregrounds). In such cases, it is possible that machine learning (ML) techniques can be used to exploit the non-Gaussian features of the foregrounds and thereby improve component separation. However, many ML techniques require the use of complex, difficult-to-interpret operations on the data. We propose a hybrid method whereby we train an ML model using only combinations of the data that $\\textit{do not contain the signal}$, and combine the resulting ML-predicted foreground estimate with the ILC solution to reduce the error from the ILC. We demonstrate our methods on simulations of extragalactic temperature and Galactic polarization foregrounds, and show that our ML model can exploit non-Gaussian features, such as point sources and spatially-varying spectral indices, to produce lower-variance maps than ILC - eg, reducing the variance of the B-mode residual by factors of up to 5 - while preserving the signal of interest in an unbiased manner. Moreover, we often find improved performance when applying our model to foreground models on which it was not trained.","sentences":["Analysis of microwave sky signals, such as the cosmic microwave background, often requires component separation with multi-frequency methods, where different signals are isolated by their frequency behaviors.","Many so-called \"blind\" methods, such as the internal linear combination (ILC), make minimal assumptions about the spatial distribution of the signal or contaminants, and only assume knowledge of the frequency dependence of the signal.","The ILC is a minimum-variance linear combination of the measured frequency maps.","In the case of Gaussian, statistically isotropic fields, this is the optimal linear combination, as the variance is the only statistic of interest.","However, in many cases the signal we wish to isolate, or the foregrounds we wish to remove, are non-Gaussian and/or statistically anisotropic (in particular for Galactic foregrounds).","In such cases, it is possible that machine learning (ML) techniques can be used to exploit the non-Gaussian features of the foregrounds and thereby improve component separation.","However, many ML techniques require the use of complex, difficult-to-interpret operations on the data.","We propose a hybrid method whereby we train an ML model using only combinations of the data that $\\textit{do not contain the signal}$, and combine the resulting ML-predicted foreground estimate with the ILC solution to reduce the error from the ILC.","We demonstrate our methods on simulations of extragalactic temperature and Galactic polarization foregrounds, and show that our ML model can exploit non-Gaussian features, such as point sources and spatially-varying spectral indices, to produce lower-variance maps than ILC - eg, reducing the variance of the B-mode residual by factors of up to 5 - while preserving the signal of interest in an unbiased manner.","Moreover, we often find improved performance when applying our model to foreground models on which it was not trained."],"url":"http://arxiv.org/abs/2404.03557v1","category":"astro-ph.CO"}
{"created":"2024-04-04 16:07:21","title":"Robot Safety Monitoring using Programmable Light Curtains","abstract":"As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical. Currently, laser-based light curtain sensors are widely used in factories for safety monitoring. While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space. Furthermore, they are typically expensive. Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces. Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope. Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots. Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors. We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution.","sentences":["As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical.","Currently, laser-based light curtain sensors are widely used in factories for safety monitoring.","While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space.","Furthermore, they are typically expensive.","Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces.","Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope.","Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots.","Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors.","We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution."],"url":"http://arxiv.org/abs/2404.03556v1","category":"cs.RO"}
{"created":"2024-04-04 15:51:43","title":"Quantum querying based on multicontrolled Toffoli gates for causal Feynman loop configurations and directed acyclic graphs","abstract":"Quantum algorithms are a promising framework for a proper treatment of Feynman loop integrals due to the existence of a manifestly causal representation scenario. Particularly, unfolding causal configurations of multiloop Feynman diagrams is understood as querying \\textit{directed acyclic graph} (DAG) configurations of undirected graphs in graph theory. In this paper we present a quantum algorithm for querying causality of multiloop Feynman diagrams using an ingenious change in the logic of the design of the oracle operator. The construction of the quantum oracle is surprisingly based exclusively on multicontrolled Toffoli gates and XNOT gates. The efficiency of the algorithm is evaluated performing a comparison with a quantum algorithm based on binary clauses. Additionally, we explicitly analise several three-, four- and five-eloop topologies, which have not been previously explored due to their higher complexity.","sentences":["Quantum algorithms are a promising framework for a proper treatment of Feynman loop integrals due to the existence of a manifestly causal representation scenario.","Particularly, unfolding causal configurations of multiloop Feynman diagrams is understood as querying \\textit{directed acyclic graph} (DAG) configurations of undirected graphs in graph theory.","In this paper we present a quantum algorithm for querying causality of multiloop Feynman diagrams using an ingenious change in the logic of the design of the oracle operator.","The construction of the quantum oracle is surprisingly based exclusively on multicontrolled Toffoli gates and XNOT gates.","The efficiency of the algorithm is evaluated performing a comparison with a quantum algorithm based on binary clauses.","Additionally, we explicitly analise several three-, four- and five-eloop topologies, which have not been previously explored due to their higher complexity."],"url":"http://arxiv.org/abs/2404.03544v1","category":"quant-ph"}
{"created":"2024-04-04 15:47:51","title":"A swimming bacterium in a two-fluid model of a polymer solution","abstract":"We analyse the motion of a flagellated bacterium in a two-fluid medium using slender body theory. The two-fluid model is useful for describing a body moving through a complex fluid with a microstructure whose length scale is comparable to the characteristic scale of the body. This is true for bacterial motion in biological fluids (entangled polymer solutions), where the entanglement results in a porous microstructure with typical pore diameters comparable to or larger than the flagellar bundle diameter but smaller than the diameter of the bacterial head. Thus the polymer and solvent satisfy different boundary conditions on the flagellar bundle and move with different velocities close to it. This gives rise to a screening length $L_B$ within which the fluids exchange momentum and the relative velocity between the two fluids decays. In this work, both the solvent and polymer of the two-fluid medium are modeled as Newtonian fluids with different viscosities $\\mu_s$ and $\\mu_p$ (viscosity ratio $\\lambda = \\mu_p/\\mu_s$), thereby capturing the effects solely introduced by the microstructure of the complex fluid. From our calculations, we observe an increased drag anisotropy for a rigid, slender flagellar bundle moving through this two-fluid medium, resulting in an enhanced swimming velocity of the organism. The results are sensitive to the interaction between the bundle and the polymer and we discuss two physical scenarios corresponding to two types of interaction. Our model provides an explanation for the experimentally observed enhancement of swimming velocity of bacteria in entangled polymer solutions and motivates further experimental investigations.","sentences":["We analyse the motion of a flagellated bacterium in a two-fluid medium using slender body theory.","The two-fluid model is useful for describing a body moving through a complex fluid with a microstructure whose length scale is comparable to the characteristic scale of the body.","This is true for bacterial motion in biological fluids (entangled polymer solutions), where the entanglement results in a porous microstructure with typical pore diameters comparable to or larger than the flagellar bundle diameter but smaller than the diameter of the bacterial head.","Thus the polymer and solvent satisfy different boundary conditions on the flagellar bundle and move with different velocities close to it.","This gives rise to a screening length $L_B$ within which the fluids exchange momentum and the relative velocity between the two fluids decays.","In this work, both the solvent and polymer of the two-fluid medium are modeled as Newtonian fluids with different viscosities $\\mu_s$ and $\\mu_p$ (viscosity ratio $\\lambda = \\mu_p/\\mu_s$), thereby capturing the effects solely introduced by the microstructure of the complex fluid.","From our calculations, we observe an increased drag anisotropy for a rigid, slender flagellar bundle moving through this two-fluid medium, resulting in an enhanced swimming velocity of the organism.","The results are sensitive to the interaction between the bundle and the polymer and we discuss two physical scenarios corresponding to two types of interaction.","Our model provides an explanation for the experimentally observed enhancement of swimming velocity of bacteria in entangled polymer solutions and motivates further experimental investigations."],"url":"http://arxiv.org/abs/2404.03540v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 15:35:43","title":"COMO: Compact Mapping and Odometry","abstract":"We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points. Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points. The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference. To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames. Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry.","sentences":["We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points.","Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points.","The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference.","To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames.","Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry."],"url":"http://arxiv.org/abs/2404.03531v1","category":"cs.CV"}
{"created":"2024-04-04 15:32:34","title":"Operator growth and spread complexity in open quantum systems","abstract":"Commonly, the notion of \"quantum chaos'' refers to the fast scrambling of information throughout complex quantum systems undergoing unitary evolution. Motivated by the Krylov complexity and the operator growth hypothesis, we demonstrate that the entropy of the population distribution for an operator in time is a useful way to capture the complexity of the internal information dynamics of a system when subject to an environment and is, in principle, agnostic to the specific choice of operator basis. We demonstrate its effectiveness for the Sachdev-Ye-Kitaev (SYK) model, examining the dynamics of the system in both its Krylov basis and the basis of operator strings. We prove that the former basis minimises spread complexity while the latter is an eigenbasis for high dissipation. In both cases, we probe the long-time dynamics of the model and the phenomenological effects of decoherence on the complexity of the dynamics.","sentences":["Commonly, the notion of \"quantum chaos'' refers to the fast scrambling of information throughout complex quantum systems undergoing unitary evolution.","Motivated by the Krylov complexity and the operator growth hypothesis, we demonstrate that the entropy of the population distribution for an operator in time is a useful way to capture the complexity of the internal information dynamics of a system when subject to an environment and is, in principle, agnostic to the specific choice of operator basis.","We demonstrate its effectiveness for the Sachdev-Ye-Kitaev (SYK) model, examining the dynamics of the system in both its Krylov basis and the basis of operator strings.","We prove that the former basis minimises spread complexity while the latter is an eigenbasis for high dissipation.","In both cases, we probe the long-time dynamics of the model and the phenomenological effects of decoherence on the complexity of the dynamics."],"url":"http://arxiv.org/abs/2404.03529v1","category":"quant-ph"}
{"created":"2024-04-04 15:21:40","title":"Model Checking Recursive Probabilistic Programs with Conditioning","abstract":"We address the problem of model checking temporal logic specifications for probabilistic programs with recursive procedures, nested queries, and conditioning expressed with observe statements. We introduce probabilistic Operator Precedence Automata (pOPA), a new class of probabilistic pushdown automata suitable to model constructs and behaviors of probabilistic programs. We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\\mathcal{X}$) on a pOPA in single EXPTIME. POTL$^f_\\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements. We provide the first probabilistic model checking implementation of context-free language properties for probabilistic pushdown systems.","sentences":["We address the problem of model checking temporal logic specifications for probabilistic programs with recursive procedures, nested queries, and conditioning expressed with observe statements.","We introduce probabilistic Operator Precedence Automata (pOPA), a new class of probabilistic pushdown automata suitable to model constructs and behaviors of probabilistic programs.","We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\\mathcal{X}$) on a pOPA in single EXPTIME.","POTL$^f_\\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements.","We provide the first probabilistic model checking implementation of context-free language properties for probabilistic pushdown systems."],"url":"http://arxiv.org/abs/2404.03515v1","category":"cs.LO"}
{"created":"2024-04-04 15:14:49","title":"Materials for High Temperature Digital Electronics","abstract":"Silicon microelectronics, consisting of complementary metal oxide semiconductor (CMOS) technology, have changed nearly all aspects of human life from communication to transportation, entertainment, and healthcare. Despite the widespread and mainstream use, current silicon-based devices suffer significant reliability issues at temperatures exceeding 125 {\\deg}C. The emergent technological frontiers of space exploration, geothermal energy harvesting, nuclear energy, unmanned avionic systems, and autonomous driving will rely on control systems, sensors, and communication devices which operate at temperatures as high as 500 {\\deg}C and beyond. At these extreme temperatures, active (heat exchanger, phase change cooling) or passive (fins and thermal interface materials) cooling strategies add significant mass and complication which is often infeasible. Thus, new material solutions beyond conventional silicon CMOS devices are necessary for high temperature, resilient electronic systems. Accomplishing this will require a united effort to explore development, integration, and ultimately manufacturing of non-silicon-based logic and memory technologies, non-traditional metals for interconnects, and ceramic packaging technology.","sentences":["Silicon microelectronics, consisting of complementary metal oxide semiconductor (CMOS) technology, have changed nearly all aspects of human life from communication to transportation, entertainment, and healthcare.","Despite the widespread and mainstream use, current silicon-based devices suffer significant reliability issues at temperatures exceeding 125 {\\deg}C.","The emergent technological frontiers of space exploration, geothermal energy harvesting, nuclear energy, unmanned avionic systems, and autonomous driving will rely on control systems, sensors, and communication devices which operate at temperatures as high as 500 {\\deg}C and beyond.","At these extreme temperatures, active (heat exchanger, phase change cooling) or passive (fins and thermal interface materials) cooling strategies add significant mass and complication which is often infeasible.","Thus, new material solutions beyond conventional silicon CMOS devices are necessary for high temperature, resilient electronic systems.","Accomplishing this will require a united effort to explore development, integration, and ultimately manufacturing of non-silicon-based logic and memory technologies, non-traditional metals for interconnects, and ceramic packaging technology."],"url":"http://arxiv.org/abs/2404.03510v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 15:09:25","title":"Entanglement Degradation in the Presence of Markovian Noise: a Statistical Analysis","abstract":"Adopting a statistical approach we study the degradation of entanglement of a quantum system under the action of an ensemble of randomly distributed Markovian noise. This enables us to address scenarios where only limited information is available on the mechanisms that rule the noisy evolution of the model. As an application, we characterize the statistic of entanglement deterioration for a quantum memory formed by $n$ qudits that undergo randomly distributed local, uniform, Markovian noise evolution.","sentences":["Adopting a statistical approach we study the degradation of entanglement of a quantum system under the action of an ensemble of randomly distributed Markovian noise.","This enables us to address scenarios where only limited information is available on the mechanisms that rule the noisy evolution of the model.","As an application, we characterize the statistic of entanglement deterioration for a quantum memory formed by $n$ qudits that undergo randomly distributed local, uniform, Markovian noise evolution."],"url":"http://arxiv.org/abs/2404.03505v1","category":"quant-ph"}
{"created":"2024-04-04 14:58:16","title":"Total positivity and two inequalities by Athanasiadis and Tzanaki","abstract":"Let $\\Delta$ be a (d-1)-dimensional simplicial complex and h^\\Delta = (h_0^ ,.. , h_d) its h-vector. For a face uniform subdivision operation \\F we write \\Delta_\\F for the subdivided complex and H_\\F for the matrix such that h^{\\Delta_\\F} = H_\\F h^\\Delta.   In connection with the real rootedness of symmetric decompositions Athanasiadis and Tzanaki studied for strictly positive h-vectors the inequalities   h_0 / h_1 \\leq h_1 / h_{d-1} \\leq .... \\leq h_d / h_0 and h_1 / h_{d-1} \\geq ... \\geq h_{d-2} / h_2 \\geq h_{d-1} / h_1.   In this paper we show that if the inequalities holds for a simplicial complex $\\Delta$ and H_\\F is TP_2 (all entries and two minors are non-negative) then the inequalities hold for \\Delta_\\F.   We prove that if \\F is the barycentric subdivision then H_\\F is TP_2. If \\F is the rth-edgewise subdivision then work of Diaconis and Fulman shows H_\\F is TP_2. Indeed in this case by work of Mao and Wang H_\\F is even TP.","sentences":["Let $\\Delta$ be a (d-1)-dimensional simplicial complex and h^\\Delta = (h_0^ ,.. , h_d) its h-vector.","For a face uniform subdivision operation \\F we write \\Delta_\\F for the subdivided complex and H_\\F for the matrix such that h^{\\Delta_\\F} = H_\\F h^\\Delta.   ","In connection with the real rootedness of symmetric decompositions Athanasiadis and Tzanaki studied for strictly positive h-vectors the inequalities   h_0 / h_1 \\leq h_1 / h_{d-1} \\leq ....","\\leq h_d / h_0 and h_1 / h_{d-1} \\geq ...","\\geq h_{d-2} / h_2 \\geq h_{d-1} / h_1.   ","In this paper we show that if the inequalities holds for a simplicial complex $\\Delta$ and H_\\F is TP_2","(all entries and two minors are non-negative) then the inequalities hold for \\Delta_\\F.   We prove that if \\F is the barycentric subdivision then H_\\F is TP_2.","If \\F is the rth-edgewise subdivision then work of Diaconis and Fulman shows H_\\F is TP_2.","Indeed in this case by work of Mao and Wang H_\\F is even TP."],"url":"http://arxiv.org/abs/2404.03500v1","category":"math.CO"}
{"created":"2024-04-04 14:56:41","title":"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work","abstract":"In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.","sentences":["In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement.","This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants.","This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots.","By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting.","Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs.","The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry."],"url":"http://arxiv.org/abs/2404.03498v1","category":"cs.RO"}
{"created":"2024-04-04 14:43:43","title":"Design of Stickbug: a Six-Armed Precision Pollination Robot","abstract":"This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses. Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability. Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity. Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination. Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate. Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files.","sentences":["This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses.","Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability.","Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity.","Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination.","Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate.","Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files."],"url":"http://arxiv.org/abs/2404.03489v1","category":"cs.RO"}
{"created":"2024-04-04 14:42:04","title":"Electronic transport, metal-insulator transition, and Wigner crystallization in transition metal dichalcogenide monolayers","abstract":"Two recent electronic transport experiments from Columbia University and Harvard University have reported record high mobility and low channel densities in transition metal dichalcogenide (TMD) WSe$_2$ monolayers [J. Pack, et al., arXiv:2310.19782; A. Y. Joe, et al., Phys. Rev. Lett. 132, 056303 (2024)]. A two-dimensional (2D) metal-insulator transition (MIT) is demonstrated in the Columbia sample at low densities, a regime where the formation of a Wigner crystal (WC) is theoretically anticipated in the absence of disorder. We employ the finite-temperature Boltzmann theory to understand the low-temperature transport properties of monolayer TMDs, taking into account realistic disorder scattering. We analyze the experimental results, focusing on the 2D MIT behavior and the influence of temperature and density on mobility and resistivity in the metallic phase. We provide a discussion of the nontrivial carrier density dependence of our transport results. Our analysis elucidates the linear-in-$T$ resistivity in the metallic phase, attributing it to Friedel oscillations associated with screened charged impurities. Furthermore, we explore whether Coulomb disorder could lead to the MIT through either a quantum Anderson localization transition or a classical percolation transition. Our theoretical estimates of the disorder-induced MIT critical densities, although smaller, are within a factor of ~2 of the experimental critical density. We examine the exceptionally high melting temperature ~10 K of WCs observed experimentally in the MoSe$_2$ systems at low density, an order of magnitude larger than the pristine melting temperature. This suggests that the observed 2D low-density MIT behavior is likely a result of the complex interplay between disorder effects and interaction-driven WC physics, offering a comprehensive understanding of the low-temperature transport phenomena in TMD monolayers.","sentences":["Two recent electronic transport experiments from Columbia University and Harvard University have reported record high mobility and low channel densities in transition metal dichalcogenide (TMD) WSe$_2$ monolayers [J. Pack, et al., arXiv:2310.19782; A. Y. Joe, et al., Phys.","Rev. Lett.","132, 056303 (2024)].","A two-dimensional (2D) metal-insulator transition (MIT) is demonstrated in the Columbia sample at low densities, a regime where the formation of a Wigner crystal (WC) is theoretically anticipated in the absence of disorder.","We employ the finite-temperature Boltzmann theory to understand the low-temperature transport properties of monolayer TMDs, taking into account realistic disorder scattering.","We analyze the experimental results, focusing on the 2D MIT behavior and the influence of temperature and density on mobility and resistivity in the metallic phase.","We provide a discussion of the nontrivial carrier density dependence of our transport results.","Our analysis elucidates the linear-in-$T$ resistivity in the metallic phase, attributing it to Friedel oscillations associated with screened charged impurities.","Furthermore, we explore whether Coulomb disorder could lead to the MIT through either a quantum Anderson localization transition or a classical percolation transition.","Our theoretical estimates of the disorder-induced MIT critical densities, although smaller, are within a factor of ~2 of the experimental critical density.","We examine the exceptionally high melting temperature ~10 K of WCs observed experimentally in the MoSe$_2$ systems at low density, an order of magnitude larger than the pristine melting temperature.","This suggests that the observed 2D low-density MIT behavior is likely a result of the complex interplay between disorder effects and interaction-driven WC physics, offering a comprehensive understanding of the low-temperature transport phenomena in TMD monolayers."],"url":"http://arxiv.org/abs/2404.03488v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 14:40:55","title":"Explicit Witt basis over the tensor product of Clifford algebras and octonions","abstract":"In this article, we investigate how the Witt basis serves as a link between real and complex variables in higher-dimensional spaces. Our focus is on the detailed construction of the Witt basis within the tensor product space combining Clifford algebra and multiple octonionic spaces. This construction effectively introduces complex coordinates. The technique is based on a specific subgroup of octonionic automorphisms, distinguished by binary codes. This method allows us to perform a Hermitian analysis of the complex structures within the tensor product space.","sentences":["In this article, we investigate how the Witt basis serves as a link between real and complex variables in higher-dimensional spaces.","Our focus is on the detailed construction of the Witt basis within the tensor product space combining Clifford algebra and multiple octonionic spaces.","This construction effectively introduces complex coordinates.","The technique is based on a specific subgroup of octonionic automorphisms, distinguished by binary codes.","This method allows us to perform a Hermitian analysis of the complex structures within the tensor product space."],"url":"http://arxiv.org/abs/2404.03487v1","category":"math.CV"}
{"created":"2024-04-04 14:37:06","title":"Coupled harmonics due to time-modulated point scatterers","abstract":"We consider the resonance and scattering properties of a composite medium containing scatterers whose properties are modulated in time. When excited with an incident wave of a single frequency, the scattered field consists of a family of coupled harmonics at frequencies differing by the frequency of temporal modulation. Similarly, the temporal modulation induces coupling between the resonance frequencies, leading to exceptional points at certain modulation amplitudes. Moreover, the lack of energy conservation causes scattering coefficients to blow up when (complex) resonances cross the real axis. We have developed an integral operator approach to characterize the scattering problem and, for high-contrast scatterers, we present small-volume asymptotic formulas analogous to the classical results for the static (unmodulated) case. We conclude the paper with a boundary integral formulation of the time-modulated problem, which gives an efficient numerical approach and corroborates the asymptotic formulas.","sentences":["We consider the resonance and scattering properties of a composite medium containing scatterers whose properties are modulated in time.","When excited with an incident wave of a single frequency, the scattered field consists of a family of coupled harmonics at frequencies differing by the frequency of temporal modulation.","Similarly, the temporal modulation induces coupling between the resonance frequencies, leading to exceptional points at certain modulation amplitudes.","Moreover, the lack of energy conservation causes scattering coefficients to blow up when (complex) resonances cross the real axis.","We have developed an integral operator approach to characterize the scattering problem and, for high-contrast scatterers, we present small-volume asymptotic formulas analogous to the classical results for the static (unmodulated) case.","We conclude the paper with a boundary integral formulation of the time-modulated problem, which gives an efficient numerical approach and corroborates the asymptotic formulas."],"url":"http://arxiv.org/abs/2404.03483v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 14:33:37","title":"Evolutionary theory of convective organization","abstract":"Observed patterns of convective cloud would be extremely improbable from random walks in an abstract space of configurations. Forcing is sometimes the driver, but complexity can also develop spontaneously. Here pattern evolution is considered as a natural selection process in a strategic game among configurations, akin to ecological succession. Information (entropy) quantifies improbability, interpreted as Darwinian fitness to the extent larger-scale forcings are properly accounted. Reconciling inferred or revealed fitness with energetics could make convection a showcase of evolutionary theory, simply as reinterpretation of spectral kinetic energy (KE) budgets for dry convection. For moist convection, the flux of a total energy E, the teleological reason for convection, is conjectured to be the central resource of an evolutionary game, with water playing a constraining role like nutrients in ecology. Shannon information H is reviewed in context of this evolutionary reasoning. Analysis of a new Cloud Botany shallow convection simulation set shows growth of H over tens of hours (perhaps more pertinently, it grows with KE throughput or cumulative buoyancy flux). Anisotropy and precipitation boost H in qualitatively distinct and contingent ways, like separate species or ecological guilds. Deep convection over lowland South America also shows many-hour evolution times, beyond simple convective adjustment notions. If evolving horizontal patterns contain or imply information about vertical density profiles, they could become a new resource in data assimilation to predict larger scale flow.","sentences":["Observed patterns of convective cloud would be extremely improbable from random walks in an abstract space of configurations.","Forcing is sometimes the driver, but complexity can also develop spontaneously.","Here pattern evolution is considered as a natural selection process in a strategic game among configurations, akin to ecological succession.","Information (entropy) quantifies improbability, interpreted as Darwinian fitness to the extent larger-scale forcings are properly accounted.","Reconciling inferred or revealed fitness with energetics could make convection a showcase of evolutionary theory, simply as reinterpretation of spectral kinetic energy (KE) budgets for dry convection.","For moist convection, the flux of a total energy E, the teleological reason for convection, is conjectured to be the central resource of an evolutionary game, with water playing a constraining role like nutrients in ecology.","Shannon information H is reviewed in context of this evolutionary reasoning.","Analysis of a new Cloud Botany shallow convection simulation set shows growth of H over tens of hours (perhaps more pertinently, it grows with KE throughput or cumulative buoyancy flux).","Anisotropy and precipitation boost H in qualitatively distinct and contingent ways, like separate species or ecological guilds.","Deep convection over lowland South America also shows many-hour evolution times, beyond simple convective adjustment notions.","If evolving horizontal patterns contain or imply information about vertical density profiles, they could become a new resource in data assimilation to predict larger scale flow."],"url":"http://arxiv.org/abs/2404.03480v1","category":"nlin.AO"}
{"created":"2024-04-04 14:23:42","title":"Measurable Structure Factors of Dense Dispersions Containing Polydisperse, Optically Inhomogeneous Particles","abstract":"We exemplarily investigate how optical properties of single scatterers in interacting multi-particle systems influence measurable structure factors. Both particles with linear gradients of their scattering length density and core-shell structures evoke characteristic deviations between the weighted sum $\\langle S(Q)\\rangle$ of partial structure factors in a multicomponent system and experimentally accessible, measurable structure factors $S_{\\mathrm{M}}(Q)$. While $\\langle S(Q)\\rangle$ contains only structural information of self-organising systems, $S_{\\mathrm{M}}(Q)$ additionally is influenced by optical properties of their constituents resulting in features such as changing amplitudes, additional peaks in the low wavevector region or splitting of higher-order maxima which are not related to structural reasons. Hence, a careful data analysis regarding size-distribution and optical properties of single scatters is mandatory to avoid a misinterpretation of measurable structure factors.","sentences":["We exemplarily investigate how optical properties of single scatterers in interacting multi-particle systems influence measurable structure factors.","Both particles with linear gradients of their scattering length density and core-shell structures evoke characteristic deviations between the weighted sum $\\langle S(Q)\\rangle$ of partial structure factors in a multicomponent system and experimentally accessible, measurable structure factors $S_{\\mathrm{M}}(Q)$. While $\\langle S(Q)\\rangle$ contains only structural information of self-organising systems, $S_{\\mathrm{M}}(Q)$ additionally is influenced by optical properties of their constituents resulting in features such as changing amplitudes, additional peaks in the low wavevector region or splitting of higher-order maxima which are not related to structural reasons.","Hence, a careful data analysis regarding size-distribution and optical properties of single scatters is mandatory to avoid a misinterpretation of measurable structure factors."],"url":"http://arxiv.org/abs/2404.03470v1","category":"cond-mat.soft"}
{"created":"2024-04-04 14:23:12","title":"Class-E, Active Electrically-Small Antenna for High-Power Wideband Transmission at the High-Frequency (HF) Band","abstract":"Antennas operating at the high-frequency (HF) band (3-30 MHz) are frequently electrically small due to the large wavelength of electromagnetic waves (10-100 m). However, the bandwidth-efficiency products of passively matched electrically small antennas (ESAs) are fundamentally limited. Wideband HF waveforms using bandwidths of 24 kHz or more have recently received significant attention in military communications applications. Efficiently radiating such signals from conventional passive ESAs is very challenging due to fundamental physical limits on bandwidth-efficiency products of ESAs. However, active antennas are not subject to the same constraints. In this work, we present the design and experimental characterization of a high-power, active ESA with enhanced bandwidth-efficiency product compared to {that of} passively matched ESAs. Specifically, the proposed active ESA can radiate wideband HF signals with banwidths of 24 kHz or more, with total efficiencies up to 80$\\%$, and radiated power levels approaching 100 W. Our approach uses a highly-efficient, integrated class-E switching circuit specifically designed to drive an electrically small, high-Q HF antenna over a bandwidth exceeding 24 kHz. Using a high-Q RLC antenna model, we have successfully demonstrated wideband binary ASK, PSK, and FSK modulations with the proposed class-E switching architecture. Experimental results indicate that the bandwidth-efficiency product of this class-E active antenna is 5.4-9.8 dB higher than that of an equivalent passive design with the same data rate, and bit-error-rate (BER).","sentences":["Antennas operating at the high-frequency (HF) band (3-30 MHz) are frequently electrically small due to the large wavelength of electromagnetic waves (10-100 m).","However, the bandwidth-efficiency products of passively matched electrically small antennas (ESAs) are fundamentally limited.","Wideband HF waveforms using bandwidths of 24 kHz or more have recently received significant attention in military communications applications.","Efficiently radiating such signals from conventional passive ESAs is very challenging due to fundamental physical limits on bandwidth-efficiency products of ESAs.","However, active antennas are not subject to the same constraints.","In this work, we present the design and experimental characterization of a high-power, active ESA with enhanced bandwidth-efficiency product compared to {that of} passively matched ESAs.","Specifically, the proposed active ESA can radiate wideband HF signals with banwidths of 24 kHz or more, with total efficiencies up to 80$\\%$, and radiated power levels approaching 100 W. Our approach uses a highly-efficient, integrated class-E switching circuit specifically designed to drive an electrically small, high-Q HF antenna over a bandwidth exceeding 24 kHz.","Using a high-Q RLC antenna model, we have successfully demonstrated wideband binary ASK, PSK, and FSK modulations with the proposed class-E switching architecture.","Experimental results indicate that the bandwidth-efficiency product of this class-E active antenna is 5.4-9.8 dB higher than that of an equivalent passive design with the same data rate, and bit-error-rate (BER)."],"url":"http://arxiv.org/abs/2404.03468v1","category":"physics.app-ph"}
{"created":"2024-04-04 14:18:23","title":"Patrick Moss 25/10/1947--17/3/2024","abstract":"Patrick Moss (1947--2024) had two distinct lives as a mathematician. The first was as a ring theorist in the late 1970s, in which he worked with Ginn and Lenagan as a student. After a long career as an inspirational mathematics teacher, Patrick completed a doctorate under my supervision in 2003. This led to a second mathematical life in arithmetic dynamics almost forty years after his first period of research. This is a short obituary of his remarkable contributions, several of which have stimulated further research.","sentences":["Patrick Moss (1947--2024) had two distinct lives as a mathematician.","The first was as a ring theorist in the late 1970s, in which he worked with Ginn and Lenagan as a student.","After a long career as an inspirational mathematics teacher, Patrick completed a doctorate under my supervision in 2003.","This led to a second mathematical life in arithmetic dynamics almost forty years after his first period of research.","This is a short obituary of his remarkable contributions, several of which have stimulated further research."],"url":"http://arxiv.org/abs/2404.03464v1","category":"math.HO"}
{"created":"2024-04-04 14:01:47","title":"Charting the Complex Structure Landscape of F-theory","abstract":"We explore the landscape of F-theory compactifications on Calabi--Yau fourfolds whose complex structure moduli space is the thrice-punctured sphere. As a first part, we enumerate all such Calabi--Yau fourfolds under the additional requirement that it has a large complex structure and conifold point at two of the punctures. We find 14 monodromy tuples by demanding the monodromy around infinity to be quasi-unipotent. As second part, we study the four different types of phases arising at infinity. For each we consider a working example where we determine the leading periods and other physical couplings. We also included a notebook that sets up the period vectors for any of these models.","sentences":["We explore the landscape of F-theory compactifications on Calabi--Yau fourfolds whose complex structure moduli space is the thrice-punctured sphere.","As a first part, we enumerate all such Calabi--Yau fourfolds under the additional requirement that it has a large complex structure and conifold point at two of the punctures.","We find 14 monodromy tuples by demanding the monodromy around infinity to be quasi-unipotent.","As second part, we study the four different types of phases arising at infinity.","For each we consider a working example where we determine the leading periods and other physical couplings.","We also included a notebook that sets up the period vectors for any of these models."],"url":"http://arxiv.org/abs/2404.03456v1","category":"hep-th"}
{"created":"2024-04-04 13:44:41","title":"Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering","abstract":"This paper proposes an algorithm for combined contact detection and state estimation for legged robots. The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground. The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state. The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa. This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot. The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts. The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts.","sentences":["This paper proposes an algorithm for combined contact detection and state estimation for legged robots.","The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground.","The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state.","The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa.","This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot.","The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts.","The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts."],"url":"http://arxiv.org/abs/2404.03444v1","category":"cs.RO"}
{"created":"2024-04-04 13:39:49","title":"Privacy Engineering From Principles to Practice: A Roadmap","abstract":"Privacy engineering is gaining momentum in industry and academia alike. So far, manifold low-level primitives and higher-level methods and strategies have successfully been established. Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice.","sentences":["Privacy engineering is gaining momentum in industry and academia alike.","So far, manifold low-level primitives and higher-level methods and strategies have successfully been established.","Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice."],"url":"http://arxiv.org/abs/2404.03442v1","category":"cs.CR"}
{"created":"2024-04-04 13:27:22","title":"Learning From Simplicial Data Based on Random Walks and 1D Convolutions","abstract":"Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks.","sentences":["Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes.","While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically.","To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships.","Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks.","We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks."],"url":"http://arxiv.org/abs/2404.03434v1","category":"cs.LG"}
{"created":"2024-04-04 13:24:33","title":"MEDIATE: Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange","abstract":"Recent advances in multi-agent systems (MAS) have shown that incorporating peer incentivization (PI) mechanisms vastly improves cooperation. Especially in social dilemmas, communication between the agents helps to overcome sub-optimal Nash equilibria. However, incentivization tokens need to be carefully selected. Furthermore, real-world applications might yield increased privacy requirements and limited exchange. Therefore, we extend the PI protocol for mutual acknowledgment token exchange (MATE) and provide additional analysis on the impact of the chosen tokens. Building upon those insights, we propose mutually endorsed distributed incentive acknowledgment token exchange (MEDIATE), an extended PI architecture employing automatic token derivation via decentralized consensus. Empirical results show the stable agreement on appropriate tokens yielding superior performance compared to static tokens and state-of-the-art approaches in different social dilemma environments with various reward distributions.","sentences":["Recent advances in multi-agent systems (MAS) have shown that incorporating peer incentivization (PI) mechanisms vastly improves cooperation.","Especially in social dilemmas, communication between the agents helps to overcome sub-optimal Nash equilibria.","However, incentivization tokens need to be carefully selected.","Furthermore, real-world applications might yield increased privacy requirements and limited exchange.","Therefore, we extend the PI protocol for mutual acknowledgment token exchange (MATE) and provide additional analysis on the impact of the chosen tokens.","Building upon those insights, we propose mutually endorsed distributed incentive acknowledgment token exchange (MEDIATE), an extended PI architecture employing automatic token derivation via decentralized consensus.","Empirical results show the stable agreement on appropriate tokens yielding superior performance compared to static tokens and state-of-the-art approaches in different social dilemma environments with various reward distributions."],"url":"http://arxiv.org/abs/2404.03431v1","category":"cs.MA"}
{"created":"2024-04-04 13:13:47","title":"GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration","abstract":"State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub.","sentences":["State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants.","These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization.","This often leads to misalignments in the calibration process.","Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations.","This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems.","Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms.","We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment.","We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results.","Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms.","The code is open source and available on GitHub."],"url":"http://arxiv.org/abs/2404.03427v1","category":"cs.RO"}
{"created":"2024-04-04 12:58:46","title":"Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View","abstract":"Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors. However, real-world scenarios are far more complex and exceed the capabilities of these methods. We therefore propose a hybrid method following a divide-and-conquer strategy. We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components. By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image. We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system. This enables the pipeline to naturally improve as future methods can replace the individual modules. We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works. Project page: https://andreeadogaru.github.io/Gen3DSR.","sentences":["Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors.","However, real-world scenarios are far more complex and exceed the capabilities of these methods.","We therefore propose a hybrid method following a divide-and-conquer strategy.","We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components.","By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image.","We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system.","This enables the pipeline to naturally improve as future methods can replace the individual modules.","We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works.","Project page: https://andreeadogaru.github.io/Gen3DSR."],"url":"http://arxiv.org/abs/2404.03421v1","category":"cs.CV"}
{"created":"2024-04-04 12:46:01","title":"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens","abstract":"This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/","sentences":["This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding.","The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos.","Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos.","MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components.","The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively.","Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/"],"url":"http://arxiv.org/abs/2404.03413v1","category":"cs.CV"}
{"created":"2024-04-04 12:45:49","title":"RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling","abstract":"Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems. For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures. Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered. We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair. For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods. We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control. Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques. In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement. Accompanying code and video can be found at https://mit-realm.github.io/radium/","sentences":["Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems.","For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures.","Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered.","We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair.","For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods.","We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control.","Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques.","In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement.","Accompanying code and video can be found at https://mit-realm.github.io/radium/"],"url":"http://arxiv.org/abs/2404.03412v1","category":"cs.RO"}
{"created":"2024-04-04 12:08:28","title":"Curves in the Fourier zeros of polytopal regions and the Pompeiu problem","abstract":"We prove that any finite union $P$ of interior-disjoint polytopes in ${\\mathbb R}^d$ has the Pompeiu property, a result first proved by Williams [Wil76]. This means that if a continuous function $f$ on $R^d$ integrates to 0 on any congruent copy of $P$ then $f$ is identically 0. By a fundamental result of Brown, Schreiber and Taylor [BST73] this is equivalent to showing that the Fourier-Laplace transform of the indicator function of $P$ does not vanish identically on any 0-centered complex sphere in ${\\mathbb C}^d$ . Our proof initially follows the recent one of Machado and Robins [MR23] who are using the Brion-Barvinok formula for the Fourier-Laplace transform of a polytope. But we simplify this method considerably by removing the use of properties of Bessel function zeros. Instead we use some elementary arguments on the growth of linear combinations of exponentials with rational functions as coefficients. Our approach allows us to prove the non-existence of complex spheres of any center in the zero-set of the Fourier-Laplace transform. The planar case is even simpler in that we do not even need the Brion-Barvinok formula. We then go further in the question of which sets can be contained in the null set of the Fourier-Laplace transform of a polytope by extending results of Engel [Eng23] who showed that rationally parametrized hypersurfaces, under some mild conditions, cannot be contained in this null-set. We show that a rationally parametrized curve which is not contained in an affine hyperplane in ${\\mathbb C}^d$ cannot be contained in this null-set. Results about curves parametrized by meromorphic functions are also given.","sentences":["We prove that any finite union $P$ of interior-disjoint polytopes in ${\\mathbb R}^d$ has the Pompeiu property, a result first proved by Williams [Wil76].","This means that if a continuous function $f$ on $R^d$ integrates to 0 on any congruent copy of $P$ then $f$ is identically 0.","By a fundamental result of Brown, Schreiber and Taylor [BST73] this is equivalent to showing that the Fourier-Laplace transform of the indicator function of $P$ does not vanish identically on any 0-centered complex sphere in ${\\mathbb C}^d$ .","Our proof initially follows the recent one of Machado and Robins","[MR23] who are using the Brion-Barvinok formula for the Fourier-Laplace transform of a polytope.","But we simplify this method considerably by removing the use of properties of Bessel function zeros.","Instead we use some elementary arguments on the growth of linear combinations of exponentials with rational functions as coefficients.","Our approach allows us to prove the non-existence of complex spheres of any center in the zero-set of the Fourier-Laplace transform.","The planar case is even simpler in that we do not even need the Brion-Barvinok formula.","We then go further in the question of which sets can be contained in the null set of the Fourier-Laplace transform of a polytope by extending results of Engel [Eng23] who showed that rationally parametrized hypersurfaces, under some mild conditions, cannot be contained in this null-set.","We show that a rationally parametrized curve which is not contained in an affine hyperplane in ${\\mathbb C}^d$ cannot be contained in this null-set.","Results about curves parametrized by meromorphic functions are also given."],"url":"http://arxiv.org/abs/2404.03405v1","category":"math.CA"}
{"created":"2024-04-04 12:03:15","title":"On steady solutions of the Hall-MHD system in Besov spaces","abstract":"In this paper, we investigate the well-posedness and ill-posedness issues for the incompressible stationary Hall-magnetohydrodynamic (Hall-MHD) system in $\\mathbb{R}^3.$ We first show the existence and uniqueness of solutions provided with the forces in $\\dot B^{3/p-3}_{p,r}(\\mathbb{R}^3)$ for $1\\leq p <3$ and $r=1$. Moreover, this result can be extended to any $1\\leq r\\leq \\infty$ whenever $p=2,$ without any additional assumption on the physical parameters. On the other hand, we establish some ill-posedness results for Hall-MHD system by using the discontinuity of the solution mapping of the three-dimensional stationary Navier-Stokes equations in \\emph{critical} function spaces $\\dot{B}^{3/p-1}_{p,r}(\\mathbb{R}^3)$ ($p\\geq 3$).","sentences":["In this paper, we investigate the well-posedness and ill-posedness issues for the incompressible stationary Hall-magnetohydrodynamic (Hall-MHD) system in $\\mathbb{R}^3.$ We first show the existence and uniqueness of solutions provided with the forces in $\\dot B^{3/p-3}_{p,r}(\\mathbb{R}^3)$ for $1\\leq p <3$ and $r=1$. Moreover, this result can be extended to any $1\\leq r\\leq \\infty$ whenever $p=2,$ without any additional assumption on the physical parameters.","On the other hand, we establish some ill-posedness results for Hall-MHD system by using the discontinuity of the solution mapping of the three-dimensional stationary Navier-Stokes equations in \\emph{critical} function spaces $\\dot{B}^{3/p-1}_{p,r}(\\mathbb{R}^3)$ ($p\\geq 3$)."],"url":"http://arxiv.org/abs/2404.03402v1","category":"math.AP"}
{"created":"2024-04-04 12:00:01","title":"An asynchronous discontinuous Galerkin method for massively parallel PDE solvers","abstract":"The discontinuous Galerkin (DG) method is widely being used to solve hyperbolic partial differential equations (PDEs) due to its ability to provide high-order accurate solutions in complex geometries, capture discontinuities, and exhibit high arithmetic intensity. However, the scalability of DG-based solvers is impeded by communication bottlenecks arising from the data movement and synchronization requirements at extreme scales. To address these challenges, recent studies have focused on the development of asynchronous computing approaches for PDE solvers. Herein, we introduce the asynchronous DG (ADG) method, which combines the benefits of the DG method with asynchronous computing to overcome communication bottlenecks. The ADG method relaxes the need for data communication and synchronization at a mathematical level, allowing processing elements to operate independently regardless of the communication status, thus potentially improving the scalability of solvers. The proposed ADG method ensures flux conservation and effectively addresses challenges arising from asynchrony. To assess its stability, Fourier-mode analysis is employed to examine the dissipation and dispersion behavior of fully-discrete equations that use the DG and ADG schemes along with the Runge-Kutta (RK) time integration scheme. Furthermore, an error analysis within a statistical framework is presented, which demonstrates that the ADG method with standard numerical fluxes achieves at most first-order accuracy. To recover accuracy, we introduce asynchrony-tolerant (AT) fluxes that utilize data from multiple time levels. Extensive numerical experiments were conducted to validate the performance of the ADG-AT scheme for both linear and nonlinear problems.","sentences":["The discontinuous Galerkin (DG) method is widely being used to solve hyperbolic partial differential equations (PDEs) due to its ability to provide high-order accurate solutions in complex geometries, capture discontinuities, and exhibit high arithmetic intensity.","However, the scalability of DG-based solvers is impeded by communication bottlenecks arising from the data movement and synchronization requirements at extreme scales.","To address these challenges, recent studies have focused on the development of asynchronous computing approaches for PDE solvers.","Herein, we introduce the asynchronous DG (ADG) method, which combines the benefits of the DG method with asynchronous computing to overcome communication bottlenecks.","The ADG method relaxes the need for data communication and synchronization at a mathematical level, allowing processing elements to operate independently regardless of the communication status, thus potentially improving the scalability of solvers.","The proposed ADG method ensures flux conservation and effectively addresses challenges arising from asynchrony.","To assess its stability, Fourier-mode analysis is employed to examine the dissipation and dispersion behavior of fully-discrete equations that use the DG and ADG schemes along with the Runge-Kutta (RK) time integration scheme.","Furthermore, an error analysis within a statistical framework is presented, which demonstrates that the ADG method with standard numerical fluxes achieves at most first-order accuracy.","To recover accuracy, we introduce asynchrony-tolerant (AT) fluxes that utilize data from multiple time levels.","Extensive numerical experiments were conducted to validate the performance of the ADG-AT scheme for both linear and nonlinear problems."],"url":"http://arxiv.org/abs/2404.03399v1","category":"physics.comp-ph"}
{"created":"2024-04-04 11:57:11","title":"Characterization of the Early Dynamics of Solar Coronal Bright Fronts","abstract":"We present a comprehensive characterization of 26 CME-driven compressive waves known as Coronal Bright Fronts (CBFs) observed in the low solar corona between 2010 and 2017. These CBFs have been found to be associated with SEP events near Earth, indicating their importance in understanding space weather phenomena. The aim of this study is to analyze and describe the early dynamics of CBFs using a physics-based heliospheric SEP forecasting system known as the SPREAdFAST framework. This framework utilizes a chain of data-driven analytic and numerical models to predict SEP fluxes at multiple locations in the inner heliosphere by considering their acceleration at CMEs near the Sun and subsequent interplanetary transport. To estimate the time-dependent plasma and compression parameters of the CBFs, we utilized sequences of base-difference images obtained from the AIA instrument on board the SDO satellite, and measurements of the height-time profiles of the CMEs obtained from the LASCO instrument on board the SOHO satellite. We employed kinematic measurements and plasma model results to derive these parameters. The SPREAdFAST framework facilitated the analysis and correlation of these observations with SEP events near Earth. Our analysis yielded statistical relations and distributions for both the shocks and plasma parameters associated with the 26 CBFs investigated. By combining the observations from the AIA and LASCO instruments, as well as the data products from the SPREAdFAST framework, we obtained a comprehensive understanding of the early dynamics of CBFs, including their temporal evolution, plasma properties, and compressional characteristics. These findings contribute to the growing body of knowledge in the field and have implications for space weather forecasting and the study of SEP events.","sentences":["We present a comprehensive characterization of 26 CME-driven compressive waves known as Coronal Bright Fronts (CBFs) observed in the low solar corona between 2010 and 2017.","These CBFs have been found to be associated with SEP events near Earth, indicating their importance in understanding space weather phenomena.","The aim of this study is to analyze and describe the early dynamics of CBFs using a physics-based heliospheric SEP forecasting system known as the SPREAdFAST framework.","This framework utilizes a chain of data-driven analytic and numerical models to predict SEP fluxes at multiple locations in the inner heliosphere by considering their acceleration at CMEs near the Sun and subsequent interplanetary transport.","To estimate the time-dependent plasma and compression parameters of the CBFs, we utilized sequences of base-difference images obtained from the AIA instrument on board the SDO satellite, and measurements of the height-time profiles of the CMEs obtained from the LASCO instrument on board the SOHO satellite.","We employed kinematic measurements and plasma model results to derive these parameters.","The SPREAdFAST framework facilitated the analysis and correlation of these observations with SEP events near Earth.","Our analysis yielded statistical relations and distributions for both the shocks and plasma parameters associated with the 26 CBFs investigated.","By combining the observations from the AIA and LASCO instruments, as well as the data products from the SPREAdFAST framework, we obtained a comprehensive understanding of the early dynamics of CBFs, including their temporal evolution, plasma properties, and compressional characteristics.","These findings contribute to the growing body of knowledge in the field and have implications for space weather forecasting and the study of SEP events."],"url":"http://arxiv.org/abs/2404.03396v1","category":"astro-ph.SR"}
{"created":"2024-04-04 11:56:51","title":"Movable Antennas-Assisted Secure Transmission Without Eavesdroppers' Instantaneous CSI","abstract":"Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions. In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers. Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel. Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation. Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice. However, the problem is highly non-convex because the objective includes the complex incomplete gamma function. To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model. Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm. Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas.","sentences":["Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions.","In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers.","Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel.","Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation.","Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice.","However, the problem is highly non-convex because the objective includes the complex incomplete gamma function.","To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model.","Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm.","Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.","As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas."],"url":"http://arxiv.org/abs/2404.03395v1","category":"cs.IT"}
{"created":"2024-04-04 11:32:17","title":"A unified Euler--Lagrange system for analyzing continuous-time accelerated gradient methods","abstract":"This paper presents an Euler--Lagrange system for a continuous-time model of the accelerated gradient methods in smooth convex optimization and proposes an associated Lyapunov-function-based convergence analysis framework. Recently, ordinary differential equations (ODEs) with dumping terms have been developed to intuitively interpret the accelerated gradient methods, and the design of unified model describing the various individual ODE models have been examined. In existing reports, the Lagrangian, which results in the Euler-Lagrange equation, and the Lyapunov function for the convergence analysis have been separately proposed for each ODE. This paper proposes a unified Euler--Lagrange system and its Lyapunov function to cover the existing various models. In the convergence analysis using the Lyapunov function, a condition that parameters in the Lagrangian and Lyapunov function must satisfy is derived, and a parameter design for improving the convergence rate naturally results in the mysterious dumping coefficients. Especially, a symmetric Bregman divergence can lead to a relaxed condition of the parameters and a resulting improved convergence rate. As an application of this study, a slight modification in the Lyapunov function establishes the similar convergence proof for ODEs with smooth approximation in nondifferentiable objective function minimization.","sentences":["This paper presents an Euler--Lagrange system for a continuous-time model of the accelerated gradient methods in smooth convex optimization and proposes an associated Lyapunov-function-based convergence analysis framework.","Recently, ordinary differential equations (ODEs) with dumping terms have been developed to intuitively interpret the accelerated gradient methods, and the design of unified model describing the various individual ODE models have been examined.","In existing reports, the Lagrangian, which results in the Euler-Lagrange equation, and the Lyapunov function for the convergence analysis have been separately proposed for each ODE.","This paper proposes a unified Euler--Lagrange system and its Lyapunov function to cover the existing various models.","In the convergence analysis using the Lyapunov function, a condition that parameters in the Lagrangian and Lyapunov function must satisfy is derived, and a parameter design for improving the convergence rate naturally results in the mysterious dumping coefficients.","Especially, a symmetric Bregman divergence can lead to a relaxed condition of the parameters and a resulting improved convergence rate.","As an application of this study, a slight modification in the Lyapunov function establishes the similar convergence proof for ODEs with smooth approximation in nondifferentiable objective function minimization."],"url":"http://arxiv.org/abs/2404.03383v1","category":"math.OC"}
{"created":"2024-04-04 11:14:01","title":"The Nearest Graph Laplacian in Frobenius Norm","abstract":"We address the problem of finding the nearest graph Laplacian to a given matrix, with the distance measured using the Frobenius norm. Specifically, for the directed graph Laplacian, we propose two novel algorithms by reformulating the problem as convex quadratic optimization problems with a special structure: one based on the active set method and the other on direct computation of Karush-Kuhn-Tucker (KKT) points. The proposed algorithms can be applied to system identification and model reduction problems involving Laplacian dynamics. We demonstrate that these algorithms possess lower time complexities and the finite termination property, unlike the interior point method and V-FISTA, the latter of which is an accelerated projected gradient method. Our numerical experiments confirm the effectiveness of the proposed algorithms.","sentences":["We address the problem of finding the nearest graph Laplacian to a given matrix, with the distance measured using the Frobenius norm.","Specifically, for the directed graph Laplacian, we propose two novel algorithms by reformulating the problem as convex quadratic optimization problems with a special structure: one based on the active set method and the other on direct computation of Karush-Kuhn-Tucker (KKT) points.","The proposed algorithms can be applied to system identification and model reduction problems involving Laplacian dynamics.","We demonstrate that these algorithms possess lower time complexities and the finite termination property, unlike the interior point method and V-FISTA, the latter of which is an accelerated projected gradient method.","Our numerical experiments confirm the effectiveness of the proposed algorithms."],"url":"http://arxiv.org/abs/2404.03371v1","category":"math.OC"}
{"created":"2024-04-04 11:09:49","title":"Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity","abstract":"Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states. Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions. This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors. PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as graphs. To take advantage of this relational inductive bias, graph neural networks (GNNs) have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies. In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected. Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy. This approach has not been explored in GNN literature yet because many available PSH graphs are limited to their respective subsystem boundaries, which makes the method unsuitable to be applied directly. In this work, we introduce the application of spectral-temporal graph neural networks, which leverage self-attention mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors. Our method effectively fuses data from the PSH's subsystems by operating on a unified, system-wide graph, learned directly from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability.","sentences":["Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states.","Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions.","This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors.","PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as graphs.","To take advantage of this relational inductive bias, graph neural networks (GNNs) have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies.","In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected.","Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy.","This approach has not been explored in GNN literature yet because many available PSH graphs are limited to their respective subsystem boundaries, which makes the method unsuitable to be applied directly.","In this work, we introduce the application of spectral-temporal graph neural networks, which leverage self-attention mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors.","Our method effectively fuses data from the PSH's subsystems by operating on a unified, system-wide graph, learned directly from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability."],"url":"http://arxiv.org/abs/2404.03368v1","category":"cs.LG"}
{"created":"2024-04-04 11:09:04","title":"Photonic Quantum Computing","abstract":"Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation. Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature. The field is largely divided between discrete- and continuous-variable photonic quantum computation. In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set. Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors. In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position). The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection. Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation. This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning.","sentences":["Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation.","Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature.","The field is largely divided between discrete- and continuous-variable photonic quantum computation.","In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set.","Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors.","In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position).","The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection.","Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation.","This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning."],"url":"http://arxiv.org/abs/2404.03367v1","category":"quant-ph"}
{"created":"2024-04-04 11:04:44","title":"Space Physiology and Technology: Musculoskeletal Adaptations, Countermeasures, and the Opportunity for Wearable Robotics","abstract":"Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth. While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors. A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space. This review focuses on the impact of the environment in space on the musculoskeletal system. It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health. Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative.","sentences":["Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth.","While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors.","A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space.","This review focuses on the impact of the environment in space on the musculoskeletal system.","It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health.","Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative."],"url":"http://arxiv.org/abs/2404.03363v1","category":"cs.RO"}
{"created":"2024-04-04 10:51:51","title":"Implementation of complex-valued sliding mode controllers in three-phase power converters","abstract":"This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters. The paper includes the description of the algorithms and a detailed analysis of the proposed implementations. The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system. The performance of the methods is compared in numerical simulations, and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform.","sentences":["This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters.","The paper includes the description of the algorithms and a detailed analysis of the proposed implementations.","The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system.","The performance of the methods is compared in numerical simulations, and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform."],"url":"http://arxiv.org/abs/2404.03358v1","category":"eess.SY"}
{"created":"2024-04-04 10:49:52","title":"Fast Computation of Robust Dynamic Operating Envelopes Based on Non-convex OPF for Unbalanced Distribution Networks","abstract":"Robust dynamic operating envelopes (RDOEs) solve the problem of secure allocation of latent network capacity to flexible distributed energy resources (DER) in unbalanced distribution networks. As the computational complexity of RDOEs is much higher than that of DOEs, which disregard uncertainties in network parameters and DER capacity utilisation, existing approaches to computing RDOEs have relied on linearised unbalanced three-phase optimal power flow (UTOPF) models to numerate the network feasible region approximately. The use of linearised models, however, risks producing RDOEs that undermine network integrity due to inherent errors in the approximation. This letter presents a practical sensitivity-filtering technique to simplify RDOE numerical computation based on non-convex UTOPF formulations. The accuracy and efficiency of the proposed approach are demonstrated on RDOE allocation with various fairness metrics by testing on representative Australian distribution networks.","sentences":["Robust dynamic operating envelopes (RDOEs) solve the problem of secure allocation of latent network capacity to flexible distributed energy resources (DER) in unbalanced distribution networks.","As the computational complexity of RDOEs is much higher than that of DOEs, which disregard uncertainties in network parameters and DER capacity utilisation, existing approaches to computing RDOEs have relied on linearised unbalanced three-phase optimal power flow (UTOPF) models to numerate the network feasible region approximately.","The use of linearised models, however, risks producing RDOEs that undermine network integrity due to inherent errors in the approximation.","This letter presents a practical sensitivity-filtering technique to simplify RDOE numerical computation based on non-convex UTOPF formulations.","The accuracy and efficiency of the proposed approach are demonstrated on RDOE allocation with various fairness metrics by testing on representative Australian distribution networks."],"url":"http://arxiv.org/abs/2404.03355v1","category":"math.OC"}
{"created":"2024-04-04 10:37:21","title":"Inverse scattering transform for the coupled Lakshmanan-Porsezian-Daniel equation with nonzero boundary conditions","abstract":"The challenge of solving the initial value problem for the coupled Lakshmanan Porsezian Daniel equation, while considering nonzero boundary conditions at infinity, is addressed through the development of a suitable inverse scattering transform. Analytical properties of the Jost eigenfunctions are examined, along with the analysis of scattering coefficient characteristics. This analysis leads to the derivation of additional auxiliary eigenfunctions necessary for the comprehensive investigation of the fundamental eigenfunctions. Two symmetry conditions are discussed to study the eigenfunctions and scattering coefficients. These symmetry results are utilized to rigorously define the discrete spectrum and ascertain the corresponding symmetries of scattering datas. The inverse scattering problem is formulated by the Riemann-Hilbert problem. Then we can derive the exact solutions by coupled Lakshmanan Porsezian Daniel equation, the novel soliton solutions are derived and examined in detail.","sentences":["The challenge of solving the initial value problem for the coupled Lakshmanan Porsezian Daniel equation, while considering nonzero boundary conditions at infinity, is addressed through the development of a suitable inverse scattering transform.","Analytical properties of the Jost eigenfunctions are examined, along with the analysis of scattering coefficient characteristics.","This analysis leads to the derivation of additional auxiliary eigenfunctions necessary for the comprehensive investigation of the fundamental eigenfunctions.","Two symmetry conditions are discussed to study the eigenfunctions and scattering coefficients.","These symmetry results are utilized to rigorously define the discrete spectrum and ascertain the corresponding symmetries of scattering datas.","The inverse scattering problem is formulated by the Riemann-Hilbert problem.","Then we can derive the exact solutions by coupled Lakshmanan Porsezian Daniel equation, the novel soliton solutions are derived and examined in detail."],"url":"http://arxiv.org/abs/2404.03351v1","category":"nlin.SI"}
{"created":"2024-04-04 10:23:26","title":"Out-of-equilibrium thermodynamics of autocatalytic networks","abstract":"The purpose of this work is to clarify how the stoichiometric trait of autocatalytic networks, namely their absence of a conservation law, shapes their non-equilibrium behavior. To do so, we consider an autocatalytic network coupled with external species acting as food/waste materials, necessary to fulfill mass conservation. Then, we show that the production of autocatalytic species requires a conservative influx of these external species. From this, we derive the thermodynamic potential of an autocatalytic sub-network. The latter can be obtained from the usual semigrand free-energy of an open system by taking into account the conservative work associated to the influx of external species fueling the production of autocatalytic species. In the end, we identify the cost dedicated to the production of autocatalytic species and its efficiency. It reveals that sustaining steady production of species in an autocatalytic network is possible only if they are coupled with the environment.","sentences":["The purpose of this work is to clarify how the stoichiometric trait of autocatalytic networks, namely their absence of a conservation law, shapes their non-equilibrium behavior.","To do so, we consider an autocatalytic network coupled with external species acting as food/waste materials, necessary to fulfill mass conservation.","Then, we show that the production of autocatalytic species requires a conservative influx of these external species.","From this, we derive the thermodynamic potential of an autocatalytic sub-network.","The latter can be obtained from the usual semigrand free-energy of an open system by taking into account the conservative work associated to the influx of external species fueling the production of autocatalytic species.","In the end, we identify the cost dedicated to the production of autocatalytic species and its efficiency.","It reveals that sustaining steady production of species in an autocatalytic network is possible only if they are coupled with the environment."],"url":"http://arxiv.org/abs/2404.03347v1","category":"physics.chem-ph"}
{"created":"2024-04-04 10:04:44","title":"Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation","abstract":"In recent years, deep reinforcement learning (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation. However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments. This work introduces a Population-Based Reinforcement Learning (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel. The PBRL framework is applied to three state-of-the-art RL algorithms -- PPO, SAC, and DDPG -- dynamically adjusting hyperparameters based on the performance of learning agents. The experiments are performed on four challenging tasks in Isaac Gym -- Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick -- by analyzing the effect of population size and mutation mechanisms for hyperparameters. The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents. The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer. Code and videos of the learned policies are available on our project website.","sentences":["In recent years, deep reinforcement learning (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation.","However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments.","This work introduces a Population-Based Reinforcement Learning (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel.","The PBRL framework is applied to three state-of-the-art RL algorithms -- PPO, SAC, and DDPG -- dynamically adjusting hyperparameters based on the performance of learning agents.","The experiments are performed on four challenging tasks in Isaac Gym -- Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick -- by analyzing the effect of population size and mutation mechanisms for hyperparameters.","The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents.","The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer.","Code and videos of the learned policies are available on our project website."],"url":"http://arxiv.org/abs/2404.03336v1","category":"cs.RO"}
{"created":"2024-04-04 10:03:42","title":"Control and homogenization of a system of coupled parabolic equations with an oscillating coefficient","abstract":"In this article, we study the uniform null controllability problem for a system of coupled parabolic equations with an oscillating coefficient. This is done in three steps -- first, we study the spectral properties of an elliptic operator; second, we allow the system to evolve freely and obtain the required decay; third, we use a Carleman estimate to prove a suitable observability result. This uniform null controllability property is then used to homogenize the associated coupled parabolic system.","sentences":["In this article, we study the uniform null controllability problem for a system of coupled parabolic equations with an oscillating coefficient.","This is done in three steps -- first, we study the spectral properties of an elliptic operator; second, we allow the system to evolve freely and obtain the required decay; third, we use a Carleman estimate to prove a suitable observability result.","This uniform null controllability property is then used to homogenize the associated coupled parabolic system."],"url":"http://arxiv.org/abs/2404.03335v1","category":"math.AP"}
{"created":"2024-04-04 10:02:18","title":"First detection in space of the high-energy isomer of cyanomethanimine: H2CNCN","abstract":"We report the first detection in the interstellar medium of $N$-cyanomethanimine (H$_2$CNCN), the stable dimer of HCN of highest energy, and the most complex organic molecule identified in space containing the prebiotically relevant NCN backbone. We have identified a plethora of $a$-type rotational transitions with 3 $\\leq J_\\text{up} \\leq$ 11 and $K_\\text{a} \\leq$ 2 that belong to this species towards the Galactic Center G+0.693-0.027 molecular cloud, the only interstellar source showing the three cyanomethanimine isomers (including the $Z$- and $E$- isomers of $C$-cyanomethanimine, HNCHCN). We have derived a total column density for H$_2$CNCN of (2.9$\\, \\pm \\,$0.1)$\\times$10$^{12}$ cm$^{-2}$, which translates into a total molecular abundance with respect to H$_2$ of (2.1$\\, \\pm \\,$0.3)$\\times$10$^{-11}$. We have also revisited the previous detection of $E$- and $Z$-HNCHCN, and found a total $C/N$-cyanomethanimine abundance ratio of 31.8$\\, \\pm \\,$1.8 and a $Z/E$-HNCHCN ratio of 4.5$\\, \\pm \\,$0.2. While the latter can be explained on the basis of thermodynamic equilibrium, chemical kinetics are more likely responsible for the observed $C/N$-cyanomethanimine abundance ratio, where the gas-phase reaction between methanimine (CH$_2$NH) and the cyanogen radical (CN) arises as the primary formation route.","sentences":["We report the first detection in the interstellar medium of $N$-cyanomethanimine (H$_2$CNCN), the stable dimer of HCN of highest energy, and the most complex organic molecule identified in space containing the prebiotically relevant NCN backbone.","We have identified a plethora of $a$-type rotational transitions with 3 $\\leq J_\\text{up} \\leq$ 11 and $K_\\text{a} \\leq$ 2 that belong to this species towards the Galactic Center G+0.693-0.027 molecular cloud, the only interstellar source showing the three cyanomethanimine isomers (including the $Z$- and $E$- isomers of $C$-cyanomethanimine, HNCHCN).","We have derived a total column density for H$_2$CNCN of (2.9$\\, \\pm \\,$0.1)$\\times$10$^{12}$ cm$^{-2}$, which translates into a total molecular abundance with respect to H$_2$ of (2.1$\\, \\pm \\,$0.3)$\\times$10$^{-11}$. We have also revisited the previous detection of $E$- and $Z$-HNCHCN, and found a total $C/N$-cyanomethanimine abundance ratio of 31.8$\\, \\pm \\,$1.8 and a $Z/E$-HNCHCN ratio of 4.5$\\, \\pm \\,$0.2.","While the latter can be explained on the basis of thermodynamic equilibrium, chemical kinetics are more likely responsible for the observed $C/N$-cyanomethanimine abundance ratio, where the gas-phase reaction between methanimine (CH$_2$NH) and the cyanogen radical (CN) arises as the primary formation route."],"url":"http://arxiv.org/abs/2404.03334v1","category":"astro-ph.GA"}
{"created":"2024-04-04 09:57:29","title":"LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace","abstract":"Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a~provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.","sentences":["Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure.","Gradient-based methods have emerged as a common approach to large-scale bilevel problems.","However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck.","To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process.","As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient.","Moreover, we propose a~provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system.","To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization.","This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks."],"url":"http://arxiv.org/abs/2404.03331v1","category":"math.OC"}
{"created":"2024-04-04 09:35:48","title":"Exploring Lightweight Federated Learning for Distributed Load Forecasting","abstract":"Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner. This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data. We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework. The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems. With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform.","sentences":["Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner.","This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data.","We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework.","The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems.","With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform."],"url":"http://arxiv.org/abs/2404.03320v1","category":"cs.LG"}
{"created":"2024-04-04 09:35:42","title":"Early warning systems for financial markets of emerging economies","abstract":"We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point. The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets. The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest. Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail. We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023.","sentences":["We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point.","The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets.","The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest.","Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail.","We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023."],"url":"http://arxiv.org/abs/2404.03319v1","category":"econ.EM"}
{"created":"2024-04-04 09:23:51","title":"The complexity of non-stationary ideals","abstract":"We present an overview of results on the question of whether the non-stationary ideal of an uncountable regular cardinal $\\kappa$ can be defined by a $\\Pi_1$-formula using parameters of hereditary cardinality at most $\\kappa$. These results show that this question is deeply connected to several central topics of current research in set theory.","sentences":["We present an overview of results on the question of whether the non-stationary ideal of an uncountable regular cardinal $\\kappa$ can be defined by a $\\Pi_1$-formula using parameters of hereditary cardinality at most $\\kappa$. These results show that this question is deeply connected to several central topics of current research in set theory."],"url":"http://arxiv.org/abs/2404.03315v1","category":"math.LO"}
{"created":"2024-04-04 09:17:22","title":"M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews","abstract":"Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions. Motivational interviews exhibit three important characteristics. First, there are two distinct roles, namely client and therapist. Second, they are often highly emotionally charged, which can be expressed both in text and in prosody. Finally, context is of central importance to classify any given utterance. Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues. In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification. Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour. Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context. With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification. In extensive ablation studies, we quantify the improvement resulting from each contribution.","sentences":["Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions.","Motivational interviews exhibit three important characteristics.","First, there are two distinct roles, namely client and therapist.","Second, they are often highly emotionally charged, which can be expressed both in text and in prosody.","Finally, context is of central importance to classify any given utterance.","Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues.","In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification.","Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour.","Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context.","With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification.","In extensive ablation studies, we quantify the improvement resulting from each contribution."],"url":"http://arxiv.org/abs/2404.03312v1","category":"cs.CL"}
{"created":"2024-04-04 09:13:06","title":"Non-wellfounded parsimonious proofs and non-uniform complexity","abstract":"In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality ! is interpreted as a constructor for streams over finite data. We present non-wellfounded parsimonious proof systems capturing the classes $\\mathbf{FPTIME}$ and $\\mathbf{FP}/\\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination. Completeness relies on an encoding of polynomial Turing machines with advice.   As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems.","sentences":["In this paper we investigate the complexity-theoretical aspects of cyclic and non-wellfounded proofs in the context of parsimonious logic, a variant of linear logic where the exponential modality !","is interpreted as a constructor for streams over finite data.","We present non-wellfounded parsimonious proof systems capturing the classes $\\mathbf{FPTIME}$ and $\\mathbf{FP}/\\mathsf{poly}$. Soundness is established via a polynomial modulus of continuity for continuous cut-elimination.","Completeness relies on an encoding of polynomial Turing machines with advice.   ","As a byproduct of our proof methods, we establish a series of characterisation results for various finitary proof systems."],"url":"http://arxiv.org/abs/2404.03311v1","category":"cs.LO"}
{"created":"2024-04-04 09:08:04","title":"Optimistic Online Non-stochastic Control via FTRL","abstract":"This paper brings the concept of \"optimism\" to the new and promising framework of online Non-stochastic Control (NSC). Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs. The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family. This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds. These new bounds are commensurate with the oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to the order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail. By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers.","sentences":["This paper brings the concept of \"optimism\" to the new and promising framework of online Non-stochastic Control (NSC).","Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs.","The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family.","This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds.","These new bounds are commensurate with the oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to the order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail.","By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers."],"url":"http://arxiv.org/abs/2404.03309v1","category":"cs.LG"}
{"created":"2024-04-04 08:57:52","title":"Evolutionary game on any hypergraph","abstract":"Cooperation plays a fundamental role in societal and biological domains, and the population structure profoundly shapes the dynamics of evolution. Practically, individuals behave either altruistically or egoistically in multiple groups, such as relatives, friends and colleagues, and feedbacks from these groupwise interactions will contribute to one's cognition and behavior. Due to the intricacy within and between groups, exploration of evolutionary dynamics over hypergraphs is relatively limited to date. To uncover this conundrum, we develop a higher-order random walk framework for five distinct updating rules, thus establishing explicit conditions for cooperation emergence on hypergraphs, and finding the overlaps between groups tend to foster cooperative behaviors. Our systematic analysis quantifies how the order and hyperdegree govern evolutionary outcomes. We also discover that whenever following a group wisdom update protocol, choosing a high-fitness group to interact equally within its members, cooperators will significantly prevail throughout the community. These findings underscore a crucial role of higher-order interaction and interdisciplinary collaboration throughout a broad range of living systems, favoring social prosperity.","sentences":["Cooperation plays a fundamental role in societal and biological domains, and the population structure profoundly shapes the dynamics of evolution.","Practically, individuals behave either altruistically or egoistically in multiple groups, such as relatives, friends and colleagues, and feedbacks from these groupwise interactions will contribute to one's cognition and behavior.","Due to the intricacy within and between groups, exploration of evolutionary dynamics over hypergraphs is relatively limited to date.","To uncover this conundrum, we develop a higher-order random walk framework for five distinct updating rules, thus establishing explicit conditions for cooperation emergence on hypergraphs, and finding the overlaps between groups tend to foster cooperative behaviors.","Our systematic analysis quantifies how the order and hyperdegree govern evolutionary outcomes.","We also discover that whenever following a group wisdom update protocol, choosing a high-fitness group to interact equally within its members, cooperators will significantly prevail throughout the community.","These findings underscore a crucial role of higher-order interaction and interdisciplinary collaboration throughout a broad range of living systems, favoring social prosperity."],"url":"http://arxiv.org/abs/2404.03305v1","category":"nlin.AO"}
{"created":"2024-04-04 08:52:30","title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?","abstract":"By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.","sentences":["By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks.","However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages.","In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions.","We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions.","Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context.","Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents.","Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions.","Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information."],"url":"http://arxiv.org/abs/2404.03302v1","category":"cs.CL"}
{"created":"2024-04-04 08:52:25","title":"Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics","abstract":"Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.","sentences":["Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale).","Scalar implicatures arise from the consideration of alternative statements which could have been made.","They can be triggered by scalar adjectives and require listeners to reason pragmatically about them.","Some scalar adjectives are more likely to trigger scalar implicatures than others.","This phenomenon is referred to as scalar diversity.","In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity.","We find that they encode rich lexical-semantic information about scalar adjectives.","However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity.","We also compare current models of different sizes and complexities and find that larger models are not always better.","Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives."],"url":"http://arxiv.org/abs/2404.03301v1","category":"cs.CL"}
{"created":"2024-04-04 08:41:10","title":"Use Cases for High Performance Research Desktops","abstract":"High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems. These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure. By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work. Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment. In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD. This paper defines the concept of High Performance Research Desktops and summarizes use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years. Based on these use cases, possible future directions are presented.","sentences":["High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems.","These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure.","By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work.","Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment.","In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD.","This paper defines the concept of High Performance Research Desktops and summarizes use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years.","Based on these use cases, possible future directions are presented."],"url":"http://arxiv.org/abs/2404.03298v1","category":"cs.DC"}
{"created":"2024-04-04 08:38:40","title":"Gibbs measures for hardcore-SOS models on Cayley trees","abstract":"We investigate the finite-state $p$-solid-on-solid model, for $p=\\infty$, on Cayley trees of order $k\\geq 2$ and establish a system of functional equations where each solution corresponds to a (splitting) Gibbs measure of the model. Our main result is that, for three states, $k=2,3$ and increasing coupling strength, the number of translation-invariant Gibbs measures behaves as $1\\to3\\to5\\to6\\to7$. This phase diagram is qualitatively similar to the one observed for three-state $p$-SOS models with $p>0$ and, in the case of $k=2$, we demonstrate that, on the level of the functional equations, the transition $p\\to\\infty$ is continuous.","sentences":["We investigate the finite-state $p$-solid-on-solid model, for $p=\\infty$, on Cayley trees of order $k\\geq 2$ and establish a system of functional equations where each solution corresponds to a (splitting) Gibbs measure of the model.","Our main result is that, for three states, $k=2,3$ and increasing coupling strength, the number of translation-invariant Gibbs measures behaves as $1\\to3\\to5\\to6\\to7$. This phase diagram is qualitatively similar to the one observed for three-state $p$-SOS models with $p>0$ and, in the case of $k=2$, we demonstrate that, on the level of the functional equations, the transition $p\\to\\infty$ is continuous."],"url":"http://arxiv.org/abs/2404.03297v1","category":"math-ph"}
{"created":"2024-04-04 17:58:21","title":"Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation","abstract":"Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset. Code is available at https://github.com/heshuting555/DsHmp.","sentences":["Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues.","Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues.","However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception.","In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues.","In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension.","Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues.","Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales.","Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects.","These contributions yield state-of-the-art performance across five datasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&F}$ improvement on the challenging $\\textbf{MeViS}$ dataset.","Code is available at https://github.com/heshuting555/DsHmp."],"url":"http://arxiv.org/abs/2404.03645v1","category":"cs.CV"}
{"created":"2024-04-04 17:55:41","title":"Cooperation between electron-phonon coupling and electronic interaction in bilayer nickelates La$_3$Ni$_2$O$_7$","abstract":"The recent observation of high-T$_c$ superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under pressure has garnered significant interests. While researches have predominantly focused on the role of electron-electron interactions in the superconducting mechanism, the impact of electron-phonon coupling (EPC) has remained elusive. In this work, we perform first-principles calculations to study the phonon spectrum and electron-phonon coupling within La$_3$Ni$_2$O$_7$ under pressure and explore of the interplay between EPC and electronic interactions on the superconductivity by employing functional renormalization group approach. Our calculations reveal that EPC alone is insufficient to trigger superconductivity in La$_3$Ni$_2$O$_7$ under pressure. We identify unique out-of-plane and in-plane breathing phonon modes which selectively couple with the Ni $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, showcasing an orbital-selective EPC. Within the bilayer two-orbital model, it is revealed that solely electronic interactions foster $s_{\\pm}$-wave pairing characterized by notable frustration in the band space, leading to a low transition temperature. Remarkably, we find that this out-of-plane EPC can act in concert with electronic interactions to promote the onsite and interlayer pairing in the $d_{z^2}$ orbital, partially releasing the pairing frustration and thus elevating T$_c$. In contrast, the inclusion of in-plane EPC only marginally affects the superconductivity, distinct from the cuprates. Potential experimental implications in La$_3$Ni$_2$O$_7$ are also discussed.","sentences":["The recent observation of high-T$_c$ superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under pressure has garnered significant interests.","While researches have predominantly focused on the role of electron-electron interactions in the superconducting mechanism, the impact of electron-phonon coupling (EPC) has remained elusive.","In this work, we perform first-principles calculations to study the phonon spectrum and electron-phonon coupling within La$_3$Ni$_2$O$_7$ under pressure and explore of the interplay between EPC and electronic interactions on the superconductivity by employing functional renormalization group approach.","Our calculations reveal that EPC alone is insufficient to trigger superconductivity in La$_3$Ni$_2$O$_7$ under pressure.","We identify unique out-of-plane and in-plane breathing phonon modes which selectively couple with the Ni $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, showcasing an orbital-selective EPC.","Within the bilayer two-orbital model, it is revealed that solely electronic interactions foster $s_{\\pm}$-wave pairing characterized by notable frustration in the band space, leading to a low transition temperature.","Remarkably, we find that this out-of-plane EPC can act in concert with electronic interactions to promote the onsite and interlayer pairing in the $d_{z^2}$ orbital, partially releasing the pairing frustration and thus elevating T$_c$.","In contrast, the inclusion of in-plane EPC only marginally affects the superconductivity, distinct from the cuprates.","Potential experimental implications in La$_3$Ni$_2$O$_7$ are also discussed."],"url":"http://arxiv.org/abs/2404.03638v1","category":"cond-mat.supr-con"}
{"created":"2024-04-04 17:32:37","title":"Fundamental inequalities for the iterated Fourier-cosine convolution with Gaussian weight and its application","abstract":"Derived from the results in (Math. Nachr., 283(12):1758-1770, 2010), in this paper, we devoted to studying the boundedness properties for Fourier-cosine convolution weighted by Gaussian functions via Young's type theorem and Saitoh's type inequality. New norm estimations in the weighted space are obtained and application of the corresponding class of convolutions in Fredholm's second kind of integral equation is discussed. The conditions for the solvability of this equation in $L_1$ space are also found, along with the analysis of an illustrative example, which exemplifies that the present object and method solve cases that are not under the conditions of previously known techniques.","sentences":["Derived from the results in (Math. Nachr.",", 283(12):1758-1770, 2010), in this paper, we devoted to studying the boundedness properties for Fourier-cosine convolution weighted by Gaussian functions via Young's type theorem and Saitoh's type inequality.","New norm estimations in the weighted space are obtained and application of the corresponding class of convolutions in Fredholm's second kind of integral equation is discussed.","The conditions for the solvability of this equation in $L_1$ space are also found, along with the analysis of an illustrative example, which exemplifies that the present object and method solve cases that are not under the conditions of previously known techniques."],"url":"http://arxiv.org/abs/2404.03609v1","category":"math.CA"}
{"created":"2024-04-04 17:25:30","title":"Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization","abstract":"We consider the problem of accurate quantization for language models, where both the weights and activations are uniformly quantized to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware. In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques. We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams. We then propose a simple strategy which regularizes a layer's inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization. We show that regularizing both the inputs and outputs is crucial for preventing a model's \"migrating\" the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult. When combined with weight PTQ, we show that our approach can obtain a W4A4 model that performs competitively to the standard-precision W16A16 baseline.","sentences":["We consider the problem of accurate quantization for language models, where both the weights and activations are uniformly quantized to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware.","In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques.","We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams.","We then propose a simple strategy which regularizes a layer's inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization.","We show that regularizing both the inputs and outputs is crucial for preventing a model's \"migrating\" the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult.","When combined with weight PTQ, we show that our approach can obtain a W4A4 model that performs competitively to the standard-precision W16A16 baseline."],"url":"http://arxiv.org/abs/2404.03605v1","category":"cs.LG"}
{"created":"2024-04-04 16:47:02","title":"Nuclear Matter Equation of State in the Brueckner-Hartree-Fock Approach and Standard Skyrme Energy-Density Functionals","abstract":"The equation of state of asymmetric nuclear matter as well as the neutron and proton effective masses and their partial-wave and spin-isospin decomposition are analyzed within the Brueckner--Hartree--Fock approach. Theoretical uncertainties for all these quantities are estimated by using several phase-shift-equivalent nucleon-nucleon forces together with two types of three-nucleon forces, phenomenological and microscopic. It is shown that the choice of the three-nucleon force plays an important role above saturation density, leading to different density dependencies of the energy per particle. These results are compared to the standard form of the Skyrme energy-density functional and we find that it is not possible to reproduce the BHF predictions in the $(S,T)$ channels in symmetric and neutron matter above saturation density, already at the level of the two-body interaction, and even more including the three-body interaction.","sentences":["The equation of state of asymmetric nuclear matter as well as the neutron and proton effective masses and their partial-wave and spin-isospin decomposition are analyzed within the Brueckner--Hartree--Fock approach.","Theoretical uncertainties for all these quantities are estimated by using several phase-shift-equivalent nucleon-nucleon forces together with two types of three-nucleon forces, phenomenological and microscopic.","It is shown that the choice of the three-nucleon force plays an important role above saturation density, leading to different density dependencies of the energy per particle.","These results are compared to the standard form of the Skyrme energy-density functional and we find that it is not possible to reproduce the BHF predictions in the $(S,T)$ channels in symmetric and neutron matter above saturation density, already at the level of the two-body interaction, and even more including the three-body interaction."],"url":"http://arxiv.org/abs/2404.03583v1","category":"nucl-th"}
{"created":"2024-04-04 16:38:40","title":"Enhanced mobility of quantum droplets in periodic lattices","abstract":"We predict that one- and two-dimensional self-bound quantum droplets, forming in Bose-Einstein condensates in the presence of Lee-Huang-Yang (LHY) quantum corrections to the mean-field energy, may demonstrate exceptional mobility in periodic optical lattices and that they may exhibit considerable displacements across the lattice, remaining dynamically stable, even under weak initial phase kicks imparted to them. Mobility properties of quantum droplets are determined by their internal structure and strongly depend on the number of particles in them. We find that due to the peculiar effect of the LHY quantum corrections, odd (i.e., on-site centered) and even (i.e., intersite-centered) one-dimensional quantum droplets feature alternating mobility and immobility bands closely corresponding to the regions, where translational perturbation mode is unstable and stable, respectively. This picture becomes even richer in two-dimensional case, where odd-odd, even-odd or even-even quantum droplets also feature alternating mobility and immobility domains, and where, surprisingly, the droplet may be mobile in one direction, but immobile in the orthogonal direction. We link changes in mobility properties with multiple intersections of energy $E(\\mu)$ and norm $N(\\mu)$ dependencies for droplets with different internal structure.","sentences":["We predict that one-","and two-dimensional self-bound quantum droplets, forming in Bose-Einstein condensates in the presence of Lee-Huang-Yang (LHY) quantum corrections to the mean-field energy, may demonstrate exceptional mobility in periodic optical lattices and that they may exhibit considerable displacements across the lattice, remaining dynamically stable, even under weak initial phase kicks imparted to them.","Mobility properties of quantum droplets are determined by their internal structure and strongly depend on the number of particles in them.","We find that due to the peculiar effect of the LHY quantum corrections, odd (i.e., on-site centered) and even (i.e., intersite-centered) one-dimensional quantum droplets feature alternating mobility and immobility bands closely corresponding to the regions, where translational perturbation mode is unstable and stable, respectively.","This picture becomes even richer in two-dimensional case, where odd-odd, even-odd or even-even quantum droplets also feature alternating mobility and immobility domains, and where, surprisingly, the droplet may be mobile in one direction, but immobile in the orthogonal direction.","We link changes in mobility properties with multiple intersections of energy $E(\\mu)$ and norm $N(\\mu)$ dependencies for droplets with different internal structure."],"url":"http://arxiv.org/abs/2404.03573v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-04 16:29:08","title":"The Cauchy problem for the nonlinear Schr\u00f6dinger equation with a convolution potential","abstract":"This paper investigates the nonlinear Schr\\\"{o}dinger equation with a singular convolution potential. It demonstrates the local well-posedness of this equation in a modified Sobolev space linked to the energy. Additionally, we derive conditions under which the solutions are uniformly bounded in the energy space. This finding is closely linked to the existence of standing waves for this equation.","sentences":["This paper investigates the nonlinear Schr\\\"{o}dinger equation with a singular convolution potential.","It demonstrates the local well-posedness of this equation in a modified Sobolev space linked to the energy.","Additionally, we derive conditions under which the solutions are uniformly bounded in the energy space.","This finding is closely linked to the existence of standing waves for this equation."],"url":"http://arxiv.org/abs/2404.03568v1","category":"math.AP"}
{"created":"2024-04-04 16:18:37","title":"EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German","abstract":"In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English. Compared to the original EASSE framework, EASSE-multi does not focus only on English. It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages. In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE. Further, we compare text simplification results when evaluating with different language or tokenization settings of the metrics. Based on this, we formulate recommendations on how to make the evaluation of (German) TS models more transparent and better comparable. The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at https://github.com/rstodden/easse-de.","sentences":["In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English.","Compared to the original EASSE framework, EASSE-multi does not focus only on English.","It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages.","In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE.","Further, we compare text simplification results when evaluating with different language or tokenization settings of the metrics.","Based on this, we formulate recommendations on how to make the evaluation of (German) TS models more transparent and better comparable.","The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at https://github.com/rstodden/easse-de."],"url":"http://arxiv.org/abs/2404.03563v1","category":"cs.CL"}
{"created":"2024-04-04 15:24:25","title":"Hub Network Design Problem with Capacity, Congestion and Heterogeneous Economies of Scale","abstract":"We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale. We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation. We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation. The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company.","sentences":["We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale.","We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation.","We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation.","The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company."],"url":"http://arxiv.org/abs/2404.03521v1","category":"math.OC"}
{"created":"2024-04-04 15:24:00","title":"Formal deformations of modular forms and multiple L-values","abstract":"We relate analytically defined deformations of modular curves and modular forms from the literature to motivic periods via cohomological descriptions of deformation theory. Leveraging cohomological vanishing results, we prove the existence and essential uniqueness of deformations, which we make constructive via established Lie algebraic arguments and a notion of formal logarithmic deformations. Further, we construct a canonical and a totally holomorphic canonical universal family of deformations of modular forms of all weights, which we obtain from the canonical cocycle associated with periods on the moduli space $\\mathcal{M}_{1,1}$. Our uniqueness statement shows that non-critical multiple $\\mathrm{L}$-values, which appear in our deformations but are a priori non-geometric, are genuinely linked to deformations. Our work thus suggests a new geometric perspective on them.","sentences":["We relate analytically defined deformations of modular curves and modular forms from the literature to motivic periods via cohomological descriptions of deformation theory.","Leveraging cohomological vanishing results, we prove the existence and essential uniqueness of deformations, which we make constructive via established Lie algebraic arguments and a notion of formal logarithmic deformations.","Further, we construct a canonical and a totally holomorphic canonical universal family of deformations of modular forms of all weights, which we obtain from the canonical cocycle associated with periods on the moduli space $\\mathcal{M}_{1,1}$. Our uniqueness statement shows that non-critical multiple $\\mathrm{L}$-values, which appear in our deformations but are a priori non-geometric, are genuinely linked to deformations.","Our work thus suggests a new geometric perspective on them."],"url":"http://arxiv.org/abs/2404.03519v1","category":"math.NT"}
{"created":"2024-04-04 15:23:14","title":"SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation","abstract":"Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. Our code is available at https://github.com/MartyrPenink/SDPose.","sentences":["Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE).","Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms.","Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts.","Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models.","To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters.","Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model.","Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs.","Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods.","Our code is available at https://github.com/MartyrPenink/SDPose."],"url":"http://arxiv.org/abs/2404.03518v1","category":"cs.CV"}
{"created":"2024-04-04 15:08:59","title":"Magnetic signals from oceanic tides: new satellite observations and applications","abstract":"Tidal flow of seawater across the Earth's magnetic field induces electric currents and magnetic fields within the ocean and solid Earth. The amplitude and phase of the induced fields depends on electrical properties of both the seawater and the solid Earth, thus can be used as a proxy to study seabed properties or potentially for monitoring long-term trends in the global ocean climatology. This paper presents new global oceanic tidal magnetic field models and their uncertainties for four tidal constituents, including $M_2, N_2, O_1$ and for the first time $Q_1$. Models are obtained through a robust least-squares analysis of magnetic field observations from the Swarm and CHAMP satellites using a specially designed data selection scheme. We compare the retrieved magnetic signals with several alternative models reported in the literature. Additionally, we validate them using a series of high-resolution global 3-D electromagnetic simulations and place constraints on the conductivity of sub-oceanic mantle for all tidal constituents, revealing an excellent agreement between all tidal constituents and the oceanic upper mantle structure.","sentences":["Tidal flow of seawater across the Earth's magnetic field induces electric currents and magnetic fields within the ocean and solid Earth.","The amplitude and phase of the induced fields depends on electrical properties of both the seawater and the solid Earth, thus can be used as a proxy to study seabed properties or potentially for monitoring long-term trends in the global ocean climatology.","This paper presents new global oceanic tidal magnetic field models and their uncertainties for four tidal constituents, including $M_2, N_2, O_1$ and for the first time $Q_1$. Models are obtained through a robust least-squares analysis of magnetic field observations from the Swarm and CHAMP satellites using a specially designed data selection scheme.","We compare the retrieved magnetic signals with several alternative models reported in the literature.","Additionally, we validate them using a series of high-resolution global 3-D electromagnetic simulations and place constraints on the conductivity of sub-oceanic mantle for all tidal constituents, revealing an excellent agreement between all tidal constituents and the oceanic upper mantle structure."],"url":"http://arxiv.org/abs/2404.03504v1","category":"physics.geo-ph"}
{"created":"2024-04-04 14:50:50","title":"About Test-time training for outlier detection","abstract":"In this paper, we introduce DOUST, our method applying test-time training for outlier detection, significantly improving the detection performance. After thoroughly evaluating our algorithm on common benchmark datasets, we discuss a common problem and show that it disappears with a large enough test set. Thus, we conclude that under reasonable conditions, our algorithm can reach almost supervised performance even when no labeled outliers are given.","sentences":["In this paper, we introduce DOUST, our method applying test-time training for outlier detection, significantly improving the detection performance.","After thoroughly evaluating our algorithm on common benchmark datasets, we discuss a common problem and show that it disappears with a large enough test set.","Thus, we conclude that under reasonable conditions, our algorithm can reach almost supervised performance even when no labeled outliers are given."],"url":"http://arxiv.org/abs/2404.03495v1","category":"cs.LG"}
{"created":"2024-04-04 14:39:07","title":"Combining exchangeable p-values","abstract":"Significant recent progress has been made on deriving combination rules that can take as input a set of arbitrarily dependent p-values, and produce as output a single valid p-value. Here, we show that under the assumption of exchangeability of the p-values, many of those rules can be improved (made more powerful). While this observation by itself has practical implications (for example, under repeated tests involving data splitting), it also has implications for combining arbitrarily dependent p-values, since the latter can be made exchangeable by applying a uniformly random permutation. In particular, we derive several simple randomized combination rules for arbitrarily dependent p-values that are more powerful than their deterministic counterparts. For example, we derive randomized and exchangeable improvements of well known p-value combination rules like \"twice the median\" and \"twice the average\", as well as geometric and harmonic means. The main technical advance is to show that all these combination rules can be obtained by calibrating the p-values to e-values (using an $\\alpha$-dependent calibrator), averaging those e-values, converting to a level $\\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests. The improvements are delivered via recent randomized and exchangeable variants of Markov's inequality.","sentences":["Significant recent progress has been made on deriving combination rules that can take as input a set of arbitrarily dependent p-values, and produce as output a single valid p-value.","Here, we show that under the assumption of exchangeability of the p-values, many of those rules can be improved (made more powerful).","While this observation by itself has practical implications (for example, under repeated tests involving data splitting), it also has implications for combining arbitrarily dependent p-values, since the latter can be made exchangeable by applying a uniformly random permutation.","In particular, we derive several simple randomized combination rules for arbitrarily dependent p-values that are more powerful than their deterministic counterparts.","For example, we derive randomized and exchangeable improvements of well known p-value combination rules like \"twice the median\" and \"twice the average\", as well as geometric and harmonic means.","The main technical advance is to show that all these combination rules can be obtained by calibrating the p-values to e-values (using an $\\alpha$-dependent calibrator), averaging those e-values, converting to a level $\\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests.","The improvements are delivered via recent randomized and exchangeable variants of Markov's inequality."],"url":"http://arxiv.org/abs/2404.03484v1","category":"math.ST"}
{"created":"2024-04-04 14:08:11","title":"On the sum of fifth powers in arithmetic progression","abstract":"In this paper we study equation $$(x-dr)^5+\\cdots+x^5+\\cdots+(x+dr)^5=y^p$$ under the condition $\\gcd(x,r)=1$. We present a recipe for proving the non-existence of non-trivial integer solutions of the above equation, and as an application we obtain explicit results for the cases $d=2,3$ (the case $d=1$ was already solved). We also prove an asymptotic result for $d\\equiv 1, 7\\pmod9$. Our main tools include the modular method, employing Frey curves and their associated modular forms, as well as the symplectic argument.","sentences":["In this paper we study equation $$(x-dr)^5+\\cdots+x^5+\\cdots+(x+dr)^5=y^p$$ under the condition $\\gcd(x,r)=1$. We present a recipe for proving the non-existence of non-trivial integer solutions of the above equation, and as an application we obtain explicit results for the cases $d=2,3$ (the case $d=1$ was already solved).","We also prove an asymptotic result for $d\\equiv 1, 7\\pmod9$.","Our main tools include the modular method, employing Frey curves and their associated modular forms, as well as the symplectic argument."],"url":"http://arxiv.org/abs/2404.03457v1","category":"math.NT"}
{"created":"2024-04-04 13:36:27","title":"Fractional-charge hadrons and leptons to tell the Standard Model group apart","abstract":"The gauge group of strong and electroweak interactions in Nature could be any of the four that share the same Lie algebra, $SU(3)_c\\times SU(2)_L\\times U(1)_Y/Z_p\\equiv G_p$ with $Z_p=\\left\\{Z_6,Z_3,Z_2,Z_1\\right\\}$. Each of these cases allows in its spectrum for the matter fields of the SM but also for new distinctive representations, e.g. under the assumption that $q_L$ possesses the minimum possible hypercharge in Nature, $G_p$ allows for particles with a multiple of $p\\,e/6$ for electric charge. This letter discusses how these new possibilities in the spectrum could be used to tell the SM group apart.","sentences":["The gauge group of strong and electroweak interactions in Nature could be any of the four that share the same Lie algebra, $SU(3)_c\\times SU(2)_L\\times U(1)_Y/Z_p\\equiv G_p$ with $Z_p=\\left\\{Z_6,Z_3,Z_2,Z_1\\right\\}$. Each of these cases allows in its spectrum for the matter fields of the SM but also for new distinctive representations, e.g. under the assumption that $q_L$ possesses the minimum possible hypercharge in Nature, $G_p$ allows for particles with a multiple of $p\\,e/6$ for electric charge.","This letter discusses how these new possibilities in the spectrum could be used to tell the SM group apart."],"url":"http://arxiv.org/abs/2404.03438v1","category":"hep-ph"}
{"created":"2024-04-04 13:09:26","title":"Accurate estimation of feature importance faithfulness for tree models","abstract":"In this paper, we consider a perturbation-based metric of predictive faithfulness of feature rankings (or attributions) that we call PGI squared. When applied to decision tree-based regression models, the metric can be computed accurately and efficiently for arbitrary independent feature perturbation distributions. In particular, the computation does not involve Monte Carlo sampling that has been typically used for computing similar metrics and which is inherently prone to inaccuracies. Moreover, we propose a method of ranking features by their importance for the tree model's predictions based on PGI squared. Our experiments indicate that in some respects, the method may identify the globally important features better than the state-of-the-art SHAP explainer","sentences":["In this paper, we consider a perturbation-based metric of predictive faithfulness of feature rankings (or attributions) that we call PGI squared.","When applied to decision tree-based regression models, the metric can be computed accurately and efficiently for arbitrary independent feature perturbation distributions.","In particular, the computation does not involve Monte Carlo sampling that has been typically used for computing similar metrics and which is inherently prone to inaccuracies.","Moreover, we propose a method of ranking features by their importance for the tree model's predictions based on PGI squared.","Our experiments indicate that in some respects, the method may identify the globally important features better than the state-of-the-art SHAP explainer"],"url":"http://arxiv.org/abs/2404.03426v1","category":"cs.LG"}
{"created":"2024-04-04 13:04:39","title":"Empirical Bayes for the Reluctant Frequentist","abstract":"Empirical Bayes methods offer valuable tools for a large class of compound decision problems. In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation. Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models. A more extensive introductory treatment will eventually be available in \\citet{kg24}. The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data.","sentences":["Empirical Bayes methods offer valuable tools for a large class of compound decision problems.","In this tutorial we describe some basic principles of the empirical Bayes paradigm stressing their frequentist interpretation.","Emphasis is placed on recent developments of nonparametric maximum likelihood methods for estimating mixture models.","A more extensive introductory treatment will eventually be available in \\citet{kg24}.","The methods are illustrated with an extended application to models of heterogeneous income dynamics based on PSID data."],"url":"http://arxiv.org/abs/2404.03422v1","category":"stat.ME"}
{"created":"2024-04-04 12:02:35","title":"Riemannian Covariance Fitting for Direction-of-Arrival Estimation","abstract":"Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions. Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices. We assert that this oversight leads to inherent limitations. In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF. We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics. Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions. Comparing these features to classical beamformers shows significant advantage. In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels.","sentences":["Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions.","Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices.","We assert that this oversight leads to inherent limitations.","In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF.","We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics.","Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions.","Comparing these features to classical beamformers shows significant advantage.","In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels."],"url":"http://arxiv.org/abs/2404.03401v1","category":"eess.SP"}
{"created":"2024-04-04 11:44:17","title":"Material design optimization for large-m 11B4C-based Ni/Ti supermirror neutron optics","abstract":"State-of-the-art Ni/Ti supermirror neutron optics have limited reflected intensity and a restricted neutron energy range due to the interface width. Incorporating low-neutron-absorbing 11B4C enhances reflectivity and allows for thinner layers to be deposited, with which more efficient supermirrors with higher m-values can be realized. However, incorporating 11B4C reduces the optical contrast, limiting the attainable reflectivity at low scattering vectors, making this approach infeasible. This study explores various approaches to optimize the material design of 11B4C-containing Ni/Ti supermirrors to maintain high reflectivity at low scattering vectors and achieve low interface widths at large scattering vectors. The scattering length density contrast versus interface width is investigated for multilayer periods of 30 {\\AA}, 48 {\\AA}, and 84 {\\AA}, for designs involving pure Ni/Ti multilayers, multilayers with 11B4C co-deposited in Ni and Ti layers, multilayers with 11B4C co-deposited only in Ni layers, and multilayers with 11B4C as thin interlayers between Ni and Ti layers. Our results suggest that a depth-graded hybrid material design by incorporating 11B4C inside the Ni and Ti layers, below approximately 26 {\\AA}, and introducing 1.5 {\\AA} 11B4C interlayers between the thicker Ni and Ti layers can achieve a higher reflectivity than state-of-the-art Ni/Ti multilayers over the entire scattering vector range.","sentences":["State-of-the-art Ni/Ti supermirror neutron optics have limited reflected intensity and a restricted neutron energy range due to the interface width.","Incorporating low-neutron-absorbing 11B4C enhances reflectivity and allows for thinner layers to be deposited, with which more efficient supermirrors with higher m-values can be realized.","However, incorporating 11B4C reduces the optical contrast, limiting the attainable reflectivity at low scattering vectors, making this approach infeasible.","This study explores various approaches to optimize the material design of 11B4C-containing Ni/Ti supermirrors to maintain high reflectivity at low scattering vectors and achieve low interface widths at large scattering vectors.","The scattering length density contrast versus interface width is investigated for multilayer periods of 30 {\\AA}, 48 {\\AA}, and 84 {\\AA}, for designs involving pure Ni/Ti multilayers, multilayers with 11B4C co-deposited in Ni and Ti layers, multilayers with 11B4C co-deposited only in Ni layers, and multilayers with 11B4C as thin interlayers between Ni and Ti layers.","Our results suggest that a depth-graded hybrid material design by incorporating 11B4C inside the Ni and Ti layers, below approximately 26 {\\AA}, and introducing 1.5 {\\AA} 11B4C interlayers between the thicker Ni and Ti layers can achieve a higher reflectivity than state-of-the-art Ni/Ti multilayers over the entire scattering vector range."],"url":"http://arxiv.org/abs/2404.03390v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 11:24:33","title":"3D scaling laws and projection effects in The300-NIKA2 Sunyaev-Zeldovich Large Program Twin Samples","abstract":"The abundance of galaxy clusters with mass and redshift is a well-known cosmological probe. The cluster mass is a key parameter for studies that aim to constrain cosmological parameters using galaxy clusters, making it critical to understand and properly account for the errors in its estimates. Subsequently, it becomes important to correctly calibrate scaling relations between observables like the integrated Compton parameter and the mass of the cluster.   The NIKA2 Sunyaev-Zeldovich Large program (LPSZ) enables one to map the intracluster medium profiles in the mm-wavelength band with great details (resolution of $11 \\ \\mathrm{\\&}\\ 17^{\\prime \\prime}$ at $1.2 \\ \\mathrm{\\&}\\ 2 $ mm, respectively) and hence, to estimate the cluster hydrostatic mass more precisely than previous SZ observations. However, there are certain systematic effects which can only be accounted for with the use of simulations. For this purpose, we employ THE THREE HUNDRED simulations which have been modelled with a range of physics modules to simulate galaxy clusters. The so-called twin samples are constructed by picking synthetic clusters of galaxies with properties close to the observational targets of the LPSZ. In particular, we use the Compton parameter maps and projected total mass maps of these twin samples along 29 different lines of sight. We investigate the scatter that projection induces on the total masses. Eventually, we consider the statistical values along different lines of sight to construct a kind of 3D scaling law between the integrated Compton parameter, total mass, and overdensity of the galaxy clusters to determine the overdensity that is least impacted by the projection effect.","sentences":["The abundance of galaxy clusters with mass and redshift is a well-known cosmological probe.","The cluster mass is a key parameter for studies that aim to constrain cosmological parameters using galaxy clusters, making it critical to understand and properly account for the errors in its estimates.","Subsequently, it becomes important to correctly calibrate scaling relations between observables like the integrated Compton parameter and the mass of the cluster.   ","The NIKA2 Sunyaev-Zeldovich Large program (LPSZ) enables one to map the intracluster medium profiles in the mm-wavelength band with great details (resolution of $11 \\ \\mathrm{\\&}\\ 17^{\\prime \\prime}$ at $1.2 \\ \\mathrm{\\&}\\ 2 $ mm, respectively) and hence, to estimate the cluster hydrostatic mass more precisely than previous SZ observations.","However, there are certain systematic effects which can only be accounted for with the use of simulations.","For this purpose, we employ THE THREE HUNDRED simulations which have been modelled with a range of physics modules to simulate galaxy clusters.","The so-called twin samples are constructed by picking synthetic clusters of galaxies with properties close to the observational targets of the LPSZ.","In particular, we use the Compton parameter maps and projected total mass maps of these twin samples along 29 different lines of sight.","We investigate the scatter that projection induces on the total masses.","Eventually, we consider the statistical values along different lines of sight to construct a kind of 3D scaling law between the integrated Compton parameter, total mass, and overdensity of the galaxy clusters to determine the overdensity that is least impacted by the projection effect."],"url":"http://arxiv.org/abs/2404.03376v1","category":"astro-ph.CO"}
{"created":"2024-04-04 11:16:16","title":"Elementary Analysis of Policy Gradient Methods","abstract":"Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in reinforcement learning. There have been a flurry of recent activities in studying these algorithms from the theoretical aspect. Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations. In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods. Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy. New and elementary analysis techniques have been developed to establish these results.","sentences":["Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in reinforcement learning.","There have been a flurry of recent activities in studying these algorithms from the theoretical aspect.","Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations.","In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods.","Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy.","New and elementary analysis techniques have been developed to establish these results."],"url":"http://arxiv.org/abs/2404.03372v1","category":"math.OC"}
{"created":"2024-04-04 10:50:41","title":"Agora Elevator Bodily Sensation Study -- a report","abstract":"This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience. It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity? The following report documents the study, procedure, results and findings.","sentences":["This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience.","It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity?","The following report documents the study, procedure, results and findings."],"url":"http://arxiv.org/abs/2404.03356v1","category":"cs.HC"}
{"created":"2024-04-04 10:16:01","title":"The Classification of all weak solutions to $-\u0394u={u^{-\u03b3}}$ in the half-space","abstract":"We provide the classification of all the positive solutions to $-\\Delta u=\\frac{1}{u^\\gamma}$ in the half space, under minimal assumption.","sentences":["We provide the classification of all the positive solutions to $-\\Delta u=\\frac{1}{u^\\gamma}$ in the half space, under minimal assumption."],"url":"http://arxiv.org/abs/2404.03343v1","category":"math.AP"}
{"created":"2024-04-04 10:10:38","title":"Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks","abstract":"Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks. Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked. Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed. Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID). Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle. The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance. 2) The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration. Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks.","sentences":["Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks.","Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked.","Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed.","Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID).","Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle.","The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance.","2)","The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration.","Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks."],"url":"http://arxiv.org/abs/2404.03340v1","category":"cs.CV"}
{"created":"2024-04-04 09:57:08","title":"3D Growth and Remodeling Theory Supports the Hypothesis of Staphyloma Formation from Local Scleral Weakening under Normal Intraocular Pressure","abstract":"$\\bf{Purpose}$: To assess whether Growth & Remodeling (G&R) theory could explain staphyloma formation from a local scleral weakening.   $\\bf{Methods}$: A finite element model of a healthy eye was reconstructed, including the following connective tissues: the lamina cribrosa, the peripapillary sclera, and the peripheral sclera. The scleral shell was modelled as a constrained mixture, consisting of an isotropic ground matrix and two collagen fiber families (circumferential and meridional). The homogenized constrained mixture model was employed to simulate the adaptation of the sclera to alterations in its biomechanical environment over a duration of 13.7 years. G&R processes were triggered by reducing the shear stiffness of the ground matrix in the peripapillary sclera and lamina cribrosa by 85%. Three distinct G&R scenarios were investigated: (1) low mass turnover rate in combination with transmural volumetric growth; (2) high mass turnover rate in combination with transmural volumetric growth; and (3) high mass turnover rate in combination with mass density growth.   $\\bf{Results}$: In scenario 1, we observed a significant outpouching of the posterior pole, closely resembling the shape of a Type-III staphyloma. Additionally, we found a notable change in scleral curvature and a thinning of the peripapillary sclera by 84%. In contrast, scenarios 2 and 3 exhibited less drastic deformations, with stable posterior staphylomas after approximately 7 years.   $\\bf{Conclusions}$: Our framework suggests that local scleral weakening is sufficient to trigger staphyloma formation under normal intraocular pressure. With patient-specific scleral geometries (obtainable via wide-field optical coherence tomography), our framework could aid in identifying individuals at risk of developing posterior staphylomas.","sentences":["$\\bf{Purpose}$: To assess whether Growth & Remodeling (G&R) theory could explain staphyloma formation from a local scleral weakening.   ","$\\bf{Methods}$: A finite element model of a healthy eye was reconstructed, including the following connective tissues: the lamina cribrosa, the peripapillary sclera, and the peripheral sclera.","The scleral shell was modelled as a constrained mixture, consisting of an isotropic ground matrix and two collagen fiber families (circumferential and meridional).","The homogenized constrained mixture model was employed to simulate the adaptation of the sclera to alterations in its biomechanical environment over a duration of 13.7 years.","G&R processes were triggered by reducing the shear stiffness of the ground matrix in the peripapillary sclera and lamina cribrosa by 85%.","Three distinct G&R scenarios were investigated: (1) low mass turnover rate in combination with transmural volumetric growth; (2) high mass turnover rate in combination with transmural volumetric growth; and (3) high mass turnover rate in combination with mass density growth.   ","$\\bf{Results}$: In scenario 1, we observed a significant outpouching of the posterior pole, closely resembling the shape of a Type-III staphyloma.","Additionally, we found a notable change in scleral curvature and a thinning of the peripapillary sclera by 84%.","In contrast, scenarios 2 and 3 exhibited less drastic deformations, with stable posterior staphylomas after approximately 7 years.   ","$\\bf{Conclusions}$: Our framework suggests that local scleral weakening is sufficient to trigger staphyloma formation under normal intraocular pressure.","With patient-specific scleral geometries (obtainable via wide-field optical coherence tomography), our framework could aid in identifying individuals at risk of developing posterior staphylomas."],"url":"http://arxiv.org/abs/2404.03330v1","category":"cs.CE"}
{"created":"2024-04-04 08:50:32","title":"MusE GAs FLOw and Wind (MEGAFLOW) XI. Scaling relations between outflows and host galaxy properties","abstract":"Absorption line spectroscopy using background quasars can provide strong constraints on galactic outflows. In this paper, we investigate possible scaling relations between outflow properties, namely outflow velocity \\Vout, the mass ejection rate $\\dot M_{\\rm out}$, and the mass loading factor $\\eta$ and the host galaxy properties, such as star formation rate (SFR), SFR surface density, redshift, and stellar mass using galactic outflows probed by background quasars from MEGAFLOW and other surveys. We find that $V_{\\rm out}$ ($\\eta$) is (anti-)correlated with SFR and SFR surface density. We extend the formalism of momentum-driven outflows of Heckman et al. to show that it applies not only to down the barrel studies but also to winds probed by background quasars, suggesting a possible universal wind formalism. Under this formalism, we find a clear distinction between ``strong'' and ``weak'' outflows where ``strong'' outflows seem to have tighter correlations with galaxy properties (SFR or galaxy stellar mass) than ``weak'' outflows.","sentences":["Absorption line spectroscopy using background quasars can provide strong constraints on galactic outflows.","In this paper, we investigate possible scaling relations between outflow properties, namely outflow velocity \\Vout, the mass ejection rate $\\dot M_{\\rm out}$, and the mass loading factor $\\eta$ and the host galaxy properties, such as star formation rate (SFR), SFR surface density, redshift, and stellar mass using galactic outflows probed by background quasars from MEGAFLOW and other surveys.","We find that $V_{\\rm out}$ ($\\eta$) is (anti-)correlated with SFR and SFR surface density.","We extend the formalism of momentum-driven outflows of Heckman et al. to show that it applies not only to down the barrel studies but also to winds probed by background quasars, suggesting a possible universal wind formalism.","Under this formalism, we find a clear distinction between ``strong'' and ``weak'' outflows where ``strong'' outflows seem to have tighter correlations with galaxy properties (SFR or galaxy stellar mass) than ``weak'' outflows."],"url":"http://arxiv.org/abs/2404.03300v1","category":"astro-ph.GA"}
{"created":"2024-04-04 17:58:31","title":"Locating and Editing Factual Associations in Mamba","abstract":"We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers. Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models. Third, we examine the linearity of Mamba's representations of factual relations. Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall. We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities.","sentences":["We investigate the mechanisms of factual recall in the Mamba state space model.","Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized.","To investigate this, we conduct four lines of experiments on Mamba.","First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers.","Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models.","Third, we examine the linearity of Mamba's representations of factual relations.","Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall.","We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities."],"url":"http://arxiv.org/abs/2404.03646v1","category":"cs.CL"}
{"created":"2024-04-04 17:54:12","title":"PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments","abstract":"Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes. Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments. This work offers a robust and adaptable solution for manipulating ungraspable objects.","sentences":["Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes.","Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen.","Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments.","This work offers a robust and adaptable solution for manipulating ungraspable objects."],"url":"http://arxiv.org/abs/2404.03634v1","category":"cs.RO"}
{"created":"2024-04-04 15:49:25","title":"Physics Informed Neural Networks for Free Shear Flows","abstract":"The transformative impact of machine learning, particularly Deep Learning (DL), on scientific and engineering domains is evident. In the context of computational fluid dynamics (CFD), Physics-Informed Neural Networks (PINNs) represent a significant innovation, enabling data-driven fluid simulations while incorporating physics-based laws described by partial differential equations (PDEs). While PINNs have demonstrated efficacy in various fluid flow scenarios, a noticeable gap exists in their application to simulate jet flows - an essential category in engineering. Jets, crucial for downburst outflow, ventilation, and heat transfer, lack comprehensive exploration through PINNs in existing literature. This study addresses this gap by focusing on the application of PINNs to simulate steady jet flows, specifically 2D planar turbulent jet flow scenarios. The novelty lies not only in adapting PINNs for simulating jet flows but also in overcoming challenges such as poor convergence during training, attributed to imbalances in loss terms. We propose a novel PINN architecture for Reynolds-Averaged Navier-Stokes (RANS) simulation of steady turbulent jet flows, without the need for turbulence models and simulation data. Additionally, an extended dynamic weighting strategy is introduced to enhance the balance of loss term contributions in the PINN, resulting in improved convergence and more accurate predictions.","sentences":["The transformative impact of machine learning, particularly Deep Learning (DL), on scientific and engineering domains is evident.","In the context of computational fluid dynamics (CFD), Physics-Informed Neural Networks (PINNs) represent a significant innovation, enabling data-driven fluid simulations while incorporating physics-based laws described by partial differential equations (PDEs).","While PINNs have demonstrated efficacy in various fluid flow scenarios, a noticeable gap exists in their application to simulate jet flows - an essential category in engineering.","Jets, crucial for downburst outflow, ventilation, and heat transfer, lack comprehensive exploration through PINNs in existing literature.","This study addresses this gap by focusing on the application of PINNs to simulate steady jet flows, specifically 2D planar turbulent jet flow scenarios.","The novelty lies not only in adapting PINNs for simulating jet flows but also in overcoming challenges such as poor convergence during training, attributed to imbalances in loss terms.","We propose a novel PINN architecture for Reynolds-Averaged Navier-Stokes (RANS) simulation of steady turbulent jet flows, without the need for turbulence models and simulation data.","Additionally, an extended dynamic weighting strategy is introduced to enhance the balance of loss term contributions in the PINN, resulting in improved convergence and more accurate predictions."],"url":"http://arxiv.org/abs/2404.03542v1","category":"physics.flu-dyn"}
{"created":"2024-04-04 15:29:50","title":"Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data","abstract":"This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning. We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy. Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning. Our solution combines both offline data sharing and approximate gradient coding techniques. Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data.","sentences":["This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning.","We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy.","Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning.","Our solution combines both offline data sharing and approximate gradient coding techniques.","Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data."],"url":"http://arxiv.org/abs/2404.03524v1","category":"cs.LG"}
{"created":"2024-04-04 14:25:21","title":"Lower bounds for graph reconstruction with maximal independent set queries","abstract":"We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden graph. We show that randomised adaptive algorithms need at least $\\Omega(\\Delta^2 \\log(n / \\Delta) / \\log \\Delta)$ queries to reconstruct $n$-vertex graphs of maximum degree $\\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\\Omega(\\Delta^2 \\log(n / \\Delta))$ for randomised non-adaptive algorithms. We also prove that deterministic non-adaptive algorithms require at least $\\Omega(\\Delta^3 \\log n / \\log \\Delta)$ queries.   This improves bounds of Konrad, O'Sullivan, and Traistaru, and answers one of their questions. The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds.","sentences":["We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden graph.","We show that randomised adaptive algorithms need at least $\\Omega(\\Delta^2 \\log(n / \\Delta) / \\log \\Delta)$ queries to reconstruct $n$-vertex graphs of maximum degree $\\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\\Omega(\\Delta^2 \\log(n / \\Delta))$ for randomised non-adaptive algorithms.","We also prove that deterministic non-adaptive algorithms require at least $\\Omega(\\Delta^3 \\log n / \\log \\Delta)$ queries.   ","This improves bounds of Konrad, O'Sullivan, and Traistaru, and answers one of their questions.","The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds."],"url":"http://arxiv.org/abs/2404.03472v1","category":"cs.DS"}
{"created":"2024-04-04 11:13:54","title":"Weighted Energy-Dissipation approach to semilinear gradient flows with state-dependent dissipation","abstract":"We investigate the Weighted Energy-Dissipation variational approach to semilinear gradient flows with state-dependent dissipation. A family of parameter-dependent functionals defined over entire trajectories is introduced and proved to admit global minimizers. These global minimizers correspond to solutions of elliptic-in-time regularizations of the limiting causal problem. By passing to the limit in the parameter we prove that such global minimizers converge, up to subsequences, to a solution of the gradient flow.","sentences":["We investigate the Weighted Energy-Dissipation variational approach to semilinear gradient flows with state-dependent dissipation.","A family of parameter-dependent functionals defined over entire trajectories is introduced and proved to admit global minimizers.","These global minimizers correspond to solutions of elliptic-in-time regularizations of the limiting causal problem.","By passing to the limit in the parameter we prove that such global minimizers converge, up to subsequences, to a solution of the gradient flow."],"url":"http://arxiv.org/abs/2404.03370v1","category":"math.AP"}
{"created":"2024-04-04 09:53:00","title":"DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement","abstract":"Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow. In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging. Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function. In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space. We also design a variance suppression loss for regulating the additional offset term. Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed. Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods.","sentences":["Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow.","In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging.","Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function.","In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space.","We also design a variance suppression loss for regulating the additional offset term.","Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed.","Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods."],"url":"http://arxiv.org/abs/2404.03327v1","category":"cs.CV"}
{"created":"2024-04-04 09:48:14","title":"A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off","abstract":"The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy. Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\\textit{epsilon ($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field.","sentences":["The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets.","In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve \"noisy\" representations.","To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy.","Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other.","In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\\textit{epsilon ($\\varepsilon$)}$ parameter, or privacy budget.","In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction.","As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field."],"url":"http://arxiv.org/abs/2404.03324v1","category":"cs.CL"}
{"created":"2024-04-04 08:37:27","title":"AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution","abstract":"Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors. These bit mappings are calibrated and fine-tuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000. Codes are available at https://github.com/Cheeun/AdaBM.","sentences":["Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs.","Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks.","Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy.","However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage.","To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds.","We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors.","These bit mappings are calibrated and fine-tuned using only a small number of calibration images.","We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000.","Codes are available at https://github.com/Cheeun/AdaBM."],"url":"http://arxiv.org/abs/2404.03296v1","category":"cs.CV"}
{"created":"2024-04-04 17:57:53","title":"Exploring scalar contributions with $K^+ \\to \u03c0^+ \\ell^+ \\ell^-$","abstract":"The rare kaon decay $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ is an interesting process that offers insights into both Standard Model physics and potential New Physics contributions. While primarily driven by vector interactions in the Standard Model, it also provides a window to explore non-standard contributions. In this letter, we analyse the potential of $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ decays to test the limits on scalar contributions. A simple yet effective analysis using differential decay width and the Forward-Backward Asymmetry is proposed to achieve the state-of-the-art limits on the extent of these scalar contributions. The bounds are obtained for the first time since the E865 experiment through a reinterpretation of the NA48/2 and NA62 experimental results.","sentences":["The rare kaon decay $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ is an interesting process that offers insights into both Standard Model physics and potential New Physics contributions.","While primarily driven by vector interactions in the Standard Model, it also provides a window to explore non-standard contributions.","In this letter, we analyse the potential of $K^+ \\to \\pi^+ \\ell^+ \\ell^-$ decays to test the limits on scalar contributions.","A simple yet effective analysis using differential decay width and the Forward-Backward Asymmetry is proposed to achieve the state-of-the-art limits on the extent of these scalar contributions.","The bounds are obtained for the first time since the E865 experiment through a reinterpretation of the NA48/2 and NA62 experimental results."],"url":"http://arxiv.org/abs/2404.03643v1","category":"hep-ph"}
{"created":"2024-04-04 17:55:51","title":"Representation theory of the reflection equation algebra I: A quantization of Sylvester's law of inertia","abstract":"We prove a version of Sylvester's law of inertia for the Reflection Equation Algebra (=REA). We will only be concerned with the REA constructed from the $R$-matrix associated to the standard $q$-deformation of $GL(N,\\mathbb{C})$. For $q$ positive, this particular REA comes equipped with a natural $*$-structure, by which it can be viewed as a $q$-deformation of the $*$-algebra of polynomial functions on the space of self-adjoint $N$-by-$N$-matrices. We will show that this REA satisfies a type $I$-condition, so that its irreducible representations can in principle be classified. Moreover, we will show that, up to the adjoint action of quantum $GL(N,\\mathbb{C})$, any irreducible representation of the REA is determined by its \\emph{extended signature}, which is a classical signature vector extended by a parameter in $\\mathbb{R}/\\mathbb{Z}$. It is this latter result that we see as a quantized version of Sylvester's law of inertia.","sentences":["We prove a version of Sylvester's law of inertia for the Reflection Equation Algebra (=REA).","We will only be concerned with the REA constructed from the $R$-matrix associated to the standard $q$-deformation of $GL(N,\\mathbb{C})$. For $q$ positive, this particular REA comes equipped with a natural $*$-structure, by which it can be viewed as a $q$-deformation of the $*$-algebra of polynomial functions on the space of self-adjoint $N$-by-$N$-matrices.","We will show that this REA satisfies a type $I$-condition, so that its irreducible representations can in principle be classified.","Moreover, we will show that, up to the adjoint action of quantum $GL(N,\\mathbb{C})$, any irreducible representation of the REA is determined by its \\emph{extended signature}, which is a classical signature vector extended by a parameter in $\\mathbb{R}/\\mathbb{Z}$. It is this latter result that we see as a quantized version of Sylvester's law of inertia."],"url":"http://arxiv.org/abs/2404.03640v1","category":"math.RT"}
{"created":"2024-04-04 17:53:56","title":"Existence and asymptotic behaviour of solutions for a multi-dimensional fractional thin-film equation","abstract":"In this paper, we discuss existence and finite speed of propagation for the solutions to an initial-boundary value problem for a family of fractional thin-film equations in a bounded domain in $\\mathbb{R}^d$. The nonlocal operator we consider is the spectral fractional Laplacian with Neumann boundary conditions. In the case of a \"strong slippage\" regime with \"complete wetting\" interfacial conditions, we prove local entropy estimates that entail finite speed of propagation of the support and a lower bound for the waiting time phenomenon.","sentences":["In this paper, we discuss existence and finite speed of propagation for the solutions to an initial-boundary value problem for a family of fractional thin-film equations in a bounded domain in $\\mathbb{R}^d$. The nonlocal operator we consider is the spectral fractional Laplacian with Neumann boundary conditions.","In the case of a \"strong slippage\" regime with \"complete wetting\" interfacial conditions, we prove local entropy estimates that entail finite speed of propagation of the support and a lower bound for the waiting time phenomenon."],"url":"http://arxiv.org/abs/2404.03633v1","category":"math.AP"}
{"created":"2024-04-04 16:18:26","title":"Multi-mode masers of thermally polarized nuclear spins in solution NMR","abstract":"We present experimental single and multimode sustained 1H NMR masers in solution on thermally polarized spins at room temperature and 9.4 T achieved through the electronic control of radiation feedback (radiation damping). Our observations illustrate the breakdown of the usual three-dimensional Maxwell-Bloch equations for radiation feedback and a simple toy model of few coupled classical moments is used to interpret these experiments.","sentences":["We present experimental single and multimode sustained 1H NMR masers in solution on thermally polarized spins at room temperature and 9.4 T achieved through the electronic control of radiation feedback (radiation damping).","Our observations illustrate the breakdown of the usual three-dimensional Maxwell-Bloch equations for radiation feedback and a simple toy model of few coupled classical moments is used to interpret these experiments."],"url":"http://arxiv.org/abs/2404.03562v1","category":"physics.chem-ph"}
{"created":"2024-04-04 15:31:21","title":"BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering","abstract":"Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text.","sentences":["Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner.","Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language.","Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text.","We utilize multilingual LLMs to understand various languages and correlate entities and relations universally.","By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG.","To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters.","Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG.","Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text."],"url":"http://arxiv.org/abs/2404.03528v1","category":"cs.CL"}
{"created":"2024-04-04 13:56:38","title":"Schr\u00f6dinger equation in dimension two with competing logarithmic self-interaction","abstract":"In this paper we study the equation \\[ -\\Delta u +(\\log |\\cdot|*|u|^2)u=(\\log|\\cdot|*|u|^q)|u|^{q-2}u, \\qquad \\hbox{ in }\\mathbb{R}^2, \\] where $8/3 < q < 4$. By means of variational arguments, we find infinitely many radially symmetric classical solutions. The main difficulties rely on the competition between the two nonlocal terms and on the presence of logarithmic kernels, which have not a prescribed sign. In addition, in order to find finite energy solutions, a suitable functional setting analysis is required.","sentences":["In this paper we study the equation \\[ -\\Delta u +(\\log |\\cdot|*|u|^2)u=(\\log|\\cdot|*|u|^q)|u|^{q-2}u, \\qquad \\hbox{ in }\\mathbb{R}^2, \\] where $8/3 < q < 4$. By means of variational arguments, we find infinitely many radially symmetric classical solutions.","The main difficulties rely on the competition between the two nonlocal terms and on the presence of logarithmic kernels, which have not a prescribed sign.","In addition, in order to find finite energy solutions, a suitable functional setting analysis is required."],"url":"http://arxiv.org/abs/2404.03452v1","category":"math.AP"}
{"created":"2024-04-04 13:06:19","title":"Information entropy for central $^{197}$Au+$^{197}$Au collisions in the UrQMD model","abstract":"Multiplicity information entropy in central $^{197}$Au + $^{197}$Au collisions at impact parameters of 0$-$3 fm are calculated at various center of mass energies ($\\sqrt{s_{\\rm NN}}$) of 5.0, 7.7, 11.5, 14.5, 19.6, 27.0, 32.0, 35.0, 39.0, and 54.4 GeV using the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD). The simulations in UrQMD model are compared with hydro modes with three different equations of state (EoS) and also with a default mode without hydrodynamics. The study reveals that the information entropies of baryons, net-baryons and net-protons with different equations of state in the hydro modes exhibit first decreases and then slowly increases with the increase of $\\sqrt{s_{\\rm NN}}$, while those of hadrons and anti-hadrons, antibaryons show a monotonous increase with $\\sqrt{s_{\\rm NN}}$. An enhancement is found around $\\sqrt{s_{\\rm NN}}$ $\\sim$ 30 GeV potentially corresponding to the critical endpoint with chiral hadron gas EoS and Bag model EoS.","sentences":["Multiplicity information entropy in central $^{197}$Au + $^{197}$Au collisions at impact parameters of 0$-$3 fm are calculated at various center of mass energies ($\\sqrt{s_{\\rm NN}}$) of 5.0, 7.7, 11.5, 14.5, 19.6, 27.0, 32.0, 35.0, 39.0, and 54.4 GeV using the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD).","The simulations in UrQMD model are compared with hydro modes with three different equations of state (EoS) and also with a default mode without hydrodynamics.","The study reveals that the information entropies of baryons, net-baryons and net-protons with different equations of state in the hydro modes exhibit first decreases and then slowly increases with the increase of $\\sqrt{s_{\\rm NN}}$, while those of hadrons and anti-hadrons, antibaryons show a monotonous increase with $\\sqrt{s_{\\rm NN}}$. An enhancement is found around $\\sqrt{s_{\\rm NN}}$ $\\sim$ 30 GeV potentially corresponding to the critical endpoint with chiral hadron gas EoS and Bag model EoS."],"url":"http://arxiv.org/abs/2404.03424v1","category":"nucl-th"}
{"created":"2024-04-04 11:04:21","title":"Exploring Universe acceleration through observational constraints via Hubble parameter reconstruction","abstract":"In this article, we introduce an innovative parametric representation of the Hubble parameter, providing a model-independent means to explore the dynamics of an accelerating cosmos. The model's parameters are rigorously constrained through a Markov Chain Monte Carlo (MCMC) approach, leveraging a comprehensive dataset consisting of 31 data points from cosmic chronometers (CC), 1701 updated observations of Pantheon supernovae type Ia (SNeIa), and 6 data points from baryonic acoustic oscillations (BAO). Our analysis delves into the behavior of various cosmological parameters within the model, including the transition from a decelerating phase to an accelerating one, as well as the density parameters and the equation of state (EoS) parameter. The outcomes of our investigation reveal that the equation of state parameter aligns with characteristics reminiscent of the phantom model, supporting the prevailing understanding of our universe's current state of acceleration. This research contributes valuable insights into the ongoing cosmic expansion and underscores the utility of our novel parametric approach.","sentences":["In this article, we introduce an innovative parametric representation of the Hubble parameter, providing a model-independent means to explore the dynamics of an accelerating cosmos.","The model's parameters are rigorously constrained through a Markov Chain Monte Carlo (MCMC) approach, leveraging a comprehensive dataset consisting of 31 data points from cosmic chronometers (CC), 1701 updated observations of Pantheon supernovae type Ia (SNeIa), and 6 data points from baryonic acoustic oscillations (BAO).","Our analysis delves into the behavior of various cosmological parameters within the model, including the transition from a decelerating phase to an accelerating one, as well as the density parameters and the equation of state (EoS) parameter.","The outcomes of our investigation reveal that the equation of state parameter aligns with characteristics reminiscent of the phantom model, supporting the prevailing understanding of our universe's current state of acceleration.","This research contributes valuable insights into the ongoing cosmic expansion and underscores the utility of our novel parametric approach."],"url":"http://arxiv.org/abs/2404.03362v1","category":"astro-ph.CO"}
{"created":"2024-04-04 10:57:42","title":"Geometrisation of Fermions in Flat Spacetimes","abstract":"Requiring physical consistency in a classical flat spacetime geometrisation of fermions is shown to suggest the introduction of torsion. A resulting simple model for that torsion produces a localised quantum-like particle as a solution of a spinor identity that closely resembles the Dirac equation of quantum electrodynamics. All relevant integrals involving solutions of the spinor identity converge and require that the spin, which is no longer intrinsic but arises from circulating currents, be 1/2 whilst the magnetic moment takes the quantum value of 1 magneton, characteristic of an isolated Dirac particle. The underlying torsion associates the otherwise strictly localised particle with an extended spacetime structure that may be relevant to wider quantum phenomena.","sentences":["Requiring physical consistency in a classical flat spacetime geometrisation of fermions is shown to suggest the introduction of torsion.","A resulting simple model for that torsion produces a localised quantum-like particle as a solution of a spinor identity that closely resembles the Dirac equation of quantum electrodynamics.","All relevant integrals involving solutions of the spinor identity converge and require that the spin, which is no longer intrinsic but arises from circulating currents, be 1/2 whilst the magnetic moment takes the quantum value of 1 magneton, characteristic of an isolated Dirac particle.","The underlying torsion associates the otherwise strictly localised particle with an extended spacetime structure that may be relevant to wider quantum phenomena."],"url":"http://arxiv.org/abs/2404.03360v1","category":"hep-th"}
{"created":"2024-04-04 10:30:28","title":"VF-NeRF: Viewshed Fields for Rigid NeRF Registration","abstract":"3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese.","sentences":["3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes.","This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF).","In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given.","Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras.","We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese."],"url":"http://arxiv.org/abs/2404.03349v1","category":"cs.CV"}
{"created":"2024-04-04 10:12:34","title":"Defect of irreducible plane curves with simple singularities","abstract":"In this note we focus on the defect of singular plane curve that was recently introduced by Dimca. Roughly speaking, the defect of a reduced plane curve measures the discrepancy from the property of being a free curve. We find some lower-bound on the defect for certain classes of irreducible plane curves admitting nodes, ordinary cusps and ordinary triple points. The main result of the note tells us that reduced simply singular plane curves with sufficiently high Arnold exponents are never free.","sentences":["In this note we focus on the defect of singular plane curve that was recently introduced by Dimca.","Roughly speaking, the defect of a reduced plane curve measures the discrepancy from the property of being a free curve.","We find some lower-bound on the defect for certain classes of irreducible plane curves admitting nodes, ordinary cusps and ordinary triple points.","The main result of the note tells us that reduced simply singular plane curves with sufficiently high Arnold exponents are never free."],"url":"http://arxiv.org/abs/2404.03341v1","category":"math.AG"}
{"created":"2024-04-04 10:08:52","title":"Approximation of Some Nonlinear Fractional Order BVPs by Weighted Residual Methods","abstract":"To extract the approximate solutions in the case of nonlinear fractional order differential equations with the homogeneous and nonhomogeneous boundary conditions, the weighted residual method is embedded here. We exploit three methods such as Galerkin, Least Square, and Collocation for the efficient numerical solution of nonlinear two-point boundary value problems. Some nonlinear cases are examined for observing the maximum absolute errors by the considered methods, demonstrating the accuracy and reliability of the present technique using the modified Legendre and modified Bernoulli polynomials as weight functions. The mathematical formulations and computational algorithms are more straightforward and uncomplicated to understand. Absolute errors and the graphical representation reflect that our method is more accurate and reliable.","sentences":["To extract the approximate solutions in the case of nonlinear fractional order differential equations with the homogeneous and nonhomogeneous boundary conditions, the weighted residual method is embedded here.","We exploit three methods such as Galerkin, Least Square, and Collocation for the efficient numerical solution of nonlinear two-point boundary value problems.","Some nonlinear cases are examined for observing the maximum absolute errors by the considered methods, demonstrating the accuracy and reliability of the present technique using the modified Legendre and modified Bernoulli polynomials as weight functions.","The mathematical formulations and computational algorithms are more straightforward and uncomplicated to understand.","Absolute errors and the graphical representation reflect that our method is more accurate and reliable."],"url":"http://arxiv.org/abs/2404.03338v1","category":"math.NA"}
{"created":"2024-04-04 09:34:41","title":"Cartan Flat Non-degenerate CR Lie Groups","abstract":"In this paper we determine all the simply connected non-degenerate CR Lie groups, which are flat with respect to the Cartan connection: in terms of associated Lie algebras, we assert that the only Cartan flat non-degenerate CR Lie algebras are $\\mathfrak{su}(2)$, $\\mathfrak{sl}(2,\\mathbb{R})$, $\\mathfrak{aff}(\\mathbb{R}) \\oplus \\mathbb{R}$, and $\\mathfrak{h}_{2m+1}$ with its modifications, where $\\mathfrak{aff}(\\mathbb{R})$ is the affine Lie algebra of dimension 2 and $\\mathfrak{h}_{2m+1}$ is the Heisenberg Lie algebra of dimension $2m+1$. Furthermore, we determine all the (flat and non-flat) non-degenerate CR structures on each of these Lie groups.","sentences":["In this paper we determine all the simply connected non-degenerate CR Lie groups, which are flat with respect to the Cartan connection: in terms of associated Lie algebras, we assert that the only Cartan flat non-degenerate CR Lie algebras are $\\mathfrak{su}(2)$, $\\mathfrak{sl}(2,\\mathbb{R})$, $\\mathfrak{aff}(\\mathbb{R})","\\oplus \\mathbb{R}$, and $\\mathfrak{h}_{2m+1}$ with its modifications, where $\\mathfrak{aff}(\\mathbb{R})$ is the affine Lie algebra of dimension 2 and $\\mathfrak{h}_{2m+1}$ is the Heisenberg Lie algebra of dimension $2m+1$. Furthermore, we determine all the (flat and non-flat) non-degenerate CR structures on each of these Lie groups."],"url":"http://arxiv.org/abs/2404.03318v1","category":"math.DG"}
{"created":"2024-04-04 17:34:35","title":"Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior","abstract":"Much of the research in online moderation focuses on punitive actions. However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms. We extend this research by studying the \"creator heart\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given. We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users. We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time. We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users. We discuss avenues for extending our study to understanding positive signals from moderators on other platforms.","sentences":["Much of the research in online moderation focuses on punitive actions.","However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms.","We extend this research by studying the \"creator heart\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given.","We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users.","We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time.","We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users.","We discuss avenues for extending our study to understanding positive signals from moderators on other platforms."],"url":"http://arxiv.org/abs/2404.03612v1","category":"cs.HC"}
{"created":"2024-04-04 16:50:10","title":"PAC-learning of free-fermionic states is NP-hard","abstract":"Free-fermionic states, also known as matchgates or Gaussian states, are a fundamental class of quantum states due to their efficient classical simulability and their crucial role across various domains of Physics. With the advent of quantum devices, experiments now yield data from quantum states, including estimates of expectation values. We establish that deciding whether a given dataset, formed by a few Majorana correlation functions estimates, can be consistent with a free-fermionic state is an NP-complete problem. Our result also extends to datasets formed by estimates of Pauli expectation values. This is in stark contrast to the case of stabilizer states, where the analogous problem can be efficiently solved. Moreover, our results directly imply that free-fermionic states are computationally hard to properly PAC-learn, where PAC-learning of quantum states is a learning framework introduced by Aaronson. Remarkably, this is the first class of classically simulable quantum states shown to have this property.","sentences":["Free-fermionic states, also known as matchgates or Gaussian states, are a fundamental class of quantum states due to their efficient classical simulability and their crucial role across various domains of Physics.","With the advent of quantum devices, experiments now yield data from quantum states, including estimates of expectation values.","We establish that deciding whether a given dataset, formed by a few Majorana correlation functions estimates, can be consistent with a free-fermionic state is an NP-complete problem.","Our result also extends to datasets formed by estimates of Pauli expectation values.","This is in stark contrast to the case of stabilizer states, where the analogous problem can be efficiently solved.","Moreover, our results directly imply that free-fermionic states are computationally hard to properly PAC-learn, where PAC-learning of quantum states is a learning framework introduced by Aaronson.","Remarkably, this is the first class of classically simulable quantum states shown to have this property."],"url":"http://arxiv.org/abs/2404.03585v1","category":"quant-ph"}
{"created":"2024-04-04 16:48:40","title":"Towards more realistic human motion prediction with attention to motion coordination","abstract":"Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.","sentences":["Joint relation modeling is a curial component in human motion prediction.","Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned.","However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously.","Thus, the final predicted motions usually appear unrealistic.","To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations.","Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned.","Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner.","Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction.","Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW."],"url":"http://arxiv.org/abs/2404.03584v1","category":"cs.CV"}
{"created":"2024-04-04 13:36:51","title":"Science, Technology, Engineering, and Mathematics Undergraduates' Knowledge and Interest in Quantum Careers: Barriers and Opportunities to Building a Diverse Quantum Workforce","abstract":"Efforts to build the workforce in support of the second quantum revolution are growing, including the creation of education programs that will prepare students for jobs in this area. We surveyed 186 undergraduate students with majors across the STEM disciplines and followed up with group interviews to understand their perspectives. The project was designed to understand what these STEM students know about quantum and quantum career opportunities and their level of interest in pursuing a career related to quantum. We found that most of the students know very little about quantum. Nevertheless, except for students in the life sciences, there was an interest in quantum careers. Across STEM majors, women were less likely to express interest in quantum careers than men, but this difference disappeared when we examined only physical and computer science majors. Of the few students who had knowledge of quantum concepts, most learned about this topic from online media, especially online videos. Some students reported learning about quantum in high school classes, where it was taught as an extension beyond the usual topics of the course. The undergraduate STEM students in our study identified multiple ways they would like to learn more about quantum, including short videos, seminars, courses, certificates, and degree programs.","sentences":["Efforts to build the workforce in support of the second quantum revolution are growing, including the creation of education programs that will prepare students for jobs in this area.","We surveyed 186 undergraduate students with majors across the STEM disciplines and followed up with group interviews to understand their perspectives.","The project was designed to understand what these STEM students know about quantum and quantum career opportunities and their level of interest in pursuing a career related to quantum.","We found that most of the students know very little about quantum.","Nevertheless, except for students in the life sciences, there was an interest in quantum careers.","Across STEM majors, women were less likely to express interest in quantum careers than men, but this difference disappeared when we examined only physical and computer science majors.","Of the few students who had knowledge of quantum concepts, most learned about this topic from online media, especially online videos.","Some students reported learning about quantum in high school classes, where it was taught as an extension beyond the usual topics of the course.","The undergraduate STEM students in our study identified multiple ways they would like to learn more about quantum, including short videos, seminars, courses, certificates, and degree programs."],"url":"http://arxiv.org/abs/2404.03439v1","category":"physics.ed-ph"}
{"created":"2024-04-04 12:38:14","title":"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?","abstract":"Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md .","sentences":["Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs.","Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input.","However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison.","Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies.","Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models.","We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs.","(2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models.","(3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods.","The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md ."],"url":"http://arxiv.org/abs/2404.03411v1","category":"cs.LG"}
{"created":"2024-04-04 11:53:37","title":"Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation","abstract":"In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.","sentences":["In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects.","On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination.","This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM.","The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels.","Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance."],"url":"http://arxiv.org/abs/2404.03394v1","category":"cs.CV"}
{"created":"2024-04-04 11:49:56","title":"Two Tricks to Improve Unsupervised Segmentation Learning","abstract":"We present two practical improvement techniques for unsupervised segmentation learning. These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods. Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs. Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme. This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other. Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques.","sentences":["We present two practical improvement techniques for unsupervised segmentation learning.","These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods.","Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs.","Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme.","This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other.","Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques."],"url":"http://arxiv.org/abs/2404.03392v1","category":"cs.CV"}
{"created":"2024-04-04 11:12:33","title":"Data harvesting vs data farming: A study of the importance of variation vs sample size in deep learning-based auto-segmentation for breast cancer patients","abstract":"The aim of this study was to investigate the difference in output, when training a model in three different scenarios: a large clinical delineated data set (with 700/78 patients for training/testing, from the Danish Breast Cancer Group (DBCG) RT Nation Study), a clinical but curated dataset (with 328/36 patients for training/testing, from the DBCG RT Nation Study) and a smaller, but dedicated data set created by delineation experts (with 123/14 patients for training/testing, consensus delineations created by delineation experts). The model performance was estimated based on the performance metrics dice similarity coefficient (DSC), Hausdorff 95th percentile (HD95) and mean surface distance (MSD). Models were tested in test sets from their own cohort, and afterwards also compared in the dedicated data test set. The difference between model output was finally estimated by measuring the mean width and cranial caudal length of the model output for the models. When testing the model output between the clinical models and the dedicated models in their own test set, the two clinical models had a poorer performance, than the dedicated models, but not all metrics showed statistically significance. When testing the models in the dedicated data, the dedicated model showed a slightly better performance, along with fewer segmentation outliers. As a way of taking advantage of the strength from both types of data set, it could be an option to use a large clinical data set as a baseline model, and then finetune with smaller sized cohorts with dedicated delineations.","sentences":["The aim of this study was to investigate the difference in output, when training a model in three different scenarios: a large clinical delineated data set (with 700/78 patients for training/testing, from the Danish Breast Cancer Group (DBCG) RT Nation Study), a clinical but curated dataset (with 328/36 patients for training/testing, from the DBCG RT Nation Study) and a smaller, but dedicated data set created by delineation experts (with 123/14 patients for training/testing, consensus delineations created by delineation experts).","The model performance was estimated based on the performance metrics dice similarity coefficient (DSC), Hausdorff 95th percentile (HD95) and mean surface distance (MSD).","Models were tested in test sets from their own cohort, and afterwards also compared in the dedicated data test set.","The difference between model output was finally estimated by measuring the mean width and cranial caudal length of the model output for the models.","When testing the model output between the clinical models and the dedicated models in their own test set, the two clinical models had a poorer performance, than the dedicated models, but not all metrics showed statistically significance.","When testing the models in the dedicated data, the dedicated model showed a slightly better performance, along with fewer segmentation outliers.","As a way of taking advantage of the strength from both types of data set, it could be an option to use a large clinical data set as a baseline model, and then finetune with smaller sized cohorts with dedicated delineations."],"url":"http://arxiv.org/abs/2404.03369v1","category":"physics.med-ph"}
{"created":"2024-04-04 15:44:52","title":"On the penalization by the perimeter in shape optimization applied to Dirichlet inverse obstacle problem","abstract":"This paper is devoted to the understanding of regularisation process in the shape optimization approach to the so-called Dirichlet inverse obstacle problem for elliptic operators. More precisely, we study two different regularisations of the very classical shape optimization approach consisting in minimizing a mismatched functional. The first one is an implicit regularisation when working in the class of inclusion having a uniform $\\varepsilon$-cone property, a natural class in shape optimization. As this regularity is not trivial to guarantee numerically, we discuss the regularisation by perimeter penalization. We show that this second regularisation provides a stability gain in the minimization process.","sentences":["This paper is devoted to the understanding of regularisation process in the shape optimization approach to the so-called Dirichlet inverse obstacle problem for elliptic operators.","More precisely, we study two different regularisations of the very classical shape optimization approach consisting in minimizing a mismatched functional.","The first one is an implicit regularisation when working in the class of inclusion having a uniform $\\varepsilon$-cone property, a natural class in shape optimization.","As this regularity is not trivial to guarantee numerically, we discuss the regularisation by perimeter penalization.","We show that this second regularisation provides a stability gain in the minimization process."],"url":"http://arxiv.org/abs/2404.03536v1","category":"math.OC"}
{"created":"2024-04-04 15:05:50","title":"A QAOA approach with fake devices: A case study for the maximum cut in ring graphs","abstract":"The quantum approximate optimization algorithm (QAOA) can require considerable processing time for developers to test and debug their codes on expensive quantum devices. One avenue to circumvent this difficulty is to use the error maps of quantum devices, where a local simulator can be automatically configured to mimic an actual device backend. In our work, we evaluated some error maps of quantum devices, known as fake devices, that are freely available in the cloud. The QAOA and the problem of maximum cut in 2-regular connected graphs, known as ring of disagrees, were used as tools for the noise analysis. The approximation ratio, the expectation energy and the probability of success for this problem have been evaluated in two scenarios. First, the quantities were studied through noisy simulations using fake devices. Second, error mitigation methods such as optimization levels and translation (connectivity mapping) of the original problem were applied. These results were then compared with the analytical solution of the ring graph. The study shows that error mitigation methods were crucial in obtaining better results for the expectation value of the energy, the approximation ratio, and the probability of success for the ring graphs.","sentences":["The quantum approximate optimization algorithm (QAOA) can require considerable processing time for developers to test and debug their codes on expensive quantum devices.","One avenue to circumvent this difficulty is to use the error maps of quantum devices, where a local simulator can be automatically configured to mimic an actual device backend.","In our work, we evaluated some error maps of quantum devices, known as fake devices, that are freely available in the cloud.","The QAOA and the problem of maximum cut in 2-regular connected graphs, known as ring of disagrees, were used as tools for the noise analysis.","The approximation ratio, the expectation energy and the probability of success for this problem have been evaluated in two scenarios.","First, the quantities were studied through noisy simulations using fake devices.","Second, error mitigation methods such as optimization levels and translation (connectivity mapping) of the original problem were applied.","These results were then compared with the analytical solution of the ring graph.","The study shows that error mitigation methods were crucial in obtaining better results for the expectation value of the energy, the approximation ratio, and the probability of success for the ring graphs."],"url":"http://arxiv.org/abs/2404.03501v1","category":"quant-ph"}
{"created":"2024-04-04 13:46:49","title":"Radiative corrections to the spin asymmetry in elastic polarized electron - nucleus collisions at high energy","abstract":"Improving the numerical precision, dispersion corrections to the beam-normal spin asymmetry which arise from the dominant nuclear excitations, are estimated up to a collision energy of 1 GeV. A nonperturbative calculation of vacuum polarization and the vertex plus self-energy correction, using optimized potentials, indicates that for small scattering angles both these quantum electrodynamical (QED) effects on the spin asymmetry decrease with energy above 200 MeV and can mostly be neglected at high energies. Examples are given for the 12C and 208Pb nuclei. While our results disagree with the measured high-energy spin asymmetry for 12C, they are able to explain the data on 208Pb near 1 GeV.","sentences":["Improving the numerical precision, dispersion corrections to the beam-normal spin asymmetry which arise from the dominant nuclear excitations, are estimated up to a collision energy of 1 GeV. A nonperturbative calculation of vacuum polarization and the vertex plus self-energy correction, using optimized potentials, indicates that for small scattering angles both these quantum electrodynamical (QED) effects on the spin asymmetry decrease with energy above 200 MeV and can mostly be neglected at high energies.","Examples are given for the 12C and 208Pb nuclei.","While our results disagree with the measured high-energy spin asymmetry for 12C, they are able to explain the data on 208Pb near 1 GeV."],"url":"http://arxiv.org/abs/2404.03445v1","category":"nucl-th"}
{"created":"2024-04-04 13:38:42","title":"Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity","abstract":"This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks. The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers. The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link. To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC. Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target. Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC. To efficiently quantize the signal samples, a Karhunen-Lo\\`eve Transform coding scheme is employed. Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization. Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC.","sentences":["This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks.","The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers.","The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link.","To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC.","Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target.","Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC.","To efficiently quantize the signal samples, a Karhunen-Lo\\`eve Transform coding scheme is employed.","Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization.","Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC."],"url":"http://arxiv.org/abs/2404.03440v1","category":"cs.IT"}
{"created":"2024-04-04 12:49:42","title":"Future Predictive Success-or-Failure Classification for Long-Horizon Robotic Tasks","abstract":"Automating long-horizon tasks with a robotic arm has been a central research topic in robotics. Optimization-based action planning is an efficient approach for creating an action plan to complete a given task. Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects. The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually. To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically. The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions. The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan. This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution. The regularization term improves future prediction and classification performance. The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments.","sentences":["Automating long-horizon tasks with a robotic arm has been a central research topic in robotics.","Optimization-based action planning is an efficient approach for creating an action plan to complete a given task.","Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects.","The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually.","To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically.","The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions.","The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan.","This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution.","The regularization term improves future prediction and classification performance.","The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments."],"url":"http://arxiv.org/abs/2404.03415v1","category":"cs.RO"}
{"created":"2024-04-04 10:45:07","title":"Towards Pareto Optimal Throughput in Small Language Model Serving","abstract":"Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.","sentences":["Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks.","Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance.","In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels.","Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator.","In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs."],"url":"http://arxiv.org/abs/2404.03353v1","category":"cs.CL"}
{"created":"2024-04-04 09:22:18","title":"Spatio-Spectral Structure Tensor Total Variation for Hyperspectral Image Denoising and Destriping","abstract":"This paper proposes a novel regularization method, named Spatio-Spectral Structure Tensor Total Variation (S3TTV), for denoising and destriping of hyperspectral (HS) images. HS images are inevitably contaminated by various types of noise, during acquisition process, due to the measurement equipment and the environment. For HS image denoising and destriping tasks, Spatio-Spectral Total Variation (SSTV), defined using second-order spatio-spectral differences, is widely known as a powerful regularization approach that models the underlying spatio-spectral properties. However, since SSTV refers only to adjacent pixels/bands, semi-local spatial structures are not preserved during denoising process. To address this problem, we newly design S3TTV, defined by the sum of the nuclear norms of matrices consisting of second-order spatio-spectral differences in small spectral blocks (we call these matrices as spatio-spectral structure tensors). The proposed regularization method simultaneously models the spatial piecewise-smoothness, the spatial similarity between adjacent bands, and the spectral correlation across all bands in small spectral blocks, leading to effective noise removal while preserving the semi-local spatial structures. Furthermore, we formulate the HS image denoising and destriping problem as a convex optimization problem involving S3TTV and develop an algorithm based on a preconditioned primal-dual splitting method to solve this problem efficiently. Finally, we demonstrate the effectiveness of S3TTV by comparing it with existing methods, including state-of-the-art ones through denoising and destriping experiments.","sentences":["This paper proposes a novel regularization method, named Spatio-Spectral Structure Tensor Total Variation (S3TTV), for denoising and destriping of hyperspectral (HS) images.","HS images are inevitably contaminated by various types of noise, during acquisition process, due to the measurement equipment and the environment.","For HS image denoising and destriping tasks, Spatio-Spectral Total Variation (SSTV), defined using second-order spatio-spectral differences, is widely known as a powerful regularization approach that models the underlying spatio-spectral properties.","However, since SSTV refers only to adjacent pixels/bands, semi-local spatial structures are not preserved during denoising process.","To address this problem, we newly design S3TTV, defined by the sum of the nuclear norms of matrices consisting of second-order spatio-spectral differences in small spectral blocks (we call these matrices as spatio-spectral structure tensors).","The proposed regularization method simultaneously models the spatial piecewise-smoothness, the spatial similarity between adjacent bands, and the spectral correlation across all bands in small spectral blocks, leading to effective noise removal while preserving the semi-local spatial structures.","Furthermore, we formulate the HS image denoising and destriping problem as a convex optimization problem involving S3TTV and develop an algorithm based on a preconditioned primal-dual splitting method to solve this problem efficiently.","Finally, we demonstrate the effectiveness of S3TTV by comparing it with existing methods, including state-of-the-art ones through denoising and destriping experiments."],"url":"http://arxiv.org/abs/2404.03313v1","category":"eess.SP"}
{"created":"2024-04-04 08:21:32","title":"Adsorption of Gases on Ti3C2Tx MXene: Implications from X-ray Photoelectron Spectroscopy","abstract":"One of the most explored MXenes is the Ti3C2Tx, where Tx is designated to inherently formed termination species. Among many applications, Ti3C2Tx is an excellent material for energy storage, energy converting, and CO2-capturing devices. However, active sites for adsorption and surface reactions on the Ti3C2Tx-surface are still open questions to explore, which have implications for preparation methods when to obtain correct and optimized surface requirements. Here we use X-ray photoelectron spectroscopy to study adsorption of common gas molecules such as H2, CO2, and H2O, which all might be present in energy storage, energy converting, and CO2-capturing devices based on 2D flakes of Ti3C2Tx. The study shows that H2O, with a strong bonding to the Ti-Ti bridge-sites, can be considered as a termination species. An H2O terminated Ti3C2Tx-surface restricts the CO2 adsorption to the Ti on-top sites and reduces the ability to store positive ions, such as Li+ and Na+. On the other hand, an H2O terminated Ti3C2Tx-surface shows the capability to split water. The study further shows that H2 has the ability to remove F at moderate temperatures. The results from this study have implications for correct selection of MXene preparations and the environment around the MXene in different implementations.","sentences":["One of the most explored MXenes is the Ti3C2Tx, where Tx is designated to inherently formed termination species.","Among many applications, Ti3C2Tx is an excellent material for energy storage, energy converting, and CO2-capturing devices.","However, active sites for adsorption and surface reactions on the Ti3C2Tx-surface are still open questions to explore, which have implications for preparation methods when to obtain correct and optimized surface requirements.","Here we use X-ray photoelectron spectroscopy to study adsorption of common gas molecules such as H2, CO2, and H2O, which all might be present in energy storage, energy converting, and CO2-capturing devices based on 2D flakes of Ti3C2Tx.","The study shows that H2O, with a strong bonding to the Ti-Ti bridge-sites, can be considered as a termination species.","An H2O terminated Ti3C2Tx-surface restricts the CO2 adsorption to the Ti on-top sites and reduces the ability to store positive ions, such as Li+ and Na+.","On the other hand, an H2O terminated Ti3C2Tx-surface shows the capability to split water.","The study further shows that H2 has the ability to remove F at moderate temperatures.","The results from this study have implications for correct selection of MXene preparations and the environment around the MXene in different implementations."],"url":"http://arxiv.org/abs/2404.03287v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 17:49:30","title":"A Canonical Quantization of Poisson Manifolds: a 2-Groupoid Scheme","abstract":"We canonically quantize a Poisson manifold to a Lie 2-groupoid, complete with a quantization map, and show that it relates geometric and deformation quantization: the perturbative expansion in $\\hbar$ of the (formal) convolution of two quantized functions yields Kontsevich's star product. Meanwhile, we can push forward this quantization map (by integrating over homotopies of paths) to obtain a quantization map in traditional geometric quantization. This gives a polarization-free, path integral definition of the quantization map, which does not have a prior definition for Poisson manifolds, and which only has a partial prescription for symplectic manifolds. We construct conventional quantum mechanics from this perspective.","sentences":["We canonically quantize a Poisson manifold to a Lie 2-groupoid, complete with a quantization map, and show that it relates geometric and deformation quantization: the perturbative expansion in $\\hbar$ of the (formal) convolution of two quantized functions yields Kontsevich's star product.","Meanwhile, we can push forward this quantization map (by integrating over homotopies of paths) to obtain a quantization map in traditional geometric quantization.","This gives a polarization-free, path integral definition of the quantization map, which does not have a prior definition for Poisson manifolds, and which only has a partial prescription for symplectic manifolds.","We construct conventional quantum mechanics from this perspective."],"url":"http://arxiv.org/abs/2404.03628v1","category":"math.SG"}
{"created":"2024-04-04 17:43:34","title":"Identifying Quasars from the DESI Bright Galaxy Survey","abstract":"The Dark Energy Spectroscopic Instrument (DESI) cosmology survey includes a Bright Galaxy Survey (BGS) which will yield spectra for over ten million bright galaxies (r<20.2 AB mag). The resulting sample will be valuable for both cosmological and astrophysical studies. However, the star/galaxy separation criterion implemented in the nominal BGS target selection algorithm excludes quasar host galaxies in addition to bona fide stars. While this excluded population is comparatively rare (~3-4 per square degrees), it may hold interesting clues regarding galaxy and quasar physics. Therefore, we present a target selection strategy that was implemented to recover these missing active galactic nuclei (AGN) from the BGS sample. The design of the selection criteria was both motivated and confirmed using spectroscopy. The resulting BGS-AGN sample is uniformly distributed over the entire DESI footprint. According to DESI survey validation data, the sample comprises 93% quasi-stellar objects (QSOs), 3% narrow-line AGN or blazars with a galaxy contamination rate of 2% and a stellar contamination rate of 2%. Peaking around redshift z=0.5, the BGS-AGN sample is intermediary between quasars from the rest of the BGS and those from the DESI QSO sample in terms of redshifts and AGN luminosities. The stacked spectrum is nearly identical to that of the DESI QSO targets, confirming that the sample is dominated by quasars. We highlight interesting small populations reaching z>2 which are either faint quasars with nearby projected companions or very bright quasars with strong absorption features including the Lyman-apha forest, metal absorbers and/or broad absorption lines.","sentences":["The Dark Energy Spectroscopic Instrument (DESI) cosmology survey includes a Bright Galaxy Survey (BGS) which will yield spectra for over ten million bright galaxies (r<20.2 AB mag).","The resulting sample will be valuable for both cosmological and astrophysical studies.","However, the star/galaxy separation criterion implemented in the nominal BGS target selection algorithm excludes quasar host galaxies in addition to bona fide stars.","While this excluded population is comparatively rare (~3-4 per square degrees), it may hold interesting clues regarding galaxy and quasar physics.","Therefore, we present a target selection strategy that was implemented to recover these missing active galactic nuclei (AGN) from the BGS sample.","The design of the selection criteria was both motivated and confirmed using spectroscopy.","The resulting BGS-AGN sample is uniformly distributed over the entire DESI footprint.","According to DESI survey validation data, the sample comprises 93% quasi-stellar objects (QSOs), 3% narrow-line AGN or blazars with a galaxy contamination rate of 2% and a stellar contamination rate of 2%.","Peaking around redshift z=0.5, the BGS-AGN sample is intermediary between quasars from the rest of the BGS and those from the DESI QSO sample in terms of redshifts and AGN luminosities.","The stacked spectrum is nearly identical to that of the DESI QSO targets, confirming that the sample is dominated by quasars.","We highlight interesting small populations reaching z>2 which are either faint quasars with nearby projected companions or very bright quasars with strong absorption features including the Lyman-apha forest, metal absorbers and/or broad absorption lines."],"url":"http://arxiv.org/abs/2404.03621v1","category":"astro-ph.GA"}
{"created":"2024-04-04 17:02:28","title":"DiffDet4SAR: Diffusion-based Aircraft Target Detection Network for SAR Images","abstract":"Aircraft target detection in SAR images is a challenging task due to the discrete scattering points and severe background clutter interference. Currently, methods with convolution-based or transformer-based paradigms cannot adequately address these issues. In this letter, we explore diffusion models for SAR image aircraft target detection for the first time and propose a novel \\underline{Diff}usion-based aircraft target \\underline{Det}ection network \\underline{for} \\underline{SAR} images (DiffDet4SAR). Specifically, the proposed DiffDet4SAR yields two main advantages for SAR aircraft target detection: 1) DiffDet4SAR maps the SAR aircraft target detection task to a denoising diffusion process of bounding boxes without heuristic anchor size selection, effectively enabling large variations in aircraft sizes to be accommodated; and 2) the dedicatedly designed Scattering Feature Enhancement (SFE) module further reduces the clutter intensity and enhances the target saliency during inference. Extensive experimental results on the SAR-AIRcraft-1.0 dataset show that the proposed DiffDet4SAR achieves 88.4\\% mAP$_{50}$, outperforming the state-of-the-art methods by 6\\%. Code is availabel at \\href{https://github.com/JoyeZLearning/DiffDet4SAR}.","sentences":["Aircraft target detection in SAR images is a challenging task due to the discrete scattering points and severe background clutter interference.","Currently, methods with convolution-based or transformer-based paradigms cannot adequately address these issues.","In this letter, we explore diffusion models for SAR image aircraft target detection for the first time and propose a novel \\underline{Diff}usion-based aircraft target \\underline{Det}ection network \\underline{for} \\underline{SAR} images (DiffDet4SAR).","Specifically, the proposed DiffDet4SAR yields two main advantages for SAR aircraft target detection: 1) DiffDet4SAR maps the SAR aircraft target detection task to a denoising diffusion process of bounding boxes without heuristic anchor size selection, effectively enabling large variations in aircraft sizes to be accommodated; and 2) the dedicatedly designed Scattering Feature Enhancement (SFE) module further reduces the clutter intensity and enhances the target saliency during inference.","Extensive experimental results on the SAR-AIRcraft-1.0 dataset show that the proposed DiffDet4SAR achieves 88.4\\% mAP$_{50}$, outperforming the state-of-the-art methods by 6\\%.","Code is availabel at \\href{https://github.com/JoyeZLearning/DiffDet4SAR}."],"url":"http://arxiv.org/abs/2404.03595v1","category":"eess.IV"}
{"created":"2024-04-04 16:55:24","title":"$\\bar b \\bar b u d$ and $\\bar b \\bar b u s$ tetraquarks from lattice QCD using symmetric correlation matrices with both local and scattering interpolating operators","abstract":"We study the $\\bar b \\bar b u d$ tetraquark with quantum numbers $I(J^P) = 0(1^+)$ as well as the $\\bar b \\bar b u s$ tetraquark with quantum numbers $J^P = 1^+$ using lattice QCD. We improve on existing work by including both local and scattering interpolating operators on both sides of the correlation functions and use symmetric correlation matrices. This allows not only a reliable determination of the energies of QCD-stable tetraquark ground states, but also of low-lying excited states, which are meson-meson scattering states. The latter is particularly important for future finite-volume scattering analyses. Here, we perform chiral and continuum extrapolations of just the ground-state energies, for which finite-volume effects are expected to be small. Our resulting tetraquark binding energies, $-100 \\pm 10\\:^{+36}_{-43}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u d$ and $-30 \\pm 3\\:^{+11}_{-31}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u s$, are consistent with other recent lattice-QCD predictions.","sentences":["We study the $\\bar b \\bar b u d$ tetraquark with quantum numbers $I(J^P)","= 0(1^+)$ as well as the $\\bar b \\bar b u s$ tetraquark with quantum numbers $J^P = 1^+$ using lattice QCD.","We improve on existing work by including both local and scattering interpolating operators on both sides of the correlation functions and use symmetric correlation matrices.","This allows not only a reliable determination of the energies of QCD-stable tetraquark ground states, but also of low-lying excited states, which are meson-meson scattering states.","The latter is particularly important for future finite-volume scattering analyses.","Here, we perform chiral and continuum extrapolations of just the ground-state energies, for which finite-volume effects are expected to be small.","Our resulting tetraquark binding energies, $-100 \\pm 10\\:^{+36}_{-43}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u d$ and $-30 \\pm 3\\:^{+11}_{-31}\\:\\:{\\rm MeV}$ for $\\bar b \\bar b u s$, are consistent with other recent lattice-QCD predictions."],"url":"http://arxiv.org/abs/2404.03588v1","category":"hep-lat"}
{"created":"2024-04-04 16:42:16","title":"The influence of substantial intragranular orientation gradients on the micromechanical response of heavily-worked material","abstract":"In this study, we examine the role of relatively large amounts of intragranular lattice misorientation - present after many thermomechanical processes - through three-dimensional (3D) micromechanical simulations of forged Al-7085 with a modified T7452 temper. We utilize near-field high-energy X-ray diffraction microscopy (HEDM) to measure 3D spatial orientation fields, and employ a novel reconstruction method that utilizes grain orientation envelopes measured using far-field HEDM to enable reconstruction of grains with intragranular orientation spreads >10{\\deg}. We construct two primary virtual polycrystalline specimens for use in crystal plasticity simulations to assess the effect of appreciable intragranular misorientation on the predicted deformation response: the first a faithful representation of the HEDM reconstruction, the second a microstructure with no intragranular misorientation (i.e., grain-averaged orientations). We find significant differences in the predicted deformation mechanism activation, distribution of stress, and distribution of plastic strain between simulations containing intragranular misorientation and those with homogenized orientations. The influence of elastic anisotropy is discussed, along with the effects of intragranular misorientation on fatigue life through the calculation and analysis of fatigue indicator parameters.","sentences":["In this study, we examine the role of relatively large amounts of intragranular lattice misorientation - present after many thermomechanical processes - through three-dimensional (3D) micromechanical simulations of forged Al-7085 with a modified T7452 temper.","We utilize near-field high-energy X-ray diffraction microscopy (HEDM) to measure 3D spatial orientation fields, and employ a novel reconstruction method that utilizes grain orientation envelopes measured using far-field HEDM to enable reconstruction of grains with intragranular orientation spreads >10{\\deg}.","We construct two primary virtual polycrystalline specimens for use in crystal plasticity simulations to assess the effect of appreciable intragranular misorientation on the predicted deformation response: the first a faithful representation of the HEDM reconstruction, the second a microstructure with no intragranular misorientation (i.e., grain-averaged orientations).","We find significant differences in the predicted deformation mechanism activation, distribution of stress, and distribution of plastic strain between simulations containing intragranular misorientation and those with homogenized orientations.","The influence of elastic anisotropy is discussed, along with the effects of intragranular misorientation on fatigue life through the calculation and analysis of fatigue indicator parameters."],"url":"http://arxiv.org/abs/2404.03579v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 16:39:21","title":"The Rise of Faint, Red AGN at $z>4$: A Sample of Little Red Dots in the JWST Extragalactic Legacy Fields","abstract":"We present a sample of 341 \"little red dots\" (LRDs) spanning the redshift range $z\\sim2-11$ using data from the CEERS, PRIMER, JADES, UNCOVER and NGDEEP surveys. These sources are likely heavily-reddened AGN that trace a previously-hidden phase of dust-obscured black hole growth in the early Universe. Unlike past use of color indices to identify LRDs, we employ continuum slope fitting using shifting bandpasses to sample the same rest-frame emission blueward and redward of the Balmer break. This approach allows us to identify LRDs over a wider redshift range and is less susceptible to contamination from galaxies with strong breaks that otherwise lack a rising red continuum. The redshift distribution of our sample increases at $z<8$ and then undergoes a rapid decline at $z\\sim4.5$, which may tie the emergence, and obscuration, of these sources to the inside-out growth that galaxies experience during this epoch. We find that LRDs are 2-3 dex more numerous than bright quasars at $z\\sim5-7$, but their number density is only 0.6-1 dex higher than X-ray and UV selected AGN at these redshifts. Within our sample, we have identified the first X-ray detected LRDs at $z=3.1$ and $z=4.66$. An X-ray spectral analysis confirms that these AGN are moderately obscured with $\\log\\,(N_{\\rm H}/{\\rm cm}^{2}$) of $23.3^{+0.4}_{-1.3}$ and $22.72^{+0.13}_{-0.16}$. Our analysis reveals that reddened AGN emission dominates their rest-optical light, while the rest-UV originates from their host galaxies. We also present NIRSpec follow-up spectroscopy of 17 LRDs that show broad emission lines consistent with AGN activity. The confirmed AGN fraction of our sample is $71\\%$ for sources with F444W$<26.5$. In addition, we find three LRDs with narrow blue-shifted Balmer absorption features in their spectra, suggesting an outflow of high-density, low ionization gas from near the central engine of these faint, red AGN.","sentences":["We present a sample of 341 \"little red dots\" (LRDs) spanning the redshift range $z\\sim2-11$ using data from the CEERS, PRIMER, JADES, UNCOVER and NGDEEP surveys.","These sources are likely heavily-reddened AGN that trace a previously-hidden phase of dust-obscured black hole growth in the early Universe.","Unlike past use of color indices to identify LRDs, we employ continuum slope fitting using shifting bandpasses to sample the same rest-frame emission blueward and redward of the Balmer break.","This approach allows us to identify LRDs over a wider redshift range and is less susceptible to contamination from galaxies with strong breaks that otherwise lack a rising red continuum.","The redshift distribution of our sample increases at $z<8$ and then undergoes a rapid decline at $z\\sim4.5$, which may tie the emergence, and obscuration, of these sources to the inside-out growth that galaxies experience during this epoch.","We find that LRDs are 2-3 dex more numerous than bright quasars at $z\\sim5-7$, but their number density is only 0.6-1 dex higher than X-ray and UV selected AGN at these redshifts.","Within our sample, we have identified the first X-ray detected LRDs at $z=3.1$ and $z=4.66$. An X-ray spectral analysis confirms that these AGN are moderately obscured with $\\log\\,(N_{\\rm H}/{\\rm cm}^{2}$) of $23.3^{+0.4}_{-1.3}$ and $22.72^{+0.13}_{-0.16}$. Our analysis reveals that reddened AGN emission dominates their rest-optical light, while the rest-UV originates from their host galaxies.","We also present NIRSpec follow-up spectroscopy of 17 LRDs that show broad emission lines consistent with AGN activity.","The confirmed AGN fraction of our sample is $71\\%$ for sources with F444W$<26.5$. In addition, we find three LRDs with narrow blue-shifted Balmer absorption features in their spectra, suggesting an outflow of high-density, low ionization gas from near the central engine of these faint, red AGN."],"url":"http://arxiv.org/abs/2404.03576v1","category":"astro-ph.GA"}
{"created":"2024-04-04 16:30:53","title":"Extra Higgs boson searches at the LHC","abstract":"Many searches for additional Higgs bosons, which are predicted by a lot of interesting models beyond the standard model, have been performed at the LHC. Some selected latest results of the searches for extra Higgs bosons at the LHC are presented. These additional Higgs bosons could be produced either directly from the parton interactions or from the decays of the observed standard model Higgs boson or a new heavier resonance. The searches used the data from proton-proton collisions delivered by the LHC at a centre-of-mass energy of $\\sqrt{s}=13~\\TeV$ and recorded with the ATLAS and CMS detectors. No direct evidence of new physics has been observed yet. Several mild excesses were observed in some final states. More data is needed to conclude on the nature of these excesses.","sentences":["Many searches for additional Higgs bosons, which are predicted by a lot of interesting models beyond the standard model, have been performed at the LHC.","Some selected latest results of the searches for extra Higgs bosons at the LHC are presented.","These additional Higgs bosons could be produced either directly from the parton interactions or from the decays of the observed standard model Higgs boson or a new heavier resonance.","The searches used the data from proton-proton collisions delivered by the LHC at a centre-of-mass energy of $\\sqrt{s}=13~\\TeV$ and recorded with the ATLAS and CMS detectors.","No direct evidence of new physics has been observed yet.","Several mild excesses were observed in some final states.","More data is needed to conclude on the nature of these excesses."],"url":"http://arxiv.org/abs/2404.03571v1","category":"hep-ex"}
{"created":"2024-04-04 15:37:50","title":"Impact of the Magnetic Horizon on the Interpretation of the Pierre Auger Observatory Spectrum and Composition Data","abstract":"The flux of ultra-high energy cosmic rays reaching Earth above the ankle energy (5 EeV) can be described as a mixture of nuclei injected by extragalactic sources with very hard spectra and a low rigidity cutoff. Extragalactic magnetic fields existing between the Earth and the closest sources can affect the observed CR spectrum by reducing the flux of low-rigidity particles reaching Earth. We perform a combined fit of the spectrum and distributions of depth of shower maximum measured with the Pierre Auger Observatory including the effect of this magnetic horizon in the propagation of UHECRs in the intergalactic space. We find that, within a specific range of the various experimental and phenomenological systematics, the magnetic horizon effect can be relevant for turbulent magnetic field strengths in the local neighbourhood of order $B_{\\rm rms}\\simeq (50-100)\\,{\\rm nG}\\,(20\\rm{Mpc}/{d_{\\rm s})( 100\\,\\rm{kpc}/L_{\\rm coh}})^{1/2}$, with $d_{\\rm s}$ the typical intersource separation and $L_{\\rm coh}$ the magnetic field coherence length. When this is the case, the inferred slope of the source spectrum becomes softer and can be closer to the expectations of diffusive shock acceleration, i.e., $\\propto E^{-2}$. An additional cosmic-ray population with higher source density and softer spectra, presumably also extragalactic and dominating the cosmic-ray flux at EeV energies, is also required to reproduce the overall spectrum and composition results for all energies down to 0.6~EeV.","sentences":["The flux of ultra-high energy cosmic rays reaching Earth above the ankle energy (5 EeV) can be described as a mixture of nuclei injected by extragalactic sources with very hard spectra and a low rigidity cutoff.","Extragalactic magnetic fields existing between the Earth and the closest sources can affect the observed CR spectrum by reducing the flux of low-rigidity particles reaching Earth.","We perform a combined fit of the spectrum and distributions of depth of shower maximum measured with the Pierre Auger Observatory including the effect of this magnetic horizon in the propagation of UHECRs in the intergalactic space.","We find that, within a specific range of the various experimental and phenomenological systematics, the magnetic horizon effect can be relevant for turbulent magnetic field strengths in the local neighbourhood of order $B_{\\rm rms}\\simeq (50-100)\\,{\\rm nG}\\,(20\\rm{Mpc}/{d_{\\rm s})( 100\\,\\rm{kpc}/L_{\\rm coh}})^{1/2}$, with $d_{\\rm s}$ the typical intersource separation and $L_{\\rm coh}$ the magnetic field coherence length.","When this is the case, the inferred slope of the source spectrum becomes softer and can be closer to the expectations of diffusive shock acceleration,","i.e., $\\propto E^{-2}$. An additional cosmic-ray population with higher source density and softer spectra, presumably also extragalactic and dominating the cosmic-ray flux at EeV energies, is also required to reproduce the overall spectrum and composition results for all energies down to 0.6~EeV."],"url":"http://arxiv.org/abs/2404.03533v1","category":"astro-ph.HE"}
{"created":"2024-04-04 15:24:03","title":"Correlation and Spectral Density Functions in Mode-Stirred Reverberation -- II. Spectral Moments, Sampling, Noise, EMI and Understirring","abstract":"In part I, spectral moments and kurtosis were established as parameters in analytic models of correlation and spectral density functions for dynamic reverberation fields. In this part II, several practical limitations affecting the accuracy of estimating these parameters from measured stir sweep data are investigated. For sampled fields, the contributions of finite differencing and aliasing are evaluated. Finite differencing results in a negative bias that depends, to leading order, quadratically on the product of the sampling time interval and the stir bandwidth. Numerical estimates of moments extracted directly from sampled stir sweeps show good agreement with values obtained by an autocovariance method. The effects of data decimation and noise-to-stir ratios of RMS amplitudes are determined and experimentally verified. In addition, the dependencies on the noise-to-stir-bandwidth ratio, EMI, and unstirred energy are characterized.","sentences":["In part I, spectral moments and kurtosis were established as parameters in analytic models of correlation and spectral density functions for dynamic reverberation fields.","In this part II, several practical limitations affecting the accuracy of estimating these parameters from measured stir sweep data are investigated.","For sampled fields, the contributions of finite differencing and aliasing are evaluated.","Finite differencing results in a negative bias that depends, to leading order, quadratically on the product of the sampling time interval and the stir bandwidth.","Numerical estimates of moments extracted directly from sampled stir sweeps show good agreement with values obtained by an autocovariance method.","The effects of data decimation and noise-to-stir ratios of RMS amplitudes are determined and experimentally verified.","In addition, the dependencies on the noise-to-stir-bandwidth ratio, EMI, and unstirred energy are characterized."],"url":"http://arxiv.org/abs/2404.03520v1","category":"physics.class-ph"}
{"created":"2024-04-04 15:22:30","title":"Inclusive $\\bar{B}\\to X_s \\ell^+\\ell^-$ at the LHC: theory predictions and new-physics reach","abstract":"We present theoretical predictions for observables in inclusive $\\bar{B}\\to X_s \\ell^+\\ell^-$ suitable for measurements at hadron colliders through a sum-over-exclusive approach. At low $q^2$ we calculate the branching ratio and three angular observables. At high $q^2$ we provide the branching ratio and the ratio of the $\\bar B \\to X_s \\ell^+\\ell^-$ rate with respect to the inclusive $\\bar B \\to X_u \\ell \\bar\\nu$ rate with the same phase-space cut. We compare our predictions to an extraction of the experimental rate through a sum-over-exclusive method using branching ratios of the exclusive $\\bar B \\to K^{(*)}\\mu^+\\mu^-$ modes measured at LHCb. Our analysis does not support a recent claim about a deficit in the inclusive branching ratio in the high-$q^2$ region. Finally, we present current model-independent bounds on new physics and emphasize the potential of complementary analyses of $\\bar{B}\\to X_s \\ell^+\\ell^-$ at Belle II and the LHC.","sentences":["We present theoretical predictions for observables in inclusive $\\bar{B}\\to X_s \\ell^+\\ell^-$ suitable for measurements at hadron colliders through a sum-over-exclusive approach.","At low $q^2$ we calculate the branching ratio and three angular observables.","At high $q^2$ we provide the branching ratio and the ratio of the $\\bar B \\to X_s \\ell^+\\ell^-$ rate with respect to the inclusive $\\bar B \\to X_u \\ell \\bar\\nu$ rate with the same phase-space cut.","We compare our predictions to an extraction of the experimental rate through a sum-over-exclusive method using branching ratios of the exclusive $\\bar B \\to K^{(*)}\\mu^+\\mu^-$ modes measured at LHCb.","Our analysis does not support a recent claim about a deficit in the inclusive branching ratio in the high-$q^2$ region.","Finally, we present current model-independent bounds on new physics and emphasize the potential of complementary analyses of $\\bar{B}\\to X_s \\ell^+\\ell^-$ at Belle II and the LHC."],"url":"http://arxiv.org/abs/2404.03517v1","category":"hep-ph"}
{"created":"2024-04-04 15:08:55","title":"Wilson Loops and Random Matrices","abstract":"Linear confinement with Casimir scaling of the string tension in confining gauge theories is a consequence of a certain property of the Polyakov loop related to random matrices. This mechanism does not depend on the details of the theories (neither the gauge group nor dimensions) and explains approximate Casimir scaling below string-breaking length. In this paper, we study 3d SU(2) pure Yang-Mills theory numerically and find the same random-matrix behavior for rectangular Wilson loops. We conjecture that this is a universal feature of strongly coupled confining gauge theories.","sentences":["Linear confinement with Casimir scaling of the string tension in confining gauge theories is a consequence of a certain property of the Polyakov loop related to random matrices.","This mechanism does not depend on the details of the theories (neither the gauge group nor dimensions) and explains approximate Casimir scaling below string-breaking length.","In this paper, we study 3d SU(2) pure Yang-Mills theory numerically and find the same random-matrix behavior for rectangular Wilson loops.","We conjecture that this is a universal feature of strongly coupled confining gauge theories."],"url":"http://arxiv.org/abs/2404.03503v1","category":"hep-th"}
{"created":"2024-04-04 14:21:35","title":"Integrated correlators at strong coupling in an orbifold of $\\mathcal{N}=4$ SYM","abstract":"We consider the $4d$ $\\mathcal{N}=2$ superconformal quiver gauge theory obtained by a $\\mathbb{Z}_2$ orbifold of $\\mathcal{N}=4$ super Yang-Mills (SYM). By exploiting supersymmetric localization, we study the integrated correlator of two Coulomb branch and two moment map operators and the integrated correlator of four moment map operators, determining exact expressions valid for any value of the 't Hooft coupling in the planar limit. Additionally, for the second correlator, we obtain an exact expression also for the next-to-planar contribution. Then, we derive the leading terms of their strong-coupling expansions and outline the differences with respect to the $\\mathcal{N}=4$ SYM theory.","sentences":["We consider the $4d$ $\\mathcal{N}=2$ superconformal quiver gauge theory obtained by a $\\mathbb{Z}_2$ orbifold of $\\mathcal{N}=4$ super Yang-Mills (SYM).","By exploiting supersymmetric localization, we study the integrated correlator of two Coulomb branch and two moment map operators and the integrated correlator of four moment map operators, determining exact expressions valid for any value of the 't Hooft coupling in the planar limit.","Additionally, for the second correlator, we obtain an exact expression also for the next-to-planar contribution.","Then, we derive the leading terms of their strong-coupling expansions and outline the differences with respect to the $\\mathcal{N}=4$ SYM theory."],"url":"http://arxiv.org/abs/2404.03466v1","category":"hep-th"}
{"created":"2024-04-04 14:20:45","title":"Superclimbing modes in transverse quantum fluids: signature statistical and dynamical features","abstract":"Superclimbing modes are hallmark degrees of freedom of transverse quantum fluids describing wide superfluid one-dimensional interfaces and/or edges with negligible Peierls barrier. We report the first direct numeric evidence of quantum shape fluctuations -- caused by superclimbing modes -- in simple lattice models, as well as at the free edge of an incomplete solid monolayer of $^4$He adsorbed on graphite. Our data unambiguously reveals the defining feature of the superclimbing modes -- canonical conjugation of the edge displacement field to the field of superfluid phase -- and its unexpected implication, i.e., that superfluid stiffness can be inferred from density snapshots.","sentences":["Superclimbing modes are hallmark degrees of freedom of transverse quantum fluids describing wide superfluid one-dimensional interfaces and/or edges with negligible Peierls barrier.","We report the first direct numeric evidence of quantum shape fluctuations -- caused by superclimbing modes -- in simple lattice models, as well as at the free edge of an incomplete solid monolayer of $^4$He adsorbed on graphite.","Our data unambiguously reveals the defining feature of the superclimbing modes -- canonical conjugation of the edge displacement field to the field of superfluid phase -- and its unexpected implication, i.e., that superfluid stiffness can be inferred from density snapshots."],"url":"http://arxiv.org/abs/2404.03465v1","category":"cond-mat.other"}
{"created":"2024-04-04 14:09:04","title":"Heating of Millisecond Pulsars by Magnetic Field Decay","abstract":"Millisecond pulsars (MSPs) are believed to be very old neutron stars (NSs) whose age may exceed significantly $10^8$ yrs. Although cooling scenarios of isolated NSs predict for that age a surface temperature $T_s\\sim 10^4$ K, observations of the nearest MSP J0437-4715 indicate $T_s$ well above that value. Besides the heating of the polar cap surface by backflowing charged particles, Joule heating in the crust can contribute to the overall heat budget of MSPs. Since the dipolar field component, derived from $P$ and $\\dot{P}$ measurements, is much too weak for remarkable heating, smaller-scale structures should be analysed whether they can supply the demanded heat. For this purpose we study the small scale field structure of radio pulsars. Magnetic field components, significantly stronger than the dipolar one, may exist especially at the surface of MSPs. We assign upper limits to the strength of single field components up to a multipolarity of $l=10$ and the corresponding deviations from axial symmetry $m \\le l$. Arguments are provided that the decay of the small-scale components with $l=3$ or $l=4$ of the crustal magnetic field may cause the relatively high surface temperature of isolated MSPs.","sentences":["Millisecond pulsars (MSPs) are believed to be very old neutron stars (NSs) whose age may exceed significantly $10^8$ yrs.","Although cooling scenarios of isolated NSs predict for that age a surface temperature $T_s\\sim 10^4$ K, observations of the nearest MSP J0437-4715 indicate $T_s$ well above that value.","Besides the heating of the polar cap surface by backflowing charged particles, Joule heating in the crust can contribute to the overall heat budget of MSPs.","Since the dipolar field component, derived from $P$ and $\\dot{P}$ measurements, is much too weak for remarkable heating, smaller-scale structures should be analysed whether they can supply the demanded heat.","For this purpose we study the small scale field structure of radio pulsars.","Magnetic field components, significantly stronger than the dipolar one, may exist especially at the surface of MSPs.","We assign upper limits to the strength of single field components up to a multipolarity of $l=10$ and the corresponding deviations from axial symmetry $m \\le l$. Arguments are provided that the decay of the small-scale components with $l=3$ or $l=4$ of the crustal magnetic field may cause the relatively high surface temperature of isolated MSPs."],"url":"http://arxiv.org/abs/2404.03458v1","category":"astro-ph.HE"}
{"created":"2024-04-04 13:49:47","title":"CO radial gradients in the bulge of M31","abstract":"We present new H- and K-band spectroscopy for the bulge of M31, taken with the LUCI spectrograph at the Large Binocular Telescope (LBT). We studied radial trends of CO absorption features (namely, CO1.58, CO1.60, CO1.64, CO1.66, CO1.68, CO2.30, CO2.32, CO2.35) in the bulge of M31, out to a galactocentric distance of 100'' (380pc). We find that most COs do not exhibit a strong radial gradient, despite the strong metallicity gradient inferred from the optical spectral range, except for CO1.64, showing a steep increase in the center. We compared the observed line strengths to predictions of different state-of-the-art stellar population models, including an updated version of EMILES models, which also uses the extended IRTF spectral library. The observed COs are close to models' predictions, but in some models they turn out to be underestimated. We find that the lack of radial gradients is due to the combination of increasing CO strength with metallicity and C abundance, and decreasing CO strength with IMF slope and O abundance. We speculate that the steep gradient of CO1.64 might be due to Na overabundance. Remarkably, we were able to fit, at the same time, optical indices and all the NIR COs except for CO1.68, leaving abundance ratios (i.e., [C/Fe], [O/Fe], and [Mg/Fe]) as free-fitting parameters, imposing age and metallicity constraints from the optical, with no significant contribution from intermediate-age populations. For the majority of the bulge, we find [Mg/Fe]~0.15dex, [O/Fe] larger than [Mg/Fe] (by ~0.1dex), and C abundance consistent with that of Mg. In the central (few arcsec) region, we still find an enhancement of O and Mg, but significantly lower [C/Fe]. We find that the COs' line strengths of the bulge are significantly lower than those of massive galaxies, possibly because of a difference in carbon abundance, as well as, to some extent, total metallicity.","sentences":["We present new H- and K-band spectroscopy for the bulge of M31, taken with the LUCI spectrograph at the Large Binocular Telescope (LBT).","We studied radial trends of CO absorption features (namely, CO1.58, CO1.60, CO1.64, CO1.66, CO1.68, CO2.30, CO2.32, CO2.35) in the bulge of M31, out to a galactocentric distance of 100'' (380pc).","We find that most COs do not exhibit a strong radial gradient, despite the strong metallicity gradient inferred from the optical spectral range, except for CO1.64, showing a steep increase in the center.","We compared the observed line strengths to predictions of different state-of-the-art stellar population models, including an updated version of EMILES models, which also uses the extended IRTF spectral library.","The observed COs are close to models' predictions, but in some models they turn out to be underestimated.","We find that the lack of radial gradients is due to the combination of increasing CO strength with metallicity and C abundance, and decreasing CO strength with IMF slope and O abundance.","We speculate that the steep gradient of CO1.64 might be due to Na overabundance.","Remarkably, we were able to fit, at the same time, optical indices and all the NIR COs except for CO1.68, leaving abundance ratios (i.e., [C/Fe], [O/Fe], and [Mg/Fe]) as free-fitting parameters, imposing age and metallicity constraints from the optical, with no significant contribution from intermediate-age populations.","For the majority of the bulge, we find [Mg/Fe]~0.15dex, [O/Fe] larger than [Mg/Fe] (by ~0.1dex), and C abundance consistent with that of Mg.","In the central (few arcsec) region, we still find an enhancement of O and Mg, but significantly lower [C/Fe].","We find that the COs' line strengths of the bulge are significantly lower than those of massive galaxies, possibly because of a difference in carbon abundance, as well as, to some extent, total metallicity."],"url":"http://arxiv.org/abs/2404.03448v1","category":"astro-ph.GA"}
{"created":"2024-04-04 12:58:23","title":"Modeling temporal dependency of longitudinal data: use of multivariate geometric skew-normal copula","abstract":"Use of copula for the purpose of modeling dependence has been receiving considerable attention in recent times. On the other hand, search for multivariate copulas with desirable dependence properties also is an important area of research. When fitting regression models to non-Gaussian longitudinal data, multivariate Gaussian copula is commonly used to account for temporal dependence of the repeated measurements. But using symmetric multivariate Gaussian copula is not preferable in every situation, since it can not capture non-exchangeable dependence or tail dependence, if present in the data. Hence to ensure reliable inference, it is important to look beyond the Gaussian dependence assumption. In this paper, we construct geometric skew-normal copula from multivariate geometric skew-normal (MGSN) distribution proposed by Kundu (2014) and Kundu (2017) in order to model temporal dependency of non-Gaussian longitudinal data. First we investigate the theoretical properties of the proposed multivariate copula, and then develop regression models for both continuous and discrete longitudinal data. The quantile function of this copula is independent of the correlation matrix of its respective multivariate distribution, which provides computational advantage in terms of likelihood inference compared to the class of copulas derived from skew-elliptical distributions by Azzalini & Valle (1996). Moreover, composite likelihood inference is possible for this multivariate copula, which facilitates to estimate parameters from ordered probit model with same dependence structure as geometric skew-normal distribution. We conduct extensive simulation studies to validate our proposed models and therefore apply them to analyze the longitudinal dependence of two real world data sets. Finally, we report our findings in terms of improvements over multivariate Gaussian copula based regression models.","sentences":["Use of copula for the purpose of modeling dependence has been receiving considerable attention in recent times.","On the other hand, search for multivariate copulas with desirable dependence properties also is an important area of research.","When fitting regression models to non-Gaussian longitudinal data, multivariate Gaussian copula is commonly used to account for temporal dependence of the repeated measurements.","But using symmetric multivariate Gaussian copula is not preferable in every situation, since it can not capture non-exchangeable dependence or tail dependence, if present in the data.","Hence to ensure reliable inference, it is important to look beyond the Gaussian dependence assumption.","In this paper, we construct geometric skew-normal copula from multivariate geometric skew-normal (MGSN) distribution proposed by Kundu (2014) and Kundu (2017) in order to model temporal dependency of non-Gaussian longitudinal data.","First we investigate the theoretical properties of the proposed multivariate copula, and then develop regression models for both continuous and discrete longitudinal data.","The quantile function of this copula is independent of the correlation matrix of its respective multivariate distribution, which provides computational advantage in terms of likelihood inference compared to the class of copulas derived from skew-elliptical distributions by Azzalini & Valle (1996).","Moreover, composite likelihood inference is possible for this multivariate copula, which facilitates to estimate parameters from ordered probit model with same dependence structure as geometric skew-normal distribution.","We conduct extensive simulation studies to validate our proposed models and therefore apply them to analyze the longitudinal dependence of two real world data sets.","Finally, we report our findings in terms of improvements over multivariate Gaussian copula based regression models."],"url":"http://arxiv.org/abs/2404.03420v1","category":"stat.ME"}
{"created":"2024-04-04 12:49:48","title":"Towards tailored magnetic anisotropy: A first-principles study of L1$_0$ FeNi ultrathin films","abstract":"In previous experiments, thin films of L1$\\mathrm{_0}$ FeNi with different surfaces, including (001), (110) and (111), were produced and studied. Each surface defines a different alignment of the crystallographic tetragonal axis with respect to the film's plane, resulting in different magnetic anisotropies. In this study, we use density functional theory calculations to examine three series of L1$\\mathrm{_0}$ FeNi films with surfaces (001), (010), and (111), and with thicknesses ranging from 0.5 to 3 nm (from 4 to 16 atomic monolayers). Our results show that films (001) have perpendicular magnetic anisotropy, while (010) favor in-plane magnetization, with a clear preference for the tetragonal axis [001]. We propose calling this type of in-plane anisotropy fixed in plane. A film with surface (111) and a thickness of four atomic monolayers has the magnetization easy axis almost perpendicular to the plane of the film. As the thickness of the (111) film increases, the direction of magnetization rotates towards a tetragonal axis [001], positioned at an angle of about 45$^{o}$ to the plane of the film. Furthermore, the magnetic moment of ultrathin films increases by a maximum of 5%, and the most significant changes in spin and orbital magnetic moments occur at a depth of about three near-surface atomic monolayers. The presented results could be useful for experimental efforts to synthesize ultrathin L1$\\mathrm{_0}$ FeNi films with different surfaces. Ultrathin L1$\\mathrm{_0}$ FeNi films with varying magnetic anisotropies may find applications in spintronic devices.","sentences":["In previous experiments, thin films of L1$\\mathrm{_0}$ FeNi with different surfaces, including (001), (110) and (111), were produced and studied.","Each surface defines a different alignment of the crystallographic tetragonal axis with respect to the film's plane, resulting in different magnetic anisotropies.","In this study, we use density functional theory calculations to examine three series of L1$\\mathrm{_0}$ FeNi films with surfaces (001), (010), and (111), and with thicknesses ranging from 0.5 to 3 nm (from 4 to 16 atomic monolayers).","Our results show that films (001) have perpendicular magnetic anisotropy, while (010) favor in-plane magnetization, with a clear preference for the tetragonal axis","[001].","We propose calling this type of in-plane anisotropy fixed in plane.","A film with surface (111) and a thickness of four atomic monolayers has the magnetization easy axis almost perpendicular to the plane of the film.","As the thickness of the (111) film increases, the direction of magnetization rotates towards a tetragonal axis [001], positioned at an angle of about 45$^{o}$ to the plane of the film.","Furthermore, the magnetic moment of ultrathin films increases by a maximum of 5%, and the most significant changes in spin and orbital magnetic moments occur at a depth of about three near-surface atomic monolayers.","The presented results could be useful for experimental efforts to synthesize ultrathin L1$\\mathrm{_0}$ FeNi films with different surfaces.","Ultrathin L1$\\mathrm{_0}$ FeNi films with varying magnetic anisotropies may find applications in spintronic devices."],"url":"http://arxiv.org/abs/2404.03416v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 12:03:56","title":"Chandra X-ray Analysis of Herbig Ae/Be Stars","abstract":"Herbig Ae/Be (HAeBe) stars are intermediate-mass pre-main sequence stars, characterized by infrared excess and emission lines. They are observed to emit X-rays, whose origin is a matter of discussion and not settled yet. X-ray emission is not expected in HAeBe stars, as they lack the sub-surface convective zone. In this study, we retrieved observations from the Chandra archive for 62 HAeBe stars, among which 44 sources (detection fraction $\\sim$71%) were detected in X-rays, with 7 being new detections. We use this sample as a test bed to conduct a comparative analysis of the X-ray properties of HAeBe stars and their low-mass counterparts, T Tauri Stars (TTSs). Further, we compare the X-ray properties of HAeBe stars and TTSs with optical and IR properties to constrain the X-ray emission mechanism in HAeBe stars. We found no correlation between X-ray emission and disk properties of HAeBe stars, confirming that X-rays are not related to accretion shocks. About 56% of HAeBe stars without any known sub-arcsec companions have lower plasma temperatures (kT $\\leq$ 2 keV). We observe flaring/variability in HAeBe stars with confirmed low-mass companions. These stars show plasma temperatures > 2 keV, similar to TTSs. Guided by this information we discuss the role of a T Tauri companion for X-ray emission seen in our sample of HAeBe stars. From the results obtained in this paper, we suggest that X-ray emission from HAeBe stars may not be related to accretion shocks or hidden TTS, but rather can be due to magnetically driven coronal emission.","sentences":["Herbig Ae/Be (HAeBe) stars are intermediate-mass pre-main sequence stars, characterized by infrared excess and emission lines.","They are observed to emit X-rays, whose origin is a matter of discussion and not settled yet.","X-ray emission is not expected in HAeBe stars, as they lack the sub-surface convective zone.","In this study, we retrieved observations from the Chandra archive for 62 HAeBe stars, among which 44 sources (detection fraction $\\sim$71%) were detected in X-rays, with 7 being new detections.","We use this sample as a test bed to conduct a comparative analysis of the X-ray properties of HAeBe stars and their low-mass counterparts, T Tauri Stars (TTSs).","Further, we compare the X-ray properties of HAeBe stars and TTSs with optical and IR properties to constrain the X-ray emission mechanism in HAeBe stars.","We found no correlation between X-ray emission and disk properties of HAeBe stars, confirming that X-rays are not related to accretion shocks.","About 56% of HAeBe stars without any known sub-arcsec companions have lower plasma temperatures (kT $\\leq$ 2 keV).","We observe flaring/variability in HAeBe stars with confirmed low-mass companions.","These stars show plasma temperatures > 2 keV, similar to TTSs.","Guided by this information we discuss the role of a T Tauri companion for X-ray emission seen in our sample of HAeBe stars.","From the results obtained in this paper, we suggest that X-ray emission from HAeBe stars may not be related to accretion shocks or hidden TTS, but rather can be due to magnetically driven coronal emission."],"url":"http://arxiv.org/abs/2404.03403v1","category":"astro-ph.SR"}
{"created":"2024-04-04 11:58:03","title":"Controllable non-Hermitian qubit-qubit Coupling in Superconducting quantum Circuit","abstract":"With a high-loss resonator supplying the non-Hermiticity, we study the Energy level degeneracy and quantum state evolution in tunable coupling superconducting quantum circuit. The qubit's effective energy level and damping rate can be continually tuned in superconducting circuit, and the positions and numbers of level degenerate points are controllable. The efficient of quantum state exchange and the asymmetry of quantum state evolution can be tuned with non-hermitian and nonreciprocal coupling between two qubits. The controllable non-Hermiticity provides new insights and methods for exploring the unconventional quantum effects in superconducting quantum circuit.","sentences":["With a high-loss resonator supplying the non-Hermiticity, we study the Energy level degeneracy and quantum state evolution in tunable coupling superconducting quantum circuit.","The qubit's effective energy level and damping rate can be continually tuned in superconducting circuit, and the positions and numbers of level degenerate points are controllable.","The efficient of quantum state exchange and the asymmetry of quantum state evolution can be tuned with non-hermitian and nonreciprocal coupling between two qubits.","The controllable non-Hermiticity provides new insights and methods for exploring the unconventional quantum effects in superconducting quantum circuit."],"url":"http://arxiv.org/abs/2404.03397v1","category":"quant-ph"}
{"created":"2024-04-04 11:23:33","title":"Search for the $B_s^0 \\rightarrow \u03bc^+\u03bc^-\u03b3$ decay","abstract":"A search for the fully reconstructed $B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma$ decay is performed at the LHCb experiment using proton-proton collisions at $\\sqrt{s}=13$\\,TeV corresponding to an integrated luminosity of $5.4\\,\\mathrm{fb^{-1}}$. No significant signal is found and upper limits on the branching fraction in intervals of the dimuon mass are set   \\begin{align}   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) < 4.2\\times10^{-8},~&m(\\mu\\mu)\\in[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2} ,\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) < 7.7\\times10^{-8},~&m(\\mu\\mu)\\in[1.70,~2.88]\\,\\mathrm{GeV/c^2},\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) < 4.2\\times10^{-8},~&m(\\mu\\mu)\\in[3.92 ,~m_{B_s^0}]\\,\\mathrm{GeV/c^2},\\nonumber \\end{align} at 95\\% confidence level. Additionally, upper limits are set on the branching fraction in the $[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2}$ dimuon mass region excluding the contribution from the intermediate $\\phi(1020)$ meson, and in the region combining all dimuon-mass intervals.","sentences":["A search for the fully reconstructed $B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma$ decay is performed at the LHCb experiment using proton-proton collisions at $\\sqrt{s}=13$\\,TeV corresponding to an integrated luminosity of $5.4\\,\\mathrm{fb^{-1}}$. No significant signal is found and upper limits on the branching fraction in intervals of the dimuon mass are set   \\begin{align}   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) <","4.2\\times10^{-8},~&m(\\mu\\mu)\\in[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2} ,\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) <","7.7\\times10^{-8},~&m(\\mu\\mu)\\in[1.70,~2.88]\\,\\mathrm{GeV/c^2},\\nonumber   {\\cal B}(B_s^0 \\rightarrow \\mu^+\\mu^-\\gamma) <","4.2\\times10^{-8},~&m(\\mu\\mu)\\in[3.92 ,~m_{B_s^0}]\\,\\mathrm{GeV/c^2},\\nonumber \\end{align} at 95\\% confidence level.","Additionally, upper limits are set on the branching fraction in the $[2m_\\mu,~1.70]\\,\\mathrm{GeV/c^2}$ dimuon mass region excluding the contribution from the intermediate $\\phi(1020)$ meson, and in the region combining all dimuon-mass intervals."],"url":"http://arxiv.org/abs/2404.03375v1","category":"hep-ex"}
{"created":"2024-04-04 11:06:50","title":"Electronic structure of noncentrosymmetric B20 compound HfSn and tuning of multifold band-crossing points","abstract":"We present a detailed theoretical study of the electronic structure of hafnium tin HfSn crystallizing in a B$20$ structure, renowned for the diversity of physical and peculiar topological properties. The chiral crystal structure of these materials protects multifold band crossings located at high symmetry points. We employ density functional methods to reveal basic features of the band structure and Fermi surface topology of HfSn, on top of which an effective tight-binding model is built. The compound exhibits a fourfold band crossing pinned at the ${\\Gamma}$ point. We investigate routes that can shift such crossings towards the Fermi level, offering a unique way to possibly tune the compound`s properties. Specifically, we show that the energy position of the fourfold crossing can be easily manipulated via external perturbations such as strain and pressure. Considering that this point carries a topological charge larger than one, such tuning is of great importance. We anticipate that the approach presented in the current study can be utilized to investigate symmetry protected crossings in a wide class of materials.","sentences":["We present a detailed theoretical study of the electronic structure of hafnium tin HfSn crystallizing in a B$20$ structure, renowned for the diversity of physical and peculiar topological properties.","The chiral crystal structure of these materials protects multifold band crossings located at high symmetry points.","We employ density functional methods to reveal basic features of the band structure and Fermi surface topology of HfSn, on top of which an effective tight-binding model is built.","The compound exhibits a fourfold band crossing pinned at the ${\\Gamma}$ point.","We investigate routes that can shift such crossings towards the Fermi level, offering a unique way to possibly tune the compound`s properties.","Specifically, we show that the energy position of the fourfold crossing can be easily manipulated via external perturbations such as strain and pressure.","Considering that this point carries a topological charge larger than one, such tuning is of great importance.","We anticipate that the approach presented in the current study can be utilized to investigate symmetry protected crossings in a wide class of materials."],"url":"http://arxiv.org/abs/2404.03365v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 10:08:54","title":"Significantly Enhanced Vacancy Diffusion in Mn-containing Alloys","abstract":"Manipulating point defects for tailored macroscopic properties remains a formidable challenge in materials science. This study demonstrates a proof-of-principle for a universal law involving element Mn, significantly enhancing vacancy diffusion through an unprecedented anomalous Friedel Oscillations phenomenon, across most metals in the periodic table. The correlation between Mn-induced point-defect dynamic changes and intrinsic macro-properties is robustly validated through the first-principles theory and well-designed experiments. The physical origin stems from Mn's exceptionally large effective intra-elemental 3d electron interactions, surpassing the Coulomb attraction induced by vacancy and disrupting the electron screening effect. Given the ubiquitous nature of vacancies and their recognition as the most crucial defects influencing nearly all physical and mechanical properties of crystalline materials, this outcome may drive advances in a broad domain.","sentences":["Manipulating point defects for tailored macroscopic properties remains a formidable challenge in materials science.","This study demonstrates a proof-of-principle for a universal law involving element Mn, significantly enhancing vacancy diffusion through an unprecedented anomalous Friedel Oscillations phenomenon, across most metals in the periodic table.","The correlation between Mn-induced point-defect dynamic changes and intrinsic macro-properties is robustly validated through the first-principles theory and well-designed experiments.","The physical origin stems from Mn's exceptionally large effective intra-elemental 3d electron interactions, surpassing the Coulomb attraction induced by vacancy and disrupting the electron screening effect.","Given the ubiquitous nature of vacancies and their recognition as the most crucial defects influencing nearly all physical and mechanical properties of crystalline materials, this outcome may drive advances in a broad domain."],"url":"http://arxiv.org/abs/2404.03339v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 10:01:40","title":"The Impact-driven Atmospheric Loss of Super-Earths around Different Spectral Type Host Stars","abstract":"The planet's mass loss is important for the planet's formation and evolution. The radius valley (RV) is believed to be triggered by evaporation-induced mass loss. As an alternative mechanism for the RV, the mass loss of post-impact planets is thoroughly investigated in this work. The impact energy is converted to the planet's internal energy, enhancing its core energy and accelerating mass loss and orbital migration. As the host star changes from K-type to F-type, the planet's mass loss and orbital migration increase. When the initial gas-to-core mass ratio (GCR) is small, the migration efficiency for planets around K-type stars will increase, which helps to suppress mass loss and retain the planet's mass and radius within a specific range. On the contrary, planets around more massive F-type stars experience more substantial mass loss, potentially leading to complete mass loss, and migrate to orbits with longer periods. Our calculation shows that planets around different spectral types of host stars give rise to an RV ranging from 1.3-2.0 $R_{\\oplus}$, consistent with the observed range of 1.3-2.6 $R_{\\oplus}$. Despite the presence of uncertain parameters, the planetesimal impact can promote the RV establishment for planets around host stars of different spectral types.","sentences":["The planet's mass loss is important for the planet's formation and evolution.","The radius valley (RV) is believed to be triggered by evaporation-induced mass loss.","As an alternative mechanism for the RV, the mass loss of post-impact planets is thoroughly investigated in this work.","The impact energy is converted to the planet's internal energy, enhancing its core energy and accelerating mass loss and orbital migration.","As the host star changes from K-type to F-type, the planet's mass loss and orbital migration increase.","When the initial gas-to-core mass ratio (GCR) is small, the migration efficiency for planets around K-type stars will increase, which helps to suppress mass loss and retain the planet's mass and radius within a specific range.","On the contrary, planets around more massive F-type stars experience more substantial mass loss, potentially leading to complete mass loss, and migrate to orbits with longer periods.","Our calculation shows that planets around different spectral types of host stars give rise to an RV ranging from 1.3-2.0 $R_{\\oplus}$, consistent with the observed range of 1.3-2.6 $R_{\\oplus}$. Despite the presence of uncertain parameters, the planetesimal impact can promote the RV establishment for planets around host stars of different spectral types."],"url":"http://arxiv.org/abs/2404.03333v1","category":"astro-ph.EP"}
{"created":"2024-04-04 08:31:53","title":"On bipartite and tripartite entanglement at present and future particle colliders","abstract":"Entanglement, rooted in the non-deterministic, non-local nature of quantum mechanics, serves as a fundamental correlation. High-energy particle colliders offer a unique platform for exploring entanglement in the relativistic regime. The recent observation of entanglement in $t\\bar{t}$ production by ATLAS has sparked significant interest in investigating entanglement phenomena at colliders. While bipartite entanglement receives extensive attention, tripartite entanglement remains relatively uncharted. We investigate tripartite entanglement in $t\\bar{t}Z$ production at the Large Hadron Collider (LHC) within the Standard Model and with a dimension-$8$ effective operator. Additionally, we explore bipartite entanglement in $t\\bar{t}$, $tW^-$, and di-boson production processes, namely $W^+W^-$, $ZZ$, and $W^+Z$, at the LHC and future $e^+e^-$ collider. We numerically compute various measures of entanglement through Monte Carlo events based on the spin density matrix, with its elements (polarization and spin correlation) obtained by analyzing the angular distribution of the final decayed leptons.","sentences":["Entanglement, rooted in the non-deterministic, non-local nature of quantum mechanics, serves as a fundamental correlation.","High-energy particle colliders offer a unique platform for exploring entanglement in the relativistic regime.","The recent observation of entanglement in $t\\bar{t}$ production by ATLAS has sparked significant interest in investigating entanglement phenomena at colliders.","While bipartite entanglement receives extensive attention, tripartite entanglement remains relatively uncharted.","We investigate tripartite entanglement in $t\\bar{t}Z$ production at the Large Hadron Collider (LHC) within the Standard Model and with a dimension-$8$ effective operator.","Additionally, we explore bipartite entanglement in $t\\bar{t}$, $tW^-$, and di-boson production processes, namely $W^+W^-$, $ZZ$, and $W^+Z$, at the LHC and future $e^+e^-$ collider.","We numerically compute various measures of entanglement through Monte Carlo events based on the spin density matrix, with its elements (polarization and spin correlation) obtained by analyzing the angular distribution of the final decayed leptons."],"url":"http://arxiv.org/abs/2404.03292v1","category":"hep-ph"}
{"created":"2024-04-04 08:28:18","title":"$\\texttt{globin}$: A spectropolarimetric inversion code for the coupled inference of atomic line parameters","abstract":"For many transitions, atomic data, such as the oscillator strength (log(gf)) and the central wavelength of the line, are poorly constrained or even unknown. We present and test a new inversion method that infers atomic line parameters and the height stratification of the atmospheric parameters from spatially resolved spectropolarimetric observations of the Sun. This method is implemented in the new inversion code $\\texttt{globin}$. The new method imposes a spatial coupling in inversion parameters common to all pixels, such as the atomic parameters of the observed spectral lines, and infers atmospheric parameters for each spatial pixel individually. The uniqueness of this method lies in its ability to retrieve reliable atomic parameters even for heavily blended spectral lines. We tested the method by applying it to a set of 18 spectral lines between 4015 \\r{A} and 4017 \\r{A}, synthesized from a 3D magnetohydrodynamic simulation containing a sunspot and the quiet Sun region around it. The results were then compared with a previously used inversion method where atomic parameters were determined for every pixel independently (pixel-by-pixel method). The new method was able to retrieve the log(gf) values of all lines to an accuracy of 0.004 dex, while the pixel-by-pixel method retrieved the same parameter to an accuracy of only 0.025 dex. The largest differences between the two methods are evident for the heavily blended lines, with the former method performing better than the latter. In addition, the new method is also able to infer reliable atmospheric parameters in all the inverted pixels by successfully disentangling the degeneracies between the atomic and atmospheric parameters. The new method is well suited for the reliable determination of both atomic and atmospheric parameters and works well on all spectral lines, including those that are weak and/or severely blended.","sentences":["For many transitions, atomic data, such as the oscillator strength (log(gf)) and the central wavelength of the line, are poorly constrained or even unknown.","We present and test a new inversion method that infers atomic line parameters and the height stratification of the atmospheric parameters from spatially resolved spectropolarimetric observations of the Sun.","This method is implemented in the new inversion code $\\texttt{globin}$. The new method imposes a spatial coupling in inversion parameters common to all pixels, such as the atomic parameters of the observed spectral lines, and infers atmospheric parameters for each spatial pixel individually.","The uniqueness of this method lies in its ability to retrieve reliable atomic parameters even for heavily blended spectral lines.","We tested the method by applying it to a set of 18 spectral lines between 4015 \\r{A} and 4017 \\r{A}, synthesized from a 3D magnetohydrodynamic simulation containing a sunspot and the quiet Sun region around it.","The results were then compared with a previously used inversion method where atomic parameters were determined for every pixel independently (pixel-by-pixel method).","The new method was able to retrieve the log(gf) values of all lines to an accuracy of 0.004 dex, while the pixel-by-pixel method retrieved the same parameter to an accuracy of only 0.025 dex.","The largest differences between the two methods are evident for the heavily blended lines, with the former method performing better than the latter.","In addition, the new method is also able to infer reliable atmospheric parameters in all the inverted pixels by successfully disentangling the degeneracies between the atomic and atmospheric parameters.","The new method is well suited for the reliable determination of both atomic and atmospheric parameters and works well on all spectral lines, including those that are weak and/or severely blended."],"url":"http://arxiv.org/abs/2404.03291v1","category":"astro-ph.SR"}
