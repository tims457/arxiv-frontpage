{"created":"2024-03-25 12:58:28","title":"On Stability Behaviors of 5D M-theory Black Objects","abstract":"Using $N = 2$ supergravity formalism, we investigate certain behaviors of five dimensional black objects from the compactification of M-theory on a Calabi-Yau three-fold. The manifold has been constructed as the intersection of two homogeneous polynomials of degrees $ (\\omega+2,1)$ and $ (2,1) $ in a product of two weighted projective spaces given by $ \\mathbb{WP}^{4}(\\omega,1,1,1,1) \\times\\mathbb{P}^{1}$. First, we determine the allowed electric charge regions of the BPS and non BPS black holes obtained by wrapping M2-branes on appropriate two cycles in such a proposed Calabi-Yau three-fold. After that, we calculate the entropy of these solutions which takes a maximal value corresponding to $\\omega=1$ defining the ordinary projective space $\\mathbb{P}^{4}$. For generic values of $\\omega$, we show that the non BPS states are unstable. Then, we conduct a similar study of five dimensional black strings. Concerning the allowed magnetic charge regions of the BPS and non BPS black stringy solutions derived from M5-branes on dual divisors, we calculate the tension taking a minimal value for $\\mathbb{P}^{4}$. By determining the recombination factor, we show that the non-BPS black string states are stable in the allowed regions in the magnetic charge space.","sentences":["Using $N = 2$ supergravity formalism, we investigate certain behaviors of five dimensional black objects from the compactification of M-theory on a Calabi-Yau three-fold.","The manifold has been constructed as the intersection of two homogeneous polynomials of degrees $ (\\omega+2,1)$ and $ (2,1) $ in a product of two weighted projective spaces given by $ \\mathbb{WP}^{4}(\\omega,1,1,1,1) \\times\\mathbb{P}^{1}$.","First, we determine the allowed electric charge regions of the BPS and non BPS black holes obtained by wrapping M2-branes on appropriate two cycles in such a proposed Calabi-Yau three-fold.","After that, we calculate the entropy of these solutions which takes a maximal value corresponding to $\\omega=1$ defining the ordinary projective space $\\mathbb{P}^{4}$. For generic values of $\\omega$, we show that the non BPS states are unstable.","Then, we conduct a similar study of five dimensional black strings.","Concerning the allowed magnetic charge regions of the BPS and non BPS black stringy solutions derived from M5-branes on dual divisors, we calculate the tension taking a minimal value for $\\mathbb{P}^{4}$. By determining the recombination factor, we show that the non-BPS black string states are stable in the allowed regions in the magnetic charge space."],"url":"http://arxiv.org/abs/2403.16724v1","category":"hep-th"}
{"created":"2024-03-25 12:57:41","title":"Static equilibrium of multi-black holes in expanding bubbles in five dimensions","abstract":"We investigate possible configurations for vacuum multi-black holes that maintain static equilibrium in expanding bubbles. Our analysis assumes a five-dimensional Weyl metric to describe the spacetime, facilitating the derivation of solutions based on the provided rod structure. We consider a spacetime having expanding bubbles caused by one or two acceleration horizons, and show that various configurations such as two bubbles, four bubbles devoid of horizons, a black saturn, a black di-ring, a bicycling black ring (orthogonal black di-ring), and a five-dimensional black hole binary can achieve equilibrium within expanding bubbles. Specifically, we demonstrate that equilibrium requires two acceleration horizons on both sides for the bicycling ring and the five-dimensional black hole binary. However, only one acceleration horizon is necessary for achieving equilibrium in the case of the black saturn and the black di-ring.","sentences":["We investigate possible configurations for vacuum multi-black holes that maintain static equilibrium in expanding bubbles.","Our analysis assumes a five-dimensional Weyl metric to describe the spacetime, facilitating the derivation of solutions based on the provided rod structure.","We consider a spacetime having expanding bubbles caused by one or two acceleration horizons, and show that various configurations such as two bubbles, four bubbles devoid of horizons, a black saturn, a black di-ring, a bicycling black ring (orthogonal black di-ring), and a five-dimensional black hole binary can achieve equilibrium within expanding bubbles.","Specifically, we demonstrate that equilibrium requires two acceleration horizons on both sides for the bicycling ring and the five-dimensional black hole binary.","However, only one acceleration horizon is necessary for achieving equilibrium in the case of the black saturn and the black di-ring."],"url":"http://arxiv.org/abs/2403.16723v1","category":"hep-th"}
{"created":"2024-03-25 12:57:16","title":"Anti-de Sitter Momentum Space in 3D and 4D Quantum Gravity","abstract":"There has been strong interest in the possibility that in the quantum-gravity realm momentum space might be curved, mainly focusing, especially for what concerns phenomenological implications, on the case of a de Sitter momentum space. We here take as starting point the known fact that quantum gravity coupled to matter in $2+1$ spacetime dimensions gives rise to an effective picture characterized by a momentum space with anti-de Sitter geometry, and we point out some key properties of $2+1$-dimensional anti-de Sitter momentum space. We observe that it is impossible to implement all of these properties in theories with a $3+1$-dimensional anti-de Sitter momentum space, and we then investigate, with the aim of providing guidance to the relevant phenomenology focusing on possible modified laws of conservation of momenta, the implications of giving up, in the $3+1$-dimensional case, some of the properties of the $2+1$-dimensional case.","sentences":["There has been strong interest in the possibility that in the quantum-gravity realm momentum space might be curved, mainly focusing, especially for what concerns phenomenological implications, on the case of a de Sitter momentum space.","We here take as starting point the known fact that quantum gravity coupled to matter in $2+1$ spacetime dimensions gives rise to an effective picture characterized by a momentum space with anti-de Sitter geometry, and we point out some key properties of $2+1$-dimensional anti-de Sitter momentum space.","We observe that it is impossible to implement all of these properties in theories with a $3+1$-dimensional anti-de Sitter momentum space, and we then investigate, with the aim of providing guidance to the relevant phenomenology focusing on possible modified laws of conservation of momenta, the implications of giving up, in the $3+1$-dimensional case, some of the properties of the $2+1$-dimensional case."],"url":"http://arxiv.org/abs/2403.16721v1","category":"gr-qc"}
{"created":"2024-03-25 12:57:03","title":"Derivation of Jacobian matrices for the error propagation of charged particles traversing magnetic fields and materials","abstract":"In high-energy physics experiments, the trajectories of charged particles are reconstructed using track reconstruction algorithms. Such algorithms need to both identify the set of measurements from a single charged particle and to fit the parameters by propagating tracks along the measurements. The propagation of the track parameter uncertainties is an important component in the track fitting to get the optimal precision in the fitted parameters. The error propagation is performed at the surface intersections by calculating a Jacobian matrix corresponding to the surface-to-surface transport. This paper derives the Jacobian matrix in a general manner to harmonize with semi-analytical numerical integration methods developed for inhomogeneous magnetic fields and materials. The Jacobian and transported covariance matrices are validated by simulating the charged particles between two surfaces and comparing with the results of numerical methods.","sentences":["In high-energy physics experiments, the trajectories of charged particles are reconstructed using track reconstruction algorithms.","Such algorithms need to both identify the set of measurements from a single charged particle and to fit the parameters by propagating tracks along the measurements.","The propagation of the track parameter uncertainties is an important component in the track fitting to get the optimal precision in the fitted parameters.","The error propagation is performed at the surface intersections by calculating a Jacobian matrix corresponding to the surface-to-surface transport.","This paper derives the Jacobian matrix in a general manner to harmonize with semi-analytical numerical integration methods developed for inhomogeneous magnetic fields and materials.","The Jacobian and transported covariance matrices are validated by simulating the charged particles between two surfaces and comparing with the results of numerical methods."],"url":"http://arxiv.org/abs/2403.16720v1","category":"hep-ex"}
{"created":"2024-03-25 12:56:48","title":"Towards a Formalisation of Value-based Actions and Consequentialist Ethics","abstract":"Agents act to bring about a state of the world that is more compatible with their personal or institutional values. To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation. Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile. Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential.","sentences":["Agents act to bring about a state of the world that is more compatible with their personal or institutional values.","To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation.","Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile.","Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential."],"url":"http://arxiv.org/abs/2403.16719v1","category":"cs.MA"}
{"created":"2024-03-25 12:53:42","title":"Superconformal Symmetry and Index Theory","abstract":"Formulation and supersymmetry localization of superconformal indices for $\\mathcal{N}=2B$ superconformal quantum mechanics are reviewed by providing a generalization to fixed point submanifolds of resolved target space geometries, and future applications to gauged scaling quivers are discussed.","sentences":["Formulation and supersymmetry localization of superconformal indices for $\\mathcal{N}=2B$ superconformal quantum mechanics are reviewed by providing a generalization to fixed point submanifolds of resolved target space geometries, and future applications to gauged scaling quivers are discussed."],"url":"http://arxiv.org/abs/2403.16716v1","category":"hep-th"}
{"created":"2024-03-25 12:52:51","title":"Modulational electrostatic wave-wave interactions in plasma fluids modeled by asymmetric coupled nonlinear Schr\u00f6dinger (CNLS) equations","abstract":"The interaction between two co-propagating electrostatic wavepackets characterized by arbitrary carrier wavenumber is considered. A one-dimensional (1D) non-magnetized plasma model is adopted, consisting of a cold inertial ion fluid evolving against a thermalized (Maxwell-Boltzmann distributed) electron background. A multiple-scale perturbation method is employed to reduce the original model equations to a pair of coupled nonlinear Schr\\\"odinger (CNLS) equations governing the dynamics of the wavepacket amplitudes (envelopes). The CNLS equations are in general asymmetric for arbitrary carrier wabvenumbers. Similar CNLS systems have been derived in the past in various physical contexts, and were found to support soliton, breather, and rogue wave solutions, among others. A detailed stability analysis reveals that modulational instability (MI) is possible in a wide range of values in the parameter space. The instability window and the corresponding growth rate are determined, considering different case studies, and their dependence on the carrier and the perturbation wavenumber is investigated from first principles. Wave-wave coupling is shown to favor MI occurrence by extending its range of occurrence and by enhancing its growth rate. Our findings generalize previously known results usually associated with symmetric NLS equations in nonlinear optics, though taking into account the difference between the different envelope wavenumbers and thus group velocities.","sentences":["The interaction between two co-propagating electrostatic wavepackets characterized by arbitrary carrier wavenumber is considered.","A one-dimensional (1D) non-magnetized plasma model is adopted, consisting of a cold inertial ion fluid evolving against a thermalized (Maxwell-Boltzmann distributed) electron background.","A multiple-scale perturbation method is employed to reduce the original model equations to a pair of coupled nonlinear Schr\\\"odinger (CNLS) equations governing the dynamics of the wavepacket amplitudes (envelopes).","The CNLS equations are in general asymmetric for arbitrary carrier wabvenumbers.","Similar CNLS systems have been derived in the past in various physical contexts, and were found to support soliton, breather, and rogue wave solutions, among others.","A detailed stability analysis reveals that modulational instability (MI) is possible in a wide range of values in the parameter space.","The instability window and the corresponding growth rate are determined, considering different case studies, and their dependence on the carrier and the perturbation wavenumber is investigated from first principles.","Wave-wave coupling is shown to favor MI occurrence by extending its range of occurrence and by enhancing its growth rate.","Our findings generalize previously known results usually associated with symmetric NLS equations in nonlinear optics, though taking into account the difference between the different envelope wavenumbers and thus group velocities."],"url":"http://arxiv.org/abs/2403.16715v1","category":"physics.plasm-ph"}
{"created":"2024-03-25 12:51:49","title":"A Mixed Multiscale Spectral Generalized Finite Element Method","abstract":"We present a multiscale mixed finite element method for solving second order elliptic equations with general $L^{\\infty}$-coefficients arising from flow in highly heterogeneous porous media. Our approach is based on a multiscale spectral generalized finite element method (MS-GFEM) and exploits the superior local mass conservation properties of mixed finite elements. Following the MS-GFEM framework, optimal local approximation spaces are built for the velocity field by solving local eigenvalue problems over generalized harmonic spaces. The resulting global velocity space is then enriched suitably to ensure inf-sup stability. We develop the mixed MS-GFEM for both continuous and discrete formulations, with Raviart-Thomas based mixed finite elements underlying the discrete method. Exponential convergence with respect to local degrees of freedom is proven at both the continuous and discrete levels. Numerical results are presented to support the theory and to validate the proposed method.","sentences":["We present a multiscale mixed finite element method for solving second order elliptic equations with general $L^{\\infty}$-coefficients arising from flow in highly heterogeneous porous media.","Our approach is based on a multiscale spectral generalized finite element method (MS-GFEM) and exploits the superior local mass conservation properties of mixed finite elements.","Following the MS-GFEM framework, optimal local approximation spaces are built for the velocity field by solving local eigenvalue problems over generalized harmonic spaces.","The resulting global velocity space is then enriched suitably to ensure inf-sup stability.","We develop the mixed MS-GFEM for both continuous and discrete formulations, with Raviart-Thomas based mixed finite elements underlying the discrete method.","Exponential convergence with respect to local degrees of freedom is proven at both the continuous and discrete levels.","Numerical results are presented to support the theory and to validate the proposed method."],"url":"http://arxiv.org/abs/2403.16714v1","category":"math.NA"}
{"created":"2024-03-25 12:51:22","title":"Design Patterns for Multilevel Modeling and Simulation","abstract":"Multilevel modeling and simulation (M&S) is becoming increasingly relevant due to the benefits that this methodology offers. Multilevel models allow users to describe a system at multiple levels of detail. From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required. From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time. A downside of multilevel M&S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on. In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models. The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on. Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&S application area.","sentences":["Multilevel modeling and simulation (M&S) is becoming increasingly relevant due to the benefits that this methodology offers.","Multilevel models allow users to describe a system at multiple levels of detail.","From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required.","From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time.","A downside of multilevel M&S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on.","In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models.","The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on.","Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&S application area."],"url":"http://arxiv.org/abs/2403.16713v1","category":"cs.SE"}
{"created":"2024-03-25 12:49:40","title":"Chase Termination Beyond Polynomial Time","abstract":"The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering. However, it is merely a semi-decision procedure, which may fail to terminate. Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of \"cycle\" in the chase process. We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions. This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions. Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k-ExpTime. We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k-ExpSpace, respectively.","sentences":["The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering.","However, it is merely a semi-decision procedure, which may fail to terminate.","Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of \"cycle\" in the chase process.","We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions.","This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions.","Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k-ExpTime.","We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k-ExpSpace, respectively."],"url":"http://arxiv.org/abs/2403.16712v1","category":"cs.DB"}
{"created":"2024-03-25 12:48:27","title":"A Gauss-Bonnet formula for the renormalized area of minimal submanifolds of Poincar\u00e9-Einstein manifolds","abstract":"Assuming the extrinsic $Q$-curvature admits a decomposition into the Pfaffian, a scalar conformal submanifold invariant, and a tangential divergence, we prove that the renormalized area of an even-dimensional minimal submanifold of a Poincar\\'e-Einstein manifold can be expressed as a linear combination of its Euler characteristic and the integral of a scalar conformal submanifold invariant. We derive such a decomposition of the extrinsic $Q$-curvature in dimensions two and four, thereby recovering and generalizing results of Alexakis-Mazzeo and Tyrrell, respectively. We also conjecture such a decomposition for general natural submanifold scalars whose integral over compact submanifolds is conformally invariant, and verify our conjecture in dimensions two and four. Our results also apply to the area of a compact even-dimensional minimal submanifold of an Einstein manifold.","sentences":["Assuming the extrinsic $Q$-curvature admits a decomposition into the Pfaffian, a scalar conformal submanifold invariant, and a tangential divergence, we prove that the renormalized area of an even-dimensional minimal submanifold of a Poincar\\'e-Einstein manifold can be expressed as a linear combination of its Euler characteristic and the integral of a scalar conformal submanifold invariant.","We derive such a decomposition of the extrinsic $Q$-curvature in dimensions two and four, thereby recovering and generalizing results of Alexakis-Mazzeo and Tyrrell, respectively.","We also conjecture such a decomposition for general natural submanifold scalars whose integral over compact submanifolds is conformally invariant, and verify our conjecture in dimensions two and four.","Our results also apply to the area of a compact even-dimensional minimal submanifold of an Einstein manifold."],"url":"http://arxiv.org/abs/2403.16710v1","category":"math.DG"}
{"created":"2024-03-25 12:46:15","title":"Facile synthesis of CoSi alloy with rich vacancy for base- and solvent-free aerobic oxidation of aromatic alcohols","abstract":"Rational design and green synthesis of low-cost and robust catalysts efficient for the selective oxidation of various alcohols are full of challenges. Herein, we report a fast and solvent-free arc-melting (AM) method to controllably synthesize semimetal CoSi alloy (abbreviated as AM-CoSi) that is efficient for the base- and solvent-free oxidation of six types of aromatic alcohols. X-ray absorption fine structure (XAFS), electron paramagnetic resonance (EPR), and aberration corrected high angle annular dark field scanning transmission electron microscope (AC HAADF-STEM) confirmed the successful synthesis of AM-CoSi with rich Si vacancy (Siv). The as-prepared CoSi alloy catalysts exhibit an order of magnitude activity enhancement in the oxidation of model reactant benzyl alcohol (BAL) to benzyl benzoate (BBE) compared with its mono counterparts, whereas 70 % yield of BBE which is the highest yield to date. Experimental results and DFT calculations well verify that the CoSi alloy structure improves the BAL conversion and Si vacancy mainly contributes to the generation of BBE. After that, CoSi alloy maintains high stability and a potential pathway is rationally proposed. Besides, CoSi alloy also efficiently works for the selective oxidation of various alcohols with different groups. This work demonstrates for the first time that semimetal CoSi alloy is robust for the green oxidation of various alcohols and provides a vast opportunity for reasonable design and application of other semimetal alloy catalysts.","sentences":["Rational design and green synthesis of low-cost and robust catalysts efficient for the selective oxidation of various alcohols are full of challenges.","Herein, we report a fast and solvent-free arc-melting (AM) method to controllably synthesize semimetal CoSi alloy (abbreviated as AM-CoSi) that is efficient for the base- and solvent-free oxidation of six types of aromatic alcohols.","X-ray absorption fine structure (XAFS), electron paramagnetic resonance (EPR), and aberration corrected high angle annular dark field scanning transmission electron microscope (AC HAADF-STEM) confirmed the successful synthesis of AM-CoSi with rich Si vacancy (Siv).","The as-prepared CoSi alloy catalysts exhibit an order of magnitude activity enhancement in the oxidation of model reactant benzyl alcohol (BAL) to benzyl benzoate (BBE) compared with its mono counterparts, whereas 70 % yield of BBE which is the highest yield to date.","Experimental results and DFT calculations well verify that the CoSi alloy structure improves the BAL conversion and Si vacancy mainly contributes to the generation of BBE.","After that, CoSi alloy maintains high stability and a potential pathway is rationally proposed.","Besides, CoSi alloy also efficiently works for the selective oxidation of various alcohols with different groups.","This work demonstrates for the first time that semimetal CoSi alloy is robust for the green oxidation of various alcohols and provides a vast opportunity for reasonable design and application of other semimetal alloy catalysts."],"url":"http://arxiv.org/abs/2403.16708v1","category":"physics.chem-ph"}
{"created":"2024-03-25 12:44:52","title":"One-Shot Domain Incremental Learning","abstract":"Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our technique through experiments on open datasets.","sentences":["Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification.","In DIL, we assume that samples on new domains are observed over time.","The models must classify inputs on all domains.","In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently.","Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL.","We first empirically show that existing DIL methods do not work well in one-shot DIL.","We have analyzed the reason for this failure through various investigations.","According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers.","Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our technique through experiments on open datasets."],"url":"http://arxiv.org/abs/2403.16707v1","category":"cs.LG"}
{"created":"2024-03-25 12:33:10","title":"Resonant Beam Communications: A New Design Paradigm and Challenges","abstract":"Resonant beam communications (RBCom), which adopt oscillating photons between two separate retroreflectors for information transmission, exhibit potential advantages over other types of wireless optical communications (WOC). However, echo interference generated by the modulated beam reflected from the receiver affects the transmission of the desired information. To tackle this challenge, a synchronization-based point-to-point RBCom system is proposed to eliminate the echo interference, and the design for the transmitter and receiver is discussed. Subsequently, the performance of the proposed RBCom is evaluated and compared with that of visible light communications (VLC) and free space optical communications (FOC). Finally, future research directions are outlined and several implementation challenges of RBCom systems are highlighted.","sentences":["Resonant beam communications (RBCom), which adopt oscillating photons between two separate retroreflectors for information transmission, exhibit potential advantages over other types of wireless optical communications (WOC).","However, echo interference generated by the modulated beam reflected from the receiver affects the transmission of the desired information.","To tackle this challenge, a synchronization-based point-to-point RBCom system is proposed to eliminate the echo interference, and the design for the transmitter and receiver is discussed.","Subsequently, the performance of the proposed RBCom is evaluated and compared with that of visible light communications (VLC) and free space optical communications (FOC).","Finally, future research directions are outlined and several implementation challenges of RBCom systems are highlighted."],"url":"http://arxiv.org/abs/2403.16699v1","category":"cs.IT"}
{"created":"2024-03-25 12:31:01","title":"DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization","abstract":"Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain. Recent work, PromptStyler, employs text prompts to simulate different distribution shifts in the joint vision-language space, allowing the model to generalize effectively to unseen domains without using any images. However, 1) PromptStyler's style generation strategy has limitations, as all style patterns are fixed after the first training phase. This leads to the training set in the second training phase being restricted to a limited set of styles. Additionally, 2) the frozen text encoder in PromptStyler result in the encoder's output varying with the style of the input text prompts, making it difficult for the model to learn domain-invariant features. In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues. The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles. Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets.","sentences":["Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain.","Recent work, PromptStyler, employs text prompts to simulate different distribution shifts in the joint vision-language space, allowing the model to generalize effectively to unseen domains without using any images.","However, 1) PromptStyler's style generation strategy has limitations, as all style patterns are fixed after the first training phase.","This leads to the training set in the second training phase being restricted to a limited set of styles.","Additionally, 2) the frozen text encoder in PromptStyler result in the encoder's output varying with the style of the input text prompts, making it difficult for the model to learn domain-invariant features.","In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues.","The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles.","Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity.","Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets."],"url":"http://arxiv.org/abs/2403.16697v1","category":"cs.CV"}
{"created":"2024-03-25 12:26:32","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading in Prostate Cancer","abstract":"Prostate cancer is a dominant health concern calling for advanced diagnostic tools. Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures. A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles. The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance. Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades. The ConvNeXt model was capable of learning a balance between complexity and generalizability. Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially improving diagnostic efficiency for prostate cancer.","sentences":["Prostate cancer is a dominant health concern calling for advanced diagnostic tools.","Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures.","A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles.","The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance.","Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades.","The ConvNeXt model was capable of learning a balance between complexity and generalizability.","Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially improving diagnostic efficiency for prostate cancer."],"url":"http://arxiv.org/abs/2403.16695v1","category":"eess.IV"}
{"created":"2024-03-25 12:23:12","title":"Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography","abstract":"In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.","sentences":["In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education.","LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students.","Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic.","This research recruited 34 undergraduate students as participants, who were randomly divided into two groups.","The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers.","Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\".","The research findings show comparable scores between the two groups on the retention test.","However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test.","Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity.","However, its strengths on promoting students.","knowledge application and creativity were insignificant.","Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses.","Combining ChatGPT with traditional human teachers might be a more ideal approach.","The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching."],"url":"http://arxiv.org/abs/2403.16687v1","category":"cs.CY"}
{"created":"2024-03-25 12:21:38","title":"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation","abstract":"The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.","sentences":["The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups.","While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language.","Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity.","This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech.","Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem.","Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem.","Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task.","To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech.","Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation.","ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly."],"url":"http://arxiv.org/abs/2403.16685v1","category":"cs.CL"}
{"created":"2024-03-25 12:19:11","title":"CHANG-ES. XXX. 10 kpc Radio Lobes in The Sombrero Galaxy","abstract":"We report the discovery of the 10 kilo-parsec (kpc) scale radio lobes in the Sombrero galaxy (NGC 4594), using data from the Continuum Halos in Nearby Galaxies - an Expanded Very Large Array (VLA) Survey (CHANG-ES) project. We further examine the balance between the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas. At the radii $r$ of ~(1-10) kpc, the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas are generally in balance. This implies that the jets could expand into the surroundings at least to r ~ 10 kpc. The feedback from the active galactic nucleus (AGN) jet responsible for the large-scale lobes may help to explain the unusually high X-ray luminosity of this massive quiescent isolated disk galaxy, although more theoretical work is needed to further examine this possibility.","sentences":["We report the discovery of the 10 kilo-parsec (kpc) scale radio lobes in the Sombrero galaxy (NGC 4594), using data from the Continuum Halos in Nearby Galaxies - an Expanded Very Large Array (VLA) Survey (CHANG-ES) project.","We further examine the balance between the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas.","At the radii $r$ of ~(1-10) kpc, the magnetic pressure inside the lobes and the thermal pressure of the ambient hot gas are generally in balance.","This implies that the jets could expand into the surroundings at least to r ~ 10 kpc.","The feedback from the active galactic nucleus (AGN) jet responsible for the large-scale lobes may help to explain the unusually high X-ray luminosity of this massive quiescent isolated disk galaxy, although more theoretical work is needed to further examine this possibility."],"url":"http://arxiv.org/abs/2403.16682v1","category":"astro-ph.GA"}
{"created":"2024-03-25 12:15:55","title":"A note on generalization bounds for losses with finite moments","abstract":"This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.","sentences":["This paper studies the truncation method from Alquier","[1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails.","Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \\to \\infty$ and the loss is essentially bounded.","Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance.","This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature.","Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes.","In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings."],"url":"http://arxiv.org/abs/2403.16681v1","category":"stat.ML"}
{"created":"2024-03-25 12:15:47","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","abstract":"Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization. Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary. An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/tum-pbs/SFBC.","sentences":["Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics.","Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning.","We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation.","We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy.","Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization.","Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary.","An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/tum-pbs/SFBC."],"url":"http://arxiv.org/abs/2403.16680v1","category":"cs.LG"}
{"created":"2024-03-25 12:15:42","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks","abstract":"Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows. Previous studies already demonstrated AI's potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability. To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections. Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing fine-tuned image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures. The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides. We demonstrated that DeepGleason is capable of highly accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806, AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison revealed that the ConvNeXt model was superior performance-wise on our dataset to established and other modern architectures like transformers. Furthermore, we were able to outperform the current state-of-the-art in tile-wise fine-classification with a sensitivity and specificity of 0.94 and 0.98 for benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs Gleason 4 & 5 classification, respectively. Our tool contributes to the wider adoption of AI-based Gleason grading within the research community and paves the way for broader clinical application of deep learning models in digital pathology. DeepGleason is open-source and publicly available for research application in the following Git repository: https://github.com/frankkramer-lab/DeepGleason.","sentences":["Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows.","Previous studies already demonstrated AI's potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability.","To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections.","Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing fine-tuned image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures.","The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides.","We demonstrated that DeepGleason is capable of highly accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806, AUC of 0.991, and Accuracy of 0.974.","The internal architecture comparison revealed that the ConvNeXt model was superior performance-wise on our dataset to established and other modern architectures like transformers.","Furthermore, we were able to outperform the current state-of-the-art in tile-wise fine-classification with a sensitivity and specificity of 0.94 and 0.98 for benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs Gleason 4 & 5 classification, respectively.","Our tool contributes to the wider adoption of AI-based Gleason grading within the research community and paves the way for broader clinical application of deep learning models in digital pathology.","DeepGleason is open-source and publicly available for research application in the following Git repository: https://github.com/frankkramer-lab/DeepGleason."],"url":"http://arxiv.org/abs/2403.16678v1","category":"eess.IV"}
{"created":"2024-03-25 12:14:34","title":"Design and Performance of Resonant Beam Communications -- Part I: Quasi-Static Scenario","abstract":"This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios. Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed. Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam. With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel. Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds. Finally, numerical results validate our analysis. Part II of this paper discusses the performance of the RBCom system under the mobile scenario.","sentences":["This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios.","Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed.","Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam.","With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel.","Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds.","Finally, numerical results validate our analysis.","Part II of this paper discusses the performance of the RBCom system under the mobile scenario."],"url":"http://arxiv.org/abs/2403.16676v1","category":"cs.IT"}
{"created":"2024-03-25 12:13:20","title":"Understanding the Functional Roles of Modelling Components in Spiking Neural Networks","abstract":"Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation. With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios. This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models.","sentences":["Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity.","Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear.","By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs.","Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs.","Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation.","With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios.","This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models."],"url":"http://arxiv.org/abs/2403.16674v1","category":"cs.NE"}
{"created":"2024-03-25 12:07:45","title":"Probabilistic bivariate Bell polynomials","abstract":"Let Y be a random variable whose moment generating function exists in some neighborhood of the origin. We consider the probabilistic bivariate Bell polynomials associated with Y and the probabilistic bivariate r-Bell polynomials associated with Y. For those polynomials, we derive the recurrence relations corresponding to the ones found by Zheng and Li for the bivariate Bell polynomials and the bivariate r-Bell polynomials.","sentences":["Let Y be a random variable whose moment generating function exists in some neighborhood of the origin.","We consider the probabilistic bivariate Bell polynomials associated with Y and the probabilistic bivariate r-Bell polynomials associated with Y. For those polynomials, we derive the recurrence relations corresponding to the ones found by Zheng and Li for the bivariate Bell polynomials and the bivariate r-Bell polynomials."],"url":"http://arxiv.org/abs/2403.16670v1","category":"math.NT"}
{"created":"2024-03-25 12:04:03","title":"Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization","abstract":"Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective. Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored. Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed. In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches. Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives.","sentences":["Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective.","Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored.","Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed.","In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches.","Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives."],"url":"http://arxiv.org/abs/2403.16667v1","category":"cs.AI"}
{"created":"2024-03-25 12:01:27","title":"Revisiting the Sleeping Beauty problem","abstract":"The Sleeping Beauty problem is a probability riddle with no definite solution for more than two decades and its solution is of great interest in many fields of knowledge. There are two main competing solutions to the problem: the halfer approach, and the thirder approach. The main reason for disagreement in the literature is connected to the use of different probability spaces to represent the same probabilistic riddle. In this work, we analyse the problem from a mathematical perspective, identifying probability distributions induced directly from the thought experiment's rules. The precise choices of probability spaces provide both halfer and thirder solutions to the problem. To try and decide on which approach to follow, a criterion involving the information available to Sleeping Beauty is proposed.","sentences":["The Sleeping Beauty problem is a probability riddle with no definite solution for more than two decades and its solution is of great interest in many fields of knowledge.","There are two main competing solutions to the problem: the halfer approach, and the thirder approach.","The main reason for disagreement in the literature is connected to the use of different probability spaces to represent the same probabilistic riddle.","In this work, we analyse the problem from a mathematical perspective, identifying probability distributions induced directly from the thought experiment's rules.","The precise choices of probability spaces provide both halfer and thirder solutions to the problem.","To try and decide on which approach to follow, a criterion involving the information available to Sleeping Beauty is proposed."],"url":"http://arxiv.org/abs/2403.16666v1","category":"math.HO"}
{"created":"2024-03-25 11:56:30","title":"Phase separation dynamics in a symmetric binary mixture of ultrasoft particles","abstract":"Phase separation plays an role in determining the self-assembly of biological and soft-matter systems. In biological systems, liquid-liquid phase separation inside a cell leads to the formation of various macromolecular aggregates. The interaction among these aggregates is soft, i.e., these can significantly overlap at a small energy cost. From the computer simulation point of view, these complex macromolecular aggregates are generally modeled by the so-called soft particles. The effective interaction between two particles is defined via the generalized exponential potential (GEM-n) with n = 4. Here, using molecular dynamics simulations, we study the phase separation dynamics of a size-symmetric binary mixture of ultrasoft particles. We find that when the mixture is quenched to a lower temperature below the critical temperature, the two components spontaneously start to separate. Domains of the two components form, and the equal-time order parameter reveals that the domains grow in a power-law manner with exponent 1/3, which is consistent with the Lifshitz-Slyozov law for conserved systems. Further, the static structure factor shows a power-law decay with exponent 4 consistent with the Porod law.","sentences":["Phase separation plays an role in determining the self-assembly of biological and soft-matter systems.","In biological systems, liquid-liquid phase separation inside a cell leads to the formation of various macromolecular aggregates.","The interaction among these aggregates is soft, i.e., these can significantly overlap at a small energy cost.","From the computer simulation point of view, these complex macromolecular aggregates are generally modeled by the so-called soft particles.","The effective interaction between two particles is defined via the generalized exponential potential (GEM-n) with n = 4.","Here, using molecular dynamics simulations, we study the phase separation dynamics of a size-symmetric binary mixture of ultrasoft particles.","We find that when the mixture is quenched to a lower temperature below the critical temperature, the two components spontaneously start to separate.","Domains of the two components form, and the equal-time order parameter reveals that the domains grow in a power-law manner with exponent 1/3, which is consistent with the Lifshitz-Slyozov law for conserved systems.","Further, the static structure factor shows a power-law decay with exponent 4 consistent with the Porod law."],"url":"http://arxiv.org/abs/2403.16663v1","category":"cond-mat.soft"}
{"created":"2024-03-25 11:56:29","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict","abstract":"Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks.","sentences":["Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence.","High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans.","However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge.","To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web.","Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation.","To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations.","Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks."],"url":"http://arxiv.org/abs/2403.16662v1","category":"cs.CL"}
{"created":"2024-03-25 11:54:35","title":"Dynamics of Cayley Forms","abstract":"The most natural first-order PDE's to be imposed on a Cayley 4-form in eight dimensions is the condition that it is closed. As is known, this implies integrability of the Spin(7) structure defined by the Cayley form, as well as Ricci-flatness of the associated metric. We address the question as to what the most natural second-order in derivatives set of conditions is. We start by constructing the most general diffeomorphism invariant second order in derivatives Lagrangian that is quadratic in the perturbations of the Cayley form. We find that there is a one-parameter family of such Lagrangians. We then describe a non-linear completion of the linear story. To this end, we parametrise the intrinsic torsion of a Spin(7) structure by a 3-form, and show that this 3-form is completely determined by the exterior derivative of the Cayley form. We construct an action functional, which depends on the Cayley 4-form and and auxiliary 3-form as independent variables. There is a unique functional whose Euler-Lagrange equation for the auxiliary 3-form states that it is equal to the torsion 3-form. There is, however, a more general one-parameter family of functionals that can be constructed, and we show how the linearisation of these functionals reproduces the linear story. For any member of our family of theories, the Euler-Lagrange equations are written only using the operator of exterior differentiation of forms, and do not require the knowledge of the metric-compatible Levi-Civita connection. Geometrically, there is a preferred member in the family of Lagrangians, and we propose that its Euler-Lagrange equations are the most natural second-order equations to be satisfied by Cayley forms. Our construction also leads to a natural geometric flow in the space of Cayley forms, defined as the gradient flow of our action functional.","sentences":["The most natural first-order PDE's to be imposed on a Cayley 4-form in eight dimensions is the condition that it is closed.","As is known, this implies integrability of the Spin(7) structure defined by the Cayley form, as well as Ricci-flatness of the associated metric.","We address the question as to what the most natural second-order in derivatives set of conditions is.","We start by constructing the most general diffeomorphism invariant second order in derivatives Lagrangian that is quadratic in the perturbations of the Cayley form.","We find that there is a one-parameter family of such Lagrangians.","We then describe a non-linear completion of the linear story.","To this end, we parametrise the intrinsic torsion of a Spin(7) structure by a 3-form, and show that this 3-form is completely determined by the exterior derivative of the Cayley form.","We construct an action functional, which depends on the Cayley 4-form and and auxiliary 3-form as independent variables.","There is a unique functional whose Euler-Lagrange equation for the auxiliary 3-form states that it is equal to the torsion 3-form.","There is, however, a more general one-parameter family of functionals that can be constructed, and we show how the linearisation of these functionals reproduces the linear story.","For any member of our family of theories, the Euler-Lagrange equations are written only using the operator of exterior differentiation of forms, and do not require the knowledge of the metric-compatible Levi-Civita connection.","Geometrically, there is a preferred member in the family of Lagrangians, and we propose that its Euler-Lagrange equations are the most natural second-order equations to be satisfied by Cayley forms.","Our construction also leads to a natural geometric flow in the space of Cayley forms, defined as the gradient flow of our action functional."],"url":"http://arxiv.org/abs/2403.16661v1","category":"math.DG"}
{"created":"2024-03-25 11:51:24","title":"A classical Bousso bound for higher derivative corrections to general relativity","abstract":"We prove the classical version of the covariant entropy bound (also known as the Bousso bound) in arbitrary diffeomorphism invariant gravitational theories. We focus on theories for which the higher derivative terms are considered as small corrections in the Lagrangian to Einstein's two-derivative theory of general relativity (GR). Even if the higher derivative corrections are treated perturbatively, we provide instances of specific configurations for which they can potentially violate the Bousso bound. To tackle this obstruction, we propose a modification in the Bousso bound that incorporates the offending contributions from the higher derivative corrections. Our proposed modifications are equivalent to replacing the Bekenstein-Hawking area term by Wald's definition (with dynamical corrections as suggested by Wall) for the black hole entropy. Hence, the modifications are physically well motivated by results from the laws of black hole mechanics in higher derivative theories.","sentences":["We prove the classical version of the covariant entropy bound (also known as the Bousso bound) in arbitrary diffeomorphism invariant gravitational theories.","We focus on theories for which the higher derivative terms are considered as small corrections in the Lagrangian to Einstein's two-derivative theory of general relativity (GR).","Even if the higher derivative corrections are treated perturbatively, we provide instances of specific configurations for which they can potentially violate the Bousso bound.","To tackle this obstruction, we propose a modification in the Bousso bound that incorporates the offending contributions from the higher derivative corrections.","Our proposed modifications are equivalent to replacing the Bekenstein-Hawking area term by Wald's definition (with dynamical corrections as suggested by Wall) for the black hole entropy.","Hence, the modifications are physically well motivated by results from the laws of black hole mechanics in higher derivative theories."],"url":"http://arxiv.org/abs/2403.16658v1","category":"hep-th"}
{"created":"2024-03-25 11:47:53","title":"Graph Augmentation for Recommendation","abstract":"Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.","sentences":["Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited.","However, directly applying existing GCL models to real-world recommendation environments poses challenges.","There are two primary issues to address.","Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance.","Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing.","To address these challenges, we propose a principled framework called GraphAug.","This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems.","The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation.","Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model.","The outcomes consistently unveil its superiority over existing baseline methods.","The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug."],"url":"http://arxiv.org/abs/2403.16656v1","category":"cs.LG"}
{"created":"2024-03-25 11:45:21","title":"Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT","abstract":"Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text.   This project aims at analyzing different kinds of error that occurs in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%).","sentences":["Text continues to remain a relevant form of representation for information.","Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech.","While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content.","All these variety of mechanisms of text generation also introduce errors into the captured text.   ","This project aims at analyzing different kinds of error that occurs in text documents.","The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text.","Transfer learning of these models with available dataset is performed to finetune their capacity for error correction.","A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories.","It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%)."],"url":"http://arxiv.org/abs/2403.16655v1","category":"cs.CL"}
{"created":"2024-03-25 11:42:01","title":"A Novel Loss Function-based Support Vector Machine for Binary Classification","abstract":"The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM. Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis. The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method.","sentences":["The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin.","This oversight affects the generalization ability of the SVM classifier to some extent.","To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM).","By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM.","Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM.","To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis.","The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.16654v1","category":"cs.LG"}
{"created":"2024-03-25 11:37:15","title":"CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment","abstract":"Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``\\textit{Helpful and Harmless}'' dataset.","sentences":["Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users.","However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.","To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly.","CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process.","Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences.","Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``\\textit{Helpful and Harmless}'' dataset."],"url":"http://arxiv.org/abs/2403.16649v1","category":"cs.AI"}
{"created":"2024-03-25 11:33:39","title":"Effect of Coriolis Force on Electrical Conductivity Tensor for Rotating Hadron Resonance Gas","abstract":"We have investigated the influence of the Coriolis force on the electrical conductivity of hadronic matter formed in relativistic nuclear collisions, employing the Hadron Resonance Gas (HRG) model. A rotating matter in the peripheral heavy ion collisions can be expected from the initial stage of quark matter to late-stage hadronic matter. Present work is focused on rotating hadronic matter, whose medium constituents - hadron resonances can face a non-zero Coriolis force, which can influence the hadronic flow or conductivity. We estimate this conductivity tensor by using the relativistic Boltzmann transport equation. In the absence of Coriolis force, an isotropic conductivity tensor for hadronic matter is expected. However, our study finds that the presence of Coriolis force can generate an anisotropic conductivity tensor with three main conductivity components - parallel, perpendicular, and Hall, similar to the effect of Lorentz force at a finite magnetic field. Our study has indicated that a noticeable anisotropy of conductivity tensor can be found within the phenomenological range of angular velocity $\\Omega= 0.001-0.02$ GeV and hadronic scattering radius $a=0.2-2$ fm.","sentences":["We have investigated the influence of the Coriolis force on the electrical conductivity of hadronic matter formed in relativistic nuclear collisions, employing the Hadron Resonance Gas (HRG) model.","A rotating matter in the peripheral heavy ion collisions can be expected from the initial stage of quark matter to late-stage hadronic matter.","Present work is focused on rotating hadronic matter, whose medium constituents - hadron resonances can face a non-zero Coriolis force, which can influence the hadronic flow or conductivity.","We estimate this conductivity tensor by using the relativistic Boltzmann transport equation.","In the absence of Coriolis force, an isotropic conductivity tensor for hadronic matter is expected.","However, our study finds that the presence of Coriolis force can generate an anisotropic conductivity tensor with three main conductivity components - parallel, perpendicular, and Hall, similar to the effect of Lorentz force at a finite magnetic field.","Our study has indicated that a noticeable anisotropy of conductivity tensor can be found within the phenomenological range of angular velocity $\\Omega= 0.001-0.02$ GeV and hadronic scattering radius $a=0.2-2$ fm."],"url":"http://arxiv.org/abs/2403.16647v1","category":"hep-ph"}
{"created":"2024-03-25 11:31:45","title":"Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations","abstract":"Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation. Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety. This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate. A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM). The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures. The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information. The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%). The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation.","sentences":["Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation.","Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety.","This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate.","A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM).","The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures.","The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information.","The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%).","The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation."],"url":"http://arxiv.org/abs/2403.16645v1","category":"cs.HC"}
{"created":"2024-03-25 11:28:52","title":"Multi-Scale Texture Loss for CT denoising with GANs","abstract":"Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimization. Our approach also introduces a self-attention layer that dynamically aggregates the multi-scale texture information extracted from the images. We validate our approach by carrying out extensive experiments in the context of low-dose CT denoising, a challenging application that aims to enhance the quality of noisy CT scans. We utilize three publicly available datasets, including one simulated and two real datasets. The results are promising as compared to other well-established loss functions, being also consistent across three different GAN architectures. The code is available at: https://github.com/FrancescoDiFeola/DenoTextureLoss","sentences":["Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging.","However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images.","In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image.","To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM).","Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training.","To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimization.","Our approach also introduces a self-attention layer that dynamically aggregates the multi-scale texture information extracted from the images.","We validate our approach by carrying out extensive experiments in the context of low-dose CT denoising, a challenging application that aims to enhance the quality of noisy CT scans.","We utilize three publicly available datasets, including one simulated and two real datasets.","The results are promising as compared to other well-established loss functions, being also consistent across three different GAN architectures.","The code is available at: https://github.com/FrancescoDiFeola/DenoTextureLoss"],"url":"http://arxiv.org/abs/2403.16640v1","category":"eess.IV"}
{"created":"2024-03-25 11:26:18","title":"AI-Generated Video Detection via Spatio-Temporal Anomaly Learning","abstract":"The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos. Malicious users can easily create non-existent videos to spread false information. This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN). Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively. Results of such sub-detectors are fused to further enhance the discrimination ability. A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation. Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme. Code and dataset will be available at https://github.com/multimediaFor/AIGVDet.","sentences":["The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos.","Malicious users can easily create non-existent videos to spread false information.","This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN).","Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively.","Results of such sub-detectors are fused to further enhance the discrimination ability.","A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation.","Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme.","Code and dataset will be available at https://github.com/multimediaFor/AIGVDet."],"url":"http://arxiv.org/abs/2403.16638v1","category":"cs.CV"}
{"created":"2024-03-25 11:24:23","title":"Detecting affine equivalences between rational or non-rational, meromorphic parametric curves in any dimension","abstract":"We generalize previous results by the authors to provide an algorithm for computing the affine equivalences between two parametric curves, either rational or with non-algebraic, meromorphic components under certain conditions, in any dimension. The algorithm completely avoids polynomial system solving, and uses bivariate factoring, instead, as a fundamental tool.","sentences":["We generalize previous results by the authors to provide an algorithm for computing the affine equivalences between two parametric curves, either rational or with non-algebraic, meromorphic components under certain conditions, in any dimension.","The algorithm completely avoids polynomial system solving, and uses bivariate factoring, instead, as a fundamental tool."],"url":"http://arxiv.org/abs/2403.16636v1","category":"math.AG"}
{"created":"2024-03-25 11:21:52","title":"SASHIMI-SIDM: Semi-analytical subhalo modelling for self-interacting dark matter at sub-galactic scales","abstract":"We combine the semi-analytical structure formation model, SASHIMI, which predicts subhalo populations in collisionless, cold dark matter (CDM), with a parametric model that maps CDM halos to self-interacting dark matter (SIDM) halos. The resulting model, SASHIMI-SIDM, generates SIDM subhalo populations down to sub-galactic mass scales, for an arbitrary input cross section, in minutes. We show that SASHIMI-SIDM agrees with SIDM subhalo populations from high-resolution cosmological zoom-in simulations in resolved regimes. Crucially, we predict that the fraction of core-collapsed subhalos peaks at a mass scale determined by the input SIDM cross section and decreases toward higher halo masses, consistent with the predictions of gravothermal models and cosmological simulations. For the first time, we also show that the core-collapsed fraction decreases toward lower halo masses; this result is uniquely enabled by our semi-analytical approach. As a proof of principle, we apply SASHIMI-SIDM to predict the boost to the local dark matter density and annihilation rate from core-collapsed SIDM subhalos, which can be enhanced relative to CDM by an order of magnitude for viable SIDM models. Thus, SASHIMI-SIDM provides an efficient and reliable tool for scanning SIDM parameter space and testing it with astrophysical observations. The code is publicly available at https://github.com/shinichiroando/sashimi-si.","sentences":["We combine the semi-analytical structure formation model, SASHIMI, which predicts subhalo populations in collisionless, cold dark matter (CDM), with a parametric model that maps CDM halos to self-interacting dark matter (SIDM) halos.","The resulting model, SASHIMI-SIDM, generates SIDM subhalo populations down to sub-galactic mass scales, for an arbitrary input cross section, in minutes.","We show that SASHIMI-SIDM agrees with SIDM subhalo populations from high-resolution cosmological zoom-in simulations in resolved regimes.","Crucially, we predict that the fraction of core-collapsed subhalos peaks at a mass scale determined by the input SIDM cross section and decreases toward higher halo masses, consistent with the predictions of gravothermal models and cosmological simulations.","For the first time, we also show that the core-collapsed fraction decreases toward lower halo masses; this result is uniquely enabled by our semi-analytical approach.","As a proof of principle, we apply SASHIMI-SIDM to predict the boost to the local dark matter density and annihilation rate from core-collapsed SIDM subhalos, which can be enhanced relative to CDM by an order of magnitude for viable SIDM models.","Thus, SASHIMI-SIDM provides an efficient and reliable tool for scanning SIDM parameter space and testing it with astrophysical observations.","The code is publicly available at https://github.com/shinichiroando/sashimi-si."],"url":"http://arxiv.org/abs/2403.16633v1","category":"astro-ph.CO"}
{"created":"2024-03-25 11:21:09","title":"Endogenous Fragility in Opaque Supply Chains","abstract":"This paper investigates the role of supply chain unobservability in generating endogenously fragile production networks. In a simple production game, in which firms need to multisource to hedge against suppliers' risk under unobservability, firms underdiversify vis-a-vis the social optimum. The unobservability of suppliers' relations is the driver behind this. In production networks where upstream risk is highly correlated and supplier relationships are not observable, the marginal risk reduction of adding an additional supplier is low, because this additional supplier's risk is likely to be correlated to that of existing suppliers. This channel reduces firm incentives to diversify, which gives rise to inefficiently fragile production networks. By solving the social planner problem, I show that, if the risk reduction experienced downstream resulting from upstream diversification were to be internalised by upstream firms, endogenous production networks would be resilient to most levels of risk. Furthermore, I show that the opaqueness of the supply chain yields less fragile but more inefficient production networks. Despite its stylised form,the model identifies the trade-off firms face when diversifying risk and isolates the mechanism that aggregates these decisions into a production network. Furthermore, it maps the conditions of the trade-off, such as expected profits of the firm or the pairing costs, to the properties of the production network.","sentences":["This paper investigates the role of supply chain unobservability in generating endogenously fragile production networks.","In a simple production game, in which firms need to multisource to hedge against suppliers' risk under unobservability, firms underdiversify vis-a-vis the social optimum.","The unobservability of suppliers' relations is the driver behind this.","In production networks where upstream risk is highly correlated and supplier relationships are not observable, the marginal risk reduction of adding an additional supplier is low, because this additional supplier's risk is likely to be correlated to that of existing suppliers.","This channel reduces firm incentives to diversify, which gives rise to inefficiently fragile production networks.","By solving the social planner problem, I show that, if the risk reduction experienced downstream resulting from upstream diversification were to be internalised by upstream firms, endogenous production networks would be resilient to most levels of risk.","Furthermore, I show that the opaqueness of the supply chain yields less fragile but more inefficient production networks.","Despite its stylised form,the model identifies the trade-off firms face when diversifying risk and isolates the mechanism that aggregates these decisions into a production network.","Furthermore, it maps the conditions of the trade-off, such as expected profits of the firm or the pairing costs, to the properties of the production network."],"url":"http://arxiv.org/abs/2403.16632v1","category":"econ.TH"}
{"created":"2024-03-25 11:16:23","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","abstract":"Recent advancements in diffusion models have positioned them at the forefront of image generation. Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process. To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency. Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation. We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively. Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation.","sentences":["Recent advancements in diffusion models have positioned them at the forefront of image generation.","Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process.","To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency.","Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation.","We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively.","Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation."],"url":"http://arxiv.org/abs/2403.16627v1","category":"cs.CV"}
{"created":"2024-03-25 11:15:02","title":"Presenting Interval Pomsets with Interfaces","abstract":"Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account. In this paper, we develop a presentation of ipomsets as generated by a graph of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators. Using this presentation, we show that also subsumptions are generated by elementary relations. We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages.","sentences":["Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account.","In this paper, we develop a presentation of ipomsets as generated by a graph of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators.","Using this presentation, we show that also subsumptions are generated by elementary relations.","We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages."],"url":"http://arxiv.org/abs/2403.16626v1","category":"cs.FL"}
{"created":"2024-03-25 11:10:17","title":"Stable solution and extremal solution for the fractional $p$-Laplacian equation","abstract":"To our knowledge, this paper is the first attempt to consider the stable solution and extremal solution for the fractional $p$-Laplacian equation: $(-\\Delta)_p^s u= \\lambda f(u),\\; u> 0 ~\\text{in}~\\Omega;\\; u=0\\;\\text{in}~ \\mathbb{R}^N\\setminus\\Omega$, where $p>1$, $s\\in (0,1)$, $N>sp$, $\\lambda>0$ and $\\Omega$ is a bounded domain with continuous boundary. We first construct the notion of stable solution, and then we prove that when $f$ is of class $C^1$, nondecreasing and such that $f(0)>0$ and $\\underset{t\\to \\infty}{\\lim}\\frac{f(t)}{t^{p-1}}=\\infty$, there exists an extremal parameter $\\lambda^*\\in (0, \\infty)$ such that a bounded minimal solution $u_\\lambda$ exists if $\\lambda\\in (0, \\lambda^*)$, and no bounded solution exists if $\\lambda>\\lambda^*$, no $W_0^{s,p}(\\Omega)$ solution exists if in addition $f(t)^{\\frac{1}{p-1}}$ is convex. Moreover, this family of minimal solutions are stable, and nondecreasing in $\\lambda$, therefore the extremal function $u^*:=\\underset{\\lambda\\to\\lambda^*}{\\lim}u_\\lambda$ exists.   For the regularity of the extremal function, we first show the $L^r$-estimates for the equation $(-\\Delta)_p^su=g$ with $g\\in W_0^{s, p}(\\Omega)^*\\cap L^q(\\Omega)$, $q\\ge 1$. When $f$ is a power-like nonlinearity, we derive the $W_0^{s,p}(\\Omega)$ regularity of $u^*$ in all dimension and $L^{\\infty}(\\Omega)$ regularity of $u^*$ in some low dimensions. For more general nonlinearities, when $f$ is class of $C^2$ and such that some convexity assumptions, then $u^*\\in W_0^{s,p}(\\Omega)$ if $N<sp(1+\\frac{p}{p-1})$ and $u^*\\in L^{\\infty}(\\Omega)$ if $N<\\frac{sp^2}{p-1}$. Furthermore, when the limit $\\tau=\\underset{t\\to\\infty}{\\lim}\\frac{f(t)f''(t)}{f'(t)^2}$ exists and $\\tau>\\frac{p-2}{p-1}$, the results above can be improved as: $u^*\\in W_0^{s,p}(\\Omega)$ for all dimensions, and $u^*\\in L^{\\infty}(\\Omega)$ if $N<sp+\\frac{4sp}{p-1}$.","sentences":["To our knowledge, this paper is the first attempt to consider the stable solution and extremal solution for the fractional $p$-Laplacian equation: $(-\\Delta)_p^s u= \\lambda f(u),\\; u> 0 ~\\text{in}~\\Omega;\\; u=0\\;\\text{in}~ \\mathbb{R}^N\\setminus\\Omega$, where $p>1$, $s\\in (0,1)$, $N>sp$, $\\lambda>0$ and $\\Omega$ is a bounded domain with continuous boundary.","We first construct the notion of stable solution, and then we prove that when $f$ is of class $C^1$, nondecreasing and such that $f(0)>0$ and $\\underset{t\\to \\infty}{\\lim}\\frac{f(t)}{t^{p-1}}=\\infty$, there exists an extremal parameter $\\lambda^*\\in (0, \\infty)$ such that a bounded minimal solution $u_\\lambda$ exists if $\\lambda\\in (0, \\lambda^*)$, and no bounded solution exists if $\\lambda>\\lambda^*$, no $W_0^{s,p}(\\Omega)$ solution exists if in addition $f(t)^{\\frac{1}{p-1}}$ is convex.","Moreover, this family of minimal solutions are stable, and nondecreasing in $\\lambda$, therefore the extremal function $u^*:=\\underset{\\lambda\\to\\lambda^*}{\\lim}u_\\lambda$ exists.   ","For the regularity of the extremal function, we first show the $L^r$-estimates for the equation $(-\\Delta)_p^su=g$ with $g\\in W_0^{s, p}(\\Omega)^*\\cap L^q(\\Omega)$, $q\\ge 1$.","When $f$ is a power-like nonlinearity, we derive the $W_0^{s,p}(\\Omega)$ regularity of $u^*$ in all dimension and $L^{\\infty}(\\Omega)$ regularity of $u^*$ in some low dimensions.","For more general nonlinearities, when $f$ is class of $C^2$ and such that some convexity assumptions, then $u^*\\in W_0^{s,p}(\\Omega)$ if $N<sp(1+\\frac{p}{p-1})$ and $u^*\\in L^{\\infty}(\\Omega)$ if $N<\\frac{sp^2}{p-1}$. Furthermore, when the limit $\\tau=\\underset{t\\to\\infty}{\\lim}\\frac{f(t)f''(t)}{f'(t)^2}$ exists and $\\tau>\\frac{p-2}{p-1}$, the results above can be improved as: $u^*\\in W_0^{s,p}(\\Omega)$ for all dimensions, and $u^*\\in L^{\\infty}(\\Omega)$ if $N<sp+\\frac{4sp}{p-1}$."],"url":"http://arxiv.org/abs/2403.16624v1","category":"math.AP"}
{"created":"2024-03-25 10:30:22","title":"SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation","abstract":"In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery. Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts. In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks. The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models. To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation. We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity. Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency. We employ the novel data instances for downstream segmentation, as a form of data augmentation. In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs. We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data.","sentences":["In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery.","Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts.","In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks.","The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models.","To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation.","We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity.","Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency.","We employ the novel data instances for downstream segmentation, as a form of data augmentation.","In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs.","We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data."],"url":"http://arxiv.org/abs/2403.16605v1","category":"cs.CV"}
{"created":"2024-03-25 10:27:42","title":"Finite size effects and optimization of the calculation of the surface tension in surfactant mixtures at liquid/vapour interfaces","abstract":"The surface tension of monolayers with mixtures of anionic and nonionic surfactant at the liquid/vapour interface is studied. Previous works have observed that calculations of the surface tension of simple fluids show artificial oscillations for small interfacial areas, indicating that the surface tension data fluctuate due to the finite size effects and periodic boundary conditions. In the case of simulations of monolayers composed of surfactant mixtures, the surface tension not only oscillates for small areas but can also give non-physical data, such as negative values. Analysis of the monolayers with different surfactant mixtures, ionic (DTAB, CTAB, SDS) and nonionic (SB3-12), was done for density profiles, parameters of order and pair correlation functions for small and large box areas and all of them present similar behaviour. The fluctuations and the non-physical values of the surface tension are corrected when boxes with large interfacial areas are considered. The results indicate that in order to obtain reliable values of the surface tension, in computer simulations, it is important to choose not only the correct force field but also the appropriate size of the simulation box.","sentences":["The surface tension of monolayers with mixtures of anionic and nonionic surfactant at the liquid/vapour interface is studied.","Previous works have observed that calculations of the surface tension of simple fluids show artificial oscillations for small interfacial areas, indicating that the surface tension data fluctuate due to the finite size effects and periodic boundary conditions.","In the case of simulations of monolayers composed of surfactant mixtures, the surface tension not only oscillates for small areas but can also give non-physical data, such as negative values.","Analysis of the monolayers with different surfactant mixtures, ionic (DTAB, CTAB, SDS) and nonionic (SB3-12), was done for density profiles, parameters of order and pair correlation functions for small and large box areas and all of them present similar behaviour.","The fluctuations and the non-physical values of the surface tension are corrected when boxes with large interfacial areas are considered.","The results indicate that in order to obtain reliable values of the surface tension, in computer simulations, it is important to choose not only the correct force field but also the appropriate size of the simulation box."],"url":"http://arxiv.org/abs/2403.16604v1","category":"cond-mat.soft"}
{"created":"2024-03-25 10:22:28","title":"On the $\\mathbb{Z}_p$-extensions of a totally $p$-adic imaginary quadratic field -- With an appendix by Jean-Fran\u00e7ois Jaulent","abstract":"Let $k$ be an imaginary quadratic field, and let $p$ be an odd prime number split in $k$. We analyze some properties of arbitrary $\\mathbb{Z}_p$-extensions K/k. These properties are governed by the Hase norm residue symbol of the fundamental $p$-unit of $k$, in terms of the valuation $\\delta_p(k)$ of a Fermat quotient, which determines the order of the logarithmic class group $\\widetilde{\\mathcal{H}_k}$ (Theorem 2.2) and leads, under some conditions, to generalizations of Gold's criterion characterizing $\\lambda_p(K/k) = 1$ (Theorems 3.3, 5.1, 5.3). This uses the higher rank Chevalley-Herbrand formulas, for the filtrations of the p-class groups in $K$, that we gave in the 1994's, and the theorem of $\\lambda$-stability (2022). This study is in connection with articles of Gold, Sands, Dummit-Ford-Kisilevsky-Sands, Ozaki, Jaulent, Fujii. In Appendix A, is given a general proof, by Jaulent, of the link between $\\widetilde{\\mathcal{H}_k}$ and $\\delta_p(k)$ in a broader context. Numerical illustrations (with pari/gp programs) are given.","sentences":["Let $k$ be an imaginary quadratic field, and let $p$ be an odd prime number split in $k$. We analyze some properties of arbitrary $\\mathbb{Z}_p$-extensions K/k. These properties are governed by the Hase norm residue symbol of the fundamental $p$-unit of $k$, in terms of the valuation $\\delta_p(k)$ of a Fermat quotient, which determines the order of the logarithmic class group $\\widetilde{\\mathcal{H}_k}$ (Theorem 2.2) and leads, under some conditions, to generalizations of Gold's criterion characterizing $\\lambda_p(K/k) = 1$ (Theorems 3.3, 5.1, 5.3).","This uses the higher rank Chevalley-Herbrand formulas, for the filtrations of the p-class groups in $K$, that we gave in the 1994's, and the theorem of $\\lambda$-stability (2022).","This study is in connection with articles of Gold, Sands, Dummit-Ford-Kisilevsky-Sands, Ozaki, Jaulent, Fujii.","In Appendix A, is given a general proof, by Jaulent, of the link between $\\widetilde{\\mathcal{H}_k}$ and $\\delta_p(k)$ in a broader context.","Numerical illustrations (with pari/gp programs) are given."],"url":"http://arxiv.org/abs/2403.16603v1","category":"math.NT"}
{"created":"2024-03-25 10:19:56","title":"Superadiabatic dynamical density functional theory for colloidal suspensions under homogeneous steady-shear","abstract":"The superadiabatic dynamical density functional theory (superadiabatic-DDFT) is a promising new method for the study of colloidal systems out-of-equilibrium. Within this approach the viscous forces arising from interparticle interactions are accounted for in a natural way by treating explicitly the dynamics of the two-body correlations. For bulk systems subject to spatially homogeneous shear we use the superadiabatic-DDFT framework to calculate the steady-state pair distribution function and the corresponding viscosity for low values of the shear-rate. We then consider a variant of the central approximation underlying this superadiabatic theory and obtain an inhomogeneous generalization of a rheological bulk theory due to Russel and Gast. This paper thus establishes for the first time a connection between DDFT approaches, formulated to treat inhomogeneous systems, and existing work addressing nonequilibrium microstructure and rheology in bulk colloidal suspensions.","sentences":["The superadiabatic dynamical density functional theory (superadiabatic-DDFT) is a promising new method for the study of colloidal systems out-of-equilibrium.","Within this approach the viscous forces arising from interparticle interactions are accounted for in a natural way by treating explicitly the dynamics of the two-body correlations.","For bulk systems subject to spatially homogeneous shear we use the superadiabatic-DDFT framework to calculate the steady-state pair distribution function and the corresponding viscosity for low values of the shear-rate.","We then consider a variant of the central approximation underlying this superadiabatic theory and obtain an inhomogeneous generalization of a rheological bulk theory due to Russel and Gast.","This paper thus establishes for the first time a connection between DDFT approaches, formulated to treat inhomogeneous systems, and existing work addressing nonequilibrium microstructure and rheology in bulk colloidal suspensions."],"url":"http://arxiv.org/abs/2403.16599v1","category":"cond-mat.soft"}
{"created":"2024-03-25 10:17:11","title":"GNS-construction for positive C*-valued sesquilinear maps on a quasi *-algebra","abstract":"The GNS construction for positive invariant sesquilinear forms on quasi *-algebras is generalized to a class of positive C*-valued sesquilinear maps on quasi *-algebras. The result is a *-representation taking values in a space of operators acting on a certain quasi-normed C-module.","sentences":["The GNS construction for positive invariant sesquilinear forms on quasi *-algebras is generalized to a class of positive C*-valued sesquilinear maps on quasi *-algebras.","The result is a *-representation taking values in a space of operators acting on a certain quasi-normed C-module."],"url":"http://arxiv.org/abs/2403.16597v1","category":"math.OA"}
{"created":"2024-03-25 10:09:03","title":"TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques","abstract":"The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries. However, this capability has raised concerns regarding misinformation and personal information leakage. In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts. Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches. We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods. Our methods obtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies.","sentences":["The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries.","However, this capability has raised concerns regarding misinformation and personal information leakage.","In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts.","Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches.","We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods.","Our methods obtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies."],"url":"http://arxiv.org/abs/2403.16592v1","category":"cs.CL"}
{"created":"2024-03-25 10:06:45","title":"Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy","abstract":"The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP established under uniform prior distribution. These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\\xi$-LDP also confers $\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions.","sentences":["The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP).","Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge.","In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs.","We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness.","Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP established under uniform prior distribution.","These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\\xi$-LDP also confers $\\xi$-MBP, and vice versa.","Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions."],"url":"http://arxiv.org/abs/2403.16591v1","category":"cs.LG"}
{"created":"2024-03-25 10:03:46","title":"Extremal properties of max-autoregressive moving average processes for modelling extreme river flows","abstract":"Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models. River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay. Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows. This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes. We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process. We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold. We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable. We illustrate results for river flow data from the UK River Thames.","sentences":["Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models.","River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay.","Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows.","This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes.","We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process.","We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold.","We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable.","We illustrate results for river flow data from the UK River Thames."],"url":"http://arxiv.org/abs/2403.16590v1","category":"stat.ME"}
{"created":"2024-03-25 09:56:59","title":"Intrinsic Dipole Hall effect in tMoTe$_2$ moir\u00e9: magnetoelectricity and contact-free signature of topological transitions","abstract":"We discover an intrinsic dipole Hall effect in a variety of magnetic insulating states at integer fillings of twisted MoTe$_2$ moir\\'e superlattice, including topologically trivial and nontrivial ferro-, antiferro-, and ferri-magnetic configurations. The dipole Hall current, in linear response to in-plane electric field, generates an in-plane orbital magnetization $M_{\\parallel}$ along the field, through which an AC field can drive magnetization oscillation up to THz range. Upon the continuous topological phase transitions from trivial to quantum anomalous Hall states in both ferromagnetic and antiferromagnetic configurations, the dipole Hall current and $M_{\\parallel}$ have an abrupt sign change, enabling contact free detection of the transitions through the magnetic stray field. In configurations where the linear response is forbidden by symmetry, the dipole Hall current and $M_{\\parallel}$ appear as a crossed nonlinear response to both in-plane and out-of-plane electric fields. These magnetoelectric phenomena showcase novel functionalities of insulators from the interplay between magnetism, topology and electrical polarization.","sentences":["We discover an intrinsic dipole Hall effect in a variety of magnetic insulating states at integer fillings of twisted MoTe$_2$ moir\\'e superlattice, including topologically trivial and nontrivial ferro-, antiferro-, and ferri-magnetic configurations.","The dipole Hall current, in linear response to in-plane electric field, generates an in-plane orbital magnetization $M_{\\parallel}$ along the field, through which an AC field can drive magnetization oscillation up to THz range.","Upon the continuous topological phase transitions from trivial to quantum anomalous Hall states in both ferromagnetic and antiferromagnetic configurations, the dipole Hall current and $M_{\\parallel}$ have an abrupt sign change, enabling contact free detection of the transitions through the magnetic stray field.","In configurations where the linear response is forbidden by symmetry, the dipole Hall current and $M_{\\parallel}$ appear as a crossed nonlinear response to both in-plane and out-of-plane electric fields.","These magnetoelectric phenomena showcase novel functionalities of insulators from the interplay between magnetism, topology and electrical polarization."],"url":"http://arxiv.org/abs/2403.16586v1","category":"cond-mat.str-el"}
{"created":"2024-03-25 09:50:37","title":"Amino Acids and Their Biological Derivatives Modulate Protein-Protein Interactions In an Additive Way","abstract":"Protein-protein interactions (PPI) differ when measured in test tubes and cells due to the complexity of the intracellular environment. Free amino acids (AAs) and their derivatives constitute a significant fraction of the intracellular volume and mass. Recently, we have found that AAs have a general property of rendering protein dispersions more stable by reducing the net attractive part of PPI. Here, we study the effects on PPI of different AA derivatives, AA mixtures, and short peptides. We find that all the tested AA derivatives modulate PPI in solution as well as AAs. Furthermore, we show that the modulation effect is additive when AAs form mixtures or are bound into short peptides. Therefore, this study demonstrates the universal effect of a class of small molecules (i.e. AAs and their biological derivatives) on the modulation of PPI and provides insights into rationally designing biocompatible molecules for stabilizing protein interactions and consequently tuning protein functions.","sentences":["Protein-protein interactions (PPI) differ when measured in test tubes and cells due to the complexity of the intracellular environment.","Free amino acids (AAs) and their derivatives constitute a significant fraction of the intracellular volume and mass.","Recently, we have found that AAs have a general property of rendering protein dispersions more stable by reducing the net attractive part of PPI.","Here, we study the effects on PPI of different AA derivatives, AA mixtures, and short peptides.","We find that all the tested AA derivatives modulate PPI in solution as well as AAs.","Furthermore, we show that the modulation effect is additive when AAs form mixtures or are bound into short peptides.","Therefore, this study demonstrates the universal effect of a class of small molecules (i.e. AAs and their biological derivatives) on the modulation of PPI and provides insights into rationally designing biocompatible molecules for stabilizing protein interactions and consequently tuning protein functions."],"url":"http://arxiv.org/abs/2403.16583v1","category":"physics.bio-ph"}
{"created":"2024-03-25 09:49:42","title":"In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data","abstract":"Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures (LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data. We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases. Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought. To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task. We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach.","sentences":["Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration.","When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary.","Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction.","However, they face substantial challenges when dealing with multiple input patterns.","The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions.","In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications.","We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures (LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations.","The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data.","We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases.","Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought.","To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task.","We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach."],"url":"http://arxiv.org/abs/2403.16582v1","category":"cs.LG"}
{"created":"2024-03-25 09:46:47","title":"Experimental demonstration of a thermal-EM concentrator for enhancing EM signals and converging heat fluxes simultaneously","abstract":"Simultaneously concentrating EM waves and heat fluxes to the same target region within an on-chip system carries substantial academic research importance and practical application value. Nevertheless, existing researches are primarily aimed at the design and experimentation of concentrators for individual EM waves or temperature fields. In this work, a thermal-EM concentrator, capable of simultaneously concentrating EM waves and heat fluxes, is designed using transformation optics/thermodynamics and fabricated with engineered EM-thermal metamaterials. The concentrating effects of the proposed thermal-EM concentrator on the thermal fluxes and EM waves are verified through numerical simulations and experimental measurements, respectively, which are in good agreement with each other. Both numerically simulated and experimentally measured results demonstrate the concentrating capability of the proposed thermal-EM concentrator, which can concentrate broadband TM-polarized EM waves ranging from 8-12 GHz and heat/cold flows to the same target region within an on-chip operating environment. The thermal-EM concentrator exhibits a thermal focusing efficiency close to 100% and more than three times enhancement of the magnetic field at the designed center frequency of 10 GHz. The proposed thermal-EM concentrator can be utilized for efficient cooling for the specified component and simultaneously enhancing the EM antenna's radiation/reception efficiency within an on-chip system.","sentences":["Simultaneously concentrating EM waves and heat fluxes to the same target region within an on-chip system carries substantial academic research importance and practical application value.","Nevertheless, existing researches are primarily aimed at the design and experimentation of concentrators for individual EM waves or temperature fields.","In this work, a thermal-EM concentrator, capable of simultaneously concentrating EM waves and heat fluxes, is designed using transformation optics/thermodynamics and fabricated with engineered EM-thermal metamaterials.","The concentrating effects of the proposed thermal-EM concentrator on the thermal fluxes and EM waves are verified through numerical simulations and experimental measurements, respectively, which are in good agreement with each other.","Both numerically simulated and experimentally measured results demonstrate the concentrating capability of the proposed thermal-EM concentrator, which can concentrate broadband TM-polarized EM waves ranging from 8-12 GHz and heat/cold flows to the same target region within an on-chip operating environment.","The thermal-EM concentrator exhibits a thermal focusing efficiency close to 100% and more than three times enhancement of the magnetic field at the designed center frequency of 10 GHz.","The proposed thermal-EM concentrator can be utilized for efficient cooling for the specified component and simultaneously enhancing the EM antenna's radiation/reception efficiency within an on-chip system."],"url":"http://arxiv.org/abs/2403.16579v1","category":"physics.gen-ph"}
{"created":"2024-03-25 09:43:56","title":"SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging","abstract":"Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks. This indicates that SegICL effectively address new segmentation tasks based on contextual information. Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks. Our code will be released soon.","sentences":["Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement.","Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance.","For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation.","Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset).","Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks.","This indicates that SegICL effectively address new segmentation tasks based on contextual information.","Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks.","Our code will be released soon."],"url":"http://arxiv.org/abs/2403.16578v1","category":"cs.CV"}
{"created":"2024-03-25 09:43:26","title":"Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems","abstract":"Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things. The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values. Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values. This highly degrades the physical properties of these applications while does not improve their functionality. To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience. The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values. This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block. The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation. The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a face recognition neural network are demonstrated by means of a wide range of simulation and synthesis results.","sentences":["Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things.","The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values.","Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values.","This highly degrades the physical properties of these applications while does not improve their functionality.","To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience.","The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values.","This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block.","The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation.","The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a face recognition neural network are demonstrated by means of a wide range of simulation and synthesis results."],"url":"http://arxiv.org/abs/2403.16577v1","category":"cs.AR"}
{"created":"2024-03-25 09:41:49","title":"Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization","abstract":"Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity, demonstrating the superiority of our approach.","sentences":["Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature.","In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality.","Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens.","Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference.","Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion.","Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity, demonstrating the superiority of our approach."],"url":"http://arxiv.org/abs/2403.16576v1","category":"q-bio.BM"}
{"created":"2024-03-25 09:40:21","title":"Rotating metrics and new multipole moments from scattering amplitudes in arbitrary dimensions","abstract":"We compute the vacuum metric generated by a generic rotating object in arbitrary dimensions up to third post-Minkowskian order by computing the classical contribution of scattering amplitudes describing the graviton emission by massive spin-1 particles up to two loops. The solution depends on the mass, angular momenta, and on up to two parameters related to generic quadrupole moments. In $D=4$ spacetime dimensions, we recover the vacuum Hartle-Thorne solution describing a generic spinning object to second order in the angular momentum, of which the Kerr metric is a particular case obtained for a specific mass quadrupole moment dictated by the uniqueness theorem. At the level of the effective action, the case of minimal couplings corresponds to the Kerr black hole, while any other mass quadrupole moment requires non-minimal couplings. In $D>4$, the absence of black-hole uniqueness theorems implies that there are multiple spinning black hole solutions with different topology. Using scattering amplitudes, we find a generic solution depending on the mass, angular momenta, the mass quadrupole moment, and a new stress quadrupole moment which does not exist in $D=4$. As special cases, we recover the Myers-Perry and the single-angular-momentum black ring solutions, to third and first post-Minkowksian order, respectively. Interestingly, at variance with the four dimensional case, none of these solutions corresponds to the minimal coupling in the effective action. This shows that, from the point of view of scattering amplitudes, black holes are the \"simplest\" General Relativity vacuum solutions only in $D=4$.","sentences":["We compute the vacuum metric generated by a generic rotating object in arbitrary dimensions up to third post-Minkowskian order by computing the classical contribution of scattering amplitudes describing the graviton emission by massive spin-1 particles up to two loops.","The solution depends on the mass, angular momenta, and on up to two parameters related to generic quadrupole moments.","In $D=4$ spacetime dimensions, we recover the vacuum Hartle-Thorne solution describing a generic spinning object to second order in the angular momentum, of which the Kerr metric is a particular case obtained for a specific mass quadrupole moment dictated by the uniqueness theorem.","At the level of the effective action, the case of minimal couplings corresponds to the Kerr black hole, while any other mass quadrupole moment requires non-minimal couplings.","In $D>4$, the absence of black-hole uniqueness theorems implies that there are multiple spinning black hole solutions with different topology.","Using scattering amplitudes, we find a generic solution depending on the mass, angular momenta, the mass quadrupole moment, and a new stress quadrupole moment which does not exist in $D=4$. As special cases, we recover the Myers-Perry and the single-angular-momentum black ring solutions, to third and first post-Minkowksian order, respectively.","Interestingly, at variance with the four dimensional case, none of these solutions corresponds to the minimal coupling in the effective action.","This shows that, from the point of view of scattering amplitudes, black holes are the \"simplest\" General Relativity vacuum solutions only in $D=4$."],"url":"http://arxiv.org/abs/2403.16574v1","category":"hep-th"}
{"created":"2024-03-25 09:36:51","title":"NSINA: A News Corpus for Sinhala","abstract":"The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.","sentences":["The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources.","This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets.","In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation.","The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language.","NSINA is the largest news corpus for Sinhala, available up to date."],"url":"http://arxiv.org/abs/2403.16571v1","category":"cs.CL"}
{"created":"2024-03-25 09:36:20","title":"Spectropolarimetry of Fraunhofer lines in local upper solar atmosphere","abstract":"Spectropolarimetric results of Fraunhofer lines between 516.3nm and 532.6nm are presented in local upper solar chromosphere, transition zone and inner corona below a height of about 0.04 solar radius above the solar limb. The data were acquired on Nov.3, 2013 during a total solar eclipse in Gabon by the prototype Fiber Arrayed Solar Optical Telescope(FASOT). It is found that the polarization amplitudes of the Fraunhofer lines in these layers depend strongly on specific spectral lines. Fraunhofer line at MgI$b_{1}$518.4nm can have a polarization amplitude up to 0.36$\\%$ with respective to the continuum polarization level, while the polarizations of some lines like FeI/CrI524.7nm and FeI525.0nm are often under the detection limit 6.0$\\times 10^{-4}$. The polarizations of the Fraunhofer lines, like the emission lines and the continuum, increase with height as a whole trend. The fractional linear polarization amplitudes of inner F-corona can be close to those of inner E-corona, and in general larger than those of inner K-corona. Rotation of the polarization direction of Fraunhofer line is often accompanied with variations in their polarization amplitudes and profile shapes. It is also judged from these polarimetric properties, along with evidences, that neutral atoms exist in these atmospheric layers. Thus the inner F-corona described here is induced by the neutral atoms, and the entropy of the inner corona evaluated becomes larger than those in the underneath layers due to more microstates found.","sentences":["Spectropolarimetric results of Fraunhofer lines between 516.3nm and 532.6nm are presented in local upper solar chromosphere, transition zone and inner corona below a height of about 0.04 solar radius above the solar limb.","The data were acquired on Nov.3, 2013 during a total solar eclipse in Gabon by the prototype Fiber Arrayed Solar Optical Telescope(FASOT).","It is found that the polarization amplitudes of the Fraunhofer lines in these layers depend strongly on specific spectral lines.","Fraunhofer line at MgI$b_{1}$518.4nm can have a polarization amplitude up to 0.36$\\%$ with respective to the continuum polarization level, while the polarizations of some lines like FeI/CrI524.7nm and FeI525.0nm are often under the detection limit 6.0$\\times 10^{-4}$.","The polarizations of the Fraunhofer lines, like the emission lines and the continuum, increase with height as a whole trend.","The fractional linear polarization amplitudes of inner F-corona can be close to those of inner E-corona, and in general larger than those of inner K-corona.","Rotation of the polarization direction of Fraunhofer line is often accompanied with variations in their polarization amplitudes and profile shapes.","It is also judged from these polarimetric properties, along with evidences, that neutral atoms exist in these atmospheric layers.","Thus the inner F-corona described here is induced by the neutral atoms, and the entropy of the inner corona evaluated becomes larger than those in the underneath layers due to more microstates found."],"url":"http://arxiv.org/abs/2403.16570v1","category":"astro-ph.SR"}
{"created":"2024-03-25 09:36:10","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors","abstract":"Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks.","sentences":["Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks.","Nonetheless, these techniques could potentially generate misleading explanations.","Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy.","It poses a serious challenge in ensuring the reliability of XAI methods.","To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks.","We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training.","The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks."],"url":"http://arxiv.org/abs/2403.16569v1","category":"cs.LG"}
{"created":"2024-03-25 09:32:29","title":"Decoupling parameter variation from noise: Biquadratic Lyapunov forms in data-driven LPV control","abstract":"A promising step from linear towards nonlinear data-driven control is via the design of controllers for linear parameter-varying (LPV) systems, which are linear systems whose parameters are varying along a measurable scheduling signal. However, the interplay between uncertainty arising from corrupted data and the parameter-varying nature of these systems impacts the stability analysis, and limits the generalization of well-understood data-driven methods for linear time-invariant systems. In this work, we decouple this interplay using a recently developed variant of the Fundamental Lemma for LPV systems and the viewpoint of data-informativity, in combination with biquadratic Lyapunov forms. Together, these allow us to develop novel linear matrix inequality conditions for the existence of scheduling-dependent Lyapunov functions, incorporating the intrinsic nonlinearity. Appealingly, these results are stated purely in terms of the collected data and bounds on the noise, and they are computationally favorable to check.","sentences":["A promising step from linear towards nonlinear data-driven control is via the design of controllers for linear parameter-varying (LPV) systems, which are linear systems whose parameters are varying along a measurable scheduling signal.","However, the interplay between uncertainty arising from corrupted data and the parameter-varying nature of these systems impacts the stability analysis, and limits the generalization of well-understood data-driven methods for linear time-invariant systems.","In this work, we decouple this interplay using a recently developed variant of the Fundamental Lemma for LPV systems and the viewpoint of data-informativity, in combination with biquadratic Lyapunov forms.","Together, these allow us to develop novel linear matrix inequality conditions for the existence of scheduling-dependent Lyapunov functions, incorporating the intrinsic nonlinearity.","Appealingly, these results are stated purely in terms of the collected data and bounds on the noise, and they are computationally favorable to check."],"url":"http://arxiv.org/abs/2403.16565v1","category":"eess.SY"}
{"created":"2024-03-25 09:32:09","title":"Molecular Communication-Based Intelligent Dopamine Rate Modulator for Parkinson's Disease Treatment","abstract":"Parkinson's disease (PD) is a progressive neurodegenerative disease, and it is caused by the loss of dopaminergic neurons in the basal ganglia (BG). Currently, there is no definite cure for PD, and available treatments mainly aim to alleviate its symptoms. Due to impaired neurotransmitter-based information transmission in PD, molecular communication-based approaches can be employed as potential solutions to address this issue. Molecular Communications (MC) is a bio-inspired communication method utilizing molecules for carrying information. This mode of communication stands out for developing bio-compatible nanomachines for diagnosing and treating, particularly in addressing neurodegenerative diseases like PD, due to its compatibility with biological systems. This study presents a novel treatment method that introduces an Intelligent Dopamine Rate Modulator (IDRM), which is located in the synaptic gap between the substantia nigra pars compacta (SNc) and striatum to compensate for insufficiency dopamine release in BG caused by PD. For storing dopamine in the IDRM, dopamine compound (DAC) is swallowed and crossed through the digestive system, blood circulatory system, blood-brain barrier (BBB), and brain extracellular matrix uptakes with IDRMs. Here, the DAC concentration is calculated in these regions, revealing that the required exogenous dopamine consistently reaches IDRM. Therefore, the perpetual dopamine insufficiency in BG associated with PD can be compensated. This method reduces drug side effects because dopamine is not released in other brain regions. Unlike other treatments, this approach targets the root cause of PD rather than just reducing symptoms.","sentences":["Parkinson's disease (PD) is a progressive neurodegenerative disease, and it is caused by the loss of dopaminergic neurons in the basal ganglia (BG).","Currently, there is no definite cure for PD, and available treatments mainly aim to alleviate its symptoms.","Due to impaired neurotransmitter-based information transmission in PD, molecular communication-based approaches can be employed as potential solutions to address this issue.","Molecular Communications (MC) is a bio-inspired communication method utilizing molecules for carrying information.","This mode of communication stands out for developing bio-compatible nanomachines for diagnosing and treating, particularly in addressing neurodegenerative diseases like PD, due to its compatibility with biological systems.","This study presents a novel treatment method that introduces an Intelligent Dopamine Rate Modulator (IDRM), which is located in the synaptic gap between the substantia nigra pars compacta (SNc) and striatum to compensate for insufficiency dopamine release in BG caused by PD.","For storing dopamine in the IDRM, dopamine compound (DAC) is swallowed and crossed through the digestive system, blood circulatory system, blood-brain barrier (BBB), and brain extracellular matrix uptakes with IDRMs.","Here, the DAC concentration is calculated in these regions, revealing that the required exogenous dopamine consistently reaches IDRM.","Therefore, the perpetual dopamine insufficiency in BG associated with PD can be compensated.","This method reduces drug side effects because dopamine is not released in other brain regions.","Unlike other treatments, this approach targets the root cause of PD rather than just reducing symptoms."],"url":"http://arxiv.org/abs/2403.16564v1","category":"cs.ET"}
{"created":"2024-03-25 09:24:34","title":"Phantom scalar field cosmologies constrained by early cosmic measurements","abstract":"In this work, we explore new constraints on phantom scalar field cosmologies with a scalar field employing early times catalogues related to CMB measurements, along with the local standard observables, like Supernovae Type Ia (SNIa), $H(z)$ measurements (Cosmic Clocks), and Baryon Acoustic Oscillations (BAO) baselines. In particular, we studied a tracker phantom field with hyperbolic polar coordinates that have been proposed in the literature. The main goal is to obtain precise cosmological constraints for $H_0$ and $\\sigma_8$, in comparison to other constructions that present tension in early cosmological parameters. Our results show that phantom scalar field cosmologies have a reduced statistical tension on $H_0$ that it is less than 3$\\sigma$ using model-independent CMB catalogues as SPT-3G+WMAP9 and ACTPol DR-4+WMAP9 baselines. This suggests these models using a different phantom potential might address the Hubble constant problem and reduce the systematics involved.","sentences":["In this work, we explore new constraints on phantom scalar field cosmologies with a scalar field employing early times catalogues related to CMB measurements, along with the local standard observables, like Supernovae Type Ia (SNIa), $H(z)$ measurements (Cosmic Clocks), and Baryon Acoustic Oscillations (BAO) baselines.","In particular, we studied a tracker phantom field with hyperbolic polar coordinates that have been proposed in the literature.","The main goal is to obtain precise cosmological constraints for $H_0$ and $\\sigma_8$, in comparison to other constructions that present tension in early cosmological parameters.","Our results show that phantom scalar field cosmologies have a reduced statistical tension on $H_0$ that it is less than 3$\\sigma$ using model-independent CMB catalogues as SPT-3G+WMAP9 and ACTPol DR-4+WMAP9 baselines.","This suggests these models using a different phantom potential might address the Hubble constant problem and reduce the systematics involved."],"url":"http://arxiv.org/abs/2403.16562v1","category":"gr-qc"}
{"created":"2024-03-25 09:24:05","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","abstract":"Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.","sentences":["Federated Learning (FL) heavily depends on label quality for its performance.","However, the label distribution among individual clients is always both noisy and heterogeneous.","The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches.","To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples.","In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance.","To mitigate overfitting, we address this concern from two perspectives.","Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise.","Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models.","We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets.","The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios."],"url":"http://arxiv.org/abs/2403.16561v1","category":"cs.LG"}
{"created":"2024-03-25 09:18:48","title":"Active Admittance Control with Iterative Learning for General-Purpose Contact-Rich Manipulation","abstract":"Force interaction is inevitable when robots face multiple operation scenarios. How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem. Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism. The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model. The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility. Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method. Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively.","sentences":["Force interaction is inevitable when robots face multiple operation scenarios.","How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem.","Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism.","The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model.","The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility.","Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method.","Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively."],"url":"http://arxiv.org/abs/2403.16560v1","category":"cs.RO"}
{"created":"2024-03-25 09:17:15","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","abstract":"Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models.","sentences":["Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied.","This lack of exploration is primarily due to two key challenges.","Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships.","Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.","To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG).","ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions.","Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge.","Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models."],"url":"http://arxiv.org/abs/2403.16558v1","category":"cs.CV"}
{"created":"2024-03-25 09:10:57","title":"Hybrid low-dimensional limiting state of charge estimator for multi-cell lithium-ion batteries","abstract":"The state of charge (SOC) of lithium-ion batteries needs to be accurately estimated for safety and reliability purposes. For battery packs made of a large number of cells, it is not always feasible to design one SOC estimator per cell due to limited computational resources. Instead, only the minimum and the maximum SOC need to be estimated. The challenge is that the cells having minimum and maximum SOC typically change over time. In this context, we present a low-dimensional hybrid estimator of the minimum (maximum) SOC, whose convergence is analytically guaranteed. We consider for this purpose a battery consisting of cells interconnected in series, which we model by electric equivalent circuit models. We then present the hybrid estimator, which runs an observer designed for a single cell at any time instant, selected by a switching-like logic mechanism. We establish a practical exponential stability property for the estimation error on the minimum (maximum) SOC thereby guaranteeing the ability of the hybrid scheme to generate accurate estimates of the minimum (maximum) SOC. The analysis relies on non-smooth hybrid Lyapunov techniques. A numerical illustration is provided to showcase the relevance of the proposed approach.","sentences":["The state of charge (SOC) of lithium-ion batteries needs to be accurately estimated for safety and reliability purposes.","For battery packs made of a large number of cells, it is not always feasible to design one SOC estimator per cell due to limited computational resources.","Instead, only the minimum and the maximum SOC need to be estimated.","The challenge is that the cells having minimum and maximum SOC typically change over time.","In this context, we present a low-dimensional hybrid estimator of the minimum (maximum) SOC, whose convergence is analytically guaranteed.","We consider for this purpose a battery consisting of cells interconnected in series, which we model by electric equivalent circuit models.","We then present the hybrid estimator, which runs an observer designed for a single cell at any time instant, selected by a switching-like logic mechanism.","We establish a practical exponential stability property for the estimation error on the minimum (maximum) SOC thereby guaranteeing the ability of the hybrid scheme to generate accurate estimates of the minimum (maximum) SOC.","The analysis relies on non-smooth hybrid Lyapunov techniques.","A numerical illustration is provided to showcase the relevance of the proposed approach."],"url":"http://arxiv.org/abs/2403.16555v1","category":"eess.SY"}
{"created":"2024-03-25 09:04:14","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","abstract":"The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate the effectiveness of our approach.","sentences":["The black-box nature of deep learning models in NLP hinders their widespread application.","The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions.","Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations.","In this work, we introduce a novel method, namely Poincar\\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity.","Inspired by Poincar\\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures.","Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm.","Experimental results demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.16554v1","category":"cs.CL"}
{"created":"2024-03-25 08:57:27","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","abstract":"Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training. QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer","sentences":["Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance.","However, existing models in this domain still suffer from suboptimal performance.","We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity.","ii)","We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation.","iii)","We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers.","Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training.","QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets.","Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%.","To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer"],"url":"http://arxiv.org/abs/2403.16552v1","category":"cs.NE"}
{"created":"2024-03-25 08:54:18","title":"Coupling elastic media to gravitational waves: an effective field theory approach","abstract":"The interaction of a gravitational wave (GW) with an elastic body is usually described in terms of a GW \"force\" driving the oscillations of the body's normal modes. However, this description is only possible for GW frequencies for which the response of the elastic body is dominated by a few normal modes. At higher frequencies the normal modes blend into a quasi-continuum and a field-theoretical description, as pioneered by Dyson already in 1969, becomes necessary. However, since the metric perturbation $h_{\\mu\\nu}$ is an intrinsically relativistic object, a consistent coupling to GWs can only be obtained within a relativistic (and, in fact generally covariant) theory of elasticity. We develop such a formalism using the methods of modern effective field theories, and we use it to provide a derivation of the interaction of elastic bodies with GWs valid also in the high-frequency regime, providing a first-principle derivation of Dyson's result (and partially correcting it). We also stress that the field-theoretical results are obtained working in the TT frame, while the description in terms of a force driving the normal modes is only valid in the proper detector frame. We show how to transform the results between the two frames. Beside an intrinsic conceptual interest, these results are relevant to the computation of the sensitivity of the recently proposed Lunar Gravitational Wave Antenna.","sentences":["The interaction of a gravitational wave (GW) with an elastic body is usually described in terms of a GW \"force\" driving the oscillations of the body's normal modes.","However, this description is only possible for GW frequencies for which the response of the elastic body is dominated by a few normal modes.","At higher frequencies the normal modes blend into a quasi-continuum and a field-theoretical description, as pioneered by Dyson already in 1969, becomes necessary.","However, since the metric perturbation $h_{\\mu\\nu}$ is an intrinsically relativistic object, a consistent coupling to GWs can only be obtained within a relativistic (and, in fact generally covariant) theory of elasticity.","We develop such a formalism using the methods of modern effective field theories, and we use it to provide a derivation of the interaction of elastic bodies with GWs valid also in the high-frequency regime, providing a first-principle derivation of Dyson's result (and partially correcting it).","We also stress that the field-theoretical results are obtained working in the TT frame, while the description in terms of a force driving the normal modes is only valid in the proper detector frame.","We show how to transform the results between the two frames.","Beside an intrinsic conceptual interest, these results are relevant to the computation of the sensitivity of the recently proposed Lunar Gravitational Wave Antenna."],"url":"http://arxiv.org/abs/2403.16550v1","category":"gr-qc"}
{"created":"2024-03-25 08:43:27","title":"Unipolar opical transitons in nanoclusters of ellipsoidal geometry","abstract":"The quantum states of an ellipsoidal nanocluster of a heterophase system InAs / GaAs are studied using an exact analytical approach, in contrast to the generally accepted theoretical model based on the adiabatic approximation. It is shown that the spectrum of a nanoobject is formed from local groups, consisting of discrete levels, separated by terahertz frequency intervals. A double random degeneration of certain spectrum states is revealed. Allowed mid-infrared (IR) optical transitions between different spectral states are analyzed. The role of dimensional parameters and features of the shape of a nanoobject in the characteristics of unipolar transitions is assessed. The absorption spectrum for the transition in the lower part of the substructure spectrum is calculated.","sentences":["The quantum states of an ellipsoidal nanocluster of a heterophase system InAs / GaAs are studied using an exact analytical approach, in contrast to the generally accepted theoretical model based on the adiabatic approximation.","It is shown that the spectrum of a nanoobject is formed from local groups, consisting of discrete levels, separated by terahertz frequency intervals.","A double random degeneration of certain spectrum states is revealed.","Allowed mid-infrared (IR) optical transitions between different spectral states are analyzed.","The role of dimensional parameters and features of the shape of a nanoobject in the characteristics of unipolar transitions is assessed.","The absorption spectrum for the transition in the lower part of the substructure spectrum is calculated."],"url":"http://arxiv.org/abs/2403.16546v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-25 08:38:50","title":"The Role of Mean Absolute Deviation Function in Obtaining Smooth Estimation for Distribution and Density Functions: Beta Regression Approach","abstract":"Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment. This article introduces a simple approach but novel for estimating both functions in one process to have smooth curves for both via left mean absolute deviation (MAD) function and beta regression approach. Our approach explores estimation of both functions by smoothing the first derivative of left MAD function to obtain the final optimal smooth estimates. The derivation for these final smooth estimates under conditions of nondecreasing distribution function and nonnegative density function are performed by applying beta regression of a polynomial degree on the first derivative of left MAD function where the degree of polynomial is chosen among the models that have less mean absolute residuals under the constraint of nonnegativity for the first derivative of regression vector of expected values. A general class of normal, logistic and Gumbel distributions is derived as proposed smooth estimators for the distribution and density functions using logit, probit and cloglog links, respectively. This approach is applied to simulated data from unimodal, bimodal, tri-modal and skew distributions and an application to real data set is given.","sentences":["Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment.","This article introduces a simple approach but novel for estimating both functions in one process to have smooth curves for both via left mean absolute deviation (MAD) function and beta regression approach.","Our approach explores estimation of both functions by smoothing the first derivative of left MAD function to obtain the final optimal smooth estimates.","The derivation for these final smooth estimates under conditions of nondecreasing distribution function and nonnegative density function are performed by applying beta regression of a polynomial degree on the first derivative of left MAD function where the degree of polynomial is chosen among the models that have less mean absolute residuals under the constraint of nonnegativity for the first derivative of regression vector of expected values.","A general class of normal, logistic and Gumbel distributions is derived as proposed smooth estimators for the distribution and density functions using logit, probit and cloglog links, respectively.","This approach is applied to simulated data from unimodal, bimodal, tri-modal and skew distributions and an application to real data set is given."],"url":"http://arxiv.org/abs/2403.16544v1","category":"stat.ME"}
{"created":"2024-03-25 08:36:06","title":"Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning","abstract":"Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where information is scarce. Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.","sentences":["Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification.","Representations of textual data extract rich information spanning the domain, entities, and relations.","In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning.","While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped.","To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.","Our method employs contrastive learning to extract complementary discriminative information from these individual representations.","This is particularly relevant in low-resource settings where information is scarce.","Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available.","We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints."],"url":"http://arxiv.org/abs/2403.16543v1","category":"cs.CL"}
{"created":"2024-03-25 08:19:07","title":"Uncovering faint lensed gravitational-wave signals and reprioritizing their follow-up analysis using galaxy lensing forecasts with detected counterparts","abstract":"Like light, gravitational waves can be gravitationally lensed by massive astrophysical objects. For galaxy and galaxy-cluster lenses, one expects to see strong lensing -- forecasted to become observable in the coming years -- where the original wave is split into multiple copies with the same frequency evolution but different overall arrival times, phases, amplitudes, and signal strengths. Some of these images can be below the detection threshold and require targeted search methods, based on tailor-made template banks. These searches can be made more sensitive by using our knowledge of the typical distribution and morphology of lenses to predict the time delay, magnification, and image-type ordering of the lensed images. Here, we show that when a subset of the images is super-threshold, they can be used to construct a more constrained prediction of the arrival time of the remaining signals, enhancing our ability to identify lensing candidate signals. Our suggested method effectively reduces the list of triggers requiring follow-up and generally re-ranks the genuine counterpart higher in the lensing candidate list. Therefore, in the future, if one observes two or three lensed images, the information they provide can be used to identify their sub-threshold counterparts, thus allowing identification of additional lensed images. Finding such images would also strengthen our evidence for the event being lensed.","sentences":["Like light, gravitational waves can be gravitationally lensed by massive astrophysical objects.","For galaxy and galaxy-cluster lenses, one expects to see strong lensing -- forecasted to become observable in the coming years -- where the original wave is split into multiple copies with the same frequency evolution but different overall arrival times, phases, amplitudes, and signal strengths.","Some of these images can be below the detection threshold and require targeted search methods, based on tailor-made template banks.","These searches can be made more sensitive by using our knowledge of the typical distribution and morphology of lenses to predict the time delay, magnification, and image-type ordering of the lensed images.","Here, we show that when a subset of the images is super-threshold, they can be used to construct a more constrained prediction of the arrival time of the remaining signals, enhancing our ability to identify lensing candidate signals.","Our suggested method effectively reduces the list of triggers requiring follow-up and generally re-ranks the genuine counterpart higher in the lensing candidate list.","Therefore, in the future, if one observes two or three lensed images, the information they provide can be used to identify their sub-threshold counterparts, thus allowing identification of additional lensed images.","Finding such images would also strengthen our evidence for the event being lensed."],"url":"http://arxiv.org/abs/2403.16532v1","category":"gr-qc"}
{"created":"2024-03-25 08:16:06","title":"An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models","abstract":"Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations. We perform experiments using a text-to-image generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fusion mechanism on two common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and 50% increased training speed compared to a strong U-ViT baseline with an early fusion.","sentences":["Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video.","However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc.","We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment.","We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations.","We perform experiments using a text-to-image generation task on the MS-COCO dataset.","We compare our intermediate fusion mechanism with the classic early fusion mechanism on two common conditioning methods on a U-shaped ViT backbone.","Our intermediate fusion model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and 50% increased training speed compared to a strong U-ViT baseline with an early fusion."],"url":"http://arxiv.org/abs/2403.16530v1","category":"cs.CV"}
{"created":"2024-03-25 08:15:08","title":"Exploit High-Dimensional RIS Information to Localization: What Is the Impact of Faulty Element?","abstract":"This paper proposes a novel localization algorithm using the reconfigurable intelligent surface (RIS) received signal, i.e., RIS information. Compared with BS received signal, i.e., BS information, RIS information offers higher dimension and richer feature set, thereby providing an enhanced capacity to distinguish positions of the mobile users (MUs). Additionally, we address a practical scenario where RIS contains some unknown (number and places) faulty elements that cannot receive signals. Initially, we employ transfer learning to design a two-phase transfer learning (TPTL) algorithm, designed for accurate detection of faulty elements. Then our objective is to regain the information lost from the faulty elements and reconstruct the complete high-dimensional RIS information for localization. To this end, we propose a transfer-enhanced dual-stage (TEDS) algorithm. In \\emph{Stage I}, we integrate the CNN and variational autoencoder (VAE) to obtain the RIS information, which in \\emph{Stage II}, is input to the transferred DenseNet 121 to estimate the location of the MU. To gain more insight, we propose an alternative algorithm named transfer-enhanced direct fingerprint (TEDF) algorithm which only requires the BS information. The comparison between TEDS and TEDF reveals the effectiveness of faulty element detection and the benefits of utilizing the high-dimensional RIS information for localization. Besides, our empirical results demonstrate that the performance of the localization algorithm is dominated by the high-dimensional RIS information and is robust to unoptimized phase shifts and signal-to-noise ratio (SNR).","sentences":["This paper proposes a novel localization algorithm using the reconfigurable intelligent surface (RIS) received signal, i.e., RIS information.","Compared with BS received signal, i.e., BS information, RIS information offers higher dimension and richer feature set, thereby providing an enhanced capacity to distinguish positions of the mobile users (MUs).","Additionally, we address a practical scenario where RIS contains some unknown (number and places) faulty elements that cannot receive signals.","Initially, we employ transfer learning to design a two-phase transfer learning (TPTL) algorithm, designed for accurate detection of faulty elements.","Then our objective is to regain the information lost from the faulty elements and reconstruct the complete high-dimensional RIS information for localization.","To this end, we propose a transfer-enhanced dual-stage (TEDS) algorithm.","In \\emph{Stage I}, we integrate the CNN and variational autoencoder (VAE) to obtain the RIS information, which in \\emph{Stage II}, is input to the transferred DenseNet 121 to estimate the location of the MU.","To gain more insight, we propose an alternative algorithm named transfer-enhanced direct fingerprint (TEDF) algorithm which only requires the BS information.","The comparison between TEDS and TEDF reveals the effectiveness of faulty element detection and the benefits of utilizing the high-dimensional RIS information for localization.","Besides, our empirical results demonstrate that the performance of the localization algorithm is dominated by the high-dimensional RIS information and is robust to unoptimized phase shifts and signal-to-noise ratio (SNR)."],"url":"http://arxiv.org/abs/2403.16529v1","category":"eess.SP"}
{"created":"2024-03-25 08:11:02","title":"Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art","abstract":"Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment. While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model's decision, and detect when it may be hallucinating. In this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.","sentences":["Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry.","The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based.","While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time.","The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing.","Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios.","Large language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment.","While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor.","We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model's decision, and detect when it may be hallucinating.","In this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field."],"url":"http://arxiv.org/abs/2403.16527v1","category":"cs.AI"}
{"created":"2024-03-25 08:09:04","title":"Deep Learning Based Measure of Name Concentration Risk","abstract":"We propose a new deep learning approach for the quantification of name concentration risk in loan portfolios. Our approach is tailored for small portfolios and allows for both an actuarial as well as a mark-to-market definition of loss. The training of our neural network relies on Monte Carlo simulations with importance sampling which we explicitly formulate for the CreditRisk${+}$ and the ratings-based CreditMetrics model. Numerical results based on simulated as well as real data demonstrate the accuracy of our new approach and its superior performance compared to existing analytical methods for assessing name concentration risk in small and concentrated portfolios.","sentences":["We propose a new deep learning approach for the quantification of name concentration risk in loan portfolios.","Our approach is tailored for small portfolios and allows for both an actuarial as well as a mark-to-market definition of loss.","The training of our neural network relies on Monte Carlo simulations with importance sampling which we explicitly formulate for the CreditRisk${+}$ and the ratings-based CreditMetrics model.","Numerical results based on simulated as well as real data demonstrate the accuracy of our new approach and its superior performance compared to existing analytical methods for assessing name concentration risk in small and concentrated portfolios."],"url":"http://arxiv.org/abs/2403.16525v1","category":"q-fin.RM"}
{"created":"2024-03-25 08:09:01","title":"Harnessing the power of LLMs for normative reasoning in MASs","abstract":"Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning. The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making. This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research. We present our vision for creating normative LLM agents. In particular, we discuss how the recently proposed \"LLM agent\" approaches can be extended to implement such normative LLM agents. We also highlight challenges in this emerging field. This paper thus aims to foster collaboration between MAS, NLP and LLM researchers in order to advance the field of normative agents.","sentences":["Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals.","In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents.","However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning.","The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making.","This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research.","We present our vision for creating normative LLM agents.","In particular, we discuss how the recently proposed \"LLM agent\" approaches can be extended to implement such normative LLM agents.","We also highlight challenges in this emerging field.","This paper thus aims to foster collaboration between MAS, NLP and LLM researchers in order to advance the field of normative agents."],"url":"http://arxiv.org/abs/2403.16524v1","category":"cs.AI"}
{"created":"2024-03-25 08:06:08","title":"Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis","abstract":"Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and has at least two directed paths to $Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants. Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant. With the path information, causal order is identifiable under some graphical conditions. A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method.","sentences":["Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios.","One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise.","For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution.","However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent.","Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and has at least two directed paths to $Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants.","Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant.","With the path information, causal order is identifiable under some graphical conditions.","A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.16523v1","category":"stat.ML"}
{"created":"2024-03-25 08:05:08","title":"Galactic dynamics in the presence of scalaron: A perspective from $\\boldsymbol{f(R)}$ gravity","abstract":"We consider $f(R)$ modified gravity with the chameleon mechanism as an alternative approach to address the dark matter issue on the galactic scale. Metric formalism of $f(R)$ theory is considered in this study. A mathematical transformation tool called conformal transformation which transforms the action from Jordan to Einstein frame is employed to simplify the fourth-order modified field equation and to describe the extra degree of freedom $f_R$ by using a minimally coupled scalar field (scalaron) showing the chameleonic character. Then, we examine the viability of a newly introduced $f(R)$ gravity model on behalf of the chameleonic behavior of the scalaron. The model analyzes this behavior of the scalaron successfully with the singularity correction. Further, we consider a test particle (star) in a static, spherically symmetric spacetime to investigate the importance of the scalaron in galactic dynamics. In the non-relativistic limit, the rotational velocity equation for the particle with scalaron contribution is derived. This contribution is found to be model dependent. We generate the rotation curves using the velocity equation and fit the predicted curves to observational data of a set of thirty seven sample galaxies of different categories. The curves are fitted based on two fitting parameters $M_0$ and $r_c$. The fitting shows good agreement of the prediction with the observed data.","sentences":["We consider $f(R)$ modified gravity with the chameleon mechanism as an alternative approach to address the dark matter issue on the galactic scale.","Metric formalism of $f(R)$ theory is considered in this study.","A mathematical transformation tool called conformal transformation which transforms the action from Jordan to Einstein frame is employed to simplify the fourth-order modified field equation and to describe the extra degree of freedom $f_R$ by using a minimally coupled scalar field (scalaron) showing the chameleonic character.","Then, we examine the viability of a newly introduced $f(R)$ gravity model on behalf of the chameleonic behavior of the scalaron.","The model analyzes this behavior of the scalaron successfully with the singularity correction.","Further, we consider a test particle (star) in a static, spherically symmetric spacetime to investigate the importance of the scalaron in galactic dynamics.","In the non-relativistic limit, the rotational velocity equation for the particle with scalaron contribution is derived.","This contribution is found to be model dependent.","We generate the rotation curves using the velocity equation and fit the predicted curves to observational data of a set of thirty seven sample galaxies of different categories.","The curves are fitted based on two fitting parameters $M_0$ and $r_c$. The fitting shows good agreement of the prediction with the observed data."],"url":"http://arxiv.org/abs/2403.16522v1","category":"gr-qc"}
{"created":"2024-03-25 08:03:33","title":"Employing High-Dimensional RIS Information for RIS-aided Localization Systems","abstract":"Reconfigurable intelligent surface (RIS)-aided localization systems have attracted extensive research attention due to their accuracy enhancement capabilities. However, most studies primarily utilized the base stations (BS) received signal, i.e., BS information, for localization algorithm design, neglecting the potential of RIS received signal, i.e., RIS information. Compared with BS information, RIS information offers higher dimension and richer feature set, thereby significantly improving the ability to extract positions of the mobile users (MUs). Addressing this oversight, this paper explores the algorithm design based on the high-dimensional RIS information. Specifically, we first propose a RIS information reconstruction (RIS-IR) algorithm to reconstruct the high-dimensional RIS information from the low-dimensional BS information. The proposed RIS-IR algorithm comprises a data processing module for preprocessing BS information, a convolution neural network (CNN) module for feature extraction, and an output module for outputting the reconstructed RIS information. Then, we propose a transfer learning based fingerprint (TFBF) algorithm that employs the reconstructed high-dimensional RIS information for MU localization. This involves adapting a pre-trained DenseNet-121 model to map the reconstructed RIS signal to the MU's three-dimensional (3D) position. Empirical results affirm that the localization performance is significantly influenced by the high-dimensional RIS information and maintains robustness against unoptimized phase shifts.","sentences":["Reconfigurable intelligent surface (RIS)-aided localization systems have attracted extensive research attention due to their accuracy enhancement capabilities.","However, most studies primarily utilized the base stations (BS) received signal, i.e., BS information, for localization algorithm design, neglecting the potential of RIS received signal, i.e., RIS information.","Compared with BS information, RIS information offers higher dimension and richer feature set, thereby significantly improving the ability to extract positions of the mobile users (MUs).","Addressing this oversight, this paper explores the algorithm design based on the high-dimensional RIS information.","Specifically, we first propose a RIS information reconstruction (RIS-IR) algorithm to reconstruct the high-dimensional RIS information from the low-dimensional BS information.","The proposed RIS-IR algorithm comprises a data processing module for preprocessing BS information, a convolution neural network (CNN) module for feature extraction, and an output module for outputting the reconstructed RIS information.","Then, we propose a transfer learning based fingerprint (TFBF) algorithm that employs the reconstructed high-dimensional RIS information for MU localization.","This involves adapting a pre-trained DenseNet-121 model to map the reconstructed RIS signal to the MU's three-dimensional (3D) position.","Empirical results affirm that the localization performance is significantly influenced by the high-dimensional RIS information and maintains robustness against unoptimized phase shifts."],"url":"http://arxiv.org/abs/2403.16521v1","category":"eess.SP"}
{"created":"2024-03-25 08:01:33","title":"Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study","abstract":"Norms are an important component of the social fabric of society by prescribing expected behaviour. In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust. Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning. However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness. In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations. This paper evaluates the capability of LLMs to detecting norm violations. Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated. For our evaluations we first obtained the ground truth from three human evaluators for each story. Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4). Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind. Also, we identify areas where these models perform poorly and discuss implications for future work.","sentences":["Norms are an important component of the social fabric of society by prescribing expected behaviour.","In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust.","Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning.","However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness.","In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations.","This paper evaluates the capability of LLMs to detecting norm violations.","Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated.","For our evaluations we first obtained the ground truth from three human evaluators for each story.","Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4).","Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind.","Also, we identify areas where these models perform poorly and discuss implications for future work."],"url":"http://arxiv.org/abs/2403.16517v1","category":"cs.MA"}
{"created":"2024-03-25 08:00:43","title":"Visually Guided Generative Text-Layout Pre-training for Document Intelligence","abstract":"Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering.","sentences":["Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells).","To this end, we propose visually guided generative text-layout pre-training, named ViTLP.","Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence.","In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length.","ViTLP can function as a native OCR model to localize and recognize texts of document images.","Besides, ViTLP can be effectively applied to various downstream VDU tasks.","Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering."],"url":"http://arxiv.org/abs/2403.16516v1","category":"cs.CL"}
{"created":"2024-03-25 07:58:58","title":"Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models","abstract":"In the last few years, generative models have shown their powerful capabilities in synthesizing realistic images in both quality and diversity (i.e., facial images, and natural subjects). Unfortunately, the artifact patterns in fake images synthesized by different generative models are inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake. In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties. In this paper, we employ natural traces shared only by real images as an additional predictive target in the detector. Specifically, the natural traces are learned from the wild real images and we introduce extended supervised contrastive learning to bring them closer to real images and further away from fake ones. This motivates the detector to make decisions based on the proximity of images to the natural traces. To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising 6 GAN and 6 diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations. Experimental results show that our proposed method gives 96.1% mAP significantly outperforms the baselines. Extensive experiments conducted on the widely recognized platform Midjourney reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment. The source code and partial self-built dataset are available in supplementary material.","sentences":["In the last few years, generative models have shown their powerful capabilities in synthesizing realistic images in both quality and diversity (i.e., facial images, and natural subjects).","Unfortunately, the artifact patterns in fake images synthesized by different generative models are inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake.","In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties.","In this paper, we employ natural traces shared only by real images as an additional predictive target in the detector.","Specifically, the natural traces are learned from the wild real images and we introduce extended supervised contrastive learning to bring them closer to real images and further away from fake ones.","This motivates the detector to make decisions based on the proximity of images to the natural traces.","To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising 6 GAN and 6 diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations.","Experimental results show that our proposed method gives 96.1% mAP significantly outperforms the baselines.","Extensive experiments conducted on the widely recognized platform Midjourney reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment.","The source code and partial self-built dataset are available in supplementary material."],"url":"http://arxiv.org/abs/2403.16513v1","category":"cs.CV"}
{"created":"2024-03-25 07:55:29","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","abstract":"In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages.","sentences":["In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages.","Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish.","In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages.","Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment.","Moreover, we provide valuable insights into various facets of ICL for low-resource languages.","Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in.","Our work highlights the importance of advancing ICL research, particularly for low-resource languages."],"url":"http://arxiv.org/abs/2403.16512v1","category":"cs.CL"}
{"created":"2024-03-25 07:54:38","title":"Extremality of collections of sets with respect to general perturbations","abstract":"The paper proposes another extension of the extremal principle. A new extremality model involving arbitrary families of perturbations (deformations) of the given sets is studied. It generalizes the conventional model based on linear translations of the sets as well as its set-valued extensions. This approach leads to a more general and simpler version of fuzzy separation. We demonstrate the applicability of the new model to set-valued optimization problems, weakening the assumptions of the known results and streamlining their proofs.","sentences":["The paper proposes another extension of the extremal principle.","A new extremality model involving arbitrary families of perturbations (deformations) of the given sets is studied.","It generalizes the conventional model based on linear translations of the sets as well as its set-valued extensions.","This approach leads to a more general and simpler version of fuzzy separation.","We demonstrate the applicability of the new model to set-valued optimization problems, weakening the assumptions of the known results and streamlining their proofs."],"url":"http://arxiv.org/abs/2403.16511v1","category":"math.OC"}
{"created":"2024-03-25 07:54:18","title":"Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework","abstract":"Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \\url{https://github.com/ICTMCG/Make-Your-Anchor}.","sentences":["Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging.","In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements.","Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances.","We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances.","To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference.","Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos.","Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods.","Project page: \\url{https://github.com/ICTMCG/Make-Your-Anchor}."],"url":"http://arxiv.org/abs/2403.16510v1","category":"cs.CV"}
{"created":"2024-03-25 07:48:34","title":"Human Understanding AI Paper Challenge 2024 -- Dataset Design","abstract":"In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life. This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development.","sentences":["In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life.","This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development."],"url":"http://arxiv.org/abs/2403.16509v1","category":"cs.LG"}
{"created":"2024-03-25 07:47:52","title":"Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning","abstract":"Current approaches for learning for planning have yet to achieve competitive performance against classical planners in several domains, and have poor overall performance. In this work, we construct novel graph representations of lifted planning tasks and use the WL algorithm to generate features from them. These features are used with classical machine learning methods which have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster than the state-of-the-art deep learning for planning models. Our novel approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the $h^{\\text{FF}}$ heuristic in a fair competition setting. It also outperforms or ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on plan quality. WL-GOOSE is the first learning for planning model which achieves these feats. Furthermore, we study the connections between our novel WL feature generation method, previous theoretically flavoured learning architectures, and Description Logic Features for planning.","sentences":["Current approaches for learning for planning have yet to achieve competitive performance against classical planners in several domains, and have poor overall performance.","In this work, we construct novel graph representations of lifted planning tasks and use the WL algorithm to generate features from them.","These features are used with classical machine learning methods which have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster than the state-of-the-art deep learning for planning models.","Our novel approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the $h^{\\text{FF}}$ heuristic in a fair competition setting.","It also outperforms or ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on plan quality.","WL-GOOSE is the first learning for planning model which achieves these feats.","Furthermore, we study the connections between our novel WL feature generation method, previous theoretically flavoured learning architectures, and Description Logic Features for planning."],"url":"http://arxiv.org/abs/2403.16508v1","category":"cs.AI"}
{"created":"2024-03-25 07:34:42","title":"Learning To Guide Human Decision Makers With Vision-Language Models","abstract":"There is increasing interest in developing AIs for assisting human decision making in \\textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   %   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   %   This \\textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine's decisions due to \\textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   %   As a remedy, we introduce \\textit{learning to guide} (LTG), an alternative framework in which -- rather than taking control from the human expert -- the machine provides \\textit{guidance} useful for decision making, and the human is entirely responsible for coming up with a decision.   %   In order to ensure guidance is \\textit{interpretable} and \\textit{task-specific}, we develop \\method, an approach for turning \\textit{any} vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback.   %   Our empirical evaluation highlights the promise of \\method on a challenging, real-world medical diagnosis task.","sentences":["There is increasing interest in developing AIs for assisting human decision making in \\textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   ","%   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   ","%   This \\textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios.","On the one hand, the expert may end up over-relying on the machine's decisions due to \\textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI.","On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   ","%   As a remedy, we introduce \\textit{learning to guide} (LTG), an alternative framework in which -- rather than taking control from the human expert -- the machine provides \\textit{guidance} useful for decision making, and the human is entirely responsible for coming up with a decision.   ","%   ","In order to ensure guidance is \\textit{interpretable} and \\textit{task-specific}, we develop \\method, an approach for turning \\textit{any} vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback.   ","%   Our empirical evaluation highlights the promise of \\method on a challenging, real-world medical diagnosis task."],"url":"http://arxiv.org/abs/2403.16501v1","category":"cs.AI"}
{"created":"2024-03-25 07:34:06","title":"Self-Supervised Learning for Medical Image Data with Anatomy-Oriented Imaging Planes","abstract":"Self-supervised learning has emerged as a powerful tool for pretraining deep networks on unlabeled data, prior to transfer learning of target tasks with limited annotation. The relevance between the pretraining pretext and target tasks is crucial to the success of transfer learning. Various pretext tasks have been proposed to utilize properties of medical image data (e.g., three dimensionality), which are more relevant to medical image analysis than generic ones for natural images. However, previous work rarely paid attention to data with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance imaging views. As these imaging planes are defined according to the anatomy of the imaged organ, pretext tasks effectively exploiting this information can pretrain the networks to gain knowledge on the organ of interest. In this work, we propose two complementary pretext tasks for this group of medical image data based on the spatial relationship of the imaging planes. The first is to learn the relative orientation between the imaging planes and implemented as regressing their intersecting lines. The second exploits parallel imaging planes to regress their relative slice locations within a stack. Both pretext tasks are conceptually straightforward and easy to implement, and can be combined in multitask learning for better representation learning. Thorough experiments on two anatomical structures (heart and knee) and representative target tasks (semantic segmentation and classification) demonstrate that the proposed pretext tasks are effective in pretraining deep networks for remarkably boosted performance on the target tasks, and superior to other recent approaches.","sentences":["Self-supervised learning has emerged as a powerful tool for pretraining deep networks on unlabeled data, prior to transfer learning of target tasks with limited annotation.","The relevance between the pretraining pretext and target tasks is crucial to the success of transfer learning.","Various pretext tasks have been proposed to utilize properties of medical image data (e.g., three dimensionality), which are more relevant to medical image analysis than generic ones for natural images.","However, previous work rarely paid attention to data with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance imaging views.","As these imaging planes are defined according to the anatomy of the imaged organ, pretext tasks effectively exploiting this information can pretrain the networks to gain knowledge on the organ of interest.","In this work, we propose two complementary pretext tasks for this group of medical image data based on the spatial relationship of the imaging planes.","The first is to learn the relative orientation between the imaging planes and implemented as regressing their intersecting lines.","The second exploits parallel imaging planes to regress their relative slice locations within a stack.","Both pretext tasks are conceptually straightforward and easy to implement, and can be combined in multitask learning for better representation learning.","Thorough experiments on two anatomical structures (heart and knee) and representative target tasks (semantic segmentation and classification) demonstrate that the proposed pretext tasks are effective in pretraining deep networks for remarkably boosted performance on the target tasks, and superior to other recent approaches."],"url":"http://arxiv.org/abs/2403.16499v1","category":"cs.CV"}
{"created":"2024-03-25 07:29:39","title":"BackCom Assisted Hybrid NOMA Uplink Transmission for Ambient IoT","abstract":"Hybrid non-orthogonal multiple access (H-NOMA) has recently received significant attention as a general framework of multiple access, where both conventional orthogonal multiple access (OMA) and pure NOMA are its special cases. This paper focuses on the application of H-NOMA to ambient Internet of Things (IoT) with energy-constrained devices, where a new backscatter communication (BackCom) assisted H-NOMA uplink scheme is developed. Resource allocation for H-NOMA uplink transmission is also considered, where an overall power minimization problem is formulated. Insightful understandings for the key features of BackCom assisted H-NOMA and its difference from conventional H-NOMA are illustrated by developing analytical results for the two-user special case. For the general multi-user scenario, two algorithms, one based on the branch-bound (BB) principle and the other based on successive convex approximation (SCA), are developed to realize different tradeoffs between the system performance and complexity. The numerical results are also provided to verify the accuracy of the developed analytical results and demonstrate the performance gain of H-NOMA over OMA.","sentences":["Hybrid non-orthogonal multiple access (H-NOMA) has recently received significant attention as a general framework of multiple access, where both conventional orthogonal multiple access (OMA) and pure NOMA are its special cases.","This paper focuses on the application of H-NOMA to ambient Internet of Things (IoT) with energy-constrained devices, where a new backscatter communication (BackCom) assisted H-NOMA uplink scheme is developed.","Resource allocation for H-NOMA uplink transmission is also considered, where an overall power minimization problem is formulated.","Insightful understandings for the key features of BackCom assisted H-NOMA and its difference from conventional H-NOMA are illustrated by developing analytical results for the two-user special case.","For the general multi-user scenario, two algorithms, one based on the branch-bound (BB) principle and the other based on successive convex approximation (SCA), are developed to realize different tradeoffs between the system performance and complexity.","The numerical results are also provided to verify the accuracy of the developed analytical results and demonstrate the performance gain of H-NOMA over OMA."],"url":"http://arxiv.org/abs/2403.16498v1","category":"cs.IT"}
{"created":"2024-03-25 07:23:23","title":"LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting","abstract":"Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series. Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated convolution layers, and periodic features are extracted by dynamic graph convolution layers. For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features. Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results. Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\% and a maximum improvement of 16.78\\% over baseline models. The source code is available at https://github.com/GeoX-Lab/LSTTN.","sentences":["Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models.","However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow.","Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation.","To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow.","First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series.","Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated convolution layers, and periodic features are extracted by dynamic graph convolution layers.","For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features.","Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results.","Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\% and a maximum improvement of 16.78\\% over baseline models.","The source code is available at https://github.com/GeoX-Lab/LSTTN."],"url":"http://arxiv.org/abs/2403.16495v1","category":"cs.LG"}
{"created":"2024-03-25 07:22:22","title":"CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid Convolution and Transformer Neural Networks","abstract":"We present CT-Bound, a fast boundary estimation method for noisy images using a hybrid Convolution and Transformer neural network. The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization of image boundaries. It first estimates a parametric representation of boundary structures only using the input image within a small receptive field and then refines the boundary structure in the parameter domain without accessing the input image. Because of this, a part of the network can be easily trained using naive, synthetic images and still generalized to real images, and the entire architecture is computationally efficient as the boundary refinement is non-iterative and not in the image domain. Compared with the previous highest accuracy methods, our experiment shows that CT-Bound is 100 times faster, producing comparably accurate, high-quality boundary and color maps. We also demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra fine-tuning and real-time boundary map and color map videos at ten frames per second.","sentences":["We present CT-Bound, a fast boundary estimation method for noisy images using a hybrid Convolution and Transformer neural network.","The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization of image boundaries.","It first estimates a parametric representation of boundary structures only using the input image within a small receptive field and then refines the boundary structure in the parameter domain without accessing the input image.","Because of this, a part of the network can be easily trained using naive, synthetic images and still generalized to real images, and the entire architecture is computationally efficient as the boundary refinement is non-iterative and not in the image domain.","Compared with the previous highest accuracy methods, our experiment shows that CT-Bound is 100 times faster, producing comparably accurate, high-quality boundary and color maps.","We also demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra fine-tuning and real-time boundary map and color map videos at ten frames per second."],"url":"http://arxiv.org/abs/2403.16494v1","category":"cs.CV"}
{"created":"2024-03-25 07:16:08","title":"Ensuring Disturbance Rejection Performance by Synthesizing Grid-Following and Grid-Forming Inverters in Power Systems","abstract":"To meet the dynamic requirement of power systems, it is imperative for grid-connected inverters to ensure good disturbance rejection performance (DRP) under variable grid conditions. This letter discovers and theoretically proves that for the general networks, synthesizing grid-following (GFL) inverters and grid-forming (GFM) inverters can more effectively ensure the DRP of multiple inverters, compared to homogeneous inverter-based systems that solely use either GFL or GFM inverters. The combination of GFL inverters and GFM inverters can concurrently increases the smallest eigenvalue and decreases the largest eigenvalue of the grounded network Laplacian matrix. This can be equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM inverters, thereby ensuring the DRP of inverters both in weak and strong grids. The result reveals the necessity of synthesizing diverse inverter control schemes from the network-based perspective. Sensitivity function-based analysis and real-time simulations confirm the effectiveness of our results.","sentences":["To meet the dynamic requirement of power systems, it is imperative for grid-connected inverters to ensure good disturbance rejection performance (DRP) under variable grid conditions.","This letter discovers and theoretically proves that for the general networks, synthesizing grid-following (GFL) inverters and grid-forming (GFM) inverters can more effectively ensure the DRP of multiple inverters, compared to homogeneous inverter-based systems that solely use either GFL or GFM inverters.","The combination of GFL inverters and GFM inverters can concurrently increases the smallest eigenvalue and decreases the largest eigenvalue of the grounded network Laplacian matrix.","This can be equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM inverters, thereby ensuring the DRP of inverters both in weak and strong grids.","The result reveals the necessity of synthesizing diverse inverter control schemes from the network-based perspective.","Sensitivity function-based analysis and real-time simulations confirm the effectiveness of our results."],"url":"http://arxiv.org/abs/2403.16488v1","category":"eess.SY"}
{"created":"2024-03-25 07:12:51","title":"Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation","abstract":"This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities.","sentences":["This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation.","We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning.","Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking.","The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics.","ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN.","ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit.","The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities."],"url":"http://arxiv.org/abs/2403.16485v1","category":"cs.RO"}
{"created":"2024-03-25 07:08:13","title":"Automatic Construction of a Large-Scale Corpus for Geoparsing Using Wikipedia Hyperlinks","abstract":"Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts. Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation. For evaluating geoparsing systems, several corpora have been proposed in previous work. However, these corpora are small-scale and suffer from the coverage of location expressions on general domains. In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates. With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions. 45.6% of location expressions are ambiguous and refer to more than one location with the same notation. In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates. By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts. Experimental results show that there remains room for improvement by disambiguating location expressions.","sentences":["Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts.","Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation.","For evaluating geoparsing systems, several corpora have been proposed in previous work.","However, these corpora are small-scale and suffer from the coverage of location expressions on general domains.","In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles.","WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates.","With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing.","The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions.","45.6% of location expressions are ambiguous and refer to more than one location with the same notation.","In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates.","By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts.","Experimental results show that there remains room for improvement by disambiguating location expressions."],"url":"http://arxiv.org/abs/2403.16483v1","category":"cs.CL"}
{"created":"2024-03-25 07:06:53","title":"Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models","abstract":"Recent studies show that deployed deep learning (DL) models such as those of Tensor Flow Lite (TFLite) can be easily extracted from real-world applications and devices by attackers to generate many kinds of attacks like adversarial attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent the aforementioned threats. Traditional software protection techniques have been widely explored, if on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate the DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract the on-device model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing units, configures the computing units to a graph, and then generates customized code to implement and deploy the ML solution without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively.","sentences":["Recent studies show that deployed deep learning (DL) models such as those of Tensor Flow Lite (TFLite) can be easily extracted from real-world applications and devices by attackers to generate many kinds of attacks like adversarial attacks.","Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent the aforementioned threats.","Traditional software protection techniques have been widely explored, if on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing software protection techniques.","However, due to the complexity of DL models, there is no automatic method that can translate the DL models to pure code.","To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract the on-device model information and synthesize a customized executable program for a wide range of DL models.","CustomDLCoder first parses the DL model, extracts its backend computing units, configures the computing units to a graph, and then generates customized code to implement and deploy the ML solution without explicit model representation.","The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model.","In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process.","Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing.","Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively.","Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively."],"url":"http://arxiv.org/abs/2403.16479v1","category":"cs.SE"}
{"created":"2024-03-25 07:04:24","title":"Towards Cooperative Maneuver Planning in Mixed Traffic at Urban Intersections","abstract":"Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas. Apart from sharing of awareness and perception information over wireless communication links, cooperative maneuver planning may facilitate active guidance of connected automated vehicles at urban intersections. Research in automatic intersection management put forth a large body of works that mostly employ rule-based or optimization-based approaches primarily in fully automated simulated environments. In this work, we present two cooperative planning approaches that are capable of handling mixed traffic, i.e., the road being shared by automated vehicles and regular vehicles driven by humans. Firstly, we propose an optimization-based planner trained on real driving data that cyclically selects the most efficient out of multiple predicted coordinated maneuvers. Additionally, we present a cooperative planning approach based on graph-based reinforcement learning, which conquers the lack of ground truth data for cooperative maneuvers. We present evaluation results of both cooperative planners in high-fidelity simulation and real-world traffic. Simulative experiments in fully automated traffic and mixed traffic show that cooperative maneuver planning leads to less delay due to interaction and a reduced number of stops. In real-world experiments with three prototype connected automated vehicles in public traffic, both planners demonstrate their ability to perform efficient cooperative maneuvers.","sentences":["Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas.","Apart from sharing of awareness and perception information over wireless communication links, cooperative maneuver planning may facilitate active guidance of connected automated vehicles at urban intersections.","Research in automatic intersection management put forth a large body of works that mostly employ rule-based or optimization-based approaches primarily in fully automated simulated environments.","In this work, we present two cooperative planning approaches that are capable of handling mixed traffic, i.e., the road being shared by automated vehicles and regular vehicles driven by humans.","Firstly, we propose an optimization-based planner trained on real driving data that cyclically selects the most efficient out of multiple predicted coordinated maneuvers.","Additionally, we present a cooperative planning approach based on graph-based reinforcement learning, which conquers the lack of ground truth data for cooperative maneuvers.","We present evaluation results of both cooperative planners in high-fidelity simulation and real-world traffic.","Simulative experiments in fully automated traffic and mixed traffic show that cooperative maneuver planning leads to less delay due to interaction and a reduced number of stops.","In real-world experiments with three prototype connected automated vehicles in public traffic, both planners demonstrate their ability to perform efficient cooperative maneuvers."],"url":"http://arxiv.org/abs/2403.16478v1","category":"cs.RO"}
{"created":"2024-03-25 07:04:16","title":"Safeguarding Next Generation Multiple Access Using Physical Layer Security Techniques: A Tutorial","abstract":"Driven by the ever-increasing requirements of ultra-high spectral efficiency, ultra-low latency, and massive connectivity, the forefront of wireless research calls for the design of advanced next generation multiple access schemes to facilitate provisioning of these stringent demands. This inspires the embrace of non-orthogonal multiple access (NOMA) in future wireless communication networks. Nevertheless, the support of massive access via NOMA leads to additional security threats, due to the open nature of the air interface, the broadcast characteristic of radio propagation as well as intertwined relationship among paired NOMA users. To address this specific challenge, the superimposed transmission of NOMA can be explored as new opportunities for security aware design, for example, multiuser interference inherent in NOMA can be constructively engineered to benefit communication secrecy and privacy. The purpose of this tutorial is to provide a comprehensive overview on the state-of-the-art physical layer security techniques that guarantee wireless security and privacy for NOMA networks, along with the opportunities, technical challenges, and future research trends.","sentences":["Driven by the ever-increasing requirements of ultra-high spectral efficiency, ultra-low latency, and massive connectivity, the forefront of wireless research calls for the design of advanced next generation multiple access schemes to facilitate provisioning of these stringent demands.","This inspires the embrace of non-orthogonal multiple access (NOMA) in future wireless communication networks.","Nevertheless, the support of massive access via NOMA leads to additional security threats, due to the open nature of the air interface, the broadcast characteristic of radio propagation as well as intertwined relationship among paired NOMA users.","To address this specific challenge, the superimposed transmission of NOMA can be explored as new opportunities for security aware design, for example, multiuser interference inherent in NOMA can be constructively engineered to benefit communication secrecy and privacy.","The purpose of this tutorial is to provide a comprehensive overview on the state-of-the-art physical layer security techniques that guarantee wireless security and privacy for NOMA networks, along with the opportunities, technical challenges, and future research trends."],"url":"http://arxiv.org/abs/2403.16477v1","category":"cs.IT"}
{"created":"2024-03-25 06:57:30","title":"Sweeping Arrangements of Non-Piercing Curves in Plane","abstract":"Let $\\Gamma$ be a finite set of Jordan curves in the plane. For any curve $\\gamma \\in \\Gamma$, we denote the bounded region enclosed by $\\gamma$ as $\\tilde{\\gamma}$. We say that $\\Gamma$ is a non-piercing family if for any two curves $\\alpha , \\beta \\in \\Gamma$, $\\tilde{\\alpha} \\setminus \\tilde{\\beta}$ is a connected region. A non-piercing family of curves generalizes a family of $2$-intersecting curves in which each pair of curves intersect in at most two points. Snoeyink and Hershberger (``Sweeping Arrangements of Curves'', SoCG '89) proved that if we are given a family $\\mathcal{C}$ of $2$-intersecting curves and a fixed curve $C\\in\\mathcal{C}$, then the arrangement can be \\emph{swept} by $C$, i.e., $C$ can be continuously shrunk to any point $p \\in \\tilde{C}$ in such a way that the we have a family of $2$-intersecting curves throughout the process. In this paper, we generalize the result of Snoeyink and Hershberger to the setting of non-piercing curves. We show that given an arrangement of non-piercing curves $\\Gamma$, and a fixed curve $\\gamma\\in \\Gamma$, the arrangement can be swept by $\\gamma$ so that the arrangement remains non-piercing throughout the process. We also give a shorter and simpler proof of the result of Snoeyink and Hershberger and cite applications of their result, where our result leads to a generalization.","sentences":["Let $\\Gamma$ be a finite set of Jordan curves in the plane.","For any curve $\\gamma \\in \\Gamma$, we denote the bounded region enclosed by $\\gamma$ as $\\tilde{\\gamma}$. We say that $\\Gamma$ is a non-piercing family if for any two curves $\\alpha , \\beta \\in \\Gamma$, $\\tilde{\\alpha} \\setminus \\tilde{\\beta}$ is a connected region.","A non-piercing family of curves generalizes a family of $2$-intersecting curves in which each pair of curves intersect in at most two points.","Snoeyink and Hershberger (``Sweeping Arrangements of Curves'', SoCG '89) proved that if we are given a family $\\mathcal{C}$ of $2$-intersecting curves and a fixed curve $C\\in\\mathcal{C}$, then the arrangement can be \\emph{swept} by $C$, i.e., $C$ can be continuously shrunk to any point $p \\in \\tilde{C}$ in such a way that the we have a family of $2$-intersecting curves throughout the process.","In this paper, we generalize the result of Snoeyink and Hershberger to the setting of non-piercing curves.","We show that given an arrangement of non-piercing curves $\\Gamma$, and a fixed curve $\\gamma\\in \\Gamma$, the arrangement can be swept by $\\gamma$ so that the arrangement remains non-piercing throughout the process.","We also give a shorter and simpler proof of the result of Snoeyink and Hershberger and cite applications of their result, where our result leads to a generalization."],"url":"http://arxiv.org/abs/2403.16474v1","category":"cs.CG"}
{"created":"2024-03-25 06:56:27","title":"Power-Aware Sparse Reflect Beamforming in Active RIS-aided Interference Channels","abstract":"Active reconfigurable intelligent surface (RIS) has attracted significant attention in wireless communications, due to its reflecting elements (REs) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications. In this paper, we are interested in active RIS-aided interference channels in which $K$ user pairs share the same time and frequency resources with the aid of active RIS. Thanks to the promising amplitude amplification capability, activating a moderate number of REs, rather than all of them, is sufficient for the active RIS to mitigate cross-channel interferences. Motivated by this, we propose a power-aware sparse reflect beamforming design for the active RIS-aided interference channels, which allows the active RIS to flexibly adjust the number of activated REs for the sake of reducing hardware and power costs. Specifically, we establish the power consumption model in which only those activated REs consume the biasing and operation power that supports the amplitude amplification, yielding an $\\ell_0$-norm power consumption function. Based on the proposed model, we investigate a sum-rate maximization problem and an active RIS power minimization problem by carefully designing the sparse reflect beamforming vector. To solve these problems, we first replace the nonconvex $\\ell_0$-norm function with an iterative reweighted $\\ell_1$-norm function. Then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (DCA) is used to solve the active RIS power minimization. Numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active RIS in interference channels.","sentences":["Active reconfigurable intelligent surface (RIS) has attracted significant attention in wireless communications, due to its reflecting elements (REs) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications.","In this paper, we are interested in active RIS-aided interference channels in which $K$ user pairs share the same time and frequency resources with the aid of active RIS.","Thanks to the promising amplitude amplification capability, activating a moderate number of REs, rather than all of them, is sufficient for the active RIS to mitigate cross-channel interferences.","Motivated by this, we propose a power-aware sparse reflect beamforming design for the active RIS-aided interference channels, which allows the active RIS to flexibly adjust the number of activated REs for the sake of reducing hardware and power costs.","Specifically, we establish the power consumption model in which only those activated REs consume the biasing and operation power that supports the amplitude amplification, yielding an $\\ell_0$-norm power consumption function.","Based on the proposed model, we investigate a sum-rate maximization problem and an active RIS power minimization problem by carefully designing the sparse reflect beamforming vector.","To solve these problems, we first replace the nonconvex $\\ell_0$-norm function with an iterative reweighted $\\ell_1$-norm function.","Then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (DCA) is used to solve the active RIS power minimization.","Numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active RIS in interference channels."],"url":"http://arxiv.org/abs/2403.16472v1","category":"cs.IT"}
{"created":"2024-03-25 06:46:27","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator","abstract":"A generative adversarial network (GAN)-based vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics. However, this data-driven model requires a large amount of training data incurring high data-collection costs. This fact motivates us to train a GAN-based vocoder on limited data. A promising solution is to augment the training data to avoid overfitting. However, a standard discriminator is unconditional and insensitive to distributional changes caused by data augmentation. Thus, augmented speech (which can be extraordinary) may be considered real speech. To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmented distribution. Experimental results indicate that AugCondD improves speech quality under limited data conditions while achieving comparable speech quality under sufficient data conditions. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.","sentences":["A generative adversarial network (GAN)-based vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics.","However, this data-driven model requires a large amount of training data incurring high data-collection costs.","This fact motivates us to train a GAN-based vocoder on limited data.","A promising solution is to augment the training data to avoid overfitting.","However, a standard discriminator is unconditional and insensitive to distributional changes caused by data augmentation.","Thus, augmented speech (which can be extraordinary) may be considered real speech.","To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmented distribution.","Experimental results indicate that AugCondD improves speech quality under limited data conditions while achieving comparable speech quality under sufficient data conditions.","Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/."],"url":"http://arxiv.org/abs/2403.16464v1","category":"cs.SD"}
{"created":"2024-03-25 06:45:09","title":"Few-shot Named Entity Recognition via Superposition Concept Discrimination","abstract":"Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models. To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases. Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts.","sentences":["Few-shot NER aims to identify entities of target types with only limited number of illustrative instances.","Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency.","In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm.","Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary.","Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus.","Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models.","To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases.","Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts."],"url":"http://arxiv.org/abs/2403.16463v1","category":"cs.CL"}
{"created":"2024-03-25 06:43:28","title":"FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data","abstract":"Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments. Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods.","sentences":["Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training.","However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness.","In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments.","Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods."],"url":"http://arxiv.org/abs/2403.16460v1","category":"cs.LG"}
{"created":"2024-03-25 06:41:25","title":"Next Generation Advanced Transceiver Technologies for 6G","abstract":"To accommodate new applications such as extended reality, fully autonomous vehicular networks and the metaverse, next generation wireless networks are going to be subject to much more stringent performance requirements than the fifth-generation (5G) in terms of data rates, reliability, latency, and connectivity. It is thus necessary to develop next generation advanced transceiver (NGAT) technologies for efficient signal transmission and reception. In this tutorial, we explore the evolution of NGAT from three different perspectives. Specifically, we first provide an overview of new-field NGAT technology, which shifts from conventional far-field channel models to new near-field channel models. Then, three new-form NGAT technologies and their design challenges are presented, including reconfigurable intelligent surfaces, flexible antennas, and holographic multi-input multi-output (MIMO) systems. Subsequently, we discuss recent advances in semantic-aware NGAT technologies, which can utilize new metrics for advanced transceiver designs. Finally, we point out other promising transceiver technologies for future research.","sentences":["To accommodate new applications such as extended reality, fully autonomous vehicular networks and the metaverse, next generation wireless networks are going to be subject to much more stringent performance requirements than the fifth-generation (5G) in terms of data rates, reliability, latency, and connectivity.","It is thus necessary to develop next generation advanced transceiver (NGAT) technologies for efficient signal transmission and reception.","In this tutorial, we explore the evolution of NGAT from three different perspectives.","Specifically, we first provide an overview of new-field NGAT technology, which shifts from conventional far-field channel models to new near-field channel models.","Then, three new-form NGAT technologies and their design challenges are presented, including reconfigurable intelligent surfaces, flexible antennas, and holographic multi-input multi-output (MIMO) systems.","Subsequently, we discuss recent advances in semantic-aware NGAT technologies, which can utilize new metrics for advanced transceiver designs.","Finally, we point out other promising transceiver technologies for future research."],"url":"http://arxiv.org/abs/2403.16458v1","category":"cs.IT"}
{"created":"2024-03-25 06:36:10","title":"Flux Quantization on 11-dimensional Superspace","abstract":"Flux quantization of the C-field in 11d supergravity is arguably necessary for the (UV-)completion of the theory, in that it determines the torsion charges carried by small numbers of M-branes. However, hypotheses about C-field flux-quantization (\"models of the C-field\") have previously been discussed only in the bosonic sector of 11d supergravity and ignoring the supergravity equations of motion. Here we highlight a duality-symmetric formulation of on-shell 11d supergravity on superspace, observe that this naturally lends itself to completion of the theory by flux quantization, and indeed that 11d super-spacetimes are put on-shell by carrying quantizable duality-symmetric super-C-field flux; the proof of which we present in detail.","sentences":["Flux quantization of the C-field in 11d supergravity is arguably necessary for the (UV-)completion of the theory, in that it determines the torsion charges carried by small numbers of M-branes.","However, hypotheses about C-field flux-quantization (\"models of the C-field\") have previously been discussed only in the bosonic sector of 11d supergravity and ignoring the supergravity equations of motion.","Here we highlight a duality-symmetric formulation of on-shell 11d supergravity on superspace, observe that this naturally lends itself to completion of the theory by flux quantization, and indeed that 11d super-spacetimes are put on-shell by carrying quantizable duality-symmetric super-C-field flux; the proof of which we present in detail."],"url":"http://arxiv.org/abs/2403.16456v1","category":"hep-th"}
{"created":"2024-03-25 06:33:23","title":"Photon orbits and phase transition for Letelier AdS black holes immersed in perfect fluid dark matter","abstract":"We obtain an exact solution of spherically symmetric Letelier AdS black holes immersed in perfect fluid dark matter (PFDM). Considering the cosmological constant as the positive pressure of the system and volume as its conjugate variable, we analyse the thermodynamics of our black holes in the extended phase space. Owing to the background clouds of strings parameter ($a$) and the parameter endowed with PFDM ($\\beta$), we analyse the Hawking temperature, entropy and specific heat. We also investigate the relationship between the photon sphere radius and the phase transition for the Letelier AdS black holes immersed in PFDM. Through the analysis, we find with a particular condition, there are non-monotonic behaviours between the photon sphere radius, the impact parameter, the PFDM parameter, temperature, and pressure. We can regard both the changes of photon sphere radius and impact parameter before and after phase transition as the order parameter; their critical exponents near the critical point are equal to the same value 1/2, just like ordinary thermal systems. These indicate that a universal relation of gravity may exist near the critical point for a black hole thermodynamic system.","sentences":["We obtain an exact solution of spherically symmetric Letelier AdS black holes immersed in perfect fluid dark matter (PFDM).","Considering the cosmological constant as the positive pressure of the system and volume as its conjugate variable, we analyse the thermodynamics of our black holes in the extended phase space.","Owing to the background clouds of strings parameter ($a$) and the parameter endowed with PFDM ($\\beta$), we analyse the Hawking temperature, entropy and specific heat.","We also investigate the relationship between the photon sphere radius and the phase transition for the Letelier AdS black holes immersed in PFDM.","Through the analysis, we find with a particular condition, there are non-monotonic behaviours between the photon sphere radius, the impact parameter, the PFDM parameter, temperature, and pressure.","We can regard both the changes of photon sphere radius and impact parameter before and after phase transition as the order parameter; their critical exponents near the critical point are equal to the same value 1/2, just like ordinary thermal systems.","These indicate that a universal relation of gravity may exist near the critical point for a black hole thermodynamic system."],"url":"http://arxiv.org/abs/2403.16454v1","category":"gr-qc"}
{"created":"2024-03-25 06:31:13","title":"Determinants of Uruguay's Real Effective Exchange Rate: A Mundell-Fleming Model Approach","abstract":"This study examines the factors influencing the short-term real effective exchange rate (REER) in Uruguay by applying an extended Mundell-Fleming model. Analyzing the impact of the US lending rate (USLR), money supply (M2), inflation (CPI), and the world interest rate (WIR), the paper uses a linear regression model with Newey-West standard errors. Key findings reveal that an increase in the USLR, CPI, and M2 is associated with a depreciation of the REER. In contrast, WIR shows no significant impact. These findings are consistent with the theoretical expectations of the Mundell-Fleming model regarding open economies under floating exchange rates. Therefore, authorities should tighten monetary policy, control inflation, adjust fiscal strategies, and boost exports in response to Peso depreciation.","sentences":["This study examines the factors influencing the short-term real effective exchange rate (REER) in Uruguay by applying an extended Mundell-Fleming model.","Analyzing the impact of the US lending rate (USLR), money supply (M2), inflation (CPI), and the world interest rate (WIR), the paper uses a linear regression model with Newey-West standard errors.","Key findings reveal that an increase in the USLR, CPI, and M2 is associated with a depreciation of the REER.","In contrast, WIR shows no significant impact.","These findings are consistent with the theoretical expectations of the Mundell-Fleming model regarding open economies under floating exchange rates.","Therefore, authorities should tighten monetary policy, control inflation, adjust fiscal strategies, and boost exports in response to Peso depreciation."],"url":"http://arxiv.org/abs/2403.16452v1","category":"econ.GN"}
{"created":"2024-03-25 06:30:54","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","abstract":"We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.","sentences":["We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations.","We have built and evaluated DeepMachining based on manufacturing data from factories.","Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states.","Then, we fine-tune the pretrained model to adapt to specific machining tasks.","We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools.","To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines."],"url":"http://arxiv.org/abs/2403.16451v1","category":"cs.LG"}
{"created":"2024-03-25 06:22:27","title":"Camera-aware Label Refinement for Unsupervised Person Re-identification","abstract":"Unsupervised person re-identification aims to retrieve images of a specified person without identity labels. Many recent unsupervised Re-ID approaches adopt clustering-based methods to measure cross-camera feature similarity to roughly divide images into clusters. They ignore the feature distribution discrepancy induced by camera domain gap, resulting in the unavoidable performance degradation. Camera information is usually available, and the feature distribution in the single camera usually focuses more on the appearance of the individual and has less intra-identity variance. Inspired by the observation, we introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel \\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by clustering intra-camera similarity. Specifically, we employ intra-camera training to obtain reliable local pseudo labels within each camera, and then refine global labels generated by inter-camera clustering and train the discriminative model using more reliable global pseudo labels in a self-paced manner. Meanwhile, we develop a camera-alignment module to align feature distributions under different cameras, which could help deal with the camera variance further. Extensive experiments validate the superiority of our proposed method over state-of-the-art approaches. The code is accessible at https://github.com/leeBooMla/CALR.","sentences":["Unsupervised person re-identification aims to retrieve images of a specified person without identity labels.","Many recent unsupervised Re-ID approaches adopt clustering-based methods to measure cross-camera feature similarity to roughly divide images into clusters.","They ignore the feature distribution discrepancy induced by camera domain gap, resulting in the unavoidable performance degradation.","Camera information is usually available, and the feature distribution in the single camera usually focuses more on the appearance of the individual and has less intra-identity variance.","Inspired by the observation, we introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel \\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by clustering intra-camera similarity.","Specifically, we employ intra-camera training to obtain reliable local pseudo labels within each camera, and then refine global labels generated by inter-camera clustering and train the discriminative model using more reliable global pseudo labels in a self-paced manner.","Meanwhile, we develop a camera-alignment module to align feature distributions under different cameras, which could help deal with the camera variance further.","Extensive experiments validate the superiority of our proposed method over state-of-the-art approaches.","The code is accessible at https://github.com/leeBooMla/CALR."],"url":"http://arxiv.org/abs/2403.16450v1","category":"cs.CV"}
{"created":"2024-03-25 06:19:47","title":"Multinomial random combinatorial structures and $r$-versions of Stirling, Eulerian and Lah numbers","abstract":"We introduce multinomial and $r$-variants of several classic objects of combinatorial probability, such as the random recursive and Hoppe trees, random set partitions and compositions, the Chinese restaurant process, Feller's coupling, and some others. Just as various classic combinatorial numbers - like Stirling, Eulerian and Lah numbers - emerge as essential ingredients defining the distributions of the mentioned processes, the so-called $r$-versions of these numbers appear in exact distributional formulas for the multinomial and $r$-counterparts. This approach allows us to offer a concise probabilistic interpretation for various identities involving $r$-versions of these combinatorial numbers, which were either unavailable or meaningful only for specific values of the parameter $r$. We analyze the derived distributions for fixed-size structures and establish distributional limit theorems as the size tends to infinity. Utilizing the aforementioned generalized Stirling numbers of both kinds, we define and analyze $(r,s)$-Lah distributions, which have arisen in the existing literature on combinatorial probability in various contexts.","sentences":["We introduce multinomial and $r$-variants of several classic objects of combinatorial probability, such as the random recursive and Hoppe trees, random set partitions and compositions, the Chinese restaurant process, Feller's coupling, and some others.","Just as various classic combinatorial numbers - like Stirling, Eulerian and Lah numbers - emerge as essential ingredients defining the distributions of the mentioned processes, the so-called $r$-versions of these numbers appear in exact distributional formulas for the multinomial and $r$-counterparts.","This approach allows us to offer a concise probabilistic interpretation for various identities involving $r$-versions of these combinatorial numbers, which were either unavailable or meaningful only for specific values of the parameter $r$. We analyze the derived distributions for fixed-size structures and establish distributional limit theorems as the size tends to infinity.","Utilizing the aforementioned generalized Stirling numbers of both kinds, we define and analyze $(r,s)$-Lah distributions, which have arisen in the existing literature on combinatorial probability in various contexts."],"url":"http://arxiv.org/abs/2403.16448v1","category":"math.PR"}
{"created":"2024-03-25 06:09:55","title":"CodeS: Natural Language to Code Repository via Multi-Layer Sketch","abstract":"The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.","sentences":["The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development.","In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo).","This task aims to generate an entire code repository from its natural language requirements.","To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch.","Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller.","RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch.","To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis.","For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU.","For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies.","Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task."],"url":"http://arxiv.org/abs/2403.16443v1","category":"cs.CL"}
{"created":"2024-03-25 06:05:50","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions","abstract":"Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.","sentences":["Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape.","However, it is unclear to what extent VLMs prioritize this information to represent concepts.","We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs.","EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM.","Then, we inspect the descriptions to identify the features that contribute to VLM representations.","We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT.","More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts.","Also, our analysis reveals that different VLMs prioritize different attributes in their representations.","Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations."],"url":"http://arxiv.org/abs/2403.16442v1","category":"cs.CL"}
{"created":"2024-03-25 05:37:16","title":"Evaluating Large Language Models with Runtime Behavior of Program Execution","abstract":"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs.","sentences":["Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities.","To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval).","Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient.","Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning.","To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution.","We utilize existing code benchmarks and adapt them to new benchmarks within our framework.","A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3).","Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs."],"url":"http://arxiv.org/abs/2403.16437v1","category":"cs.SE"}
{"created":"2024-03-25 05:27:35","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models","abstract":"Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by \\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo.","sentences":["Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks.","Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization.","Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm.","Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm.","However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses.","In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens.","Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by \\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo."],"url":"http://arxiv.org/abs/2403.16432v1","category":"cs.CL"}
{"created":"2024-03-25 05:22:34","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding","abstract":"Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously. Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks. This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects. In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner. Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship. In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks. A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training. Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset. Code is available at https://github.com/SAITPublic/DOCTR.","sentences":["Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously.","Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks.","This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects.","In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner.","Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship.","In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks.","A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training.","Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset.","Code is available at https://github.com/SAITPublic/DOCTR."],"url":"http://arxiv.org/abs/2403.16431v1","category":"cs.CV"}
{"created":"2024-03-25 05:12:21","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects","abstract":"We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views. Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects. Our efforts will enrich the community's knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.","sentences":["We interact with the world with our hands and see it through our own (egocentric) perspective.","A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation.","Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement.","To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits.","Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks.","Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views.","Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects.","Our efforts will enrich the community's knowledge foundation and facilitate future hand studies on egocentric hand-object interactions."],"url":"http://arxiv.org/abs/2403.16428v1","category":"cs.CV"}
{"created":"2024-03-25 05:12:18","title":"Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation","abstract":"Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \\underline{Re}flective \\underline{Re}inforcement \\underline{L}arge \\underline{L}anguage \\underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs.   To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors.   To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent.   It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations. Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.","sentences":["Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   ","However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   ","Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   ","To address such issues, we propose a \\underline{Re}flective \\underline{Re}inforcement \\underline{L}arge \\underline{L}anguage \\underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   ","In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs.   ","To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors.   ","To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent.   ","It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations.","Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16427v1","category":"cs.AI"}
{"created":"2024-03-25 05:04:52","title":"An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations","abstract":"This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.","sentences":["This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH).","The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries.","The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness.","The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH.","Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs."],"url":"http://arxiv.org/abs/2403.16424v1","category":"cs.AI"}
{"created":"2024-03-25 05:02:36","title":"Canonical Quantization of the U(1) Gauge Field in the Rindler Coordinates","abstract":"This paper describes the canonical quantization of the U(1) gauge field across all four regions in the Rindler coordinates in the Lorentz covariant gauge. In the four regions (future, past, left and right Rindler-wedges) in the Rindler coordinates, defining the gauge-fixed Lagrangian in the Lorentz covariant gauge, which is composed of the U(1) gauge field, the $B$-field and ghost fields. Since the U(1) gauge and $B$-fields are decoupled from the ghost fields by the property of the U(1) gauge theory, the U(1) gauge field and the $B$-field are examined in this study.   Then, by solving the equations of motion obtained from the gauge-fixed Lagrangian, the solutions of each mode of the U(1) gauge field and the $B$-field can be obtained. Following this, with the Klein-Gordon inner-product defined in the Rindler coordinates, the normalization constants of each of those mode-solutions are determined.   Subsequently, formulating the canonical commutation relations of the U(1) gauge field and its canonical conjugate momentum, the commutation relations of the coefficient of each mode-solution in each direction of the U(1) gauge field in each region of the Rindler coordinates are obtained. From these, it can be seen that those coefficients have physical meaning as creation/annihilation operators. The polarization vectors in each region of the Rindler coordinates are also given in this study.   From these, it is shown that the Minkowski ground state can be given as the outer-product of the quantum states multiplied by the creation operators of the U(1) gauge field in the left and right Rindler-wedge states.   Then, obtaining the density matrix of the U(1) gauge theory in the right Rindler-wedge from that, it is shown that the U(1) gauge field in the constant accelerated system feels the Unruh temperature as well.","sentences":["This paper describes the canonical quantization of the U(1) gauge field across all four regions in the Rindler coordinates in the Lorentz covariant gauge.","In the four regions (future, past, left and right Rindler-wedges) in the Rindler coordinates, defining the gauge-fixed Lagrangian in the Lorentz covariant gauge, which is composed of the U(1) gauge field, the $B$-field and ghost fields.","Since the U(1) gauge and $B$-fields are decoupled from the ghost fields by the property of the U(1) gauge theory, the U(1) gauge field and the $B$-field are examined in this study.   ","Then, by solving the equations of motion obtained from the gauge-fixed Lagrangian, the solutions of each mode of the U(1) gauge field and the $B$-field can be obtained.","Following this, with the Klein-Gordon inner-product defined in the Rindler coordinates, the normalization constants of each of those mode-solutions are determined.   ","Subsequently, formulating the canonical commutation relations of the U(1) gauge field and its canonical conjugate momentum, the commutation relations of the coefficient of each mode-solution in each direction of the U(1) gauge field in each region of the Rindler coordinates are obtained.","From these, it can be seen that those coefficients have physical meaning as creation/annihilation operators.","The polarization vectors in each region of the Rindler coordinates are also given in this study.   ","From these, it is shown that the Minkowski ground state can be given as the outer-product of the quantum states multiplied by the creation operators of the U(1) gauge field in the left and right Rindler-wedge states.   ","Then, obtaining the density matrix of the U(1) gauge theory in the right Rindler-wedge from that, it is shown that the U(1) gauge field in the constant accelerated system feels the Unruh temperature as well."],"url":"http://arxiv.org/abs/2403.16423v1","category":"hep-th"}
{"created":"2024-03-25 04:54:49","title":"Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation","abstract":"Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research. We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches. We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, OCR precision, recall, F1 score, accuracy, and edit distance scores. For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval, respectively. Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature","sentences":["Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention.","However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images.","The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications.","To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout.","Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research.","We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text.","Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches.","We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, OCR precision, recall, F1 score, accuracy, and edit distance scores.","For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval, respectively.","Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature"],"url":"http://arxiv.org/abs/2403.16422v1","category":"cs.CV"}
{"created":"2024-03-25 04:50:02","title":"Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling","abstract":"This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo simulations. We used the LiteX framework to generate a Petitbateau FemtoRV RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board. The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different benchmark applications when compared to GNU Scientific Library software random number generation. These speedups are achievable because the benchmarks spend an average of 90.0 % of their execution time generating random samples. The results of the Monte Carlo benchmark programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times$that of the results produced by the GNU Scientific Library random number generators. The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA. The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC.","sentences":["This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo simulations.","We used the LiteX framework to generate a Petitbateau FemtoRV RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board.","The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different benchmark applications when compared to GNU Scientific Library software random number generation.","These speedups are achievable because the benchmarks spend an average of 90.0 % of their execution time generating random samples.","The results of the Monte Carlo benchmark programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times$that of the results produced by the GNU Scientific Library random number generators.","The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA.","The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC."],"url":"http://arxiv.org/abs/2403.16421v1","category":"cs.AR"}
{"created":"2024-03-25 04:47:44","title":"Real-Time Recognition of Vortex Beams Modes Through Random Diffusive at the Speed of Light","abstract":"Optical vortex beam with orbital angular momentum (OAM) has great potential to increase the capacity of optical communication and information processing in classical and quantum regimes. Nevertheless, important challenges that influence the optical data transmission in free space is the existence of diffusers along the optical path, which causes inevitable information loss during the wave propagation. Numerous algorithms have been proposed successively for identifying the modes of vortex beams propagating through scattering media. However, these methods all require completion on a computer, which is energyintensive and energy consuming. Here, we propose an all-optical regime for identifying the modes of vortex light fields propagating through scattering media. After training by deep learning, our model can recognize the mode of vortex beam through unknown phase diffusers, demonstrating generalization to new random diffusers that have never been encountered before. Once physically deployed, the entire setup will rapidly identify the modes of vortex light propagating through scattering media at the speed of light, and the entire inference process will consume zero energy except for illumination source. Our research represents a significant step towards highly accurate recognition of vortex light modes propagating through complex scattering media, providing significant guidance for the application of optical communication in complex environments.","sentences":["Optical vortex beam with orbital angular momentum (OAM) has great potential to increase the capacity of optical communication and information processing in classical and quantum regimes.","Nevertheless, important challenges that influence the optical data transmission in free space is the existence of diffusers along the optical path, which causes inevitable information loss during the wave propagation.","Numerous algorithms have been proposed successively for identifying the modes of vortex beams propagating through scattering media.","However, these methods all require completion on a computer, which is energyintensive and energy consuming.","Here, we propose an all-optical regime for identifying the modes of vortex light fields propagating through scattering media.","After training by deep learning, our model can recognize the mode of vortex beam through unknown phase diffusers, demonstrating generalization to new random diffusers that have never been encountered before.","Once physically deployed, the entire setup will rapidly identify the modes of vortex light propagating through scattering media at the speed of light, and the entire inference process will consume zero energy except for illumination source.","Our research represents a significant step towards highly accurate recognition of vortex light modes propagating through complex scattering media, providing significant guidance for the application of optical communication in complex environments."],"url":"http://arxiv.org/abs/2403.16420v1","category":"physics.optics"}
{"created":"2024-03-25 04:43:47","title":"An incremental MaxSAT-based model to learn balanced rules","abstract":"The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset. Finally, IMLIB and IMLI are compared using diverse databases. IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes.","sentences":["The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions.","However, in certain cases, accuracy alone may not be sufficient.","Many real-world problems also demand explanations and interpretability behind the predictions.","One of the most popular interpretable models that are classification rules.","This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB.","This new model was based on two other approaches, one based on SAT and the other on MaxSAT.","The one based on SAT limits the size of each generated rule, making it possible to balance them.","We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules.","The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset.","Finally, IMLIB and IMLI are compared using diverse databases.","IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes."],"url":"http://arxiv.org/abs/2403.16418v1","category":"cs.LG"}
{"created":"2024-03-25 04:34:20","title":"Leveraging Large Language Model to Generate a Novel Metaheuristic Algorithm with CRISPE Framework","abstract":"In this paper, we borrow the large language model (LLM) ChatGPT-3.5 to automatically and quickly design a new metaheuristic algorithm (MA) with only a small amount of input. The novel animal-inspired MA named zoological search optimization (ZSO) draws inspiration from the collective behaviors of animals for solving continuous optimization problems. Specifically, the basic ZSO algorithm involves two search operators: the prey-predator interaction operator and the social flocking operator to balance exploration and exploitation well. Besides, the standard prompt engineering framework CRISPE (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment) is responsible for the specific prompt design. Furthermore, we designed four variants of the ZSO algorithm with slight human-interacted adjustment. In numerical experiments, we comprehensively investigate the performance of ZSO-derived algorithms on CEC2014 benchmark functions, CEC2022 benchmark functions, and six engineering optimization problems. 20 popular and state-of-the-art MAs are employed as competitors. The experimental results and statistical analysis confirm the efficiency and effectiveness of ZSO-derived algorithms. At the end of this paper, we explore the prospects for the development of the metaheuristics community under the LLM era.","sentences":["In this paper, we borrow the large language model (LLM) ChatGPT-3.5 to automatically and quickly design a new metaheuristic algorithm (MA) with only a small amount of input.","The novel animal-inspired MA named zoological search optimization (ZSO) draws inspiration from the collective behaviors of animals for solving continuous optimization problems.","Specifically, the basic ZSO algorithm involves two search operators: the prey-predator interaction operator and the social flocking operator to balance exploration and exploitation well.","Besides, the standard prompt engineering framework CRISPE (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment) is responsible for the specific prompt design.","Furthermore, we designed four variants of the ZSO algorithm with slight human-interacted adjustment.","In numerical experiments, we comprehensively investigate the performance of ZSO-derived algorithms on CEC2014 benchmark functions, CEC2022 benchmark functions, and six engineering optimization problems.","20 popular and state-of-the-art MAs are employed as competitors.","The experimental results and statistical analysis confirm the efficiency and effectiveness of ZSO-derived algorithms.","At the end of this paper, we explore the prospects for the development of the metaheuristics community under the LLM era."],"url":"http://arxiv.org/abs/2403.16417v1","category":"cs.NE"}
{"created":"2024-03-25 04:21:06","title":"How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation","abstract":"Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time. CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on LLMs. While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results. (2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single prompt template proves challenging. To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items. Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results.","sentences":["Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time.","CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus.","Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields.","Simultaneously, efforts are underway to construct user simulators based on LLMs.","While these works showcase innovation, they also come with certain limitations that require attention.","In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research.","To achieve this goal, we conduct analytical validation on the notable work, iEvaLM.","Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results.","(2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators.","(3) Controlling the output of the user simulator through a single prompt template proves challenging.","To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items.","Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results."],"url":"http://arxiv.org/abs/2403.16416v1","category":"cs.AI"}
{"created":"2024-03-25 04:17:31","title":"Generation of $\u03b3$-photons and pairs with transverse orbital angular momentum via spatiotemporal optical vortex pulse","abstract":"We present the generation of well-collimated $\\gamma$-photons and pairs with extrinsic transverse orbital angular momentum (TOAM) through the head-on collision of an intense spatiotemporal optical vortex (STOV) pulse carrying intrinsic TOAM with a high-energy electron beam. It is found that the TOAM of STOV pulse remains almost unchanged, and the TOAM is conserved in the center-of-mass frame (CMF). Moreover, there exhibits duality for particles TOAM in the CMF and laboratory frame (LF) when the initial location of high-energy electron beam is different. Furthermore, the TOAM of $\\gamma$-photons in the CMF increases while that of positrons decreases as the topological charge of STOV pulse increases, whereas in the LF, the TOAM of both $\\gamma$-photons and positrons decreases. And the result under the same pulse intensity is better than that under the same pulse energy. The increase in the initial energy of high-energy electrons leads to an enhancement of the TOAM for both $\\gamma$-photons and positrons in both frames. $\\gamma$-photons and electrons/positrons with TOAM as a new degree of freedom maybe have an extensive applications in optical communication, astrophysics and nanomaterials and so on.","sentences":["We present the generation of well-collimated $\\gamma$-photons and pairs with extrinsic transverse orbital angular momentum (TOAM) through the head-on collision of an intense spatiotemporal optical vortex (STOV) pulse carrying intrinsic TOAM with a high-energy electron beam.","It is found that the TOAM of STOV pulse remains almost unchanged, and the TOAM is conserved in the center-of-mass frame (CMF).","Moreover, there exhibits duality for particles TOAM in the CMF and laboratory frame (LF) when the initial location of high-energy electron beam is different.","Furthermore, the TOAM of $\\gamma$-photons in the CMF increases while that of positrons decreases as the topological charge of STOV pulse increases, whereas in the LF, the TOAM of both $\\gamma$-photons and positrons decreases.","And the result under the same pulse intensity is better than that under the same pulse energy.","The increase in the initial energy of high-energy electrons leads to an enhancement of the TOAM for both $\\gamma$-photons and positrons in both frames.","$\\gamma$-photons and electrons/positrons with TOAM as a new degree of freedom maybe have an extensive applications in optical communication, astrophysics and nanomaterials and so on."],"url":"http://arxiv.org/abs/2403.16414v1","category":"physics.optics"}
{"created":"2024-03-25 04:14:07","title":"Unsupervised Template-assisted Point Cloud Shape Correspondence Network","abstract":"Unsupervised point cloud shape correspondence aims to establish point-wise correspondences between source and target point clouds. Existing methods obtain correspondences directly by computing point-wise feature similarity between point clouds. However, non-rigid objects possess strong deformability and unusual shapes, making it a longstanding challenge to directly establish correspondences between point clouds with unconventional shapes. To address this challenge, we propose an unsupervised Template-Assisted point cloud shape correspondence Network, termed TANet, including a template generation module and a template assistance module. The proposed TANet enjoys several merits. Firstly, the template generation module establishes a set of learnable templates with explicit structures. Secondly, we introduce a template assistance module that extensively leverages the generated templates to establish more accurate shape correspondences from multiple perspectives. Extensive experiments on four human and animal datasets demonstrate that TANet achieves favorable performance against state-of-the-art methods.","sentences":["Unsupervised point cloud shape correspondence aims to establish point-wise correspondences between source and target point clouds.","Existing methods obtain correspondences directly by computing point-wise feature similarity between point clouds.","However, non-rigid objects possess strong deformability and unusual shapes, making it a longstanding challenge to directly establish correspondences between point clouds with unconventional shapes.","To address this challenge, we propose an unsupervised Template-Assisted point cloud shape correspondence Network, termed TANet, including a template generation module and a template assistance module.","The proposed TANet enjoys several merits.","Firstly, the template generation module establishes a set of learnable templates with explicit structures.","Secondly, we introduce a template assistance module that extensively leverages the generated templates to establish more accurate shape correspondences from multiple perspectives.","Extensive experiments on four human and animal datasets demonstrate that TANet achieves favorable performance against state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16412v1","category":"cs.CV"}
{"created":"2024-03-25 04:05:23","title":"Spike-NeRF: Neural Radiance Field Based On Spike Camera","abstract":"As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking. Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes. Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time. To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function. We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator. Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes. Our code and data will be released soon.","sentences":["As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking.","Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes.","Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time.","To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function.","We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator.","Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes.","Our code and data will be released soon."],"url":"http://arxiv.org/abs/2403.16410v1","category":"cs.CV"}
{"created":"2024-03-25 03:47:53","title":"A Survey on Long Video Generation: Challenges, Methods, and Prospects","abstract":"Video generation is a rapidly advancing research area, garnering significant attention due to its broad range of applications. One critical aspect of this field is the generation of long-duration videos, which presents unique challenges and opportunities. This paper presents the first survey of recent advancements in long video generation and summarises them into two key paradigms: divide and conquer temporal autoregressive.   We delve into the common models employed in each paradigm, including aspects of network design and conditioning techniques. Furthermore, we offer a comprehensive overview and classification of the datasets and evaluation metrics which are crucial for advancing long video generation research. Concluding with a summary of existing studies, we also discuss the emerging challenges and future directions in this dynamic field. We hope that this survey will serve as an essential reference for researchers and practitioners in the realm of long video generation.","sentences":["Video generation is a rapidly advancing research area, garnering significant attention due to its broad range of applications.","One critical aspect of this field is the generation of long-duration videos, which presents unique challenges and opportunities.","This paper presents the first survey of recent advancements in long video generation and summarises them into two key paradigms: divide and conquer temporal autoregressive.   ","We delve into the common models employed in each paradigm, including aspects of network design and conditioning techniques.","Furthermore, we offer a comprehensive overview and classification of the datasets and evaluation metrics which are crucial for advancing long video generation research.","Concluding with a summary of existing studies, we also discuss the emerging challenges and future directions in this dynamic field.","We hope that this survey will serve as an essential reference for researchers and practitioners in the realm of long video generation."],"url":"http://arxiv.org/abs/2403.16407v1","category":"cs.CV"}
{"created":"2024-03-25 03:38:06","title":"Douglas-Rudin Approximation theorem for operator-valued functions on the unit ball of $\\mathbb{C}^d$","abstract":"Douglas and Rudin proved that any unimodular function on the unit circle $\\T$ can be uniformly approximated by quotients of inner functions. We extend this result to the operator-valued unimodular functions defined on the boundary of the open unit ball of $\\mathbb{C}^d$. Our proof technique combines the spectral theorem for unitary operators with the Douglas-Rudin theorem in the scalar case to bootstrap the result to the operator-valued case. This yields a new proof and a significant generalization of Barclay's result [Proc. Lond. Math. Soc. 2009] on approximation of matrix-valued unimodular functions on $\\T$.","sentences":["Douglas and Rudin proved that any unimodular function on the unit circle $\\T$ can be uniformly approximated by quotients of inner functions.","We extend this result to the operator-valued unimodular functions defined on the boundary of the open unit ball of $\\mathbb{C}^d$. Our proof technique combines the spectral theorem for unitary operators with the Douglas-Rudin theorem in the scalar case to bootstrap the result to the operator-valued case.","This yields a new proof and a significant generalization of Barclay's result [Proc. Lond.","Math.","Soc. 2009] on approximation of matrix-valued unimodular functions on $\\T$."],"url":"http://arxiv.org/abs/2403.16401v1","category":"math.FA"}
{"created":"2024-03-25 03:26:01","title":"Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data","abstract":"Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating. To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.","sentences":["Federated learning achieves effective performance in modeling decentralized data.","In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data.","However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models.","The former indicates that representation collapse in local model will subsequently impact the global model and other local models.","The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals.","In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data.","Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA).","FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating.","To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100."],"url":"http://arxiv.org/abs/2403.16398v1","category":"cs.LG"}
{"created":"2024-03-25 03:23:10","title":"RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks","abstract":"Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding strategy that leverages radio propagation models to capture the spatial-spectral correlation inherent in the data. This graph-based representation not only simplifies data handling but also enables tailored label sampling during training, significantly enhancing the framework's adaptability for deployment. Subsequently, The GAT is employed to generalize the radiomap information across various frequency bands. Extensive experiments using raytracing datasets based on real-world environments have demonstrated RadioGAT's enhanced accuracy in supervised learning settings and its robustness in semi-supervised scenarios. These results underscore RadioGAT's effectiveness and practicality for MB-RMR in environments with limited data availability.","sentences":["Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning.","However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges.","These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements.","To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets.","RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources.","The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding strategy that leverages radio propagation models to capture the spatial-spectral correlation inherent in the data.","This graph-based representation not only simplifies data handling but also enables tailored label sampling during training, significantly enhancing the framework's adaptability for deployment.","Subsequently, The GAT is employed to generalize the radiomap information across various frequency bands.","Extensive experiments using raytracing datasets based on real-world environments have demonstrated RadioGAT's enhanced accuracy in supervised learning settings and its robustness in semi-supervised scenarios.","These results underscore RadioGAT's effectiveness and practicality for MB-RMR in environments with limited data availability."],"url":"http://arxiv.org/abs/2403.16397v1","category":"eess.SP"}
{"created":"2024-03-25 03:18:39","title":"Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation","abstract":"The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations. But there lacks a formal understanding of how entity-relation compositions can be effectively learned. Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining. We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage. We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance. Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size. This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size. Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for reasoning with abstract relations.","sentences":["The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations.","But there lacks a formal understanding of how entity-relation compositions can be effectively learned.","Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining.","We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization.","We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage.","We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance.","Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size.","This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size.","Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for reasoning with abstract relations."],"url":"http://arxiv.org/abs/2403.16394v1","category":"cs.CL"}
{"created":"2024-03-25 03:17:27","title":"Concurrent Linguistic Error Detection (CLED) for Large Language Models","abstract":"The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only relies on the outputs of the model, then it can be used on LLMs in which there is no access to the internal nodes. The proposed CLED scheme has been evaluated on the T5 model when used for news summarization and on the OPUS-MT model when used for translation. In both cases, the same set of linguistic features has been used for error detection to illustrate the applicability of the proposed scheme beyond a specific case. The results show that CLED can detect most of the errors at a low overhead penalty. The use of the concurrent classifier also enables a trade-off between error detection effectiveness and its associated overhead, so providing flexibility to a designer.","sentences":["The wide adoption of Large language models (LLMs) makes their dependability a pressing concern.","Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue.","In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes.","An interesting observation is that the output of LLMs in error-free operation should be valid and normal text.","Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error.","Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors.","Since the proposed error detection mechanism only relies on the outputs of the model, then it can be used on LLMs in which there is no access to the internal nodes.","The proposed CLED scheme has been evaluated on the T5 model when used for news summarization and on the OPUS-MT model when used for translation.","In both cases, the same set of linguistic features has been used for error detection to illustrate the applicability of the proposed scheme beyond a specific case.","The results show that CLED can detect most of the errors at a low overhead penalty.","The use of the concurrent classifier also enables a trade-off between error detection effectiveness and its associated overhead, so providing flexibility to a designer."],"url":"http://arxiv.org/abs/2403.16393v1","category":"cs.AI"}
{"created":"2024-03-25 03:12:51","title":"Illuminating Systematic Trends in Nuclear Data with Generative Machine Learning Models","abstract":"We introduce a novel method for studying systematic trends in nuclear reaction data using generative adversarial networks. Libraries of nuclear cross section evaluations exhibit intricate systematic trends across the nuclear landscape, and predictive models capable of reproducing and analyzing these trends are valuable for many applications. We have developed a predictive model using deep generative adversarial networks to learn trends from the inelastic neutron scattering channel of TENDL for even-even nuclei. The system predicts cross sections based on adding/subtracting particles to/from the target nucleus. It can thus help identify cross sections that break from expected trends and predict beyond the limit of current experiments. Our model can produce good predictions for cross section curves for many nuclides, and it is most robust near the line of stability. We also create an ensemble of predictions to leverage different correlations and estimate model uncertainty. This research marks an important first step in computer generation of nuclear cross-section libraries.","sentences":["We introduce a novel method for studying systematic trends in nuclear reaction data using generative adversarial networks.","Libraries of nuclear cross section evaluations exhibit intricate systematic trends across the nuclear landscape, and predictive models capable of reproducing and analyzing these trends are valuable for many applications.","We have developed a predictive model using deep generative adversarial networks to learn trends from the inelastic neutron scattering channel of TENDL for even-even nuclei.","The system predicts cross sections based on adding/subtracting particles to/from the target nucleus.","It can thus help identify cross sections that break from expected trends and predict beyond the limit of current experiments.","Our model can produce good predictions for cross section curves for many nuclides, and it is most robust near the line of stability.","We also create an ensemble of predictions to leverage different correlations and estimate model uncertainty.","This research marks an important first step in computer generation of nuclear cross-section libraries."],"url":"http://arxiv.org/abs/2403.16389v1","category":"nucl-th"}
{"created":"2024-03-25 03:06:54","title":"On a fibrational construction for optics, lenses, and Dialectica categories","abstract":"Categories of lenses/optics and Dialectica categories are both comprised of bidirectional morphisms of basically the same form. In this work we show how they can be considered a special case of an overarching fibrational construction, generalizing Hofstra's construction of Dialectica fibrations and Spivak's construction of generalized lenses. This construction turns a tower of Grothendieck fibrations into another tower of fibrations by iteratively twisting each of the components, using the opposite fibration construction.","sentences":["Categories of lenses/optics and Dialectica categories are both comprised of bidirectional morphisms of basically the same form.","In this work we show how they can be considered a special case of an overarching fibrational construction, generalizing Hofstra's construction of Dialectica fibrations and Spivak's construction of generalized lenses.","This construction turns a tower of Grothendieck fibrations into another tower of fibrations by iteratively twisting each of the components, using the opposite fibration construction."],"url":"http://arxiv.org/abs/2403.16388v1","category":"math.CT"}
{"created":"2024-03-25 03:02:51","title":"Dia-LLaMA: Towards Large Language Model-driven CT Report Generation","abstract":"Medical report generation has achieved remarkable advancements yet has still been faced with several challenges. First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses. Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information. Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs. Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges. In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance prompts. Considering the high dimension of CT, we leverage a pre-trained ViT3D with perceiver to extract the visual information. To tailor the LLM for report generation and emphasize abnormality, we extract additional diagnostic information by referring to a disease prototype memory bank, which is updated during training to capture common disease representations. Furthermore, we introduce disease-aware attention to enable the model to adjust attention for different diseases. Experiments on the chest CT dataset demonstrated that our proposed method outperformed previous methods and achieved state-of-the-art on both clinical efficacy performance and natural language generation metrics. The code will be made publically available.","sentences":["Medical report generation has achieved remarkable advancements yet has still been faced with several challenges.","First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses.","Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information.","Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs.","Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges.","In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance prompts.","Considering the high dimension of CT, we leverage a pre-trained ViT3D with perceiver to extract the visual information.","To tailor the LLM for report generation and emphasize abnormality, we extract additional diagnostic information by referring to a disease prototype memory bank, which is updated during training to capture common disease representations.","Furthermore, we introduce disease-aware attention to enable the model to adjust attention for different diseases.","Experiments on the chest CT dataset demonstrated that our proposed method outperformed previous methods and achieved state-of-the-art on both clinical efficacy performance and natural language generation metrics.","The code will be made publically available."],"url":"http://arxiv.org/abs/2403.16386v1","category":"cs.CV"}
{"created":"2024-03-25 03:02:27","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA","abstract":"Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight the significance of the proposed step-by-step generation. By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning. We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks.","sentences":["Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics.","Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions.","In this work, we address the lack of reasoning ability by data augmentation.","We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images.","The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python.","This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline.","Experimental results highlight the significance of the proposed step-by-step generation.","By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets.","In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning.","We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks."],"url":"http://arxiv.org/abs/2403.16385v1","category":"cs.CV"}
{"created":"2024-03-25 03:00:34","title":"Nonequilibrium Bound for Canonical Nonlinearity Under Single-Shot Work","abstract":"For classical discrete systems under constant composition (specifically substitutional alloys), canonical average acts as a map from a set of many-body interatomic interactions to a set of configuration in thermodynamic equilibrium, which is generally nonlinear. In terms of the configurational geometry (i.e., information about configurational density of states), the nonlinearity has been measured as special vector on configuration space, which is extended to Kullback-Leibler (KL) divergence on statistical manifold. Although they successfully provide new insight into how the geometry of lattice characterizes the nonlinearity, their application is essentially restricted to thermodynamic equilibrium. Based on the resource theory (especially, thermo-majorization), we here extend the applicability of the nonlinearity to nonequilibrium states obtained through single-shot work on Gibbs state. We derive the bound for the extended nonlinearity in nonequilibrium state, characterized by the nonlinearity in equilibrium state, Renyi alpha-divergence between equilibrium and nonequilibrium distribution, temperature and work.","sentences":["For classical discrete systems under constant composition (specifically substitutional alloys), canonical average acts as a map from a set of many-body interatomic interactions to a set of configuration in thermodynamic equilibrium, which is generally nonlinear.","In terms of the configurational geometry (i.e., information about configurational density of states), the nonlinearity has been measured as special vector on configuration space, which is extended to Kullback-Leibler (KL) divergence on statistical manifold.","Although they successfully provide new insight into how the geometry of lattice characterizes the nonlinearity, their application is essentially restricted to thermodynamic equilibrium.","Based on the resource theory (especially, thermo-majorization), we here extend the applicability of the nonlinearity to nonequilibrium states obtained through single-shot work on Gibbs state.","We derive the bound for the extended nonlinearity in nonequilibrium state, characterized by the nonlinearity in equilibrium state, Renyi alpha-divergence between equilibrium and nonequilibrium distribution, temperature and work."],"url":"http://arxiv.org/abs/2403.16383v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-25 02:53:32","title":"FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models","abstract":"In recent years, there has been significant progress in the development of text-to-image generative models. Evaluating the quality of the generative models is one essential step in the development process. Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical. Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset. We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level). We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection. We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations, including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets. Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup. We release the condensed subset of these commonly used datasets to help facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at https://github.com/thu-nics/FlashEval.","sentences":["In recent years, there has been significant progress in the development of text-to-image generative models.","Evaluating the quality of the generative models is one essential step in the development process.","Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical.","Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset.","We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level).","We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection.","We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations, including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets.","Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup.","We release the condensed subset of these commonly used datasets to help facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at https://github.com/thu-nics/FlashEval."],"url":"http://arxiv.org/abs/2403.16379v1","category":"cs.CV"}
{"created":"2024-03-25 02:53:32","title":"Tensor Neural Network Based Machine Learning Method for Elliptic Multiscale Problems","abstract":"In this paper, we introduce a type of tensor neural network based machine learning method to solve elliptic multiscale problems. Based on the special structure, we can do the direct and highly accurate high dimensional integrations for the tensor neural network functions without Monte Carlo process. Here, with the help of homogenization techniques, the multiscale problem is first transformed to the high dimensional limit problem with reasonable accuracy. Then, based on the tensor neural network, we design a type of machine learning method to solve the derived high dimensional limit problem. The proposed method in this paper brings a new way to design numerical methods for computing more general multiscale problems with high accuracy. Several numerical examples are also provided to validate the accuracy of the proposed numerical methods.","sentences":["In this paper, we introduce a type of tensor neural network based machine learning method to solve elliptic multiscale problems.","Based on the special structure, we can do the direct and highly accurate high dimensional integrations for the tensor neural network functions without Monte Carlo process.","Here, with the help of homogenization techniques, the multiscale problem is first transformed to the high dimensional limit problem with reasonable accuracy.","Then, based on the tensor neural network, we design a type of machine learning method to solve the derived high dimensional limit problem.","The proposed method in this paper brings a new way to design numerical methods for computing more general multiscale problems with high accuracy.","Several numerical examples are also provided to validate the accuracy of the proposed numerical methods."],"url":"http://arxiv.org/abs/2403.16380v1","category":"math.NA"}
{"created":"2024-03-25 02:52:42","title":"Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models","abstract":"The rise of large language models (LLMs) has opened new opportunities in Recommender Systems (RSs) by enhancing user behavior modeling and content understanding. However, current approaches that integrate LLMs into RSs solely utilize either LLM or conventional recommender model (CRM) to generate final recommendations, without considering which data segments LLM or CRM excel in. To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that LLMs excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for LLM, requiring substantial training data and a long training time for comparable performance. This suggests potential synergies in the combination between LLM and CRM. Motivated by these insights, we propose Collaborative Recommendation with conventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}). In this framework, we first jointly train LLM and CRM and address the issue of decision boundary shifts through alignment loss. Then, the resource-efficient CRM, with a shorter inference time, handles simple and moderate samples, while LLM processes the small subset of challenging samples for CRM. Our experimental results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM methods significantly, underscoring its effectiveness in recommendation tasks.","sentences":["The rise of large language models (LLMs) has opened new opportunities in Recommender Systems (RSs) by enhancing user behavior modeling and content understanding.","However, current approaches that integrate LLMs into RSs solely utilize either LLM or conventional recommender model (CRM) to generate final recommendations, without considering which data segments LLM or CRM excel in.","To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an LLM (LLaMA2-7B) on various groups of data samples.","Our findings reveal that LLMs excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for LLM, requiring substantial training data and a long training time for comparable performance.","This suggests potential synergies in the combination between LLM and CRM.","Motivated by these insights, we propose Collaborative Recommendation with conventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}).","In this framework, we first jointly train LLM and CRM and address the issue of decision boundary shifts through alignment loss.","Then, the resource-efficient CRM, with a shorter inference time, handles simple and moderate samples, while LLM processes the small subset of challenging samples for CRM.","Our experimental results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM methods significantly, underscoring its effectiveness in recommendation tasks."],"url":"http://arxiv.org/abs/2403.16378v1","category":"cs.IR"}
{"created":"2024-03-25 02:31:57","title":"Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation","abstract":"Sequential Recommenders have been widely applied in various online services, aiming to model users' dynamic interests from their sequential interactions. With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated. However, existing sequential recommender models often struggle to handle such lifelong sequences. The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence. Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged. In this work, we investigate the performance of Mamba for lifelong sequential recommendation (i.e., length>=2k). More specifically, we leverage the Mamba block to model lifelong user sequences selectively. We conduct extensive experiments to evaluate the performance of representative sequential recommendation models in the setting of lifelong sequences. Experiments on two real-world datasets demonstrate the superiority of Mamba. We found that RecMamba achieves performance comparable to the representative model while significantly reducing training duration by approximately 70% and memory costs by 80%. Codes and data are available at \\url{https://github.com/nancheng58/RecMamba}.","sentences":["Sequential Recommenders have been widely applied in various online services, aiming to model users' dynamic interests from their sequential interactions.","With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated.","However, existing sequential recommender models often struggle to handle such lifelong sequences.","The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence.","Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged.","In this work, we investigate the performance of Mamba for lifelong sequential recommendation (i.e., length>=2k).","More specifically, we leverage the Mamba block to model lifelong user sequences selectively.","We conduct extensive experiments to evaluate the performance of representative sequential recommendation models in the setting of lifelong sequences.","Experiments on two real-world datasets demonstrate the superiority of Mamba.","We found that RecMamba achieves performance comparable to the representative model while significantly reducing training duration by approximately 70% and memory costs by 80%.","Codes and data are available at \\url{https://github.com/nancheng58/RecMamba}."],"url":"http://arxiv.org/abs/2403.16371v1","category":"cs.IR"}
{"created":"2024-03-25 02:30:32","title":"GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation","abstract":"This paper tackles a novel yet challenging problem: how to transfer knowledge from the emerging Segment Anything Model (SAM) -- which reveals impressive zero-shot instance segmentation capacity -- to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data. This poses considerable challenges due to SAM's inability to provide semantic labels and the large capacity gap between SAM and the student. To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve knowledge transfer. Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement. This subtly enhances TA's prediction capacity on panoramic images. DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation (MKA) module to efficiently transfer the multi-level feature knowledge from TA and ensemble logits to learn a compact student model. Extensive experiments on two benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU improvement over the state-of-the-art (SOTA) domain adaptation methods. Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters.","sentences":["This paper tackles a novel yet challenging problem: how to transfer knowledge from the emerging Segment Anything Model (SAM) -- which reveals impressive zero-shot instance segmentation capacity -- to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data.","This poses considerable challenges due to SAM's inability to provide semantic labels and the large capacity gap between SAM and the student.","To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve knowledge transfer.","Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement.","This subtly enhances TA's prediction capacity on panoramic images.","DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits.","Moreover, we introduce a Multi-level Knowledge Adaptation (MKA) module to efficiently transfer the multi-level feature knowledge from TA and ensemble logits to learn a compact student model.","Extensive experiments on two benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU improvement over the state-of-the-art (SOTA) domain adaptation methods.","Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters."],"url":"http://arxiv.org/abs/2403.16370v1","category":"cs.CV"}
{"created":"2024-03-25 02:17:54","title":"Learning Action-based Representations Using Invariance","abstract":"Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts distant state features that are relevant for control. We demonstrate that action-bisimulation pretraining on reward-free, uniformly random data improves sample efficiency in several environments, including a photorealistic 3D simulation domain, Habitat. Additionally, we provide theoretical analysis and qualitative results demonstrating the information captured by action-bisimulation.","sentences":["Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors.","A representation that captures controllability identifies these state elements by determining what affects agent control.","While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem.","Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away.","To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint.","By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts distant state features that are relevant for control.","We demonstrate that action-bisimulation pretraining on reward-free, uniformly random data improves sample efficiency in several environments, including a photorealistic 3D simulation domain, Habitat.","Additionally, we provide theoretical analysis and qualitative results demonstrating the information captured by action-bisimulation."],"url":"http://arxiv.org/abs/2403.16369v1","category":"cs.LG"}
{"created":"2024-03-25 02:17:20","title":"Distilling Semantic Priors from SAM to Efficient Image Restoration Models","abstract":"In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models. Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising.","sentences":["In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance.","The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks.","However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models.","The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency.","To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process.","Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme.","SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image.","SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models.","Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors.","We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising."],"url":"http://arxiv.org/abs/2403.16368v1","category":"cs.CV"}
{"created":"2024-03-25 02:03:38","title":"Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion","abstract":"Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness. Our implementation code is publicly available at: https://github.com/hsouri/GDP .","sentences":["Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection.","As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it.","Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons.","However, some base samples may be significantly more amenable to poisoning than others.","As a result, we may be able to craft more potent poisons by carefully choosing the base samples.","In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks.","Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness.","Our implementation code is publicly available at: https://github.com/hsouri/GDP ."],"url":"http://arxiv.org/abs/2403.16365v1","category":"cs.LG"}
{"created":"2024-03-25 01:58:39","title":"Non-relativistic stellar structure in the Fierz--Pauli theory and generic linear massive gravity","abstract":"We study the structure of static spherical stars composed of non-relativistic matter in linear massive gravity with or without the Fierz-Pauli (FP) tuning. Adopting a polytropic equation of state, we construct master differential equations for the stellar profile function, which is fourth order in the FP theory or sixth order in generic non-FP theories, where the difference in the differential order reflects the presence of a ghost spin-0 graviton in the latter. In both cases, even when the spin-0 ghost is present, we find exact solutions with finite radius for the polytropic indices n = 0 and 1. Analyzing the dependences of the stellar radius, mass, and Yukawa charge on the graviton masses, we observe that a discontinuous behavior arises in the massless limit of the FP theory similarly to the van Dam-Veltman-Zakharov discontinuity, while it is absent in non-FP theories. We discuss rough observational constraints on the graviton masses.","sentences":["We study the structure of static spherical stars composed of non-relativistic matter in linear massive gravity with or without the Fierz-Pauli (FP) tuning.","Adopting a polytropic equation of state, we construct master differential equations for the stellar profile function, which is fourth order in the FP theory or sixth order in generic non-FP theories, where the difference in the differential order reflects the presence of a ghost spin-0 graviton in the latter.","In both cases, even when the spin-0 ghost is present, we find exact solutions with finite radius for the polytropic indices n = 0 and 1.","Analyzing the dependences of the stellar radius, mass, and Yukawa charge on the graviton masses, we observe that a discontinuous behavior arises in the massless limit of the FP theory similarly to the van Dam-Veltman-Zakharov discontinuity, while it is absent in non-FP theories.","We discuss rough observational constraints on the graviton masses."],"url":"http://arxiv.org/abs/2403.16363v1","category":"gr-qc"}
{"created":"2024-03-25 01:47:14","title":"Application of Floquet-Magnus expansion and Fer expansion to investigate the chemical shift anisotropy in solid-state NMR when irradiated with the triple oscillating field technique","abstract":"The Floquet-Magnus and Fer expansion schemes were introduced in solid-state nuclear magnetic resonance (NMR) in 2011 and 2006, respectively. Key features of the Floquet magnus expansion are its ability to account for the calculations developed in a finite-dimensional Hilbert space instead of an infinite-dimensional space within the Floquet theory as well as its use of its distinguishable function from other theories such as average Hamiltonian theory, Floquet theory, and Fer expansion, which facilitates the evaluation of the spin behavior in between the stroboscopic observation points. This article focuses on revisiting the Floquet-Magnus and Fer expansion approaches and applying both methods to calculate the effective Hamiltonians and propagators, which control the spin system evolution during the Triple Oscillating Field Technique radiation experiment (TOFU). The TOFU pulse sequence is an important technique that was shown to avoid the dipolar truncation problem and form a new basis for accurate distance measurement by solid-state NMR. We take advantage of the interaction frequencies and the time modulation arising from the TOFU pulse sequence allowing selective recoupling of specific terms in the Hamiltonian that fulfill determined specific conditions. The work presented unifies and generalizes existing results of the Floquet-Magnus and Fer expansions and delivers illustrations of novel springs that boost previous applications that are based on the classical information. We believe that the generality of this work points to potential applications in problems related to theoretical developments of spectroscopy as well as interdisciplinary research areas whenever they include the spin dynamics concepts.","sentences":["The Floquet-Magnus and Fer expansion schemes were introduced in solid-state nuclear magnetic resonance (NMR) in 2011 and 2006, respectively.","Key features of the Floquet magnus expansion are its ability to account for the calculations developed in a finite-dimensional Hilbert space instead of an infinite-dimensional space within the Floquet theory as well as its use of its distinguishable function from other theories such as average Hamiltonian theory, Floquet theory, and Fer expansion, which facilitates the evaluation of the spin behavior in between the stroboscopic observation points.","This article focuses on revisiting the Floquet-Magnus and Fer expansion approaches and applying both methods to calculate the effective Hamiltonians and propagators, which control the spin system evolution during the Triple Oscillating Field Technique radiation experiment (TOFU).","The TOFU pulse sequence is an important technique that was shown to avoid the dipolar truncation problem and form a new basis for accurate distance measurement by solid-state NMR.","We take advantage of the interaction frequencies and the time modulation arising from the TOFU pulse sequence allowing selective recoupling of specific terms in the Hamiltonian that fulfill determined specific conditions.","The work presented unifies and generalizes existing results of the Floquet-Magnus and Fer expansions and delivers illustrations of novel springs that boost previous applications that are based on the classical information.","We believe that the generality of this work points to potential applications in problems related to theoretical developments of spectroscopy as well as interdisciplinary research areas whenever they include the spin dynamics concepts."],"url":"http://arxiv.org/abs/2403.16359v1","category":"physics.chem-ph"}
{"created":"2024-03-25 01:44:34","title":"ChebMixer: Efficient Graph Representation Learning with MLP Mixer","abstract":"Graph neural networks have achieved remarkable success in learning graph representations, especially graph Transformer, which has recently shown superior performance on various graph mining tasks. However, graph Transformer generally treats nodes as tokens, which results in quadratic complexity regarding the number of nodes during self-attention computation. The graph MLP Mixer addresses this challenge by using the efficient MLP Mixer technique from computer vision. However, the time-consuming process of extracting graph tokens limits its performance. In this paper, we present a novel architecture named ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens. Firstly, we produce multiscale representations of graph nodes via fast Chebyshev polynomial-based spectral filtering. Next, we consider each node's multiscale representations as a sequence of tokens and refine the node representation with an effective MLP Mixer. Finally, we aggregate the multiscale representations of nodes through Chebyshev interpolation. Owing to the powerful representation capabilities and fast computational properties of MLP Mixer, we can quickly extract more informative node representations to improve the performance of downstream tasks. The experimental results prove our significant improvements in a variety of scenarios ranging from graph node classification to medical image segmentation.","sentences":["Graph neural networks have achieved remarkable success in learning graph representations, especially graph Transformer, which has recently shown superior performance on various graph mining tasks.","However, graph Transformer generally treats nodes as tokens, which results in quadratic complexity regarding the number of nodes during self-attention computation.","The graph MLP Mixer addresses this challenge by using the efficient MLP Mixer technique from computer vision.","However, the time-consuming process of extracting graph tokens limits its performance.","In this paper, we present a novel architecture named ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens.","Firstly, we produce multiscale representations of graph nodes via fast Chebyshev polynomial-based spectral filtering.","Next, we consider each node's multiscale representations as a sequence of tokens and refine the node representation with an effective MLP Mixer.","Finally, we aggregate the multiscale representations of nodes through Chebyshev interpolation.","Owing to the powerful representation capabilities and fast computational properties of MLP Mixer, we can quickly extract more informative node representations to improve the performance of downstream tasks.","The experimental results prove our significant improvements in a variety of scenarios ranging from graph node classification to medical image segmentation."],"url":"http://arxiv.org/abs/2403.16358v1","category":"cs.CV"}
{"created":"2024-03-25 01:33:03","title":"Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability","abstract":"We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation. We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics. We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner. The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints. Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints. We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map. We evaluate the efficacy of our planning framework on Digit bipedal robot simulation in MuJoCo.","sentences":["We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain.","In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation.","Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations.","Thus, the problems of terrain mapping and locomotion stability are intertwined.","We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation.","We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics.","We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner.","The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints.","Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints.","We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map.","We evaluate the efficacy of our planning framework on Digit bipedal robot simulation in MuJoCo."],"url":"http://arxiv.org/abs/2403.16356v1","category":"cs.RO"}
{"created":"2024-03-25 01:12:57","title":"ChatDBG: An AI-Powered Debugging Assistant","abstract":"This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times.","sentences":["This paper presents ChatDBG, the first AI-powered debugging assistant.","ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers.","ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\".","To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer.","Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python.","Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors.","For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%.","ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times."],"url":"http://arxiv.org/abs/2403.16354v1","category":"cs.SE"}
{"created":"2024-03-25 00:51:25","title":"A Multivariate Berry--Esseen theorem for time-dependent expanding dynamical systems","abstract":"We adapt Stein's method to obtain Berry--Esseen type error bounds in the multivariate central limit theorem for non-stationary processes generated by time-dependent compositions of uniformly expanding dynamical systems. In a particular case of random dynamical systems with a strongly mixing base transformation, we derive an error estimate of order $O(N^{-1/2})$ in the quenched multivariate CLT, provided that the covariance matrix \\say{grows linearly} with the number of summands $N$. The error in the normal approximation is estimated for the class of all convex sets.","sentences":["We adapt Stein's method to obtain Berry--Esseen type error bounds in the multivariate central limit theorem for non-stationary processes generated by time-dependent compositions of uniformly expanding dynamical systems.","In a particular case of random dynamical systems with a strongly mixing base transformation, we derive an error estimate of order $O(N^{-1/2})$ in the quenched multivariate CLT, provided that the covariance matrix \\say{grows linearly} with the number of summands $N$. The error in the normal approximation is estimated for the class of all convex sets."],"url":"http://arxiv.org/abs/2403.16349v1","category":"math.DS"}
{"created":"2024-03-25 00:51:13","title":"Quadratic embedding constants of fan graphs and graph joins","abstract":"We derive a general formula for the quadratic embedding constant of a graph join $\\bar{K}_m+G$, where $\\bar{K}_m$ is the empty graph on $m\\ge1$ vertices and $G$ is an arbitrary graph. Applying our formula to a fan graph $K_1+P_n$, where $K_1=\\bar{K}_1$ is the singleton graph and $P_n$ is the path on $n\\ge1$ vertices, we show that $\\mathrm{QEC}(K_1+P_n)=-\\tilde{\\alpha}_n-2$, where $\\tilde{\\alpha}_n$ is the minimal zero of a new polynomial $\\Phi_n(x)$ related to Chebyshev polynomials of the second kind. Moreover, for an even $n$ we have $\\tilde{\\alpha}_n=\\min\\mathrm{ev}(A_n)$, where the right-hand side is the An minimal eigenvalue of the adjacency matrix $A_n$ of $P_n$. For an odd $n$ we show that $\\min\\mathrm{ev}(A_{n+1})\\le\\tilde{\\alpha}_n<\\min\\mathrm{ev}(A_n)$.","sentences":["We derive a general formula for the quadratic embedding constant of a graph join $\\bar{K}_m+G$, where $\\bar{K}_m$ is the empty graph on $m\\ge1$ vertices and $G$ is an arbitrary graph.","Applying our formula to a fan graph $K_1+P_n$, where $K_1=\\bar{K}_1$ is the singleton graph and $P_n$ is the path on $n\\ge1$ vertices, we show that $\\mathrm{QEC}(K_1+P_n)=-\\tilde{\\alpha}_n-2$, where $\\tilde{\\alpha}_n$ is the minimal zero of a new polynomial $\\Phi_n(x)$ related to Chebyshev polynomials of the second kind.","Moreover, for an even $n$ we have $\\tilde{\\alpha}_n=\\min\\mathrm{ev}(A_n)$, where the right-hand side is the An minimal eigenvalue of the adjacency matrix $A_n$ of $P_n$. For an odd $n$ we show that $\\min\\mathrm{ev}(A_{n+1})\\le\\tilde{\\alpha}_n<\\min\\mathrm{ev}(A_n)$."],"url":"http://arxiv.org/abs/2403.16348v1","category":"math.CO"}
{"created":"2024-03-25 00:50:27","title":"ChatGPT Incorrectness Detection in Software Reviews","abstract":"We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 - 0.75.","sentences":["We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks.","We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses.","We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses.","CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts).","The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response.","In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 - 0.75."],"url":"http://arxiv.org/abs/2403.16347v1","category":"cs.SE"}
{"created":"2024-03-25 00:43:44","title":"Enhanced Facet Generation with LLM Editing","abstract":"In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine. The first strategy is multi-task learning to predict SERP. By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules. The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model. Overall performance improves when small model and LLM are combined rather than facet generation individually.","sentences":["In information retrieval, facet identification of a user query is an important task.","If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results.","Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine.","However, there are challenges in extending it to other applications when a search engine operates as part of the model.","First, search engines are constantly updated.","Therefore, additional information may change during training and test, which may reduce performance.","The second challenge is that public search engines cannot search for internal documents.","Therefore, a separate search system needs to be built to incorporate documents from private domains within the company.","We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine.","The first strategy is multi-task learning to predict SERP.","By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules.","The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model.","Overall performance improves when small model and LLM are combined rather than facet generation individually."],"url":"http://arxiv.org/abs/2403.16345v1","category":"cs.CL"}
{"created":"2024-03-25 00:42:50","title":"Percentile Optimization in Wireless Networks- Part I: Power Control for Max-Min-Rate to Sum-Rate Maximization (and Everything in Between)","abstract":"Improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks. While a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them. In this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users. Part I focuses on the class of least-percentile throughput maximization through power control. This class subsumes the well-known max-min and max-sum-rate optimization problems as special cases. Apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly NP-hard in general for multiuser interference networks, making optimization extremely challenging. We propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points. Comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster.","sentences":["Improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks.","While a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them.","In this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users.","Part I focuses on the class of least-percentile throughput maximization through power control.","This class subsumes the well-known max-min and max-sum-rate optimization problems as special cases.","Apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly NP-hard in general for multiuser interference networks, making optimization extremely challenging.","We propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points.","Comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster."],"url":"http://arxiv.org/abs/2403.16344v1","category":"cs.IT"}
{"created":"2024-03-25 00:39:45","title":"A generalization of the first Tits construction","abstract":"Let $F$ be a field of characteristic not 2 or 3. The first Tits construction is a well-known tripling process to construct separable cubic Jordan algebras, especially Albert algebras. We generalize the first Tits construction by choosing the scalar employed in the tripling process outside of the base field. This yields a new family of nonassociative unital algebras which carry a cubic map, and maps that can be viewed as generalized adjoint and generalized trace maps. These maps display properties often similar to the ones in the classical setup. In particular, the cubic norm map permits some kind of weak Jordan composition law.","sentences":["Let $F$ be a field of characteristic not 2 or 3.","The first Tits construction is a well-known tripling process to construct separable cubic Jordan algebras, especially Albert algebras.","We generalize the first Tits construction by choosing the scalar employed in the tripling process outside of the base field.","This yields a new family of nonassociative unital algebras which carry a cubic map, and maps that can be viewed as generalized adjoint and generalized trace maps.","These maps display properties often similar to the ones in the classical setup.","In particular, the cubic norm map permits some kind of weak Jordan composition law."],"url":"http://arxiv.org/abs/2403.16342v1","category":"math.RA"}
{"created":"2024-03-25 00:24:10","title":"Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks","abstract":"Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system. The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle's life cycle). Lossless compression doesn't provide sufficient compression ratios, hence, lossy video compression has been explored. It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms. However, there is limited work in this area to provide a solid conclusion. In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts. Fisheye cameras are commonly used in automotive systems for 3D object detection task. In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fisheye camera images. We demonstrate that the achievable compression with negligible impact depends on the dataset and temporal prediction of the video codec. We propose a radial distortion-aware zonal metric to evaluate the performance of artifacts in fisheye images. In addition, we present a novel method for estimating affine mode parameters of the latest VVC codec, and suggest some areas for improvement in video codecs for the application to fisheye imagery.","sentences":["Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system.","The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle's life cycle).","Lossless compression doesn't provide sufficient compression ratios, hence, lossy video compression has been explored.","It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms.","However, there is limited work in this area to provide a solid conclusion.","In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts.","Fisheye cameras are commonly used in automotive systems for 3D object detection task.","In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fisheye camera images.","We demonstrate that the achievable compression with negligible impact depends on the dataset and temporal prediction of the video codec.","We propose a radial distortion-aware zonal metric to evaluate the performance of artifacts in fisheye images.","In addition, we present a novel method for estimating affine mode parameters of the latest VVC codec, and suggest some areas for improvement in video codecs for the application to fisheye imagery."],"url":"http://arxiv.org/abs/2403.16338v1","category":"cs.CV"}
{"created":"2024-03-25 00:23:38","title":"On solution of tropical discrete best approximation problems","abstract":"We consider a discrete best approximation problem formulated in the framework of tropical algebra, which deals with the theory and applications of algebraic systems with idempotent operations. Given a set of samples of input and output of an unknown function, the problem is to construct a generalized tropical Puiseux polynomial that best approximates the function in the sense of a tropical distance function. The construction of an approximate polynomial involves the evaluation of both unknown coefficient and exponent of each monomial in the polynomial. To solve the approximation problem, we first reduce the problem to an equation in unknown vector of coefficients, which is given by a matrix with entries parameterized by unknown exponents. We derive a best approximate solution of the equation, which yields both vector of coefficients and approximation error parameterized by the exponents. Optimal values of exponents are found by minimization of the approximation error, which is reduced to a minimization of a function of exponents over all partitions of a finite set. We solve this minimization problem in terms of max-plus algebra (where addition is defined as maximum and multiplication as arithmetic addition) by using a computational procedure based on the agglomerative clustering technique. This solution is extended to the minimization problem of finding optimal exponents in the polynomial in terms of max-algebra (where addition is defined as maximum). The results obtained are applied to develop new solutions for conventional problems of discrete best approximation of real functions by piecewise linear functions and piecewise Puiseux polynomials. We discuss computational complexity of the proposed solution and estimate upper bounds on the computational time. We demonstrate examples of approximation problems solved in terms of max-plus and max-algebra, and give graphical illustrations.","sentences":["We consider a discrete best approximation problem formulated in the framework of tropical algebra, which deals with the theory and applications of algebraic systems with idempotent operations.","Given a set of samples of input and output of an unknown function, the problem is to construct a generalized tropical Puiseux polynomial that best approximates the function in the sense of a tropical distance function.","The construction of an approximate polynomial involves the evaluation of both unknown coefficient and exponent of each monomial in the polynomial.","To solve the approximation problem, we first reduce the problem to an equation in unknown vector of coefficients, which is given by a matrix with entries parameterized by unknown exponents.","We derive a best approximate solution of the equation, which yields both vector of coefficients and approximation error parameterized by the exponents.","Optimal values of exponents are found by minimization of the approximation error, which is reduced to a minimization of a function of exponents over all partitions of a finite set.","We solve this minimization problem in terms of max-plus algebra (where addition is defined as maximum and multiplication as arithmetic addition) by using a computational procedure based on the agglomerative clustering technique.","This solution is extended to the minimization problem of finding optimal exponents in the polynomial in terms of max-algebra (where addition is defined as maximum).","The results obtained are applied to develop new solutions for conventional problems of discrete best approximation of real functions by piecewise linear functions and piecewise Puiseux polynomials.","We discuss computational complexity of the proposed solution and estimate upper bounds on the computational time.","We demonstrate examples of approximation problems solved in terms of max-plus and max-algebra, and give graphical illustrations."],"url":"http://arxiv.org/abs/2403.16337v1","category":"math.NA"}
{"created":"2024-03-25 00:21:34","title":"Predictive Inference in Multi-environment Scenarios","abstract":"We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.","sentences":["We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments.","We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios.","Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems.","We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets."],"url":"http://arxiv.org/abs/2403.16336v1","category":"stat.ML"}
{"created":"2024-03-25 00:17:43","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline","abstract":"The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy parameters. To overcome this challenge, we introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method tailored specifically for ultrasound applications. USLoRA allows for selective fine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters compared to fully fine-tuning only the UNet portion of SD. To enhance dataset diversity, we incorporate different adjectives into the generation process prompts, thereby desensitizing the classifiers to intensity changes across different images. This approach is inspired by clinicians' decision-making processes regarding breast tumors, where tumor shape often plays a more crucial role than intensity. In conclusion, our pipeline not only outperforms classifiers trained on the original dataset but also demonstrates superior performance when encountering unseen datasets. The source code is available at https://github.com/yasamin-med/MEDDAP.","sentences":["The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data.","However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties.","Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer.","To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples.","Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy parameters.","To overcome this challenge, we introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method tailored specifically for ultrasound applications.","USLoRA allows for selective fine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters compared to fully fine-tuning only the UNet portion of SD.","To enhance dataset diversity, we incorporate different adjectives into the generation process prompts, thereby desensitizing the classifiers to intensity changes across different images.","This approach is inspired by clinicians' decision-making processes regarding breast tumors, where tumor shape often plays a more crucial role than intensity.","In conclusion, our pipeline not only outperforms classifiers trained on the original dataset but also demonstrates superior performance when encountering unseen datasets.","The source code is available at https://github.com/yasamin-med/MEDDAP."],"url":"http://arxiv.org/abs/2403.16335v1","category":"eess.IV"}
{"created":"2024-03-25 00:15:34","title":"Graphs Generalization under Distribution Shifts","abstract":"Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution. To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process. However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology. Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations across domains by modeling the potential seen or unseen variations of attribute distribution and topological structure and (2) minimize the discrepancy of the variation in a representation space where the target is to predict semantic labels. Extensive experiment results indicate that our model outperforms baseline methods on node-level OOD generalization across domains in distribution shift on node features and topological structures simultaneously.","sentences":["Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution.","To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process.","However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges.","Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology.","Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge.","To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER).","The goal is to (1) diversify variations across domains by modeling the potential seen or unseen variations of attribute distribution and topological structure and (2) minimize the discrepancy of the variation in a representation space where the target is to predict semantic labels.","Extensive experiment results indicate that our model outperforms baseline methods on node-level OOD generalization across domains in distribution shift on node features and topological structures simultaneously."],"url":"http://arxiv.org/abs/2403.16334v1","category":"cs.LG"}
{"created":"2024-03-25 00:10:38","title":"Expanding the frontiers of cool-dwarf asteroseismology with ESPRESSO. Detection of solar-like oscillations in the K5 dwarf $\u03b5$ Indi","abstract":"Fuelled by space photometry, asteroseismology is vastly benefitting the study of cool main-sequence stars, which exhibit convection-driven solar-like oscillations. Even so, the tiny oscillation amplitudes in K dwarfs continue to pose a challenge to space-based asteroseismology. A viable alternative is offered by the lower stellar noise over the oscillation timescales in Doppler observations. In this letter we present the definite detection of solar-like oscillations in the bright K5 dwarf $\\epsilon$ Indi based on time-intensive observations collected with the ESPRESSO spectrograph at the VLT, thus making it the coolest seismic dwarf ever observed. We measured the frequencies of a total of 19 modes of degree $\\ell=0$--2 along with $\\nu_{\\rm max}=5305\\pm176\\:{\\rm \\mu Hz}$ and $\\Delta\\nu=201.25\\pm0.16\\:{\\rm \\mu Hz}$. The peak amplitude of radial modes is $2.6\\pm0.5\\:{\\rm cm\\,s^{-1}}$, or a mere ${\\sim} 14\\%$ of the solar value. Measured mode amplitudes are ${\\sim} 2$ times lower than predicted from a nominal $L/M$ scaling relation and favour a scaling closer to $(L/M)^{1.5}$ below ${\\sim} 5500\\:{\\rm K}$, carrying important implications for our understanding of the coupling efficiency between pulsations and near-surface convection in K dwarfs. This detection conclusively shows that precise asteroseismology of cool dwarfs is possible down to at least the mid-K regime using next-generation spectrographs on large-aperture telescopes, effectively opening up a new domain in observational asteroseismology.","sentences":["Fuelled by space photometry, asteroseismology is vastly benefitting the study of cool main-sequence stars, which exhibit convection-driven solar-like oscillations.","Even so, the tiny oscillation amplitudes in K dwarfs continue to pose a challenge to space-based asteroseismology.","A viable alternative is offered by the lower stellar noise over the oscillation timescales in Doppler observations.","In this letter we present the definite detection of solar-like oscillations in the bright K5 dwarf $\\epsilon$ Indi based on time-intensive observations collected with the ESPRESSO spectrograph at the VLT, thus making it the coolest seismic dwarf ever observed.","We measured the frequencies of a total of 19 modes of degree $\\ell=0$--2 along with $\\nu_{\\rm max}=5305\\pm176\\:{\\rm \\mu Hz}$ and","$\\Delta\\nu=201.25\\pm0.16\\:{\\rm \\mu Hz}$. The peak amplitude of radial modes is $2.6\\pm0.5\\:{\\rm cm\\,s^{-1}}$, or a mere ${\\sim} 14\\%$ of the solar value.","Measured mode amplitudes are ${\\sim} 2$ times lower than predicted from a nominal $L/M$ scaling relation and favour a scaling closer to $(L/M)^{1.5}$ below ${\\sim} 5500\\:{\\rm K}$, carrying important implications for our understanding of the coupling efficiency between pulsations and near-surface convection in K dwarfs.","This detection conclusively shows that precise asteroseismology of cool dwarfs is possible down to at least the mid-K regime using next-generation spectrographs on large-aperture telescopes, effectively opening up a new domain in observational asteroseismology."],"url":"http://arxiv.org/abs/2403.16333v1","category":"astro-ph.SR"}
{"created":"2024-03-24 23:22:02","title":"Artificial Neural Microcircuits as Building Blocks: Concept and Challenges","abstract":"Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation. However the current trend is for ANNs to be structurally homogeneous. Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems. How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges uncovered during these efforts and explorations of methods by which they might be overcome.","sentences":["Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation.","However the current trend is for ANNs to be structurally homogeneous.","Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting.","In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems.","How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges uncovered during these efforts and explorations of methods by which they might be overcome."],"url":"http://arxiv.org/abs/2403.16327v1","category":"cs.NE"}
{"created":"2024-03-24 23:16:56","title":"Minimal Cellular Resolutions of Path Ideals","abstract":"In this paper, we prove that the path ideals of both paths and cycles have minimal cellular resolutions. Specifically, these minimal free resolutions coincide with the Barile-Macchia resolutions for paths, and their generalized counterparts for cycles. Our result on cycles marks the first instance in the literature where the minimal free resolution of edge ideals of cycles has been established. Furthermore, edge ideals of cycles represent the first class of ideals that lack a minimal Barile-Macchia resolution, yet have a minimal generalized Barile-Macchia resolution.","sentences":["In this paper, we prove that the path ideals of both paths and cycles have minimal cellular resolutions.","Specifically, these minimal free resolutions coincide with the Barile-Macchia resolutions for paths, and their generalized counterparts for cycles.","Our result on cycles marks the first instance in the literature where the minimal free resolution of edge ideals of cycles has been established.","Furthermore, edge ideals of cycles represent the first class of ideals that lack a minimal Barile-Macchia resolution, yet have a minimal generalized Barile-Macchia resolution."],"url":"http://arxiv.org/abs/2403.16324v1","category":"math.AC"}
{"created":"2024-03-24 22:59:24","title":"Enhancing Quantum Entanglement in Bipartite Systems: Leveraging Optimal Control and Physics-Informed Neural Networks","abstract":"Quantum entanglement stands at the forefront of quantum information science, heralding new paradigms in quantum communication, computation, and cryptography. This paper introduces a quantum optimal control approach by focusing on entanglement measures rather than targeting predefined maximally entangled states. Leveraging the indirect Pontryagin Minimum Principle, we formulate an optimal control problem centered on maximizing an enhanced lower bound of the entanglement measure within a shortest timeframe in the presence of input constraints. We derive optimality conditions based on Pontryagin's Minimum Principle tailored for a matrix-valued dynamic control system and tackle the resulting boundary value problem through a Physics-Informed Neural Network, which is adept at handling differential matrix equations. The proposed strategy not only refines the process of generating entangled states but also introduces a method with increased sensitivity in detecting entangled states, thereby overcoming the limitations of conventional concurrence estimation.","sentences":["Quantum entanglement stands at the forefront of quantum information science, heralding new paradigms in quantum communication, computation, and cryptography.","This paper introduces a quantum optimal control approach by focusing on entanglement measures rather than targeting predefined maximally entangled states.","Leveraging the indirect Pontryagin Minimum Principle, we formulate an optimal control problem centered on maximizing an enhanced lower bound of the entanglement measure within a shortest timeframe in the presence of input constraints.","We derive optimality conditions based on Pontryagin's Minimum Principle tailored for a matrix-valued dynamic control system and tackle the resulting boundary value problem through a Physics-Informed Neural Network, which is adept at handling differential matrix equations.","The proposed strategy not only refines the process of generating entangled states but also introduces a method with increased sensitivity in detecting entangled states, thereby overcoming the limitations of conventional concurrence estimation."],"url":"http://arxiv.org/abs/2403.16321v1","category":"quant-ph"}
{"created":"2024-03-24 22:53:16","title":"AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans","abstract":"Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code will be made publicly available at https://github.com/artonson/autoinst.","sentences":["Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments.","Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations.","Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain.","To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations.","To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals.","To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances.","We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals.","To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations.","Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline.","The code will be made publicly available at https://github.com/artonson/autoinst."],"url":"http://arxiv.org/abs/2403.16318v1","category":"cs.CV"}
{"created":"2024-03-24 22:42:40","title":"Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective","abstract":"We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\\\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\\\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of our results are that: (i) it is possible to obtain complexity results for both convex and nonconvex problems with the (local or global) Lipschitz constant being replaced by a constant of local subgradient variation and (ii) mean width of the subdifferential set around the optima plays a role in the complexity of nonsmooth optimization, particularly in parallel settings. A consequence of (ii) is that for any error parameter $\\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex optimization is lower than its sequential oracle complexity by a factor $\\tilde{\\Omega}\\big(\\frac{1}{\\epsilon}\\big)$ whenever the objective function is piecewise linear with polynomially many pieces in the input size. This is particularly surprising as existing parallel complexity lower bounds are based on such classes of functions. The seeming contradiction is resolved by considering the region in which the algorithm is allowed to query the objective.","sentences":["We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense.","The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\\\"{o}lder/Lipschitz continuity of its gradient.","Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\\\"{o}lder continuous gradient.","When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.''","Some highlights of our results are that: (i) it is possible to obtain complexity results for both convex and nonconvex problems with the (local or global) Lipschitz constant being replaced by a constant of local subgradient variation and (ii) mean width of the subdifferential set around the optima plays a role in the complexity of nonsmooth optimization, particularly in parallel settings.","A consequence of (ii) is that for any error parameter $\\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex optimization is lower than its sequential oracle complexity by a factor $\\tilde{\\Omega}\\big(\\frac{1}{\\epsilon}\\big)$ whenever the objective function is piecewise linear with polynomially many pieces in the input size.","This is particularly surprising as existing parallel complexity lower bounds are based on such classes of functions.","The seeming contradiction is resolved by considering the region in which the algorithm is allowed to query the objective."],"url":"http://arxiv.org/abs/2403.16317v1","category":"math.OC"}
{"created":"2024-03-24 22:40:54","title":"On interpolation categories for the hyperoctahedral group","abstract":"Two different types of Deligne categories have been defined to interpolate the finite dimensional complex representations of the hyperoctahedral group. The first one, initially defined by Knop and then further studied by Likeng and Savage, uses a categorical analogue of the permutation representation as a tensor generator. The second one, due to Flake and Maassen, is tensor generated by a categorical analogue of the reflection representation. We construct a symmetric monoidal functor between the two and show that it is an equivalence of symmetric monoidal categories.","sentences":["Two different types of Deligne categories have been defined to interpolate the finite dimensional complex representations of the hyperoctahedral group.","The first one, initially defined by Knop and then further studied by Likeng and Savage, uses a categorical analogue of the permutation representation as a tensor generator.","The second one, due to Flake and Maassen, is tensor generated by a categorical analogue of the reflection representation.","We construct a symmetric monoidal functor between the two and show that it is an equivalence of symmetric monoidal categories."],"url":"http://arxiv.org/abs/2403.16316v1","category":"math.RT"}
{"created":"2024-03-24 22:26:33","title":"Standard and Non-Standard Aspects of Neutrino Physics","abstract":"This review provides a succinct overview of the basic aspects of neutrino physics. The topics covered include: neutrinos in the standard model and the three-neutrino mixing scheme; the current status of neutrino oscillation measurements and what remains to be determined; the seesaw mechanisms for neutrino mass generation and the associated phenomenology, including the leptogenesis mechanism to explain the observed matter-antimatter asymmetry of the Universe; models for the origin of the pattern of neutrino mixing and lepton masses based on discrete flavour symmetries and modular invariance.","sentences":["This review provides a succinct overview of the basic aspects of neutrino physics.","The topics covered include: neutrinos in the standard model and the three-neutrino mixing scheme; the current status of neutrino oscillation measurements and what remains to be determined; the seesaw mechanisms for neutrino mass generation and the associated phenomenology, including the leptogenesis mechanism to explain the observed matter-antimatter asymmetry of the Universe; models for the origin of the pattern of neutrino mixing and lepton masses based on discrete flavour symmetries and modular invariance."],"url":"http://arxiv.org/abs/2403.16308v1","category":"hep-ph"}
{"created":"2024-03-24 21:41:41","title":"SoK: An Essential Guide For Using Malware Sandboxes In Security Applications: Challenges, Pitfalls, and Lessons Learned","abstract":"Malware sandboxes provide many benefits for security applications, but they are complex. These complexities can overwhelm new users in different research areas and make it difficult to select, configure, and use sandboxes. Even worse, incorrectly using sandboxes can have a negative impact on security applications. In this paper, we address this knowledge gap by systematizing 84 representative papers for using x86/64 malware sandboxes in the academic literature. We propose a novel framework to simplify sandbox components and organize the literature to derive practical guidelines for using sandboxes. We evaluate the proposed guidelines systematically using three common security applications and demonstrate that the choice of different sandboxes can significantly impact the results. Specifically, our results show that the proposed guidelines improve the sandbox observable activities by at least 1.6x and up to 11.3x. Furthermore, we observe a roughly 25% improvement in accuracy, precision, and recall when using the guidelines to help with a malware family classification task. We conclude by affirming that there is no \"silver bullet\" sandbox deployment that generalizes, and we recommend that users apply our framework to define a scope for their analysis, a threat model, and derive context about how the sandbox artifacts will influence their intended use case. Finally, it is important that users document their experiment, limitations, and potential solutions for reproducibility","sentences":["Malware sandboxes provide many benefits for security applications, but they are complex.","These complexities can overwhelm new users in different research areas and make it difficult to select, configure, and use sandboxes.","Even worse, incorrectly using sandboxes can have a negative impact on security applications.","In this paper, we address this knowledge gap by systematizing 84 representative papers for using x86/64 malware sandboxes in the academic literature.","We propose a novel framework to simplify sandbox components and organize the literature to derive practical guidelines for using sandboxes.","We evaluate the proposed guidelines systematically using three common security applications and demonstrate that the choice of different sandboxes can significantly impact the results.","Specifically, our results show that the proposed guidelines improve the sandbox observable activities by at least 1.6x and up to 11.3x.","Furthermore, we observe a roughly 25% improvement in accuracy, precision, and recall when using the guidelines to help with a malware family classification task.","We conclude by affirming that there is no \"silver bullet\" sandbox deployment that generalizes, and we recommend that users apply our framework to define a scope for their analysis, a threat model, and derive context about how the sandbox artifacts will influence their intended use case.","Finally, it is important that users document their experiment, limitations, and potential solutions for reproducibility"],"url":"http://arxiv.org/abs/2403.16304v1","category":"cs.CR"}
{"created":"2024-03-24 21:29:39","title":"Large Language Models in Biomedical and Health Informatics: A Bibliometric Review","abstract":"Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as well as healthcare delivery and patient outcomes. This comprehensive review serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of LLMs in BHI.","sentences":["Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research.","This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023.","It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine.","To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field.","Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations.","Looking ahead, we consider how LLMs could further transform biomedical research as well as healthcare delivery and patient outcomes.","This comprehensive review serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of LLMs in BHI."],"url":"http://arxiv.org/abs/2403.16303v1","category":"cs.DL"}
{"created":"2024-03-24 21:05:36","title":"MRSch: Multi-Resource Scheduling for HPC","abstract":"Emerging workloads in high-performance computing (HPC) are embracing significant changes, such as having diverse resource requirements instead of being CPU-centric. This advancement forces cluster schedulers to consider multiple schedulable resources during decision-making. Existing scheduling studies rely on heuristic or optimization methods, which are limited by an inability to adapt to new scenarios for ensuring long-term scheduling performance. We present an intelligent scheduling agent named MRSch for multi-resource scheduling in HPC that leverages direct future prediction (DFP), an advanced multi-objective reinforcement learning algorithm. While DFP demonstrated outstanding performance in a gaming competition, it has not been previously explored in the context of HPC scheduling. Several key techniques are developed in this study to tackle the challenges involved in multi-resource scheduling. These techniques enable MRSch to learn an appropriate scheduling policy automatically and dynamically adapt its policy in response to workload changes via dynamic resource prioritizing. We compare MRSch with existing scheduling methods through extensive tracebase simulations. Our results demonstrate that MRSch improves scheduling performance by up to 48% compared to the existing scheduling methods.","sentences":["Emerging workloads in high-performance computing (HPC) are embracing significant changes, such as having diverse resource requirements instead of being CPU-centric.","This advancement forces cluster schedulers to consider multiple schedulable resources during decision-making.","Existing scheduling studies rely on heuristic or optimization methods, which are limited by an inability to adapt to new scenarios for ensuring long-term scheduling performance.","We present an intelligent scheduling agent named MRSch for multi-resource scheduling in HPC that leverages direct future prediction (DFP), an advanced multi-objective reinforcement learning algorithm.","While DFP demonstrated outstanding performance in a gaming competition, it has not been previously explored in the context of HPC scheduling.","Several key techniques are developed in this study to tackle the challenges involved in multi-resource scheduling.","These techniques enable MRSch to learn an appropriate scheduling policy automatically and dynamically adapt its policy in response to workload changes via dynamic resource prioritizing.","We compare MRSch with existing scheduling methods through extensive tracebase simulations.","Our results demonstrate that MRSch improves scheduling performance by up to 48% compared to the existing scheduling methods."],"url":"http://arxiv.org/abs/2403.16298v1","category":"cs.DC"}
{"created":"2024-03-24 21:05:28","title":"Round Robin Active Sequential Change Detection for Dependent Multi-Channel Data","abstract":"This paper considers the problem of sequentially detecting a change in the joint distribution of multiple data sources under a sampling constraint. Specifically, the channels or sources generate observations that are independent over time, but not necessarily independent at any given time instant. The sources follow an initial joint distribution, and at an unknown time instant, the joint distribution of an unknown subset of sources changes. Importantly, there is a hard constraint that only a fixed number of sources are allowed to be sampled at each time instant. The goal is to sequentially observe the sources according to the constraint, and stop sampling as quickly as possible after the change while controlling the false alarm rate below a user-specified level. The sources can be selected dynamically based on the already collected data, and thus, a policy for this problem consists of a joint sampling and change-detection rule. A non-randomized policy is studied, and an upper bound is established on its worst-case conditional expected detection delay with respect to both the change point and the observations from the affected sources before the change.","sentences":["This paper considers the problem of sequentially detecting a change in the joint distribution of multiple data sources under a sampling constraint.","Specifically, the channels or sources generate observations that are independent over time, but not necessarily independent at any given time instant.","The sources follow an initial joint distribution, and at an unknown time instant, the joint distribution of an unknown subset of sources changes.","Importantly, there is a hard constraint that only a fixed number of sources are allowed to be sampled at each time instant.","The goal is to sequentially observe the sources according to the constraint, and stop sampling as quickly as possible after the change while controlling the false alarm rate below a user-specified level.","The sources can be selected dynamically based on the already collected data, and thus, a policy for this problem consists of a joint sampling and change-detection rule.","A non-randomized policy is studied, and an upper bound is established on its worst-case conditional expected detection delay with respect to both the change point and the observations from the affected sources before the change."],"url":"http://arxiv.org/abs/2403.16297v1","category":"stat.ME"}
{"created":"2024-03-24 21:02:35","title":"LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation","abstract":"With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well. As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language. Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document's context will support such harmonized legal definitions across different regulations and thus avoid ambiguities. In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using retrieval augmented generation (RAG) and existing term definitions present in different legislative documents. For this, definition elements are built by extracting definitions from existing documents. Using definition elements and RAG, a Definitions article can be suggested on demand for a legislative document that is being drafted. We demonstrate and evaluate the functionality of LexDrafter using a collection of EU documents from the energy domain. The code for LexDrafter framework is available at https://github.com/achouhan93/LexDrafter.","sentences":["With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well.","As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language.","Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document's context will support such harmonized legal definitions across different regulations and thus avoid ambiguities.","In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using retrieval augmented generation (RAG) and existing term definitions present in different legislative documents.","For this, definition elements are built by extracting definitions from existing documents.","Using definition elements and RAG, a Definitions article can be suggested on demand for a legislative document that is being drafted.","We demonstrate and evaluate the functionality of LexDrafter using a collection of EU documents from the energy domain.","The code for LexDrafter framework is available at https://github.com/achouhan93/LexDrafter."],"url":"http://arxiv.org/abs/2403.16295v1","category":"cs.CL"}
{"created":"2024-03-24 20:48:36","title":"latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction","abstract":"We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data.","sentences":["We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture.","Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible.","In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data.","The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians.","From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network.","We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data."],"url":"http://arxiv.org/abs/2403.16292v1","category":"cs.CV"}
{"created":"2024-03-24 20:43:29","title":"Guessing human intentions to avoid dangerous situations in caregiving robots","abstract":"For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been conducted to test the implementation's robustness, precision and real-time response, including a simulated scenario, a human-in-the-loop hybrid configuration and a real-world scenario.","sentences":["For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately.","This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided.","This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions.","We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time.","We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people.","Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations.","The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios.","Three experiments have been conducted to test the implementation's robustness, precision and real-time response, including a simulated scenario, a human-in-the-loop hybrid configuration and a real-world scenario."],"url":"http://arxiv.org/abs/2403.16291v1","category":"cs.RO"}
{"created":"2024-03-24 20:41:19","title":"An Information Theoretic Treatment of Animal Movement Tracks","abstract":"The two-dimensional track of an animal on a landscape has progressed over the past three decades from hourly to second-by-second recordings of locations. Track segmentation methods for analyzing the behavioral information in such relocation data has lagged somewhat behind, with scales of analysis currently at the sub-hourly to minute level. A new approach is needed to bring segmentation analysis down to a second-by-second level. Here, such an approach is presented that rests heavily on concepts from Shannon's Information Theory. In this paper, we first briefly review and update concepts relating to movement path segmentation. We then discuss how cluster analysis can be used to organize the smallest viable statistical movement elements (StaMEs), which are $\\mu$ steps long, and to code the next level of movement elements called ``words'' that are $m \\mu$ steps long. Centroids of these word clusters are identified as canonical activity modes (CAMs). Unlike current segmentation schemes, the approach presented here allows us to provide entropy measures for movement paths, compute the coding efficiencies of derived StaMEs and CAMs, and assess error rates in the allocation of strings of $m$ StaMEs to CAM types. In addition our approach allows us to employ the Jensen-Shannon divergence measure to assess and compare the best choices for the various parameters (number of steps in a StaME, number of StaME types, number of StaMEs in a word, number of CAM types), as well as the best clustering methods for generating segments that can then be used to interpret and predict sequences of higher order segments. The theory presented here provides another tool in our toolbox for dealing with the effects of global change on the movement and redistribution of animals across altered landscapes","sentences":["The two-dimensional track of an animal on a landscape has progressed over the past three decades from hourly to second-by-second recordings of locations.","Track segmentation methods for analyzing the behavioral information in such relocation data has lagged somewhat behind, with scales of analysis currently at the sub-hourly to minute level.","A new approach is needed to bring segmentation analysis down to a second-by-second level.","Here, such an approach is presented that rests heavily on concepts from Shannon's Information Theory.","In this paper, we first briefly review and update concepts relating to movement path segmentation.","We then discuss how cluster analysis can be used to organize the smallest viable statistical movement elements (StaMEs), which are $\\mu$ steps long, and to code the next level of movement elements called ``words'' that are $m \\mu$ steps long.","Centroids of these word clusters are identified as canonical activity modes (CAMs).","Unlike current segmentation schemes, the approach presented here allows us to provide entropy measures for movement paths, compute the coding efficiencies of derived StaMEs and CAMs, and assess error rates in the allocation of strings of $m$ StaMEs to CAM types.","In addition our approach allows us to employ the Jensen-Shannon divergence measure to assess and compare the best choices for the various parameters (number of steps in a StaME, number of StaME types, number of StaMEs in a word, number of CAM types), as well as the best clustering methods for generating segments that can then be used to interpret and predict sequences of higher order segments.","The theory presented here provides another tool in our toolbox for dealing with the effects of global change on the movement and redistribution of animals across altered landscapes"],"url":"http://arxiv.org/abs/2403.16290v1","category":"q-bio.PE"}
{"created":"2024-03-24 20:40:51","title":"Engineering Safety Requirements for Autonomous Driving with Large Language Models","abstract":"Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","sentences":["Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps.","Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update.","In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements.","This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements.","We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria.","We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively.","Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency."],"url":"http://arxiv.org/abs/2403.16289v1","category":"cs.AI"}
{"created":"2024-03-24 20:37:33","title":"Study of Workload Interference with Intelligent Routing on Dragonfly","abstract":"Dragonfly interconnect is a crucial network technology for supercomputers. To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair. While link utilization is increased, workload performance is often offset by network contention. Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency. However, its effectiveness in reducing workload interference is unknown. In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system. We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns. We also present two metrics to characterize application communication intensity. Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics. Several key insights are made from the analysis.","sentences":["Dragonfly interconnect is a crucial network technology for supercomputers.","To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair.","While link utilization is increased, workload performance is often offset by network contention.","Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency.","However, its effectiveness in reducing workload interference is unknown.","In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system.","We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns.","We also present two metrics to characterize application communication intensity.","Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics.","Several key insights are made from the analysis."],"url":"http://arxiv.org/abs/2403.16288v1","category":"cs.NI"}
{"created":"2024-03-24 20:08:16","title":"The Evolution of Football Betting- A Machine Learning Approach to Match Outcome Forecasting and Bookmaker Odds Estimation","abstract":"This paper explores the significant history of professional football and the betting industry, tracing its evolution from clandestine beginnings to a lucrative multi-million-pound enterprise. Initiated by the legalization of gambling in 1960 and complemented by advancements in football data gathering pioneered by Thorold Charles Reep, the symbiotic relationship between these sectors has propelled rapid growth and innovation. Over the past six decades, both industries have undergone radical transformations, with data collection methods evolving from rudimentary notetaking to sophisticated technologies such as high-definition cameras and Artificial Intelligence (AI)-driven analytics. Therefore, the primary aim of this study is to utilize Machine Learning (ML) algorithms to forecast premier league football match outcomes. By analyzing historical data and investigating the significance of various features, the study seeks to identify the most effective predictive models and discern key factors influencing match results. Additionally, the study aims to utilize these forecasting to inform the establishment of bookmaker odds, providing insights into the impact of different variables on match outcomes. By highlighting the potential for informed decision-making in sports forecasting and betting, this study opens up new avenues for research and practical applications in the domain of sports analytics.","sentences":["This paper explores the significant history of professional football and the betting industry, tracing its evolution from clandestine beginnings to a lucrative multi-million-pound enterprise.","Initiated by the legalization of gambling in 1960 and complemented by advancements in football data gathering pioneered by Thorold Charles Reep, the symbiotic relationship between these sectors has propelled rapid growth and innovation.","Over the past six decades, both industries have undergone radical transformations, with data collection methods evolving from rudimentary notetaking to sophisticated technologies such as high-definition cameras and Artificial Intelligence (AI)-driven analytics.","Therefore, the primary aim of this study is to utilize Machine Learning (ML) algorithms to forecast premier league football match outcomes.","By analyzing historical data and investigating the significance of various features, the study seeks to identify the most effective predictive models and discern key factors influencing match results.","Additionally, the study aims to utilize these forecasting to inform the establishment of bookmaker odds, providing insights into the impact of different variables on match outcomes.","By highlighting the potential for informed decision-making in sports forecasting and betting, this study opens up new avenues for research and practical applications in the domain of sports analytics."],"url":"http://arxiv.org/abs/2403.16282v1","category":"cs.LG"}
{"created":"2024-03-24 19:52:53","title":"Combined Task and Motion Planning Via Sketch Decompositions (Extended Version with Supplementary Material)","abstract":"The challenge in combined task and motion planning (TAMP) is the effective integration of a search over a combinatorial space, usually carried out by a task planner, and a search over a continuous configuration space, carried out by a motion planner. Using motion planners for testing the feasibility of task plans and filling out the details is not effective because it makes the geometrical constraints play a passive role. This work introduces a new interleaved approach for integrating the two dimensions of TAMP that makes use of sketches, a recent simple but powerful language for expressing the decomposition of problems into subproblems. A sketch has width 1 if it decomposes the problem into subproblems that can be solved greedily in linear time. In the paper, a general sketch is introduced for several classes of TAMP problems which has width 1 under suitable assumptions. While sketch decompositions have been developed for classical planning, they offer two important benefits in the context of TAMP. First, when a task plan is found to be unfeasible due to the geometric constraints, the combinatorial search resumes in a specific sub-problem. Second, the sampling of object configurations is not done once, globally, at the start of the search, but locally, at the start of each subproblem. Optimizations of this basic setting are also considered and experimental results over existing and new pick-and-place benchmarks are reported.","sentences":["The challenge in combined task and motion planning (TAMP) is the effective integration of a search over a combinatorial space, usually carried out by a task planner, and a search over a continuous configuration space, carried out by a motion planner.","Using motion planners for testing the feasibility of task plans and filling out the details is not effective because it makes the geometrical constraints play a passive role.","This work introduces a new interleaved approach for integrating the two dimensions of TAMP that makes use of sketches, a recent simple but powerful language for expressing the decomposition of problems into subproblems.","A sketch has width 1 if it decomposes the problem into subproblems that can be solved greedily in linear time.","In the paper, a general sketch is introduced for several classes of TAMP problems which has width 1 under suitable assumptions.","While sketch decompositions have been developed for classical planning, they offer two important benefits in the context of TAMP.","First, when a task plan is found to be unfeasible due to the geometric constraints, the combinatorial search resumes in a specific sub-problem.","Second, the sampling of object configurations is not done once, globally, at the start of the search, but locally, at the start of each subproblem.","Optimizations of this basic setting are also considered and experimental results over existing and new pick-and-place benchmarks are reported."],"url":"http://arxiv.org/abs/2403.16277v1","category":"cs.RO"}
{"created":"2024-03-24 19:50:49","title":"AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue","abstract":"In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information. Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments. Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos. We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task.","sentences":["In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD).","While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited.","Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively.","To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations.","We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information.","Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments.","Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos.","We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task."],"url":"http://arxiv.org/abs/2403.16276v1","category":"cs.CV"}
{"created":"2024-03-24 19:47:37","title":"M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and Scheduling","abstract":"In this paper, we present a novel problem coined multi-robot, multi-objective, and multi-mode routing and scheduling (M^3RS). The formulation for M^3RS is introduced for time-bound multi-robot, multi-objective routing and scheduling missions where each task has multiple execution modes. Different execution modes have distinct resource consumption, associated execution time, and quality. M^3RS assigns the optimal sequence of tasks and the execution modes to each agent. The routes and associated modes depend on user preferences for different objective criteria. The need for M^3RS comes from multi-robot applications in which a trade-off between multiple criteria arises from different task execution modes. We use M^3RS for the application of multi-robot disinfection in public locations. The objectives considered for disinfection application are disinfection quality and number of tasks completed. A mixed-integer linear programming model is proposed for M^3RS. Then, a time-efficient column generation scheme is presented to tackle the issue of computation times for larger problem instances. The advantage of using multiple modes over fixed execution mode is demonstrated using experiments on synthetic data. The results suggest that M^3RS provides flexibility to the user in terms of available solutions and performs well in joint performance metrics. The application of the proposed problem is shown for a team of disinfection robots.} The videos for the experiments are available on the project website: https://sites.google.com/view/g-robot/m3rs/ .","sentences":["In this paper, we present a novel problem coined multi-robot, multi-objective, and multi-mode routing and scheduling (M^3RS).","The formulation for M^3RS is introduced for time-bound multi-robot, multi-objective routing and scheduling missions where each task has multiple execution modes.","Different execution modes have distinct resource consumption, associated execution time, and quality.","M^3RS assigns the optimal sequence of tasks and the execution modes to each agent.","The routes and associated modes depend on user preferences for different objective criteria.","The need for M^3RS comes from multi-robot applications in which a trade-off between multiple criteria arises from different task execution modes.","We use M^3RS for the application of multi-robot disinfection in public locations.","The objectives considered for disinfection application are disinfection quality and number of tasks completed.","A mixed-integer linear programming model is proposed for M^3RS.","Then, a time-efficient column generation scheme is presented to tackle the issue of computation times for larger problem instances.","The advantage of using multiple modes over fixed execution mode is demonstrated using experiments on synthetic data.","The results suggest that M^3RS provides flexibility to the user in terms of available solutions and performs well in joint performance metrics.","The application of the proposed problem is shown for a team of disinfection robots.}","The videos for the experiments are available on the project website: https://sites.google.com/view/g-robot/m3rs/ ."],"url":"http://arxiv.org/abs/2403.16275v1","category":"cs.RO"}
{"created":"2024-03-24 19:43:13","title":"Long-lived, pulse-induced absorption in $\\mathrm{LiNb}_{1-x}\\mathrm{Ta}_x\\mathrm{O}_3$ solid solutions: the case of three intrinsic defect sites for electron localization with strong coupling","abstract":"Femto-/nanosecond pulse-induced, red and near-infrared absorption is studied in $\\mathrm{LiNb}_{1-x}\\mathrm{Ta}_{x}\\mathrm{O}_3$ (LNT) solid solutions with the goal to probe the intrinsic defect structure via the formation, transport and recombination of optically generated small bound electron polarons with strong coupling to the lattice. As a result, long-lived transients are uncovered for LNT which exceed lifetimes of LN and LT by a factor of up to 100 over the entire range of investigated compositions. At the same time, the starting amplitude varies in the range of $\\alpha_\\mathrm{li}^0\\approx10-100\\,\\mathrm{m}^{-1}$ as a function of $x$ and exceed the ones of LN and LT by a factor of up to ten. The results are interpreted in the model of three-dimensional small polaron hopping transport considering the simultaneous presence of three different types of small bound polarons, in particular of small electron $\\mathrm{Nb}_\\mathrm{Li}^{4+}$ and $\\mathrm{Ta}_\\mathrm{Li}^{4+}$ antisite polarons, and of small electron $\\mathrm{Ta}_\\mathrm{V}^{4+}$ interstitial polarons. We conclude that the differences between LNT, LN, and LT may point to model systems that consist of one (LN), two (LT) and three (LNT) intrinsic defect centers for electron localization.","sentences":["Femto-/nanosecond pulse-induced, red and near-infrared absorption is studied in $\\mathrm{LiNb}_{1-x}\\mathrm{Ta}_{x}\\mathrm{O}_3$ (LNT) solid solutions with the goal to probe the intrinsic defect structure via the formation, transport and recombination of optically generated small bound electron polarons with strong coupling to the lattice.","As a result, long-lived transients are uncovered for LNT which exceed lifetimes of LN and LT by a factor of up to 100 over the entire range of investigated compositions.","At the same time, the starting amplitude varies in the range of $\\alpha_\\mathrm{li}^0\\approx10-100\\,\\mathrm{m}^{-1}$ as a function of $x$ and exceed the ones of LN and LT by a factor of up to ten.","The results are interpreted in the model of three-dimensional small polaron hopping transport considering the simultaneous presence of three different types of small bound polarons, in particular of small electron $\\mathrm{Nb}_\\mathrm{Li}^{4+}$ and $\\mathrm{Ta}_\\mathrm{Li}^{4+}$ antisite polarons, and of small electron $\\mathrm{Ta}_\\mathrm{V}^{4+}$ interstitial polarons.","We conclude that the differences between LNT, LN, and LT may point to model systems that consist of one (LN), two (LT) and three (LNT) intrinsic defect centers for electron localization."],"url":"http://arxiv.org/abs/2403.16274v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-24 19:34:33","title":"L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction","abstract":"Pre-training strategies based on self-supervised learning (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision. Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging. Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support. In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known Transformer-based MAE. In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking. Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends. The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progression. Using OPHDIAT, a large follow-up screening dataset targeting diabetic retinopathy (DR), we evaluated the pre-trained weights on a longitudinal task, which is to predict the severity label of the next visit within 3 years based on the past time series examinations. Our results demonstrated the relevancy of both time-aware position embedding and masking strategies based on disease progression knowledge. Compared to popular baseline models and standard longitudinal Transformers, these simple yet effective extensions significantly enhance the predictive ability of deep classification models.","sentences":["Pre-training strategies based on self-supervised learning (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision.","Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging.","Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support.","In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known Transformer-based MAE.","In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking.","Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends.","The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progression.","Using OPHDIAT, a large follow-up screening dataset targeting diabetic retinopathy (DR), we evaluated the pre-trained weights on a longitudinal task, which is to predict the severity label of the next visit within 3 years based on the past time series examinations.","Our results demonstrated the relevancy of both time-aware position embedding and masking strategies based on disease progression knowledge.","Compared to popular baseline models and standard longitudinal Transformers, these simple yet effective extensions significantly enhance the predictive ability of deep classification models."],"url":"http://arxiv.org/abs/2403.16272v1","category":"cs.CV"}
{"created":"2024-03-24 18:57:58","title":"An extended scheme of classical special functions","abstract":"A unifying scheme of classical special functions of hypergeometric type obeying orthogonality or biorthogonality relations is described. It expands the Askey scheme of classical orthogonal polynomials and its $q$-analogue based on the Askey--Wilson polynomials. On the top, it has two-index biorthogonal functions formed from elliptic hypergeometric series with the absolutely continuous measure determined by the elliptic beta integral. A new result is an inclusion of complex hypergeometric functions into the scheme. Its further potential generalizations are discussed as well.","sentences":["A unifying scheme of classical special functions of hypergeometric type obeying orthogonality or biorthogonality relations is described.","It expands the Askey scheme of classical orthogonal polynomials and its $q$-analogue based on the Askey--Wilson polynomials.","On the top, it has two-index biorthogonal functions formed from elliptic hypergeometric series with the absolutely continuous measure determined by the elliptic beta integral.","A new result is an inclusion of complex hypergeometric functions into the scheme.","Its further potential generalizations are discussed as well."],"url":"http://arxiv.org/abs/2403.16264v1","category":"math.CA"}
{"created":"2024-03-24 18:49:16","title":"HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under Unknown Vertical Ground Motion","abstract":"This paper presents a hierarchical control framework that enables robust quadrupedal locomotion on a dynamic rigid surface (DRS) with general and unknown vertical motions. The key novelty of the framework lies in its higher layer, which is a discrete-time, provably stabilizing footstep controller. The basis of the footstep controller is a new hybrid, time-varying, linear inverted pendulum (HT-LIP) model that is low-dimensional and accurately captures the essential robot dynamics during DRS locomotion. A new set of sufficient stability conditions are then derived to directly guide the controller design for ensuring the asymptotic stability of the HT-LIP model under general, unknown, vertical DRS motions. Further, the footstep controller is cast as a computationally efficient quadratic program that incorporates the proposed HT-LIP model and stability conditions. The middle layer takes the desired footstep locations generated by the higher layer as input to produce kinematically feasible full-body reference trajectories, which are then accurately tracked by a lower-layer torque controller. Hardware experiments on a Unitree Go1 quadrupedal robot confirm the robustness of the proposed framework under various unknown, aperiodic, vertical DRS motions and uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and sudden pushes).","sentences":["This paper presents a hierarchical control framework that enables robust quadrupedal locomotion on a dynamic rigid surface (DRS) with general and unknown vertical motions.","The key novelty of the framework lies in its higher layer, which is a discrete-time, provably stabilizing footstep controller.","The basis of the footstep controller is a new hybrid, time-varying, linear inverted pendulum (HT-LIP) model that is low-dimensional and accurately captures the essential robot dynamics during DRS locomotion.","A new set of sufficient stability conditions are then derived to directly guide the controller design for ensuring the asymptotic stability of the HT-LIP model under general, unknown, vertical DRS motions.","Further, the footstep controller is cast as a computationally efficient quadratic program that incorporates the proposed HT-LIP model and stability conditions.","The middle layer takes the desired footstep locations generated by the higher layer as input to produce kinematically feasible full-body reference trajectories, which are then accurately tracked by a lower-layer torque controller.","Hardware experiments on a Unitree Go1 quadrupedal robot confirm the robustness of the proposed framework under various unknown, aperiodic, vertical DRS motions and uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and sudden pushes)."],"url":"http://arxiv.org/abs/2403.16262v1","category":"cs.RO"}
{"created":"2024-03-24 18:43:04","title":"Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble","abstract":"Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into distinct supervision tasks. This innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages diverse training tasks to generate distinct comprehensions of the data and labels, thereby extending the feature representation field.   Our experimental results demonstrate the superior performance of the MC Ensemble strategy in OOD detection compared to both the naive Deep Ensemble method and a standalone model of comparable size. This underscores the effectiveness of our proposed approach in enhancing the model's capability to detect instances outside its training distribution.","sentences":["Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection.","Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   ","However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods.","We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   ","To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into distinct supervision tasks.","This innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages diverse training tasks to generate distinct comprehensions of the data and labels, thereby extending the feature representation field.   ","Our experimental results demonstrate the superior performance of the MC Ensemble strategy in OOD detection compared to both the naive Deep Ensemble method and a standalone model of comparable size.","This underscores the effectiveness of our proposed approach in enhancing the model's capability to detect instances outside its training distribution."],"url":"http://arxiv.org/abs/2403.16260v1","category":"cs.LG"}
{"created":"2024-03-24 18:40:37","title":"On the effective generation of direct images of pluricanonical bundles in mixed characteristic","abstract":"We present an effective global generation result for direct images of pluricanonical bundles in mixed characteristic. This is a mixed characteristic analog of Ejiri's theorem in positive characteristic and the theorem of Popa and Schnell regarding their Fujita-type conjecture in characteristic zero. As an application, we establish a weak positivity statement for the relative canonical sheaf of a smooth morphism in mixed characteristic. Using this weak positivity result, we prove that images of Fano schemes under smooth morphisms are again Fano in mixed characteristic.","sentences":["We present an effective global generation result for direct images of pluricanonical bundles in mixed characteristic.","This is a mixed characteristic analog of Ejiri's theorem in positive characteristic and the theorem of Popa and Schnell regarding their Fujita-type conjecture in characteristic zero.","As an application, we establish a weak positivity statement for the relative canonical sheaf of a smooth morphism in mixed characteristic.","Using this weak positivity result, we prove that images of Fano schemes under smooth morphisms are again Fano in mixed characteristic."],"url":"http://arxiv.org/abs/2403.16259v1","category":"math.AG"}
{"created":"2024-03-24 18:33:16","title":"Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis","abstract":"While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.","sentences":["While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels.","To address this limitation, we adopt a non-isotropic diffusion model at the decoder side.","This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images.","Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step.","This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk.","The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks.","The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster.","Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings."],"url":"http://arxiv.org/abs/2403.16258v1","category":"eess.IV"}
{"created":"2024-03-24 18:15:36","title":"Instantaneous control strategies for magnetically confined fusion plasma","abstract":"The principle behind magnetic fusion is to confine high temperature plasma inside a device in such a way that the nuclei of deuterium and tritium joining together can release energy. The high temperatures generated needs the plasma to be isolated from the wall of the device to avoid damages and the scope of external magnetic fields is to achieve this goal. In this paper, to face this challenge from a numerical perspective, we propose an instantaneous control mathematical approach to steer a plasma into a given spatial region. From the modeling point of view, we focus on the Vlasov equation in a bounded domain with self induced electric field and an external strong magnetic field. The main feature of the control strategy employed is that it provides a feedback on the equation of motion based on an instantaneous prediction of the discretized system. This permits to directly embed the minimization of a given cost functional into the particle interactions of the corresponding Vlasov model. The numerical results demonstrate the validity of our control approach and the capability of an external magnetic field, even if in a simplified setting, to lead the plasma far from the boundaries.","sentences":["The principle behind magnetic fusion is to confine high temperature plasma inside a device in such a way that the nuclei of deuterium and tritium joining together can release energy.","The high temperatures generated needs the plasma to be isolated from the wall of the device to avoid damages and the scope of external magnetic fields is to achieve this goal.","In this paper, to face this challenge from a numerical perspective, we propose an instantaneous control mathematical approach to steer a plasma into a given spatial region.","From the modeling point of view, we focus on the Vlasov equation in a bounded domain with self induced electric field and an external strong magnetic field.","The main feature of the control strategy employed is that it provides a feedback on the equation of motion based on an instantaneous prediction of the discretized system.","This permits to directly embed the minimization of a given cost functional into the particle interactions of the corresponding Vlasov model.","The numerical results demonstrate the validity of our control approach and the capability of an external magnetic field, even if in a simplified setting, to lead the plasma far from the boundaries."],"url":"http://arxiv.org/abs/2403.16254v1","category":"math.NA"}
{"created":"2024-03-25 11:53:49","title":"Algorithms and data structures for numerical computations with automatic precision estimation","abstract":"We introduce data structures and algorithms to count numerical inaccuracies arising from usage of floating numbers described in IEEE 754. Here we describe how to estimate precision for some collection of functions most commonly used for array manipulations and training of neural networks. For highly optimized functions like matrix multiplication, we provide a fast estimation of precision and some hint how the estimation can be strengthened.","sentences":["We introduce data structures and algorithms to count numerical inaccuracies arising from usage of floating numbers described in IEEE 754.","Here we describe how to estimate precision for some collection of functions most commonly used for array manipulations and training of neural networks.","For highly optimized functions like matrix multiplication, we provide a fast estimation of precision and some hint how the estimation can be strengthened."],"url":"http://arxiv.org/abs/2403.16660v1","category":"math.NA"}
{"created":"2024-03-25 11:49:24","title":"Six new eccentric eclipsing systems with a third body","abstract":"We present the discovery of six new triple stellar system candidates composed of an inner eccentric-orbit eclipsing binary with an apsidal motion. These stars were studied using new, precise TESS light curves and a long-term collection of older photometric ground-based data. These data were used for the monitoring of ETVs (eclipse timing variations) and to detect the slow apsidal movements along with additional periodic signals. The systems analysed were ASASSN-V J012214.37+643943.3 (orbital period 2.01156 d, eccentricity 0.15, third body with 3.3 yr period); ASASSN-V J052227.78+345257.6 (2.42673 d, 0.35, 3.2 yr); ASASSN-V J203158.98+410731.4 (2.53109 d, 0.20, 2.7 yr); ASASSN-V J230945.10+605349.3 (2.08957 d, 0.18, 2.3 yr); ASASSN-V J231028.27+590841.8 (2.41767 d, 0.43, 4.9 yr); and NSV 14698 (3.30047 d, 0.147, 0.5 yr). In the system ASASSN-V J230945.10+605349.3, we detected a second eclipsing pair (per 2.99252 d) and found adequate ETV for the pair B, proving its 2+2 bound quadruple nature. All of these detected systems deserve special attention from long-term studies for their three-body dynamics since their outer orbital periods are not too long and because some dynamical effects should be detectable during the next decades. The system NSV 14698 especially seems to be the most interesting from the dynamical point of view due to it having the shortest outer period of the systems we studied, its fast apsidal motion, and its possible orbital changes during the whole 20th century.","sentences":["We present the discovery of six new triple stellar system candidates composed of an inner eccentric-orbit eclipsing binary with an apsidal motion.","These stars were studied using new, precise TESS light curves and a long-term collection of older photometric ground-based data.","These data were used for the monitoring of ETVs (eclipse timing variations) and to detect the slow apsidal movements along with additional periodic signals.","The systems analysed were ASASSN-V J012214.37+643943.3 (orbital period 2.01156 d, eccentricity 0.15, third body with 3.3 yr period); ASASSN-V J052227.78+345257.6 (2.42673 d, 0.35, 3.2 yr); ASASSN-V J203158.98+410731.4 (2.53109 d, 0.20, 2.7 yr); ASASSN-V J230945.10+605349.3 (2.08957 d, 0.18, 2.3 yr); ASASSN-V J231028.27+590841.8 (2.41767 d, 0.43, 4.9 yr); and NSV 14698 (3.30047 d, 0.147, 0.5 yr).","In the system ASASSN-V J230945.10+605349.3, we detected a second eclipsing pair (per 2.99252 d) and found adequate ETV for the pair B, proving its 2+2 bound quadruple nature.","All of these detected systems deserve special attention from long-term studies for their three-body dynamics since their outer orbital periods are not too long and because some dynamical effects should be detectable during the next decades.","The system NSV 14698 especially seems to be the most interesting from the dynamical point of view due to it having the shortest outer period of the systems we studied, its fast apsidal motion, and its possible orbital changes during the whole 20th century."],"url":"http://arxiv.org/abs/2403.16657v1","category":"astro-ph.SR"}
{"created":"2024-03-25 11:37:39","title":"Probing Stellar Clusters from Gaia DR2 as Galactic PeVatrons: I -- Expected Gamma-ray and Neutrino Emission","abstract":"Young & massive stellar clusters (SCs) are a potential source of galactic cosmic rays up to very high energies as a result of two possible acceleration scenarios. Collective stellar winds from massive member stars form a wind-blown bubble with a termination shock (TS) at which particle acceleration to PeV energies may occur. Furthermore, shock acceleration may occur at SNRs expanding inside the bubble. By applying a model of CR acceleration at both the wind TS and SNR shocks to catalogues of known SCs derived from Gaia DR2, we identify the most promising targets to search for evidence of PeVatron activity. Predictions for the secondary fluxes of gamma-ray and neutrino emission are derived based on particle acceleration at the collective wind TS and the subsequent hadronic interactions with the surrounding medium. Predictions from our modelling under baseline and optimistic scenarios are compared to data, finding consistent results. We estimate the detection prospects for future gamma-ray and neutrino experiments. We find that degree-scale angular sizes of the wind-blown bubbles are typical, that may pose a challenge for experimental detection. A shortlist of the most promising candidates is provided, with an anticipated flux range. Of order 10 SCs may be detectable with future facilities, and 1-5 could be currently operating as PeVatrons. Of these, three gamma-ray detected SCs have data within our predicted range. Our model can consistently describe gamma-ray measurements of SC emission. Several further as-yet-undetected SCs offer promising targets for future observations, although the flux range allowed by our model can be large (> factor 10). The large angular size of the wind-blown bubble may lead to low surface brightness emission, worsening the problem of source confusion. Nevertheless, we discuss how further work will help to constrain SCs as PeVatron candidates. (abridged)","sentences":["Young & massive stellar clusters (SCs) are a potential source of galactic cosmic rays up to very high energies as a result of two possible acceleration scenarios.","Collective stellar winds from massive member stars form a wind-blown bubble with a termination shock (TS) at which particle acceleration to PeV energies may occur.","Furthermore, shock acceleration may occur at SNRs expanding inside the bubble.","By applying a model of CR acceleration at both the wind TS and SNR shocks to catalogues of known SCs derived from Gaia DR2, we identify the most promising targets to search for evidence of PeVatron activity.","Predictions for the secondary fluxes of gamma-ray and neutrino emission are derived based on particle acceleration at the collective wind TS and the subsequent hadronic interactions with the surrounding medium.","Predictions from our modelling under baseline and optimistic scenarios are compared to data, finding consistent results.","We estimate the detection prospects for future gamma-ray and neutrino experiments.","We find that degree-scale angular sizes of the wind-blown bubbles are typical, that may pose a challenge for experimental detection.","A shortlist of the most promising candidates is provided, with an anticipated flux range.","Of order 10 SCs may be detectable with future facilities, and 1-5 could be currently operating as PeVatrons.","Of these, three gamma-ray detected SCs have data within our predicted range.","Our model can consistently describe gamma-ray measurements of SC emission.","Several further as-yet-undetected SCs offer promising targets for future observations, although the flux range allowed by our model can be large (> factor 10).","The large angular size of the wind-blown bubble may lead to low surface brightness emission, worsening the problem of source confusion.","Nevertheless, we discuss how further work will help to constrain SCs as PeVatron candidates.","(abridged)"],"url":"http://arxiv.org/abs/2403.16650v1","category":"astro-ph.HE"}
{"created":"2024-03-25 11:29:19","title":"Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution","abstract":"Artifact-free super-resolution (SR) aims to translate low-resolution images into their high-resolution counterparts with a strict integrity of the original content, eliminating any distortions or synthetic details. While traditional diffusion-based SR techniques have demonstrated remarkable abilities to enhance image detail, they are prone to artifact introduction during iterative procedures. Such artifacts, ranging from trivial noise to unauthentic textures, deviate from the true structure of the source image, thus challenging the integrity of the super-resolution process. In this work, we propose Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that delves into the latent space to effectively identify and mitigate the propagation of artifacts. Our SARGD begins by using an artifact detector to identify implausible pixels, creating a binary mask that highlights artifacts. Following this, the Reality Guidance Refinement (RGR) process refines artifacts by integrating this mask with realistic latent representations, improving alignment with the original image. Nonetheless, initial realistic-latent representations from lower-quality images result in over-smoothing in the final output. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism. It dynamically computes a reality score, enhancing the sharpness of the realistic latent. These alternating mechanisms collectively achieve artifact-free super-resolution. Extensive experiments demonstrate the superiority of our method, delivering detailed artifact-free high-resolution images while reducing sampling steps by 2X. We release our code at https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.","sentences":["Artifact-free super-resolution (SR) aims to translate low-resolution images into their high-resolution counterparts with a strict integrity of the original content, eliminating any distortions or synthetic details.","While traditional diffusion-based SR techniques have demonstrated remarkable abilities to enhance image detail, they are prone to artifact introduction during iterative procedures.","Such artifacts, ranging from trivial noise to unauthentic textures, deviate from the true structure of the source image, thus challenging the integrity of the super-resolution process.","In this work, we propose Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that delves into the latent space to effectively identify and mitigate the propagation of artifacts.","Our SARGD begins by using an artifact detector to identify implausible pixels, creating a binary mask that highlights artifacts.","Following this, the Reality Guidance Refinement (RGR) process refines artifacts by integrating this mask with realistic latent representations, improving alignment with the original image.","Nonetheless, initial realistic-latent representations from lower-quality images result in over-smoothing in the final output.","To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.","It dynamically computes a reality score, enhancing the sharpness of the realistic latent.","These alternating mechanisms collectively achieve artifact-free super-resolution.","Extensive experiments demonstrate the superiority of our method, delivering detailed artifact-free high-resolution images while reducing sampling steps by 2X. We release our code at https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git."],"url":"http://arxiv.org/abs/2403.16643v1","category":"eess.IV"}
{"created":"2024-03-25 11:28:37","title":"Investigating the Readability of Test Code: Combining Scientific and Practical Views","abstract":"The readability of source code is key for understanding and maintaining software systems and tests. Several studies investigate the readability of source code, but there is limited research on the readability of test code and related influence factors. We investigate the factors that influence the readability of test code from an academic perspective complemented by practical views. First, we perform a Systematic Mapping Study (SMS) with a focus on scientific literature. Second, we extend this study by reviewing grey literature sources for practical aspects on test code readability and understandability. Finally, we conduct a controlled experiment on the readability of a selected set of test cases to collect additional knowledge on influence factors discussed in practice. The result set of the SMS includes 19 primary studies from the scientific literature. The grey literature search reveals 62 sources for information on test code readability. Based on an analysis of these sources, we identified a combined set of 14 factors that influence the readability of test code. 7 of these factors were found in scientific and grey literature, while some factors were mainly discussed in academia (2) or industry (5) with limited overlap. The controlled experiment on practically relevant influence factors showed that the investigated factors have a significant impact on readability for half of the selected test cases. Our review of scientific and grey literature showed that test code readability is of interest for academia and industry with a consensus on key influence factors. However, we also found factors only discussed by practitioners. For some of these factors we were able to confirm an impact on readability in a first experiment. Therefore, we see the need to bring together academic and industry viewpoints to achieve a common view on the readability of software test code.","sentences":["The readability of source code is key for understanding and maintaining software systems and tests.","Several studies investigate the readability of source code, but there is limited research on the readability of test code and related influence factors.","We investigate the factors that influence the readability of test code from an academic perspective complemented by practical views.","First, we perform a Systematic Mapping Study (SMS) with a focus on scientific literature.","Second, we extend this study by reviewing grey literature sources for practical aspects on test code readability and understandability.","Finally, we conduct a controlled experiment on the readability of a selected set of test cases to collect additional knowledge on influence factors discussed in practice.","The result set of the SMS includes 19 primary studies from the scientific literature.","The grey literature search reveals 62 sources for information on test code readability.","Based on an analysis of these sources, we identified a combined set of 14 factors that influence the readability of test code.","7 of these factors were found in scientific and grey literature, while some factors were mainly discussed in academia (2) or industry (5) with limited overlap.","The controlled experiment on practically relevant influence factors showed that the investigated factors have a significant impact on readability for half of the selected test cases.","Our review of scientific and grey literature showed that test code readability is of interest for academia and industry with a consensus on key influence factors.","However, we also found factors only discussed by practitioners.","For some of these factors we were able to confirm an impact on readability in a first experiment.","Therefore, we see the need to bring together academic and industry viewpoints to achieve a common view on the readability of software test code."],"url":"http://arxiv.org/abs/2403.16639v1","category":"cs.SE"}
{"created":"2024-03-25 11:22:38","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for Computations in Matlab","abstract":"Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements. This fact has led to an increasing adoption of GA in applied mathematics and engineering problems. However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with. This prevents wider adoption among more applied professionals. To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License. SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations. It supports both numeric and symbolic computations in high-dimensional GAs. Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature. Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics. Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems.","sentences":["Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements.","This fact has led to an increasing adoption of GA in applied mathematics and engineering problems.","However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with.","This prevents wider adoption among more applied professionals.","To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License.","SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations.","It supports both numeric and symbolic computations in high-dimensional GAs.","Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature.","Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics.","Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems."],"url":"http://arxiv.org/abs/2403.16634v1","category":"cs.MS"}
{"created":"2024-03-25 10:50:19","title":"Preparation of tautomer-pure molecular beams by electrostatic deflection","abstract":"Tautomers are ubiquitous throughout chemistry, and typically considered inseparable in solution. Yet (bio)chemical activity is highly tautomer specific, with common examples being the amino and nucleic acids. While tautomers exist in an equilibrium in solution, in the cold environment of a molecular beam the barrier to tautomerization is typically much too high for interconversion, and tautomers can be considered separate species. Here we demonstrate the separation of tautomers and production of tautomerically-pure gas-phase samples. We show this for the 2-pyridone / 2-hydroxypyridine system, an important structural motif in both uracil and cytosine. Spatial separation of the tautomers is achieved via electrostatic deflection in strong inhomogeneous fields. We furthermore collect tautomer-resolved photoelectron spectra using femtosecond multiphoton ionization. This paves the way for studying the structure-function-dynamic relationship on the level of individual tautomers, using approaches that typically lack the resolution to do so, such as ultrafast dynamics experiments.","sentences":["Tautomers are ubiquitous throughout chemistry, and typically considered inseparable in solution.","Yet (bio)chemical activity is highly tautomer specific, with common examples being the amino and nucleic acids.","While tautomers exist in an equilibrium in solution, in the cold environment of a molecular beam the barrier to tautomerization is typically much too high for interconversion, and tautomers can be considered separate species.","Here we demonstrate the separation of tautomers and production of tautomerically-pure gas-phase samples.","We show this for the 2-pyridone / 2-hydroxypyridine system, an important structural motif in both uracil and cytosine.","Spatial separation of the tautomers is achieved via electrostatic deflection in strong inhomogeneous fields.","We furthermore collect tautomer-resolved photoelectron spectra using femtosecond multiphoton ionization.","This paves the way for studying the structure-function-dynamic relationship on the level of individual tautomers, using approaches that typically lack the resolution to do so, such as ultrafast dynamics experiments."],"url":"http://arxiv.org/abs/2403.16617v1","category":"physics.chem-ph"}
{"created":"2024-03-25 09:47:07","title":"Identification of Cyclists' Route Choice Criteria","abstract":"The behavior of cyclists when choosing the path to follow along a road network is not uniform. Some of them are mostly interested in minimizing the travelled distance, but some others may also take into account other features such as safety of the roads or pollution. Individuating the different groups of users, estimating the numerical consistency of each of these groups, and reporting the weights assigned by each group to different characteristics of the road network, is quite relevant. Indeed, when decision makers need to assign some budget for infrastructural interventions, they need to know the impact of their decisions, and this is strictly related to the way users perceive different features of the road network. In this paper, we propose an optimization approach to detect the weights assigned to different road features by various user groups, leveraging knowledge of the true paths followed by them, accessible, for example, through data collected by bike-sharing services.","sentences":["The behavior of cyclists when choosing the path to follow along a road network is not uniform.","Some of them are mostly interested in minimizing the travelled distance, but some others may also take into account other features such as safety of the roads or pollution.","Individuating the different groups of users, estimating the numerical consistency of each of these groups, and reporting the weights assigned by each group to different characteristics of the road network, is quite relevant.","Indeed, when decision makers need to assign some budget for infrastructural interventions, they need to know the impact of their decisions, and this is strictly related to the way users perceive different features of the road network.","In this paper, we propose an optimization approach to detect the weights assigned to different road features by various user groups, leveraging knowledge of the true paths followed by them, accessible, for example, through data collected by bike-sharing services."],"url":"http://arxiv.org/abs/2403.16580v1","category":"math.OC"}
{"created":"2024-03-25 09:33:21","title":"Higher-spin gauge theories in three spacetime dimensions","abstract":"These lecture notes provide an introduction to higher-spin gauge theories in three spacetime dimensions, with a focus on their asymptotic symmetries, their holographic description in terms of conformal field theories with W-symmetries as well as on their couplings to scalar matter.","sentences":["These lecture notes provide an introduction to higher-spin gauge theories in three spacetime dimensions, with a focus on their asymptotic symmetries, their holographic description in terms of conformal field theories with W-symmetries as well as on their couplings to scalar matter."],"url":"http://arxiv.org/abs/2403.16567v1","category":"hep-th"}
{"created":"2024-03-25 08:58:29","title":"Imaging quantum interference in a monolayer Kitaev quantum spin liquid candidate","abstract":"Single atomic defects are prominent windows to look into host quantum states because collective responses from the host states emerge as localized states around the defects. Friedel oscillations and Kondo clouds in Fermi liquids are quintessential examples. However, the situation is quite different for quantum spin liquid (QSL), an exotic state of matter with fractionalized quasiparticles and topological order arising from a profound impact of quantum entanglement. Elucidating the underlying local electronic property has been challenging due to the charge neutrality of fractionalized quasiparticles and the insulating nature of QSLs. Here, using spectroscopic-imaging scanning tunneling microscopy, we report atomically resolved images of monolayer $\\alpha$-RuCl$_3$, the most promising Kitaev QSL candidate, on metallic substrates. We find quantum interference in the insulator manifesting as incommensurate and decaying spatial oscillations of the local density of states around defects with a characteristic bias dependence. The oscillation differs from any known spatial structures in its nature and does not exist in other Mott insulators, implying it is an exotic oscillation involved with excitations unique to $\\alpha$-RuCl$_3$. Numerical simulations can reproduce the observed oscillation by assuming that itinerant Majorana fermions of Kitaev QSL are scattered across the Majorana Fermi surface. The oscillation provides a new approach to exploring Kitaev QSLs through the local response against defects like Friedel oscillations in metals.","sentences":["Single atomic defects are prominent windows to look into host quantum states because collective responses from the host states emerge as localized states around the defects.","Friedel oscillations and Kondo clouds in Fermi liquids are quintessential examples.","However, the situation is quite different for quantum spin liquid (QSL), an exotic state of matter with fractionalized quasiparticles and topological order arising from a profound impact of quantum entanglement.","Elucidating the underlying local electronic property has been challenging due to the charge neutrality of fractionalized quasiparticles and the insulating nature of QSLs.","Here, using spectroscopic-imaging scanning tunneling microscopy, we report atomically resolved images of monolayer $\\alpha$-RuCl$_3$, the most promising Kitaev QSL candidate, on metallic substrates.","We find quantum interference in the insulator manifesting as incommensurate and decaying spatial oscillations of the local density of states around defects with a characteristic bias dependence.","The oscillation differs from any known spatial structures in its nature and does not exist in other Mott insulators, implying it is an exotic oscillation involved with excitations unique to $\\alpha$-RuCl$_3$. Numerical simulations can reproduce the observed oscillation by assuming that itinerant Majorana fermions of Kitaev QSL are scattered across the Majorana Fermi surface.","The oscillation provides a new approach to exploring Kitaev QSLs through the local response against defects like Friedel oscillations in metals."],"url":"http://arxiv.org/abs/2403.16553v1","category":"cond-mat.str-el"}
{"created":"2024-03-25 08:28:39","title":"Fluorophore signal and detection enhancement in nanowire biosensors","abstract":"Semiconductor nanowires have been demonstrated as an efficient platform for fluorescence-based biosensors. Here, we study theoretically how GaP nanowires (i) enhance the excitation intensity at the position of fluorophores attached to the nanowire sidewall, (ii) enhance the probability to collect photons emitted from the fluorophores by directing them preferentially into the numerical aperture of collection objective, and (iii) through the Purcell effect increase the quantum yield of fluorophores. We find that the optimum diameter depends strongly on the fluorophore emission wavelength. In addition to an overall signal-detection scheme, we model imaging-based detection of the fluorescence.","sentences":["Semiconductor nanowires have been demonstrated as an efficient platform for fluorescence-based biosensors.","Here, we study theoretically how GaP nanowires (i) enhance the excitation intensity at the position of fluorophores attached to the nanowire sidewall, (ii) enhance the probability to collect photons emitted from the fluorophores by directing them preferentially into the numerical aperture of collection objective, and (iii) through the Purcell effect increase the quantum yield of fluorophores.","We find that the optimum diameter depends strongly on the fluorophore emission wavelength.","In addition to an overall signal-detection scheme, we model imaging-based detection of the fluorescence."],"url":"http://arxiv.org/abs/2403.16537v1","category":"physics.optics"}
{"created":"2024-03-25 07:20:57","title":"Parity-sensitive inhomogeneous dephasing of macroscopic spin ensembles","abstract":"Spin ensembles play a pivotal role in various quantum applications such as metrology and simulating many-body physics. Recent research has proposed utilizing spin cat states to encode logical quantum information, with potentially logical lifetimes on the order of seconds via enhanced collective interactions that scale with system size. We investigate the dynamics of spin cat states under inhomogeneous broadening, revealing a phenomenon termed `parity-sensitive inhomogeneous dephasing': odd cat states are significantly more susceptible to inhomogeneous dephasing compared to even cat states due to parity symmetry. Additionally, from a mean-field analysis of the driven-dissipative dynamics, we identify a synchronization phase transition wherein the ensemble becomes completely dephased beyond a critical inhomogeneous linewidth. Our findings shed light on the stability of collective spin states, important for advancing quantum technologies.","sentences":["Spin ensembles play a pivotal role in various quantum applications such as metrology and simulating many-body physics.","Recent research has proposed utilizing spin cat states to encode logical quantum information, with potentially logical lifetimes on the order of seconds via enhanced collective interactions that scale with system size.","We investigate the dynamics of spin cat states under inhomogeneous broadening, revealing a phenomenon termed `parity-sensitive inhomogeneous dephasing': odd cat states are significantly more susceptible to inhomogeneous dephasing compared to even cat states due to parity symmetry.","Additionally, from a mean-field analysis of the driven-dissipative dynamics, we identify a synchronization phase transition wherein the ensemble becomes completely dephased beyond a critical inhomogeneous linewidth.","Our findings shed light on the stability of collective spin states, important for advancing quantum technologies."],"url":"http://arxiv.org/abs/2403.16491v1","category":"quant-ph"}
{"created":"2024-03-25 07:15:06","title":"ColonyOS -- A Meta-Operating System for Distributed Computing Across Heterogeneous Platform","abstract":"This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC. Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums. This makes it possible to develop AI workflows and applications that can operate across platforms. At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture. These executors collectively form a colony, serving as a unified computing unit. To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict zero-trust security protocol. Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications. These are sent to a Colonies server, which acts as intermediary between applications and the executors. Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor. Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms. Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum. The paper describes design principles and implementation details of ColonyOS.","sentences":["This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC.","Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums.","This makes it possible to develop AI workflows and applications that can operate across platforms.","At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture.","These executors collectively form a colony, serving as a unified computing unit.","To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict zero-trust security protocol.","Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications.","These are sent to a Colonies server, which acts as intermediary between applications and the executors.","Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor.","Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms.","Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum.","The paper describes design principles and implementation details of ColonyOS."],"url":"http://arxiv.org/abs/2403.16486v1","category":"cs.DC"}
{"created":"2024-03-25 07:08:01","title":"Determined Multi-Label Learning via Similarity-Based Prompt","abstract":"In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data. Additionally, we introduce a similarity-based prompt learning method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental prompt with richer semantic information. Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods.","sentences":["In multi-label classification, each training instance is associated with multiple class labels simultaneously.","Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications.","To alleviate this problem, a novel labeling setting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks.","In this novel labeling setting, each training instance is associated with a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates whether the training instance contains the provided class label.","The provided class label is randomly and uniformly selected from the whole candidate labels set.","Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets.","In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data.","Additionally, we introduce a similarity-based prompt learning method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental prompt with richer semantic information.","Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16482v1","category":"cs.LG"}
{"created":"2024-03-25 06:17:54","title":"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm","abstract":"Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language. Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.","sentences":["Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language.","Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations.","However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements.","To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment.","The evaluation paradigm contains three basic elements: metric, data, and algorithm.","Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess.","Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure.","Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP.","The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities.","Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE.","Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice."],"url":"http://arxiv.org/abs/2403.16446v1","category":"cs.CL"}
{"created":"2024-03-25 05:10:34","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras","abstract":"Event cameras are increasingly popular in robotics due to their beneficial features, such as low latency, energy efficiency, and high dynamic range. Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters. These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion. This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly fast adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (slow adaptation). Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database. We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time. To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information. Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods. Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms.","sentences":["Event cameras are increasingly popular in robotics due to their beneficial features, such as low latency, energy efficiency, and high dynamic range.","Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters.","These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion.","This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly fast adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (slow adaptation).","Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database.","We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time.","To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information.","Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods.","Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms."],"url":"http://arxiv.org/abs/2403.16425v1","category":"cs.RO"}
{"created":"2024-03-25 03:56:19","title":"Large-scale Array for Radio Astronomy on the Farside","abstract":"At the Royal Society meeting in 2023, we have mainly presented our lunar orbit array concept called DSL, and also briefly introduced a concept of a lunar surface array, LARAF. As the DSL concept had been presented before, in this article we introduce the LARAF. We propose to build an array in the far side of the Moon, with a master station which handles the data collection and processing, and 20 stations with maximum baseline of 10 km. Each station consists 12 membrane antenna units, and the stations are connected to the master station by power line and optical fiber. The array will make interferometric observation in the 0.1-50 MHz band during the lunar night, powered by regenerated fuel cells (RFCs). The whole array can be carried to the lunar surface with a heavy rocket mission, and deployed with a rover in 8 months. Such an array would be an important step in the long term development of lunar based ultralong wavelength radio astronomy. It has a sufficiently high sensitivity to observe many radio sources in the sky, though still short of the dark age fluctuations. We discuss the possible options in the power supply, data communication, deployment, etc.","sentences":["At the Royal Society meeting in 2023, we have mainly presented our lunar orbit array concept called DSL, and also briefly introduced a concept of a lunar surface array, LARAF.","As the DSL concept had been presented before, in this article we introduce the LARAF.","We propose to build an array in the far side of the Moon, with a master station which handles the data collection and processing, and 20 stations with maximum baseline of 10 km.","Each station consists 12 membrane antenna units, and the stations are connected to the master station by power line and optical fiber.","The array will make interferometric observation in the 0.1-50 MHz band during the lunar night, powered by regenerated fuel cells (RFCs).","The whole array can be carried to the lunar surface with a heavy rocket mission, and deployed with a rover in 8 months.","Such an array would be an important step in the long term development of lunar based ultralong wavelength radio astronomy.","It has a sufficiently high sensitivity to observe many radio sources in the sky, though still short of the dark age fluctuations.","We discuss the possible options in the power supply, data communication, deployment, etc."],"url":"http://arxiv.org/abs/2403.16409v1","category":"astro-ph.IM"}
{"created":"2024-03-25 02:37:18","title":"A new social welfare function with a number of desirable properties","abstract":"By relaxing the dominating set in three ways (e.g., from \"each member beats every non-member\" to \"each member beats or ties every non-member, with an additional requirement that at least one member beat every non-member\"), we propose a new social welfare function, which satisfies a number of desirable properties including Condorcet winner principle, Condorcet loser principle, strong Gehrlein-stability (hence Smith set principle), anonymity, neutrality, weak Pareto, strong Pareto, non-dictatorship, and [independence of irrelevant alternatives (IIA) when the pairwise majority relation is an ordering on the alternative set]. If the pairwise majority relation is complete and transitive, the proposed method yields a collective preference relation that coincides with the input majority relation. It thus shares the same collective preference function on the dichotomous domain with the approval voting and the majority voting. It runs in polynomial time and thus possesses a competitive advantage over a number of computationally intractable voting rules such as the Dodgson's rule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's tournament equilibrium set (TEQ) rule. When it is used in tournaments, its winner belongs to the uncovered set, the top cycle set, the Smith set, and the Schwartz set. In addition, in a tournament where the number of alternatives is not more than 4, its winner set is a subset, sometimes proper, of the Copeland winner set. Whether this attractive argument is still valid in four-more-alternative tournaments remains an open question.","sentences":["By relaxing the dominating set in three ways (e.g., from \"each member beats every non-member\" to \"each member beats or ties every non-member, with an additional requirement that at least one member beat every non-member\"), we propose a new social welfare function, which satisfies a number of desirable properties including Condorcet winner principle, Condorcet loser principle, strong Gehrlein-stability (hence Smith set principle), anonymity, neutrality, weak Pareto, strong Pareto, non-dictatorship, and","[independence of irrelevant alternatives (IIA) when the pairwise majority relation is an ordering on the alternative set].","If the pairwise majority relation is complete and transitive, the proposed method yields a collective preference relation that coincides with the input majority relation.","It thus shares the same collective preference function on the dichotomous domain with the approval voting and the majority voting.","It runs in polynomial time and thus possesses a competitive advantage over a number of computationally intractable voting rules such as the Dodgson's rule, the Kemeny's rule, the Slater's rule, the Banks rule, and the Schwartz's tournament equilibrium set (TEQ) rule.","When it is used in tournaments, its winner belongs to the uncovered set, the top cycle set, the Smith set, and the Schwartz set.","In addition, in a tournament where the number of alternatives is not more than 4, its winner set is a subset, sometimes proper, of the Copeland winner set.","Whether this attractive argument is still valid in four-more-alternative tournaments remains an open question."],"url":"http://arxiv.org/abs/2403.16373v1","category":"econ.TH"}
{"created":"2024-03-25 00:59:35","title":"3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical Image Segmentation","abstract":"Medical image segmentation (MIS) aims to finely segment various organs. It requires grasping global information from both parts and the entire image for better segmenting, and clinically there are often certain requirements for segmentation efficiency. Convolutional neural networks (CNNs) have made considerable achievements in MIS. However, they are difficult to fully collect global context information and their pooling layer may cause information loss. Capsule networks, which combine the benefits of CNNs while taking into account additional information such as relative location that CNNs do not, have lately demonstrated some advantages in MIS. Vision Transformer (ViT) employs transformers in visual tasks. Transformer based on attention mechanism has excellent global inductive modeling capabilities and is expected to capture longrange information. Moreover, there have been resent studies on making ViT more lightweight to minimize model complexity and increase efficiency. In this paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps, which combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our encoder uses capsule blocks and EfficientViT blocks to jointly capture local and global semantic information more effectively and efficiently with less information loss, while the decoder employs CNN blocks and EfficientViT blocks to catch ffner details for segmentation. We conduct experiments on various datasets, including iSeg-2017, Hippocampus and Cardiac to verify the performance and efficiency of 3D-EffiViTCaps, which performs better than previous 3D CNN-based, 3D Capsule-based and 3D Transformer-based models. We further implement a series of ablation experiments on the main blocks. Our code is available at: https://github.com/HidNeuron/3D-EffiViTCaps.","sentences":["Medical image segmentation (MIS) aims to finely segment various organs.","It requires grasping global information from both parts and the entire image for better segmenting, and clinically there are often certain requirements for segmentation efficiency.","Convolutional neural networks (CNNs) have made considerable achievements in MIS.","However, they are difficult to fully collect global context information and their pooling layer may cause information loss.","Capsule networks, which combine the benefits of CNNs while taking into account additional information such as relative location that CNNs do not, have lately demonstrated some advantages in MIS.","Vision Transformer (ViT) employs transformers in visual tasks.","Transformer based on attention mechanism has excellent global inductive modeling capabilities and is expected to capture longrange information.","Moreover, there have been resent studies on making ViT more lightweight to minimize model complexity and increase efficiency.","In this paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps, which combines 3D capsule blocks with 3D EfficientViT blocks for MIS.","Our encoder uses capsule blocks and EfficientViT blocks to jointly capture local and global semantic information more effectively and efficiently with less information loss, while the decoder employs CNN blocks and EfficientViT blocks to catch ffner details for segmentation.","We conduct experiments on various datasets, including iSeg-2017, Hippocampus and Cardiac to verify the performance and efficiency of 3D-EffiViTCaps, which performs better than previous 3D CNN-based, 3D Capsule-based and 3D Transformer-based models.","We further implement a series of ablation experiments on the main blocks.","Our code is available at: https://github.com/HidNeuron/3D-EffiViTCaps."],"url":"http://arxiv.org/abs/2403.16350v1","category":"eess.IV"}
{"created":"2024-03-24 22:35:54","title":"Datasets of Great Britain Primary Substations Integrated with Household Heating Information","abstract":"The growing demand for electrified heating, electrified transportation, and power-intensive data centres challenge distribution networks. If electrification projects are carried out without considering electrical distribution infrastructure, there could be unexpected blackouts and financial losses. Datasets containing real-world distribution network information are required to address this. On the other hand, social data, such as household heating composition, are closely coupled with people's lives. Studying the coupling between the energy system and society is important in promoting social welfare. To fill these gaps, this paper introduces two datasets. The first is the main dataset for the distribution networks in Great Britain (GB), collecting information on firm capacity, peak demands, locations, and parent transmission nodes (the Grid Supply Point, namely GSP) for all primary substations (PSs). PSs are a crucial part of the UK distribution network and are at the lowest voltage level (11 kV) with publicly available data for most UK Distribution Network Operators (DNOs). Substation firm capacity and peak demand facilitate an understanding of the remaining room of the existing network. The parent GSP information helps link the dataset of distribution networks to datasets of transmission networks. The second dataset extends the main network dataset, linking each PS to information about the number of households that use different types of central heating recorded in census data. The derivation of the second dataset is based on locations of PSs collected in the main dataset with appropriate assumptions. The derivation process may also be replicated to integrate other social datasets.","sentences":["The growing demand for electrified heating, electrified transportation, and power-intensive data centres challenge distribution networks.","If electrification projects are carried out without considering electrical distribution infrastructure, there could be unexpected blackouts and financial losses.","Datasets containing real-world distribution network information are required to address this.","On the other hand, social data, such as household heating composition, are closely coupled with people's lives.","Studying the coupling between the energy system and society is important in promoting social welfare.","To fill these gaps, this paper introduces two datasets.","The first is the main dataset for the distribution networks in Great Britain (GB), collecting information on firm capacity, peak demands, locations, and parent transmission nodes (the Grid Supply Point, namely GSP) for all primary substations (PSs).","PSs are a crucial part of the UK distribution network and are at the lowest voltage level (11 kV) with publicly available data for most UK Distribution Network Operators (DNOs).","Substation firm capacity and peak demand facilitate an understanding of the remaining room of the existing network.","The parent GSP information helps link the dataset of distribution networks to datasets of transmission networks.","The second dataset extends the main network dataset, linking each PS to information about the number of households that use different types of central heating recorded in census data.","The derivation of the second dataset is based on locations of PSs collected in the main dataset with appropriate assumptions.","The derivation process may also be replicated to integrate other social datasets."],"url":"http://arxiv.org/abs/2403.16313v1","category":"eess.SY"}
{"created":"2024-03-24 19:54:30","title":"A descent basis for the Garsia-Procesi module","abstract":"We assign to each Young diagram $\\lambda$ a subset $\\mathcal{B}_{\\lambda'}$ of the collection of Garsia-Stanton descent monomials, and prove that it determines a basis of the Garsia-Procesi module $R_\\lambda$, whose graded character is the Hall-Littlewood polynomial $\\tilde{H}_{\\lambda}[X;t]$. This basis is a major index analogue of the basis $\\mathcal{B}_\\lambda \\subset R_\\lambda$ defined by certain recursions in due to Garsia and Procesi, in the same way that the descent basis is related to the Artin basis of the coinvariant algebra $R_n$, which in fact corresponds to the case when $\\lambda=1^n$. By anti-symmetrizing a subset of this basis with respect to the corresponding Young subgroup under the Springer action, we obtain a basis in the parabolic case, as well as a corresponding formula for the expansion of $\\tilde{H}_{\\lambda}[X;t]$. Despite a similar appearance, it does not appear obvious how to connect these formulas appear to the specialization of the modified Macdonald formula of Haglund, Haiman and Loehr at $q=0$.","sentences":["We assign to each Young diagram $\\lambda$ a subset $\\mathcal{B}_{\\lambda'}$ of the collection of Garsia-Stanton descent monomials, and prove that it determines a basis of the Garsia-Procesi module $R_\\lambda$, whose graded character is the Hall-Littlewood polynomial $\\tilde{H}_{\\lambda}[X;t]$.","This basis is a major index analogue of the basis $\\mathcal{B}_\\lambda \\subset R_\\lambda$ defined by certain recursions in due to Garsia and Procesi, in the same way that the descent basis is related to the Artin basis of the coinvariant algebra $R_n$, which in fact corresponds to the case when $\\lambda=1^n$. By anti-symmetrizing a subset of this basis with respect to the corresponding Young subgroup under the Springer action, we obtain a basis in the parabolic case, as well as a corresponding formula for the expansion of $\\tilde{H}_{\\lambda}[X;t]$. Despite a similar appearance, it does not appear obvious how to connect these formulas appear to the specialization of the modified Macdonald formula of Haglund, Haiman and Loehr at $q=0$."],"url":"http://arxiv.org/abs/2403.16278v1","category":"math.RT"}
{"created":"2024-03-24 18:53:57","title":"Emotion Recognition from the perspective of Activity Recognition","abstract":"Applications of an efficient emotion recognition system can be found in several domains such as medicine, driver fatigue surveillance, social robotics, and human-computer interaction. Appraising human emotional states, behaviors, and reactions displayed in real-world settings can be accomplished using latent continuous dimensions. Continuous dimensional models of human affect, such as those based on valence and arousal are more accurate in describing a broad range of spontaneous everyday emotions than more traditional models of discrete stereotypical emotion categories (e.g. happiness, surprise). Most of the prior work on estimating valence and arousal considers laboratory settings and acted data. But, for emotion recognition systems to be deployed and integrated into real-world mobile and computing devices, we need to consider data collected in the world. Action recognition is a domain of Computer Vision that involves capturing complementary information on appearance from still frames and motion between frames. In this paper, we treat emotion recognition from the perspective of action recognition by exploring the application of deep learning architectures specifically designed for action recognition, for continuous affect recognition. We propose a novel three-stream end-to-end deep learning regression pipeline with an attention mechanism, which is an ensemble design based on sub-modules of multiple state-of-the-art action recognition systems. The pipeline constitutes a novel data pre-processing approach with a spatial self-attention mechanism to extract keyframes. The optical flow of high-attention regions of the face is extracted to capture temporal context. AFEW-VA in-the-wild dataset has been used to conduct comparative experiments. Quantitative analysis shows that the proposed model outperforms multiple standard baselines of both emotion recognition and action recognition models.","sentences":["Applications of an efficient emotion recognition system can be found in several domains such as medicine, driver fatigue surveillance, social robotics, and human-computer interaction.","Appraising human emotional states, behaviors, and reactions displayed in real-world settings can be accomplished using latent continuous dimensions.","Continuous dimensional models of human affect, such as those based on valence and arousal are more accurate in describing a broad range of spontaneous everyday emotions than more traditional models of discrete stereotypical emotion categories (e.g. happiness, surprise).","Most of the prior work on estimating valence and arousal considers laboratory settings and acted data.","But, for emotion recognition systems to be deployed and integrated into real-world mobile and computing devices, we need to consider data collected in the world.","Action recognition is a domain of Computer Vision that involves capturing complementary information on appearance from still frames and motion between frames.","In this paper, we treat emotion recognition from the perspective of action recognition by exploring the application of deep learning architectures specifically designed for action recognition, for continuous affect recognition.","We propose a novel three-stream end-to-end deep learning regression pipeline with an attention mechanism, which is an ensemble design based on sub-modules of multiple state-of-the-art action recognition systems.","The pipeline constitutes a novel data pre-processing approach with a spatial self-attention mechanism to extract keyframes.","The optical flow of high-attention regions of the face is extracted to capture temporal context.","AFEW-VA in-the-wild dataset has been used to conduct comparative experiments.","Quantitative analysis shows that the proposed model outperforms multiple standard baselines of both emotion recognition and action recognition models."],"url":"http://arxiv.org/abs/2403.16263v1","category":"cs.CV"}
{"created":"2024-03-24 17:21:32","title":"On the Equivalency, Substitutability, and Flexibility of Synthetic Data","abstract":"We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios. Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs. Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection. To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps. Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic data not only enhances model performance but also demonstrates substitutability for real data, with 60% to 80% replacement without performance loss. In addition, our study of the impact of synthetic data distributions on downstream performance reveals the importance of flexible data generators in narrowing domain gaps for improved model adaptability.","sentences":["We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios.","Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs.","Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection.","To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps.","Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17.","Our results suggest that synthetic data not only enhances model performance but also demonstrates substitutability for real data, with 60% to 80% replacement without performance loss.","In addition, our study of the impact of synthetic data distributions on downstream performance reveals the importance of flexible data generators in narrowing domain gaps for improved model adaptability."],"url":"http://arxiv.org/abs/2403.16244v1","category":"cs.LG"}
{"created":"2024-03-24 16:48:10","title":"On machine learning analysis of atomic force microscopy images for image classification, sample surface recognition","abstract":"Atomic force microscopy (AFM or SPM) imaging is one of the best matches with machine learning (ML) analysis among microscopy techniques. The digital format of AFM images allows for direct utilization in ML algorithms without the need for additional processing. Additionally, AFM enables the simultaneous imaging of distributions of over a dozen different physicochemical properties of sample surfaces, a process known as multidimensional imaging. While this wealth of information can be challenging to analyze using traditional methods, ML provides a seamless approach to this task. However, the relatively slow speed of AFM imaging poses a challenge in applying deep learning methods broadly used in image recognition. This Prospective is focused on ML recognition/classification when using a relatively small number of AFM images, small database. We discuss ML methods other than popular deep-learning neural networks. The described approach has already been successfully used to analyze and classify the surfaces of biological cells. It can be applied to recognize medical images, specific material processing, in forensic studies, even to identify the authenticity of arts. A general template for ML analysis specific to AFM is suggested, with a specific example of the identification of cell phenotype. Special attention is given to the analysis of the statistical significance of the obtained results, an important feature that is often overlooked in papers dealing with machine learning. A simple method for finding statistical significance is also described.","sentences":["Atomic force microscopy (AFM or SPM) imaging is one of the best matches with machine learning (ML) analysis among microscopy techniques.","The digital format of AFM images allows for direct utilization in ML algorithms without the need for additional processing.","Additionally, AFM enables the simultaneous imaging of distributions of over a dozen different physicochemical properties of sample surfaces, a process known as multidimensional imaging.","While this wealth of information can be challenging to analyze using traditional methods, ML provides a seamless approach to this task.","However, the relatively slow speed of AFM imaging poses a challenge in applying deep learning methods broadly used in image recognition.","This Prospective is focused on ML recognition/classification when using a relatively small number of AFM images, small database.","We discuss ML methods other than popular deep-learning neural networks.","The described approach has already been successfully used to analyze and classify the surfaces of biological cells.","It can be applied to recognize medical images, specific material processing, in forensic studies, even to identify the authenticity of arts.","A general template for ML analysis specific to AFM is suggested, with a specific example of the identification of cell phenotype.","Special attention is given to the analysis of the statistical significance of the obtained results, an important feature that is often overlooked in papers dealing with machine learning.","A simple method for finding statistical significance is also described."],"url":"http://arxiv.org/abs/2403.16230v1","category":"physics.bio-ph"}
{"created":"2024-03-24 16:41:50","title":"Dual-modal Prior Semantic Guided Infrared and Visible Image Fusion for Intelligent Transportation System","abstract":"Infrared and visible image fusion (IVF) plays an important role in intelligent transportation system (ITS). The early works predominantly focus on boosting the visual appeal of the fused result, and only several recent approaches have tried to combine the high-level vision task with IVF. However, they prioritize the design of cascaded structure to seek unified suitable features and fit different tasks. Thus, they tend to typically bias toward to reconstructing raw pixels without considering the significance of semantic features. Therefore, we propose a novel prior semantic guided image fusion method based on the dual-modality strategy, improving the performance of IVF in ITS. Specifically, to explore the independent significant semantic of each modality, we first design two parallel semantic segmentation branches with a refined feature adaptive-modulation (RFaM) mechanism. RFaM can perceive the features that are semantically distinct enough in each semantic segmentation branch. Then, two pilot experiments based on the two branches are conducted to capture the significant prior semantic of two images, which then is applied to guide the fusion task in the integration of semantic segmentation branches and fusion branches. In addition, to aggregate both high-level semantics and impressive visual effects, we further investigate the frequency response of the prior semantics, and propose a multi-level representation-adaptive fusion (MRaF) module to explicitly integrate the low-frequent prior semantic with the high-frequent details. Extensive experiments on two public datasets demonstrate the superiority of our method over the state-of-the-art image fusion approaches, in terms of either the visual appeal or the high-level semantics.","sentences":["Infrared and visible image fusion (IVF) plays an important role in intelligent transportation system (ITS).","The early works predominantly focus on boosting the visual appeal of the fused result, and only several recent approaches have tried to combine the high-level vision task with IVF.","However, they prioritize the design of cascaded structure to seek unified suitable features and fit different tasks.","Thus, they tend to typically bias toward to reconstructing raw pixels without considering the significance of semantic features.","Therefore, we propose a novel prior semantic guided image fusion method based on the dual-modality strategy, improving the performance of IVF in ITS.","Specifically, to explore the independent significant semantic of each modality, we first design two parallel semantic segmentation branches with a refined feature adaptive-modulation (RFaM) mechanism.","RFaM can perceive the features that are semantically distinct enough in each semantic segmentation branch.","Then, two pilot experiments based on the two branches are conducted to capture the significant prior semantic of two images, which then is applied to guide the fusion task in the integration of semantic segmentation branches and fusion branches.","In addition, to aggregate both high-level semantics and impressive visual effects, we further investigate the frequency response of the prior semantics, and propose a multi-level representation-adaptive fusion (MRaF) module to explicitly integrate the low-frequent prior semantic with the high-frequent details.","Extensive experiments on two public datasets demonstrate the superiority of our method over the state-of-the-art image fusion approaches, in terms of either the visual appeal or the high-level semantics."],"url":"http://arxiv.org/abs/2403.16227v1","category":"cs.CV"}
{"created":"2024-03-24 16:36:12","title":"Bi-Level Control of Weaving Sections in Mixed Traffic Environments with Connected and Automated Vehicles","abstract":"Connected and automated vehicles (CAVs) can be beneficial for improving the operation of highway bottlenecks such as weaving sections. This paper proposes a bi-level control approach based on an upper-level deep reinforcement learning controller and a lower-level model predictive controller to coordinate the lane-changings of a mixed fleet of CAVs and human-driven vehicles (HVs) in weaving sections. The upper level represents a roadside controller that collects vehicular information from the entire weaving section and determines the control weights used in the lower-level controller. The lower level is implemented within each CAV, which takes the control weights from the upper-level controller and generates the acceleration and steering angle for individual CAVs based on the local situation. The lower-level controller further incorporates an HV trajectory predictor, which is capable of handling the dynamic topology of vehicles in weaving scenarios with intensive mandatory lane changes. The case study inspired by a real weaving section in Basel, Switzerland, shows that our method consistently outperforms state-of-the-art benchmarks.","sentences":["Connected and automated vehicles (CAVs) can be beneficial for improving the operation of highway bottlenecks such as weaving sections.","This paper proposes a bi-level control approach based on an upper-level deep reinforcement learning controller and a lower-level model predictive controller to coordinate the lane-changings of a mixed fleet of CAVs and human-driven vehicles (HVs) in weaving sections.","The upper level represents a roadside controller that collects vehicular information from the entire weaving section and determines the control weights used in the lower-level controller.","The lower level is implemented within each CAV, which takes the control weights from the upper-level controller and generates the acceleration and steering angle for individual CAVs based on the local situation.","The lower-level controller further incorporates an HV trajectory predictor, which is capable of handling the dynamic topology of vehicles in weaving scenarios with intensive mandatory lane changes.","The case study inspired by a real weaving section in Basel, Switzerland, shows that our method consistently outperforms state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2403.16225v1","category":"eess.SY"}
{"created":"2024-03-24 16:30:05","title":"Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization","abstract":"Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers. As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets. Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature. One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text. In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers. We demonstrate this concept in the cybersecurity domain. One modality of the KG represents observable information from the papers, such as the categories in which they were published or the authors. The second modality uncovers latent (hidden) patterns of text extracted through hierarchical and semantic non-negative matrix factorization (NMF), such as named entities, topics or clusters, and keywords. We illustrate this concept by consolidating more than two million scientific papers uploaded to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by building a cyber-domain-specific KG.","sentences":["Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers.","As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets.","Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature.","One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text.","In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers.","We demonstrate this concept in the cybersecurity domain.","One modality of the KG represents observable information from the papers, such as the categories in which they were published or the authors.","The second modality uncovers latent (hidden) patterns of text extracted through hierarchical and semantic non-negative matrix factorization (NMF), such as named entities, topics or clusters, and keywords.","We illustrate this concept by consolidating more than two million scientific papers uploaded to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by building a cyber-domain-specific KG."],"url":"http://arxiv.org/abs/2403.16222v1","category":"cs.AI"}
{"created":"2024-03-24 16:18:27","title":"CoverUp: Coverage-Guided LLM-Based Test Generation","abstract":"This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.","sentences":["This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs).","CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches.","The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board.","On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%).","We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes."],"url":"http://arxiv.org/abs/2403.16218v1","category":"cs.SE"}
{"created":"2024-03-24 16:09:21","title":"Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane","abstract":"We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting.","sentences":["We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass.","Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part.","The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes.","During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes.","Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts.","The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting."],"url":"http://arxiv.org/abs/2403.16210v1","category":"cs.CV"}
{"created":"2024-03-24 16:08:10","title":"Image Captioning in news report scenario","abstract":"Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the narrative in news reporting through a more intuitive image captioning framework.","sentences":["Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP).","This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond.","Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images.","However, much of the existing body of work primarily centers around understanding scenes and actions.","In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices.","This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information.","Our endeavor shows a broader horizon, enriching the narrative in news reporting through a more intuitive image captioning framework."],"url":"http://arxiv.org/abs/2403.16209v1","category":"cs.CV"}
{"created":"2024-03-24 15:59:47","title":"Rumor Detection with a novel graph neural network approach","abstract":"The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these two modules to classify the rumors. Since malicious users intend to subvert our model after deployment, we further develop a greedy attack scheme to analyze the cost of three adversarial attacks: graph attack, comment attack, and joint attack. Evaluation results on two public datasets illustrate that the proposed MODEL outperforms the state-of-the-art rumor detection models. We also demonstrate our method performs well for early rumor detection. Moreover, the proposed detection method is more robust to adversarial attacks compared to the best existing method. Importantly, we show that it requires a high cost for attackers to subvert user correlation pattern, demonstrating the importance of considering user correlation for rumor detection.","sentences":["The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public.","How to debunk rumors as early as possible remains a challenging problem.","Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity.","In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media.","Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure.","Then we combine the learned representations from these two modules to classify the rumors.","Since malicious users intend to subvert our model after deployment, we further develop a greedy attack scheme to analyze the cost of three adversarial attacks: graph attack, comment attack, and joint attack.","Evaluation results on two public datasets illustrate that the proposed MODEL outperforms the state-of-the-art rumor detection models.","We also demonstrate our method performs well for early rumor detection.","Moreover, the proposed detection method is more robust to adversarial attacks compared to the best existing method.","Importantly, we show that it requires a high cost for attackers to subvert user correlation pattern, demonstrating the importance of considering user correlation for rumor detection."],"url":"http://arxiv.org/abs/2403.16206v1","category":"cs.AI"}
{"created":"2024-03-24 15:56:03","title":"Maximum Polygon Packing: The CG:SHOP Challenge 2024","abstract":"We give an overview of the 2024 Computational Geometry Challenge targeting the problem \\textsc{Maximum Polygon Packing}: Given a convex region $P$ in the plane, and a collection of simple polygons $Q_1, \\ldots, Q_n$, each $Q_i$ with a respective value $c_i$, find a subset $S \\subseteq \\{1, \\ldots,n\\}$ and a feasible packing within $P$ of the polygons $Q_i$ (without rotation) for $i \\in S$, maximizing $\\sum_{i \\in S} c_i$. Geometric packing problems, such as this, present significant computational challenges and are of substantial practical importance.","sentences":["We give an overview of the 2024 Computational Geometry Challenge targeting the problem \\textsc{Maximum Polygon Packing}: Given a convex region $P$ in the plane, and a collection of simple polygons $Q_1, \\ldots, Q_n$, each $Q_i$ with a respective value $c_i$, find a subset $S \\subseteq \\{1, \\ldots,n\\}$ and a feasible packing within $P$ of the polygons $Q_i$ (without rotation) for $i \\in S$, maximizing $\\sum_{i \\in S} c_i$.","Geometric packing problems, such as this, present significant computational challenges and are of substantial practical importance."],"url":"http://arxiv.org/abs/2403.16203v1","category":"cs.CG"}
{"created":"2024-03-24 15:14:44","title":"Logic-based Explanations for Linear Support Vector Classifiers with Reject Option","abstract":"Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems. It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist. This further increases the confidence of the model. Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results. While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present. We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option. We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations. Obtained results show that our proposed method gives shorter explanations with reduced time cost.","sentences":["Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems.","It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist.","This further increases the confidence of the model.","Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results.","While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present.","We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option.","We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations.","Obtained results show that our proposed method gives shorter explanations with reduced time cost."],"url":"http://arxiv.org/abs/2403.16190v1","category":"cs.AI"}
{"created":"2024-03-24 14:38:18","title":"Mixed-Initiative Human-Robot Teaming under Suboptimality with Online Bayesian Adaptation","abstract":"For effective human-agent teaming, robots and other artificial intelligence (AI) agents must infer their human partner's abilities and behavioral response patterns and adapt accordingly. Most prior works make the unrealistic assumption that one or more teammates can act near-optimally. In real-world collaboration, humans and autonomous agents can be suboptimal, especially when each only has partial domain knowledge. In this work, we develop computational modeling and optimization techniques for enhancing the performance of suboptimal human-agent teams, where the human and the agent have asymmetric capabilities and act suboptimally due to incomplete environmental knowledge. We adopt an online Bayesian approach that enables a robot to infer people's willingness to comply with its assistance in a sequential decision-making game. Our user studies show that user preferences and team performance indeed vary with robot intervention styles, and our approach for mixed-initiative collaborations enhances objective team performance ($p<.001$) and subjective measures, such as user's trust ($p<.001$) and perceived likeability of the robot ($p<.001$).","sentences":["For effective human-agent teaming, robots and other artificial intelligence (AI) agents must infer their human partner's abilities and behavioral response patterns and adapt accordingly.","Most prior works make the unrealistic assumption that one or more teammates can act near-optimally.","In real-world collaboration, humans and autonomous agents can be suboptimal, especially when each only has partial domain knowledge.","In this work, we develop computational modeling and optimization techniques for enhancing the performance of suboptimal human-agent teams, where the human and the agent have asymmetric capabilities and act suboptimally due to incomplete environmental knowledge.","We adopt an online Bayesian approach that enables a robot to infer people's willingness to comply with its assistance in a sequential decision-making game.","Our user studies show that user preferences and team performance indeed vary with robot intervention styles, and our approach for mixed-initiative collaborations enhances objective team performance ($p<.001$) and subjective measures, such as user's trust ($p<.001$) and perceived likeability of the robot ($p<.001$)."],"url":"http://arxiv.org/abs/2403.16178v1","category":"cs.RO"}
{"created":"2024-03-24 14:08:24","title":"An Analytic Solution to Covariance Propagation in Neural Networks","abstract":"Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.","sentences":["Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems.","However, this often involves costly or inaccurate sampling methods and approximations.","This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks.","A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU.","The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks."],"url":"http://arxiv.org/abs/2403.16163v1","category":"cs.LG"}
{"created":"2024-03-24 14:04:40","title":"Multi-Task Learning with Multi-Task Optimization","abstract":"Multi-task learning solves multiple correlated tasks. However, conflicts may exist between them. In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs. To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization. Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems. These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization. A theorem proving faster convergence through the inclusion of such transfers is presented. We investigate the proposed multi-task learning with multi-task optimization for solving various problem settings including image classification, scene understanding, and multi-target regression. Comprehensive experiments confirm that the proposed method significantly advances the state-of-the-art in discovering sets of Pareto-optimized models. Notably, on the large image dataset we tested on, namely NYUv2, the hypervolume convergence achieved by our method was found to be nearly two times faster than the next-best among the state-of-the-art.","sentences":["Multi-task learning solves multiple correlated tasks.","However, conflicts may exist between them.","In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs.","To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization.","Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems.","These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization.","A theorem proving faster convergence through the inclusion of such transfers is presented.","We investigate the proposed multi-task learning with multi-task optimization for solving various problem settings including image classification, scene understanding, and multi-target regression.","Comprehensive experiments confirm that the proposed method significantly advances the state-of-the-art in discovering sets of Pareto-optimized models.","Notably, on the large image dataset we tested on, namely NYUv2, the hypervolume convergence achieved by our method was found to be nearly two times faster than the next-best among the state-of-the-art."],"url":"http://arxiv.org/abs/2403.16162v1","category":"cs.AI"}
{"created":"2024-03-24 13:44:57","title":"One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation","abstract":"Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines. In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning. Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors. During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them. We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor faults. The masked model not only simplifies the overall FDIA pipeline, but also outperforms existing approaches. Our proposed technique has the potential to significantly improve the accuracy and reliability of sensor measurements in complex engineering systems in real-time, and could be applied to other types of sensors and engineering systems in the future. We believe that our proposed framework can contribute to the development of more efficient and effective FDIA techniques for a wide range of applications.","sentences":["Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines.","In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning.","Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors.","During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them.","We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor faults.","The masked model not only simplifies the overall FDIA pipeline, but also outperforms existing approaches.","Our proposed technique has the potential to significantly improve the accuracy and reliability of sensor measurements in complex engineering systems in real-time, and could be applied to other types of sensors and engineering systems in the future.","We believe that our proposed framework can contribute to the development of more efficient and effective FDIA techniques for a wide range of applications."],"url":"http://arxiv.org/abs/2403.16153v1","category":"cs.LG"}
{"created":"2024-03-24 13:43:43","title":"A Survey on Consumer IoT Traffic: Security and Privacy","abstract":"For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious traffic analysis, security analysis, and measurement. At last, we discuss the new challenges and future research directions.","sentences":["For the past few years, the Consumer Internet of Things (CIoT) has entered public lives.","While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns.","In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community.","From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved.","We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified.","Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious traffic analysis, security analysis, and measurement.","At last, we discuss the new challenges and future research directions."],"url":"http://arxiv.org/abs/2403.16149v1","category":"cs.CR"}
{"created":"2024-03-24 13:35:40","title":"Angular constraints on planar frameworks","abstract":"Consider a collection of points and the sets of slopes or directions of the lines between pairs of points. It is known that the algebraic matroid on this set of elements is the well studied 2-dimensional rigidity matroid. This article analyzes a construction on top of the set of slopes given by an angle constraint system of incidences and angles. In this setting we provide a matricial rigidity formulation of the problem for colored graphs, an algebro-geometric reformulation, precise necessary conditions and a combinatorial characterization of the generic behaviour for a special case.","sentences":["Consider a collection of points and the sets of slopes or directions of the lines between pairs of points.","It is known that the algebraic matroid on this set of elements is the well studied 2-dimensional rigidity matroid.","This article analyzes a construction on top of the set of slopes given by an angle constraint system of incidences and angles.","In this setting we provide a matricial rigidity formulation of the problem for colored graphs, an algebro-geometric reformulation, precise necessary conditions and a combinatorial characterization of the generic behaviour for a special case."],"url":"http://arxiv.org/abs/2403.16145v1","category":"math.CO"}
{"created":"2024-03-24 13:28:27","title":"What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?","abstract":"We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method. A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets. After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label. As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method.","sentences":["We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method.","A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets.","After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label.","As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method."],"url":"http://arxiv.org/abs/2403.16142v1","category":"cs.CL"}
{"created":"2024-03-24 13:06:05","title":"Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions","abstract":"In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previous surveys, this paper provides a more updated and comprehensive summary of the research, discusses future research directions, and contributes to the advancement of this field.","sentences":["In recent years, complementary recommendation has received extensive attention in the e-commerce domain.","In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024.","Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products.","Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start.","Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research.","Compared to previous surveys, this paper provides a more updated and comprehensive summary of the research, discusses future research directions, and contributes to the advancement of this field."],"url":"http://arxiv.org/abs/2403.16135v1","category":"cs.IR"}
{"created":"2024-03-24 13:03:49","title":"Enriching the physics program of the CMS experiment via data scouting and data parking","abstract":"Specialized data-taking and data-processing techniques were introduced by the CMS experiment in Run 1 of the CERN LHC to enhance the sensitivity of searches for new physics and the precision of standard model measurements. These techniques, termed data scouting and data parking, extend the data-taking capabilities of CMS beyond the original design specifications. The novel data-scouting strategy trades complete event information for higher event rates, while keeping the data bandwidth within limits. Data parking involves storing a large amount of raw detector data collected by algorithms with low trigger thresholds to be processed when sufficient computational power is available to handle such data. The research program of the CMS Collaboration is greatly expanded with these techniques. The implementation, performance, and physics results obtained with data scouting and data parking in CMS over the last decade are discussed in this Report, along with new developments aimed at further improving low-mass physics sensitivity over the next years of data taking.","sentences":["Specialized data-taking and data-processing techniques were introduced by the CMS experiment in Run 1 of the CERN LHC to enhance the sensitivity of searches for new physics and the precision of standard model measurements.","These techniques, termed data scouting and data parking, extend the data-taking capabilities of CMS beyond the original design specifications.","The novel data-scouting strategy trades complete event information for higher event rates, while keeping the data bandwidth within limits.","Data parking involves storing a large amount of raw detector data collected by algorithms with low trigger thresholds to be processed when sufficient computational power is available to handle such data.","The research program of the CMS Collaboration is greatly expanded with these techniques.","The implementation, performance, and physics results obtained with data scouting and data parking in CMS over the last decade are discussed in this Report, along with new developments aimed at further improving low-mass physics sensitivity over the next years of data taking."],"url":"http://arxiv.org/abs/2403.16134v1","category":"hep-ex"}
{"created":"2024-03-24 13:03:35","title":"SSHPool: The Separated Subgraph-based Hierarchical Pooling","abstract":"In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsulating rich intrinsic structural characteristics. Furthermore, we develop an end-to-end GNN framework associated with the proposed SSHPool module for graph classification. Experimental results demonstrate the superior performance of the proposed model on real-world datasets, significantly outperforming state-of-the-art GNN methods in terms of the classification accuracies.","sentences":["In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification.","To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs.","We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph.","Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs).","By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsulating rich intrinsic structural characteristics.","Furthermore, we develop an end-to-end GNN framework associated with the proposed SSHPool module for graph classification.","Experimental results demonstrate the superior performance of the proposed model on real-world datasets, significantly outperforming state-of-the-art GNN methods in terms of the classification accuracies."],"url":"http://arxiv.org/abs/2403.16133v1","category":"cs.AI"}
{"created":"2024-03-24 13:01:05","title":"AKBR: Learning Adaptive Kernel-based Representations for Graph Classification","abstract":"In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel matrix can be theoretically seen as the embedding vector of a sample graph, the proposed AKBR model is able to directly employ the resulting kernel matrix as the graph feature matrix and input it into the classifier for classification (i.e., the SoftMax layer), naturally providing an end-to-end learning architecture between the kernel computation as well as the classifier. Experimental results show that the proposed AKBR model outperforms existing state-of-the-art graph kernels and deep learning methods on standard graph benchmarks.","sentences":["In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification.","Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs.","To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs.","The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions.","Since each row of the resulting kernel matrix can be theoretically seen as the embedding vector of a sample graph, the proposed AKBR model is able to directly employ the resulting kernel matrix as the graph feature matrix and input it into the classifier for classification (i.e., the SoftMax layer), naturally providing an end-to-end learning architecture between the kernel computation as well as the classifier.","Experimental results show that the proposed AKBR model outperforms existing state-of-the-art graph kernels and deep learning methods on standard graph benchmarks."],"url":"http://arxiv.org/abs/2403.16130v1","category":"cs.LG"}
{"created":"2024-03-24 12:49:30","title":"WangchanLion and WangchanX MRC Eval","abstract":"This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improve our model in the future. Our code is public at https://github.com/vistec-AI/WangchanLion.","sentences":["This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language.","Our model is based on SEA-LION and a collection of instruction following datasets.","To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license.","To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad.","Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings.","In addition, our evaluation goes beyond the traditional MRC.","We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality.","Evaluation results provide insight into how we can improve our model in the future.","Our code is public at https://github.com/vistec-AI/WangchanLion."],"url":"http://arxiv.org/abs/2403.16127v1","category":"cs.CL"}
{"created":"2024-03-24 12:15:28","title":"Self-Supervised Multi-Frame Neural Scene Flow","abstract":"Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving. Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear. Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds. This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks. Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds. Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theoretical evaluation of its generalization abilities. Our analysis confirms that the proposed method maintains a limited generalization error, suggesting that adding multiple frames to the scene flow optimization process does not detract from its generalizability. Extensive experimental results on large-scale autonomous driving Waymo Open and Argoverse lidar datasets demonstrate that the proposed method achieves state-of-the-art performance.","sentences":["Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving.","Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear.","Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds.","This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks.","Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds.","Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theoretical evaluation of its generalization abilities.","Our analysis confirms that the proposed method maintains a limited generalization error, suggesting that adding multiple frames to the scene flow optimization process does not detract from its generalizability.","Extensive experimental results on large-scale autonomous driving Waymo Open and Argoverse lidar datasets demonstrate that the proposed method achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.16116v1","category":"cs.CV"}
{"created":"2024-03-24 12:05:23","title":"Opportunities and challenges in the application of large artificial intelligence models in radiology","abstract":"Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models. Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology. Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.","sentences":["Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development.","As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field.","This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models.","Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology.","Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography."],"url":"http://arxiv.org/abs/2403.16112v1","category":"cs.CV"}
{"created":"2024-03-24 11:52:39","title":"A Transformer approach for Electricity Price Forecasting","abstract":"This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.","sentences":["This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model.","As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism.","Hence, showing that the attention layer is enough for capturing the temporal patterns.","The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research.","The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation."],"url":"http://arxiv.org/abs/2403.16108v1","category":"cs.LG"}
{"created":"2024-03-24 11:50:49","title":"Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR","abstract":"Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants' abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing. Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR.","sentences":["Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR.","Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space.","However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform.","We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures.","Participants predominantly favored creating gestures with their hands.","The type of tasks and participants' abilities influence their choice of body parts for gesture design.","Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing.","Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR."],"url":"http://arxiv.org/abs/2403.16107v1","category":"cs.HC"}
{"created":"2024-03-24 11:49:53","title":"Viscoelastic material properties determine contact mechanics of hydrogel spheres","abstract":"Granular materials are ubiquitous in nature and industry; their mechanical behavior has been of academic and engineering interest for centuries. One of the reasons for their rather complex mechanical behavior is that stresses exerted on a granular material propagate only through contacts between the grains. These contacts can change as the packing evolves. This makes any deformation and mechanical response from a granular packing a function of the nature of contacts between the grains and the material response of the material the grains are made of. We present a study in which we isolate the role of the grain material in the contact forces acting between two particles sliding past each other. We use hydrogel particles and find that a viscoelastic material model, in which the shear modulus decays with time, coupled with a simple Coulomb friction model captures the experimental results. The results suggest that the particle material evolution itself may play a role in the collective behavior of granular materials.","sentences":["Granular materials are ubiquitous in nature and industry; their mechanical behavior has been of academic and engineering interest for centuries.","One of the reasons for their rather complex mechanical behavior is that stresses exerted on a granular material propagate only through contacts between the grains.","These contacts can change as the packing evolves.","This makes any deformation and mechanical response from a granular packing a function of the nature of contacts between the grains and the material response of the material the grains are made of.","We present a study in which we isolate the role of the grain material in the contact forces acting between two particles sliding past each other.","We use hydrogel particles and find that a viscoelastic material model, in which the shear modulus decays with time, coupled with a simple Coulomb friction model captures the experimental results.","The results suggest that the particle material evolution itself may play a role in the collective behavior of granular materials."],"url":"http://arxiv.org/abs/2403.16105v1","category":"cond-mat.soft"}
{"created":"2024-03-24 11:33:18","title":"Evaluating Fairness Metrics Across Borders from Human Perceptions","abstract":"Which fairness metrics are appropriately applicable in your contexts? There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics. Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness. However, these surveys were limited in scope, including only a few hundred participants within a single country. In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios. We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics. Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case. This investigation explores the relationship between personal attributes and the choice of fairness metrics, uncovering a significant influence of national context on these preferences.","sentences":["Which fairness metrics are appropriately applicable in your contexts?","There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics.","Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness.","However, these surveys were limited in scope, including only a few hundred participants within a single country.","In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios.","We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics.","Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case.","This investigation explores the relationship between personal attributes and the choice of fairness metrics, uncovering a significant influence of national context on these preferences."],"url":"http://arxiv.org/abs/2403.16101v1","category":"cs.AI"}
{"created":"2024-03-24 11:32:43","title":"Specifying Agent Ethics (Blue Sky Ideas)","abstract":"We consider the question of what properties a Machine Ethics system should have. This question is complicated by the existence of ethical dilemmas with no agreed upon solution. We provide an example to motivate why we do not believe falling back on the elicitation of values from stakeholders is sufficient to guarantee correctness of such systems. We go on to define two broad categories of ethical property that have arisen in our own work and present a challenge to the community to approach this question in a more systematic way.","sentences":["We consider the question of what properties a Machine Ethics system should have.","This question is complicated by the existence of ethical dilemmas with no agreed upon solution.","We provide an example to motivate why we do not believe falling back on the elicitation of values from stakeholders is sufficient to guarantee correctness of such systems.","We go on to define two broad categories of ethical property that have arisen in our own work and present a challenge to the community to approach this question in a more systematic way."],"url":"http://arxiv.org/abs/2403.16100v1","category":"cs.AI"}
{"created":"2024-03-24 11:29:55","title":"A Multi-Label Dataset of French Fake News: Human and Machine Insights","abstract":"We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press","sentences":["We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators.","By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers.","We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus.","We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News.","The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press"],"url":"http://arxiv.org/abs/2403.16099v1","category":"cs.CL"}
{"created":"2024-03-24 11:27:16","title":"Can Language Models Pretend Solvers? Logic Code Simulation with LLMs","abstract":"Transformer-based large language models (LLMs) have demonstrated significant potential in addressing logic problems. capitalizing on the great capabilities of LLMs for code-related activities, several frameworks leveraging logical solvers for logic reasoning have been proposed recently. While existing research predominantly focuses on viewing LLMs as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention. This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs. To further investigate this novel task, we formulate our three research questions: Can LLMs efficiently simulate the outputs of logic codes? What strength arises along with logic code simulation? And what pitfalls? To address these inquiries, we curate three novel datasets tailored for the logic code simulation task and undertake thorough experiments to establish the baseline performance of LLMs in code simulation. Subsequently, we introduce a pioneering LLM-based code simulation technique, Dual Chains of Logic (DCoL). This technique advocates a dual-path thinking approach for LLMs, which has demonstrated state-of-the-art performance compared to other LLM prompt strategies, achieving a notable improvement in accuracy by 7.06% with GPT-4-Turbo.","sentences":["Transformer-based large language models (LLMs) have demonstrated significant potential in addressing logic problems.","capitalizing on the great capabilities of LLMs for code-related activities, several frameworks leveraging logical solvers for logic reasoning have been proposed recently.","While existing research predominantly focuses on viewing LLMs as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention.","This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs.","To further investigate this novel task, we formulate our three research questions: Can LLMs efficiently simulate the outputs of logic codes?","What strength arises along with logic code simulation?","And what pitfalls?","To address these inquiries, we curate three novel datasets tailored for the logic code simulation task and undertake thorough experiments to establish the baseline performance of LLMs in code simulation.","Subsequently, we introduce a pioneering LLM-based code simulation technique, Dual Chains of Logic (DCoL).","This technique advocates a dual-path thinking approach for LLMs, which has demonstrated state-of-the-art performance compared to other LLM prompt strategies, achieving a notable improvement in accuracy by 7.06% with GPT-4-Turbo."],"url":"http://arxiv.org/abs/2403.16097v1","category":"cs.AI"}
{"created":"2024-03-24 10:07:46","title":"The Interplay of Learning, Analytics, and Artificial Intelligence in Education","abstract":"This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought processes, and the extension of human cognition via tightly integrated human-AI systems. Examples from current research and practice are examined as instances of the three conceptualisations, highlighting the potential value and limitations of each conceptualisation for education, as well as the perils of overemphasis on externalising human cognition as exemplified in today's hype surrounding generative AI tools. The paper concludes with an advocacy for a broader educational approach that includes educating people about AI and innovating educational systems to remain relevant in an AI enabled world.","sentences":["This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes.","Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI.","I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning.","Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection.","The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought processes, and the extension of human cognition via tightly integrated human-AI systems.","Examples from current research and practice are examined as instances of the three conceptualisations, highlighting the potential value and limitations of each conceptualisation for education, as well as the perils of overemphasis on externalising human cognition as exemplified in today's hype surrounding generative AI tools.","The paper concludes with an advocacy for a broader educational approach that includes educating people about AI and innovating educational systems to remain relevant in an AI enabled world."],"url":"http://arxiv.org/abs/2403.16081v1","category":"cs.CY"}
{"created":"2024-03-24 09:45:31","title":"The high-density regime of dusty plasma: Coulomb plasma","abstract":"It is shown that the dust density regimes in dusty plasma are characterized by two complementary screening processes, (a) the low dust density regime where the Debye screening is the dominant process and (b) the high dust density regime where the Coulomb screening is the dominant process. The Debye regime is characterized by a state where all dust particles carry an equal and constant charge. The high-density regime or the Coulomb plasma regime is characterized by (a) Coulomb screening where the dust charge depends on the spatial location and is screened by other dust particles in the vicinity by charge reduction, (b) quark like asymptotic freedom where dust particles, which on an average carry minimal electric charge (q tends to 0), are asymptotically free, (c) uniform dust charge density and plasma potential, (d) dust charge neutralization by a uniform background of hot ions. Thus, the Coulomb plasma is essentially a one-component plasma (OCP) with screening as opposed to electron plasma which is OCP without screening. Molecular dynamics (MD) simulations verify these properties. The MD simulations are performed, using a recently developed Hamiltonian formalism, to study the dynamics of Yukawa particles carrying variable electric charge. A hydrodynamic model for describing the collective properties of Coulomb plasma and its characteristic acoustic mode called the Coulomb acoustic wave is given.","sentences":["It is shown that the dust density regimes in dusty plasma are characterized by two complementary screening processes, (a) the low dust density regime where the Debye screening is the dominant process and (b) the high dust density regime where the Coulomb screening is the dominant process.","The Debye regime is characterized by a state where all dust particles carry an equal and constant charge.","The high-density regime or the Coulomb plasma regime is characterized by (a) Coulomb screening where the dust charge depends on the spatial location and is screened by other dust particles in the vicinity by charge reduction, (b) quark like asymptotic freedom where dust particles, which on an average carry minimal electric charge (q tends to 0), are asymptotically free, (c) uniform dust charge density and plasma potential, (d) dust charge neutralization by a uniform background of hot ions.","Thus, the Coulomb plasma is essentially a one-component plasma (OCP) with screening as opposed to electron plasma which is OCP without screening.","Molecular dynamics (MD) simulations verify these properties.","The MD simulations are performed, using a recently developed Hamiltonian formalism, to study the dynamics of Yukawa particles carrying variable electric charge.","A hydrodynamic model for describing the collective properties of Coulomb plasma and its characteristic acoustic mode called the Coulomb acoustic wave is given."],"url":"http://arxiv.org/abs/2403.16079v1","category":"physics.plasm-ph"}
{"created":"2024-03-24 09:26:53","title":"Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications","abstract":"Smart contracts are decentralized applications built atop blockchains like Ethereum. Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and justification are correct). This is likely because off-the-shelf LLMs were primarily pre-trained on a general text/code corpus and not fine-tuned on the specific domain of Solidity smart contract auditing.   In this paper, we propose TrustLLM, a general framework that combines fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications. Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause. As such, TrustLLM employs a two-stage fine-tuning approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities. However, fine-tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability. Therefore, we introduce two LLM-based agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine-tuned Reasoner model. To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM. We then compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b). On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%. The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes.","sentences":["Smart contracts are decentralized applications built atop blockchains like Ethereum.","Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and justification are correct).","This is likely because off-the-shelf LLMs were primarily pre-trained on a general text/code corpus and not fine-tuned on the specific domain of Solidity smart contract auditing.   ","In this paper, we propose TrustLLM, a general framework that combines fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.","Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause.","As such, TrustLLM employs a two-stage fine-tuning approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities.","However, fine-tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability.","Therefore, we introduce two LLM-based agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine-tuned Reasoner model.","To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM.","We then compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b).","On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%.","The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes."],"url":"http://arxiv.org/abs/2403.16073v1","category":"cs.SE"}
{"created":"2024-03-24 09:18:21","title":"Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization","abstract":"Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, a max-min mutual information regularization approach is proposed to capture speaker-insensitive latent representations. Experimental evaluations on public lip reading datasets demonstrate the effectiveness of the proposed approach under the intra-speaker and inter-speaker conditions.","sentences":["Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications.","Deep learning approaches greatly improve current lip reading systems.","However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability.","A well-trained lip reading system may perform poorly when handling a brand new speaker.","To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers.","In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics.","Furthermore, a max-min mutual information regularization approach is proposed to capture speaker-insensitive latent representations.","Experimental evaluations on public lip reading datasets demonstrate the effectiveness of the proposed approach under the intra-speaker and inter-speaker conditions."],"url":"http://arxiv.org/abs/2403.16071v1","category":"cs.AI"}
{"created":"2024-03-24 08:34:08","title":"Robust Diffusion Models for Adversarial Purification","abstract":"Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks. Extensive experiments are conducted to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks.","sentences":["Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT).","However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well.","Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy.","To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive.","We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs.","This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks.","Extensive experiments are conducted to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks."],"url":"http://arxiv.org/abs/2403.16067v1","category":"cs.CV"}
{"created":"2024-03-24 08:33:13","title":"A Temporal Graph Network Framework for Dynamic Recommendation","abstract":"Recommender systems, crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users' evolving preferences due to static data reliance. After Temporal Graph Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time. However, despite its promising capabilities, it has not been directly applied in recommender systems to date. Our study bridges this gap by directly implementing Temporal Graph Networks (TGN) in recommender systems, a first in this field. Using real-world datasets and a range of graph and history embedding methods, we show TGN's adaptability, confirming its effectiveness in dynamic recommendation scenarios.","sentences":["Recommender systems, crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users' evolving preferences due to static data reliance.","After Temporal Graph Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time.","However, despite its promising capabilities, it has not been directly applied in recommender systems to date.","Our study bridges this gap by directly implementing Temporal Graph Networks (TGN) in recommender systems, a first in this field.","Using real-world datasets and a range of graph and history embedding methods, we show TGN's adaptability, confirming its effectiveness in dynamic recommendation scenarios."],"url":"http://arxiv.org/abs/2403.16066v1","category":"cs.AI"}
{"created":"2024-03-24 08:22:36","title":"Holography inspired self-controlled reconfigurable intelligent surface","abstract":"Among various promising candidate technologies for the sixth-generation (6G) wireless communications, recent advances in microwave metasurfaces have sparked a new research area of reconfigurable intelligent surfaces (RISs). By controllably reprogramming the wireless propagation channel, RISs are envisioned to achieve low-cost wireless capacity boosting, coverage extension, and enhanced energy efficiency. To reprogram the channel, each meta-atom on RIS needs an external control signal, which is usually generated by base station (BS). However, BS-controlled RISs require complicated control cables, which hamper their massive deployments. Here, we eliminate the need for BS control by proposing a self-controlled RIS (SC-RIS), which is inspired by the optical holography principle. Different from the existing BS-controlled RISs, each meta-atom of SC-RIS is integrated with an additional power detector for holographic recording. By applying the classical Fourier-transform processing to the measured hologram, SC-RIS is capable of retrieving the user's channel state information required for beamforming, thus enabling autonomous RIS beamforming without control cables. Owing to this WiFi-like plug-and-play capability without the BS control, SC-RISs are expected to enable easy and massive deployments in the future 6G systems.","sentences":["Among various promising candidate technologies for the sixth-generation (6G) wireless communications, recent advances in microwave metasurfaces have sparked a new research area of reconfigurable intelligent surfaces (RISs).","By controllably reprogramming the wireless propagation channel, RISs are envisioned to achieve low-cost wireless capacity boosting, coverage extension, and enhanced energy efficiency.","To reprogram the channel, each meta-atom on RIS needs an external control signal, which is usually generated by base station (BS).","However, BS-controlled RISs require complicated control cables, which hamper their massive deployments.","Here, we eliminate the need for BS control by proposing a self-controlled RIS (SC-RIS), which is inspired by the optical holography principle.","Different from the existing BS-controlled RISs, each meta-atom of SC-RIS is integrated with an additional power detector for holographic recording.","By applying the classical Fourier-transform processing to the measured hologram, SC-RIS is capable of retrieving the user's channel state information required for beamforming, thus enabling autonomous RIS beamforming without control cables.","Owing to this WiFi-like plug-and-play capability without the BS control, SC-RISs are expected to enable easy and massive deployments in the future 6G systems."],"url":"http://arxiv.org/abs/2403.16062v1","category":"eess.SP"}
{"created":"2024-03-24 07:48:05","title":"Qibo: A Large Language Model for Traditional Chinese Medicine","abstract":"In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, which is a specialized tool for evaluating the performance of LLMs in the TCM domain. This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine. Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine.","sentences":["In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance.","However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources.","In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT).","Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, which is a specialized tool for evaluating the performance of LLMs in the TCM domain.","This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine.","Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine."],"url":"http://arxiv.org/abs/2403.16056v1","category":"cs.CL"}
{"created":"2024-03-24 07:04:08","title":"Semantic Is Enough: Only Semantic Information For NeRF Reconstruction","abstract":"Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels. This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component. We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach. We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model. Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation. The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding.","sentences":["Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels.","This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component.","We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach.","We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model.","Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation.","The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding."],"url":"http://arxiv.org/abs/2403.16043v1","category":"cs.CV"}
{"created":"2024-03-24 06:30:02","title":"V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception","abstract":"Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability. However, there are no real-world datasets to facilitate the real V2X cooperative perception research -- existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation. In this paper, we propose a dataset that has a mixture of multiple vehicles and smart infrastructure simultaneously to facilitate the V2X cooperative perception development with multi-modality sensing data. Our V2X-Real is collected using two connected automated vehicles and two smart infrastructures, which are all equipped with multi-modal sensors including LiDAR sensors and multi-view cameras. The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios. According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception. Comprehensive multi-class multi-agent benchmarks of SOTA cooperative perception methods are provided. The V2X-Real dataset and benchmark codes will be released.","sentences":["Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability.","However, there are no real-world datasets to facilitate the real V2X cooperative perception research -- existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation.","In this paper, we propose a dataset that has a mixture of multiple vehicles and smart infrastructure simultaneously to facilitate the V2X cooperative perception development with multi-modality sensing data.","Our V2X-Real is collected using two connected automated vehicles and two smart infrastructures, which are all equipped with multi-modal sensors including LiDAR sensors and multi-view cameras.","The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios.","According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception.","Comprehensive multi-class multi-agent benchmarks of SOTA cooperative perception methods are provided.","The V2X-Real dataset and benchmark codes will be released."],"url":"http://arxiv.org/abs/2403.16034v1","category":"cs.CV"}
{"created":"2024-03-24 06:21:35","title":"FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools","abstract":"The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development. Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools. Therefore, previous research efforts have explored learning-based methods to validate the reported warnings. Nevertheless, there are still some limitations. (1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level. These approaches lack specificity regarding individual bugs and warnings. (2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings. (3) The small number of selected projects hinders the comprehensive evaluation of these approaches. In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \\ourtool. Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention. To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA. It is ten times larger than the existing largest dataset. Then, we conducted extensive experiments on the dataset to evaluate FineWAVE. The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines.","sentences":["The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development.","Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools.","Therefore, previous research efforts have explored learning-based methods to validate the reported warnings.","Nevertheless, there are still some limitations.","(1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level.","These approaches lack specificity regarding individual bugs and warnings.","(2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings.","(3) The small number of selected projects hinders the comprehensive evaluation of these approaches.","In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \\ourtool.","Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention.","To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA.","It is ten times larger than the existing largest dataset.","Then, we conducted extensive experiments on the dataset to evaluate FineWAVE.","The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines."],"url":"http://arxiv.org/abs/2403.16032v1","category":"cs.SE"}
{"created":"2024-03-24 06:02:58","title":"Electromagnetic-Field-Based Circuit Theory and Charge-Flux-Flow Diagrams","abstract":"The conventional circuit diagrams and graph-based circuit theory are used for the phase-independent circuits such as resistor-inductor-capacitor (RLC) circuits and semiconductor transistor circuits, rather than the phase-dependent circuits such as Josephson junction circuits and quantum-phase-slip (QPS) junction circuits. in the age of artificial intelligence (AI), we present an electromagnetic-field-based circuit theory to unify the phase-independent and phase-dependent electric circuits. This theory drives two general system models for all electric circuits, and visualizes the dynamics of circuit devices with electric-charge-flow (ECF) diagrams and the magnetic-flux-flow (MFF) diagrams. ECF and MFF diagrams enable electric circuits to be designed and analyzed like the molecules composed of two kinds of atoms; they are promising for the language to train AI-aided electronic-design-automation (EDA) tools.","sentences":["The conventional circuit diagrams and graph-based circuit theory are used for the phase-independent circuits such as resistor-inductor-capacitor (RLC) circuits and semiconductor transistor circuits, rather than the phase-dependent circuits such as Josephson junction circuits and quantum-phase-slip (QPS) junction circuits.","in the age of artificial intelligence (AI), we present an electromagnetic-field-based circuit theory to unify the phase-independent and phase-dependent electric circuits.","This theory drives two general system models for all electric circuits, and visualizes the dynamics of circuit devices with electric-charge-flow (ECF) diagrams and the magnetic-flux-flow (MFF) diagrams.","ECF and MFF diagrams enable electric circuits to be designed and analyzed like the molecules composed of two kinds of atoms; they are promising for the language to train AI-aided electronic-design-automation (EDA) tools."],"url":"http://arxiv.org/abs/2403.16025v1","category":"physics.app-ph"}
{"created":"2024-03-24 05:55:39","title":"RPMArt: Towards Robust Perception and Manipulation for Articulated Objects","abstract":"Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects. Experimental results confirm our approach's effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments. The code and data will be open-sourced for reproduction. More results are published on the project website at https://r-pmart.github.io .","sentences":["Articulated objects are commonly found in daily life.","It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications.","However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios.","To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud.","Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting.","Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer.","Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects.","After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects.","Experimental results confirm our approach's effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments.","The code and data will be open-sourced for reproduction.","More results are published on the project website at https://r-pmart.github.io ."],"url":"http://arxiv.org/abs/2403.16023v1","category":"cs.RO"}
{"created":"2024-03-24 05:50:55","title":"Digital Twin Assisted Intelligent Network Management for Vehicular Applications","abstract":"The emerging data-driven methods based on artificial intelligence (AI) have paved the way for intelligent, flexible, and adaptive network management in vehicular applications. To enhance network management towards network automation, this article presents a digital twin (DT) assisted two-tier learning framework, which facilitates the automated life-cycle management of machine learning based intelligent network management functions (INMFs). Specifically, at a high tier, meta learning is employed to capture different levels of general features for the INMFs under nonstationary network conditions. At a low tier, individual learning models are customized for local networks based on fast model adaptation. Hierarchical DTs are deployed at the edge and cloud servers to assist the two-tier learning process, through closed-loop interactions with the physical network domain. Finally, a case study demonstrates the fast and accurate model adaptation ability of meta learning in comparison with benchmark schemes.","sentences":["The emerging data-driven methods based on artificial intelligence (AI) have paved the way for intelligent, flexible, and adaptive network management in vehicular applications.","To enhance network management towards network automation, this article presents a digital twin (DT) assisted two-tier learning framework, which facilitates the automated life-cycle management of machine learning based intelligent network management functions (INMFs).","Specifically, at a high tier, meta learning is employed to capture different levels of general features for the INMFs under nonstationary network conditions.","At a low tier, individual learning models are customized for local networks based on fast model adaptation.","Hierarchical DTs are deployed at the edge and cloud servers to assist the two-tier learning process, through closed-loop interactions with the physical network domain.","Finally, a case study demonstrates the fast and accurate model adaptation ability of meta learning in comparison with benchmark schemes."],"url":"http://arxiv.org/abs/2403.16021v1","category":"cs.NI"}
{"created":"2024-03-24 05:26:55","title":"Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)","abstract":"Image inpainting is the process of taking an image and generating lost or intentionally occluded portions. Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text. Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions. In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses. A critical gap in these existing models will be addressed, focusing on the ability to prompt and control what exactly is generated. We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality. Finally, we will evaluate the results of our approaches by qualitatively checking whether they generate high-quality images that correctly inpaint regions with the objects that they are instructed to produce.","sentences":["Image inpainting is the process of taking an image and generating lost or intentionally occluded portions.","Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text.","Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions.","In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses.","A critical gap in these existing models will be addressed, focusing on the ability to prompt and control what exactly is generated.","We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality.","Finally, we will evaluate the results of our approaches by qualitatively checking whether they generate high-quality images that correctly inpaint regions with the objects that they are instructed to produce."],"url":"http://arxiv.org/abs/2403.16016v1","category":"cs.CV"}
{"created":"2024-03-24 04:34:34","title":"CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering","abstract":"The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM.","sentences":["The recent advancements in artificial intelligence highlight the potential of language models in psychological health support.","While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques.","To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models.","Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies.","Unlike previous methods, our dataset emphasizes professional and structured response.","Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques.","Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality.","The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM."],"url":"http://arxiv.org/abs/2403.16008v1","category":"cs.CL"}
{"created":"2024-03-24 04:23:43","title":"A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures","abstract":"Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model. The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense experiments. The results show that FLGNN performs good robustness, and the success rate of privacy theft is further reduced by adding differential privacy defense methods.","sentences":["Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy.","However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario.","Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model.","The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets.","Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense experiments.","The results show that FLGNN performs good robustness, and the success rate of privacy theft is further reduced by adding differential privacy defense methods."],"url":"http://arxiv.org/abs/2403.16004v1","category":"cs.LG"}
{"created":"2024-03-24 04:22:37","title":"Diverse Representation Embedding for Lifelong Person Re-Identification","abstract":"Lifelong Person Re-Identification (LReID) aims to continuously learn from successive data streams, matching individuals across multiple cameras. The key challenge for LReID is how to effectively preserve old knowledge while learning new information incrementally. Task-level domain gaps and limited old task datasets are key factors leading to catastrophic forgetting in ReLD, which are overlooked in existing methods. To alleviate this problem, we propose a novel Diverse Representation Embedding (DRE) framework for LReID. The proposed DRE preserves old knowledge while adapting to new information based on instance-level and task-level layout. Concretely, an Adaptive Constraint Module (ACM) is proposed to implement integration and push away operations between multiple representations, obtaining dense embedding subspace for each instance to improve matching ability on limited old task datasets. Based on the processed diverse representation, we interact knowledge between the adjustment model and the learner model through Knowledge Update (KU) and Knowledge Preservation (KP) strategies at the task-level layout, which reduce the task-wise domain gap on both old and new tasks, and exploit diverse representation of each instance in limited datasets from old tasks, improving model performance for extended periods. Extensive experiments were conducted on eleven Re-ID datasets, including five seen datasets for training in order-1 and order-2 orders and six unseen datasets for inference. Compared to state-of-the-art methods, our method achieves significantly improved performance in holistic, large-scale, and occluded datasets.","sentences":["Lifelong Person Re-Identification (LReID) aims to continuously learn from successive data streams, matching individuals across multiple cameras.","The key challenge for LReID is how to effectively preserve old knowledge while learning new information incrementally.","Task-level domain gaps and limited old task datasets are key factors leading to catastrophic forgetting in ReLD, which are overlooked in existing methods.","To alleviate this problem, we propose a novel Diverse Representation Embedding (DRE) framework for LReID.","The proposed DRE preserves old knowledge while adapting to new information based on instance-level and task-level layout.","Concretely, an Adaptive Constraint Module (ACM) is proposed to implement integration and push away operations between multiple representations, obtaining dense embedding subspace for each instance to improve matching ability on limited old task datasets.","Based on the processed diverse representation, we interact knowledge between the adjustment model and the learner model through Knowledge Update (KU) and Knowledge Preservation (KP) strategies at the task-level layout, which reduce the task-wise domain gap on both old and new tasks, and exploit diverse representation of each instance in limited datasets from old tasks, improving model performance for extended periods.","Extensive experiments were conducted on eleven Re-ID datasets, including five seen datasets for training in order-1 and order-2 orders and six unseen datasets for inference.","Compared to state-of-the-art methods, our method achieves significantly improved performance in holistic, large-scale, and occluded datasets."],"url":"http://arxiv.org/abs/2403.16003v1","category":"cs.CV"}
{"created":"2024-03-24 03:10:39","title":"Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting","abstract":"Facial expression spotting is a significant but challenging task in facial expression analysis. The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions. In this paper, we propose a Multi-Scale Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression spotting. To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network. This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement. The subtle motion features are then converted to a facial graph representation, whose spatio-temporal graph patterns are learned by a graph convolutional network. This network learns both local and global features from multiple scales of facial graph structures using our proposed facial local graph pooling (FLGP). Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability of our model for difficult-to-classify frames. The experimental results on the SAMM-LV and CAS(ME)^2 datasets demonstrate that our method achieves state-of-the-art performance, particularly in micro-expression spotting. Ablation studies further verify the effectiveness of our proposed modules.","sentences":["Facial expression spotting is a significant but challenging task in facial expression analysis.","The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions.","In this paper, we propose a Multi-Scale Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression spotting.","To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network.","This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement.","The subtle motion features are then converted to a facial graph representation, whose spatio-temporal graph patterns are learned by a graph convolutional network.","This network learns both local and global features from multiple scales of facial graph structures using our proposed facial local graph pooling (FLGP).","Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability of our model for difficult-to-classify frames.","The experimental results on the SAMM-LV and CAS(ME)^2 datasets demonstrate that our method achieves state-of-the-art performance, particularly in micro-expression spotting.","Ablation studies further verify the effectiveness of our proposed modules."],"url":"http://arxiv.org/abs/2403.15994v1","category":"cs.CV"}
{"created":"2024-03-24 03:10:07","title":"BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval","abstract":"The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions. It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.","sentences":["The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals.","To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution.","While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets.","To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports.","Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder.","This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions.","It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks."],"url":"http://arxiv.org/abs/2403.15992v1","category":"cs.CV"}
{"created":"2024-03-24 02:54:46","title":"Knowledge-guided Machine Learning: Current Trends and Future Prospects","abstract":"This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.","sentences":["This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models.","It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results.","We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML.","We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category."],"url":"http://arxiv.org/abs/2403.15989v1","category":"cs.LG"}
{"created":"2024-03-24 02:15:14","title":"Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance Fields","abstract":"Accurate collection of plant phenotyping is critical to optimising sustainable farming practices in precision agriculture. Traditional phenotyping in controlled laboratory environments, while valuable, falls short in understanding plant growth under real-world conditions. Emerging sensor and digital technologies offer a promising approach for direct phenotyping of plants in farm environments. This study investigates a learning-based phenotyping method using the Neural Radiance Field to achieve accurate in-situ phenotyping of pepper plants in greenhouse environments. To quantitatively evaluate the performance of this method, traditional point cloud registration on 3D scanning data is implemented for comparison. Experimental result shows that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the 3D scanning methods. The mean distance error between the scanner-based method and the NeRF-based method is 0.865mm. This study shows that the learning-based NeRF method achieves similar accuracy to 3D scanning-based methods but with improved scalability and robustness.","sentences":["Accurate collection of plant phenotyping is critical to optimising sustainable farming practices in precision agriculture.","Traditional phenotyping in controlled laboratory environments, while valuable, falls short in understanding plant growth under real-world conditions.","Emerging sensor and digital technologies offer a promising approach for direct phenotyping of plants in farm environments.","This study investigates a learning-based phenotyping method using the Neural Radiance Field to achieve accurate in-situ phenotyping of pepper plants in greenhouse environments.","To quantitatively evaluate the performance of this method, traditional point cloud registration on 3D scanning data is implemented for comparison.","Experimental result shows that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the 3D scanning methods.","The mean distance error between the scanner-based method and the NeRF-based method is 0.865mm.","This study shows that the learning-based NeRF method achieves similar accuracy to 3D scanning-based methods but with improved scalability and robustness."],"url":"http://arxiv.org/abs/2403.15981v1","category":"cs.CV"}
{"created":"2024-03-24 01:20:08","title":"Towards Two-Stream Foveation-based Active Vision Learning","abstract":"Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both \"what object is being observed\" and \"where it is located\". In contrast, the \"two-stream hypothesis\" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the \"two-stream hypothesis\" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.","sentences":["Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both \"what object is being observed\" and \"where it is located\".","In contrast, the \"two-stream hypothesis\" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions.","In this work, we propose a machine learning framework inspired by the \"two-stream hypothesis\" and explore the potential benefits that it offers.","Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches.","The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model.","We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes.","The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box.","We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets."],"url":"http://arxiv.org/abs/2403.15977v1","category":"cs.CV"}
{"created":"2024-03-24 01:19:15","title":"Searches for CE\u03bdNS and Physics beyond the Standard Model using Skipper-CCDs at CONNIE","abstract":"The Coherent Neutrino-Nucleus Interaction Experiment (CONNIE) aims to detect the coherent scattering (CE$\\nu$NS) of reactor antineutrinos off silicon nuclei using thick fully-depleted high-resistivity silicon CCDs. Two Skipper-CCD sensors with sub-electron readout noise capability were installed at the experiment next to the Angra-2 reactor in 2021, making CONNIE the first experiment to employ Skipper-CCDs for reactor neutrino detection. We report on the performance of the Skipper-CCDs, the new data processing and data quality selection techniques and the event selection for CE$\\nu$NS interactions, which enable CONNIE to reach a record low detection threshold of 15 eV. The data were collected over 300 days in 2021-2022 and correspond to exposures of 14.9 g-days with the reactor-on and 3.5 g-days with the reactor-off. The difference between the reactor-on and off event rates shows no excess and yields upper limits at 95% confidence level for the neutrino interaction rates comparable with previous CONNIE limits from standard CCDs and higher exposures. Searches for new neutrino interactions beyond the Standard Model were performed, yielding an improvement on the previous CONNIE limit on a simplified model with light vector mediators. A first dark matter (DM) search by diurnal modulation was performed by CONNIE and the results represent the best limits on the DM-electron scattering cross-section, obtained by a surface-level experiment. These promising results, obtained using a very small-mass sensor, illustrate the potential of Skipper-CCDs to probe rare neutrino interactions and motivate the plans to increase the detector mass in the near future.","sentences":["The Coherent Neutrino-Nucleus Interaction Experiment (CONNIE) aims to detect the coherent scattering (CE$\\nu$NS) of reactor antineutrinos off silicon nuclei using thick fully-depleted high-resistivity silicon CCDs.","Two Skipper-CCD sensors with sub-electron readout noise capability were installed at the experiment next to the Angra-2 reactor in 2021, making CONNIE the first experiment to employ Skipper-CCDs for reactor neutrino detection.","We report on the performance of the Skipper-CCDs, the new data processing and data quality selection techniques and the event selection for CE$\\nu$NS interactions, which enable CONNIE to reach a record low detection threshold of 15 eV. The data were collected over 300 days in 2021-2022 and correspond to exposures of 14.9 g-days with the reactor-on and 3.5 g-days with the reactor-off.","The difference between the reactor-on and off event rates shows no excess and yields upper limits at 95% confidence level for the neutrino interaction rates comparable with previous CONNIE limits from standard CCDs and higher exposures.","Searches for new neutrino interactions beyond the Standard Model were performed, yielding an improvement on the previous CONNIE limit on a simplified model with light vector mediators.","A first dark matter (DM) search by diurnal modulation was performed by CONNIE and the results represent the best limits on the DM-electron scattering cross-section, obtained by a surface-level experiment.","These promising results, obtained using a very small-mass sensor, illustrate the potential of Skipper-CCDs to probe rare neutrino interactions and motivate the plans to increase the detector mass in the near future."],"url":"http://arxiv.org/abs/2403.15976v1","category":"hep-ex"}
{"created":"2024-03-24 00:46:40","title":"CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data","abstract":"This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains. Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data. For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold. We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image. We show that the CBGT-Net provides improved accuracy and robustness compared to models trained to classify from a single patch, and models leveraging an LSTM layer to classify from a fixed sequence length of patches.","sentences":["This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains.","Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data.","For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold.","We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image.","We show that the CBGT-Net provides improved accuracy and robustness compared to models trained to classify from a single patch, and models leveraging an LSTM layer to classify from a fixed sequence length of patches."],"url":"http://arxiv.org/abs/2403.15974v1","category":"cs.NE"}
