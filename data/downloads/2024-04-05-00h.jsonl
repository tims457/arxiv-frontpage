{"created":"2024-04-03 17:59:53","title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","abstract":"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.","sentences":["We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\".","This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation.","On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed.","It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability.","Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence.","VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing.","These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization.","We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning."],"url":"http://arxiv.org/abs/2404.02905v1","category":"cs.CV"}
{"created":"2024-04-03 17:59:36","title":"ALOHa: A New Measure for Hallucination in Captioning Models","abstract":"Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories. Our code is available at https://davidmchan.github.io/aloha/.","sentences":["Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene.","The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms.","In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations.","Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score.","We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.","Our code is available at https://davidmchan.github.io/aloha/."],"url":"http://arxiv.org/abs/2404.02904v1","category":"cs.CV"}
{"created":"2024-04-03 17:59:28","title":"LidarDM: Generative LiDAR Simulation in a Generated World","abstract":"We present LidarDM, a novel LiDAR generative model capable of producing realistic, layout-aware, physically plausible, and temporally coherent LiDAR videos. LidarDM stands out with two unprecedented capabilities in LiDAR generative modeling: (i) LiDAR generation guided by driving scenarios, offering significant potential for autonomous driving simulations, and (ii) 4D LiDAR point cloud generation, enabling the creation of realistic and temporally coherent sequences. At the heart of our model is a novel integrated 4D world generation framework. Specifically, we employ latent diffusion models to generate the 3D scene, combine it with dynamic actors to form the underlying 4D world, and subsequently produce realistic sensory observations within this virtual environment. Our experiments indicate that our approach outperforms competing algorithms in realism, temporal coherency, and layout consistency. We additionally show that LidarDM can be used as a generative world model simulator for training and testing perception models.","sentences":["We present LidarDM, a novel LiDAR generative model capable of producing realistic, layout-aware, physically plausible, and temporally coherent LiDAR videos.","LidarDM stands out with two unprecedented capabilities in LiDAR generative modeling: (i) LiDAR generation guided by driving scenarios, offering significant potential for autonomous driving simulations, and (ii) 4D LiDAR point cloud generation, enabling the creation of realistic and temporally coherent sequences.","At the heart of our model is a novel integrated 4D world generation framework.","Specifically, we employ latent diffusion models to generate the 3D scene, combine it with dynamic actors to form the underlying 4D world, and subsequently produce realistic sensory observations within this virtual environment.","Our experiments indicate that our approach outperforms competing algorithms in realism, temporal coherency, and layout consistency.","We additionally show that LidarDM can be used as a generative world model simulator for training and testing perception models."],"url":"http://arxiv.org/abs/2404.02903v1","category":"cs.CV"}
{"created":"2024-04-03 17:58:29","title":"The Lavrentiev phenomenon","abstract":"The basic problem of the calculus of variations consists of finding a function that minimizes an energy, like finding the fastest trajectory between two points for a point mass in a gravity field moving without friction under the influence of gravity or finding the best shape of a wing. The existence of a solution may be established in quite abstract spaces, much larger than the space of smooth functions. An important practical problem is that of being able to approach the value of the infimum of the energy. However, numerical methods work with very concrete functions and sometimes they are unable to approximate the infimum: this is the surprising Lavrentiev phenomenon. The papers that ensure the non-occurrence of the phenomenon form a recent saga, and the most general result formulated in the early '90s was actually fully proved just recently, more than 30 years later. Our aim here is to introduce the reader to the calculus of variations, to illustrate the Lavrentiev phenomenon with the simplest known example, and to give an elementary proof of the non-occurrence of the phenomenon.","sentences":["The basic problem of the calculus of variations consists of finding a function that minimizes an energy, like finding the fastest trajectory between two points for a point mass in a gravity field moving without friction under the influence of gravity or finding the best shape of a wing.","The existence of a solution may be established in quite abstract spaces, much larger than the space of smooth functions.","An important practical problem is that of being able to approach the value of the infimum of the energy.","However, numerical methods work with very concrete functions and sometimes they are unable to approximate the infimum: this is the surprising Lavrentiev phenomenon.","The papers that ensure the non-occurrence of the phenomenon form a recent saga, and the most general result formulated in the early '90s was actually fully proved just recently, more than 30 years later.","Our aim here is to introduce the reader to the calculus of variations, to illustrate the Lavrentiev phenomenon with the simplest known example, and to give an elementary proof of the non-occurrence of the phenomenon."],"url":"http://arxiv.org/abs/2404.02901v1","category":"math.OC"}
{"created":"2024-04-03 17:58:21","title":"DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets","abstract":"Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes. Further, to mitigate overfitting, we propose distilling from a flat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks. With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes. The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture. We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.","sentences":["Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks.","In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks.","However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ).","Due to this, ViT requires a large amount of data for pre-training.","Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively.","However, limited literature discusses the use of ViT for datasets with long-tailed imbalances.","In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets.","In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes.","This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes.","Further, to mitigate overfitting, we propose distilling from a flat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks.","With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes.","The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture.","We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018."],"url":"http://arxiv.org/abs/2404.02900v1","category":"cs.CV"}
{"created":"2024-04-03 17:57:15","title":"MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment","abstract":"We present MatAtlas, a method for consistent text-guided 3D model texturing. Following recent progress we leverage a large scale text-to-image generation model (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully design an RGB texturing pipeline that leverages a grid pattern diffusion, driven by depth and edges. By proposing a multi-step texture refinement process, we significantly improve the quality and 3D consistency of the texturing output. To further address the problem of baked-in lighting, we move beyond RGB colors and pursue assigning parametric materials to the assets. Given the high-quality initial RGB texture, we propose a novel material retrieval method capitalized on Large Language Models (LLM), enabling editabiliy and relightability. We evaluate our method on a wide variety of geometries and show that our method significantly outperform prior arts. We also analyze the role of each component through a detailed ablation study.","sentences":["We present MatAtlas, a method for consistent text-guided 3D model texturing.","Following recent progress we leverage a large scale text-to-image generation model (e.g., Stable Diffusion) as a prior to texture a 3D model.","We carefully design an RGB texturing pipeline that leverages a grid pattern diffusion, driven by depth and edges.","By proposing a multi-step texture refinement process, we significantly improve the quality and 3D consistency of the texturing output.","To further address the problem of baked-in lighting, we move beyond RGB colors and pursue assigning parametric materials to the assets.","Given the high-quality initial RGB texture, we propose a novel material retrieval method capitalized on Large Language Models (LLM), enabling editabiliy and relightability.","We evaluate our method on a wide variety of geometries and show that our method significantly outperform prior arts.","We also analyze the role of each component through a detailed ablation study."],"url":"http://arxiv.org/abs/2404.02899v1","category":"cs.CV"}
{"created":"2024-04-03 17:55:20","title":"A Mean Field Game Model for Timely Computation in Edge Computing Systems","abstract":"We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES. Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two. We model the problem as a large population non-cooperative game among the $N$ devices. Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user's multi-objective optimization problem, with a coupled consistency condition. By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption. In numerical simulations, we validate that a higher load at the ES may lead devices to upload their task to the ES less often.","sentences":["We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES.","Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two.","We model the problem as a large population non-cooperative game among the $N$ devices.","Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user's multi-objective optimization problem, with a coupled consistency condition.","By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption.","In numerical simulations, we validate that a higher load at the ES may lead devices to upload their task to the ES less often."],"url":"http://arxiv.org/abs/2404.02898v1","category":"cs.IT"}
{"created":"2024-04-03 17:54:37","title":"Deep Image Composition Meets Image Forgery","abstract":"Image forgery is a topic that has been studied for many years. Before the breakthrough of deep learning, forged images were detected using handcrafted features that did not require training. These traditional methods failed to perform satisfactorily even on datasets much worse in quality than real-life image manipulations. Advances in deep learning have impacted image forgery detection as much as they have impacted other areas of computer vision and have improved the state of the art. Deep learning models require large amounts of labeled data for training. In the case of image forgery, labeled data at the pixel level is a very important factor for the models to learn. None of the existing datasets have sufficient size, realism and pixel-level labeling at the same time. This is due to the high cost of producing and labeling quality images. It can take hours for an image editing expert to manipulate just one image. To bridge this gap, we automate data generation using image composition techniques that are very related to image forgery. Unlike other automated data generation frameworks, we use state of the art image composition deep learning models to generate spliced images close to the quality of real-life manipulations. Finally, we test the generated dataset on the SOTA image manipulation detection model and show that its prediction performance is lower compared to existing datasets, i.e. we produce realistic images that are more difficult to detect. Dataset will be available at https://github.com/99eren99/DIS25k .","sentences":["Image forgery is a topic that has been studied for many years.","Before the breakthrough of deep learning, forged images were detected using handcrafted features that did not require training.","These traditional methods failed to perform satisfactorily even on datasets much worse in quality than real-life image manipulations.","Advances in deep learning have impacted image forgery detection as much as they have impacted other areas of computer vision and have improved the state of the art.","Deep learning models require large amounts of labeled data for training.","In the case of image forgery, labeled data at the pixel level is a very important factor for the models to learn.","None of the existing datasets have sufficient size, realism and pixel-level labeling at the same time.","This is due to the high cost of producing and labeling quality images.","It can take hours for an image editing expert to manipulate just one image.","To bridge this gap, we automate data generation using image composition techniques that are very related to image forgery.","Unlike other automated data generation frameworks, we use state of the art image composition deep learning models to generate spliced images close to the quality of real-life manipulations.","Finally, we test the generated dataset on the SOTA image manipulation detection model and show that its prediction performance is lower compared to existing datasets, i.e. we produce realistic images that are more difficult to detect.","Dataset will be available at https://github.com/99eren99/DIS25k ."],"url":"http://arxiv.org/abs/2404.02897v1","category":"cs.CV"}
{"created":"2024-04-03 17:51:18","title":"ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline","abstract":"Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM. Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}.","sentences":["Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving.","While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.","In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment.","We first train a general Math-Critique model from the LLM itself to provide feedback signals.","Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection.","Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval.","Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger.","Related techniques have been deployed to ChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM.","Related evaluation dataset and scripts are released at \\url{https://github.com/THUDM/ChatGLM-Math}."],"url":"http://arxiv.org/abs/2404.02893v1","category":"cs.CL"}
{"created":"2024-04-03 17:44:02","title":"Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining","abstract":"Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation. Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users. They are also vulnerable to advanced Expanded Residual Block ambiguity attacks. We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports. An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature. To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model. By jointly training the verification and deployment branches, their weights become tightly coupled. The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user. Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks.","sentences":["Ensuring the legal usage of deep models is crucial to promoting trustable, accountable, and responsible artificial intelligence innovation.","Current passport-based methods that obfuscate model functionality for license-to-use and ownership verifications suffer from capacity and quality constraints, as they require retraining the owner model for new users.","They are also vulnerable to advanced Expanded Residual Block ambiguity attacks.","We propose Steganographic Passport, which uses an invertible steganographic network to decouple license-to-use from ownership verification by hiding the user's identity images into the owner-side passport and recovering them from their respective user-side passports.","An irreversible and collision-resistant hash function is used to avoid exposing the owner-side passport from the derived user-side passports and increase the uniqueness of the model signature.","To safeguard both the passport and model's weights against advanced ambiguity attacks, an activation-level obfuscation is proposed for the verification branch of the owner's model.","By jointly training the verification and deployment branches, their weights become tightly coupled.","The proposed method supports agile licensing of deep models by providing a strong ownership proof and license accountability without requiring a separate model retraining for the admission of every new user.","Experiment results show that our Steganographic Passport outperforms other passport-based deep model protection methods in robustness against various known attacks."],"url":"http://arxiv.org/abs/2404.02889v1","category":"cs.CR"}
{"created":"2024-04-03 17:42:47","title":"Branching Brownian motion versus Random Energy Model in the supercritical phase: overlap distribution and temperature susceptibility","abstract":"In comparison with Derrida's REM, we investigate the influence of the so-called decoration processes arising in the limiting extremal processes of numerous log-correlated Gaussian fields. In particular, we focus on the branching Brownian motion and two specific quantities from statistical physics in the vicinity of the critical temperature. The first one is the two-temperature overlap, whose behavior at criticality is smoothened by the decoration process - unlike the one-temperature overlap which is identical - and the second one is the temperature susceptibility, as introduced by Sales and Bouchaud, which is strictly larger in the presence of decorations and diverges, close to the critical temperature, at the same speed as for the REM but with a different multiplicative constant. We also study some general decorated cases in order to highlight the fact that the BBM has a critical behavior in some sense to be made precise.","sentences":["In comparison with Derrida's REM, we investigate the influence of the so-called decoration processes arising in the limiting extremal processes of numerous log-correlated Gaussian fields.","In particular, we focus on the branching Brownian motion and two specific quantities from statistical physics in the vicinity of the critical temperature.","The first one is the two-temperature overlap, whose behavior at criticality is smoothened by the decoration process - unlike the one-temperature overlap which is identical - and the second one is the temperature susceptibility, as introduced by Sales and Bouchaud, which is strictly larger in the presence of decorations and diverges, close to the critical temperature, at the same speed as for the REM but with a different multiplicative constant.","We also study some general decorated cases in order to highlight the fact that the BBM has a critical behavior in some sense to be made precise."],"url":"http://arxiv.org/abs/2404.02888v1","category":"math.PR"}
{"created":"2024-04-03 17:38:15","title":"PoCo: Point Context Cluster for RGBD Indoor Place Recognition","abstract":"We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database. The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors. We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning. Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation. We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%). In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment.","sentences":["We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database.","The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors.","We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning.","Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation.","We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively.","PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%).","In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment."],"url":"http://arxiv.org/abs/2404.02885v1","category":"cs.CV"}
{"created":"2024-04-03 17:34:28","title":"On the Scalability of Diffusion-based Text-to-Image Generation","abstract":"Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers. We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL's UNet. On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size. Increasing caption density and diversity improves text-image alignment performance and the learning efficiency. Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size.","sentences":["Scaling up model and data size has been quite successful for the evolution of LLMs.","However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored.","It is also unclear how to efficiently scale the model for better performance at reduced cost.","The different training settings and expensive training cost make a fair model comparison extremely difficult.","In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images.","For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs.","And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers.","We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL's UNet.","On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size.","Increasing caption density and diversity improves text-image alignment performance and the learning efficiency.","Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size."],"url":"http://arxiv.org/abs/2404.02883v1","category":"cs.CV"}
{"created":"2024-04-03 17:25:18","title":"Pair production in rotating electric fields via quantum kinetic equations: Resolving helicity states","abstract":"We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a strong electric field of circular polarization. By means of a nonperturbative approach based on the quantum kinetic equations (QKEs), we numerically calculate helicity-resolved momentum distributions of the particles produced and analyze the corresponding helicity asymmetry. It is demonstrated that the external rotating field tends to generate left-handed and right-handed particles traveling in opposite directions. Generic symmetry properties of the momentum spectra are examined analytically by means of the QKEs and also confirmed and illustrated by direct numerical computations. The helicity signatures revealed in our study are expected to provide a firmer basis for possible experimental investigations of the fundamental phenomenon of vacuum pair production in strong fields.","sentences":["We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a strong electric field of circular polarization.","By means of a nonperturbative approach based on the quantum kinetic equations (QKEs), we numerically calculate helicity-resolved momentum distributions of the particles produced and analyze the corresponding helicity asymmetry.","It is demonstrated that the external rotating field tends to generate left-handed and right-handed particles traveling in opposite directions.","Generic symmetry properties of the momentum spectra are examined analytically by means of the QKEs and also confirmed and illustrated by direct numerical computations.","The helicity signatures revealed in our study are expected to provide a firmer basis for possible experimental investigations of the fundamental phenomenon of vacuum pair production in strong fields."],"url":"http://arxiv.org/abs/2404.02878v1","category":"hep-ph"}
{"created":"2024-04-03 17:24:27","title":"FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery","abstract":"Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores. The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy. The benchmark toolkit and codes, available via https://github.com/toelt-llc/FlightScope_Bench, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications.","sentences":["Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring.","While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos.","This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery.","Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch.","This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions.","This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores.","The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy.","The benchmark toolkit and codes, available via https://github.com/toelt-llc/FlightScope_Bench, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications."],"url":"http://arxiv.org/abs/2404.02877v1","category":"cs.CV"}
{"created":"2024-04-03 17:09:00","title":"Integrating Explanations in Learning LTL Specifications from Demonstrations","abstract":"This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations. Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations. LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains. On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges. We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications. We have implemented a tool called Janaka based on our approach. Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies.","sentences":["This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations.","Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations.","LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains.","On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges.","We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications.","We have implemented a tool called Janaka based on our approach.","Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies."],"url":"http://arxiv.org/abs/2404.02872v1","category":"cs.AI"}
{"created":"2024-04-03 17:07:51","title":"Random approximation of convex bodies in Hausdorff metric","abstract":"While there is extensive literature on approximation, deterministic as well as random, of general convex bodies $K$ in the symmetric difference metric, or other metrics arising from intrinsic volumes, very little is known for corresponding random results in the Hausdorff distance when the approximant $K_n$ is given by the convex hull of $n$ independent random points chosen uniformly on the boundary or in the interior of $K$. When $K$ is a polygon and the points are chosen on its boundary, we determine the exact limiting behavior of the expected Hausdorff distance between a polygon as $n\\to\\infty$. From this we derive the behavior of the asymptotic constant for a regular polygon in the number of vertices.","sentences":["While there is extensive literature on approximation, deterministic as well as random, of general convex bodies $K$ in the symmetric difference metric, or other metrics arising from intrinsic volumes, very little is known for corresponding random results in the Hausdorff distance when the approximant $K_n$ is given by the convex hull of $n$ independent random points chosen uniformly on the boundary or in the interior of $K$. When $K$ is a polygon and the points are chosen on its boundary, we determine the exact limiting behavior of the expected Hausdorff distance between a polygon as $n\\to\\infty$. From this we derive the behavior of the asymptotic constant for a regular polygon in the number of vertices."],"url":"http://arxiv.org/abs/2404.02870v1","category":"math.MG"}
{"created":"2024-04-03 17:05:41","title":"Human Activity Recognition using Smartphones","abstract":"Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc. In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time. We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer. These readings were preprocessed using a median filter. 42 features were extracted using various methods. We then tested various machine learning algorithms along with dimensionality reduction. Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time. This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metabolic Equivalent.","sentences":["Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc.","In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time.","We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer.","These readings were preprocessed using a median filter.","42 features were extracted using various methods.","We then tested various machine learning algorithms along with dimensionality reduction.","Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time.","This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metabolic Equivalent."],"url":"http://arxiv.org/abs/2404.02869v1","category":"cs.LG"}
{"created":"2024-04-03 17:03:18","title":"UDON: A case for offloading to general purpose compute on CXL memory","abstract":"Upcoming CXL-based disaggregated memory devices feature special purpose units to offload compute to near-memory. In this paper, we explore opportunities for offloading compute to general purpose cores on CXL memory devices, thereby enabling a greater utility and diversity of offload.   We study two classes of popular memory intensive applications: ML inference and vector database as candidates for computational offload. The study uses Arm AArch64-based dual-socket NUMA systems to emulate CXL type-2 devices.   Our study shows promising results. With our ML inference model partitioning strategy for compute offload, we can place up to 90% data in remote memory with just 20% performance trade-off. Offloading Hierarchical Navigable Small World (HNSW) kernels in vector databases can provide upto 6.87$\\times$ performance improvement with under 10% offload overhead.","sentences":["Upcoming CXL-based disaggregated memory devices feature special purpose units to offload compute to near-memory.","In this paper, we explore opportunities for offloading compute to general purpose cores on CXL memory devices, thereby enabling a greater utility and diversity of offload.   ","We study two classes of popular memory intensive applications: ML inference and vector database as candidates for computational offload.","The study uses Arm AArch64-based dual-socket NUMA systems to emulate CXL type-2 devices.   ","Our study shows promising results.","With our ML inference model partitioning strategy for compute offload, we can place up to 90% data in remote memory with just 20% performance trade-off.","Offloading Hierarchical Navigable Small World (HNSW) kernels in vector databases can provide upto 6.87$\\times$ performance improvement with under 10% offload overhead."],"url":"http://arxiv.org/abs/2404.02868v1","category":"cs.ET"}
{"created":"2024-04-03 17:01:21","title":"Dark energy as a geometrical effect in geodetic brane gravity","abstract":"Within the framework of the modified geodetic brane gravity, conformed by the Regge-Teitelboim model and enhanced with a linear term in the extrinsic curvature of the brane, the possibility that under an FRW geometry this theory emulates the so-called dark energy is discussed. The cosmological behavior of this model displays a self-(non-self)-accelerated expansion of this universe which is caused by a combination of usual matter and gravitational geometric effects controlled by a $\\beta$ parameter that accompanies the correction $K$ term. Indeed, the self-accelerated branch, provided by the trace $K$ model raises the question of whether the extrinsic curvature correction terms might be suitable for dark energy candidates. We discuss the analytical expression obtained for $\\rd$ in addition to the main cosmological parameters such as the state parameter $\\omega_{\\text{\\tiny eff}}$ and the deceleration parameter $q$. Moreover, when we call for the contribution of dark radiation-like energy to be switched off, $\\Odr \\to 0$, we find the same acceleration behavior, as well as the same dark energy content provided by the DGP theory.","sentences":["Within the framework of the modified geodetic brane gravity, conformed by the Regge-Teitelboim model and enhanced with a linear term in the extrinsic curvature of the brane, the possibility that under an FRW geometry this theory emulates the so-called dark energy is discussed.","The cosmological behavior of this model displays a self-(non-self)-accelerated expansion of this universe which is caused by a combination of usual matter and gravitational geometric effects controlled by a $\\beta$ parameter that accompanies the correction $K$ term.","Indeed, the self-accelerated branch, provided by the trace $K$ model raises the question of whether the extrinsic curvature correction terms might be suitable for dark energy candidates.","We discuss the analytical expression obtained for $\\rd$ in addition to the main cosmological parameters such as the state parameter $\\omega_{\\text{\\tiny eff}}$ and the deceleration parameter $q$. Moreover, when we call for the contribution of dark radiation-like energy to be switched off, $\\Odr \\to 0$, we find the same acceleration behavior, as well as the same dark energy content provided by the DGP theory."],"url":"http://arxiv.org/abs/2404.02867v1","category":"gr-qc"}
{"created":"2024-04-03 16:57:19","title":"Forecasting the sensitivity of Pulsar Timing Arrays to gravitational wave backgrounds","abstract":"Pulsar Timing Array (PTA) observations hinted towards the existence of a stochastic gravitational wave background (SGWB) in the nHz frequency band. Still, the nature of the SGWB signal cannot be confidently inferred from current data, and the leading explanation invokes mergers of supermassive black holes. If confirmed, such discovery would not only represent a turning point in our understanding of astrophysics, but it may severely limit the capability of searching for additional cosmological sources in the nHz frequency range. In this work, we build a simple framework to forecast the sensitivity of future PTA configurations and assess the parameter estimation of SGWB, which could consist of several contributions. We release the python code fastPTA implementing this framework and ready to use.","sentences":["Pulsar Timing Array (PTA) observations hinted towards the existence of a stochastic gravitational wave background (SGWB) in the nHz frequency band.","Still, the nature of the SGWB signal cannot be confidently inferred from current data, and the leading explanation invokes mergers of supermassive black holes.","If confirmed, such discovery would not only represent a turning point in our understanding of astrophysics, but it may severely limit the capability of searching for additional cosmological sources in the nHz frequency range.","In this work, we build a simple framework to forecast the sensitivity of future PTA configurations and assess the parameter estimation of SGWB, which could consist of several contributions.","We release the python code fastPTA implementing this framework and ready to use."],"url":"http://arxiv.org/abs/2404.02864v1","category":"astro-ph.CO"}
{"created":"2024-04-03 16:50:05","title":"The Life Care Annuity: enhancing product features and refining pricing methods","abstract":"In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods. In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option. Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process. As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method. We use such a method to estimate the fair price of the product. Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder. Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach.","sentences":["In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods.","In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option.","Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process.","As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method.","We use such a method to estimate the fair price of the product.","Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder.","Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach."],"url":"http://arxiv.org/abs/2404.02858v1","category":"q-fin.CP"}
{"created":"2024-04-03 16:30:48","title":"Efficient Structure-Informed Featurization and Property Prediction of Ordered, Dilute, and Random Atomic Structures","abstract":"Structure-informed materials informatics is a rapidly evolving discipline of materials science relying on the featurization of atomic structures or configurations to construct vector, voxel, graph, graphlet, and other representations useful for machine learning prediction of properties, fingerprinting, and generative design. This work discusses how current featurizers typically perform redundant calculations and how their efficiency could be improved by considering (1) fundamentals of crystallographic (orbits) equivalency to optimize ordered cases and (2) representation-dependent equivalency to optimize cases of dilute, doped, and defect structures with broken symmetry. It also discusses and contrasts ways of (3) approximating random solid solutions occupying arbitrary lattices under such representations.   Efficiency improvements discussed in this work were implemented within pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks developed by authors since 2019 and shown to increase performance from 2 to 10 times for typical inputs. Throughout this work, the authors explicitly discuss how these advances can be applied to different kinds of similar tools in the community.","sentences":["Structure-informed materials informatics is a rapidly evolving discipline of materials science relying on the featurization of atomic structures or configurations to construct vector, voxel, graph, graphlet, and other representations useful for machine learning prediction of properties, fingerprinting, and generative design.","This work discusses how current featurizers typically perform redundant calculations and how their efficiency could be improved by considering (1) fundamentals of crystallographic (orbits) equivalency to optimize ordered cases and (2) representation-dependent equivalency to optimize cases of dilute, doped, and defect structures with broken symmetry.","It also discusses and contrasts ways of (3) approximating random solid solutions occupying arbitrary lattices under such representations.   ","Efficiency improvements discussed in this work were implemented within pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks developed by authors since 2019 and shown to increase performance from 2 to 10 times for typical inputs.","Throughout this work, the authors explicitly discuss how these advances can be applied to different kinds of similar tools in the community."],"url":"http://arxiv.org/abs/2404.02849v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 16:28:29","title":"Statistical Field Theory of Polarizable Polymer Chains with Nonlocal Dipolar Interactions","abstract":"The electromechanical response of polymeric soft matter to applied electric fields is of fundamental scientific interest as well as relevant to technologies for sensing and actuation. Several existing theoretical and numerical approaches for polarizable polymers subject to a combined applied electric field and stretch are based on discrete monomer models. In these models, accounting for the interactions between the induced dipoles on monomers is challenging due to the nonlocality of these interactions. On the other hand, the framework of statistical field theory provides a continuous description of polymer chains that potentially enables a tractable way to account for these interactions. However, prior formulations using this framework have been restricted to the case of weak anisotropy of the monomer polarizability.   This paper formulates a general approach based in the framework of statistical field theory to account for the nonlocal nature of the dipolar interactions without any restrictions on the anisotropy or nonlinearity of the polarizability of the monomer. The approach is based on 3 key elements: (1) the statistical field theory framework, in which the discrete monomers are regularized to a continuous dipole distribution; (2) a replacement of the nonlocal dipole-dipole interactions by the local electrostatics PDE with the continuous dipole distribution as the forcing; (3) the use of a completely general relation between the polarization and the local electric field. Rather than treat the dipole-dipole interactions directly, the continuous description in the field theory enables the computationally-tractable nonlocal-to-local transformation. Further, it enables the use of a realistic statistical-mechanical ensemble wherein the average far-field applied electric field is prescribed, rather than prescribing the applied field at every point in the polymer domain.","sentences":["The electromechanical response of polymeric soft matter to applied electric fields is of fundamental scientific interest as well as relevant to technologies for sensing and actuation.","Several existing theoretical and numerical approaches for polarizable polymers subject to a combined applied electric field and stretch are based on discrete monomer models.","In these models, accounting for the interactions between the induced dipoles on monomers is challenging due to the nonlocality of these interactions.","On the other hand, the framework of statistical field theory provides a continuous description of polymer chains that potentially enables a tractable way to account for these interactions.","However, prior formulations using this framework have been restricted to the case of weak anisotropy of the monomer polarizability.   ","This paper formulates a general approach based in the framework of statistical field theory to account for the nonlocal nature of the dipolar interactions without any restrictions on the anisotropy or nonlinearity of the polarizability of the monomer.","The approach is based on 3 key elements: (1) the statistical field theory framework, in which the discrete monomers are regularized to a continuous dipole distribution; (2) a replacement of the nonlocal dipole-dipole interactions by the local electrostatics PDE with the continuous dipole distribution as the forcing; (3) the use of a completely general relation between the polarization and the local electric field.","Rather than treat the dipole-dipole interactions directly, the continuous description in the field theory enables the computationally-tractable nonlocal-to-local transformation.","Further, it enables the use of a realistic statistical-mechanical ensemble wherein the average far-field applied electric field is prescribed, rather than prescribing the applied field at every point in the polymer domain."],"url":"http://arxiv.org/abs/2404.02848v1","category":"cond-mat.soft"}
{"created":"2024-04-03 16:26:01","title":"On Springer correspondence for wreath products","abstract":"We first show that the wreath product $\\Sigma_m\\wr \\Sigma_d$ between two symmetric groups appears as the generalized Weyl group of an Iwahori's generalized Tits system. We then introduce a certain subvariety of the flag variety of type A, and then give a geometric proof of its Bruhat decomposition indexed by $\\Sigma_m\\wr \\Sigma_d$, via the Bialynicki-Birula decomposition. Furthermore, we realize the group algebra $\\mathbb{Q}[\\Sigma_m\\wr \\Sigma_d]$ as the top Borel-Moore homology of a Steinberg variety. Such a geometric realization leads to a Springer correspondence for the irreducible representations over $\\mathbb{C}[\\Sigma_m\\wr \\Sigma_d]$, which can be regarded as a counterpart of the Clifford theory for wreath products.","sentences":["We first show that the wreath product $\\Sigma_m\\wr \\Sigma_d$ between two symmetric groups appears as the generalized Weyl group of an Iwahori's generalized Tits system.","We then introduce a certain subvariety of the flag variety of type A, and then give a geometric proof of its Bruhat decomposition indexed by $\\Sigma_m\\wr \\Sigma_d$, via the Bialynicki-Birula decomposition.","Furthermore, we realize the group algebra $\\mathbb{Q}[\\Sigma_m\\wr \\Sigma_d]$ as the top Borel-Moore homology of a Steinberg variety.","Such a geometric realization leads to a Springer correspondence for the irreducible representations over $\\mathbb{C}[\\Sigma_m\\wr \\Sigma_d]$, which can be regarded as a counterpart of the Clifford theory for wreath products."],"url":"http://arxiv.org/abs/2404.02846v1","category":"math.RT"}
{"created":"2024-04-03 16:22:33","title":"Scalable quantum detector tomography by high-performance computing","abstract":"At large scales, quantum systems may become advantageous over their classical counterparts at performing certain tasks. Developing tools to analyse these systems at the relevant scales, in a manner consistent with quantum mechanics, is therefore critical to benchmarking performance and characterising their operation. While classical computational approaches cannot perform like-for-like computations of quantum systems beyond a certain scale, classical high-performance computing (HPC) may nevertheless be useful for precisely these characterisation and certification tasks. By developing open-source customised algorithms using high-performance computing, we perform quantum tomography on a megascale quantum photonic detector covering a Hilbert space of $10^6$. This requires finding $10^8$ elements of the matrix corresponding to the positive operator valued measure (POVM), the quantum description of the detector, and is achieved in minutes of computation time. Moreover, by exploiting the structure of the problem, we achieve highly efficient parallel scaling, paving the way for quantum objects up to a system size of $10^{12}$ elements to be reconstructed using this method. In general, this shows that a consistent quantum mechanical description of quantum phenomena is applicable at everyday scales. More concretely, this enables the reconstruction of large-scale quantum sources, processes and detectors used in computation and sampling tasks, which may be necessary to prove their nonclassical character or quantum computational advantage.","sentences":["At large scales, quantum systems may become advantageous over their classical counterparts at performing certain tasks.","Developing tools to analyse these systems at the relevant scales, in a manner consistent with quantum mechanics, is therefore critical to benchmarking performance and characterising their operation.","While classical computational approaches cannot perform like-for-like computations of quantum systems beyond a certain scale, classical high-performance computing (HPC) may nevertheless be useful for precisely these characterisation and certification tasks.","By developing open-source customised algorithms using high-performance computing, we perform quantum tomography on a megascale quantum photonic detector covering a Hilbert space of $10^6$.","This requires finding $10^8","$ elements of the matrix corresponding to the positive operator valued measure (POVM), the quantum description of the detector, and is achieved in minutes of computation time.","Moreover, by exploiting the structure of the problem, we achieve highly efficient parallel scaling, paving the way for quantum objects up to a system size of $10^{12}$ elements to be reconstructed using this method.","In general, this shows that a consistent quantum mechanical description of quantum phenomena is applicable at everyday scales.","More concretely, this enables the reconstruction of large-scale quantum sources, processes and detectors used in computation and sampling tasks, which may be necessary to prove their nonclassical character or quantum computational advantage."],"url":"http://arxiv.org/abs/2404.02844v1","category":"quant-ph"}
{"created":"2024-04-03 16:17:53","title":"I-Design: Personalized LLM Interior Designer","abstract":"Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.","sentences":["Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality.","However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury.","To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication.","I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships.","Subsequently, an effective placement algorithm determines optimal locations for each object within the scene.","The final design is then constructed in 3D by retrieving and integrating assets from an existing object database.","Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline.","Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity."],"url":"http://arxiv.org/abs/2404.02838v1","category":"cs.AI"}
{"created":"2024-04-03 16:13:29","title":"Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison","abstract":"Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes. We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.","sentences":["Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process.","While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored.","In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes.","We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning.","Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures.","We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board."],"url":"http://arxiv.org/abs/2404.02835v1","category":"cs.CL"}
{"created":"2024-04-03 16:08:01","title":"Empowering Biomedical Discovery with AI Agents","abstract":"We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.","sentences":["We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms.","Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks.","AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows.","These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories.","AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies."],"url":"http://arxiv.org/abs/2404.02831v1","category":"cs.AI"}
{"created":"2024-04-03 16:04:59","title":"Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes","abstract":"Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models. Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis. Moreover, such models either rely on post-hoc methods or additional annotations. In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way. Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics. We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method. Further, our model provides superior interpretability against the post-hoc method. Importantly, expert radiologists validated the visual interpretability of our results, showing clinical applicability.","sentences":["Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models.","Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis.","Moreover, such models either rely on post-hoc methods or additional annotations.","In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way.","Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics.","We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method.","Further, our model provides superior interpretability against the post-hoc method.","Importantly, expert radiologists validated the visual interpretability of our results, showing clinical applicability."],"url":"http://arxiv.org/abs/2404.02830v1","category":"cs.CV"}
{"created":"2024-04-03 16:01:25","title":"A camel with a less strict diet","abstract":"A camel can carry $B$ bananas on its back. It can have $2$ bananas at a time in its stomach. For each mile the camel walks, the amount of bananas in its stomach decreases $1$. As soon as the amount of bananas in the camel's stomach is at most $1$, it can eat a new banana. When the camel's stomach is empty, the camel must eat a new banana (in order to be able to continue its itinerary).   Let there be a stock of $N$ bananas at the border of the desert. How far can the camel penetrate into the desert, starting at this point? (Of course it can form new stocks with transported bananas.)   The case $B=1$ is solved completely. The round trip variant is solved for $B=1$ as well. For $B=2$, the round trip variant is solved for $N$ which are a power of $2$ and $N \\le 8$, and estimated up to $1/(N-1)$ miles for general $N$.","sentences":["A camel can carry $B$ bananas on its back.","It can have $2$ bananas at a time in its stomach.","For each mile the camel walks, the amount of bananas in its stomach decreases $1$. As soon as the amount of bananas in the camel's stomach is at most $1$, it can eat a new banana.","When the camel's stomach is empty, the camel must eat a new banana (in order to be able to continue its itinerary).   ","Let there be a stock of $N$ bananas at the border of the desert.","How far can the camel penetrate into the desert, starting at this point?","(Of course it can form new stocks with transported bananas.)   ","The case $B=1$ is solved completely.","The round trip variant is solved for $B=1$ as well.","For $B=2$, the round trip variant is solved for $N$ which are a power of $2$ and $N \\le 8$, and estimated up to $1/(N-1)$ miles for general $N$."],"url":"http://arxiv.org/abs/2404.02828v1","category":"math.HO"}
{"created":"2024-04-03 15:59:31","title":"An Error-Bounded Lossy Compression Method with Bit-Adaptive Quantization for Particle Data","abstract":"This paper presents error-bounded lossy compression tailored for particle datasets from diverse scientific applications in cosmology, fluid dynamics, and fusion energy sciences. As today's high-performance computing capabilities advance, these datasets often reach trillions of points, posing significant visualization, analysis, and storage challenges. While error-bounded lossy compression makes it possible to represent floating-point values with strict pointwise accuracy guarantees, the lack of correlations in particle data's storage ordering often limits the compression ratio. Inspired by quantization-encoding schemes in SZ lossy compressors, we dynamically determine the number of bits to encode particles of the dataset to increase the compression ratio. Specifically, we utilize a k-d tree to partition particles into subregions and generate ``bit boxes'' centered at particles for each subregion to encode their positions. These bit boxes ensure error control while reducing the bit count used for compression. We comprehensively evaluate our method against state-of-the-art compressors on cosmology, fluid dynamics, and fusion plasma datasets.","sentences":["This paper presents error-bounded lossy compression tailored for particle datasets from diverse scientific applications in cosmology, fluid dynamics, and fusion energy sciences.","As today's high-performance computing capabilities advance, these datasets often reach trillions of points, posing significant visualization, analysis, and storage challenges.","While error-bounded lossy compression makes it possible to represent floating-point values with strict pointwise accuracy guarantees, the lack of correlations in particle data's storage ordering often limits the compression ratio.","Inspired by quantization-encoding schemes in SZ lossy compressors, we dynamically determine the number of bits to encode particles of the dataset to increase the compression ratio.","Specifically, we utilize a k-d tree to partition particles into subregions and generate ``bit boxes'' centered at particles for each subregion to encode their positions.","These bit boxes ensure error control while reducing the bit count used for compression.","We comprehensively evaluate our method against state-of-the-art compressors on cosmology, fluid dynamics, and fusion plasma datasets."],"url":"http://arxiv.org/abs/2404.02826v1","category":"cs.IT"}
{"created":"2024-04-03 15:59:00","title":"Control of high-dimensional collective dynamics by deep neural feedback laws and kinetic modelling","abstract":"Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large. Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces. To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation. Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles. Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics.","sentences":["Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large.","Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces.","To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation.","Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles.","Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics."],"url":"http://arxiv.org/abs/2404.02825v1","category":"math.OC"}
{"created":"2024-04-03 15:55:39","title":"Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models","abstract":"The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer.","sentences":["The ability of large language models (LLMs) to follow instructions is crucial to real-world applications.","Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks.","To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints.","Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality.","We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback.","Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints.","On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics.","All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer."],"url":"http://arxiv.org/abs/2404.02823v1","category":"cs.CL"}
{"created":"2024-04-03 15:51:28","title":"Harnessing Orbital Hall Effect in Spin-Orbit Torque MRAM","abstract":"Spin-Orbit Torque (SOT) Magnetic Random-Access Memory (MRAM) devices offer improved power efficiency, nonvolatility, and performance compared to static RAM, making them ideal, for instance, for cache memory applications. Efficient magnetization switching, long data retention, and high-density integration in SOT MRAM require ferromagnets (FM) with perpendicular magnetic anisotropy (PMA) combined with large torques enhanced by Orbital Hall Effect (OHE). We have engineered PMA [Co/Ni]$_3$ FM on selected OHE layers (Ru, Nb, Cr) and investigated the potential of theoretically predicted larger orbital Hall conductivity (OHC) to quantify the torque and switching current in OHE/[Co/Ni]$_3$ stacks. Our results demonstrate a $\\sim$30\\% enhancement in damping-like torque efficiency with a positive sign for the Ru OHE layer compared to a pure Pt, accompanied by a $\\sim$20\\% reduction in switching current for Ru compared to pure Pt across more than 250 devices, leading to more than a 60\\% reduction in switching power. These findings validate the application of Ru in devices relevant to industrial contexts, supporting theoretical predictions regarding its superior OHC. This investigation highlights the potential of enhanced orbital torques to improve the performance of orbital-assisted SOT-MRAM, paving the way for next-generation memory technology.","sentences":["Spin-Orbit Torque (SOT) Magnetic Random-Access Memory (MRAM) devices offer improved power efficiency, nonvolatility, and performance compared to static RAM, making them ideal, for instance, for cache memory applications.","Efficient magnetization switching, long data retention, and high-density integration in SOT MRAM require ferromagnets (FM) with perpendicular magnetic anisotropy (PMA) combined with large torques enhanced by Orbital Hall Effect (OHE).","We have engineered PMA","[Co/Ni]$_3$ FM on selected OHE layers (Ru, Nb, Cr)","and","investigated the potential of theoretically predicted larger orbital Hall conductivity (OHC) to quantify the torque and switching current in OHE/[Co/Ni]$_3$ stacks.","Our results demonstrate a $\\sim$30\\% enhancement in damping-like torque efficiency with a positive sign for the Ru OHE layer compared to a pure Pt, accompanied by a $\\sim$20\\% reduction in switching current for Ru compared to pure Pt across more than 250 devices, leading to more than a 60\\% reduction in switching power.","These findings validate the application of Ru in devices relevant to industrial contexts, supporting theoretical predictions regarding its superior OHC.","This investigation highlights the potential of enhanced orbital torques to improve the performance of orbital-assisted SOT-MRAM, paving the way for next-generation memory technology."],"url":"http://arxiv.org/abs/2404.02821v1","category":"physics.app-ph"}
{"created":"2024-04-03 15:42:25","title":"Efficient Quantum Circuits for Non-Unitary and Unitary Diagonal Operators with Space-Time-Accuracy trade-offs","abstract":"Unitary and non-unitary diagonal operators are fundamental building blocks in quantum algorithms with applications in the resolution of partial differential equations, Hamiltonian simulations, the loading of classical data on quantum computers (quantum state preparation) and many others. In this paper, we introduce a general approach to implement unitary and non-unitary diagonal operators with efficient-adjustable-depth quantum circuits. The depth, {\\sl i.e.}, the number of layers of quantum gates of the quantum circuit, is reducible with respect either to the width, {\\sl i.e.}, the number of ancilla qubits, or to the accuracy between the implemented operator and the target one. While exact methods have an optimal exponential scaling either in terms of size, {\\sl i.e.}, the total number of primitive quantum gates, or width, approximate methods prove to be efficient for the class of diagonal operators depending on smooth, at least differentiable, functions. Our approach is general enough to allow any method for diagonal operators to become adjustable-depth or approximate, decreasing the depth of the circuit by increasing its width or its approximation level. This feature offers flexibility and can match with the hardware limitations in coherence time or cumulative gate error. We illustrate these methods by performing quantum state preparation and non-unitary-real-space simulation of the diffusion equation: an initial Gaussian function is prepared on a set of qubits before being evolved through the non-unitary evolution operator of the diffusion process.","sentences":["Unitary and non-unitary diagonal operators are fundamental building blocks in quantum algorithms with applications in the resolution of partial differential equations, Hamiltonian simulations, the loading of classical data on quantum computers (quantum state preparation) and many others.","In this paper, we introduce a general approach to implement unitary and non-unitary diagonal operators with efficient-adjustable-depth quantum circuits.","The depth, {\\sl i.e.}, the number of layers of quantum gates of the quantum circuit, is reducible with respect either to the width, {\\sl i.e.}, the number of ancilla qubits, or to the accuracy between the implemented operator and the target one.","While exact methods have an optimal exponential scaling either in terms of size, {\\sl i.e.}, the total number of primitive quantum gates, or width, approximate methods prove to be efficient for the class of diagonal operators depending on smooth, at least differentiable, functions.","Our approach is general enough to allow any method for diagonal operators to become adjustable-depth or approximate, decreasing the depth of the circuit by increasing its width or its approximation level.","This feature offers flexibility and can match with the hardware limitations in coherence time or cumulative gate error.","We illustrate these methods by performing quantum state preparation and non-unitary-real-space simulation of the diffusion equation: an initial Gaussian function is prepared on a set of qubits before being evolved through the non-unitary evolution operator of the diffusion process."],"url":"http://arxiv.org/abs/2404.02819v1","category":"quant-ph"}
{"created":"2024-04-03 15:39:13","title":"Thermodynamics and Quasinormal Modes of the Dymnikova Black Hole in Higher Dimensions","abstract":"In this study, we investigate the thermodynamic properties and quasinormal modes of Dymnikova black holes within the context of higher dimensions in Einstein's general theory of relativity. We calculate the thermodynamic parameters, including the Hawking temperature and heat capacity, which allowed us to investigate the black hole's stability. Lastly the quasinormal modes with the WKB formula were calculated.","sentences":["In this study, we investigate the thermodynamic properties and quasinormal modes of Dymnikova black holes within the context of higher dimensions in Einstein's general theory of relativity.","We calculate the thermodynamic parameters, including the Hawking temperature and heat capacity, which allowed us to investigate the black hole's stability.","Lastly the quasinormal modes with the WKB formula were calculated."],"url":"http://arxiv.org/abs/2404.02818v1","category":"gr-qc"}
{"created":"2024-04-03 15:38:36","title":"A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches","abstract":"Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models. Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.","sentences":["Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks.","Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment.","Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems.","This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO.","A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches.","Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models.","Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges."],"url":"http://arxiv.org/abs/2404.02817v1","category":"cs.RO"}
{"created":"2024-04-03 15:38:13","title":"A Dual Geometric Test for Forward-Flatness","abstract":"Forward-flatness is a generalization of static feedback linearizability and a special case of a more general flatness concept for discrete-time systems. Recently, it has been shown that this practically quite relevant property can be checked by computing a unique sequence of involutive distributions which generalizes the well-known static feedback linearization test. In this paper, a dual test for forward-flatness based on a unique sequence of integrable codistributions is derived. Since the main mathematical operations for determining this sequence are the intersection of codistributions and the calculation of Lie derivatives of 1-forms, it is computationally quite efficient. Furthermore, the formulation with codistributions also facilitates a comparison with the existing discrete-time literature regarding the closely related topic of dynamic feedback linearization, which is mostly formulated in terms of 1-forms rather than vector fields. The presented results are illustrated by two examples.","sentences":["Forward-flatness is a generalization of static feedback linearizability and a special case of a more general flatness concept for discrete-time systems.","Recently, it has been shown that this practically quite relevant property can be checked by computing a unique sequence of involutive distributions which generalizes the well-known static feedback linearization test.","In this paper, a dual test for forward-flatness based on a unique sequence of integrable codistributions is derived.","Since the main mathematical operations for determining this sequence are the intersection of codistributions and the calculation of Lie derivatives of 1-forms, it is computationally quite efficient.","Furthermore, the formulation with codistributions also facilitates a comparison with the existing discrete-time literature regarding the closely related topic of dynamic feedback linearization, which is mostly formulated in terms of 1-forms rather than vector fields.","The presented results are illustrated by two examples."],"url":"http://arxiv.org/abs/2404.02816v1","category":"math.OC"}
{"created":"2024-04-03 15:34:47","title":"Auxiliary Monge-Amp\u00e8re Equations in Orbifold Setting -- a Mean-Value Inequality","abstract":"In this note, we generalize a mean-value inequality of Guo-Phong-Sturm to the setting of a compact K\\\"ahler orbifold. This shows that their reasoning is insensitive to quotient singularities. As we aim for a self-contained exposition, we generalize some fundamental results, namely the $\\alpha$-invariant estimate by H\\\"ormander and Tian, an approximation result for the psh-envelope of a $(1,1)$-form in a K\\\"ahler class by Berman and an $L^\\infty$ estimate for this envelope by Guo-Phong-Tong-Wang.","sentences":["In this note, we generalize a mean-value inequality of Guo-Phong-Sturm to the setting of a compact K\\\"ahler orbifold.","This shows that their reasoning is insensitive to quotient singularities.","As we aim for a self-contained exposition, we generalize some fundamental results, namely the $\\alpha$-invariant estimate by H\\\"ormander and Tian, an approximation result for the psh-envelope of a $(1,1)$-form in a K\\\"ahler class by Berman and an $L^\\infty$ estimate for this envelope by Guo-Phong-Tong-Wang."],"url":"http://arxiv.org/abs/2404.02812v1","category":"math.DG"}
{"created":"2024-04-03 15:31:18","title":"Generative-Contrastive Heterogeneous Graph Neural Network","abstract":"Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \\textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest contrastive and generative baselines on node classification and link prediction tasks. To reproduce our work, we have open-sourced our code at https://github.com/xxx.","sentences":["Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges.","In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks.","However, data augmentation is still limited due to the discrete and abstract nature of graphs.","To tackle the above limitations, we propose a novel \\textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}.","Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm.","This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder.","2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples.","3) A hierarchical contrastive learning strategy for capturing local and global information.","Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced discriminator under the generative-contrastive perspective.","Finally, we compare our model with seventeen baselines on eight real-world datasets.","Our model outperforms the latest contrastive and generative baselines on node classification and link prediction tasks.","To reproduce our work, we have open-sourced our code at https://github.com/xxx."],"url":"http://arxiv.org/abs/2404.02810v1","category":"cs.LG"}
{"created":"2024-04-03 15:23:22","title":"Dancing above the abyss: Environmental effects and dark matter signatures in inspirals into massive black holes","abstract":"In this dissertation, we look at environmental effects in extreme and intermediate mass ratio inspirals into massive black holes. In these systems, stellar mass compact objects orbit massive black holes and lose orbital energy due to gravitational wave emission and other dissipative forces. We explore environmental interactions with dark matter spikes, stellar distributions, accretion disks, and combine and compare them. We discuss the existence and properties of dark matter spikes in the presence of these environmental effects. The signatures of the environmental effects, such as the phase space flow, dephasing, deshifting of the periapse, and alignment with accretion disks, are examined. These signatures are quantified in isolated spike systems, in dry, and in wet inspirals. We generally find dark matter effects to be subdominant to the other environmental effects, but their impact on the waveform is still observable and identifiable. Lastly, the rates of inspirals and the impact of spikes are estimated. All of these results are obtained with the help of a code imripy that is published alongside. If dark matter spikes exist, they should be observable with space-based gravitational wave observatories","sentences":["In this dissertation, we look at environmental effects in extreme and intermediate mass ratio inspirals into massive black holes.","In these systems, stellar mass compact objects orbit massive black holes and lose orbital energy due to gravitational wave emission and other dissipative forces.","We explore environmental interactions with dark matter spikes, stellar distributions, accretion disks, and combine and compare them.","We discuss the existence and properties of dark matter spikes in the presence of these environmental effects.","The signatures of the environmental effects, such as the phase space flow, dephasing, deshifting of the periapse, and alignment with accretion disks, are examined.","These signatures are quantified in isolated spike systems, in dry, and in wet inspirals.","We generally find dark matter effects to be subdominant to the other environmental effects, but their impact on the waveform is still observable and identifiable.","Lastly, the rates of inspirals and the impact of spikes are estimated.","All of these results are obtained with the help of a code imripy that is published alongside.","If dark matter spikes exist, they should be observable with space-based gravitational wave observatories"],"url":"http://arxiv.org/abs/2404.02808v1","category":"gr-qc"}
{"created":"2024-04-03 15:23:17","title":"An Optimization Framework to Personalize Passive Cardiac Mechanics","abstract":"Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning. However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions. This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data. The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration. With a focus on characterizing the passive mechanical behavior, the framework employs structurally based anisotropic hyperelastic constitutive models and physiologically relevant boundary conditions to simulate myocardial mechanics. We use a stabilized variational multiscale formulation for solving the governing nonlinear elastodynamics equations, verified for cardiac mechanics applications. The framework is tested in myocardium models of biventricle and left atrium derived from cardiac phase-resolved computed tomographic (CT) images of a healthy subject and three patients with hypertrophic obstructive cardiomyopathy (HOCM). The impact of the choice of optimization methods and other numerical settings, including fiber direction parameters, mesh size, initial parameters for optimization, and perturbations to optimal material parameters, is assessed using a rigorous sensitivity analysis. The performance of the current iFEA is compared against an assumed power-law-based pressure-volume relation, typically used for single-phase image acquisition.","sentences":["Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning.","However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions.","This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data.","The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration.","With a focus on characterizing the passive mechanical behavior, the framework employs structurally based anisotropic hyperelastic constitutive models and physiologically relevant boundary conditions to simulate myocardial mechanics.","We use a stabilized variational multiscale formulation for solving the governing nonlinear elastodynamics equations, verified for cardiac mechanics applications.","The framework is tested in myocardium models of biventricle and left atrium derived from cardiac phase-resolved computed tomographic (CT) images of a healthy subject and three patients with hypertrophic obstructive cardiomyopathy (HOCM).","The impact of the choice of optimization methods and other numerical settings, including fiber direction parameters, mesh size, initial parameters for optimization, and perturbations to optimal material parameters, is assessed using a rigorous sensitivity analysis.","The performance of the current iFEA is compared against an assumed power-law-based pressure-volume relation, typically used for single-phase image acquisition."],"url":"http://arxiv.org/abs/2404.02807v1","category":"physics.med-ph"}
{"created":"2024-04-03 15:20:57","title":"The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers","abstract":"Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.","sentences":["Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests.","As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding.","In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates.","To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support.","We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance.","Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support.","In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals.","We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models."],"url":"http://arxiv.org/abs/2404.02806v1","category":"cs.SE"}
{"created":"2024-04-03 15:19:06","title":"Collaboratively adding context to social media posts reduces the sharing of false news","abstract":"We build a novel database of around 285,000 notes from the Twitter Community Notes program to analyze the causal influence of appending contextual information to potentially misleading posts on their dissemination. Employing a difference in difference design, our findings reveal that adding context below a tweet reduces the number of retweets by almost half. A significant, albeit smaller, effect is observed when focusing on the number of replies or quotes. Community Notes also increase by 80% the probability that a tweet is deleted by its creator. The post-treatment impact is substantial, but the overall effect on tweet virality is contingent upon the timing of the contextual information's publication. Our research concludes that, although crowdsourced fact-checking is effective, its current speed may not be adequate to substantially reduce the dissemination of misleading information on social media.","sentences":["We build a novel database of around 285,000 notes from the Twitter Community Notes program to analyze the causal influence of appending contextual information to potentially misleading posts on their dissemination.","Employing a difference in difference design, our findings reveal that adding context below a tweet reduces the number of retweets by almost half.","A significant, albeit smaller, effect is observed when focusing on the number of replies or quotes.","Community Notes also increase by 80% the probability that a tweet is deleted by its creator.","The post-treatment impact is substantial, but the overall effect on tweet virality is contingent upon the timing of the contextual information's publication.","Our research concludes that, although crowdsourced fact-checking is effective, its current speed may not be adequate to substantially reduce the dissemination of misleading information on social media."],"url":"http://arxiv.org/abs/2404.02803v1","category":"econ.GN"}
{"created":"2024-04-03 15:18:00","title":"Quantum Big-Bounce as a phenomenology of RQM in the Mini-superspace","abstract":"We investigate the emergence of a quantum Big-Bounce in the context of an isotropic Universe, filled by a self-interacting scalar field, which plays the role of a physical clock. The bouncing cosmology is the result of a scattering process, driven by the scalar field potential, which presence breaks down the frequency separation of the Wheeler-DeWitt equation, treated in strict analogy to a relativistic quantum system. Differently from previous analyses, we consider a really perturbative self-interaction potential, affecting the dynamics in a finite range of the time labeled by the scalar clock (and in particular we remove the divergent character previously allowed). The main result of the present analysis is that, when the Relativistic Quantum Mechanics formalism is properly implemented in the Mini-superspace analogy, the probability amplitude for the bounce is, both in the standard and polymerized case, characterized by a maximum in correspondence of the quasi-classical condition of a Universe minimum volume.","sentences":["We investigate the emergence of a quantum Big-Bounce in the context of an isotropic Universe, filled by a self-interacting scalar field, which plays the role of a physical clock.","The bouncing cosmology is the result of a scattering process, driven by the scalar field potential, which presence breaks down the frequency separation of the Wheeler-DeWitt equation, treated in strict analogy to a relativistic quantum system.","Differently from previous analyses, we consider a really perturbative self-interaction potential, affecting the dynamics in a finite range of the time labeled by the scalar clock (and in particular we remove the divergent character previously allowed).","The main result of the present analysis is that, when the Relativistic Quantum Mechanics formalism is properly implemented in the Mini-superspace analogy, the probability amplitude for the bounce is, both in the standard and polymerized case, characterized by a maximum in correspondence of the quasi-classical condition of a Universe minimum volume."],"url":"http://arxiv.org/abs/2404.02802v1","category":"gr-qc"}
{"created":"2024-04-03 15:17:22","title":"The Primordial Black Hole Formation -- Null Geodesic Correspondence","abstract":"We provide evidence for a correspondence between the formation of primordial black holes and the stability of circular null geodesics around the collapsing perturbation. We first show that the critical threshold of the compaction function to form a black hole in radiation is well approximated by the critical threshold for the appearance of the first unstable circular orbit. We also show that the critical exponent in the scaling law of the primordial black hole mass close to the threshold is set by the inverse of the Lyapunov coefficient of the unstable orbits when a self-similar stage is developed close to criticality.","sentences":["We provide evidence for a correspondence between the formation of primordial black holes and the stability of circular null geodesics around the collapsing perturbation.","We first show that the critical threshold of the compaction function to form a black hole in radiation is well approximated by the critical threshold for the appearance of the first unstable circular orbit.","We also show that the critical exponent in the scaling law of the primordial black hole mass close to the threshold is set by the inverse of the Lyapunov coefficient of the unstable orbits when a self-similar stage is developed close to criticality."],"url":"http://arxiv.org/abs/2404.02801v1","category":"astro-ph.CO"}
{"created":"2024-04-03 15:17:21","title":"On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension","abstract":"Question Generation aims to automatically generate questions based on a given input provided as context. A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control. In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts. We aim to control two attributes: the question's explicitness and underlying narrative elements. With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model. Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs. However, these improvements are not always statistically significant. The code is publicly available at github.com/bernardoleite/few-shot-prompting-qg-control.","sentences":["Question Generation aims to automatically generate questions based on a given input provided as context.","A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control.","In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts.","We aim to control two attributes: the question's explicitness and underlying narrative elements.","With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model.","Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs.","However, these improvements are not always statistically significant.","The code is publicly available at github.com/bernardoleite/few-shot-prompting-qg-control."],"url":"http://arxiv.org/abs/2404.02800v1","category":"cs.CL"}
{"created":"2024-04-03 15:08:57","title":"Direct, simple, and efficient computation of all components of the virtual-casing magnetic field in axisymmetric geometries with Kapur-Rokhlin quadrature","abstract":"In a recent publication (Toler et al. 2023), we demonstrated that for axisymmetric geometries, the Kapur-Rokhlin quadrature rule provided an efficient and high-order accurate method for computing the normal component, on the plasma surface, of the magnetic field due to the toroidal current flowing in the plasma, via the virtual-casing principle. The calculation was indirect, as it required the prior computation of the magnetic vector potential from the virtual-casing principle, followed by the computation of its tangential derivative by Fourier differentiation, in order to obtain the normal component of the magnetic field. Our approach did not provide the other components of the virtual-casing magnetic field.   In this letter, we show that a more direct and more general approach is available for the computation of the virtual-casing magnetic field. The Kapur-Rokhlin quadrature rule accurately calculates the principal value integrals in the expression for all the components of the magnetic field on the plasma boundary, and the numerical error converges at a rate nearly as high as the indirect method we presented previously.","sentences":["In a recent publication (Toler et al. 2023), we demonstrated that for axisymmetric geometries, the Kapur-Rokhlin quadrature rule provided an efficient and high-order accurate method for computing the normal component, on the plasma surface, of the magnetic field due to the toroidal current flowing in the plasma, via the virtual-casing principle.","The calculation was indirect, as it required the prior computation of the magnetic vector potential from the virtual-casing principle, followed by the computation of its tangential derivative by Fourier differentiation, in order to obtain the normal component of the magnetic field.","Our approach did not provide the other components of the virtual-casing magnetic field.   ","In this letter, we show that a more direct and more general approach is available for the computation of the virtual-casing magnetic field.","The Kapur-Rokhlin quadrature rule accurately calculates the principal value integrals in the expression for all the components of the magnetic field on the plasma boundary, and the numerical error converges at a rate nearly as high as the indirect method we presented previously."],"url":"http://arxiv.org/abs/2404.02799v1","category":"physics.plasm-ph"}
{"created":"2024-04-03 15:07:00","title":"AI and personalized learning: bridging the gap with modern educational goals","abstract":"Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our analysis indicates a gap between the objectives of modern education and the current direction of PL. We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies. While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.","sentences":["Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education.","Technology-based PL solutions have shown notable effectiveness in enhancing learning performance.","However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas.","In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals.","Our analysis indicates a gap between the objectives of modern education and the current direction of PL.","We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies.","While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system.","Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning."],"url":"http://arxiv.org/abs/2404.02798v1","category":"cs.CY"}
{"created":"2024-04-03 15:02:14","title":"Spin foam amplitude of the black-to-white hole transition","abstract":"It has been conjectured that quantum gravity effects may cause the black-to-white hole transition due to quantum tunneling. The transition amplitude of this process is explored within the framework of the spin foam model on a 2-complex containing 56 vertices. We develop a systematic way to construct the bulk triangulation from the boundary triangulation to obtain the 2-complex. By using Thiemann's complexifier coherent state as the boundary state to resemble the semiclassical geometry, we introduce a procedure to calculate the parameters labeling the coherent state from the continuous curved geometry. Considering that triad fields of different orientations, i.e., $e_i^a$ and $-e_i^a$, give the same intrinsic geometry of the boundary, we creatively adopt the boundary state as a superposition of the coherent states associated with both orientations. We employ the method of complex critical point to numerically compute the transition amplitude. Despite the numerical results, it is interestingly found that the transition amplitude is dominated by the terms allowing the change in orientation. This suggests that the black-to-white hole transition should be accompanied by quantum tunneling process of a change in orientation.","sentences":["It has been conjectured that quantum gravity effects may cause the black-to-white hole transition due to quantum tunneling.","The transition amplitude of this process is explored within the framework of the spin foam model on a 2-complex containing 56 vertices.","We develop a systematic way to construct the bulk triangulation from the boundary triangulation to obtain the 2-complex.","By using Thiemann's complexifier coherent state as the boundary state to resemble the semiclassical geometry, we introduce a procedure to calculate the parameters labeling the coherent state from the continuous curved geometry.","Considering that triad fields of different orientations, i.e., $e_i^a$ and $-e_i^a$, give the same intrinsic geometry of the boundary, we creatively adopt the boundary state as a superposition of the coherent states associated with both orientations.","We employ the method of complex critical point to numerically compute the transition amplitude.","Despite the numerical results, it is interestingly found that the transition amplitude is dominated by the terms allowing the change in orientation.","This suggests that the black-to-white hole transition should be accompanied by quantum tunneling process of a change in orientation."],"url":"http://arxiv.org/abs/2404.02796v1","category":"gr-qc"}
{"created":"2024-04-03 15:02:03","title":"Planning for Robust Open-loop Pushing: Exploiting Quasi-static Belief Dynamics and Contact-informed Optimization","abstract":"Non-prehensile manipulation such as pushing is typically subject to uncertain, non-smooth dynamics. However, modeling the uncertainty of the dynamics typically results in intractable belief dynamics, making data-efficient planning under uncertainty difficult. This article focuses on the problem of efficiently generating robust open-loop pushing plans. First, we investigate how the belief over object configurations propagates through quasi-static contact dynamics. We exploit the simplified dynamics to predict the variance of the object configuration without sampling from a perturbation distribution. In a sampling-based trajectory optimization algorithm, the gain of the variance is constrained in order to enforce robustness of the plan. Second, we propose an informed trajectory sampling mechanism for drawing robot trajectories that are likely to make contact with the object. This sampling mechanism is shown to significantly improve chances of finding robust solutions, especially when making-and-breaking contacts is required. We demonstrate that the proposed approach is able to synthesize bi-manual pushing trajectories, resulting in successful long-horizon pushing maneuvers without exteroceptive feedback such as vision or tactile feedback.","sentences":["Non-prehensile manipulation such as pushing is typically subject to uncertain, non-smooth dynamics.","However, modeling the uncertainty of the dynamics typically results in intractable belief dynamics, making data-efficient planning under uncertainty difficult.","This article focuses on the problem of efficiently generating robust open-loop pushing plans.","First, we investigate how the belief over object configurations propagates through quasi-static contact dynamics.","We exploit the simplified dynamics to predict the variance of the object configuration without sampling from a perturbation distribution.","In a sampling-based trajectory optimization algorithm, the gain of the variance is constrained in order to enforce robustness of the plan.","Second, we propose an informed trajectory sampling mechanism for drawing robot trajectories that are likely to make contact with the object.","This sampling mechanism is shown to significantly improve chances of finding robust solutions, especially when making-and-breaking contacts is required.","We demonstrate that the proposed approach is able to synthesize bi-manual pushing trajectories, resulting in successful long-horizon pushing maneuvers without exteroceptive feedback such as vision or tactile feedback."],"url":"http://arxiv.org/abs/2404.02795v1","category":"cs.RO"}
{"created":"2024-04-03 14:58:30","title":"Information propagation in far-from-equilibrium molecular templating networks is optimised by pseudo-equilibrium systems with negligible dissipation","abstract":"Far-from equilibrium molecular templating networks, like those that maintain the populations of RNA and protein molecules in the cell, are key biological motifs. These networks share the general property that assembled products are produced and degraded via complex pathways controlled by catalysts, including molecular templates. Although it has been suggested that the information propagated from templates to products sets a lower bound on the thermodynamic cost of these networks, this bound has not been explored rigorously to date. We show that, for an arbitrarily catalytic reaction network in steady state, the specificity with which a single product can dominate the ensemble is upper bounded, and the entropy of the product ensemble lower bounded, by a function of $\\Delta G$, the difference between the maximal and minimal free-energy changes along pathways to assembly. These simple bounds are particularly restrictive for systems with a smaller number of possible products $M$. Remarkably, however, although $\\Delta G$ constrains the information propagated to the product distribution, the systems that saturate the bound operate in a pseudo-equilibrium fashion, and there is no minimal entropy production rate for maintaining this non-equilibrium distribution. Moreover, for large systems, a vanishingly small subset of the possible products can dominate the product ensemble even for small values of $\\Delta G/\\ln M$.","sentences":["Far-from equilibrium molecular templating networks, like those that maintain the populations of RNA and protein molecules in the cell, are key biological motifs.","These networks share the general property that assembled products are produced and degraded via complex pathways controlled by catalysts, including molecular templates.","Although it has been suggested that the information propagated from templates to products sets a lower bound on the thermodynamic cost of these networks, this bound has not been explored rigorously to date.","We show that, for an arbitrarily catalytic reaction network in steady state, the specificity with which a single product can dominate the ensemble is upper bounded, and the entropy of the product ensemble lower bounded, by a function of $\\Delta G$, the difference between the maximal and minimal free-energy changes along pathways to assembly.","These simple bounds are particularly restrictive for systems with a smaller number of possible products $M$. Remarkably, however, although $\\Delta G$ constrains the information propagated to the product distribution, the systems that saturate the bound operate in a pseudo-equilibrium fashion, and there is no minimal entropy production rate for maintaining this non-equilibrium distribution.","Moreover, for large systems, a vanishingly small subset of the possible products can dominate the product ensemble even for small values of $\\Delta G/\\ln M$."],"url":"http://arxiv.org/abs/2404.02791v1","category":"physics.bio-ph"}
{"created":"2024-04-03 14:58:00","title":"MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation","abstract":"Text-to-image generation has achieved astonishing results, yet precise spatial controllability and prompt fidelity remain highly challenging. This limitation is typically addressed through cumbersome prompt engineering, scene layout conditioning, or image editing techniques which often require hand drawn masks. Nonetheless, pre-existing works struggle to take advantage of the natural instance-level compositionality of scenes due to the typically flat nature of rasterized RGB output images. Towards adressing this challenge, we introduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations of RGB images as multilayer, instance-wise RGBA decompositions, and over 100K instance images. To build MuLAn, we developed a training free pipeline which decomposes a monocular RGB image into a stack of RGBA layers comprising of background and isolated instances. We achieve this through the use of pretrained general-purpose models, and by developing three modules: image decomposition for instance discovery and extraction, instance completion to reconstruct occluded areas, and image re-assembly. We use our pipeline to create MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of image decompositions in terms of style, composition and complexity. With MuLAn, we provide the first photorealistic resource providing instance decomposition and occlusion information for high quality images, opening up new avenues for text-to-image generative AI research. With this, we aim to encourage the development of novel generation and editing technology, in particular layer-wise solutions. MuLAn data resources are available at https://MuLAn-dataset.github.io/.","sentences":["Text-to-image generation has achieved astonishing results, yet precise spatial controllability and prompt fidelity remain highly challenging.","This limitation is typically addressed through cumbersome prompt engineering, scene layout conditioning, or image editing techniques which often require hand drawn masks.","Nonetheless, pre-existing works struggle to take advantage of the natural instance-level compositionality of scenes due to the typically flat nature of rasterized RGB output images.","Towards adressing this challenge, we introduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations of RGB images as multilayer, instance-wise RGBA decompositions, and over 100K instance images.","To build MuLAn, we developed a training free pipeline which decomposes a monocular RGB image into a stack of RGBA layers comprising of background and isolated instances.","We achieve this through the use of pretrained general-purpose models, and by developing three modules: image decomposition for instance discovery and extraction, instance completion to reconstruct occluded areas, and image re-assembly.","We use our pipeline to create MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of image decompositions in terms of style, composition and complexity.","With MuLAn, we provide the first photorealistic resource providing instance decomposition and occlusion information for high quality images, opening up new avenues for text-to-image generative AI research.","With this, we aim to encourage the development of novel generation and editing technology, in particular layer-wise solutions.","MuLAn data resources are available at https://MuLAn-dataset.github.io/."],"url":"http://arxiv.org/abs/2404.02790v1","category":"cs.CV"}
{"created":"2024-04-03 14:56:06","title":"GenN2N: Generative NeRF2NeRF Translation","abstract":"We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc. Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space. Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs. To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs. The latent space is trained to align with a Gaussian distribution and the NeRFs are supervised through an adversarial loss on its renderings. To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a contrastive learning scheme. Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power. More results on our project page: https://xiangyueliu.github.io/GenN2N/","sentences":["We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc.","Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space.","Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs.","To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs.","The latent space is trained to align with a Gaussian distribution and the NeRFs are supervised through an adversarial loss on its renderings.","To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a contrastive learning scheme.","Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power.","More results on our project page: https://xiangyueliu.github.io/GenN2N/"],"url":"http://arxiv.org/abs/2404.02788v1","category":"cs.CV"}
{"created":"2024-04-03 14:55:58","title":"The Steinberg Tensor Product Theorem for General Linear Group Schemes in the Verlinde Category","abstract":"The Steinberg tensor product theorem is a fundamental result in the modular representation theory of reductive algebraic groups. It describes any finite-dimensional simple module of highest weight $\\lambda$ over such a group as the tensor product of Frobenius twists of simple modules with highest weights the weights appearing in a $p$-adic decomposition of $\\lambda$, thereby reducing the character problem to a a finite collection of weights. In recent years this theorem has been extended to various quasi-reductive supergroup schemes. In this paper, we prove the analogous result for the general linear group scheme $GL(X)$ for any object $X$ in the Verlinde category $\\mathrm{Ver}_p$.","sentences":["The Steinberg tensor product theorem is a fundamental result in the modular representation theory of reductive algebraic groups.","It describes any finite-dimensional simple module of highest weight $\\lambda$ over such a group as the tensor product of Frobenius twists of simple modules with highest weights the weights appearing in a $p$-adic decomposition of $\\lambda$, thereby reducing the character problem to a a finite collection of weights.","In recent years this theorem has been extended to various quasi-reductive supergroup schemes.","In this paper, we prove the analogous result for the general linear group scheme $GL(X)$ for any object $X$ in the Verlinde category $\\mathrm{Ver}_p$."],"url":"http://arxiv.org/abs/2404.02786v1","category":"math.RT"}
{"created":"2024-04-03 14:55:17","title":"Domain Generalization through Meta-Learning: A Survey","abstract":"Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions, paving the way for future innovation in meta-learning for domain generalization.","sentences":["Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications.","This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice.","Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains.","Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch.","This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization.","We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies.","Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field.","Our survey provides practical insights and an informed discussion on promising research directions, paving the way for future innovation in meta-learning for domain generalization."],"url":"http://arxiv.org/abs/2404.02785v1","category":"cs.LG"}
{"created":"2024-04-03 14:54:09","title":"Simulating the Universe from the cosmological horizon to halo scales","abstract":"Ultra-large scales close to the cosmological horizon will be probed by the upcoming observational campaigns. They hold the promise to constrain single-field inflation as well as general relativity, but in order to include them in the forthcoming analyses, their modelling has to be robust. In particular, fictitious gauge modes may be mistaken for primordial signals, and no consensus has emerged either from analytical modelling nor from the numerical route, obstructed by the large dynamical range to be simulated. In this work, we present a numerical technique to overcome the latter limitation: we compute the general relativistic displacement field with the N-body relativistic code gevolution and combine it with the accurate Newtonian simulation Gadget-4. This combination leads to an effective simulation reproducing the desired behaviour at the level of the matter power spectrum and bispectrum. We then measure, for the first time in a simulation, the relativistic scale-dependent bias in Poisson gauge; at redshift $z=0$, we find $b_1^{\\mathrm{GR}}=-5.7 \\pm 1.7$. Our results at the field level are only valid in the Poisson gauge and need to be complemented with a relativistic ray tracing algorithm to compute the number count observable.","sentences":["Ultra-large scales close to the cosmological horizon will be probed by the upcoming observational campaigns.","They hold the promise to constrain single-field inflation as well as general relativity, but in order to include them in the forthcoming analyses, their modelling has to be robust.","In particular, fictitious gauge modes may be mistaken for primordial signals, and no consensus has emerged either from analytical modelling nor from the numerical route, obstructed by the large dynamical range to be simulated.","In this work, we present a numerical technique to overcome the latter limitation: we compute the general relativistic displacement field with the N-body relativistic code gevolution and combine it with the accurate Newtonian simulation Gadget-4.","This combination leads to an effective simulation reproducing the desired behaviour at the level of the matter power spectrum and bispectrum.","We then measure, for the first time in a simulation, the relativistic scale-dependent bias in Poisson gauge; at redshift $z=0$, we find $b_1^{\\mathrm{GR}}=-5.7 \\pm 1.7$. Our results at the field level are only valid in the Poisson gauge and need to be complemented with a relativistic ray tracing algorithm to compute the number count observable."],"url":"http://arxiv.org/abs/2404.02783v1","category":"astro-ph.CO"}
{"created":"2024-04-03 14:52:20","title":"CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech","abstract":"With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.","sentences":["With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis.","Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences.","To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams.","Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed.","In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances."],"url":"http://arxiv.org/abs/2404.02781v1","category":"eess.AS"}
{"created":"2024-04-03 14:45:41","title":"Recovering generalized homology from Floer homology: the complex oriented case","abstract":"We associate an invariant called the completed Tate cohomology to a filtered circle-equivariant spectrum and a complex oriented cohomology theory. We show that when the filtered spectrum is the spectral symplectic cohomology of a Liouville manifold, this invariant depends only on the stable homotopy type of the underlying manifold. We make explicit computations for several complex oriented cohomology theories, including Eilenberg-Maclane spectra, Morava K-theories, their integral counterparts, and complex K-theory. We show that the result for Eilenberg-Maclane spectra depends only on the rational homology, and we use the computations for Morava K-theory to recover the integral homology (as an ungraded group). In a different direction, we use the completed Tate cohomology computations for the complex K-theory to recover the complex K-theory of the underlying manifold from its equivariant filtered Floer homotopy type. A key Floer theoretic input is the computation of local equivariant Floer theory near the orbit of an autonomous Hamiltonian, which may be of independent interest.","sentences":["We associate an invariant called the completed Tate cohomology to a filtered circle-equivariant spectrum and a complex oriented cohomology theory.","We show that when the filtered spectrum is the spectral symplectic cohomology of a Liouville manifold, this invariant depends only on the stable homotopy type of the underlying manifold.","We make explicit computations for several complex oriented cohomology theories, including Eilenberg-Maclane spectra, Morava K-theories, their integral counterparts, and complex K-theory.","We show that the result for Eilenberg-Maclane spectra depends only on the rational homology, and we use the computations for Morava K-theory to recover the integral homology (as an ungraded group).","In a different direction, we use the completed Tate cohomology computations for the complex K-theory to recover the complex K-theory of the underlying manifold from its equivariant filtered Floer homotopy type.","A key Floer theoretic input is the computation of local equivariant Floer theory near the orbit of an autonomous Hamiltonian, which may be of independent interest."],"url":"http://arxiv.org/abs/2404.02776v1","category":"math.SG"}
{"created":"2024-04-03 14:40:50","title":"Profile Likelihood via Optimisation and Differential Equations","abstract":"Profile likelihood provides a general framework to infer on a scalar parameter of a statistical model. A confidence interval is obtained by numerically finding the two abscissas where the profile log-likelihood curve intersects an horizontal line. An alternative derivation for this interval can be obtained by solving a constrained optimisation problem which can broadly be described as: maximise or minimise the parameter of interest under the constraint that the log-likelihood is high enough. This formulation allows nice geometrical interpretations; It can be used to infer on a parameter or on a known scalar function of the parameter, such as a quantile. Widely available routines for constrained optimisation can be used for this task, as well as Markov Chain Monte Carlo samplers. When the interest is on a smooth function depending on an extra continuous variable, the constrained optimisation framework can be used to derive Ordinary Differential Equation (ODE) for the confidence limits. This is illustrated with the return levels of Extreme Value models based on the Generalised Extreme Value distribution. Moreover the same ODE-based technique applies as well to the derivation of profile likelihood contours for couples of parameters. The initial value of the ODE used in the determination of the interval or the contour can itself be obtained by another auxiliary ODE with known initial value obtained by using the confidence level as the extra continuous variable.","sentences":["Profile likelihood provides a general framework to infer on a scalar parameter of a statistical model.","A confidence interval is obtained by numerically finding the two abscissas where the profile log-likelihood curve intersects an horizontal line.","An alternative derivation for this interval can be obtained by solving a constrained optimisation problem which can broadly be described as: maximise or minimise the parameter of interest under the constraint that the log-likelihood is high enough.","This formulation allows nice geometrical interpretations; It can be used to infer on a parameter or on a known scalar function of the parameter, such as a quantile.","Widely available routines for constrained optimisation can be used for this task, as well as Markov Chain Monte Carlo samplers.","When the interest is on a smooth function depending on an extra continuous variable, the constrained optimisation framework can be used to derive Ordinary Differential Equation (ODE) for the confidence limits.","This is illustrated with the return levels of Extreme Value models based on the Generalised Extreme Value distribution.","Moreover the same ODE-based technique applies as well to the derivation of profile likelihood contours for couples of parameters.","The initial value of the ODE used in the determination of the interval or the contour can itself be obtained by another auxiliary ODE with known initial value obtained by using the confidence level as the extra continuous variable."],"url":"http://arxiv.org/abs/2404.02774v1","category":"stat.CO"}
{"created":"2024-04-03 14:40:42","title":"Simultaneously Cloaking Electric and Hydrodynamic Fields via Electro-osmosis","abstract":"In this paper, we develop a general mathematical framework for the electro-osmosis problem to design simultaneous microscale electric and hydrodynamic cloaking in a Hele-Shaw configuration. A novel approach to achieving simultaneously cloaking both the electric and flow fields through a combination of scattering-cancellation technology and an electro-osmosis effect is proposed. In the design, the electric field is manipulated with scattering-cancellation technology while the pressure with electro-osmosis effect. As proof of this concept, the perfect electric and hydrodynamic cloaking conditions are derived for the cloaks with the cross-sectional shape being annulus or confocal ellipses using the layer potential techniques. Furthermore, we also propose an optimization scheme for the design of approximate cloaks within general geometries and prove the well-posedness of the optimization problem. In particular, the conditions that can ensure the simultaneous occurrence of approximate cloaks for general geometries are also established. Our theoretical findings are validated by a variety of numerical results and guide efficiently designing electric-related multiphysics cloaking.","sentences":["In this paper, we develop a general mathematical framework for the electro-osmosis problem to design simultaneous microscale electric and hydrodynamic cloaking in a Hele-Shaw configuration.","A novel approach to achieving simultaneously cloaking both the electric and flow fields through a combination of scattering-cancellation technology and an electro-osmosis effect is proposed.","In the design, the electric field is manipulated with scattering-cancellation technology while the pressure with electro-osmosis effect.","As proof of this concept, the perfect electric and hydrodynamic cloaking conditions are derived for the cloaks with the cross-sectional shape being annulus or confocal ellipses using the layer potential techniques.","Furthermore, we also propose an optimization scheme for the design of approximate cloaks within general geometries and prove the well-posedness of the optimization problem.","In particular, the conditions that can ensure the simultaneous occurrence of approximate cloaks for general geometries are also established.","Our theoretical findings are validated by a variety of numerical results and guide efficiently designing electric-related multiphysics cloaking."],"url":"http://arxiv.org/abs/2404.02773v1","category":"math.AP"}
{"created":"2024-04-03 14:34:22","title":"Precise Omega baryons from lattice QCD","abstract":"In this paper we determine the masses of $I(J^P)=0\\left(3/2^+\\right)$ and $0\\left(3/2^-\\right)$ $\\Omega$-baryon ground states using lattice QCD. We utilise Wilson-clover ensembles with $2+1$ dynamical quark flavours generated by the CLS consortium along a trajectory with a constant trace of the quark-mass matrix. We show that N$^3$LO $\\text{SU}(3)_f$ chiral perturbation theory expressions describe the ground-state masses with positive-parity well, and we use them to set the lattice scale. Methodologically, our combination of gauge-fixed wall sources and the generalized Pencil of Functions allows for high-precision determinations of the lattice spacing at a relative error of around $0.3\\%$ with controlled excited-state contamination. The fit we perform allows for the continuum value of $t_0$ to vary, thereby determining this quantity with a comparable level of precision to that of the lattice scale. Using the resulting scales our measurement of the negative-parity $\\Omega^{3/2^{-}}$ state is found to be consistent with the recently-discovered $\\Omega(2012)^-$, which can therefore be assigned the quantum numbers $I(J^P)=0\\left(3/2^-\\right)$.","sentences":["In this paper we determine the masses of $I(J^P)=0\\left(3/2^+\\right)$ and $0\\left(3/2^-\\right)$","$\\Omega$-baryon ground states using lattice QCD.","We utilise Wilson-clover ensembles with $2+1$ dynamical quark flavours generated by the CLS consortium along a trajectory with a constant trace of the quark-mass matrix.","We show that N$^3$LO $\\text{SU}(3)_f$ chiral perturbation theory expressions describe the ground-state masses with positive-parity well, and we use them to set the lattice scale.","Methodologically, our combination of gauge-fixed wall sources and the generalized Pencil of Functions allows for high-precision determinations of the lattice spacing at a relative error of around $0.3\\%$ with controlled excited-state contamination.","The fit we perform allows for the continuum value of $t_0$ to vary, thereby determining this quantity with a comparable level of precision to that of the lattice scale.","Using the resulting scales our measurement of the negative-parity $\\Omega^{3/2^{-}}$ state is found to be consistent with the recently-discovered $\\Omega(2012)^-$, which can therefore be assigned the quantum numbers $I(J^P)=0\\left(3/2^-\\right)$."],"url":"http://arxiv.org/abs/2404.02769v1","category":"hep-lat"}
{"created":"2024-04-03 14:16:17","title":"Estimation of Quantile Functionals in Linear Model","abstract":"Various indicators and measures of the real life procedures rise up as functionals of the quantile process of a parent random variable Z. However, Z can be observed only through a response in a linear model whose covariates are not under our control and the probability distribution of error terms is generally unknown. The problem is that of nonparametric estimation or other inference for such functionals. We propose an estimation procedure based on the averaged two-step regression quantile, recently developed by the authors, combined with an R-estimator of slopes of the linear model.","sentences":["Various indicators and measures of the real life procedures rise up as functionals of the quantile process of a parent random variable Z.","However, Z can be observed only through a response in a linear model whose covariates are not under our control and the probability distribution of error terms is generally unknown.","The problem is that of nonparametric estimation or other inference for such functionals.","We propose an estimation procedure based on the averaged two-step regression quantile, recently developed by the authors, combined with an R-estimator of slopes of the linear model."],"url":"http://arxiv.org/abs/2404.02764v1","category":"stat.ME"}
{"created":"2024-04-03 14:13:00","title":"A generalized Tur\u00e1n extension of the Deza--Erd\u0151s--Frankl Theorem","abstract":"For an integer $r \\ge 3$ and a subset $L \\subset [0,r-1]$, a graph $G$ is $(K_{r}, L)$-intersecting if the number of vertices in the intersection of every pair of $K_r$ in $G$ belongs to $L$. We study the maximum number of $K_r$ in an $n$-vertex $(K_{r}, L)$-intersecting graphs. The celebrated Ruzsa--Szemer\\'{e}di Theorem corresponds to the case $r=3$ and $L = \\{0,1\\}$.   For general $L$ with $2 \\le |L| \\le r-1$, we establish the upper bound $\\left(1-\\frac{1}{3r}\\right) \\prod_{\\ell \\in L}\\frac{n-\\ell}{r- \\ell}$ for large $n$, which improves the bound provided by the celebrated Deza--Erd\\H{o}s--Frankl Theorem by a factor of $1-\\frac{1}{3r}$. In the special case where $L = \\{t, t+1, \\ldots, r-1\\}$, we derive the tight upper bound for large $n$ and establish a corresponding stability result. This is an extension of the seminal Erd\\H{o}s--Ko--Rado Theorem on $t$-intersecting systems to the generalized Tur\\'{a}n setting.   Our proof for the Deza--Erd\\H{o}s--Frankl part involves an interesting combination of the $\\Delta$-system method and Tur\\'{a}n's theorem. Meanwhile, for the Erd\\H{o}s--Ko--Rado part, we employ the stability method, which relies on a theorem of Frankl regarding $t$-intersecting systems.","sentences":["For an integer $r \\ge 3$ and a subset $L \\subset [0,r-1]$, a graph $G$ is $(K_{r}, L)$-intersecting if the number of vertices in the intersection of every pair of $K_r$ in $G$ belongs to $L$. We study the maximum number of $K_r$ in an $n$-vertex $(K_{r}, L)$-intersecting graphs.","The celebrated Ruzsa--Szemer\\'{e}di Theorem corresponds to the case $r=3$ and $L = \\{0,1\\}$.   For general $L$ with $2 \\le |L| \\le r-1$, we establish the upper bound $\\left(1-\\frac{1}{3r}\\right) \\prod_{\\ell","\\in L}\\frac{n-\\ell}{r- \\ell}$ for large $n$, which improves the bound provided by the celebrated Deza--Erd\\H{o}s--Frankl Theorem by a factor of $1-\\frac{1}{3r}$. In the special case where $L = \\{t, t+1, \\ldots, r-1\\}$, we derive the tight upper bound for large $n$ and establish a corresponding stability result.","This is an extension of the seminal Erd\\H{o}s--Ko--Rado Theorem on $t$-intersecting systems to the generalized Tur\\'{a}n setting.   ","Our proof for the Deza--Erd\\H{o}s--","Frankl part involves an interesting combination of the $\\Delta$-system method and Tur\\'{a}n's theorem.","Meanwhile, for the Erd\\H{o}s--Ko--Rado part, we employ the stability method, which relies on a theorem of Frankl regarding $t$-intersecting systems."],"url":"http://arxiv.org/abs/2404.02762v1","category":"math.CO"}
{"created":"2024-04-03 14:07:02","title":"AQuA - Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs","abstract":"Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature.","sentences":["Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science.","Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible.","While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred.","In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post.","Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency.","We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score.","We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training.","The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature."],"url":"http://arxiv.org/abs/2404.02761v2","category":"cs.CL"}
{"created":"2024-04-03 14:05:39","title":"Unsupervised Occupancy Learning from Sparse Point Cloud","abstract":"Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while maximizing its entropy at the input point cloud. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve implicit shape inference with respect to baselines and the state-of-the-art using synthetic and real data.","sentences":["Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio.","Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry.","However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task.","In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs.","We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud.","We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while maximizing its entropy at the input point cloud.","Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve implicit shape inference with respect to baselines and the state-of-the-art using synthetic and real data."],"url":"http://arxiv.org/abs/2404.02759v1","category":"cs.CV"}
{"created":"2024-04-03 14:03:39","title":"Coarse spaces for non-symmetric two-level preconditioners based on local generalized eigenproblems","abstract":"Domain decomposition (DD) methods are a natural way to take advantage of parallel computers when solving large scale linear systems. Their scalability depends on the design of the coarse space used in the two-level method. The analysis of adaptive coarse spaces we present here is quite general since it applies to symmetric and non symmetric problems, to symmetric preconditioners such the additive Schwarz method (ASM) and to the non-symmetric preconditioner restricted additive Schwarz (RAS), as well as to exact or inexact subdomain solves. The coarse space is built by solving generalized eigenvalues in the subdomains and applying a well-chosen operator to the selected eigenvectors.","sentences":["Domain decomposition (DD) methods are a natural way to take advantage of parallel computers when solving large scale linear systems.","Their scalability depends on the design of the coarse space used in the two-level method.","The analysis of adaptive coarse spaces we present here is quite general since it applies to symmetric and non symmetric problems, to symmetric preconditioners such the additive Schwarz method (ASM) and to the non-symmetric preconditioner restricted additive Schwarz (RAS), as well as to exact or inexact subdomain solves.","The coarse space is built by solving generalized eigenvalues in the subdomains and applying a well-chosen operator to the selected eigenvectors."],"url":"http://arxiv.org/abs/2404.02758v1","category":"math.NA"}
{"created":"2024-04-03 14:01:47","title":"Multi-Polarization Superposition Beamforming: Novel Scheme of Transmit Power Allocation and Subcarrier Assignment","abstract":"The 5th generation (5G) new radio (NR) access technology and the beyond-5G future wireless communication require extremely high data rate and spectrum efficiency. Energy-efficient transmission/reception schemes are also regarded as an important component. The polarization domain has attracted substantial attention in this aspects. This paper is the first to propose \\textit{multi-polarization superposition beamforming (MPS-Beamforming)} with cross-polarization discrimination (XPD) and cross-polarization ratio (XPR)-aware transmit power allocation utilizing the 5G NR antenna panel structure. The appropriate orthogonal frequency division multiplexing (OFDM) subcarrier assignment algorithm is also proposed to verify the theoretical schemes via simulations. The detailed theoretical derivation along with comprehensive simulation results illustrate that the proposed novel scheme of MPS-Beamforming is significantly beneficial to the improvement of the performance in terms of the symbol error rate (SER) and signal-to-noise ratio (SNR) gain at the user equipment (UE). For instance, a provided practical wireless channel environment in the simulations exhibits 8 dB SNR gain for $10^{-4}$ SER in a deterministic channel, and 4 dB SNR gain for $10^{-5}$ SER in abundant statistical channel realizations.","sentences":["The 5th generation (5G) new radio (NR) access technology and the beyond-5G future wireless communication require extremely high data rate and spectrum efficiency.","Energy-efficient transmission/reception schemes are also regarded as an important component.","The polarization domain has attracted substantial attention in this aspects.","This paper is the first to propose \\textit{multi-polarization superposition beamforming (MPS-Beamforming)} with cross-polarization discrimination (XPD) and cross-polarization ratio (XPR)-aware transmit power allocation utilizing the 5G NR antenna panel structure.","The appropriate orthogonal frequency division multiplexing (OFDM) subcarrier assignment algorithm is also proposed to verify the theoretical schemes via simulations.","The detailed theoretical derivation along with comprehensive simulation results illustrate that the proposed novel scheme of MPS-Beamforming is significantly beneficial to the improvement of the performance in terms of the symbol error rate (SER) and signal-to-noise ratio (SNR) gain at the user equipment (UE).","For instance, a provided practical wireless channel environment in the simulations exhibits 8 dB SNR gain for $10^{-4}$ SER in a deterministic channel, and 4 dB SNR gain for $10^{-5}$ SER in abundant statistical channel realizations."],"url":"http://arxiv.org/abs/2404.02757v1","category":"eess.SP"}
{"created":"2024-04-03 13:57:08","title":"DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement","abstract":"We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos. By leveraging the capabilities of diverse large language models (LLMs), we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence. Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training. Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components. By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets like YouCook2 and ActivityNet. We outperform the previous state-of-the-art Vid2Seq across a majority of metrics, achieving this with just 0.4% of the unlabeled video data used for pre-training by Vid2Seq.","sentences":["We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos.","By leveraging the capabilities of diverse large language models (LLMs), we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence.","Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training.","Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components.","By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets like YouCook2 and ActivityNet.","We outperform the previous state-of-the-art Vid2Seq across a majority of metrics, achieving this with just 0.4% of the unlabeled video data used for pre-training by Vid2Seq."],"url":"http://arxiv.org/abs/2404.02755v1","category":"cs.CV"}
{"created":"2024-04-03 13:56:33","title":"Continual Learning of Numerous Tasks from Long-tail Distributions","abstract":"Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance. We propose a method that reuses the optimizer states in Adam by maintaining a weighted average of the second moments from previous tasks. We demonstrate that our method, compatible with most existing continual learning algorithms, effectively reduces forgetting with only a small amount of additional computational or memory costs, and provides further improvements on existing continual learning algorithms, particularly in a long-tail task sequence.","sentences":["Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge.","Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios.","In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes.","We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting.","Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance.","We propose a method that reuses the optimizer states in Adam by maintaining a weighted average of the second moments from previous tasks.","We demonstrate that our method, compatible with most existing continual learning algorithms, effectively reduces forgetting with only a small amount of additional computational or memory costs, and provides further improvements on existing continual learning algorithms, particularly in a long-tail task sequence."],"url":"http://arxiv.org/abs/2404.02754v1","category":"cs.LG"}
{"created":"2024-04-03 13:54:01","title":"The irreducibility of some families of linear series with imposed ramifications","abstract":"Suppose the generalized Brill-Noether number is zero, we prove that there exists a family of twice-marked smooth projective curves such that the family of linear series with two imposed ramification conditons is irreducible. Moreover, under certain conditions, we show that the monodromy group contains the alternating group. In the case $r=1$, the monodromy group is the full symmetric group.","sentences":["Suppose the generalized Brill-Noether number is zero, we prove that there exists a family of twice-marked smooth projective curves such that the family of linear series with two imposed ramification conditons is irreducible.","Moreover, under certain conditions, we show that the monodromy group contains the alternating group.","In the case $r=1$, the monodromy group is the full symmetric group."],"url":"http://arxiv.org/abs/2404.02753v1","category":"math.AG"}
{"created":"2024-04-03 13:44:41","title":"Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models","abstract":"This study explores the role of cross-attention during inference in text-conditional diffusion models. We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.","sentences":["This study explores the role of cross-attention during inference in text-conditional diffusion models.","We find that cross-attention outputs converge to a fixed point after few inference steps.","Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics.","Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance.","This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps.","Our empirical study on the MS-COCO validation set confirms its effectiveness.","The source code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."],"url":"http://arxiv.org/abs/2404.02747v1","category":"cs.CV"}
{"created":"2024-04-03 13:38:49","title":"Mixing Individual and Collective Behaviours to Predict Out-of-Routine Mobility","abstract":"Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics. While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours. Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy. Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods. Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility. During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models. By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges.","sentences":["Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics.","While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours.","Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy.","Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods.","Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility.","During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models.","By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges."],"url":"http://arxiv.org/abs/2404.02740v1","category":"cs.CY"}
{"created":"2024-04-03 13:36:53","title":"The Blaschke rolling theorem in Riemannian manifolds of bounded curvature","abstract":"We generalize the classical Blaschke Rolling Theorem to convex domains in Riemannian manifolds of bounded sectional curvature and arbitrary dimension. Our results are sharp and, in this sharp form, are new even in the model spaces of constant curvature.","sentences":["We generalize the classical Blaschke Rolling Theorem to convex domains in Riemannian manifolds of bounded sectional curvature and arbitrary dimension.","Our results are sharp and, in this sharp form, are new even in the model spaces of constant curvature."],"url":"http://arxiv.org/abs/2404.02739v1","category":"math.DG"}
{"created":"2024-04-03 13:35:51","title":"Adaptive Affinity-Based Generalization For MRI Imaging Segmentation Across Resource-Limited Settings","abstract":"The joint utilization of diverse data sources for medical imaging segmentation has emerged as a crucial area of research, aiming to address challenges such as data heterogeneity, domain shift, and data quality discrepancies. Integrating information from multiple data domains has shown promise in improving model generalizability and adaptability. However, this approach often demands substantial computational resources, hindering its practicality. In response, knowledge distillation (KD) has garnered attention as a solution. KD involves training light-weight models to emulate the behavior of more resource-intensive models, thereby mitigating the computational burden while maintaining performance. This paper addresses the pressing need to develop a lightweight and generalizable model for medical imaging segmentation that can effectively handle data integration challenges. Our proposed approach introduces a novel relation-based knowledge framework by seamlessly combining adaptive affinity-based and kernel-based distillation through a gram matrix that can capture the style representation across features. This methodology empowers the student model to accurately replicate the feature representations of the teacher model, facilitating robust performance even in the face of domain shift and data heterogeneity. To validate our innovative approach, we conducted experiments on publicly available multi-source prostate MRI data. The results demonstrate a significant enhancement in segmentation performance using lightweight networks. Notably, our method achieves this improvement while reducing both inference time and storage usage, rendering it a practical and efficient solution for real-time medical imaging segmentation.","sentences":["The joint utilization of diverse data sources for medical imaging segmentation has emerged as a crucial area of research, aiming to address challenges such as data heterogeneity, domain shift, and data quality discrepancies.","Integrating information from multiple data domains has shown promise in improving model generalizability and adaptability.","However, this approach often demands substantial computational resources, hindering its practicality.","In response, knowledge distillation (KD) has garnered attention as a solution.","KD involves training light-weight models to emulate the behavior of more resource-intensive models, thereby mitigating the computational burden while maintaining performance.","This paper addresses the pressing need to develop a lightweight and generalizable model for medical imaging segmentation that can effectively handle data integration challenges.","Our proposed approach introduces a novel relation-based knowledge framework by seamlessly combining adaptive affinity-based and kernel-based distillation through a gram matrix that can capture the style representation across features.","This methodology empowers the student model to accurately replicate the feature representations of the teacher model, facilitating robust performance even in the face of domain shift and data heterogeneity.","To validate our innovative approach, we conducted experiments on publicly available multi-source prostate MRI data.","The results demonstrate a significant enhancement in segmentation performance using lightweight networks.","Notably, our method achieves this improvement while reducing both inference time and storage usage, rendering it a practical and efficient solution for real-time medical imaging segmentation."],"url":"http://arxiv.org/abs/2404.02738v1","category":"cs.CV"}
{"created":"2024-04-03 13:34:09","title":"InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation","abstract":"Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements. Our codes will be available at https://github.com/InstantStyle/InstantStyle.","sentences":["Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization.","However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation.","Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others.","Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details.","Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability.","In this paper, we commence by examining several compelling yet frequently overlooked observations.","We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another.","2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.","Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements.","Our codes will be available at https://github.com/InstantStyle/InstantStyle."],"url":"http://arxiv.org/abs/2404.02733v1","category":"cs.CV"}
{"created":"2024-04-03 13:32:33","title":"Usage of OpenAlex for creating meaningful global overlay maps of science on the individual and institutional levels","abstract":"Global overlay maps of science use base maps that are overlaid by specific data (from single researchers, institutions, or countries) for visualizing scientific performance such as field-specific paper output. A procedure to create global overlay maps using OpenAlex is proposed. Six different global base maps are provided. Using one of these base maps, example overlay maps for one individual (the first author of this paper) and his research institution are shown and analyzed. A method for normalizing the overlay data is proposed. Overlay maps using raw overlay data display general concepts more pronounced than their counterparts using normalized overlay data. Advantages and limitations of the proposed overlay approach are discussed.","sentences":["Global overlay maps of science use base maps that are overlaid by specific data (from single researchers, institutions, or countries) for visualizing scientific performance such as field-specific paper output.","A procedure to create global overlay maps using OpenAlex is proposed.","Six different global base maps are provided.","Using one of these base maps, example overlay maps for one individual (the first author of this paper) and his research institution are shown and analyzed.","A method for normalizing the overlay data is proposed.","Overlay maps using raw overlay data display general concepts more pronounced than their counterparts using normalized overlay data.","Advantages and limitations of the proposed overlay approach are discussed."],"url":"http://arxiv.org/abs/2404.02732v1","category":"cs.DL"}
{"created":"2024-04-03 13:30:56","title":"Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss","abstract":"Recent research has highlighted improvements in high-quality imaging guided by event cameras, with most of these efforts concentrating on the RGB domain. However, these advancements frequently neglect the unique challenges introduced by the inherent flaws in the sensor design of event cameras in the RAW domain. Specifically, this sensor design results in the partial loss of pixel values, posing new challenges for RAW domain processes like demosaicing. The challenge intensifies as most research in the RAW domain is based on the premise that each pixel contains a value, making the straightforward adaptation of these methods to event camera demosaicing problematic. To end this, we present a Swin-Transformer-based backbone and a pixel-focus loss function for demosaicing with missing pixel values in RAW domain processing. Our core motivation is to refine a general and widely applicable foundational model from the RGB domain for RAW domain processing, thereby broadening the model's applicability within the entire imaging process. Our method harnesses multi-scale processing and space-to-depth techniques to ensure efficiency and reduce computing complexity. We also proposed the Pixel-focus Loss function for network fine-tuning to improve network convergence based on our discovery of a long-tailed distribution in training loss. Our method has undergone validation on the MIPI Demosaic Challenge dataset, with subsequent analytical experimentation confirming its efficacy. All code and trained models are released here: https://github.com/yunfanLu/ev-demosaic","sentences":["Recent research has highlighted improvements in high-quality imaging guided by event cameras, with most of these efforts concentrating on the RGB domain.","However, these advancements frequently neglect the unique challenges introduced by the inherent flaws in the sensor design of event cameras in the RAW domain.","Specifically, this sensor design results in the partial loss of pixel values, posing new challenges for RAW domain processes like demosaicing.","The challenge intensifies as most research in the RAW domain is based on the premise that each pixel contains a value, making the straightforward adaptation of these methods to event camera demosaicing problematic.","To end this, we present a Swin-Transformer-based backbone and a pixel-focus loss function for demosaicing with missing pixel values in RAW domain processing.","Our core motivation is to refine a general and widely applicable foundational model from the RGB domain for RAW domain processing, thereby broadening the model's applicability within the entire imaging process.","Our method harnesses multi-scale processing and space-to-depth techniques to ensure efficiency and reduce computing complexity.","We also proposed the Pixel-focus Loss function for network fine-tuning to improve network convergence based on our discovery of a long-tailed distribution in training loss.","Our method has undergone validation on the MIPI Demosaic Challenge dataset, with subsequent analytical experimentation confirming its efficacy.","All code and trained models are released here: https://github.com/yunfanLu/ev-demosaic"],"url":"http://arxiv.org/abs/2404.02731v1","category":"eess.IV"}
{"created":"2024-04-03 13:29:40","title":"Embedding relatively hyperbolic groups into products of binary trees","abstract":"We prove that if a group $G$ is relatively hyperbolic with respect to virtually abelian peripheral subgroups then $G$ quasiisometrically embeds into a product of binary trees. This extends the result of Buyalo, Dranishnikov and Schroeder in which they prove that a hyperbolic group quasiisometrically embeds into a product of binary trees. To prove the main result, we use the machinery of projection complexes and quasi-trees of metric spaces developed by Bestvina, Bromberg, Fujiwara and Sisto. We build on this theory by proving that one can remove certain edges from the quasi-tree of metric spaces, and be left with a tree of metric spaces which is quasiisometric to the quasi-tree of metric spaces. In particular, this reproves a result of Hume. Further, inspired by Buyalo, Dranishnikov and Schroeder's Alice's Diary, we develop a general theory of diaries and linear statistics. These notions provide a framework by which one can take a quasiisometric embedding of a metric space into a product of infinite-valence trees and upgrade it to a quasiisometric embedding into a product of binary trees.","sentences":["We prove that if a group $G$ is relatively hyperbolic with respect to virtually abelian peripheral subgroups then $G$ quasiisometrically embeds into a product of binary trees.","This extends the result of Buyalo, Dranishnikov and Schroeder in which they prove that a hyperbolic group quasiisometrically embeds into a product of binary trees.","To prove the main result, we use the machinery of projection complexes and quasi-trees of metric spaces developed by Bestvina, Bromberg, Fujiwara and Sisto.","We build on this theory by proving that one can remove certain edges from the quasi-tree of metric spaces, and be left with a tree of metric spaces which is quasiisometric to the quasi-tree of metric spaces.","In particular, this reproves a result of Hume.","Further, inspired by Buyalo, Dranishnikov and Schroeder's Alice's Diary, we develop a general theory of diaries and linear statistics.","These notions provide a framework by which one can take a quasiisometric embedding of a metric space into a product of infinite-valence trees and upgrade it to a quasiisometric embedding into a product of binary trees."],"url":"http://arxiv.org/abs/2404.02730v1","category":"math.GR"}
{"created":"2024-04-03 13:29:12","title":"Learning Sequence Attractors in Recurrent Networks with Hidden Neurons","abstract":"The brain is targeted for processing temporal sequence information. It remains largely unclear how the brain learns to store and retrieve sequence memories. Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly. We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect. We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons. The algorithm is proven to converge and lead to sequence attractors. We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets. We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain.","sentences":["The brain is targeted for processing temporal sequence information.","It remains largely unclear how the brain learns to store and retrieve sequence memories.","Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly.","We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect.","We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons.","The algorithm is proven to converge and lead to sequence attractors.","We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets.","We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain."],"url":"http://arxiv.org/abs/2404.02729v1","category":"cs.NE"}
{"created":"2024-04-03 13:28:52","title":"Unsupervised Learning of Effective Actions in Robotics","abstract":"Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate \"action prototypes\", each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results show that our effect driven discretization outperforms uniformly and randomly sampled discretizations in convergence speed and maximum reward.","sentences":["Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics.","Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions.","Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data.","In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate \"action prototypes\", each producing different effects in the environment.","After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes.","We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results show that our effect driven discretization outperforms uniformly and randomly sampled discretizations in convergence speed and maximum reward."],"url":"http://arxiv.org/abs/2404.02728v1","category":"cs.RO"}
{"created":"2024-04-03 13:27:54","title":"Harnessing the Power of Large Vision Language Models for Synthetic Image Detection","abstract":"In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda. This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioning models, we address the challenges associated with the potential misuse of synthetic images in real-world applications. Results described in this paper highlight the promising role of VLMs in the field of synthetic image detection, outperforming conventional image-based detection techniques. Code and models can be found at https://github.com/Mamadou-Keita/VLM-DETECT.","sentences":["In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions.","Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda.","This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification.","Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection.","By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models.","This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2.","By tailoring image captioning models, we address the challenges associated with the potential misuse of synthetic images in real-world applications.","Results described in this paper highlight the promising role of VLMs in the field of synthetic image detection, outperforming conventional image-based detection techniques.","Code and models can be found at https://github.com/Mamadou-Keita/VLM-DETECT."],"url":"http://arxiv.org/abs/2404.02726v1","category":"cs.CV"}
{"created":"2024-04-03 13:22:47","title":"On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices","abstract":"Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.","sentences":["Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation.","Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks.","Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon.","In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure.","Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores."],"url":"http://arxiv.org/abs/2404.02722v1","category":"cs.LG"}
{"created":"2024-04-03 13:21:58","title":"Can We Understand Plasticity Through Neural Collapse?","abstract":"This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse. We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task. Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting.","sentences":["This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse.","We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task.","Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting."],"url":"http://arxiv.org/abs/2404.02719v1","category":"cs.LG"}
{"created":"2024-04-03 13:20:36","title":"Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities","abstract":"Human-like Agents with diverse and dynamic personality could serve as an important design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive application.In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior. The Personality system includes three modules: Cognition, Emotion and Character Growth. The Behavior system comprises two modules: Planning and Action. We also build a simulation platform that enables agents to interact with the environment and other agents. Evolving Agents can simulate the human personality evolution process. Compared to its initial state, agents' personality and behavior patterns undergo believable development after several days of simulation. Agents reflect on their behavior to reason and develop new personality traits. These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution.In our experiment, we utilized simulation platform with 10 agents for evaluation. During the evaluation, these agents experienced believable and inspirational personality evolution. Through ablation and control experiments, we demonstrated the outstanding effectiveness of agent personality evolution and all modules of our agent architecture contribute to creating believable human-like agents with diverse and dynamic personalities. We also demonstrated through workshops how Evolving Agents could inspire designers.","sentences":["Human-like Agents with diverse and dynamic personality could serve as an important design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive application.","In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior.","The Personality system includes three modules: Cognition, Emotion and Character Growth.","The Behavior system comprises two modules: Planning and Action.","We also build a simulation platform that enables agents to interact with the environment and other agents.","Evolving Agents can simulate the human personality evolution process.","Compared to its initial state, agents' personality and behavior patterns undergo believable development after several days of simulation.","Agents reflect on their behavior to reason and develop new personality traits.","These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution.","In our experiment, we utilized simulation platform with 10 agents for evaluation.","During the evaluation, these agents experienced believable and inspirational personality evolution.","Through ablation and control experiments, we demonstrated the outstanding effectiveness of agent personality evolution and all modules of our agent architecture contribute to creating believable human-like agents with diverse and dynamic personalities.","We also demonstrated through workshops how Evolving Agents could inspire designers."],"url":"http://arxiv.org/abs/2404.02718v1","category":"cs.HC"}
{"created":"2024-04-03 13:20:24","title":"Automatic Prompt Selection for Large Language Models","abstract":"Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training and inference. It demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA.","sentences":["Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts.","However, designing effective prompts manually is challenging and time-consuming.","Existing methods for automatic prompt optimization either lack flexibility or efficiency.","In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts.","Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time.","Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training and inference.","It demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA."],"url":"http://arxiv.org/abs/2404.02717v1","category":"cs.CL"}
{"created":"2024-04-03 13:18:39","title":"Conditions on separability in multiqubit systems with an accelerating qubit using a conditional entropy","abstract":"The separability in multiqubit pure and mixed GHZ and W states with an accelerating qubit has been characterized using Abe Rajagopal (AR) $ q $-conditional entropy. We observe that the pure multiqubit GHZ and W states in the inertial : non-inertial bipartition with one of their qubits being accelerated will remain non-separable irrespective of the qubit's acceleration. In these systems, we effectively captured the variation of their non-separability with respect to the acceleration of the qubit and the AR $ q $-conditional entropy parameter $ q $. However, in the corresponding multiqubit mixed states obtained by introducing a noise to the above pure states, one could get stronger conditions on their separability in the inertial : non-inertial bipartition, in terms of the mixing parameter, acceleration of the qubit and the number of qubits in the system, in the asymptotic limit of parameter $ q $. These conditions obtained from AR $ q $-conditional entropy serves as a necessary conditions for separability in such multiqubit states with a relativistic qubit.","sentences":["The separability in multiqubit pure and mixed GHZ and W states with an accelerating qubit has been characterized using Abe Rajagopal (AR) $ q $-conditional entropy.","We observe that the pure multiqubit GHZ and W states in the inertial : non-inertial bipartition with one of their qubits being accelerated will remain non-separable irrespective of the qubit's acceleration.","In these systems, we effectively captured the variation of their non-separability with respect to the acceleration of the qubit and the AR $ q $-conditional entropy parameter $ q $.","However, in the corresponding multiqubit mixed states obtained by introducing a noise to the above pure states, one could get stronger conditions on their separability in the inertial : non-inertial bipartition, in terms of the mixing parameter, acceleration of the qubit and the number of qubits in the system, in the asymptotic limit of parameter $ q $.","These conditions obtained from AR $ q $-conditional entropy serves as a necessary conditions for separability in such multiqubit states with a relativistic qubit."],"url":"http://arxiv.org/abs/2404.02716v1","category":"quant-ph"}
{"created":"2024-04-03 13:10:05","title":"QDsim: An user-friendly toolbox for simulating large-scale quantum dot device","abstract":"We introduce QDsim, a python package tailored for the rapid generation of charge stability diagrams in large-scale quantum dot devices, extending beyond traditional double or triple dots. QDsim is founded on the constant interaction model from which we rephrase the task of finding the lowest energy charge configuration as a convex optimization problem. Therefore, we can leverage the existing package CVXPY, in combination with an appropriate powerful solver, for the convex optimization which streamlines the creation of stability diagrams and polytopes. Through multiple examples, we demonstrate how QDsim enables the generation of large-scale dataset that can serve a basis for the training of machine-learning models for automated tuning algorithms. While the package currently does not support quantum effects beyond the constant interaction model, QDsim is a tool that directly addresses the critical need for cost-effective and expeditious data acquisition for better tuning algorithms in order to accelerate the development of semiconductor quantum devices.","sentences":["We introduce QDsim, a python package tailored for the rapid generation of charge stability diagrams in large-scale quantum dot devices, extending beyond traditional double or triple dots.","QDsim is founded on the constant interaction model from which we rephrase the task of finding the lowest energy charge configuration as a convex optimization problem.","Therefore, we can leverage the existing package CVXPY, in combination with an appropriate powerful solver, for the convex optimization which streamlines the creation of stability diagrams and polytopes.","Through multiple examples, we demonstrate how QDsim enables the generation of large-scale dataset that can serve a basis for the training of machine-learning models for automated tuning algorithms.","While the package currently does not support quantum effects beyond the constant interaction model, QDsim is a tool that directly addresses the critical need for cost-effective and expeditious data acquisition for better tuning algorithms in order to accelerate the development of semiconductor quantum devices."],"url":"http://arxiv.org/abs/2404.02712v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 13:07:58","title":"Building test batteries based on analysing random number generator tests within the framework of algorithmic information theory","abstract":"The problem of testing random number generators is considered and it is shown that an approach based on algorithmic information theory allows us to compare the power of different tests in some cases where the available methods of mathematical statistics do not distinguish between the tests. In particular, it is shown that tests based on data compression methods using dictionaries should be included in the test batteries.","sentences":["The problem of testing random number generators is considered and it is shown that an approach based on algorithmic information theory allows us to compare the power of different tests in some cases where the available methods of mathematical statistics do not distinguish between the tests.","In particular, it is shown that tests based on data compression methods using dictionaries should be included in the test batteries."],"url":"http://arxiv.org/abs/2404.02708v1","category":"cs.IT"}
{"created":"2024-04-03 13:06:21","title":"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM","abstract":"Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.","sentences":["Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users.","Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated.","Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in.","Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text.","These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals.","To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text.","To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text.","The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness.","HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components.","HintDroid demo video: https://youtu.be/FWgfcctRbfI."],"url":"http://arxiv.org/abs/2404.02706v1","category":"cs.HC"}
{"created":"2024-04-03 13:05:34","title":"Revised bounds on local cosmic strings from NANOGrav observations","abstract":"In a recent paper, the NANOGrav collaboration studied new physics explanations of the observed pulsar timing residuals consistent with a stochastic gravitational wave background (SGWB), including cosmic strings in the Nambu-Goto (NG) approximation. Analysing one of current models for the loop distribution, it was found that the cosmic string model is disfavored compared to other sources, for example, super massive black hole binaries (SMBHBs). When both SMBHB and cosmic string models are included in the analysis, an upper bound on a string tension $G\\mu \\lesssim 10^{-10}$ was derived. However, the analysis did not accommodate results from cosmic string simulations in an underlying field theory, which indicate that at most a small fraction of string loops survive long enough to emit GW. Following and extending our previous study, we suppose that a fraction $f_{\\rm NG}$ of string loops follow NG dynamics and emit only GWs, and study the three different models of the loop distribution discussed in the LIGO-Virgo-KAGRA (LVK) collaboration analyses. We re-analyze the NANOGrav 15yrs data with our signal models by using the NANOGrav $\\texttt{ENTERPRISE}$ analysis code via the wrapper $\\texttt{PTArcade}$. We find that loop distributions similar to LVK Model B and C yield higher Bayes factor than Model A analyzed in the NANOGrav paper, as they can more easily accommodate a blue-tilted spectrum of the observed amplitude. Furthermore, because of the degeneracy of $G\\mu$ and $f_{\\rm NG}$ in determining the signal amplitude, our posterior distribution extends to higher values of $G\\mu$, and in some cases the uppermost value of credible intervals is close to the Cosmic Microwave Background limit $G\\mu \\lesssim 10^{-7}$. Hence, in addition to the pulsar timing array data, further information about the fraction of long-lived loops in a cosmic string network is required to constrain the string tension.","sentences":["In a recent paper, the NANOGrav collaboration studied new physics explanations of the observed pulsar timing residuals consistent with a stochastic gravitational wave background (SGWB), including cosmic strings in the Nambu-Goto (NG) approximation.","Analysing one of current models for the loop distribution, it was found that the cosmic string model is disfavored compared to other sources, for example, super massive black hole binaries (SMBHBs).","When both SMBHB and cosmic string models are included in the analysis, an upper bound on a string tension $G\\mu \\lesssim 10^{-10}$ was derived.","However, the analysis did not accommodate results from cosmic string simulations in an underlying field theory, which indicate that at most a small fraction of string loops survive long enough to emit GW.","Following and extending our previous study, we suppose that a fraction $f_{\\rm NG}$ of string loops follow NG dynamics and emit only GWs, and study the three different models of the loop distribution discussed in the LIGO-Virgo-KAGRA (LVK) collaboration analyses.","We re-analyze the NANOGrav 15yrs data with our signal models by using the NANOGrav $\\texttt{ENTERPRISE}$ analysis code via the wrapper $\\texttt{PTArcade}$. We find that loop distributions similar to LVK Model B and C yield higher Bayes factor than Model A analyzed in the NANOGrav paper, as they can more easily accommodate a blue-tilted spectrum of the observed amplitude.","Furthermore, because of the degeneracy of $G\\mu$ and $f_{\\rm NG}$ in determining the signal amplitude, our posterior distribution extends to higher values of $G\\mu$, and in some cases the uppermost value of credible intervals is close to the Cosmic Microwave Background limit $G\\mu \\lesssim 10^{-7}$. Hence, in addition to the pulsar timing array data, further information about the fraction of long-lived loops in a cosmic string network is required to constrain the string tension."],"url":"http://arxiv.org/abs/2404.02705v1","category":"astro-ph.CO"}
{"created":"2024-04-03 13:04:29","title":"The central limit theorems for integrable Hamiltonian systems perturbed by white noise","abstract":"In this paper, we consider the dynamics of integrable stochastic Hamiltonian systems. Utilizing the Nagaev-Guivarc'h method, we obtain several generalized results of the central limit theorem. Making use of this technique and the Birkhoff ergodic theorem, we prove that the invariant tori persist under stochastic perturbations. Moreover, they asymptotically follow a Gaussian distribution, which gives a positive answer to the stability of integrable stochastic Hamiltonian systems over time. Our results hold true for both Gaussian and non-Gaussian noises, and their intensities can be not small.","sentences":["In this paper, we consider the dynamics of integrable stochastic Hamiltonian systems.","Utilizing the Nagaev-Guivarc'h method, we obtain several generalized results of the central limit theorem.","Making use of this technique and the Birkhoff ergodic theorem, we prove that the invariant tori persist under stochastic perturbations.","Moreover, they asymptotically follow a Gaussian distribution, which gives a positive answer to the stability of integrable stochastic Hamiltonian systems over time.","Our results hold true for both Gaussian and non-Gaussian noises, and their intensities can be not small."],"url":"http://arxiv.org/abs/2404.02704v1","category":"math.DS"}
{"created":"2024-04-03 13:00:08","title":"PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders","abstract":"Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc. However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue. In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders. By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities. Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders. Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further improving the performance of PromptCodec. Experiments on LibriTTS demonstrate that our proposed PromptCodec consistently outperforms state-of-the-art neural speech codec models under all different bitrate conditions while achieving impressive performance with low bitrates.","sentences":["Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc.","However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue.","In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders.","By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities.","Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders.","Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further improving the performance of PromptCodec.","Experiments on LibriTTS demonstrate that our proposed PromptCodec consistently outperforms state-of-the-art neural speech codec models under all different bitrate conditions while achieving impressive performance with low bitrates."],"url":"http://arxiv.org/abs/2404.02702v1","category":"cs.SD"}
{"created":"2024-04-03 12:58:02","title":"Optimizing Peak Age of Information in MEC Systems: Computing Preemption and Non-preemption","abstract":"The freshness of information in real-time monitoring systems has received increasing attention, with Age of Information (AoI) emerging as a novel metric for measuring information freshness. In many applications, update packets need to be computed before being delivered to a destination. Mobile edge computing (MEC) is a promising approach for efficiently accomplishing the computing process, where the transmission process and computation process are coupled, jointly affecting freshness. In this paper, we aim to minimize the average peak AoI (PAoI) in an MEC system. We consider the generate-at-will source model and study when to generate a new update in two edge server setups: 1) computing preemption, where the packet in the computing process will be preempted by the newly arrived one, and 2) non-preemption, where the newly arrived packet will wait in the queue until the current one completes computing. We prove that the fixed threshold policy is optimal in a non-preemptive system for arbitrary transmission time and computation time distributions. In a preemptive system, we show that the transmission-aware threshold policy is optimal when the computing time follows an exponential distribution. Our numerical simulation results not only validate the theoretical findings but also demonstrate that: 1) in our problem, preemptive systems are not always superior to non-preemptive systems, even with exponential distribution, and 2) as the ratio of the mean transmission time to the mean computation time increases, the optimal threshold increases in preemptive systems but decreases in non-preemptive systems.","sentences":["The freshness of information in real-time monitoring systems has received increasing attention, with Age of Information (AoI) emerging as a novel metric for measuring information freshness.","In many applications, update packets need to be computed before being delivered to a destination.","Mobile edge computing (MEC) is a promising approach for efficiently accomplishing the computing process, where the transmission process and computation process are coupled, jointly affecting freshness.","In this paper, we aim to minimize the average peak AoI (PAoI) in an MEC system.","We consider the generate-at-will source model and study when to generate a new update in two edge server setups: 1) computing preemption, where the packet in the computing process will be preempted by the newly arrived one, and 2) non-preemption, where the newly arrived packet will wait in the queue until the current one completes computing.","We prove that the fixed threshold policy is optimal in a non-preemptive system for arbitrary transmission time and computation time distributions.","In a preemptive system, we show that the transmission-aware threshold policy is optimal when the computing time follows an exponential distribution.","Our numerical simulation results not only validate the theoretical findings but also demonstrate that: 1) in our problem, preemptive systems are not always superior to non-preemptive systems, even with exponential distribution, and 2) as the ratio of the mean transmission time to the mean computation time increases, the optimal threshold increases in preemptive systems but decreases in non-preemptive systems."],"url":"http://arxiv.org/abs/2404.02700v1","category":"cs.IT"}
{"created":"2024-04-03 12:57:19","title":"Scalable Model Editing via Customized Expert Networks","abstract":"Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editing methods. Our code is available at https: //github.com/TAL-auroraX/SCEN","sentences":["Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application.","Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner.","However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples.","To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm.","Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated.","Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert.","Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editing methods.","Our code is available at https: //github.com/TAL-auroraX/SCEN"],"url":"http://arxiv.org/abs/2404.02699v1","category":"cs.CL"}
{"created":"2024-04-03 12:54:16","title":"Model-agnostic Origin Attribution of Generated Images with Few-shot Examples","abstract":"Recent progress in visual generative models enables the generation of high-quality images. To prevent the misuse of generated images, it is important to identify the origin model that generates them. In this work, we study the origin attribution of generated images in a practical setting where only a few images generated by a source model are available and the source model cannot be accessed. The goal is to check if a given image is generated by the source model. We first formulate this problem as a few-shot one-class classification task. To solve the task, we propose OCC-CLIP, a CLIP-based framework for few-shot one-class classification, enabling the identification of an image's source model, even among multiple candidates. Extensive experiments corresponding to various generative models verify the effectiveness of our OCC-CLIP framework. Furthermore, an experiment based on the recently released DALL-E 3 API verifies the real-world applicability of our solution.","sentences":["Recent progress in visual generative models enables the generation of high-quality images.","To prevent the misuse of generated images, it is important to identify the origin model that generates them.","In this work, we study the origin attribution of generated images in a practical setting where only a few images generated by a source model are available and the source model cannot be accessed.","The goal is to check if a given image is generated by the source model.","We first formulate this problem as a few-shot one-class classification task.","To solve the task, we propose OCC-CLIP, a CLIP-based framework for few-shot one-class classification, enabling the identification of an image's source model, even among multiple candidates.","Extensive experiments corresponding to various generative models verify the effectiveness of our OCC-CLIP framework.","Furthermore, an experiment based on the recently released DALL-E 3 API verifies the real-world applicability of our solution."],"url":"http://arxiv.org/abs/2404.02697v1","category":"cs.CV"}
{"created":"2024-04-03 12:50:45","title":"Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition","abstract":"In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of face recognition, developing a novel method for privacy-preserving representation learning within an end-to-end training framework. Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss. This research provides a foundational exploration into the integration of information-theoretic privacy principles with representation learning, focusing specifically on the face recognition systems. We particularly highlight the adaptability of our framework with recent advancements in face recognition networks, such as AdaFace and ArcFace. In addition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This $\\mathsf{GenPF}$ model brings new perspectives on data generation methods with estimation-theoretic and information-theoretic privacy guarantees. Complementing these developments, we also present the deep variational PF (DVPF) model. This model proposes a tractable variational bound for measuring information leakage, enhancing the understanding of privacy preservation challenges in deep representation learning. The DVPF model, associated with both $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections with various generative models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion models. Complementing our theoretical contributions, we release a reproducible PyTorch package, facilitating further exploration and application of these privacy-preserving methodologies in face recognition systems.","sentences":["In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of face recognition, developing a novel method for privacy-preserving representation learning within an end-to-end training framework.","Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss.","This research provides a foundational exploration into the integration of information-theoretic privacy principles with representation learning, focusing specifically on the face recognition systems.","We particularly highlight the adaptability of our framework with recent advancements in face recognition networks, such as AdaFace and ArcFace.","In addition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$).","This $\\mathsf{GenPF}$ model brings new perspectives on data generation methods with estimation-theoretic and information-theoretic privacy guarantees.","Complementing these developments, we also present the deep variational PF (DVPF) model.","This model proposes a tractable variational bound for measuring information leakage, enhancing the understanding of privacy preservation challenges in deep representation learning.","The DVPF model, associated with both $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections with various generative models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion models.","Complementing our theoretical contributions, we release a reproducible PyTorch package, facilitating further exploration and application of these privacy-preserving methodologies in face recognition systems."],"url":"http://arxiv.org/abs/2404.02696v1","category":"cs.LG"}
{"created":"2024-04-03 12:48:58","title":"Comprehensive laboratory constraints on thermal desorption of interstellar ice analogues","abstract":"To explain grain growth and destruction in warm media, ice mantle formation and sublimation in cold media, and gas line emission spectroscopy, astrochemical models must mimic the gas--solid abundance ratio. Ice-sublimation mechanisms determine the position of snow lines and the nature of gas emitted by and locked inside planetary bodies in star-forming regions. To interpret observations from the interplanetary and extragalactic interstellar mediums, gas phase abundances must be modelled correctly. This study presents comprehensive thermal desorption data for interstellar ice analogues, aiming to refine astrochemical models by generating a set of benchmarks to evaluate both the kinetics and thermodynamics in astrochemical models. Our experiments focused on temperature-programmed desorption of pure and mixed ices, including Ar, CO, CO2, NH3, CH3OH, and H2O, under ultrahigh vacuum (1 x 10^-10 hPa) and low temperatures (10 K). Each experiment includes the experimental parameters, ice desorption kinetics for pure species, and the desorption yield (gas--solid ratio) for ice mixtures. From the desorption yields, we find common trends in the trapping of molecules when their abundance is compared to water: compact amorphous water ices are capable of trapping up to 20 % of volatiles (Ar, CO, and CO2), ~ 3 % of CH3OH, and ~ 5% NH3 in relation to the water content within the ice matrix; ammonium formate is not trapped in the water ice films, and compact amorphous water ice formed in situ has similar trapping capabilities to a compact amorphous water ice deposited using molecular beams. Our results highlight the limited trapping capacity of compact amorphous water ice for gases, crucial for understanding the formation of interstellar complex organic molecules.","sentences":["To explain grain growth and destruction in warm media, ice mantle formation and sublimation in cold media, and gas line emission spectroscopy, astrochemical models must mimic the gas--solid abundance ratio.","Ice-sublimation mechanisms determine the position of snow lines and the nature of gas emitted by and locked inside planetary bodies in star-forming regions.","To interpret observations from the interplanetary and extragalactic interstellar mediums, gas phase abundances must be modelled correctly.","This study presents comprehensive thermal desorption data for interstellar ice analogues, aiming to refine astrochemical models by generating a set of benchmarks to evaluate both the kinetics and thermodynamics in astrochemical models.","Our experiments focused on temperature-programmed desorption of pure and mixed ices, including Ar, CO, CO2, NH3, CH3OH, and H2O, under ultrahigh vacuum (1 x 10^-10 hPa) and low temperatures (10 K).","Each experiment includes the experimental parameters, ice desorption kinetics for pure species, and the desorption yield (gas--solid ratio) for ice mixtures.","From the desorption yields, we find common trends in the trapping of molecules when their abundance is compared to water: compact amorphous water ices are capable of trapping up to 20 % of volatiles (Ar, CO, and CO2), ~ 3 % of CH3OH, and ~ 5% NH3 in relation to the water content within the ice matrix; ammonium formate is not trapped in the water ice films, and compact amorphous water ice formed in situ has similar trapping capabilities to a compact amorphous water ice deposited using molecular beams.","Our results highlight the limited trapping capacity of compact amorphous water ice for gases, crucial for understanding the formation of interstellar complex organic molecules."],"url":"http://arxiv.org/abs/2404.02695v1","category":"astro-ph.GA"}
{"created":"2024-04-03 12:48:49","title":"Angular spectra of linear dynamical systems in discrete time","abstract":"In this work we introduce the notion of an angular spectrum for a linear discrete time nonautonomous dynamical system. The angular spectrum comprises all accumulation points of longtime averages formed by maximal principal angles between successive subspaces generated by the dynamical system. The angular spectrum is bounded by angular values which have previously been investigated by the authors. In this contribution we derive explicit formulas for the angular spectrum of some autonomous and specific nonautonomous systems. Based on a reduction principle we set up a numerical method for the general case; we investigate its convergence and apply the method to systems with a homoclinic orbit and a strange attractor. Our main theoretical result is a theorem on the invariance of the angular spectrum under summable perturbations of the given matrices (roughness theorem). It applies to systems with a so-called complete exponential dichotomy (CED), a concept which we introduce in this paper and which imposes more stringent conditions than those underlying the exponential dichotomy spectrum.","sentences":["In this work we introduce the notion of an angular spectrum for a linear discrete time nonautonomous dynamical system.","The angular spectrum comprises all accumulation points of longtime averages formed by maximal principal angles between successive subspaces generated by the dynamical system.","The angular spectrum is bounded by angular values which have previously been investigated by the authors.","In this contribution we derive explicit formulas for the angular spectrum of some autonomous and specific nonautonomous systems.","Based on a reduction principle we set up a numerical method for the general case; we investigate its convergence and apply the method to systems with a homoclinic orbit and a strange attractor.","Our main theoretical result is a theorem on the invariance of the angular spectrum under summable perturbations of the given matrices (roughness theorem).","It applies to systems with a so-called complete exponential dichotomy (CED), a concept which we introduce in this paper and which imposes more stringent conditions than those underlying the exponential dichotomy spectrum."],"url":"http://arxiv.org/abs/2404.02694v1","category":"math.DS"}
{"created":"2024-04-03 12:39:37","title":"Automated Inference of Graph Transformation Rules","abstract":"The explosion of data available in life sciences is fueling an increasing demand for expressive models and computational methods. Graph transformation is a model for dynamic systems with a large variety of applications. We introduce a novel method of the graph transformation model construction, combining generative and dynamical viewpoints to give a fully automated data-driven model inference method.   The method takes the input dynamical properties, given as a \"snapshot\" of the dynamics encoded by explicit transitions, and constructs a compatible model. The obtained model is guaranteed to be minimal, thus framing the approach as model compression (from a set of transitions into a set of rules). The compression is permissive to a lossy case, where the constructed model is allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics.   The task of graph transformation model inference is naturally highly challenging due to the combinatorics involved. We tackle the exponential explosion by proposing a heuristically minimal translation of the task into a well-established problem, set cover, for which highly optimized solutions exist. We further showcase how our results relate to Kolmogorov complexity expressed in terms of graph transformation.","sentences":["The explosion of data available in life sciences is fueling an increasing demand for expressive models and computational methods.","Graph transformation is a model for dynamic systems with a large variety of applications.","We introduce a novel method of the graph transformation model construction, combining generative and dynamical viewpoints to give a fully automated data-driven model inference method.   ","The method takes the input dynamical properties, given as a \"snapshot\" of the dynamics encoded by explicit transitions, and constructs a compatible model.","The obtained model is guaranteed to be minimal, thus framing the approach as model compression (from a set of transitions into a set of rules).","The compression is permissive to a lossy case, where the constructed model is allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics.   ","The task of graph transformation model inference is naturally highly challenging due to the combinatorics involved.","We tackle the exponential explosion by proposing a heuristically minimal translation of the task into a well-established problem, set cover, for which highly optimized solutions exist.","We further showcase how our results relate to Kolmogorov complexity expressed in terms of graph transformation."],"url":"http://arxiv.org/abs/2404.02692v1","category":"cs.DM"}
{"created":"2024-04-03 12:37:34","title":"Attention is Naturally Sparse with Gaussian Distributed Input","abstract":"The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of LLMs, paving the way for more scalable and efficient AI systems.","sentences":["The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures.","Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance.","This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs.","By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency.","Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness.","This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of LLMs, paving the way for more scalable and efficient AI systems."],"url":"http://arxiv.org/abs/2404.02690v1","category":"cs.LG"}
{"created":"2024-04-03 12:36:25","title":"Reinforcement Learning in Categorical Cybernetics","abstract":"We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as different extremal cases of this general setup: dynamic programming, Monte Carlo methods, temporal difference learning, and deep RL. We see this as strong evidence that this approach is a natural one and believe that it will be a fruitful way to think about RL in the future.","sentences":["We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes.","We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic.","The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample.","(2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration.","(3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent.","Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other.","As we show, many of the major classes of algorithms in RL can be seen as different extremal cases of this general setup: dynamic programming, Monte Carlo methods, temporal difference learning, and deep RL.","We see this as strong evidence that this approach is a natural one and believe that it will be a fruitful way to think about RL in the future."],"url":"http://arxiv.org/abs/2404.02688v1","category":"cs.LG"}
{"created":"2024-04-03 12:34:00","title":"Karma: An Experimental Study","abstract":"A system of non-tradable credits that flow between individuals like karma, hence proposed under that name, is a mechanism for repeated resource allocation that comes with attractive efficiency and fairness properties, in theory. In this study, we test karma in an online experiment in which human subjects repeatedly compete for a resource with time-varying and stochastic individual preferences or urgency to acquire the resource. We confirm that karma has significant and sustained welfare benefits even in a population with no prior training. We identify mechanism usage in contexts with sporadic high urgency, more so than with frequent moderate urgency, and implemented as an easy (binary) karma bidding scheme as particularly effective for welfare improvements: relatively larger aggregate efficiency gains are realized that are (almost) Pareto superior. These findings provide guidance for further testing and for future implementation plans of such mechanisms in the real world.","sentences":["A system of non-tradable credits that flow between individuals like karma, hence proposed under that name, is a mechanism for repeated resource allocation that comes with attractive efficiency and fairness properties, in theory.","In this study, we test karma in an online experiment in which human subjects repeatedly compete for a resource with time-varying and stochastic individual preferences or urgency to acquire the resource.","We confirm that karma has significant and sustained welfare benefits even in a population with no prior training.","We identify mechanism usage in contexts with sporadic high urgency, more so than with frequent moderate urgency, and implemented as an easy (binary) karma bidding scheme as particularly effective for welfare improvements: relatively larger aggregate efficiency gains are realized that are (almost) Pareto superior.","These findings provide guidance for further testing and for future implementation plans of such mechanisms in the real world."],"url":"http://arxiv.org/abs/2404.02687v1","category":"econ.GN"}
{"created":"2024-04-03 12:32:13","title":"Design2Cloth: 3D Cloth Generation from 2D Masks","abstract":"In recent years, there has been a significant shift in the field of digital avatar research, towards modeling, animating and reconstructing clothed human representations, as a key step towards creating realistic avatars. However, current 3D cloth generation methods are garment specific or trained completely on synthetic data, hence lacking fine details and realism. In this work, we make a step towards automatic realistic garment design and propose Design2Cloth, a high fidelity 3D generative model trained on a real world dataset from more than 2000 subject scans. To provide vital contribution to the fashion industry, we developed a user-friendly adversarial model capable of generating diverse and detailed clothes simply by drawing a 2D cloth mask. Under a series of both qualitative and quantitative experiments, we showcase that Design2Cloth outperforms current state-of-the-art cloth generative models by a large margin. In addition to the generative properties of our network, we showcase that the proposed method can be used to achieve high quality reconstructions from single in-the-wild images and 3D scans. Dataset, code and pre-trained model will become publicly available.","sentences":["In recent years, there has been a significant shift in the field of digital avatar research, towards modeling, animating and reconstructing clothed human representations, as a key step towards creating realistic avatars.","However, current 3D cloth generation methods are garment specific or trained completely on synthetic data, hence lacking fine details and realism.","In this work, we make a step towards automatic realistic garment design and propose Design2Cloth, a high fidelity 3D generative model trained on a real world dataset from more than 2000 subject scans.","To provide vital contribution to the fashion industry, we developed a user-friendly adversarial model capable of generating diverse and detailed clothes simply by drawing a 2D cloth mask.","Under a series of both qualitative and quantitative experiments, we showcase that Design2Cloth outperforms current state-of-the-art cloth generative models by a large margin.","In addition to the generative properties of our network, we showcase that the proposed method can be used to achieve high quality reconstructions from single in-the-wild images and 3D scans.","Dataset, code and pre-trained model will become publicly available."],"url":"http://arxiv.org/abs/2404.02686v1","category":"cs.CV"}
{"created":"2024-04-03 12:27:36","title":"Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers","abstract":"Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters. We experimented the efficacy of the method on varying sizes and alternative attention architectures and show that \\methodabbr significantly reduces the training time up to 2.5x times and converges to a better minimum with up to 2.6% stronger model on the LM benchmarks within the same compute budget.","sentences":["Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI).","A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers.","However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures.","In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework.","Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters.","We experimented the efficacy of the method on varying sizes and alternative attention architectures and show that \\methodabbr significantly reduces the training time up to 2.5x times and converges to a better minimum with up to 2.6% stronger model on the LM benchmarks within the same compute budget."],"url":"http://arxiv.org/abs/2404.02684v1","category":"cs.CL"}
{"created":"2024-04-03 12:24:48","title":"PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets","abstract":"Misogyny is often expressed through figurative language. Some neutral words can assume a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms. Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misogyny detection. Furthermore, we investigate LLMs' understanding of pejorative epithets by means of contextual word embeddings analysis and prompting.","sentences":["Misogyny is often expressed through figurative language.","Some neutral words can assume a negative connotation when functioning as pejorative epithets.","Disambiguating the meaning of such terms might help the detection of misogyny.","In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level.","We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection.","In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms.","Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misogyny detection.","Furthermore, we investigate LLMs' understanding of pejorative epithets by means of contextual word embeddings analysis and prompting."],"url":"http://arxiv.org/abs/2404.02681v1","category":"cs.CL"}
{"created":"2024-04-03 12:18:45","title":"Responsible Reporting for Frontier AI Development","abstract":"Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.","sentences":["Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems.","Organizations that develop and deploy frontier systems have significant access to such information.","By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems.","Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure.","We outline the key features of responsible reporting and propose mechanisms for implementing them in practice."],"url":"http://arxiv.org/abs/2404.02675v1","category":"cs.CY"}
{"created":"2024-04-03 12:14:17","title":"Explainable Ramanujan-type Congruences on Square-Classes of Arithmetic Progressions","abstract":"While examples of Ramanujan-type congruences are amply available via their relation to Hecke operators, it remains unclear which of them should be considered of combinatorial origin and which of them are mere artifacts of the connection with modular forms. Ranks and generalized ranks have been proposed as a tool to discern this question. We formalize this idea as \"explainable Ramanujan-type congruences\" with reference to Jacobi forms with singularities at torsion points, and then associated with them subspaces in a specific complex representation of the modular group. The resulting representation theoretic perspective allows us to prove that explainable Ramanujan-type congruences occur on square-classes $M \\mathbb{Z} + u^2 \\beta$ of arithmetic progressions.","sentences":["While examples of Ramanujan-type congruences are amply available via their relation to Hecke operators, it remains unclear which of them should be considered of combinatorial origin and which of them are mere artifacts of the connection with modular forms.","Ranks and generalized ranks have been proposed as a tool to discern this question.","We formalize this idea as \"explainable Ramanujan-type congruences\" with reference to Jacobi forms with singularities at torsion points, and then associated with them subspaces in a specific complex representation of the modular group.","The resulting representation theoretic perspective allows us to prove that explainable Ramanujan-type congruences occur on square-classes $M \\mathbb{Z} + u^2 \\beta$ of arithmetic progressions."],"url":"http://arxiv.org/abs/2404.02672v1","category":"math.NT"}
{"created":"2024-04-03 12:06:05","title":"Rays of the deformation cones of graphical zonotopes","abstract":"In this paper, we compute a triangulation of certain faces of the submodular cone. More precisely, graphical zonotopes are generalized permutahedra, and hence are associated to faces of the submodular cone. We give a triangulation of these faces for graphs without induced complete sub-graph on 4 vertices. This grants us access to the rays of these faces: Minkowski indecomposable deformations of these graphical zonotopes are segments and triangles. Besides, computer experiments lead to examples of graphs without induced complete sub-graph on 5 vertices, whose graphical zonotopes have high dimensional Minkowski indecomposable deformations.","sentences":["In this paper, we compute a triangulation of certain faces of the submodular cone.","More precisely, graphical zonotopes are generalized permutahedra, and hence are associated to faces of the submodular cone.","We give a triangulation of these faces for graphs without induced complete sub-graph on 4 vertices.","This grants us access to the rays of these faces: Minkowski indecomposable deformations of these graphical zonotopes are segments and triangles.","Besides, computer experiments lead to examples of graphs without induced complete sub-graph on 5 vertices, whose graphical zonotopes have high dimensional Minkowski indecomposable deformations."],"url":"http://arxiv.org/abs/2404.02669v1","category":"math.CO"}
{"created":"2024-04-03 12:02:36","title":"Design of a resonant slow extraction from low emittance electron booster rings using transverse resonance island buckets","abstract":"In this contribution we present the design of a resonant slow extraction based on the radio frequency knock-out (RF-KO) scheme, where we make use of the transverse resonance islands bucket (TRIB) optics. The generation of the TRIB optics is presented in two example lattices, that are considered for the potential upgrade of the current booster ring for the PETRA synchrotron radiation source at DESY. Simulations show an extraction efficiency in excess of 98% with a septum blade thickness of \\SI{30}{\\micro\\meter}.","sentences":["In this contribution we present the design of a resonant slow extraction based on the radio frequency knock-out (RF-KO) scheme, where we make use of the transverse resonance islands bucket (TRIB) optics.","The generation of the TRIB optics is presented in two example lattices, that are considered for the potential upgrade of the current booster ring for the PETRA synchrotron radiation source at DESY.","Simulations show an extraction efficiency in excess of 98% with a septum blade thickness of \\SI{30}{\\micro\\meter}."],"url":"http://arxiv.org/abs/2404.02665v1","category":"physics.acc-ph"}
{"created":"2024-04-03 12:00:58","title":"Ground-to-UAV 140 GHz channel measurement and modeling","abstract":"Unmanned Aerial Vehicle (UAV) assisted terahertz (THz) wireless communications have been expected to play a vital role in the next generation of wireless networks. UAVs can serve as either repeaters or data collectors within the communication link, thereby potentially augmenting the efficacy of communication systems. Despite their promise, the channel analysis and modeling specific to THz wireless channels leveraging UAVs remain under explored. This work delves into a ground-to-UAV channel at 140 GHz, with a specific focus on the influence of UAV hovering behavior on channel performance. Employing experimental measurements through an unmodulated channel setup and a geometry-based stochastic model (GBSM) that integrates three-dimensional positional coordinates and beamwidth, this work evaluates the impact of UAV dynamic movements and antenna orientation on channel performance. Our findings highlight the minimal impact of UAV orientation adjustments on channel performance and underscore the diminishing necessity for precise alignment between UAVs and ground stations as beamwidth increases.","sentences":["Unmanned Aerial Vehicle (UAV) assisted terahertz (THz) wireless communications have been expected to play a vital role in the next generation of wireless networks.","UAVs can serve as either repeaters or data collectors within the communication link, thereby potentially augmenting the efficacy of communication systems.","Despite their promise, the channel analysis and modeling specific to THz wireless channels leveraging UAVs remain under explored.","This work delves into a ground-to-UAV channel at 140 GHz, with a specific focus on the influence of UAV hovering behavior on channel performance.","Employing experimental measurements through an unmodulated channel setup and a geometry-based stochastic model (GBSM) that integrates three-dimensional positional coordinates and beamwidth, this work evaluates the impact of UAV dynamic movements and antenna orientation on channel performance.","Our findings highlight the minimal impact of UAV orientation adjustments on channel performance and underscore the diminishing necessity for precise alignment between UAVs and ground stations as beamwidth increases."],"url":"http://arxiv.org/abs/2404.02663v1","category":"eess.SP"}
{"created":"2024-04-03 11:49:43","title":"Adversarial Attacks and Dimensionality in Text Classifiers","abstract":"Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, adversarial attacks have been first identified and studied in the domain of image processing. In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks. We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effectiveness on models tuned with input samples with same embedding dimension. We utilize this sensitivity to design an adversarial defense mechanism. We use ensemble models of varying inherent dimensionality to thwart the attacks. This is tested on multiple datasets for its efficacy in providing robustness. We also study the problem of measuring adversarial perturbation using different distance metrics. For all of the aforementioned studies, we have run tests on multiple models with varying dimensionality and used a word-vector level adversarial attack to substantiate the findings.","sentences":["Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases.","They significantly undermine the ability of high-performance neural networks by forcing misclassifications.","These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it.","Historically, adversarial attacks have been first identified and studied in the domain of image processing.","In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks.","We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model.","Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effectiveness on models tuned with input samples with same embedding dimension.","We utilize this sensitivity to design an adversarial defense mechanism.","We use ensemble models of varying inherent dimensionality to thwart the attacks.","This is tested on multiple datasets for its efficacy in providing robustness.","We also study the problem of measuring adversarial perturbation using different distance metrics.","For all of the aforementioned studies, we have run tests on multiple models with varying dimensionality and used a word-vector level adversarial attack to substantiate the findings."],"url":"http://arxiv.org/abs/2404.02660v1","category":"cs.LG"}
{"created":"2024-04-03 11:40:17","title":"Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models","abstract":"Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.","sentences":["Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs).","Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs.","Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs.","However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs.","Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs.","Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL.","Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses."],"url":"http://arxiv.org/abs/2404.02657v1","category":"cs.CL"}
{"created":"2024-04-03 11:37:03","title":"Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging","abstract":"Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data. In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space. We introduce different types of non-negative matrix factorization (NMF) in few-shot learning, addressing the data scarcity issue in medical image classification. Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors. With 14 different datasets covering 11 distinct illness categories, thorough experimental results and comparison with related techniques demonstrate that NMF is a competitive alternative to PCA for few-shot learning in medical imaging, and the supervised NMF algorithms are more discriminative in the subspace with greater effectiveness. Furthermore, we show that the part-based representation of NMF, especially its supervised variants, is dramatically impactful in detecting lesion areas in medical imaging with limited samples.","sentences":["Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data.","In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space.","We introduce different types of non-negative matrix factorization (NMF) in few-shot learning, addressing the data scarcity issue in medical image classification.","Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors.","With 14 different datasets covering 11 distinct illness categories, thorough experimental results and comparison with related techniques demonstrate that NMF is a competitive alternative to PCA for few-shot learning in medical imaging, and the supervised NMF algorithms are more discriminative in the subspace with greater effectiveness.","Furthermore, we show that the part-based representation of NMF, especially its supervised variants, is dramatically impactful in detecting lesion areas in medical imaging with limited samples."],"url":"http://arxiv.org/abs/2404.02656v1","category":"cs.CV"}
{"created":"2024-04-03 11:36:12","title":"Calibrating the Confidence of Large Language Models by Eliciting Fidelity","abstract":"Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.","sentences":["Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless.","However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate.","In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models.","Then, we propose a plug-and-play method to estimate the confidence of language models.","Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets.","Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}.","Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration."],"url":"http://arxiv.org/abs/2404.02655v1","category":"cs.CL"}
{"created":"2024-04-03 11:32:48","title":"Tropical pseudostable curves","abstract":"We study the tropical version of the contraction morphism $\\mathcal{T}$ between moduli spaces of stable and pseudostable curves. By promoting $\\mathcal{T}$ to a logarithmic morphism, we obtain a piecewise linear function between the generalized cone complexes parameterizing tropical stable and pseudostable curves. The ray corresponding to the contracted divisor $\\delta_1$ is not contracted to the cone point but mapped onto a ray of $\\mathcal{M}_{g,n}^{{\\rm trop}, {\\rm ps}}$, with a slope reflecting the geometry of the desingularization of a plane cusp. We explore in detail the situation of $g=1$, where the tautological geometry of both spaces is fully described by piecewise polynomial functions on the tropical moduli spaces.","sentences":["We study the tropical version of the contraction morphism $\\mathcal{T}$ between moduli spaces of stable and pseudostable curves.","By promoting $\\mathcal{T}$ to a logarithmic morphism, we obtain a piecewise linear function between the generalized cone complexes parameterizing tropical stable and pseudostable curves.","The ray corresponding to the contracted divisor $\\delta_1$ is not contracted to the cone point but mapped onto a ray of $\\mathcal{M}_{g,n}^{{\\rm trop}, {\\rm ps}}$, with a slope reflecting the geometry of the desingularization of a plane cusp.","We explore in detail the situation of $g=1$, where the tautological geometry of both spaces is fully described by piecewise polynomial functions on the tropical moduli spaces."],"url":"http://arxiv.org/abs/2404.02654v1","category":"math.AG"}
{"created":"2024-04-03 11:32:45","title":"Mutual singularity of Riesz products on the unit sphere","abstract":"We prove analogs of Peyri\\`ere's mutual singularity theorem for standard and generalized Riesz products on the unit sphere of $\\mathbb{C}^n$, $n\\ge 2$. As a corollary, we obtain an analog of Zygmund's dichotomy for the Riesz products under consideration.","sentences":["We prove analogs of Peyri\\`ere's mutual singularity theorem for standard and generalized Riesz products on the unit sphere of $\\mathbb{C}^n$, $n\\ge 2$. As a corollary, we obtain an analog of Zygmund's dichotomy for the Riesz products under consideration."],"url":"http://arxiv.org/abs/2404.02652v1","category":"math.CV"}
{"created":"2024-04-03 11:25:20","title":"Towards detecting unanticipated bias in Large Language Models","abstract":"Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty of model decisions and to make the internal decision-making processes of LLMs more transparent, thereby identifying and understanding biases that are not immediately apparent. Through this research, we aim to contribute to the development of fairer and more transparent AI systems.","sentences":["Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems.","Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies.","This research largely targets well-known biases related to gender, race, ethnicity, and language.","However, it is clear that LLMs are also affected by other, less obvious implicit biases.","The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications.","In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods.","These approaches aim to assess the certainty of model decisions and to make the internal decision-making processes of LLMs more transparent, thereby identifying and understanding biases that are not immediately apparent.","Through this research, we aim to contribute to the development of fairer and more transparent AI systems."],"url":"http://arxiv.org/abs/2404.02650v1","category":"cs.LG"}
{"created":"2024-04-03 11:21:10","title":"A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems","abstract":"Recently, deep learning (DL) has been emerging as a promising approach for channel estimation and signal detection in wireless communications. The majority of the existing studies investigating the use of DL techniques in this domain focus on analysing channel impulse responses that are generated from only one channel distribution such as additive white Gaussian channel noise and Rayleigh channels. In practice, to cope with the dynamic nature of the wireless channel, DL methods must be re-trained on newly non-aged collected data which is costly, inefficient, and impractical. To tackle this challenge, this paper proposes a novel universal deep neural network (Uni-DNN) that can achieve high detection performance in various wireless environments without retraining the model. In particular, our proposed Uni-DNN model consists of a wireless channel classifier and a signal detector which are constructed by using DNNs. The wireless channel classifier enables the signal detector to generalise and perform optimally for multiple wireless channel distributions. In addition, to further improve the signal detection performance of the proposed model, convolutional neural network is employed. Extensive simulations using the orthogonal frequency division multiplexing scheme demonstrate that the bit error rate performance of our proposed solution can outperform conventional DL-based approaches as well as least square and minimum mean square error channel estimators in practical low pilot density scenarios.","sentences":["Recently, deep learning (DL) has been emerging as a promising approach for channel estimation and signal detection in wireless communications.","The majority of the existing studies investigating the use of DL techniques in this domain focus on analysing channel impulse responses that are generated from only one channel distribution such as additive white Gaussian channel noise and Rayleigh channels.","In practice, to cope with the dynamic nature of the wireless channel, DL methods must be re-trained on newly non-aged collected data which is costly, inefficient, and impractical.","To tackle this challenge, this paper proposes a novel universal deep neural network (Uni-DNN) that can achieve high detection performance in various wireless environments without retraining the model.","In particular, our proposed Uni-DNN model consists of a wireless channel classifier and a signal detector which are constructed by using DNNs.","The wireless channel classifier enables the signal detector to generalise and perform optimally for multiple wireless channel distributions.","In addition, to further improve the signal detection performance of the proposed model, convolutional neural network is employed.","Extensive simulations using the orthogonal frequency division multiplexing scheme demonstrate that the bit error rate performance of our proposed solution can outperform conventional DL-based approaches as well as least square and minimum mean square error channel estimators in practical low pilot density scenarios."],"url":"http://arxiv.org/abs/2404.02648v1","category":"cs.NI"}
{"created":"2024-04-03 11:16:26","title":"Ultralight scalar dark matter versus non-adiabatic perfect fluid dark matter in pulsar timing","abstract":"Recent evidence of direct detection of stochastic gravitational waves reported by pulsar timing array collaborations might open a new window for studying cosmology and astrophysical phenomena. In addition to signals from gravitational waves, there is motivation to explore residual signals from oscillating dark matter, which might partially comprise the galactic halo. We investigate fluctuations in pulsar timing originating from the coherent oscillation of scalar dark matter up to the subleading order of $\\mathcal{O}(k/m)$, as well as from acoustic oscillations of non-adiabatic perfect fluid dark matter. Both types of dark matter can generate oscillating Newtonian potential perturbations and curvature perturbations, thereby affecting pulsar timing. Our results show distinctive signatures in pulsar timing residuals and angular correlations for these dark matters. Specifically, pulsar timing residuals from non-adiabatic perfect fluid dark matter exhibit different directional dependence and are shown to be more sensitive to the distance to a pulsar. We also study the angular correlation patterns from these dark matters in the NANOGrav 15-year data set. The best fit might suggest that the composition of non-adiabatic perfect fluid dark matter in our galaxy is much greater than that of ultralight scalar dark matter.","sentences":["Recent evidence of direct detection of stochastic gravitational waves reported by pulsar timing array collaborations might open a new window for studying cosmology and astrophysical phenomena.","In addition to signals from gravitational waves, there is motivation to explore residual signals from oscillating dark matter, which might partially comprise the galactic halo.","We investigate fluctuations in pulsar timing originating from the coherent oscillation of scalar dark matter up to the subleading order of $\\mathcal{O}(k/m)$, as well as from acoustic oscillations of non-adiabatic perfect fluid dark matter.","Both types of dark matter can generate oscillating Newtonian potential perturbations and curvature perturbations, thereby affecting pulsar timing.","Our results show distinctive signatures in pulsar timing residuals and angular correlations for these dark matters.","Specifically, pulsar timing residuals from non-adiabatic perfect fluid dark matter exhibit different directional dependence and are shown to be more sensitive to the distance to a pulsar.","We also study the angular correlation patterns from these dark matters in the NANOGrav 15-year data set.","The best fit might suggest that the composition of non-adiabatic perfect fluid dark matter in our galaxy is much greater than that of ultralight scalar dark matter."],"url":"http://arxiv.org/abs/2404.02646v1","category":"gr-qc"}
{"created":"2024-04-03 11:14:37","title":"Leveraging Swarm Intelligence to Drive Autonomously: A Particle Swarm Optimization based Approach to Motion Planning","abstract":"Motion planning is an essential part of autonomous mobile platforms. A good pipeline should be modular enough to handle different vehicles, environments, and perception modules. The planning process has to cope with all the different modalities and has to have a modular and flexible design. But most importantly, it has to be safe and robust. In this paper, we want to present our motion planning pipeline with particle swarm optimization (PSO) at its core. This solution is independent of the vehicle type and has a clear and simple-to-implement interface for perception modules. Moreover, the approach stands out for being easily adaptable to new scenarios. Parallel calculation allows for fast planning cycles. Following the principles of PSO, the trajectory planer first generates a swarm of initial trajectories that are optimized afterward. We present the underlying control space and inner workings. Finally, the application to real-world automated driving is shown in the evaluation with a deeper look at the modeling of the cost function. The approach is used in our automated shuttles that have already driven more than 3.500 km safely and entirely autonomously in sub-urban everyday traffic.","sentences":["Motion planning is an essential part of autonomous mobile platforms.","A good pipeline should be modular enough to handle different vehicles, environments, and perception modules.","The planning process has to cope with all the different modalities and has to have a modular and flexible design.","But most importantly, it has to be safe and robust.","In this paper, we want to present our motion planning pipeline with particle swarm optimization (PSO) at its core.","This solution is independent of the vehicle type and has a clear and simple-to-implement interface for perception modules.","Moreover, the approach stands out for being easily adaptable to new scenarios.","Parallel calculation allows for fast planning cycles.","Following the principles of PSO, the trajectory planer first generates a swarm of initial trajectories that are optimized afterward.","We present the underlying control space and inner workings.","Finally, the application to real-world automated driving is shown in the evaluation with a deeper look at the modeling of the cost function.","The approach is used in our automated shuttles that have already driven more than 3.500 km safely and entirely autonomously in sub-urban everyday traffic."],"url":"http://arxiv.org/abs/2404.02644v1","category":"cs.RO"}
{"created":"2024-04-03 11:09:40","title":"Urban Scaling Laws","abstract":"Understanding how size influences the internal characteristics of a system is a crucial concern across various fields. Concepts like scale invariance, universalities, and fractals are fundamental to this inquiry and find application in biology, physics, and particularly urbanism. Size profoundly impacts how cities develop and function economically and socially. For example, what are the pros and cons of residing in larger cities? Is life really more expensive or less safe in larger cities? Or do they really offer more opportunities and generally higher incomes than smaller ones? To address such inquiries, we utilize theoretical tools from scaling theory, enabling a quantitative description of how a system's behavior changes across different scales, from micro to macro. Drawing parallels with research in biology and spatial economics, this chapter explores recent discoveries, ongoing progress, and unanswered questions regarding urban scaling.","sentences":["Understanding how size influences the internal characteristics of a system is a crucial concern across various fields.","Concepts like scale invariance, universalities, and fractals are fundamental to this inquiry and find application in biology, physics, and particularly urbanism.","Size profoundly impacts how cities develop and function economically and socially.","For example, what are the pros and cons of residing in larger cities?","Is life really more expensive or less safe in larger cities?","Or do they really offer more opportunities and generally higher incomes than smaller ones?","To address such inquiries, we utilize theoretical tools from scaling theory, enabling a quantitative description of how a system's behavior changes across different scales, from micro to macro.","Drawing parallels with research in biology and spatial economics, this chapter explores recent discoveries, ongoing progress, and unanswered questions regarding urban scaling."],"url":"http://arxiv.org/abs/2404.02642v1","category":"physics.soc-ph"}
{"created":"2024-04-03 10:54:07","title":"Vocabulary Attack to Hijack Large Language Model Applications","abstract":"The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single word insertion is sufficient. Second, we demonstrate that we can conduct our attack using a different model than the target model to conduct our attack with.","sentences":["The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications.","Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems.","They want the model to reveal confidential information, specific false information, or offensive behavior.","To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal.","Our approach is different.","It inserts words from the model vocabulary.","We find these words using an optimization procedure and embeddings from another LLM (attacker LLM).","We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively.","We present two main findings.","First, our approach creates inconspicuous instructions and therefore it is hard to detect.","For many attack cases, we find that even a single word insertion is sufficient.","Second, we demonstrate that we can conduct our attack using a different model than the target model to conduct our attack with."],"url":"http://arxiv.org/abs/2404.02637v1","category":"cs.CR"}
{"created":"2024-04-03 10:44:06","title":"3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization","abstract":"3D stylization, which entails the application of specific styles to three-dimensional objects, holds significant commercial potential as it enables the creation of diverse 3D objects with distinct moods and styles, tailored to specific demands of different scenes. With recent advancements in text-driven methods and artificial intelligence, the stylization process is increasingly intuitive and automated, thereby diminishing the reliance on manual labor and expertise. However, existing methods have predominantly focused on holistic stylization, thereby leaving the application of styles to individual components of a 3D object unexplored. In response, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization. Given a 3D mesh and a text prompt, 3DStyleGLIP leverages the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize the individual parts of the 3D mesh and modify their colors and local geometries to align them with the desired styles specified in the text prompt. 3DStyleGLIP is effectively trained for 3D stylization tasks through a part-level style loss working in GLIP's embedding space, supplemented by two complementary learning techniques. Extensive experimental validation confirms that our method achieves significant part-wise stylization capabilities, demonstrating promising potential in advancing the field of 3D stylization.","sentences":["3D stylization, which entails the application of specific styles to three-dimensional objects, holds significant commercial potential as it enables the creation of diverse 3D objects with distinct moods and styles, tailored to specific demands of different scenes.","With recent advancements in text-driven methods and artificial intelligence, the stylization process is increasingly intuitive and automated, thereby diminishing the reliance on manual labor and expertise.","However, existing methods have predominantly focused on holistic stylization, thereby leaving the application of styles to individual components of a 3D object unexplored.","In response, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization.","Given a 3D mesh and a text prompt, 3DStyleGLIP leverages the vision-language embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize the individual parts of the 3D mesh and modify their colors and local geometries to align them with the desired styles specified in the text prompt.","3DStyleGLIP is effectively trained for 3D stylization tasks through a part-level style loss working in GLIP's embedding space, supplemented by two complementary learning techniques.","Extensive experimental validation confirms that our method achieves significant part-wise stylization capabilities, demonstrating promising potential in advancing the field of 3D stylization."],"url":"http://arxiv.org/abs/2404.02634v1","category":"cs.CV"}
{"created":"2024-04-03 10:36:42","title":"Dynamical Casimir effect with screened scalar fields","abstract":"Understanding the nature of dark energy and dark matter is one of modern physics' greatest open problems. Scalar-tensor theories with screened scalar fields like the chameleon model are among the most popular proposed solutions. In this article, we present the first analysis of the impact of a chameleon field on the dynamical Casimir effect, whose main feature is the particle production associated with a resonant condition of boundary periodic motion in cavities. For this, we employ a recently developed method to compute the evolution of confined quantum scalar fields in a globally hyperbolic spacetime by means of time-dependent Bogoliubov transformations. As a result, we show that particle production is reduced due to the presence of the chameleon field. In addition, our results for the Bogoliubov coefficients and the mean number of created particles agree with known results in the absence of a chameleon field. Our results initiate the discussion of the evolution of quantum fields on screened scalar field backgrounds.","sentences":["Understanding the nature of dark energy and dark matter is one of modern physics' greatest open problems.","Scalar-tensor theories with screened scalar fields like the chameleon model are among the most popular proposed solutions.","In this article, we present the first analysis of the impact of a chameleon field on the dynamical Casimir effect, whose main feature is the particle production associated with a resonant condition of boundary periodic motion in cavities.","For this, we employ a recently developed method to compute the evolution of confined quantum scalar fields in a globally hyperbolic spacetime by means of time-dependent Bogoliubov transformations.","As a result, we show that particle production is reduced due to the presence of the chameleon field.","In addition, our results for the Bogoliubov coefficients and the mean number of created particles agree with known results in the absence of a chameleon field.","Our results initiate the discussion of the evolution of quantum fields on screened scalar field backgrounds."],"url":"http://arxiv.org/abs/2404.02630v1","category":"gr-qc"}
{"created":"2024-04-03 10:29:06","title":"A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference","abstract":"Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders. Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent NLI in complex domains.","sentences":["Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI).","However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning.","In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS).","Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation.","Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders.","Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent NLI in complex domains."],"url":"http://arxiv.org/abs/2404.02625v1","category":"cs.CL"}
{"created":"2024-04-03 10:11:22","title":"Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models","abstract":"We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability. DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization methods, and 2) the automated identification of biases and spurious features.","sentences":["We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability.","DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions.","Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention.","The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text.","We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization methods, and 2) the automated identification of biases and spurious features."],"url":"http://arxiv.org/abs/2404.02618v1","category":"cs.CV"}
{"created":"2024-04-03 10:05:47","title":"Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation","abstract":"Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training data, which can construct new query-document pairs as training data. Extensive offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling.","sentences":["Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement.","In most social search scenarios such as Dianping, modeling search relevance always faces two challenges.","One is that many documents in social search are very long and have much redundant information.","The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model.","To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document.","Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training data, which can construct new query-document pairs as training data.","Extensive offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling."],"url":"http://arxiv.org/abs/2404.02616v1","category":"cs.IR"}
{"created":"2024-04-03 10:01:37","title":"Structures associated with the Borromean rings complement in the Poincar\u00e9 ball","abstract":"Guided by physical needs, we deal with the rotationally isotropic Poincar\\'e ball, when considering the complement of Borromean rings embedded in it. We consistently describe the geometry of the complement and realize the fundamental group as isometry subgroup in three dimensions. Applying this realization, we reveal normal stochastization and multifractal behavior within the examined model of directed random walks on the rooted Cayley tree, whose graphs are associated with dendritic polymers. According to Penner, we construct the Teichm\\\"uller space of the decorated ideal octahedral surface related to the quotient space of the fundamental group action. Using the conformality of decoration, we define six moduli and the mapping class group generated by cyclic permutations of the ideal vertices. Intending to quantize the geometric area, we state the connection between the induced geometry and the sine-Gordon model. Due to such a correspondence we obtain the differential two-form in the cotangent bundle.","sentences":["Guided by physical needs, we deal with the rotationally isotropic Poincar\\'e ball, when considering the complement of Borromean rings embedded in it.","We consistently describe the geometry of the complement and realize the fundamental group as isometry subgroup in three dimensions.","Applying this realization, we reveal normal stochastization and multifractal behavior within the examined model of directed random walks on the rooted Cayley tree, whose graphs are associated with dendritic polymers.","According to Penner, we construct the Teichm\\\"uller space of the decorated ideal octahedral surface related to the quotient space of the fundamental group action.","Using the conformality of decoration, we define six moduli and the mapping class group generated by cyclic permutations of the ideal vertices.","Intending to quantize the geometric area, we state the connection between the induced geometry and the sine-Gordon model.","Due to such a correspondence we obtain the differential two-form in the cotangent bundle."],"url":"http://arxiv.org/abs/2404.02615v1","category":"math-ph"}
{"created":"2024-04-03 10:01:23","title":"Vestibular schwannoma growth_prediction from longitudinal MRI by time conditioned neural fields","abstract":"Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination. To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable. In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and recurrent neural networks for prospective tumor growth prediction. In the proposed method, each tumor is represented as a signed distance function (SDF) conditioned on a low-dimensional latent code. Unlike previous studies that perform tumor shape prediction directly in the image space, we predict the latent codes instead and then reconstruct future shapes from it. To deal with irregular time intervals, we introduce a time-conditioned recurrent module based on a ConvLSTM and a novel temporal encoding strategy, which enables the proposed model to output varying tumor shapes over time. The experiments on an in-house longitudinal VS dataset showed that the proposed model significantly improved the performance ($\\ge 1.6\\%$ Dice score and $\\ge0.20$ mm 95\\% Hausdorff distance), in particular for top 20\\% tumors that grow or shrink the most ($\\ge 4.6\\%$ Dice score and $\\ge 0.73$ mm 95\\% Hausdorff distance). Our code is available at ~\\burl{https://github.com/cyjdswx/DeepGrowth}","sentences":["Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination.","To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable.","In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and recurrent neural networks for prospective tumor growth prediction.","In the proposed method, each tumor is represented as a signed distance function (SDF) conditioned on a low-dimensional latent code.","Unlike previous studies that perform tumor shape prediction directly in the image space, we predict the latent codes instead and then reconstruct future shapes from it.","To deal with irregular time intervals, we introduce a time-conditioned recurrent module based on a ConvLSTM and a novel temporal encoding strategy, which enables the proposed model to output varying tumor shapes over time.","The experiments on an in-house longitudinal VS dataset showed that the proposed model significantly improved the performance ($\\ge 1.6\\%$ Dice score and $\\ge0.20$ mm 95\\% Hausdorff distance), in particular for top 20\\% tumors that grow or shrink the most ($\\ge 4.6\\%$ Dice score and $\\ge 0.73$ mm 95\\% Hausdorff distance).","Our code is available at ~\\burl{https://github.com/cyjdswx/DeepGrowth}"],"url":"http://arxiv.org/abs/2404.02614v1","category":"eess.IV"}
{"created":"2024-04-03 09:56:49","title":"Exploring Sustainable Clothing Consumption in Middle-Income Countries: A case study of Romanian consumers","abstract":"The overconsumption of consumers under today's increasingly scarce natural resources has overwhelmed the textile industry in middle-income countries, such as Romania. It is becoming more and more essential to encourage sustainable clothing consumption behaviors, such as purchasing recyclable clothes. Notwithstanding there is a limited number of studies trying to understand the intrinsic factors that motivate consumers' purchase intention toward sustainable clothes in middle-income countries. Moreover, the effect of consumers' environmental knowledge on determining their purchase intention of sustainable clothes remains understudied. Consequently, the purpose of this paper is to make a significant contribution to the sustainable consumption literature by providing a consolidated framework that explores the behavioral factors inclining Romanian consumers' purchase intention towards sustainable clothes. The foundation of this study combines consumers' social value orientation and the theory of planned behavior. the partial least square path modelling procedure was used to analyze the data of 1,018 Romanian respondents. The findings of this study show that altruistic value orientation, subjective norms, and sustainable attitudes have a positive effect on Romanian consumers' purchase intention of sustainable clothing. Thus, these insights provide essential practical implications of advocating for the consumption of sustainable clothes along with useful guidelines for practitioners in the textile industry among middle-income countries, especially in Romania, to reduce overconsumption.","sentences":["The overconsumption of consumers under today's increasingly scarce natural resources has overwhelmed the textile industry in middle-income countries, such as Romania.","It is becoming more and more essential to encourage sustainable clothing consumption behaviors, such as purchasing recyclable clothes.","Notwithstanding there is a limited number of studies trying to understand the intrinsic factors that motivate consumers' purchase intention toward sustainable clothes in middle-income countries.","Moreover, the effect of consumers' environmental knowledge on determining their purchase intention of sustainable clothes remains understudied.","Consequently, the purpose of this paper is to make a significant contribution to the sustainable consumption literature by providing a consolidated framework that explores the behavioral factors inclining Romanian consumers' purchase intention towards sustainable clothes.","The foundation of this study combines consumers' social value orientation and the theory of planned behavior.","the partial least square path modelling procedure was used to analyze the data of 1,018 Romanian respondents.","The findings of this study show that altruistic value orientation, subjective norms, and sustainable attitudes have a positive effect on Romanian consumers' purchase intention of sustainable clothing.","Thus, these insights provide essential practical implications of advocating for the consumption of sustainable clothes along with useful guidelines for practitioners in the textile industry among middle-income countries, especially in Romania, to reduce overconsumption."],"url":"http://arxiv.org/abs/2404.02612v1","category":"econ.GN"}
{"created":"2024-04-03 09:56:38","title":"SHIELD: A regularization technique for eXplainable Artificial Intelligence","abstract":"As Artificial Intelligence systems become integral across domains, the demand for explainability grows. While the effort by the scientific community is focused on obtaining a better explanation for the model, it is important not to ignore the potential of this explanation process to improve training as well. While existing efforts primarily focus on generating and evaluating explanations for black-box models, there remains a critical gap in directly enhancing models through these evaluations. This paper introduces SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a regularization technique for explainable artificial intelligence designed to improve model quality by concealing portions of input data and assessing the resulting discrepancy in predictions. In contrast to conventional approaches, SHIELD regularization seamlessly integrates into the objective function, enhancing model explainability while also improving performance. Experimental validation on benchmark datasets underscores SHIELD's effectiveness in improving Artificial Intelligence model explainability and overall performance. This establishes SHIELD regularization as a promising pathway for developing transparent and reliable Artificial Intelligence regularization techniques.","sentences":["As Artificial Intelligence systems become integral across domains, the demand for explainability grows.","While the effort by the scientific community is focused on obtaining a better explanation for the model, it is important not to ignore the potential of this explanation process to improve training as well.","While existing efforts primarily focus on generating and evaluating explanations for black-box models, there remains a critical gap in directly enhancing models through these evaluations.","This paper introduces SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a regularization technique for explainable artificial intelligence designed to improve model quality by concealing portions of input data and assessing the resulting discrepancy in predictions.","In contrast to conventional approaches, SHIELD regularization seamlessly integrates into the objective function, enhancing model explainability while also improving performance.","Experimental validation on benchmark datasets underscores SHIELD's effectiveness in improving Artificial Intelligence model explainability and overall performance.","This establishes SHIELD regularization as a promising pathway for developing transparent and reliable Artificial Intelligence regularization techniques."],"url":"http://arxiv.org/abs/2404.02611v1","category":"cs.AI"}
{"created":"2024-04-03 09:56:18","title":"The Set-Theoretic Form of Twin Prime Distribution and Its Odd-Even Imbalance","abstract":"The prime number problem falls within the realm of number theory, specifically elementary number theory. Current research approaches have unnecessarily complicated this matter. In contrast to more advanced mathematical tools, the methods of elementary number theory can effectively address the twin prime problem. The primary contribution of this article lies in establishing a set that systematically includes all twin prime pairs without omissions. This set's distribution is governed by a precise general solution formula. Furthermore, an analysis of the distribution set reveals characteristics of parity balance, enabling us to determine whether twin prime pairs are finite. Finally, our method can be extended to the general case of twin primes, such as the Polignac's conjecture.","sentences":["The prime number problem falls within the realm of number theory, specifically elementary number theory.","Current research approaches have unnecessarily complicated this matter.","In contrast to more advanced mathematical tools, the methods of elementary number theory can effectively address the twin prime problem.","The primary contribution of this article lies in establishing a set that systematically includes all twin prime pairs without omissions.","This set's distribution is governed by a precise general solution formula.","Furthermore, an analysis of the distribution set reveals characteristics of parity balance, enabling us to determine whether twin prime pairs are finite.","Finally, our method can be extended to the general case of twin primes, such as the Polignac's conjecture."],"url":"http://arxiv.org/abs/2404.02610v1","category":"math.GM"}
{"created":"2024-04-03 09:46:47","title":"A rare simultaneous detection of a mid-latitude plasma depleted structure in O($^1$D) 630.0 nm and O($^1$S) 557.7 nm all-sky airglow images on a geomagnetically quiet night","abstract":"In general, nighttime thermospheric 557.7 nm emission over mid-latitudes is predominantly masked by significantly larger mesospheric component, and hence, F-region plasma structures are rarely observed in this emission. This paper reports the first rare simultaneous detection of F-region plasma depleted structure in O($^1$D) 630.0 nm and O($^1$S) 557.7 nm airglow images from Hanle, India, a mid-latitude station (32.7{\\deg}N, 78.9{\\deg}E; Mlat. ~24.1{\\deg}N) on a geomagnetically quiet night (Ap=3) of 26 June 2021. This indicates significant enhancement of thermospheric 557.7 nm emission. Interestingly, thermospheric 557.7 nm emission was not significant on the following geomagnetically quiet night as MSTID bands were only observed in 630.0 nm images. We show that enhanced dissociative recombination caused by descent of F-layer peak over the observation region coupled with the significant increase of the electron density at thermospheric 557.7 nm emission altitude enabled the detection of the plasma depleted structure on 26 June 2021.","sentences":["In general, nighttime thermospheric 557.7 nm emission over mid-latitudes is predominantly masked by significantly larger mesospheric component, and hence, F-region plasma structures are rarely observed in this emission.","This paper reports the first rare simultaneous detection of F-region plasma depleted structure in O($^1$D) 630.0 nm and O($^1$S) 557.7 nm airglow images from Hanle, India, a mid-latitude station (32.7{\\deg}N, 78.9{\\deg}E; Mlat. ~24.1{\\deg}N) on a geomagnetically quiet night (Ap=3) of 26 June 2021.","This indicates significant enhancement of thermospheric 557.7 nm emission.","Interestingly, thermospheric 557.7 nm emission was not significant on the following geomagnetically quiet night as MSTID bands were only observed in 630.0 nm images.","We show that enhanced dissociative recombination caused by descent of F-layer peak over the observation region coupled with the significant increase of the electron density at thermospheric 557.7 nm emission altitude enabled the detection of the plasma depleted structure on 26 June 2021."],"url":"http://arxiv.org/abs/2404.02603v1","category":"physics.space-ph"}
{"created":"2024-04-03 09:33:47","title":"Determining the Tactical Challenge of Scenarios to Efficiently Test Automated Driving Systems","abstract":"The selection of relevant test scenarios for the scenario-based testing and safety validation of automated driving systems (ADSs) remains challenging. An important aspect of the relevance of a scenario is the challenge it poses for an ADS. Existing methods for calculating the challenge of a scenario aim to express the challenge in terms of a metric value. Metric values are useful to select the least or most challenging scenario. However, they fail to provide human-interpretable information on the cause of the challenge which is critical information for the efficient selection of relevant test scenarios. Therefore, this paper presents the Challenge Description Method that mitigates this issue by analyzing scenarios and providing a description of their challenge in terms of the minimum required lane changes and their difficulty. Applying the method to different highway scenarios showed that it is capable of analyzing complex scenarios and providing easy-to-understand descriptions that can be used to select relevant test scenarios.","sentences":["The selection of relevant test scenarios for the scenario-based testing and safety validation of automated driving systems (ADSs) remains challenging.","An important aspect of the relevance of a scenario is the challenge it poses for an ADS.","Existing methods for calculating the challenge of a scenario aim to express the challenge in terms of a metric value.","Metric values are useful to select the least or most challenging scenario.","However, they fail to provide human-interpretable information on the cause of the challenge which is critical information for the efficient selection of relevant test scenarios.","Therefore, this paper presents the Challenge Description Method that mitigates this issue by analyzing scenarios and providing a description of their challenge in terms of the minimum required lane changes and their difficulty.","Applying the method to different highway scenarios showed that it is capable of analyzing complex scenarios and providing easy-to-understand descriptions that can be used to select relevant test scenarios."],"url":"http://arxiv.org/abs/2404.02599v1","category":"cs.SE"}
{"created":"2024-04-03 09:27:57","title":"Combining transition path sampling with data-driven collective variables through a reactivity-biased shooting algorithm","abstract":"Rare event sampling is a central problem in modern computational chemistry research. Among the existing methods, transition path sampling (TPS) can generate unbiased representations of reaction processes. However, its efficiency depends on the ability to generate reactive trial paths, which in turn depends on the quality of the shooting algorithm used. We propose a new algorithm based on the shooting success rate, i.e. reactivity, measured as a function of a reduced set of collective variables (CVs). These variables are extracted with a machine learning approach directly from TPS simulations, using a multi-task objective function. Iteratively, this workflow significantly improves shooting efficiency without any prior knowledge of the process. In addition, the optimized CVs can be used with biased enhanced sampling methodologies to accurately reconstruct the free energy profiles. We tested the method on three different systems: a two-dimensional toy model, conformational transitions of alanine dipeptide, and hydrolysis of acetyl chloride in bulk water. In the latter, we integrated our workflow with an active learning scheme to learn a reactive machine learning-based potential, which allowed us to study the mechanism and free energy profile with an ab initio-like accuracy.","sentences":["Rare event sampling is a central problem in modern computational chemistry research.","Among the existing methods, transition path sampling (TPS) can generate unbiased representations of reaction processes.","However, its efficiency depends on the ability to generate reactive trial paths, which in turn depends on the quality of the shooting algorithm used.","We propose a new algorithm based on the shooting success rate, i.e. reactivity, measured as a function of a reduced set of collective variables (CVs).","These variables are extracted with a machine learning approach directly from TPS simulations, using a multi-task objective function.","Iteratively, this workflow significantly improves shooting efficiency without any prior knowledge of the process.","In addition, the optimized CVs can be used with biased enhanced sampling methodologies to accurately reconstruct the free energy profiles.","We tested the method on three different systems: a two-dimensional toy model, conformational transitions of alanine dipeptide, and hydrolysis of acetyl chloride in bulk water.","In the latter, we integrated our workflow with an active learning scheme to learn a reactive machine learning-based potential, which allowed us to study the mechanism and free energy profile with an ab initio-like accuracy."],"url":"http://arxiv.org/abs/2404.02597v1","category":"physics.comp-ph"}
{"created":"2024-04-03 09:27:12","title":"Stabilizing switched nonlinear systems under restricted but arbitrary switching signals","abstract":"This paper deals with input/output-to-state stability (IOSS) of switched nonlinear systems whose switching signals obey pre-specified restrictions on admissible switches between the subsystems and admissible dwell times on the subsystems. We present sufficient conditions on the subsystems, admissible switches between them and admissible dwell times on them, such that a switched system generated under all switching signals obeying the given restrictions is IOSS. Multiple Lyapunov-like functions and graph theory are the key apparatuses for our analysis. A numerical example is presented to demonstrate our results.","sentences":["This paper deals with input/output-to-state stability (IOSS) of switched nonlinear systems whose switching signals obey pre-specified restrictions on admissible switches between the subsystems and admissible dwell times on the subsystems.","We present sufficient conditions on the subsystems, admissible switches between them and admissible dwell times on them, such that a switched system generated under all switching signals obeying the given restrictions is IOSS.","Multiple Lyapunov-like functions and graph theory are the key apparatuses for our analysis.","A numerical example is presented to demonstrate our results."],"url":"http://arxiv.org/abs/2404.02596v1","category":"eess.SY"}
{"created":"2024-04-03 09:17:38","title":"Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation","abstract":"Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech. Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English. Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness. In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns. Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips. Architectural design choices are validated through comparisons with baseline models and ablation studies using subjective and objective metrics, thus confirming model performance.","sentences":["Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech.","Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English.","Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness.","In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns.","Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips.","Architectural design choices are validated through comparisons with baseline models and ablation studies using subjective and objective metrics, thus confirming model performance."],"url":"http://arxiv.org/abs/2404.02592v1","category":"cs.CL"}
{"created":"2024-04-03 09:15:38","title":"Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect","abstract":"The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning. The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation. Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B). We formally demonstrate that the negativity bias remains in this set-up. We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative.","sentences":["The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning.","The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation.","Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B).","We formally demonstrate that the negativity bias remains in this set-up.","We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative."],"url":"http://arxiv.org/abs/2404.02591v1","category":"cs.LG"}
{"created":"2024-04-03 09:15:29","title":"Condensation in zero-range processes with a fast rate","abstract":"We introduce a simple zero-range process with constant rates and one fast rate for a particular occupation number, which diverges with the system size. Surprisingly, this minor modification induces a condensation transition in the thermodynamic limit, where the structure of the condensed phase depends on the scaling of the fast rate. We study this transition and its dependence on system parameters in detail on a rigorous level using size-biased sampling. This approach generalizes to any particle system with stationary product measures, and the techniques used in this paper provide a foundation for a more systematic understanding of condensing models with a non-trivial condensed phase.","sentences":["We introduce a simple zero-range process with constant rates and one fast rate for a particular occupation number, which diverges with the system size.","Surprisingly, this minor modification induces a condensation transition in the thermodynamic limit, where the structure of the condensed phase depends on the scaling of the fast rate.","We study this transition and its dependence on system parameters in detail on a rigorous level using size-biased sampling.","This approach generalizes to any particle system with stationary product measures, and the techniques used in this paper provide a foundation for a more systematic understanding of condensing models with a non-trivial condensed phase."],"url":"http://arxiv.org/abs/2404.02590v1","category":"math.PR"}
{"created":"2024-04-03 09:14:24","title":"Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation","abstract":"Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we fine-tuned a pre-trained language model specifically for emotion recognition in conversations, facilitating real-time affective annotations for utterances. For interpretability of recognition results, we formulate personality recognition as an NLI problem by determining whether the textual description of personality labels is entailed by the dialog content. Extensive experiments on two daily conversation datasets suggest that Affective-NLI significantly outperforms (by 6%-7%) state-of-the-art approaches. Additionally, our Flow experiment demonstrates that Affective-NLI can accurately recognize the speaker's personality in the early stages of conversations by surpassing state-of-the-art methods with 22%-34%.","sentences":["Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content.","It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly.","Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance.","First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored.","Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results.","In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC.","To utilize affectivity within dialog content for accurate personality recognition, we fine-tuned a pre-trained language model specifically for emotion recognition in conversations, facilitating real-time affective annotations for utterances.","For interpretability of recognition results, we formulate personality recognition as an NLI problem by determining whether the textual description of personality labels is entailed by the dialog content.","Extensive experiments on two daily conversation datasets suggest that Affective-NLI significantly outperforms (by 6%-7%) state-of-the-art approaches.","Additionally, our Flow experiment demonstrates that Affective-NLI can accurately recognize the speaker's personality in the early stages of conversations by surpassing state-of-the-art methods with 22%-34%."],"url":"http://arxiv.org/abs/2404.02589v1","category":"cs.CL"}
{"created":"2024-04-03 09:12:22","title":"The Surprising Effectiveness of Rankers Trained on Expanded Queries","abstract":"An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do LLM based query enrichment for training queries using relevant documents. Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a principled query performance based scoring method using base and specialized ranker offers a significant improvement of up to 25% on the passage ranking task and up to 48.4% on the document ranking task when compared to the baseline performance of using original queries, even outperforming SOTA model.","sentences":["An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution.","The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries.","In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries.","Firstly, we do LLM based query enrichment for training queries using relevant documents.","Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries.","We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query.","Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution.","In our extensive experiments on the DL-Hard dataset, we find that a principled query performance based scoring method using base and specialized ranker offers a significant improvement of up to 25% on the passage ranking task and up to 48.4% on the document ranking task when compared to the baseline performance of using original queries, even outperforming SOTA model."],"url":"http://arxiv.org/abs/2404.02587v1","category":"cs.IR"}
{"created":"2024-04-03 09:08:15","title":"Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization","abstract":"Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It efficiently generates a piecewise linear approximation for the value function, significantly reducing computation time while preserving solution quality, thus marking a promising progression in the treatment of large-scale multistage stochastic programming problems.","sentences":["Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase.","Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems.","Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm.","This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function.","Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems.","It efficiently generates a piecewise linear approximation for the value function, significantly reducing computation time while preserving solution quality, thus marking a promising progression in the treatment of large-scale multistage stochastic programming problems."],"url":"http://arxiv.org/abs/2404.02583v1","category":"cs.LG"}
{"created":"2024-04-03 08:56:00","title":"Multi-Granularity Guided Fusion-in-Decoder","abstract":"In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, i.e., Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an anchor vector that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for passage pruning. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of its multi-granularity solution.","sentences":["In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results.","The model architecture that uses concatenated multiple contexts in the decoding phase, i.e., Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts.","To address this problem, we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning evidence across multiple levels of granularity.","Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification.","It aggregates evident sentences into an anchor vector that instructs the decoder.","Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for passage pruning.","Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of its multi-granularity solution."],"url":"http://arxiv.org/abs/2404.02581v1","category":"cs.CL"}
{"created":"2024-04-03 08:55:44","title":"Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation","abstract":"Optimizing deep learning models requires large amounts of annotated images, a process that is both time-intensive and costly. Especially for semantic segmentation models in which every pixel must be annotated. A potential strategy to mitigate annotation effort is active learning. Active learning facilitates the identification and selection of the most informative images from a large unlabelled pool. The underlying premise is that these selected images can improve the model's performance faster than random selection to reduce annotation effort. While active learning has demonstrated promising results on benchmark datasets like Cityscapes, its performance in the agricultural domain remains largely unexplored. This study addresses this research gap by conducting a comparative study of three active learning-based acquisition functions: Bayesian Active Learning by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random. The acquisition functions were tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing three semantic classes: background, crop and weed. Our results indicated that active learning, especially PowerBALD, yields a higher performance than Random sampling on both datasets. But due to the relatively large standard deviations, the differences observed were minimal; this was partly caused by high image redundancy and imbalanced classes. Specifically, more than 89\\% of the pixels belonged to the background class on both datasets. The absence of significant results on both datasets indicates that further research is required for applying active learning on agricultural datasets, especially if they contain a high-class imbalance and redundant images. Recommendations and insights are provided in this paper to potentially resolve such issues.","sentences":["Optimizing deep learning models requires large amounts of annotated images, a process that is both time-intensive and costly.","Especially for semantic segmentation models in which every pixel must be annotated.","A potential strategy to mitigate annotation effort is active learning.","Active learning facilitates the identification and selection of the most informative images from a large unlabelled pool.","The underlying premise is that these selected images can improve the model's performance faster than random selection to reduce annotation effort.","While active learning has demonstrated promising results on benchmark datasets like Cityscapes, its performance in the agricultural domain remains largely unexplored.","This study addresses this research gap by conducting a comparative study of three active learning-based acquisition functions: Bayesian Active Learning by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random.","The acquisition functions were tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing three semantic classes: background, crop and weed.","Our results indicated that active learning, especially PowerBALD, yields a higher performance than Random sampling on both datasets.","But due to the relatively large standard deviations, the differences observed were minimal; this was partly caused by high image redundancy and imbalanced classes.","Specifically, more than 89\\% of the pixels belonged to the background class on both datasets.","The absence of significant results on both datasets indicates that further research is required for applying active learning on agricultural datasets, especially if they contain a high-class imbalance and redundant images.","Recommendations and insights are provided in this paper to potentially resolve such issues."],"url":"http://arxiv.org/abs/2404.02580v1","category":"cs.CV"}
{"created":"2024-04-03 08:54:58","title":"Learning Alternative Ways of Performing a Task","abstract":"A common way of learning to perform a task is to observe how it is carried out by experts. However, it is well known that for most tasks there is no unique way to perform them. This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task. In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task). Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data. Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strategy of performing a task. By an iterative process based on generalisation and specialisation, we learn the underlying patterns that capture the different styles of performing a task exhibited by the examples. We illustrate our approach on two common activity recognition tasks: a surgical skills training task and a cooking domain. We evaluate the inferred models with respect to two metrics that measure how well the models represent the examples and capture the different forms of executing a task showed by the examples. We compare our results with the traditional process mining approach and show that a small set of meaningful examples is enough to obtain patterns that capture the different strategies that are followed to solve the tasks.","sentences":["A common way of learning to perform a task is to observe how it is carried out by experts.","However, it is well known that for most tasks there is no unique way to perform them.","This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task.","In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task).","Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data.","Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strategy of performing a task.","By an iterative process based on generalisation and specialisation, we learn the underlying patterns that capture the different styles of performing a task exhibited by the examples.","We illustrate our approach on two common activity recognition tasks: a surgical skills training task and a cooking domain.","We evaluate the inferred models with respect to two metrics that measure how well the models represent the examples and capture the different forms of executing a task showed by the examples.","We compare our results with the traditional process mining approach and show that a small set of meaningful examples is enough to obtain patterns that capture the different strategies that are followed to solve the tasks."],"url":"http://arxiv.org/abs/2404.02579v1","category":"cs.AI"}
{"created":"2024-04-03 08:54:42","title":"SpectroTranslator: a deep-neural network algorithm to homogenize spectroscopic parameters","abstract":"The emergence of large spectroscopic surveys requires homogenising on the same scale the quantities they measure in order to increase their scientific legacy. We developed the SpectroTranslator, a data-driven deep neural network algorithm that can convert spectroscopic parameters from the base of one survey to another. The algorithm also includes a method to estimate the importance that the various parameters play in the conversion from base A to B. As a showcase, we apply the algorithm to transform effective temperature, surface gravity, metallicity, [Mg/Fe] and los velocity from the base of GALAH into the APOGEE base. We demonstrate the efficiency of the SpectroTranslator algorithm to translate the spectroscopic parameters from one base to another using parameters directly by the survey teams, and are able to achieve a similar performance than previous works that have performed a similar type of conversion but using the full spectrum rather than the spectroscopic parameters, allowing to reduce the computational time, and to use the output of pipelines optimized for each survey. By combining the transformed GALAH catalogue with the APOGEE catalogue, we study the distribution of [Fe/H] and [Mg/Fe] across the Galaxy, and we find that the median distribution of both quantities present a vertical asymmetry at large radii. We attribute it to the recent perturbations generated by the passage of a dwarf galaxy across the disc or by the infall of the Large Magellanic Cloud. Although several aspects still need to be refined, in particular how to deal in an optimal manner with regions of the parameter space meagrely populated by stars in the training sample, the SpectroTranslator already shows its capability and promises to play a crucial role in standardizing various spectroscopic surveys onto a unified basis.","sentences":["The emergence of large spectroscopic surveys requires homogenising on the same scale the quantities they measure in order to increase their scientific legacy.","We developed the SpectroTranslator, a data-driven deep neural network algorithm that can convert spectroscopic parameters from the base of one survey to another.","The algorithm also includes a method to estimate the importance that the various parameters play in the conversion from base A to B. As a showcase, we apply the algorithm to transform effective temperature, surface gravity, metallicity, [Mg/Fe] and los velocity from the base of GALAH into the APOGEE base.","We demonstrate the efficiency of the SpectroTranslator algorithm to translate the spectroscopic parameters from one base to another using parameters directly by the survey teams, and are able to achieve a similar performance than previous works that have performed a similar type of conversion but using the full spectrum rather than the spectroscopic parameters, allowing to reduce the computational time, and to use the output of pipelines optimized for each survey.","By combining the transformed GALAH catalogue with the APOGEE catalogue, we study the distribution of [Fe/H] and [Mg/Fe] across the Galaxy, and we find that the median distribution of both quantities present a vertical asymmetry at large radii.","We attribute it to the recent perturbations generated by the passage of a dwarf galaxy across the disc or by the infall of the Large Magellanic Cloud.","Although several aspects still need to be refined, in particular how to deal in an optimal manner with regions of the parameter space meagrely populated by stars in the training sample, the SpectroTranslator already shows its capability and promises to play a crucial role in standardizing various spectroscopic surveys onto a unified basis."],"url":"http://arxiv.org/abs/2404.02578v1","category":"astro-ph.GA"}
{"created":"2024-04-03 08:49:11","title":"Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models","abstract":"Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the reasoning process of language models into two steps. (1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code. With extensive experiments on seven algorithmic reasoning tasks, we demonstrate the effectiveness of Think-and-Execute. Our approach better improves LMs' reasoning compared to several strong baselines performing instance-specific reasoning (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic. Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.","sentences":["Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution.","Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks.","Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes.","However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call.","Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve.","This paper presents Think-and-Execute, a novel framework that decomposes the reasoning process of language models into two steps.","(1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code.","With extensive experiments on seven algorithmic reasoning tasks, we demonstrate the effectiveness of Think-and-Execute.","Our approach better improves LMs' reasoning compared to several strong baselines performing instance-specific reasoning (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic.","Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions."],"url":"http://arxiv.org/abs/2404.02575v1","category":"cs.CL"}
{"created":"2024-04-03 08:47:32","title":"Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification","abstract":"Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time. Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks. Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making. This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time. The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings. Additionally, it incorporates a loss-based concept drift detection mechanism to recalculate graph prototypes when drift is detected.","sentences":["Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time.","Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks.","Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making.","This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time.","The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings.","Additionally, it incorporates a loss-based concept drift detection mechanism to recalculate graph prototypes when drift is detected."],"url":"http://arxiv.org/abs/2404.02572v1","category":"cs.LG"}
{"created":"2024-04-03 08:42:36","title":"SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing","abstract":"Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation. Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot.","sentences":["Cooking robots can enhance the home experience by reducing the burden of daily chores.","However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives.","This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks.","More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control.","Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board.","However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste.","Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation.","Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot."],"url":"http://arxiv.org/abs/2404.02569v1","category":"cs.RO"}
{"created":"2024-04-03 08:41:45","title":"On Future Power Systems Digital Twins: Towards a Standard Architecture","abstract":"The energy sector's digital transformation brings mutually dependent communication and energy infrastructure, tightening the relationship between the physical and the digital world. Digital twins (DT) are the key concept for this. This paper initially discusses the evolution of the DT concept across various engineering applications before narrowing its focus to the power systems domain. By reviewing different definitions and applications, we present a new definition of DTs specifically tailored to power systems. Based on the proposed definition and extensive deliberations and consultations with distribution system operators, energy traders, and municipalities, we introduce a standard DT ecosystem architecture that offers services beyond real-time updates and can seamlessly integrate with existing transmission and distribution system operators' processes, while reconciling with concepts such as microgrids and local energy communities based on a system-of-systems view. We also discuss the integration of power system DTs into various phases of the system's life cycle, such as long-term planning, emphasizing challenges that remain to be addressed, such as managing measurement and model errors, and uncertainty propagation. Finally, we present our vision of how artificial intelligence (AI) and machine learning (ML) can enhance several power system DT modules established in the proposed architecture.","sentences":["The energy sector's digital transformation brings mutually dependent communication and energy infrastructure, tightening the relationship between the physical and the digital world.","Digital twins (DT) are the key concept for this.","This paper initially discusses the evolution of the DT concept across various engineering applications before narrowing its focus to the power systems domain.","By reviewing different definitions and applications, we present a new definition of DTs specifically tailored to power systems.","Based on the proposed definition and extensive deliberations and consultations with distribution system operators, energy traders, and municipalities, we introduce a standard DT ecosystem architecture that offers services beyond real-time updates and can seamlessly integrate with existing transmission and distribution system operators' processes, while reconciling with concepts such as microgrids and local energy communities based on a system-of-systems view.","We also discuss the integration of power system DTs into various phases of the system's life cycle, such as long-term planning, emphasizing challenges that remain to be addressed, such as managing measurement and model errors, and uncertainty propagation.","Finally, we present our vision of how artificial intelligence (AI) and machine learning (ML) can enhance several power system DT modules established in the proposed architecture."],"url":"http://arxiv.org/abs/2404.02568v1","category":"eess.SY"}
{"created":"2024-04-03 08:29:44","title":"scenario.center: Methods from Real-world Data to a Scenario Database","abstract":"Scenario-based testing is a promising method to develop, verify and validate automated driving systems (ADS) since pure on-road testing seems inefficient for complex traffic environments. A major challenge for this approach is the provision and management of a sufficient number of scenarios to test a system. The provision, generation, and management of scenario at scale is investigated in current research. This paper presents the scenario database scenario.center ( https://scenario.center ) to process and manage scenario data covering the needs of scenario-based testing approaches comprehensively and automatically. Thereby, requirements for such databases are described. Based on those, a four-step approach is proposed. Firstly, a common input format with defined quality requirements is defined. This is utilized for detecting events and base scenarios automatically. Furthermore, methods for searchability, evaluation of data quality and different scenario generation methods are proposed to allow a broad applicability serving different needs. For evaluation, the methodology is compared to state-of-the-art scenario databases. Finally, the application and capabilities of the database are shown by applying the methodology to the inD dataset. A public demonstration of the database interface is provided at https://scenario.center .","sentences":["Scenario-based testing is a promising method to develop, verify and validate automated driving systems (ADS) since pure on-road testing seems inefficient for complex traffic environments.","A major challenge for this approach is the provision and management of a sufficient number of scenarios to test a system.","The provision, generation, and management of scenario at scale is investigated in current research.","This paper presents the scenario database scenario.center ( https://scenario.center ) to process and manage scenario data covering the needs of scenario-based testing approaches comprehensively and automatically.","Thereby, requirements for such databases are described.","Based on those, a four-step approach is proposed.","Firstly, a common input format with defined quality requirements is defined.","This is utilized for detecting events and base scenarios automatically.","Furthermore, methods for searchability, evaluation of data quality and different scenario generation methods are proposed to allow a broad applicability serving different needs.","For evaluation, the methodology is compared to state-of-the-art scenario databases.","Finally, the application and capabilities of the database are shown by applying the methodology to the inD dataset.","A public demonstration of the database interface is provided at https://scenario.center ."],"url":"http://arxiv.org/abs/2404.02561v1","category":"cs.SE"}
{"created":"2024-04-03 08:27:24","title":"Regional biases in image geolocation estimation: a case study with the SenseCity Africa dataset","abstract":"Advances in Artificial Intelligence are challenged by the biases rooted in the datasets used to train the models. In image geolocation estimation, models are mostly trained using data from specific geographic regions, notably the Western world, and as a result, they may struggle to comprehend the complexities of underrepresented regions. To assess this issue, we apply a state-of-the-art image geolocation estimation model (ISNs) to a crowd-sourced dataset of geolocated images from the African continent (SCA100), and then explore the regional and socioeconomic biases underlying the model's predictions. Our findings show that the ISNs model tends to over-predict image locations in high-income countries of the Western world, which is consistent with the geographic distribution of its training data, i.e., the IM2GPS3k dataset. Accordingly, when compared to the IM2GPS3k benchmark, the accuracy of the ISNs model notably decreases at all scales. Additionally, we cluster images of the SCA100 dataset based on how accurately they are predicted by the ISNs model and show the model's difficulties in correctly predicting the locations of images in low income regions, especially in Sub-Saharan Africa. Therefore, our results suggest that using IM2GPS3k as a training set and benchmark for image geolocation estimation and other computer vision models overlooks its potential application in the African context.","sentences":["Advances in Artificial Intelligence are challenged by the biases rooted in the datasets used to train the models.","In image geolocation estimation, models are mostly trained using data from specific geographic regions, notably the Western world, and as a result, they may struggle to comprehend the complexities of underrepresented regions.","To assess this issue, we apply a state-of-the-art image geolocation estimation model (ISNs) to a crowd-sourced dataset of geolocated images from the African continent (SCA100), and then explore the regional and socioeconomic biases underlying the model's predictions.","Our findings show that the ISNs model tends to over-predict image locations in high-income countries of the Western world, which is consistent with the geographic distribution of its training data, i.e., the IM2GPS3k dataset.","Accordingly, when compared to the IM2GPS3k benchmark, the accuracy of the ISNs model notably decreases at all scales.","Additionally, we cluster images of the SCA100 dataset based on how accurately they are predicted by the ISNs model and show the model's difficulties in correctly predicting the locations of images in low income regions, especially in Sub-Saharan Africa.","Therefore, our results suggest that using IM2GPS3k as a training set and benchmark for image geolocation estimation and other computer vision models overlooks its potential application in the African context."],"url":"http://arxiv.org/abs/2404.02558v1","category":"cs.CV"}
{"created":"2024-04-03 08:25:39","title":"Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks","abstract":"High-dimensional interpolation problems appear in various applications of uncertainty quantification, stochastic optimization and machine learning. Such problems are computationally expensive and request the use of adaptive grid generation strategies like anisotropic sparse grids to mitigate the curse of dimensionality. However, it is well known that the standard dimension-adaptive sparse grid method converges very slowly or even fails in the case of non-smooth functions. For piecewise smooth functions with kinks, we construct two novel hp-adaptive sparse grid collocation algorithms that combine low-order basis functions with local support in parts of the domain with less regularity and variable-order basis functions elsewhere. Spatial refinement is realized by means of a hierarchical multivariate knot tree which allows the construction of localised hierarchical basis functions with varying order. Hierarchical surplus is used as an error indicator to automatically detect the non-smooth region and adaptively refine the collocation points there. The local polynomial degrees are optionally selected by a greedy approach or a kink detection procedure. Three numerical benchmark examples with different dimensions are discussed and comparison with locally linear and highest degree basis functions are given to show the efficiency and accuracy of the proposed methods.","sentences":["High-dimensional interpolation problems appear in various applications of uncertainty quantification, stochastic optimization and machine learning.","Such problems are computationally expensive and request the use of adaptive grid generation strategies like anisotropic sparse grids to mitigate the curse of dimensionality.","However, it is well known that the standard dimension-adaptive sparse grid method converges very slowly or even fails in the case of non-smooth functions.","For piecewise smooth functions with kinks, we construct two novel hp-adaptive sparse grid collocation algorithms that combine low-order basis functions with local support in parts of the domain with less regularity and variable-order basis functions elsewhere.","Spatial refinement is realized by means of a hierarchical multivariate knot tree which allows the construction of localised hierarchical basis functions with varying order.","Hierarchical surplus is used as an error indicator to automatically detect the non-smooth region and adaptively refine the collocation points there.","The local polynomial degrees are optionally selected by a greedy approach or a kink detection procedure.","Three numerical benchmark examples with different dimensions are discussed and comparison with locally linear and highest degree basis functions are given to show the efficiency and accuracy of the proposed methods."],"url":"http://arxiv.org/abs/2404.02556v1","category":"math.NA"}
{"created":"2024-04-03 08:22:41","title":"An Interpretable Power System Transient Stability Assessment Method with Expert Guiding Neural-Regression-Tree","abstract":"Deep learning based transient stability assessment (TSA) has achieved great success, yet the lack of interpretability hinders its industrial application. Although a great number of studies have tried to explore the interpretability of network solutions, many problems still remain unsolved: (1) the difference between the widely accepted power system knowledge and the generated interpretive rules is large, (2) the probability characteristics of the neural network have not been fully considered during generating the interpretive rules, (3) the cost of the trade-off between accuracy and interpretability is too heavy to take. To address these issues, an interpretable power system Transient Stability Assessment method with Expert guiding Neural-Regression-Tree (TSA-ENRT) is proposed. TSA-ENRT utilizes an expert guiding nonlinear regression tree to approximate the neural network prediction and the neural network can be explained by the interpretive rules generated by the tree model. The nonlinearity of the expert guiding nonlinear regression tree is endowed with the extracted knowledge from a simple two-machine three-bus power system, which forms an expert knowledge base and thus the generated interpretive rules are more consistent with human cognition. Besides, the expert guiding tree model can build a bridge between the interpretive rules and the probability prediction of neural network in a regression way. By regularizing the neural network with the average decision length of ENRT, the association of the neural network and tree model is constructed in the model training level which provides a better trade-off between accuracy and interpretability. Extensive experiments indicate the interpretive rules generated by the proposed TSA-ENRT are highly consistent with the neural network prediction and more agreed with human expert cognition.","sentences":["Deep learning based transient stability assessment (TSA) has achieved great success, yet the lack of interpretability hinders its industrial application.","Although a great number of studies have tried to explore the interpretability of network solutions, many problems still remain unsolved: (1) the difference between the widely accepted power system knowledge and the generated interpretive rules is large, (2) the probability characteristics of the neural network have not been fully considered during generating the interpretive rules, (3) the cost of the trade-off between accuracy and interpretability is too heavy to take.","To address these issues, an interpretable power system Transient Stability Assessment method with Expert guiding Neural-Regression-Tree (TSA-ENRT) is proposed.","TSA-ENRT utilizes an expert guiding nonlinear regression tree to approximate the neural network prediction and the neural network can be explained by the interpretive rules generated by the tree model.","The nonlinearity of the expert guiding nonlinear regression tree is endowed with the extracted knowledge from a simple two-machine three-bus power system, which forms an expert knowledge base and thus the generated interpretive rules are more consistent with human cognition.","Besides, the expert guiding tree model can build a bridge between the interpretive rules and the probability prediction of neural network in a regression way.","By regularizing the neural network with the average decision length of ENRT, the association of the neural network and tree model is constructed in the model training level which provides a better trade-off between accuracy and interpretability.","Extensive experiments indicate the interpretive rules generated by the proposed TSA-ENRT are highly consistent with the neural network prediction and more agreed with human expert cognition."],"url":"http://arxiv.org/abs/2404.02555v1","category":"eess.SY"}
{"created":"2024-04-03 08:18:45","title":"Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data","abstract":"Given the rarity of significant solar flares compared to smaller ones, training effective machine learning models for solar activity forecasting is challenging due to insufficient data. This study proposes using generative deep learning models, specifically a Denoising Diffusion Probabilistic Model (DDPM), to create synthetic images of solar phenomena, including flares of varying intensities. By employing a dataset from the AIA instrument aboard the SDO spacecraft, focusing on the 171 {\\AA} band that captures various solar activities, and classifying images with GOES X-ray measurements based on flare intensity, we aim to address the data scarcity issue. The DDPM's performance is evaluated using cluster metrics, Frechet Inception Distance (FID), and F1-score, showcasing promising results in generating realistic solar imagery. We conduct two experiments: one to train a supervised classifier for event identification and another for basic flare prediction, demonstrating the value of synthetic data in managing imbalanced datasets. This research underscores the potential of DDPMs in solar data analysis and forecasting, suggesting further exploration into their capabilities for solar flare prediction and application in other deep learning and physical tasks.","sentences":["Given the rarity of significant solar flares compared to smaller ones, training effective machine learning models for solar activity forecasting is challenging due to insufficient data.","This study proposes using generative deep learning models, specifically a Denoising Diffusion Probabilistic Model (DDPM), to create synthetic images of solar phenomena, including flares of varying intensities.","By employing a dataset from the AIA instrument aboard the SDO spacecraft, focusing on the 171 {\\AA} band that captures various solar activities, and classifying images with GOES X-ray measurements based on flare intensity, we aim to address the data scarcity issue.","The DDPM's performance is evaluated using cluster metrics, Frechet Inception Distance (FID), and F1-score, showcasing promising results in generating realistic solar imagery.","We conduct two experiments: one to train a supervised classifier for event identification and another for basic flare prediction, demonstrating the value of synthetic data in managing imbalanced datasets.","This research underscores the potential of DDPMs in solar data analysis and forecasting, suggesting further exploration into their capabilities for solar flare prediction and application in other deep learning and physical tasks."],"url":"http://arxiv.org/abs/2404.02552v1","category":"astro-ph.SR"}
{"created":"2024-04-03 08:16:12","title":"On the comparison between phenomenological and kinetic theories of gas mixtures with applications to flocking","abstract":"We study the compression between the phenomenological and kinetic models for a mixture of gases from the viewpoint of collective dynamics. In the case in which constituents are Eulerian gases, balance equations for mass, momentum, and energy are the same in the main differential part, but production terms due to the interchanges between constituents are different. They coincide only when the thermal and mechanical diffusion are sufficiently small. In this paper, we first verify that both models satisfy the universal requirements of conservation laws of total mass, momentum, and energy, Galilean invariance and entropy principle. Following the work of Ha and Ruggeri (ARMA 2017), we consider spatially homogeneous models which correspond to the generalizations of the Cucker Smale model with the thermal effect. In these circumstances, we provide analytical results for the comparison between two resulting models and also present several numerical simulations to complement analytical results.","sentences":["We study the compression between the phenomenological and kinetic models for a mixture of gases from the viewpoint of collective dynamics.","In the case in which constituents are Eulerian gases, balance equations for mass, momentum, and energy are the same in the main differential part, but production terms due to the interchanges between constituents are different.","They coincide only when the thermal and mechanical diffusion are sufficiently small.","In this paper, we first verify that both models satisfy the universal requirements of conservation laws of total mass, momentum, and energy, Galilean invariance and entropy principle.","Following the work of Ha and Ruggeri (ARMA 2017), we consider spatially homogeneous models which correspond to the generalizations of the Cucker Smale model with the thermal effect.","In these circumstances, we provide analytical results for the comparison between two resulting models and also present several numerical simulations to complement analytical results."],"url":"http://arxiv.org/abs/2404.02550v1","category":"math.DS"}
{"created":"2024-04-03 08:15:37","title":"Two-line Josephson traveling wave parametric amplifier","abstract":"Feasibility of two-line design of Josephson traveling wave parametric amplifier aimed at increase of the allowed pump wave energy and hence the gain growth is analyzed and discussed. Serious restrictions follow from both the cyclic energy transfer of the pump, signal and idler waves in the coupled waveguide lines and the phase mismatch of the waves. Besides, impact of the artificial line discreteness on the phase mismatch is considered as well.","sentences":["Feasibility of two-line design of Josephson traveling wave parametric amplifier aimed at increase of the allowed pump wave energy and hence the gain growth is analyzed and discussed.","Serious restrictions follow from both the cyclic energy transfer of the pump, signal and idler waves in the coupled waveguide lines and the phase mismatch of the waves.","Besides, impact of the artificial line discreteness on the phase mismatch is considered as well."],"url":"http://arxiv.org/abs/2404.02549v1","category":"cond-mat.supr-con"}
{"created":"2024-04-03 08:15:08","title":"AI-Tutoring in Software Engineering Education","abstract":"With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.","sentences":["With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation.","The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense.","However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored.","Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.","In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis.","Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor.","Additionally, the findings highlight advantages, such as timely feedback and scalability.","However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident.","This research adds to the discourse on AI's role in education."],"url":"http://arxiv.org/abs/2404.02548v1","category":"cs.SE"}
{"created":"2024-04-03 08:05:09","title":"Analysis and approximation to parabolic optimal control problems with measure-valued controls in time","abstract":"In this paper, we investigate an optimal control problem governed by parabolic equations with measure-valued controls over time. We establish the well-posedness of the optimal control problem and derive the first-order optimality condition using Clarke's subgradients, revealing a sparsity structure in time for the optimal control. Consequently, these optimal control problems represent a generalization of impulse control for evolution equations. To discretize the optimal control problem, we employ the space-time finite element method. Here, the state equation is approximated using piecewise linear and continuous finite elements in space, alongside a Petrov-Galerkin method utilizing piecewise constant trial functions and piecewise linear and continuous test functions in time. The control variable is discretized using the variational discretization concept. For error estimation, we initially derive a priori error estimates and stabilities for the finite element discretizations of the state and adjoint equations. Subsequently, we establish weak-* convergence for the control under the norm $\\mathcal{M}(\\bar I_c;L^2(\\omega))$, with a convergence order of $O(h^\\frac{1}{2}+\\tau^\\frac{1}{4})$ for the state.","sentences":["In this paper, we investigate an optimal control problem governed by parabolic equations with measure-valued controls over time.","We establish the well-posedness of the optimal control problem and derive the first-order optimality condition using Clarke's subgradients, revealing a sparsity structure in time for the optimal control.","Consequently, these optimal control problems represent a generalization of impulse control for evolution equations.","To discretize the optimal control problem, we employ the space-time finite element method.","Here, the state equation is approximated using piecewise linear and continuous finite elements in space, alongside a Petrov-Galerkin method utilizing piecewise constant trial functions and piecewise linear and continuous test functions in time.","The control variable is discretized using the variational discretization concept.","For error estimation, we initially derive a priori error estimates and stabilities for the finite element discretizations of the state and adjoint equations.","Subsequently, we establish weak-* convergence for the control under the norm $\\mathcal{M}(\\bar I_c;L^2(\\omega))$, with a convergence order of $O(h^\\frac{1}{2}+\\tau^\\frac{1}{4})$ for the state."],"url":"http://arxiv.org/abs/2404.02546v1","category":"math.OC"}
{"created":"2024-04-03 08:03:27","title":"Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning","abstract":"Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretically proved that only a few conditions are needed to obtain accurate uncertainty constraints in the proposed method. Moreover, we develop a Grid-Mapping Pseudo-Count Soft Actor-Critic(GPC-SAC) algorithm using GPC under the Soft Actor-Critic(SAC) framework to demonstrate the effectiveness of GPC. The experimental results on D4RL benchmark datasets show that GPC-SAC has better performance and less computational cost compared to other algorithms.","sentences":["Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application.","However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions.","To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter.","Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs.","In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost.","The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count.","It is theoretically proved that only a few conditions are needed to obtain accurate uncertainty constraints in the proposed method.","Moreover, we develop a Grid-Mapping Pseudo-Count Soft Actor-Critic(GPC-SAC) algorithm using GPC under the Soft Actor-Critic(SAC) framework to demonstrate the effectiveness of GPC.","The experimental results on D4RL benchmark datasets show that GPC-SAC has better performance and less computational cost compared to other algorithms."],"url":"http://arxiv.org/abs/2404.02545v1","category":"cs.LG"}
{"created":"2024-04-03 08:01:00","title":"Semi-Supervised Unconstrained Head Pose Estimation in the Wild","abstract":"Existing head pose estimation datasets are either composed of numerous samples by non-realistic synthesis or lab collection, or limited images by labor-intensive annotating. This makes deep supervised learning based solutions compromised due to the reliance on generous labeled data. To alleviate it, we propose the first semi-supervised unconstrained head pose estimation (SemiUHPE) method, which can leverage a large amount of unlabeled wild head images. Specifically, we follow the recent semi-supervised rotation regression, and focus on the diverse and complex head pose domain. Firstly, we claim that the aspect-ratio invariant cropping of heads is superior to the previous landmark-based affine alignment, which does not fit unlabeled natural heads or practical applications where landmarks are often unavailable. Then, instead of using an empirically fixed threshold to filter out pseudo labels, we propose the dynamic entropy-based filtering by updating thresholds for adaptively removing unlabeled outliers. Moreover, we revisit the design of weak-strong augmentations, and further exploit its superiority by devising two novel head-oriented strong augmentations named pose-irrelevant cut-occlusion and pose-altering rotation consistency. Extensive experiments show that SemiUHPE can surpass SOTAs with remarkable improvements on public benchmarks under both front-range and full-range. Our code is released in \\url{https://github.com/hnuzhy/SemiUHPE}.","sentences":["Existing head pose estimation datasets are either composed of numerous samples by non-realistic synthesis or lab collection, or limited images by labor-intensive annotating.","This makes deep supervised learning based solutions compromised due to the reliance on generous labeled data.","To alleviate it, we propose the first semi-supervised unconstrained head pose estimation (SemiUHPE) method, which can leverage a large amount of unlabeled wild head images.","Specifically, we follow the recent semi-supervised rotation regression, and focus on the diverse and complex head pose domain.","Firstly, we claim that the aspect-ratio invariant cropping of heads is superior to the previous landmark-based affine alignment, which does not fit unlabeled natural heads or practical applications where landmarks are often unavailable.","Then, instead of using an empirically fixed threshold to filter out pseudo labels, we propose the dynamic entropy-based filtering by updating thresholds for adaptively removing unlabeled outliers.","Moreover, we revisit the design of weak-strong augmentations, and further exploit its superiority by devising two novel head-oriented strong augmentations named pose-irrelevant cut-occlusion and pose-altering rotation consistency.","Extensive experiments show that SemiUHPE can surpass SOTAs with remarkable improvements on public benchmarks under both front-range and full-range.","Our code is released in \\url{https://github.com/hnuzhy/SemiUHPE}."],"url":"http://arxiv.org/abs/2404.02544v1","category":"cs.CV"}
{"created":"2024-04-03 08:00:46","title":"Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset","abstract":"Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments reveal that ULTR robustly improves click prediction. However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark.","sentences":["Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data.","While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines.","The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques.","Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques.","We revisit and extend the available experiments.","We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features.","Our experiments reveal that ULTR robustly improves click prediction.","However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark."],"url":"http://arxiv.org/abs/2404.02543v1","category":"cs.IR"}
{"created":"2024-04-03 07:55:57","title":"CSEPrompts: A Benchmark of Introductory Computer Science Prompts","abstract":"Recent advances in AI, machine learning, and NLP have led to the development of a new generation of Large Language Models (LLMs) that are trained on massive amounts of data and often have trillions of parameters. Commercial applications (e.g., ChatGPT) have made this technology available to the general public, thus making it possible to use LLMs to produce high-quality texts for academic and professional purposes. Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse. Educational programs in Computer Science (CS) and related fields are particularly affected because LLMs are also capable of generating programming code in various programming languages. To help understand the potential impact of publicly available LLMs in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise prompts and multiple-choice questions retrieved from introductory CS and programming courses. We also provide experimental results on CSEPrompts to evaluate the performance of several LLMs with respect to generating Python code and answering basic computer science and programming questions.","sentences":["Recent advances in AI, machine learning, and NLP have led to the development of a new generation of Large Language Models (LLMs) that are trained on massive amounts of data and often have trillions of parameters.","Commercial applications (e.g., ChatGPT) have made this technology available to the general public, thus making it possible to use LLMs to produce high-quality texts for academic and professional purposes.","Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse.","Educational programs in Computer Science (CS) and related fields are particularly affected because LLMs are also capable of generating programming code in various programming languages.","To help understand the potential impact of publicly available LLMs in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise prompts and multiple-choice questions retrieved from introductory CS and programming courses.","We also provide experimental results on CSEPrompts to evaluate the performance of several LLMs with respect to generating Python code and answering basic computer science and programming questions."],"url":"http://arxiv.org/abs/2404.02540v2","category":"cs.CL"}
{"created":"2024-04-03 07:50:53","title":"Convergence Analysis of Flow Matching in Latent Space with Transformers","abstract":"We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.","sentences":["We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching.","We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution.","Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions.","Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest."],"url":"http://arxiv.org/abs/2404.02538v1","category":"stat.ML"}
{"created":"2024-04-03 07:44:38","title":"ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model","abstract":"In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.","sentences":["In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages.","However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape.","This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach.","In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks.","We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively."],"url":"http://arxiv.org/abs/2404.02534v1","category":"cs.CL"}
{"created":"2024-04-03 07:43:11","title":"Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game","abstract":"With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.","sentences":["With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise.","There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering.","As a result, large models counter malicious attackers' attacks using techniques such as safety alignment.","However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities.","In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent.","First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks.","After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents.","The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods.","Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations."],"url":"http://arxiv.org/abs/2404.02532v1","category":"cs.AI"}
{"created":"2024-04-03 07:33:30","title":"Severity Controlled Text-to-Image Generative Model Bias Manipulation","abstract":"Text-to-image (T2I) generative models are gaining wide popularity, especially in public domains. However, their intrinsic bias and potential malicious manipulations remain under-explored. Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models. By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias. As a by-product, this control also allows a form of precise prompt engineering to generate images which are generally implausible with regular text prompts. We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing. Our technique does not require training and is also framed as a backdoor attack with severity control using semantically-null text triggers in the prompts. With extensive analysis, we present interesting qualitative and quantitative results to expose potential manipulation possibilities for T2I models.   Key-words: Text-to-Image Models, Generative Models, Backdoor Attacks, Prompt Engineering, Bias","sentences":["Text-to-image (T2I) generative models are gaining wide popularity, especially in public domains.","However, their intrinsic bias and potential malicious manipulations remain under-explored.","Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models.","By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias.","As a by-product, this control also allows a form of precise prompt engineering to generate images which are generally implausible with regular text prompts.","We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing.","Our technique does not require training and is also framed as a backdoor attack with severity control using semantically-null text triggers in the prompts.","With extensive analysis, we present interesting qualitative and quantitative results to expose potential manipulation possibilities for T2I models.   ","Key-words: Text-to-Image Models, Generative Models, Backdoor Attacks, Prompt Engineering, Bias"],"url":"http://arxiv.org/abs/2404.02530v1","category":"cs.CV"}
{"created":"2024-04-03 07:31:16","title":"Nanophotonic inspection of deep-subwavelength integrated optoelectronic chips","abstract":"Artificial nanostructures with ultrafine and deep-subwavelength feature sizes have emerged as a paradigm-shifting platform to advanced light field management, becoming a key building block for high-performance integrated optoelectronics and flat optics. However, direct optical inspection of such integrated chips with densely packed complex and small features remains a missing metrology gap that hinders quick feedback between design and fabrications. Here, we demonstrate that photothermal nonlinear scattering microscopy can be utilized for direct imaging and resolving of integrated optoelectronic chips beyond the diffraction limit. We reveal that the inherent coupling among deep-subwavelength nanostructures supporting leaky resonances allows for the pronounced heating effect to access reversible nonlinear modulations of the confocal reflection intensity, leading to optical resolving power down to 80 nm (~lambda/7). The versatility of this approach has been exemplified by direct imaging of silicon grating couplers and metalens with a minimum critical dimension of 100 nm, as well as central processing unit (CPU) chip with 45 nm technology, unfolding the long-sought possibility of in-situ, non-destructive, high-throughput optical inspection of integrated optoelectronic chips and nanophotonic chips.","sentences":["Artificial nanostructures with ultrafine and deep-subwavelength feature sizes have emerged as a paradigm-shifting platform to advanced light field management, becoming a key building block for high-performance integrated optoelectronics and flat optics.","However, direct optical inspection of such integrated chips with densely packed complex and small features remains a missing metrology gap that hinders quick feedback between design and fabrications.","Here, we demonstrate that photothermal nonlinear scattering microscopy can be utilized for direct imaging and resolving of integrated optoelectronic chips beyond the diffraction limit.","We reveal that the inherent coupling among deep-subwavelength nanostructures supporting leaky resonances allows for the pronounced heating effect to access reversible nonlinear modulations of the confocal reflection intensity, leading to optical resolving power down to 80 nm (~lambda/7).","The versatility of this approach has been exemplified by direct imaging of silicon grating couplers and metalens with a minimum critical dimension of 100 nm, as well as central processing unit (CPU) chip with 45 nm technology, unfolding the long-sought possibility of in-situ, non-destructive, high-throughput optical inspection of integrated optoelectronic chips and nanophotonic chips."],"url":"http://arxiv.org/abs/2404.02528v1","category":"physics.optics"}
{"created":"2024-04-03 07:30:09","title":"Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling","abstract":"Learning to build 3D scene graphs is essential for real-world perception in a structured and rich fashion. However, previous 3D scene graph generation methods utilize a fully supervised learning manner and require a large amount of entity-level annotation data of objects and relations, which is extremely resource-consuming and tedious to obtain. To tackle this problem, we propose 3D-VLAP, a weakly-supervised 3D scene graph generation method via Visual-Linguistic Assisted Pseudo-labeling. Specifically, our 3D-VLAP exploits the superior ability of current large-scale visual-linguistic models to align the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds. First, we establish the positional correspondence from 3D point clouds to 2D images via camera intrinsic and extrinsic parameters, thereby achieving alignment of 3D point clouds and 2D images. Subsequently, a large-scale cross-modal visual-linguistic model is employed to indirectly align 3D instances with the textual category labels of objects by matching 2D images with object category labels. The pseudo labels for objects and relations are then produced for 3D-VLAP model training by calculating the similarity between visual embeddings and textual category embeddings of objects and relations encoded by the visual-linguistic model, respectively. Ultimately, we design an edge self-attention based graph neural network to generate scene graphs of 3D point cloud scenes. Extensive experiments demonstrate that our 3D-VLAP achieves comparable results with current advanced fully supervised methods, meanwhile significantly alleviating the pressure of data annotation.","sentences":["Learning to build 3D scene graphs is essential for real-world perception in a structured and rich fashion.","However, previous 3D scene graph generation methods utilize a fully supervised learning manner and require a large amount of entity-level annotation data of objects and relations, which is extremely resource-consuming and tedious to obtain.","To tackle this problem, we propose 3D-VLAP, a weakly-supervised 3D scene graph generation method via Visual-Linguistic Assisted Pseudo-labeling.","Specifically, our 3D-VLAP exploits the superior ability of current large-scale visual-linguistic models to align the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds.","First, we establish the positional correspondence from 3D point clouds to 2D images via camera intrinsic and extrinsic parameters, thereby achieving alignment of 3D point clouds and 2D images.","Subsequently, a large-scale cross-modal visual-linguistic model is employed to indirectly align 3D instances with the textual category labels of objects by matching 2D images with object category labels.","The pseudo labels for objects and relations are then produced for 3D-VLAP model training by calculating the similarity between visual embeddings and textual category embeddings of objects and relations encoded by the visual-linguistic model, respectively.","Ultimately, we design an edge self-attention based graph neural network to generate scene graphs of 3D point cloud scenes.","Extensive experiments demonstrate that our 3D-VLAP achieves comparable results with current advanced fully supervised methods, meanwhile significantly alleviating the pressure of data annotation."],"url":"http://arxiv.org/abs/2404.02527v1","category":"cs.CV"}
{"created":"2024-04-03 07:29:34","title":"Revisiting thermodynamic topology of Hawking-Page and Davies type phase transitions","abstract":"In this work, we propose a common vector field to study the thermodynamic topology of the Davies type and Hawking-Page phase transitions. Existing literature has shown that studying these two types of phase transitions typically requires defining two separate vector fields . In our approach, we adopt Duan's $\\phi$-mapping topological current theory to define a novel vector field, denoted as $\\phi$, whose critical points exactly correspond to the Davies point and the Hawking-Page phase transition point. More importantly, we can differentiate between these two points by their topological charge. While, the topological charge for the critical point corresponding to the Davies-type phase transition is found to be $-1$, the same for the Hawking-Page phase transition point, it is $+1$. Although our analysis is applicable to all black hole systems where both types of phase transitions are found, we illustrate it using three simple systems as examples: the Schwarzschild AdS black hole, the Reissner-Nordstr\\\"om AdS black hole in the grand canonical ensemble, and finally the Kerr AdS black holes in the grand canonical ensemble. It is wellknown that these black holes exhibit both Davies and Hawking-Page phase transitions. With our proposed vector $\\phi$, the critical points obtained for these three systems exactly match the Davies-type and Hawking-Page phase transition points, and the associated topological charges are found to be $-1$ for the Davies point and $+1$ for the Hawking-Page phase transition point.","sentences":["In this work, we propose a common vector field to study the thermodynamic topology of the Davies type and Hawking-Page phase transitions.","Existing literature has shown that studying these two types of phase transitions typically requires defining two separate vector fields .","In our approach, we adopt Duan's $\\phi$-mapping topological current theory to define a novel vector field, denoted as $\\phi$, whose critical points exactly correspond to the Davies point and the Hawking-Page phase transition point.","More importantly, we can differentiate between these two points by their topological charge.","While, the topological charge for the critical point corresponding to the Davies-type phase transition is found to be $-1$, the same for the Hawking-Page phase transition point, it is $+1$. Although our analysis is applicable to all black hole systems where both types of phase transitions are found, we illustrate it using three simple systems as examples: the Schwarzschild AdS black hole, the Reissner-Nordstr\\\"om AdS black hole in the grand canonical ensemble, and finally the Kerr AdS black holes in the grand canonical ensemble.","It is wellknown that these black holes exhibit both Davies and Hawking-Page phase transitions.","With our proposed vector $\\phi$, the critical points obtained for these three systems exactly match the Davies-type and Hawking-Page phase transition points, and the associated topological charges are found to be $-1$ for the Davies point and $+1$ for the Hawking-Page phase transition point."],"url":"http://arxiv.org/abs/2404.02526v1","category":"hep-th"}
{"created":"2024-04-03 07:26:15","title":"Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion","abstract":"Generating realistic and controllable agent behaviors in traffic simulation is crucial for the development of autonomous vehicles. This problem is often formulated as imitation learning (IL) from real-world driving data by either directly predicting future trajectories or inferring cost functions with inverse optimal control. In this paper, we draw a conceptual connection between IL and diffusion-based generative modeling and introduce a novel framework Versatile Behavior Diffusion (VBD) to simulate interactive scenarios with multiple traffic participants. Our model not only generates scene-consistent multi-agent interactions but also enables scenario editing through multi-step guidance and refinement. Experimental evaluations show that VBD achieves state-of-the-art performance on the Waymo Sim Agents benchmark. In addition, we illustrate the versatility of our model by adapting it to various applications. VBD is capable of producing scenarios conditioning on priors, integrating with model-based optimization, sampling multi-modal scene-consistent scenarios by fusing marginal predictions, and generating safety-critical scenarios when combined with a game-theoretic solver.","sentences":["Generating realistic and controllable agent behaviors in traffic simulation is crucial for the development of autonomous vehicles.","This problem is often formulated as imitation learning (IL) from real-world driving data by either directly predicting future trajectories or inferring cost functions with inverse optimal control.","In this paper, we draw a conceptual connection between IL and diffusion-based generative modeling and introduce a novel framework Versatile Behavior Diffusion (VBD) to simulate interactive scenarios with multiple traffic participants.","Our model not only generates scene-consistent multi-agent interactions but also enables scenario editing through multi-step guidance and refinement.","Experimental evaluations show that VBD achieves state-of-the-art performance on the Waymo Sim Agents benchmark.","In addition, we illustrate the versatility of our model by adapting it to various applications.","VBD is capable of producing scenarios conditioning on priors, integrating with model-based optimization, sampling multi-modal scene-consistent scenarios by fusing marginal predictions, and generating safety-critical scenarios when combined with a game-theoretic solver."],"url":"http://arxiv.org/abs/2404.02524v1","category":"cs.RO"}
{"created":"2024-04-03 07:23:03","title":"Text-driven Affordance Learning from Egocentric Vision","abstract":"Visual affordance learning is a key component for robots to understand how to interact with objects. Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios. The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects. This approach covers both hand-object and tool-object interactions. We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction. In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations. However, when we gather data for this task, manual annotations of these diverse interactions are costly. To this end, we propose a pseudo dataset creation pipeline and build a large pseudo-training dataset: TextAFF80K, consisting of over 80K instances of the contact points, trajectories, images, and text tuples. We extend existing referring expression comprehension models for our task, and experimental results show that our approach robustly handles multiple affordances, serving as a new standard for affordance learning in real-world scenarios.","sentences":["Visual affordance learning is a key component for robots to understand how to interact with objects.","Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios.","The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects.","This approach covers both hand-object and tool-object interactions.","We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction.","In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations.","However, when we gather data for this task, manual annotations of these diverse interactions are costly.","To this end, we propose a pseudo dataset creation pipeline and build a large pseudo-training dataset: TextAFF80K, consisting of over 80K instances of the contact points, trajectories, images, and text tuples.","We extend existing referring expression comprehension models for our task, and experimental results show that our approach robustly handles multiple affordances, serving as a new standard for affordance learning in real-world scenarios."],"url":"http://arxiv.org/abs/2404.02523v1","category":"cs.CV"}
{"created":"2024-04-03 07:21:46","title":"Beyond Gaps and Bumps: Spectral Siren Cosmology with Non-Parametric Population Models","abstract":"Gravitational wave standard sirens typically require electromagnetic (EM) data to obtain redshift information to constrain cosmology. Difficult to find EM counterparts for bright sirens and galaxy survey systematics for dark sirens make cosmological constraints with spectral sirens, a gravitational wave data-only approach, extremely appealing. In this work, we use the GWTC-3 BBH detections as spectral sirens to constrain the BBH population and the underlying cosmological expansion with a flexible model for the black hole mass spectrum. We use a binned Gaussian process to model the BBH mass distribution in the source frame without any astrophysical assumptions on the shape and or inclusion (or lack of) features that drive the cosmological constraints as the redshifted detector frame masses become consistent with the underlying astrophysical mass distribution features. For GWTC-3 we find a measurement on the Hubble constant of $H_0=73.0^{+13.3}_{-7.7} \\ {\\rm{km \\ s^{-1} \\ Mpc^{-1}}}$ at $68\\%$ C.L. when combined with that obtained from the bright standard siren analysis with GW170817 and its associated host galaxy NGC 4993. We find an improved estimate for the Hubble constant of around a factor of 1.4 times better than the GW170817 measurement alone. We validate our nonparametric spectral siren approach with simulations and benchmark its scalability and constraining performance when compared with parametric methods.","sentences":["Gravitational wave standard sirens typically require electromagnetic (EM) data to obtain redshift information to constrain cosmology.","Difficult to find EM counterparts for bright sirens and galaxy survey systematics for dark sirens make cosmological constraints with spectral sirens, a gravitational wave data-only approach, extremely appealing.","In this work, we use the GWTC-3 BBH detections as spectral sirens to constrain the BBH population and the underlying cosmological expansion with a flexible model for the black hole mass spectrum.","We use a binned Gaussian process to model the BBH mass distribution in the source frame without any astrophysical assumptions on the shape and or inclusion (or lack of) features that drive the cosmological constraints as the redshifted detector frame masses become consistent with the underlying astrophysical mass distribution features.","For GWTC-3 we find a measurement on the Hubble constant of $H_0=73.0^{+13.3}_{-7.7} \\ {\\rm{km \\ s^{-1} \\ Mpc^{-1}}}$ at $68\\%$ C.L. when combined with that obtained from the bright standard siren analysis with GW170817 and its associated host galaxy NGC 4993.","We find an improved estimate for the Hubble constant of around a factor of 1.4 times better than the GW170817 measurement alone.","We validate our nonparametric spectral siren approach with simulations and benchmark its scalability and constraining performance when compared with parametric methods."],"url":"http://arxiv.org/abs/2404.02522v1","category":"astro-ph.CO"}
{"created":"2024-04-03 07:20:53","title":"Space-time parallel scaling of Parareal with a Fourier Neural Operator as coarse propagator","abstract":"Iterative parallel-in-time algorithms like Parareal can extend scaling beyond the saturation of purely spatial parallelization when solving initial value problems. However, they require the user to build coarse models to handle the inevitably serial transport of information in time.This is a time consuming and difficult process since there is still only limited theoretical insight into what constitutes a good and efficient coarse model. Novel approaches from machine learning to solve differential equations could provide a more generic way to find coarse level models for parallel-in-time algorithms. This paper demonstrates that a physics-informed Fourier Neural Operator (PINO) is an effective coarse model for the parallelization in time of the two-asset Black-Scholes equation using Parareal. We demonstrate that PINO-Parareal converges as fast as a bespoke numerical coarse model and that, in combination with spatial parallelization by domain decomposition, it provides better overall speedup than both purely spatial parallelization and space-time parallelizaton with a numerical coarse propagator.","sentences":["Iterative parallel-in-time algorithms like Parareal can extend scaling beyond the saturation of purely spatial parallelization when solving initial value problems.","However, they require the user to build coarse models to handle the inevitably serial transport of information in time.","This is a time consuming and difficult process since there is still only limited theoretical insight into what constitutes a good and efficient coarse model.","Novel approaches from machine learning to solve differential equations could provide a more generic way to find coarse level models for parallel-in-time algorithms.","This paper demonstrates that a physics-informed Fourier Neural Operator (PINO) is an effective coarse model for the parallelization in time of the two-asset Black-Scholes equation using Parareal.","We demonstrate that PINO-Parareal converges as fast as a bespoke numerical coarse model and that, in combination with spatial parallelization by domain decomposition, it provides better overall speedup than both purely spatial parallelization and space-time parallelizaton with a numerical coarse propagator."],"url":"http://arxiv.org/abs/2404.02521v1","category":"math.NA"}
{"created":"2024-04-03 07:07:29","title":"Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots","abstract":"Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments. To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots. We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots. Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration. Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates. Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel odometry online for creating a reasonable constraint. The proposed method is validated through three experiments. The first indoor experiment shows that the proposed method is robust in severe degeneracy cases (long corridors) and changes in the wheel radii. The second outdoor experiment demonstrates that our method accurately estimates the sensor trajectory despite being in rough outdoor terrain owing to online uncertainty estimation of wheel odometry. The third experiment shows the proposed online calibration enables robust odometry estimation in changing terrains.","sentences":["Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments.","To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots.","We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots.","Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration.","Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates.","Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel odometry online for creating a reasonable constraint.","The proposed method is validated through three experiments.","The first indoor experiment shows that the proposed method is robust in severe degeneracy cases (long corridors) and changes in the wheel radii.","The second outdoor experiment demonstrates that our method accurately estimates the sensor trajectory despite being in rough outdoor terrain owing to online uncertainty estimation of wheel odometry.","The third experiment shows the proposed online calibration enables robust odometry estimation in changing terrains."],"url":"http://arxiv.org/abs/2404.02515v1","category":"cs.RO"}
{"created":"2024-04-03 06:53:56","title":"An Interpretable Client Decision Tree Aggregation process for Federated Learning","abstract":"Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a federated learning environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that keeps the interpretability and the precision of the base decision trees used for the aggregation. This model is based on aggregating multiple decision paths of the decision trees and can be used on different decision tree types, such as ID3 and CART. We carry out the experiments within four datasets, and the analysis shows that the tree built with the model improves the local models, and outperforms the state-of-the-art.","sentences":["Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others.","This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning.","While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models.","Decision tree structure makes the aggregation in a federated learning environment not trivial.","They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable.","In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that keeps the interpretability and the precision of the base decision trees used for the aggregation.","This model is based on aggregating multiple decision paths of the decision trees and can be used on different decision tree types, such as ID3 and CART.","We carry out the experiments within four datasets, and the analysis shows that the tree built with the model improves the local models, and outperforms the state-of-the-art."],"url":"http://arxiv.org/abs/2404.02510v1","category":"cs.LG"}
{"created":"2024-04-03 06:53:27","title":"VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments","abstract":"Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on the images. Our results show that VIAssist provides +0.21 and +0.31 higher BERTScore and ROUGE scores than the baseline, respectively.","sentences":["Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people.","An estimated 2.2 billion individuals worldwide are affected by visual impairments.","Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains.","It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning.","However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests.","For example, the target object is not fully or partially placed in the image.","This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers.","VIAssist can identify undesired images and provide detailed actions.","Finally, VIAssist can provide reliable answers to users' queries based on the images.","Our results show that VIAssist provides +0.21 and +0.31 higher BERTScore and ROUGE scores than the baseline, respectively."],"url":"http://arxiv.org/abs/2404.02508v1","category":"cs.CV"}
{"created":"2024-04-03 06:47:15","title":"Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation","abstract":"Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences. In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively. We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (\\ourwork), a novel approach that synergizes these elements to improve the quality of support provided in ESCs. By leveraging in-context learning and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs. We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to deepen situational awareness of help-seekers' mental states. Our supportive decoder integrates information from diverse knowledge sources, underpinning response generation that is both empathetic and cognitively aware. The effectiveness of \\ourwork is demonstrated through extensive automatic and human evaluations, revealing substantial improvements over numerous state-of-the-art models, with up to 13.79\\% enhancement in overall performance of ten metrics. Our codes are available for public access to facilitate further research and development.","sentences":["Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences.","In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively.","We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (\\ourwork), a novel approach that synergizes these elements to improve the quality of support provided in ESCs.","By leveraging in-context learning and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs.","We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to deepen situational awareness of help-seekers' mental states.","Our supportive decoder integrates information from diverse knowledge sources, underpinning response generation that is both empathetic and cognitively aware.","The effectiveness of \\ourwork is demonstrated through extensive automatic and human evaluations, revealing substantial improvements over numerous state-of-the-art models, with up to 13.79\\% enhancement in overall performance of ten metrics.","Our codes are available for public access to facilitate further research and development."],"url":"http://arxiv.org/abs/2404.02505v1","category":"cs.CL"}
{"created":"2024-04-03 06:45:19","title":"Valley-controlled photoswitching of metal-insulator nanotextures","abstract":"Spatial heterogeneity and phase competition are hallmarks of strongly-correlated materials, promising tunable functionality on the nanoscale. Light-induced switching of a correlated insulator to a metallic state is well established. However, optical excitation generally lacks the specificity to select sub-wavelength domains and control final textures. Here, we employ valley-selective photodoping to drive the domain-specific quench of a textured Peierls insulator. Polarized excitation leverages the anisotropy of quasi-one-dimensional states at the correlated gap to initiate an insulator-to-metal transition with minimal electronic heating. We find that averting dissipation facilitates domain-specific carrier confinement, control over nanotextured phases, and a prolonged lifetime of the metastable metallic state. Complementing existing manipulation schemes, valley-selective photoexcitation will enable the activation of electronic phase separation beyond thermodynamic limitations, facilitating optically-controlled hidden states, engineered heterostructures, and polarization-sensitive percolation networks.","sentences":["Spatial heterogeneity and phase competition are hallmarks of strongly-correlated materials, promising tunable functionality on the nanoscale.","Light-induced switching of a correlated insulator to a metallic state is well established.","However, optical excitation generally lacks the specificity to select sub-wavelength domains and control final textures.","Here, we employ valley-selective photodoping to drive the domain-specific quench of a textured Peierls insulator.","Polarized excitation leverages the anisotropy of quasi-one-dimensional states at the correlated gap to initiate an insulator-to-metal transition with minimal electronic heating.","We find that averting dissipation facilitates domain-specific carrier confinement, control over nanotextured phases, and a prolonged lifetime of the metastable metallic state.","Complementing existing manipulation schemes, valley-selective photoexcitation will enable the activation of electronic phase separation beyond thermodynamic limitations, facilitating optically-controlled hidden states, engineered heterostructures, and polarization-sensitive percolation networks."],"url":"http://arxiv.org/abs/2404.02503v1","category":"cond-mat.str-el"}
{"created":"2024-04-03 06:32:25","title":"Real-fluid simulation of ammonia cavitation in a heavy-duty fuel injector","abstract":"The reduction of greenhouse gases (GHG) emitted into the earth's atmosphere, such as carbon dioxide, has obviously become a priority. Replacing fossil fuels with cleaner renewable fuels (such as ammonia) in internal combustion engines for heavy-duty vehicles is one promising solution to reduce GHG emissions. This paper aims to study the cavitation formation in a heavy-duty injector using ammonia as fuel. The simulation is carried out using a fully compressible two-phase multi-component real-fluid model (RFM) developed in the CONVERGE CFD solver. In the RFM model, the thermodynamic and transport properties are stored in a table which is used during the run-time. The thermodynamic table is generated using the in-house Carnot thermodynamic library based on vapor-liquid equilibrium calculations coupled with a real-fluid equation of state. The RFM model allows to consider the effects of the dissolved non-condensable gas such as nitrogen on the phase change process. The obtained numerical results have confirmed that the model can tackle the phase transition phenomenon under the considered conditions. In contrast to previous numerical studies of the cavitation phenomenon using hydrocarbon fuels, the formed cavitation pockets were found to be primarily composed of ammonia vapor due to its high vapor pressure, with minimal contribution of the dissolved non-condensable nitrogen.","sentences":["The reduction of greenhouse gases (GHG) emitted into the earth's atmosphere, such as carbon dioxide, has obviously become a priority.","Replacing fossil fuels with cleaner renewable fuels (such as ammonia) in internal combustion engines for heavy-duty vehicles is one promising solution to reduce GHG emissions.","This paper aims to study the cavitation formation in a heavy-duty injector using ammonia as fuel.","The simulation is carried out using a fully compressible two-phase multi-component real-fluid model (RFM) developed in the CONVERGE CFD solver.","In the RFM model, the thermodynamic and transport properties are stored in a table which is used during the run-time.","The thermodynamic table is generated using the in-house Carnot thermodynamic library based on vapor-liquid equilibrium calculations coupled with a real-fluid equation of state.","The RFM model allows to consider the effects of the dissolved non-condensable gas such as nitrogen on the phase change process.","The obtained numerical results have confirmed that the model can tackle the phase transition phenomenon under the considered conditions.","In contrast to previous numerical studies of the cavitation phenomenon using hydrocarbon fuels, the formed cavitation pockets were found to be primarily composed of ammonia vapor due to its high vapor pressure, with minimal contribution of the dissolved non-condensable nitrogen."],"url":"http://arxiv.org/abs/2404.02500v1","category":"physics.flu-dyn"}
{"created":"2024-04-03 06:25:42","title":"Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains","abstract":"General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.","sentences":["General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain.","Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains.","In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains.","We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness.","The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well."],"url":"http://arxiv.org/abs/2404.02499v1","category":"cs.AI"}
{"created":"2024-04-03 06:24:21","title":"From Time-inconsistency to Time-consistency for Optimal Stopping Problems","abstract":"For optimal stopping problems with time-inconsistent preference, we measure the inherent level of time-inconsistency by taking the time needed to turn the naive strategies into the sophisticated ones. In particular, when in a repeated experiment the naive agent can observe her actual sequence of actions which are inconsistent with what she has planned at the initial time, she then chooses her immediate action based on the observations on her later actual behavior. The procedure is repeated until her actual sequence of actions are consistent with her plan at any time. We show that for the preference value of cumulative prospect theory, in which the time-inconsistency is due to the probability distortion, the higher the degree of probability distortion, the more severe the level of time-inconsistency, and the more time required to turn the naive strategies into the sophisticated ones.","sentences":["For optimal stopping problems with time-inconsistent preference, we measure the inherent level of time-inconsistency by taking the time needed to turn the naive strategies into the sophisticated ones.","In particular, when in a repeated experiment the naive agent can observe her actual sequence of actions which are inconsistent with what she has planned at the initial time, she then chooses her immediate action based on the observations on her later actual behavior.","The procedure is repeated until her actual sequence of actions are consistent with her plan at any time.","We show that for the preference value of cumulative prospect theory, in which the time-inconsistency is due to the probability distortion, the higher the degree of probability distortion, the more severe the level of time-inconsistency, and the more time required to turn the naive strategies into the sophisticated ones."],"url":"http://arxiv.org/abs/2404.02498v1","category":"econ.GN"}
{"created":"2024-04-03 06:24:13","title":"Enhancing Educational Outcome with Machine Learning: Modeling Friendship Formation, Measuring Peer Effect and Optimizing Class Assignment","abstract":"In this paper, we look at a school principal's class assignment problem. We break the problem into three stages (1) friendship prediction (2) peer effect estimation (3) class assignment optimization. We build a micro-founded model for friendship formation and approximate the model as a neural network. Leveraging on the predicted friendship probability adjacent matrix, we improve the traditional linear-in-means model and estimate peer effect. We propose a new instrument to address the friendship selection endogeneity. The estimated peer effect is slightly larger than the linear-in-means model estimate. Using the friendship prediction and peer effect estimation results, we simulate counterfactual peer effects for all students. We find that dividing students into gendered classrooms increases average peer effect by 0.02 point on a scale of 5. We also find that extreme mixing class assignment method improves bottom quartile students' peer effect by 0.08 point.","sentences":["In this paper, we look at a school principal's class assignment problem.","We break the problem into three stages (1) friendship prediction (2) peer effect estimation (3) class assignment optimization.","We build a micro-founded model for friendship formation and approximate the model as a neural network.","Leveraging on the predicted friendship probability adjacent matrix, we improve the traditional linear-in-means model and estimate peer effect.","We propose a new instrument to address the friendship selection endogeneity.","The estimated peer effect is slightly larger than the linear-in-means model estimate.","Using the friendship prediction and peer effect estimation results, we simulate counterfactual peer effects for all students.","We find that dividing students into gendered classrooms increases average peer effect by 0.02 point on a scale of 5.","We also find that extreme mixing class assignment method improves bottom quartile students' peer effect by 0.08 point."],"url":"http://arxiv.org/abs/2404.02497v1","category":"econ.GN"}
{"created":"2024-04-03 06:23:41","title":"On the association of GW190425 with its potential electromagnetic counterpart FRB 20190425A","abstract":"Recent work by Moroianu et al. (2022) has suggested that the binary neutron star (BNS) merger GW190425 might have a potential fast radio burst (FRB) counterpart association, FRB 20190425A, at the 2.8$\\sigma$ level of confidence with a likely host galaxy association, namely UGC10667. The authors argue that the observations are consistent with a long-lived hypermassive neutron star (HMNS) that formed promptly after the BNS merger and was stable for approximately 2.5 hours before promptly collapsing into a black hole. The HMNS remnant is required to be a highly spinning magnetar, and due to its imminent collapse, its ejected magnetosphere potentially led to the observed FRB emission. Recently, Bhardwaj et al. (2023) conclusively associated FRB 20190425A with UGC10667, potentially providing a direct host galaxy candidate for GW190425. In this work, we examine the multi-messenger association based on the space-time localization overlaps between GW190425 and the FRB host galaxy UGC10667 and find that the odds for a coincident association are $\\mathcal{O}(10)$. We validate this estimate by using a Gaussian Process (GP) density estimator. Assuming that the association is indeed real, we then perform Bayesian parameter estimation on GW190425 assuming that the BNS event took place in \\GAL. We find that the viewing angle of GW190425 excludes an on-axis system at $p(\\theta_v>30^o)\\approx99.99$\\%, highly favouring an off-axis system similar to GRB 170817A. We also find a slightly higher source frame total mass for the binary, namely, $m_{\\rm{total}} = 3.42^{+0.34}_{-0.11} M_{\\odot}$, leading to an increase on the probability of prompt collapse into a black hole and therefore disfavors the long-lived HMNS formation scenario.","sentences":["Recent work by Moroianu et al. (2022) has suggested that the binary neutron star (BNS) merger GW190425 might have a potential fast radio burst (FRB) counterpart association, FRB 20190425A, at the 2.8$\\sigma$ level of confidence with a likely host galaxy association, namely UGC10667.","The authors argue that the observations are consistent with a long-lived hypermassive neutron star (HMNS) that formed promptly after the BNS merger and was stable for approximately 2.5 hours before promptly collapsing into a black hole.","The HMNS remnant is required to be a highly spinning magnetar, and due to its imminent collapse, its ejected magnetosphere potentially led to the observed FRB emission.","Recently, Bhardwaj et al. (2023) conclusively associated FRB 20190425A with UGC10667, potentially providing a direct host galaxy candidate for GW190425.","In this work, we examine the multi-messenger association based on the space-time localization overlaps between GW190425 and the FRB host galaxy UGC10667 and find that the odds for a coincident association are $\\mathcal{O}(10)$. We validate this estimate by using a Gaussian Process (GP) density estimator.","Assuming that the association is indeed real, we then perform Bayesian parameter estimation on GW190425 assuming that the BNS event took place in \\GAL.","We find that the viewing angle of GW190425 excludes an on-axis system at $p(\\theta_v>30^o)\\approx99.99$\\%, highly favouring an off-axis system similar to GRB 170817A. We also find a slightly higher source frame total mass for the binary, namely, $m_{\\rm{total}} = 3.42^{+0.34}_{-0.11} M_{\\odot}$, leading to an increase on the probability of prompt collapse into a black hole and therefore disfavors the long-lived HMNS formation scenario."],"url":"http://arxiv.org/abs/2404.02496v1","category":"astro-ph.HE"}
{"created":"2024-04-03 06:01:50","title":"Enhanced Curvature Perturbation and Primordial Black Hole Formation in Two-stage Inflation with a break","abstract":"We investigate a model of $R^2$-gravity with a non-minimally coupled scalar field that gives rise to two-stage inflation with a break, that is, with an intermediate stage where inflation momentarily halts. We find that the power spectrum of the primordial curvature perturbation is significantly enhanced at the break scale, which can account for the primordial black hole (PBH) formation, without affecting the CMB constraint on large scales. The behavior of the curvature perturbation is carefully analyzed and we find a few notable new features in the spectrum. In particular, we find that the $k^3$ growth of the spectrum of toward the end of the first stage of inflation. We argue that this is a universal feature common to all two-stage models where the field dominating the second stage is heavy during the first stage. By appropriately tuning the model parameters, we find that our model can realize the scenario of PBHs as the cold dark matter of the Universe. We also find that we can choose the parameters so that the spectrum of the induced gravitational waves from the enhanced curvature perturbation fits the NANOGrav-15yr data of pulsar timing array observation.","sentences":["We investigate a model of $R^2$-gravity with a non-minimally coupled scalar field that gives rise to two-stage inflation with a break, that is, with an intermediate stage where inflation momentarily halts.","We find that the power spectrum of the primordial curvature perturbation is significantly enhanced at the break scale, which can account for the primordial black hole (PBH) formation, without affecting the CMB constraint on large scales.","The behavior of the curvature perturbation is carefully analyzed and we find a few notable new features in the spectrum.","In particular, we find that the $k^3$ growth of the spectrum of toward the end of the first stage of inflation.","We argue that this is a universal feature common to all two-stage models where the field dominating the second stage is heavy during the first stage.","By appropriately tuning the model parameters, we find that our model can realize the scenario of PBHs as the cold dark matter of the Universe.","We also find that we can choose the parameters so that the spectrum of the induced gravitational waves from the enhanced curvature perturbation fits the NANOGrav-15yr data of pulsar timing array observation."],"url":"http://arxiv.org/abs/2404.02492v1","category":"astro-ph.CO"}
{"created":"2024-04-03 05:58:57","title":"Measuring Social Norms of Large Language Models","abstract":"We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements.","sentences":["We present a new challenge to examine whether large language models understand social norms.","In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve.","Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws.","We design our dataset according to the K-12 curriculum.","This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students.","While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance.","We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.","This method further improves large language models to be on par with humans.","Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements."],"url":"http://arxiv.org/abs/2404.02491v1","category":"cs.CL"}
{"created":"2024-04-03 05:50:42","title":"DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation","abstract":"State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method's performance and to identify promising areas for further improvements.","sentences":["State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot.","However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information.","Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming.","To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain.","Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters.","Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets.","We complement our results with a thorough analysis for more in-depth understanding of the proposed method's performance and to identify promising areas for further improvements."],"url":"http://arxiv.org/abs/2404.02489v1","category":"cs.IR"}
{"created":"2024-04-03 05:44:03","title":"New methods for drug synergy prediction","abstract":"In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.","sentences":["In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens.","The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques.","We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with.","Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level."],"url":"http://arxiv.org/abs/2404.02484v1","category":"cs.LG"}
{"created":"2024-04-03 05:43:06","title":"Refined canonical stable Grothendieck polynomials and their duals, Part 2","abstract":"This paper is the sequel of the paper under the same title with part 1, where we introduced refined canonical stable Grothendieck polynomials and their duals with two families of infinite parameters. In this paper we give combinatorial interpretations for these polynomials using generalizations of set-valued tableaux and reverse plane partitions, respectively. Our results extend to their flagged and skew versions.","sentences":["This paper is the sequel of the paper under the same title with part 1, where we introduced refined canonical stable Grothendieck polynomials and their duals with two families of infinite parameters.","In this paper we give combinatorial interpretations for these polynomials using generalizations of set-valued tableaux and reverse plane partitions, respectively.","Our results extend to their flagged and skew versions."],"url":"http://arxiv.org/abs/2404.02483v1","category":"math.CO"}
{"created":"2024-04-03 05:40:01","title":"Noninteracting particles in a harmonic trap with a stochastically driven center","abstract":"We study a system of $N$ noninteracting particles on a line in the presence of a harmonic trap $U(x)=\\mu \\bigl[x-z(t)\\bigr]^2/2$, where the trap center $z(t)$ undergoes a bounded stochastic modulation. We show that this stochastic modulation drives the system into a nonequilibrium stationary state, where the joint distribution of the positions of the particles is not factorizable. This indicates strong correlations between the positions of the particles that are not inbuilt, but rather get generated by the dynamics itself. Moreover, we show that the stationary joint distribution can be fully characterized and has a special conditionally independent and identically distributed (CIID) structure. This special structure allows us to compute several observables analytically even in such a strongly correlated system, for an arbitrary bounded drive $z(t)$. These observables include the average density profile, the correlations between particle positions, the order and gap statistics, as well as the full counting statistics. We then apply our general results to two specific examples where (i) $z(t)$ represents a dichotomous telegraphic noise, and (ii) $z(t)$ represents an Ornstein-Uhlenbeck process. Our analytical predictions are verified in numerical simulations, finding excellent agreement.","sentences":["We study a system of $N$ noninteracting particles on a line in the presence of a harmonic trap $U(x)=\\mu \\bigl[x-z(t)\\bigr]^2/2$, where the trap center $z(t)$ undergoes a bounded stochastic modulation.","We show that this stochastic modulation drives the system into a nonequilibrium stationary state, where the joint distribution of the positions of the particles is not factorizable.","This indicates strong correlations between the positions of the particles that are not inbuilt, but rather get generated by the dynamics itself.","Moreover, we show that the stationary joint distribution can be fully characterized and has a special conditionally independent and identically distributed (CIID) structure.","This special structure allows us to compute several observables analytically even in such a strongly correlated system, for an arbitrary bounded drive $z(t)$. These observables include the average density profile, the correlations between particle positions, the order and gap statistics, as well as the full counting statistics.","We then apply our general results to two specific examples where (i) $z(t)$ represents a dichotomous telegraphic noise, and (ii) $z(t)$ represents an Ornstein-Uhlenbeck process.","Our analytical predictions are verified in numerical simulations, finding excellent agreement."],"url":"http://arxiv.org/abs/2404.02480v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 05:37:52","title":"Axial perturbations of hairy black holes in generalised scalar-tensor theories","abstract":"Gravitational wave observations can test the validity of General Relativity (GR) in the strong field regime. Certain classes of scalar-tensor theories indeed predict that compact objects can exhibit significant deviations from their GR counterparts. Here we explore the quasinormal modes of axial perturbations in spherically symmetric black holes in three such classes: (i) dilatonic black holes with an additional scalar-Ricci coupling (EdRGB), (ii) spontaneously scalarized black holes (EsRGB) with a quadratic coupling to the Gauss-Bonnet invariant and the Ricci scalar, (iii) spontaneously scalarized black holes with a quadratic and a quartic coupling to the Gauss-Bonnet invariant.","sentences":["Gravitational wave observations can test the validity of General Relativity (GR) in the strong field regime.","Certain classes of scalar-tensor theories indeed predict that compact objects can exhibit significant deviations from their GR counterparts.","Here we explore the quasinormal modes of axial perturbations in spherically symmetric black holes in three such classes: (i) dilatonic black holes with an additional scalar-Ricci coupling (EdRGB), (ii) spontaneously scalarized black holes (EsRGB) with a quadratic coupling to the Gauss-Bonnet invariant and the Ricci scalar, (iii) spontaneously scalarized black holes with a quadratic and a quartic coupling to the Gauss-Bonnet invariant."],"url":"http://arxiv.org/abs/2404.02479v1","category":"gr-qc"}
{"created":"2024-04-03 05:36:21","title":"FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning","abstract":"Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients' local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network. However, preselecting network layers for personalization may result in suboptimal storage of global knowledge. In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis. FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining parameters. This approach enables the personalization of both client parameters and subnetwork structure during the training process. Finally, we show that FedSelect outperforms recent state-of-the-art PFL algorithms under challenging client data heterogeneity settings and demonstrates robustness to various real-world distributional shifts. Our code is available at https://github.com/lapisrocks/fedselect.","sentences":["Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity.","Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients' local data distributions.","Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network.","However, preselecting network layers for personalization may result in suboptimal storage of global knowledge.","In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis.","FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining parameters.","This approach enables the personalization of both client parameters and subnetwork structure during the training process.","Finally, we show that FedSelect outperforms recent state-of-the-art PFL algorithms under challenging client data heterogeneity settings and demonstrates robustness to various real-world distributional shifts.","Our code is available at https://github.com/lapisrocks/fedselect."],"url":"http://arxiv.org/abs/2404.02478v1","category":"cs.LG"}
{"created":"2024-04-03 05:34:32","title":"Enhancing Sum-Rate Performance in Constrained Multicell Networks: A Low-Information Exchange Approach","abstract":"Despite the extensive research on massive MIMO systems for 5G telecommunications and beyond, the reality is that many deployed base stations are equipped with a limited number of antennas rather than supporting massive MIMO configurations. Furthermore, while the cell-less network concept, which eliminates cell boundaries, is under investigation, practical deployments often grapple with significantly limited backhaul connection capacities between base stations. This letter explores techniques to maximize the sum-rate performance within the constraints of these more realistically equipped multicell networks. We propose an innovative approach that dramatically reduces the need for information exchange between base stations to a mere few bits, in stark contrast to conventional methods that require the exchange of hundreds of bits. Our proposed method not only addresses the limitations imposed by current network infrastructure but also showcases significantly improved performance under these constrained conditions.","sentences":["Despite the extensive research on massive MIMO systems for 5G telecommunications and beyond, the reality is that many deployed base stations are equipped with a limited number of antennas rather than supporting massive MIMO configurations.","Furthermore, while the cell-less network concept, which eliminates cell boundaries, is under investigation, practical deployments often grapple with significantly limited backhaul connection capacities between base stations.","This letter explores techniques to maximize the sum-rate performance within the constraints of these more realistically equipped multicell networks.","We propose an innovative approach that dramatically reduces the need for information exchange between base stations to a mere few bits, in stark contrast to conventional methods that require the exchange of hundreds of bits.","Our proposed method not only addresses the limitations imposed by current network infrastructure but also showcases significantly improved performance under these constrained conditions."],"url":"http://arxiv.org/abs/2404.02477v1","category":"eess.SP"}
{"created":"2024-04-03 05:32:10","title":"Deep Reinforcement Learning for Traveling Purchaser Problems","abstract":"The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently construct the route using the policy network, and once the route is determined, the associated purchasing plan can be easily derived through linear programming, while, leveraging DRL, we can train the policy network to optimize the global solution objective. Furthermore, by introducing a meta-learning strategy, the policy network can be trained stably on large-sized TPP instances, and generalize well across instances of varying sizes and distributions, even to much larger instances that are never seen during training. Experiments on various synthetic TPP instances and the TPPLIB benchmark demonstrate that our DRL-based approach can significantly outperform well-established TPP heuristics, reducing the optimality gap by 40%-90%, and also showing an advantage in runtime, especially on large-sized instances.","sentences":["The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications.","Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance.","In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective.","The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route.","One significant benefit of our framework is that we can efficiently construct the route using the policy network, and once the route is determined, the associated purchasing plan can be easily derived through linear programming, while, leveraging DRL, we can train the policy network to optimize the global solution objective.","Furthermore, by introducing a meta-learning strategy, the policy network can be trained stably on large-sized TPP instances, and generalize well across instances of varying sizes and distributions, even to much larger instances that are never seen during training.","Experiments on various synthetic TPP instances and the TPPLIB benchmark demonstrate that our DRL-based approach can significantly outperform well-established TPP heuristics, reducing the optimality gap by 40%-90%, and also showing an advantage in runtime, especially on large-sized instances."],"url":"http://arxiv.org/abs/2404.02476v1","category":"math.OC"}
{"created":"2024-04-03 05:32:05","title":"PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts","abstract":"Robotic Process Automation (RPA) offers a valuable solution for efficiently automating tasks on the graphical user interface (GUI), by emulating human interactions, without modifying existing code. However, its broader adoption is constrained by the need for expertise in both scripting languages and workflow design. To address this challenge, we present PromptRPA, a system designed to comprehend various task-related textual prompts (e.g., goals, procedures), thereby generating and performing corresponding RPA tasks. PromptRPA incorporates a suite of intelligent agents that mimic human cognitive functions, specializing in interpreting user intent, managing external information for RPA generation, and executing operations on smartphones. The agents can learn from user feedback and continuously improve their performance based on the accumulated knowledge. Experimental results indicated a performance jump from a 22.28% success rate in the baseline to 95.21% with PromptRPA, requiring an average of 1.66 user interventions for each new task. PromptRPA presents promising applications in fields such as tutorial creation, smart assistance, and customer service.","sentences":["Robotic Process Automation (RPA) offers a valuable solution for efficiently automating tasks on the graphical user interface (GUI), by emulating human interactions, without modifying existing code.","However, its broader adoption is constrained by the need for expertise in both scripting languages and workflow design.","To address this challenge, we present PromptRPA, a system designed to comprehend various task-related textual prompts (e.g., goals, procedures), thereby generating and performing corresponding RPA tasks.","PromptRPA incorporates a suite of intelligent agents that mimic human cognitive functions, specializing in interpreting user intent, managing external information for RPA generation, and executing operations on smartphones.","The agents can learn from user feedback and continuously improve their performance based on the accumulated knowledge.","Experimental results indicated a performance jump from a 22.28% success rate in the baseline to 95.21% with PromptRPA, requiring an average of 1.66 user interventions for each new task.","PromptRPA presents promising applications in fields such as tutorial creation, smart assistance, and customer service."],"url":"http://arxiv.org/abs/2404.02475v1","category":"cs.HC"}
{"created":"2024-04-03 05:31:59","title":"uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?","abstract":"Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.","sentences":["Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box.","Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability.","Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline.","Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta.","We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality.","Findings indicate that compressed informative prompts enhance performance.","Dynamic in-context learning enhances model performance significantly.","Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking."],"url":"http://arxiv.org/abs/2404.02474v1","category":"cs.CL"}
{"created":"2024-04-03 05:28:44","title":"Safe Returning FaSTrack with Robust Control Lyapunov-Value Functions","abstract":"Real-time navigation in a priori unknown environment remains a challenging task, especially when an unexpected (unmodeled) disturbance occurs. In this paper, we propose the framework Safe Returning Fast and Safe Tracking (SR-F) that merges concepts from 1) Robust Control Lyapunov-Value Functions (R-CLVF), and 2) the Fast and Safe Tracking (FaSTrack) framework. The SR-F computes an R-CLVF offline between a model of the true system and a simplified planning model. Online, a planning algorithm is used to generate a trajectory in the simplified planning space, and the R-CLVF is used to provide a tracking controller that exponentially stabilizes to the planning model. When an unexpected disturbance occurs, the proposed SR-F algorithm provides a means for the true system to recover to the planning model. We take advantage of this mechanism to induce an artificial disturbance by ``jumping'' the planning model in open environments, forcing faster navigation. Therefore, this algorithm can both reject unexpected true disturbances and accelerate navigation speed. We validate our framework using a 10D quadrotor system and show that SR-F is empirically 20\\% faster than the original FaSTrack while maintaining safety.","sentences":["Real-time navigation in a priori unknown environment remains a challenging task, especially when an unexpected (unmodeled) disturbance occurs.","In this paper, we propose the framework Safe Returning Fast and Safe Tracking (SR-F) that merges concepts from 1) Robust Control Lyapunov-Value Functions (R-CLVF), and 2) the Fast and Safe Tracking (FaSTrack) framework.","The SR-F computes an R-CLVF offline between a model of the true system and a simplified planning model.","Online, a planning algorithm is used to generate a trajectory in the simplified planning space, and the R-CLVF is used to provide a tracking controller that exponentially stabilizes to the planning model.","When an unexpected disturbance occurs, the proposed SR-F algorithm provides a means for the true system to recover to the planning model.","We take advantage of this mechanism to induce an artificial disturbance by ``jumping'' the planning model in open environments, forcing faster navigation.","Therefore, this algorithm can both reject unexpected true disturbances and accelerate navigation speed.","We validate our framework using a 10D quadrotor system and show that SR-F is empirically 20\\% faster than the original FaSTrack while maintaining safety."],"url":"http://arxiv.org/abs/2404.02472v1","category":"cs.RO"}
{"created":"2024-04-03 05:13:23","title":"SSwsrNet: A Semi-Supervised Few-Shot Learning Framework for Wireless Signal Recognition","abstract":"Wireless signal recognition (WSR) is crucial in modern and future wireless communication networks since it aims to identify properties of the received signal. Although many deep learning-based WSR models have been developed, they still rely on a large amount of labeled training data. Thus, they cannot tackle the few-sample problem in the practically and dynamically changing wireless communication environment. To overcome this challenge, a novel SSwsrNet framework is proposed by using the deep residual shrinkage network (DRSN) and semi-supervised learning. The DRSN can learn discriminative features from noisy signals. Moreover, a modular semi-supervised learning method that combines labeled and unlabeled data using MixMatch is exploited to further improve the classification performance under few-sample conditions. Extensive simulation results on automatic modulation classification (AMC) and wireless technology classification (WTC) demonstrate that our proposed WSR scheme can achieve better performance than the benchmark schemes in terms of classification accuracy. This novel method enables more robust and adaptive signal recognition for next-generation wireless networks.","sentences":["Wireless signal recognition (WSR) is crucial in modern and future wireless communication networks since it aims to identify properties of the received signal.","Although many deep learning-based WSR models have been developed, they still rely on a large amount of labeled training data.","Thus, they cannot tackle the few-sample problem in the practically and dynamically changing wireless communication environment.","To overcome this challenge, a novel SSwsrNet framework is proposed by using the deep residual shrinkage network (DRSN) and semi-supervised learning.","The DRSN can learn discriminative features from noisy signals.","Moreover, a modular semi-supervised learning method that combines labeled and unlabeled data using MixMatch is exploited to further improve the classification performance under few-sample conditions.","Extensive simulation results on automatic modulation classification (AMC) and wireless technology classification (WTC) demonstrate that our proposed WSR scheme can achieve better performance than the benchmark schemes in terms of classification accuracy.","This novel method enables more robust and adaptive signal recognition for next-generation wireless networks."],"url":"http://arxiv.org/abs/2404.02467v1","category":"eess.SP"}
{"created":"2024-04-03 17:59:59","title":"Hitting the Thermal Target for Leptophilic Dark Matter","abstract":"We study future lepton collider prospects for testing predictive models of leptophilic dark matter candidates with a thermal origin. We calculate experimental milestones for testing the parameter space compatible with freeze-out and the associated collider signals at past, present, and future facilities. This analysis places new limits on such models by leveraging the utility of lepton colliders. At $e^+e^-$ machines, we make projections using precision $Z$-pole observables from $e^+e^-\\to l^+l^- + $ missing energy signatures at LEP and future projections for FCC-ee in these channels. Additionally, a muon collider could also probe new thermal relic parameter space in this scenario via $\\mu^+\\mu^- \\to X + $ missing energy where $X$ is any easy identifiable SM object. Collectively, these processes can probe much all of the parameter space for which DM direct annihilation to $l^+l^-$ yields the observed relic density in Higgs-like models with mass-proportional couplings to charged leptons.","sentences":["We study future lepton collider prospects for testing predictive models of leptophilic dark matter candidates with a thermal origin.","We calculate experimental milestones for testing the parameter space compatible with freeze-out and the associated collider signals at past, present, and future facilities.","This analysis places new limits on such models by leveraging the utility of lepton colliders.","At $e^+e^-$ machines, we make projections using precision $Z$-pole observables from $e^+e^-\\to l^+l^- + $ missing energy signatures at LEP and future projections for FCC-ee in these channels.","Additionally, a muon collider could also probe new thermal relic parameter space in this scenario via $\\mu^+\\mu^- \\to X + $ missing energy where $X$ is any easy identifiable SM object.","Collectively, these processes can probe much all of the parameter space for which DM direct annihilation to $l^+l^-$ yields the observed relic density in Higgs-like models with mass-proportional couplings to charged leptons."],"url":"http://arxiv.org/abs/2404.02906v1","category":"hep-ph"}
{"created":"2024-04-03 16:19:47","title":"AI-augmented Automation for Real Driving Prediction: an Industrial Use Case","abstract":"The risen complexity of automotive systems requires new development strategies and methods to master the upcoming challenges. Traditional methods need thus to be changed by an increased level of automation, and a faster continuous improvement cycle. In this context, current vehicle performance tests represent a very time-consuming and expensive task due to the need to perform the tests in real driving conditions. As a consequence, agile/iterative processes like DevOps are largely hindered by the necessity of triggering frequent tests. This paper reports on a practical experience of developing an AI-augmented solution based on Machine Learning and Model-based Engineering to support continuous vehicle development and testing. In particular, historical data collected in real driving conditions is leveraged to synthesize a high-fidelity driving simulator and hence enable performance tests in virtual environments. Based on this practical experience, this paper also proposes a conceptual framework to support predictions based on real driving behavior.","sentences":["The risen complexity of automotive systems requires new development strategies and methods to master the upcoming challenges.","Traditional methods need thus to be changed by an increased level of automation, and a faster continuous improvement cycle.","In this context, current vehicle performance tests represent a very time-consuming and expensive task due to the need to perform the tests in real driving conditions.","As a consequence, agile/iterative processes like DevOps are largely hindered by the necessity of triggering frequent tests.","This paper reports on a practical experience of developing an AI-augmented solution based on Machine Learning and Model-based Engineering to support continuous vehicle development and testing.","In particular, historical data collected in real driving conditions is leveraged to synthesize a high-fidelity driving simulator and hence enable performance tests in virtual environments.","Based on this practical experience, this paper also proposes a conceptual framework to support predictions based on real driving behavior."],"url":"http://arxiv.org/abs/2404.02841v1","category":"cs.SE"}
{"created":"2024-04-03 15:55:27","title":"Identifying Climate Targets in National Laws and Policies using Machine Learning","abstract":"Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problematic features. Finally, we investigate the characteristics of the dataset produced by running this classifier on the Climate Policy Radar (CPR) dataset of global national climate laws and policies and UNFCCC submissions, highlighting the potential of automated and scalable data collection for existing climate policy databases and supporting further research. Our work represents a significant upgrade in the accessibility of these key climate policy elements for policymakers and researchers. We publish our model at \\url{https://huggingface.co/ClimatePolicyRadar/national-climate-targets} and related dataset at \\url{https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets}.","sentences":["Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language.","Current methods for curating comprehensive views of global climate policy targets entail significant manual effort.","At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions.","In this paper we present an approach for extracting mentions of climate targets from national laws and policies.","We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text.","We investigate bias and equity impacts related to our model and identify specific years and country names as problematic features.","Finally, we investigate the characteristics of the dataset produced by running this classifier on the Climate Policy Radar (CPR) dataset of global national climate laws and policies and UNFCCC submissions, highlighting the potential of automated and scalable data collection for existing climate policy databases and supporting further research.","Our work represents a significant upgrade in the accessibility of these key climate policy elements for policymakers and researchers.","We publish our model at \\url{https://huggingface.co/ClimatePolicyRadar/national-climate-targets} and related dataset at \\url{https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets}."],"url":"http://arxiv.org/abs/2404.02822v1","category":"cs.CY"}
{"created":"2024-04-03 15:37:07","title":"Anyonic quantum multipartite maskers in the Kitaev model","abstract":"The structure of quantum mechanics forbids a bipartite scenario for masking quantum information, however, it allows multipartite maskers. The Latin squares are found to be closely related to a series of tripartite maskers. This adds another item, significantly different from the original no-cloning theorem, to the no-go theorems. On the other hand, anyonic excitations in two dimensions exhibit exotic collective behaviors of quantum physics, and open the avenue of fault-tolerant topological quantum computing. Here, we give the Latin-square construction of Abelian and Ising anyons %of in the Kitaev model and study the maskable space configuration in anyonic space. The circling and braiding of Kitaev anyons are masking operations on extended hyperdisks in anyonic space. We also realize quantum information masking in a teleportation way in the Kitaev Ising anyon model.","sentences":["The structure of quantum mechanics forbids a bipartite scenario for masking quantum information, however, it allows multipartite maskers.","The Latin squares are found to be closely related to a series of tripartite maskers.","This adds another item, significantly different from the original no-cloning theorem, to the no-go theorems.","On the other hand, anyonic excitations in two dimensions exhibit exotic collective behaviors of quantum physics, and open the avenue of fault-tolerant topological quantum computing.","Here, we give the Latin-square construction of Abelian and Ising anyons %of in the Kitaev model and study the maskable space configuration in anyonic space.","The circling and braiding of Kitaev anyons are masking operations on extended hyperdisks in anyonic space.","We also realize quantum information masking in a teleportation way in the Kitaev Ising anyon model."],"url":"http://arxiv.org/abs/2404.02814v1","category":"quant-ph"}
{"created":"2024-04-03 14:47:48","title":"Chain event graphs for assessing activity-level propositions in forensic science in relation to drug traces on banknotes","abstract":"Graphical models and likelihood ratios can be used by forensic scientists to compare support given by evidence to propositions put forward by competing parties during court proceedings. Such models can also be used to evaluate support for activity-level propositions, i.e. propositions that refer to the nature of activities associated with evidence and how this evidence came to be at a crime scene. Graphical methods can be used to show explicitly different scenarios that might explain the evidence in a case and to distinguish between evidence requiring evaluation by a jury and quantifiable evidence from the crime scene. Such visual representations can be helpful for forensic practitioners, the police and lawyers who may need to assess the value that different pieces of evidence make to their arguments in a case. In this paper we demonstrate for the first time how chain event graphs can be applied to a criminal case involving drug trafficking. We show how different types of evidence (i.e. expert judgement and data collected from a crime scene) can be combined using a chain event graph and show how the hierarchical model deriving from the graph can be used to evaluate the degree of support for different activity-level propositions in the case. We also develop a modification of the standard chain event graph to simplify their use in forensic applications.","sentences":["Graphical models and likelihood ratios can be used by forensic scientists to compare support given by evidence to propositions put forward by competing parties during court proceedings.","Such models can also be used to evaluate support for activity-level propositions, i.e. propositions that refer to the nature of activities associated with evidence and how this evidence came to be at a crime scene.","Graphical methods can be used to show explicitly different scenarios that might explain the evidence in a case and to distinguish between evidence requiring evaluation by a jury and quantifiable evidence from the crime scene.","Such visual representations can be helpful for forensic practitioners, the police and lawyers who may need to assess the value that different pieces of evidence make to their arguments in a case.","In this paper we demonstrate for the first time how chain event graphs can be applied to a criminal case involving drug trafficking.","We show how different types of evidence (i.e. expert judgement and data collected from a crime scene) can be combined using a chain event graph and show how the hierarchical model deriving from the graph can be used to evaluate the degree of support for different activity-level propositions in the case.","We also develop a modification of the standard chain event graph to simplify their use in forensic applications."],"url":"http://arxiv.org/abs/2404.02778v1","category":"stat.AP"}
{"created":"2024-04-03 14:47:48","title":"Federated Computing -- Survey on Building Blocks, Extensions and Systems","abstract":"In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles. Federated Computing (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy. This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations. The motivation behind FC extends beyond technical considerations to encompass societal implications. As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty. FC comprises of Federated Learning (FL) and Federated Analytics (FA). FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces. Current surveys capture domain-specific FL use cases, describe individual components in an FC pipeline individually or decoupled from each other, or provide a quantitative overview of the number of published papers. This work surveys more than 150 papers to distill the underlying structure of FC systems with their basic building blocks, extensions, architecture, environment, and motivation. We capture FL and FA systems individually and point out unique difference between those two.","sentences":["In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles.","Federated Computing (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy.","This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations.","The motivation behind FC extends beyond technical considerations to encompass societal implications.","As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty.","FC comprises of Federated Learning (FL) and Federated Analytics (FA).","FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces.","Current surveys capture domain-specific FL use cases, describe individual components in an FC pipeline individually or decoupled from each other, or provide a quantitative overview of the number of published papers.","This work surveys more than 150 papers to distill the underlying structure of FC systems with their basic building blocks, extensions, architecture, environment, and motivation.","We capture FL and FA systems individually and point out unique difference between those two."],"url":"http://arxiv.org/abs/2404.02779v1","category":"cs.LG"}
{"created":"2024-04-03 13:41:27","title":"Statistical mechanics and pressure of composite multimoded weakly nonlinear optical systems","abstract":"Statistical mechanics can provide a versatile theoretical framework for investigating the collective dynamics of weakly nonlinear waves-settings that can be utterly complex to describe otherwise. In optics, composite systems arise due to interactions between different frequencies and/or polarizations. The purpose of this work is to develop a thermodynamic theory that takes into account the synergistic action of multiple components. We find that the type of the nonlinearity involved can have important implications in the thermalization process and, hence, can lead to different thermal equilibrium conditions. Importantly, we derive closed-form expressions for the actual optomechanical pressure that is exerted on the system. In particular, the total optomechanical pressure is the sum of the partial pressures due to each component. Our results can be applied to a variety of weakly nonlinear optical settings such as multimode fibers, bulk waveguides, photonic lattices, and coupled microresonators. We present two specific examples, where two colors interact in a waveguide array with either a cubic or quadratic nonlinearity.","sentences":["Statistical mechanics can provide a versatile theoretical framework for investigating the collective dynamics of weakly nonlinear waves-settings that can be utterly complex to describe otherwise.","In optics, composite systems arise due to interactions between different frequencies and/or polarizations.","The purpose of this work is to develop a thermodynamic theory that takes into account the synergistic action of multiple components.","We find that the type of the nonlinearity involved can have important implications in the thermalization process and, hence, can lead to different thermal equilibrium conditions.","Importantly, we derive closed-form expressions for the actual optomechanical pressure that is exerted on the system.","In particular, the total optomechanical pressure is the sum of the partial pressures due to each component.","Our results can be applied to a variety of weakly nonlinear optical settings such as multimode fibers, bulk waveguides, photonic lattices, and coupled microresonators.","We present two specific examples, where two colors interact in a waveguide array with either a cubic or quadratic nonlinearity."],"url":"http://arxiv.org/abs/2404.02745v1","category":"physics.optics"}
{"created":"2024-04-03 13:08:26","title":"ART: The Alternating Reading Task Corpus for Speech Entrainment and Imitation","abstract":"We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication. The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three sub-corpora encompassing French-, Italian-, and Slovak-accented English. This design allows systematic investigation of speech entrainment in a controlled and less-spontaneous setting. Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment. Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects.","sentences":["We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication.","The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three sub-corpora encompassing French-, Italian-, and Slovak-accented English.","This design allows systematic investigation of speech entrainment in a controlled and less-spontaneous setting.","Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment.","Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects."],"url":"http://arxiv.org/abs/2404.02710v1","category":"cs.CL"}
{"created":"2024-04-03 12:37:32","title":"Social clustering reinforces external influence on the majority opinion model","abstract":"Public opinion is subject to peer interaction via social networks and external pressures from the media, advertising, and other actors. In this paper, we study the interaction between external and peer influence on the stochastic opinion dynamics of a majority vote model. We introduce a model where agents update their opinions based on the combined influence from their local neighbourhood (peers) and from an external actor in the transition rates. In the first model, the external influence is only felt by agents non-aligned with the external actor ('push strategy'). In the second model, agents are affected by external influence, independently of their opinions ('nudging strategy'). In both cases, the external influence increases the possible macroscopic outcomes. These outcomes are determined by the chosen strategy. We also find that the social network structure affects the opinion dynamics, with high social clustering positively reinforcing the external influence while degree heterogeneity weakens it. These findings are relevant to businesses and policy making, helping to understand how groups of individuals collectively react to external actors.","sentences":["Public opinion is subject to peer interaction via social networks and external pressures from the media, advertising, and other actors.","In this paper, we study the interaction between external and peer influence on the stochastic opinion dynamics of a majority vote model.","We introduce a model where agents update their opinions based on the combined influence from their local neighbourhood (peers) and from an external actor in the transition rates.","In the first model, the external influence is only felt by agents non-aligned with the external actor ('push strategy').","In the second model, agents are affected by external influence, independently of their opinions ('nudging strategy').","In both cases, the external influence increases the possible macroscopic outcomes.","These outcomes are determined by the chosen strategy.","We also find that the social network structure affects the opinion dynamics, with high social clustering positively reinforcing the external influence while degree heterogeneity weakens it.","These findings are relevant to businesses and policy making, helping to understand how groups of individuals collectively react to external actors."],"url":"http://arxiv.org/abs/2404.02689v1","category":"physics.soc-ph"}
{"created":"2024-04-03 10:57:47","title":"SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation","abstract":"This paper aims at achieving fine-grained building attribute segmentation in a cross-view scenario, i.e., using satellite and street-view image pairs. The main challenge lies in overcoming the significant perspective differences between street views and satellite views. In this work, we introduce SG-BEV, a novel approach for satellite-guided BEV fusion for cross-view semantic segmentation. To overcome the limitations of existing cross-view projection methods in capturing the complete building facade features, we innovatively incorporate Bird's Eye View (BEV) method to establish a spatially explicit mapping of street-view features. Moreover, we fully leverage the advantages of multiple perspectives by introducing a novel satellite-guided reprojection module, optimizing the uneven feature distribution issues associated with traditional BEV methods. Our method demonstrates significant improvements on four cross-view datasets collected from multiple cities, including New York, San Francisco, and Boston. On average across these datasets, our method achieves an increase in mIOU by 10.13% and 5.21% compared with the state-of-the-art satellite-based and cross-view methods. The code and datasets of this work will be released at https://github.com/yejy53/SG-BEV.","sentences":["This paper aims at achieving fine-grained building attribute segmentation in a cross-view scenario, i.e., using satellite and street-view image pairs.","The main challenge lies in overcoming the significant perspective differences between street views and satellite views.","In this work, we introduce SG-BEV, a novel approach for satellite-guided BEV fusion for cross-view semantic segmentation.","To overcome the limitations of existing cross-view projection methods in capturing the complete building facade features, we innovatively incorporate Bird's Eye View (BEV) method to establish a spatially explicit mapping of street-view features.","Moreover, we fully leverage the advantages of multiple perspectives by introducing a novel satellite-guided reprojection module, optimizing the uneven feature distribution issues associated with traditional BEV methods.","Our method demonstrates significant improvements on four cross-view datasets collected from multiple cities, including New York, San Francisco, and Boston.","On average across these datasets, our method achieves an increase in mIOU by 10.13% and 5.21% compared with the state-of-the-art satellite-based and cross-view methods.","The code and datasets of this work will be released at https://github.com/yejy53/SG-BEV."],"url":"http://arxiv.org/abs/2404.02638v1","category":"cs.CV"}
{"created":"2024-04-03 08:21:01","title":"Occurrence of the collective Ziman limit of heat transport in cubic semiconductors: scattering channels and size effects","abstract":"In this work, we discuss the possibility of reaching the Ziman conditions for collective heat transport in cubic bulk semiconductors, such as Si, Ge, AlAs and AlP. In natural and enriched silicon and germanium, the collective heat transport limit is impossible to reach due to strong isotopic scattering. However, we show that in hyperenriched silicon and germanium, as well as in materials with one single stable isotope like AlAs and AlP, at low temperatures, normal scattering plays an important role, making the observation of the collective heat transport possible. We further discuss the effects of sample sizes, and analyse our results for cubic materials by comparing them to bulk bismuth, in which second sound has been detected at cryogenic temperatures. We find that collective heat transport in cubic semiconductors studied in this work is expected to occur at temperatures between 10 and 20 K.","sentences":["In this work, we discuss the possibility of reaching the Ziman conditions for collective heat transport in cubic bulk semiconductors, such as Si, Ge, AlAs and AlP.","In natural and enriched silicon and germanium, the collective heat transport limit is impossible to reach due to strong isotopic scattering.","However, we show that in hyperenriched silicon and germanium, as well as in materials with one single stable isotope like AlAs and AlP, at low temperatures, normal scattering plays an important role, making the observation of the collective heat transport possible.","We further discuss the effects of sample sizes, and analyse our results for cubic materials by comparing them to bulk bismuth, in which second sound has been detected at cryogenic temperatures.","We find that collective heat transport in cubic semiconductors studied in this work is expected to occur at temperatures between 10 and 20 K."],"url":"http://arxiv.org/abs/2404.02553v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 07:12:18","title":"Differentially Private Verification of Survey-Weighted Estimates","abstract":"Several official statistics agencies release synthetic data as public use microdata files. In practice, synthetic data do not admit accurate results for every analysis. Thus, it is beneficial for agencies to provide users with feedback on the quality of their analyses of the synthetic data. One approach is to couple synthetic data with a verification server that provides users with measures of the similarity of estimates computed with the synthetic and underlying confidential data. However, such measures leak information about the confidential records, so that agencies may wish to apply disclosure control methods to the released verification measures. We present a verification measure that satisfies differential privacy and can be used when the underlying confidential are collected with a complex survey design. We illustrate the verification measure using repeated sampling simulations where the confidential data are sampled with a probability proportional to size design, and the analyst estimates a population total or mean with the synthetic data. The simulations suggest that the verification measures can provide useful information about the quality of synthetic data inferences.","sentences":["Several official statistics agencies release synthetic data as public use microdata files.","In practice, synthetic data do not admit accurate results for every analysis.","Thus, it is beneficial for agencies to provide users with feedback on the quality of their analyses of the synthetic data.","One approach is to couple synthetic data with a verification server that provides users with measures of the similarity of estimates computed with the synthetic and underlying confidential data.","However, such measures leak information about the confidential records, so that agencies may wish to apply disclosure control methods to the released verification measures.","We present a verification measure that satisfies differential privacy and can be used when the underlying confidential are collected with a complex survey design.","We illustrate the verification measure using repeated sampling simulations where the confidential data are sampled with a probability proportional to size design, and the analyst estimates a population total or mean with the synthetic data.","The simulations suggest that the verification measures can provide useful information about the quality of synthetic data inferences."],"url":"http://arxiv.org/abs/2404.02519v1","category":"cs.CR"}
{"created":"2024-04-03 07:11:19","title":"CPAISD: Core-penumbra acute ischemic stroke dataset","abstract":"We introduce the CPAISD: Core-Penumbra Acute Ischemic Stroke Dataset, aimed at enhancing the early detection and segmentation of ischemic stroke using Non-Contrast Computed Tomography (NCCT) scans. Addressing the challenges in diagnosing acute ischemic stroke during its early stages due to often non-revealing native CT findings, the dataset provides a collection of segmented NCCT images. These include annotations of ischemic core and penumbra regions, critical for developing machine learning models for rapid stroke identification and assessment. By offering a carefully collected and annotated dataset, we aim to facilitate the development of advanced diagnostic tools, contributing to improved patient care and outcomes in stroke management. Our dataset's uniqueness lies in its focus on the acute phase of ischemic stroke, with non-informative native CT scans, and includes a baseline model to demonstrate the dataset's application, encouraging further research and innovation in the field of medical imaging and stroke diagnosis.","sentences":["We introduce the CPAISD: Core-Penumbra Acute Ischemic Stroke Dataset, aimed at enhancing the early detection and segmentation of ischemic stroke using Non-Contrast Computed Tomography (NCCT) scans.","Addressing the challenges in diagnosing acute ischemic stroke during its early stages due to often non-revealing native CT findings, the dataset provides a collection of segmented NCCT images.","These include annotations of ischemic core and penumbra regions, critical for developing machine learning models for rapid stroke identification and assessment.","By offering a carefully collected and annotated dataset, we aim to facilitate the development of advanced diagnostic tools, contributing to improved patient care and outcomes in stroke management.","Our dataset's uniqueness lies in its focus on the acute phase of ischemic stroke, with non-informative native CT scans, and includes a baseline model to demonstrate the dataset's application, encouraging further research and innovation in the field of medical imaging and stroke diagnosis."],"url":"http://arxiv.org/abs/2404.02518v1","category":"eess.IV"}
{"created":"2024-04-03 05:30:14","title":"A fast-rotating blue straggler star in the tidal tail of the open cluster NGC 752","abstract":"NGC 752 is a famous Galactic open cluster of intermediate age. In recent works, a very long and asymmetric tail was newly revealed. A blue straggler star (BSS) at the periphery of the tidal tail of the cluster has been identified subsequently. We aim to perform a detailed analysis of the newly detected BSS based on the available comprehensive spectroscopic and photometric data. We also explored this BSS's possible formation pathway and age limitation based on the collected spectroscopic and photometric data. We estimated the projected rotational velocity $v\\ \\mathrm{sin}i$ and the mass of the BSS from the Large Sky Area Multi-Object Fiber Spectroscopic Telescope low-resolution spectra and multiband photometric data from various catalogs, respectively. The newly discovered BSS is confirmed as a genuine member of NGC 752. The lack of ultraviolet excess in the SED and no significant variations in the light curve imply that this BSS is likely a single star ($mass=1.86^{+3.62}_{-0.94}\\ M_{\\odot}$) formed through stellar mergers. The fast rotation velocity ($v\\ \\mathrm{sin}i=206.9\\pm4.9$~km $\\rm s^{-1}$) of the BSS may provide constraints on its age (less than a hundred million years), but more formation details require further investigation.","sentences":["NGC 752 is a famous Galactic open cluster of intermediate age.","In recent works, a very long and asymmetric tail was newly revealed.","A blue straggler star (BSS) at the periphery of the tidal tail of the cluster has been identified subsequently.","We aim to perform a detailed analysis of the newly detected BSS based on the available comprehensive spectroscopic and photometric data.","We also explored this BSS's possible formation pathway and age limitation based on the collected spectroscopic and photometric data.","We estimated the projected rotational velocity $v\\ \\mathrm{sin}i$ and the mass of the BSS from the Large Sky Area Multi-Object Fiber Spectroscopic Telescope low-resolution spectra and multiband photometric data from various catalogs, respectively.","The newly discovered BSS is confirmed as a genuine member of NGC 752.","The lack of ultraviolet excess in the SED and no significant variations in the light curve imply that this BSS is likely a single star ($mass=1.86^{+3.62}_{-0.94}\\ M_{\\odot}$) formed through stellar mergers.","The fast rotation velocity ($v\\ \\mathrm{sin}i=206.9\\pm4.9$~km $\\rm s^{-1}$) of the BSS may provide constraints on its age (less than a hundred million years), but more formation details require further investigation."],"url":"http://arxiv.org/abs/2404.02473v1","category":"astro-ph.GA"}
{"created":"2024-04-03 05:10:11","title":"Prompting for Numerical Sequences: A Case Study on Market Comment Generation","abstract":"Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer insights into creating effective prompts for tasks that generate text from numerical sequences.","sentences":["Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings.","While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking.","Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes.","In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment.","Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective.","Our findings offer insights into creating effective prompts for tasks that generate text from numerical sequences."],"url":"http://arxiv.org/abs/2404.02466v1","category":"cs.CL"}
{"created":"2024-04-03 05:07:01","title":"Creating a Trajectory for Code Writing: Algorithmic Reasoning Tasks","abstract":"Many students in introductory programming courses fare poorly in the code writing tasks of the final summative assessment. Such tasks are designed to assess whether novices have developed the analytical skills to translate from the given problem domain to coding. In the past researchers have used instruments such as code-explain and found that the extent of cognitive depth reached in these tasks correlated well with code writing ability. However, the need for manual marking and personalized interviews used for identifying cognitive difficulties limited the study to a small group of stragglers. To extend this work to larger groups, we have devised several question types with varying cognitive demands collectively called Algorithmic Reasoning Tasks (ARTs), which do not require manual marking. These tasks require levels of reasoning which can define a learning trajectory. This paper describes these instruments and the machine learning models used for validating them. We have used the data collected in an introductory programming course in the penultimate week of the semester which required attempting ART type instruments and code writing. Our preliminary research suggests ART type instruments can be combined with specific machine learning models to act as an effective learning trajectory and early prediction of code-writing skills.","sentences":["Many students in introductory programming courses fare poorly in the code writing tasks of the final summative assessment.","Such tasks are designed to assess whether novices have developed the analytical skills to translate from the given problem domain to coding.","In the past researchers have used instruments such as code-explain and found that the extent of cognitive depth reached in these tasks correlated well with code writing ability.","However, the need for manual marking and personalized interviews used for identifying cognitive difficulties limited the study to a small group of stragglers.","To extend this work to larger groups, we have devised several question types with varying cognitive demands collectively called Algorithmic Reasoning Tasks (ARTs), which do not require manual marking.","These tasks require levels of reasoning which can define a learning trajectory.","This paper describes these instruments and the machine learning models used for validating them.","We have used the data collected in an introductory programming course in the penultimate week of the semester which required attempting ART type instruments and code writing.","Our preliminary research suggests ART type instruments can be combined with specific machine learning models to act as an effective learning trajectory and early prediction of code-writing skills."],"url":"http://arxiv.org/abs/2404.02464v1","category":"cs.SE"}
{"created":"2024-04-03 05:02:46","title":"TSNet:A Two-stage Network for Image Dehazing with Multi-scale Fusion and Adaptive Learning","abstract":"Image dehazing has been a popular topic of research for a long time. Previous deep learning-based image dehazing methods have failed to achieve satisfactory dehazing effects on both synthetic datasets and real-world datasets, exhibiting poor generalization. Moreover, single-stage networks often result in many regions with artifacts and color distortion in output images. To address these issues, this paper proposes a two-stage image dehazing network called TSNet, mainly consisting of the multi-scale fusion module (MSFM) and the adaptive learning module (ALM). Specifically, MSFM and ALM enhance the generalization of TSNet. The MSFM can obtain large receptive fields at multiple scales and integrate features at different frequencies to reduce the differences between inputs and learning objectives. The ALM can actively learn of regions of interest in images and restore texture details more effectively. Additionally, TSNet is designed as a two-stage network, where the first-stage network performs image dehazing, and the second-stage network is employed to improve issues such as artifacts and color distortion present in the results of the first-stage network. We also change the learning objective from ground truth images to opposite fog maps, which improves the learning efficiency of TSNet. Extensive experiments demonstrate that TSNet exhibits superior dehazing performance on both synthetic and real-world datasets compared to previous state-of-the-art methods.","sentences":["Image dehazing has been a popular topic of research for a long time.","Previous deep learning-based image dehazing methods have failed to achieve satisfactory dehazing effects on both synthetic datasets and real-world datasets, exhibiting poor generalization.","Moreover, single-stage networks often result in many regions with artifacts and color distortion in output images.","To address these issues, this paper proposes a two-stage image dehazing network called TSNet, mainly consisting of the multi-scale fusion module (MSFM) and the adaptive learning module (ALM).","Specifically, MSFM and ALM enhance the generalization of TSNet.","The MSFM can obtain large receptive fields at multiple scales and integrate features at different frequencies to reduce the differences between inputs and learning objectives.","The ALM can actively learn of regions of interest in images and restore texture details more effectively.","Additionally, TSNet is designed as a two-stage network, where the first-stage network performs image dehazing, and the second-stage network is employed to improve issues such as artifacts and color distortion present in the results of the first-stage network.","We also change the learning objective from ground truth images to opposite fog maps, which improves the learning efficiency of TSNet.","Extensive experiments demonstrate that TSNet exhibits superior dehazing performance on both synthetic and real-world datasets compared to previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.02460v1","category":"cs.CV"}
{"created":"2024-04-03 04:53:14","title":"PhonologyBench: Evaluating Phonological Skills of Large Language Models","abstract":"Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when compared to humans. Our findings underscore the importance of studying LLM performance on phonological tasks that inadvertently impact real-world applications. Furthermore, we encourage researchers to choose LLMs that perform well on the phonological task that is closely related to the downstream application since we find that no single model consistently outperforms the others on all the tasks.","sentences":["Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research.","LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation.","Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data.","Thus, it is imperative to benchmark the phonological skills of LLMs.","To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation.","Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks.","However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when compared to humans.","Our findings underscore the importance of studying LLM performance on phonological tasks that inadvertently impact real-world applications.","Furthermore, we encourage researchers to choose LLMs that perform well on the phonological task that is closely related to the downstream application since we find that no single model consistently outperforms the others on all the tasks."],"url":"http://arxiv.org/abs/2404.02456v1","category":"cs.CL"}
{"created":"2024-04-03 04:50:43","title":"Techniques for Measuring the Inferential Strength of Forgetting Policies","abstract":"The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application. Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory. The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory. Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using Problog. The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using Problog. Although the focus is on forgetting, the results are much more general and should have wider application to other areas.","sentences":["The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application.","Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory.","The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory.","Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using Problog.","The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using Problog.","Although the focus is on forgetting, the results are much more general and should have wider application to other areas."],"url":"http://arxiv.org/abs/2404.02454v1","category":"cs.AI"}
{"created":"2024-04-03 04:31:09","title":"Task Agnostic Architecture for Algorithm Induction via Implicit Composition","abstract":"Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions. Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by Large Language Models and multi-modal foundational models. These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities. Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks. This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed. Our proposal is based on the following assumptions. Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inherently operates sequentially. Second, recent Generative AI, especially Transformer-based models, demonstrate potential as an architecture capable of constructing algorithms for a wide range of domains. For example, GPT-4 shows exceptional capability at in-context learning of novel tasks which is hard to explain in any other way than the ability to compose novel solutions from fragments on previously learnt algorithms. Third, the observation that the main missing component in developing a truly generalised network is an efficient approach for self-consistent input of previously learnt sub-steps of an algorithm and their (implicit) composition during the network's internal forward pass. Our exploration delves into current capabilities and limitations of Transformer-based and other methods in efficient and correct algorithm composition and proposes a Transformer-like architecture as well as a discrete learning framework to overcome these limitations.","sentences":["Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions.","Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by Large Language Models and multi-modal foundational models.","These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities.","Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks.","This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed.","Our proposal is based on the following assumptions.","Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inherently operates sequentially.","Second, recent Generative AI, especially Transformer-based models, demonstrate potential as an architecture capable of constructing algorithms for a wide range of domains.","For example, GPT-4 shows exceptional capability at in-context learning of novel tasks which is hard to explain in any other way than the ability to compose novel solutions from fragments on previously learnt algorithms.","Third, the observation that the main missing component in developing a truly generalised network is an efficient approach for self-consistent input of previously learnt sub-steps of an algorithm and their (implicit) composition during the network's internal forward pass.","Our exploration delves into current capabilities and limitations of Transformer-based and other methods in efficient and correct algorithm composition and proposes a Transformer-like architecture as well as a discrete learning framework to overcome these limitations."],"url":"http://arxiv.org/abs/2404.02450v1","category":"cs.LG"}
{"created":"2024-04-03 04:27:07","title":"Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief","abstract":"As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on both synthetic datasets and real datasets. The results show that our solver outperforms baselines in terms of the objective value and computation time. Moreover, we analyze the generalization and scalability of our solver, demonstrating the capability toward unseen settings and large-scale problems. Check also our project page: https://ntt-dkiku.github.io/rl-evrpeps.","sentences":["As a telecom provider, our company has a critical mission to maintain telecom services even during power outages.","To accomplish the mission, it is essential to maintain the power of the telecom base stations.","Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations.","The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations.","In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector.","The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move.","In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies.","We evaluate our solver on both synthetic datasets and real datasets.","The results show that our solver outperforms baselines in terms of the objective value and computation time.","Moreover, we analyze the generalization and scalability of our solver, demonstrating the capability toward unseen settings and large-scale problems.","Check also our project page: https://ntt-dkiku.github.io/rl-evrpeps."],"url":"http://arxiv.org/abs/2404.02448v1","category":"math.OC"}
{"created":"2024-04-03 04:26:50","title":"A Novel Approach to Breast Cancer Histopathological Image Classification Using Cross-Colour Space Feature Fusion and Quantum-Classical Stack Ensemble Method","abstract":"Breast cancer classification stands as a pivotal pillar in ensuring timely diagnosis and effective treatment. This study with histopathological images underscores the profound significance of harnessing the synergistic capabilities of colour space ensembling and quantum-classical stacking to elevate the precision of breast cancer classification. By delving into the distinct colour spaces of RGB, HSV and CIE L*u*v, the authors initiated a comprehensive investigation guided by advanced methodologies. Employing the DenseNet121 architecture for feature extraction the authors have capitalized on the robustness of Random Forest, SVM, QSVC, and VQC classifiers. This research encompasses a unique feature fusion technique within the colour space ensemble. This approach not only deepens our comprehension of breast cancer classification but also marks a milestone in personalized medical assessment. The amalgamation of quantum and classical classifiers through stacking emerges as a potent catalyst, effectively mitigating the inherent constraints of individual classifiers, paving a robust path towards more dependable and refined breast cancer identification. Through rigorous experimentation and meticulous analysis, fusion of colour spaces like RGB with HSV and RGB with CIE L*u*v, presents an classification accuracy, nearing the value of unity. This underscores the transformative potential of our approach, where the fusion of diverse colour spaces and the synergy of quantum and classical realms converge to establish a new horizon in medical diagnostics. Thus the implications of this research extend across medical disciplines, offering promising avenues for advancing diagnostic accuracy and treatment efficacy.","sentences":["Breast cancer classification stands as a pivotal pillar in ensuring timely diagnosis and effective treatment.","This study with histopathological images underscores the profound significance of harnessing the synergistic capabilities of colour space ensembling and quantum-classical stacking to elevate the precision of breast cancer classification.","By delving into the distinct colour spaces of RGB, HSV and CIE L*u*v, the authors initiated a comprehensive investigation guided by advanced methodologies.","Employing the DenseNet121 architecture for feature extraction the authors have capitalized on the robustness of Random Forest, SVM, QSVC, and VQC classifiers.","This research encompasses a unique feature fusion technique within the colour space ensemble.","This approach not only deepens our comprehension of breast cancer classification but also marks a milestone in personalized medical assessment.","The amalgamation of quantum and classical classifiers through stacking emerges as a potent catalyst, effectively mitigating the inherent constraints of individual classifiers, paving a robust path towards more dependable and refined breast cancer identification.","Through rigorous experimentation and meticulous analysis, fusion of colour spaces like RGB with HSV and RGB with CIE L*u*v, presents an classification accuracy, nearing the value of unity.","This underscores the transformative potential of our approach, where the fusion of diverse colour spaces and the synergy of quantum and classical realms converge to establish a new horizon in medical diagnostics.","Thus the implications of this research extend across medical disciplines, offering promising avenues for advancing diagnostic accuracy and treatment efficacy."],"url":"http://arxiv.org/abs/2404.02447v1","category":"cs.CV"}
{"created":"2024-04-03 04:15:29","title":"The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education","abstract":"Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers' expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including noisy and long input data and highly skewed distributions of human ratings. Our results suggest that pretrained Language Models (PLMs) demonstrate performances comparable to the agreement level of human raters for variables that are more discrete and require lower inference, but their efficacy diminishes with more complex teaching practices. Interestingly, using only teachers' utterances as input yields strong results for student-centered variables, alleviating common concerns over the difficulty of collecting and transcribing high-quality student speech data in in-person teaching settings. Our findings highlight both the potential and the limitations of current NLP techniques in the education domain, opening avenues for further exploration.","sentences":["Assessing instruction quality is a fundamental component of any improvement efforts in the education system.","However, traditional manual assessments are expensive, subjective, and heavily dependent on observers' expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback.","Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers.","This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs.","We confront two challenges inherent in NLP-based instructional analysis, including noisy and long input data and highly skewed distributions of human ratings.","Our results suggest that pretrained Language Models (PLMs) demonstrate performances comparable to the agreement level of human raters for variables that are more discrete and require lower inference, but their efficacy diminishes with more complex teaching practices.","Interestingly, using only teachers' utterances as input yields strong results for student-centered variables, alleviating common concerns over the difficulty of collecting and transcribing high-quality student speech data in in-person teaching settings.","Our findings highlight both the potential and the limitations of current NLP techniques in the education domain, opening avenues for further exploration."],"url":"http://arxiv.org/abs/2404.02444v1","category":"cs.CL"}
{"created":"2024-04-03 03:36:35","title":"AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset","abstract":"Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research. We provide 19 datasets, including real-world human driver's datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods. Dataset and codes can be found in https://sites.google.com/view/ad4rl.","sentences":["Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets.","Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets.","To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research.","We provide 19 datasets, including real-world human driver's datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios.","We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design.","Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods.","Dataset and codes can be found in https://sites.google.com/view/ad4rl."],"url":"http://arxiv.org/abs/2404.02429v1","category":"cs.LG"}
{"created":"2024-04-03 02:56:52","title":"Auxiliary task demands mask the capabilities of smaller language models","abstract":"Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of \"task demands\" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This \"demand gap\" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the lens of researchers' design choices.","sentences":["Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge.","These debates often hinge on the concept of \"task demands\" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability.","The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources.","Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands.","This \"demand gap\" is most pronounced for models with fewer parameters and less training data.","Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the lens of researchers' design choices."],"url":"http://arxiv.org/abs/2404.02418v1","category":"cs.CL"}
{"created":"2024-04-03 02:17:34","title":"Decision Transformer as a Foundation Model for Partially Observable Continuous Control","abstract":"Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT's capability to capture the parameter-agnostic structures intrinsic to control tasks. DT exhibits remarkable zero-shot generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data. These findings highlight the potential of DT as a foundational controller for general control applications.","sentences":["Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools.","Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior.","To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture.","Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design.","Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA).","Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT's capability to capture the parameter-agnostic structures intrinsic to control tasks.","DT exhibits remarkable zero-shot generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data.","These findings highlight the potential of DT as a foundational controller for general control applications."],"url":"http://arxiv.org/abs/2404.02407v1","category":"eess.SY"}
{"created":"2024-04-03 02:16:53","title":"Exploring Backdoor Vulnerabilities of Chat Models","abstract":"Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor can not be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic content.","sentences":["Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack.","The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger.","Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models.","Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention.","Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks.","In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations.","Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests.","Also, the backdoor can not be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models.","Warning:","This paper may contain toxic content."],"url":"http://arxiv.org/abs/2404.02406v1","category":"cs.CR"}
{"created":"2024-04-03 02:11:39","title":"Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM","abstract":"Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.","sentences":["Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses.","In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations.","Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies.","Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance.","Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions."],"url":"http://arxiv.org/abs/2404.02402v1","category":"cs.CL"}
{"created":"2024-04-03 01:16:20","title":"On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL","abstract":"Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure node encodings and the potential for model compression due to modality fusion redundancy. Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research.","sentences":["Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation.","With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph.","Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear.","This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5.","Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing.","We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure node encodings and the potential for model compression due to modality fusion redundancy.","Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research."],"url":"http://arxiv.org/abs/2404.02389v1","category":"cs.CL"}
{"created":"2024-04-03 00:40:38","title":"Exploring the Impact of Source Code Linearity on the Programmers Comprehension of API Code Examples","abstract":"Context: Application Programming Interface (API) code examples are an essential knowledge resource for learning APIs. However, a few user studies have explored how the structural characteristics of the source code in code examples impact their comprehensibility and reusability. Objectives: We investigated whether the (a) linearity and (b) length of the source code in API code examples affect users performance in terms of correctness and time spent. We also collected subjective ratings. Methods: We conducted an online controlled code comprehension experiment with 61 Java developers. As a case study, we used the API code examples from the Joda-Time Java library. We had participants perform code comprehension and reuse tasks on variants of the example with different lengths and degrees of linearity. Findings: Participants demonstrated faster reaction times when exposed to linear code examples. However, no substantial differences in correctness or subjective ratings were observed. Implications: Our findings suggest that the linear presentation of a source code may enhance initial example understanding and reusability. This, in turn, may provide API developers with some insights into the effective structuring of their API code examples. However, we highlight the need for further investigation.","sentences":["Context: Application Programming Interface (API) code examples are an essential knowledge resource for learning APIs.","However, a few user studies have explored how the structural characteristics of the source code in code examples impact their comprehensibility and reusability.","Objectives: We investigated whether the (a) linearity and (b) length of the source code in API code examples affect users performance in terms of correctness and time spent.","We also collected subjective ratings.","Methods: We conducted an online controlled code comprehension experiment with 61 Java developers.","As a case study, we used the API code examples from the Joda-Time Java library.","We had participants perform code comprehension and reuse tasks on variants of the example with different lengths and degrees of linearity.","Findings:","Participants demonstrated faster reaction times when exposed to linear code examples.","However, no substantial differences in correctness or subjective ratings were observed.","Implications: Our findings suggest that the linear presentation of a source code may enhance initial example understanding and reusability.","This, in turn, may provide API developers with some insights into the effective structuring of their API code examples.","However, we highlight the need for further investigation."],"url":"http://arxiv.org/abs/2404.02377v1","category":"cs.SE"}
{"created":"2024-04-03 00:21:14","title":"Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach","abstract":"Efforts on the research and development of OCR systems for Low-Resource Languages are relatively new. Low-resource languages have little training data available for training Machine Translation systems or other systems. Even though a vast amount of text has been digitized and made available on the internet the text is still in PDF and Image format, which are not instantly accessible. This paper discusses text recognition for two scripts: Bengali and Nepali; there are about 300 and 40 million Bengali and Nepali speakers respectively. In this study, using encoder-decoder transformers, a model was developed, and its efficacy was assessed using a collection of optical text images, both handwritten and printed. The results signify that the suggested technique corresponds with current approaches and achieves high precision in recognizing text in Bengali and Nepali. This study can pave the way for the advanced and accessible study of linguistics in South East Asia.","sentences":["Efforts on the research and development of OCR systems for Low-Resource Languages are relatively new.","Low-resource languages have little training data available for training Machine Translation systems or other systems.","Even though a vast amount of text has been digitized and made available on the internet the text is still in PDF and Image format, which are not instantly accessible.","This paper discusses text recognition for two scripts: Bengali and Nepali; there are about 300 and 40 million Bengali and Nepali speakers respectively.","In this study, using encoder-decoder transformers, a model was developed, and its efficacy was assessed using a collection of optical text images, both handwritten and printed.","The results signify that the suggested technique corresponds with current approaches and achieves high precision in recognizing text in Bengali and Nepali.","This study can pave the way for the advanced and accessible study of linguistics in South East Asia."],"url":"http://arxiv.org/abs/2404.02375v1","category":"cs.CL"}
{"created":"2024-04-03 00:09:05","title":"Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns","abstract":"Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on fine-tuning was confirmed as it outperformed other medical VLMs in all tasks except visual question answering. This work marks the potential of leveraging both the VLM's capabilities and the radiologist's domain knowledge to improve the capabilities of AI models in medical imaging, paving a novel way for Computer Assisted Diagnosis with a human-centred AI.","sentences":["Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis.","However, the interaction between these models and radiologists has been primarily limited to input images.","This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts.","Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation.","We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis.","Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis.","Also, the impact of eye gaze on fine-tuning was confirmed as it outperformed other medical VLMs in all tasks except visual question answering.","This work marks the potential of leveraging both the VLM's capabilities and the radiologist's domain knowledge to improve the capabilities of AI models in medical imaging, paving a novel way for Computer Assisted Diagnosis with a human-centred AI."],"url":"http://arxiv.org/abs/2404.02370v1","category":"cs.CV"}
{"created":"2024-04-02 23:16:17","title":"EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management","abstract":"This paper investigates the increasing roles of Renewable Energy Sources (RES) and Electric Vehicles (EVs). While indicating a new era of sustainable energy, these also introduce complex challenges, including the need to balance supply and demand and smooth peak consumptions amidst rising EV adoption rates. Addressing these challenges requires innovative solutions such as Demand Response (DR), energy flexibility management, Renewable Energy Communities (RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing V2G approaches often fall short in real-world adaptability, global REC optimization with other flexible assets, scalability, and user engagement. To bridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement Learning (MARL) energy management framework, leveraging the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables user-centric and multi-objective energy management by allowing each prosumer to select from a range of personal management objectives, thus encouraging engagement. Additionally, it architects' data protection and ownership through decentralized computing, where each prosumer can situate an energy management optimization node directly at their own dwelling. The local node not only manages local energy assets but also fosters REC wide optimization. The efficacy of EnergAIze was evaluated through case studies employing the CityLearn simulation framework. These simulations were instrumental in demonstrating EnergAIze's adeptness at implementing V2G technology within a REC and other energy assets. The results show reduction in peak loads, ramping, carbon emissions, and electricity costs at the REC level while optimizing for individual prosumers objectives.","sentences":["This paper investigates the increasing roles of Renewable Energy Sources (RES) and Electric Vehicles (EVs).","While indicating a new era of sustainable energy, these also introduce complex challenges, including the need to balance supply and demand and smooth peak consumptions amidst rising EV adoption rates.","Addressing these challenges requires innovative solutions such as Demand Response (DR), energy flexibility management, Renewable Energy Communities (RECs), and more specifically for EVs, Vehicle-to-Grid (V2G).","However, existing V2G approaches often fall short in real-world adaptability, global REC optimization with other flexible assets, scalability, and user engagement.","To bridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement Learning (MARL) energy management framework, leveraging the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm.","EnergAIze enables user-centric and multi-objective energy management by allowing each prosumer to select from a range of personal management objectives, thus encouraging engagement.","Additionally, it architects' data protection and ownership through decentralized computing, where each prosumer can situate an energy management optimization node directly at their own dwelling.","The local node not only manages local energy assets but also fosters REC wide optimization.","The efficacy of EnergAIze was evaluated through case studies employing the CityLearn simulation framework.","These simulations were instrumental in demonstrating EnergAIze's adeptness at implementing V2G technology within a REC and other energy assets.","The results show reduction in peak loads, ramping, carbon emissions, and electricity costs at the REC level while optimizing for individual prosumers objectives."],"url":"http://arxiv.org/abs/2404.02361v1","category":"cs.MA"}
{"created":"2024-04-02 22:54:24","title":"Semantic Augmentation in Images using Language","abstract":"Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.","sentences":["Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning.","As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples.","Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs.","Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets.","This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models."],"url":"http://arxiv.org/abs/2404.02353v1","category":"cs.CV"}
{"created":"2024-04-02 22:49:25","title":"COVID-19 Detection Based on Blood Test Parameters using Various Artificial Intelligence Methods","abstract":"In 2019, the world faced a new challenge: a COVID-19 disease caused by the novel coronavirus, SARS-CoV-2. The virus rapidly spread across the globe, leading to a high rate of mortality, which prompted health organizations to take measures to control its transmission. Early disease detection is crucial in the treatment process, and computer-based automatic detection systems have been developed to aid in this effort. These systems often rely on artificial intelligence (AI) approaches such as machine learning, neural networks, fuzzy systems, and deep learning to classify diseases. This study aimed to differentiate COVID-19 patients from others using self-categorizing classifiers and employing various AI methods. This study used two datasets: the blood test samples and radiography images. The best results for the blood test samples obtained from San Raphael Hospital, which include two classes of individuals, those with COVID-19 and those with non-COVID diseases, were achieved through the use of the Ensemble method (a combination of a neural network and two machines learning methods). The results showed that this approach for COVID-19 diagnosis is cost-effective and provides results in a shorter amount of time than other methods. The proposed model achieved an accuracy of 94.09% on the dataset used. Secondly, the radiographic images were divided into four classes: normal, viral pneumonia, ground glass opacity, and COVID-19 infection. These were used for segmentation and classification. The lung lobes were extracted from the images and then categorized into specific classes. We achieved an accuracy of 91.1% on the image dataset. Generally, this study highlights the potential of AI in detecting and managing COVID-19 and underscores the importance of continued research and development in this field.","sentences":["In 2019, the world faced a new challenge: a COVID-19 disease caused by the novel coronavirus, SARS-CoV-2.","The virus rapidly spread across the globe, leading to a high rate of mortality, which prompted health organizations to take measures to control its transmission.","Early disease detection is crucial in the treatment process, and computer-based automatic detection systems have been developed to aid in this effort.","These systems often rely on artificial intelligence (AI) approaches such as machine learning, neural networks, fuzzy systems, and deep learning to classify diseases.","This study aimed to differentiate COVID-19 patients from others using self-categorizing classifiers and employing various AI methods.","This study used two datasets: the blood test samples and radiography images.","The best results for the blood test samples obtained from San Raphael Hospital, which include two classes of individuals, those with COVID-19 and those with non-COVID diseases, were achieved through the use of the Ensemble method (a combination of a neural network and two machines learning methods).","The results showed that this approach for COVID-19 diagnosis is cost-effective and provides results in a shorter amount of time than other methods.","The proposed model achieved an accuracy of 94.09% on the dataset used.","Secondly, the radiographic images were divided into four classes: normal, viral pneumonia, ground glass opacity, and COVID-19 infection.","These were used for segmentation and classification.","The lung lobes were extracted from the images and then categorized into specific classes.","We achieved an accuracy of 91.1% on the image dataset.","Generally, this study highlights the potential of AI in detecting and managing COVID-19 and underscores the importance of continued research and development in this field."],"url":"http://arxiv.org/abs/2404.02348v1","category":"eess.IV"}
{"created":"2024-04-02 22:23:27","title":"Why do people think liberals drink lattes? How social media afforded self-presentation can shape subjective social sorting","abstract":"Social sorting, the alignment of social identities, affiliations, and/or preferences with partisan groups, can increase in-party attachment and decrease out-party tolerance. We propose that self-presentation afforded by social media profiles fosters subjective social sorting by shaping perceptions of alignments between non-political and political identifiers. Unlike previous work, we evaluate social sorting of naturally occurring, public-facing identifiers in social media profiles selected using a bottom-up approach. Using a sample of 50 million X users collected five times between 2016 and 2018, we identify users who define themselves politically and generate networks representing simultaneous co-occurrence of identifiers in profiles. We then systematically measure the alignment of non-political identifiers along political dimensions, revealing alignments that reinforce existing associations, reveal unexpected relationships, and reflect online and offline events. We find that while most identifiers bridge political divides, social sorting of identifiers along political lines is occurring to some degree in X profiles. Our results have implications for understanding the role of social media in facilitating (the perception of) polarization and polarization mitigation strategies such as bridging interventions and algorithms.","sentences":["Social sorting, the alignment of social identities, affiliations, and/or preferences with partisan groups, can increase in-party attachment and decrease out-party tolerance.","We propose that self-presentation afforded by social media profiles fosters subjective social sorting by shaping perceptions of alignments between non-political and political identifiers.","Unlike previous work, we evaluate social sorting of naturally occurring, public-facing identifiers in social media profiles selected using a bottom-up approach.","Using a sample of 50 million X users collected five times between 2016 and 2018, we identify users who define themselves politically and generate networks representing simultaneous co-occurrence of identifiers in profiles.","We then systematically measure the alignment of non-political identifiers along political dimensions, revealing alignments that reinforce existing associations, reveal unexpected relationships, and reflect online and offline events.","We find that while most identifiers bridge political divides, social sorting of identifiers along political lines is occurring to some degree in X profiles.","Our results have implications for understanding the role of social media in facilitating (the perception of) polarization and polarization mitigation strategies such as bridging interventions and algorithms."],"url":"http://arxiv.org/abs/2404.02338v1","category":"cs.SI"}
{"created":"2024-04-02 22:15:48","title":"Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation","abstract":"The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition (NER) settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each domain. Experimental results on different formal and informal datasets show that by employing these added parameters, the proposed model significantly surpasses existing practical models in performance. Remarkably, the proposed model requires only one instance for training and storage, yet achieves outstanding results across all domains, even surpassing the state-of-the-art in some. Moreover, we analyze each adaptation strategy, delineating its strengths, weaknesses, and optimal hyper-parameters for the Persian NER settings. Finally, we introduce a document-based domain detection pipeline tailored for scenarios with unknown text domains, enhancing the adaptability and practicality of this paper in real-world applications.","sentences":["The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings.","These challenges are also visible in the Persian name entity recognition (NER) settings.","Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations.","Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical.","Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters.","We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains.","This enables the model to perform comparably to individual models for each domain.","Experimental results on different formal and informal datasets show that by employing these added parameters, the proposed model significantly surpasses existing practical models in performance.","Remarkably, the proposed model requires only one instance for training and storage, yet achieves outstanding results across all domains, even surpassing the state-of-the-art in some.","Moreover, we analyze each adaptation strategy, delineating its strengths, weaknesses, and optimal hyper-parameters for the Persian NER settings.","Finally, we introduce a document-based domain detection pipeline tailored for scenarios with unknown text domains, enhancing the adaptability and practicality of this paper in real-world applications."],"url":"http://arxiv.org/abs/2404.02335v1","category":"cs.CL"}
{"created":"2024-04-02 22:04:51","title":"Comparative Study of Domain Driven Terms Extraction Using Large Language Models","abstract":"Keywords play a crucial role in bridging the gap between human understanding and machine processing of textual data. They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data. Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating information retrieval, document summarization, and content categorization. This review focuses on keyword extraction methods, emphasizing the use of three major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We employed a custom Python package to interface with these LLMs, simplifying keyword extraction. Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models. The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for GPT-3.5, 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for Falcon-7B. This paper underlines the role of prompt engineering in LLMs for better keyword extraction and discusses the impact of hallucination in LLMs on result evaluation. It also sheds light on the challenges in using LLMs for keyword extraction, including model complexity, resource demands, and optimization techniques.","sentences":["Keywords play a crucial role in bridging the gap between human understanding and machine processing of textual data.","They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data.","Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating information retrieval, document summarization, and content categorization.","This review focuses on keyword extraction methods, emphasizing the use of three major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We employed a custom Python package to interface with these LLMs, simplifying keyword extraction.","Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models.","The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for GPT-3.5, 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for Falcon-7B. This paper underlines the role of prompt engineering in LLMs for better keyword extraction and discusses the impact of hallucination in LLMs on result evaluation.","It also sheds light on the challenges in using LLMs for keyword extraction, including model complexity, resource demands, and optimization techniques."],"url":"http://arxiv.org/abs/2404.02330v1","category":"cs.CL"}
{"created":"2024-04-02 21:35:54","title":"Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization","abstract":"Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-source at https://github.com/microsoft/sammo .","sentences":["Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts.","However, prompts often require some tuning to improve performance for deployment.","Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\\em meta prompt programs}.","To address this, we introduce SAMMO, a framework for {\\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization.","We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   ","We make all code available open-source at https://github.com/microsoft/sammo ."],"url":"http://arxiv.org/abs/2404.02319v1","category":"cs.CL"}
{"created":"2024-04-02 21:20:51","title":"Is Meta-training Really Necessary for Molecular Few-Shot Learning ?","abstract":"Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.","sentences":["Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies.","We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance.","We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss.","Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies.","Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts.","In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods."],"url":"http://arxiv.org/abs/2404.02314v1","category":"cs.LG"}
{"created":"2024-04-02 21:03:37","title":"Collapse of Self-trained Language Models","abstract":"In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions. While this approach is intuitively appealing, our research reveals its practical limitations. We find that extended self-training of the GPT-2 model leads to a significant degradation in performance, resulting in repetitive and collapsed token output.","sentences":["In various fields of knowledge creation, including science, new ideas often build on pre-existing information.","In this work, we explore this concept within the context of language models.","Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions.","While this approach is intuitively appealing, our research reveals its practical limitations.","We find that extended self-training of the GPT-2 model leads to a significant degradation in performance, resulting in repetitive and collapsed token output."],"url":"http://arxiv.org/abs/2404.02305v1","category":"cs.CL"}
{"created":"2024-04-02 21:03:17","title":"Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks","abstract":"Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance. While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself. Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life. Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads. Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies. To address this gap, we introduce a graph-based virtual sensor that leverages Graph Neural Networks (GNNs) to analyze spatial-temporal dependencies among sensor signals, mapping existing measurements (temperature, vibration) to bearing loads. Since temperature and vibration signals exhibit vastly different dynamics, we propose Heterogeneous Temporal Graph Neural Networks (HTGNN), which explicitly models these signal types and their interactions for effective load prediction. Our results demonstrate that HTGNN outperforms Convolutional Neural Networks (CNNs), which struggle to capture both spatial and heterogeneous signal characteristics. These findings highlight the importance of capturing the complex spatial interactions between temperature, vibration, and load.","sentences":["Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance.","While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself.","Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life.","Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads.","Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies.","To address this gap, we introduce a graph-based virtual sensor that leverages Graph Neural Networks (GNNs) to analyze spatial-temporal dependencies among sensor signals, mapping existing measurements (temperature, vibration) to bearing loads.","Since temperature and vibration signals exhibit vastly different dynamics, we propose Heterogeneous Temporal Graph Neural Networks (HTGNN), which explicitly models these signal types and their interactions for effective load prediction.","Our results demonstrate that HTGNN outperforms Convolutional Neural Networks (CNNs), which struggle to capture both spatial and heterogeneous signal characteristics.","These findings highlight the importance of capturing the complex spatial interactions between temperature, vibration, and load."],"url":"http://arxiv.org/abs/2404.02304v1","category":"cs.LG"}
{"created":"2024-04-02 20:46:13","title":"Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs","abstract":"This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.","sentences":["This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation.","We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation.","A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images.","By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain.","This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains."],"url":"http://arxiv.org/abs/2404.02294v1","category":"cs.RO"}
{"created":"2024-04-02 20:29:59","title":"One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation","abstract":"This paper presents a novel universal perturbation method for generating robust multi-view adversarial examples in 3D object recognition. Unlike conventional attacks limited to single views, our approach operates on multiple 2D images, offering a practical and scalable solution for enhancing model scalability and robustness. This generalizable method bridges the gap between 2D perturbations and 3D-like attack capabilities, making it suitable for real-world applications.   Existing adversarial attacks may become ineffective when images undergo transformations like changes in lighting, camera position, or natural deformations. We address this challenge by crafting a single universal noise perturbation applicable to various object views. Experiments on diverse rendered 3D objects demonstrate the effectiveness of our approach. The universal perturbation successfully identified a single adversarial noise for each given set of 3D object renders from multiple poses and viewpoints. Compared to single-view attacks, our universal attacks lower classification confidence across multiple viewing angles, especially at low noise levels. A sample implementation is made available at https://github.com/memoatwit/UniversalPerturbation.","sentences":["This paper presents a novel universal perturbation method for generating robust multi-view adversarial examples in 3D object recognition.","Unlike conventional attacks limited to single views, our approach operates on multiple 2D images, offering a practical and scalable solution for enhancing model scalability and robustness.","This generalizable method bridges the gap between 2D perturbations and 3D-like attack capabilities, making it suitable for real-world applications.   ","Existing adversarial attacks may become ineffective when images undergo transformations like changes in lighting, camera position, or natural deformations.","We address this challenge by crafting a single universal noise perturbation applicable to various object views.","Experiments on diverse rendered 3D objects demonstrate the effectiveness of our approach.","The universal perturbation successfully identified a single adversarial noise for each given set of 3D object renders from multiple poses and viewpoints.","Compared to single-view attacks, our universal attacks lower classification confidence across multiple viewing angles, especially at low noise levels.","A sample implementation is made available at https://github.com/memoatwit/UniversalPerturbation."],"url":"http://arxiv.org/abs/2404.02287v1","category":"cs.CV"}
{"created":"2024-04-02 19:49:34","title":"Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges","abstract":"We investigate the effectiveness of ChatGPT in extracting norms from contracts. Norms provide a natural way to engineer multiagent systems by capturing how to govern the interactions between two or more autonomous parties. We extract norms of commitment, prohibition, authorization, and power, along with associated norm elements (the parties involved, antecedents, and consequents) from contracts. Our investigation reveals ChatGPT's effectiveness and limitations in norm extraction from contracts. ChatGPT demonstrates promising performance in norm extraction without requiring training or fine-tuning, thus obviating the need for annotated data, which is not generally available in this domain. However, we found some limitations of ChatGPT in extracting these norms that lead to incorrect norm extractions. The limitations include oversight of crucial details, hallucination, incorrect parsing of conjunctions, and empty norm elements. Enhanced norm extraction from contracts can foster the development of more transparent and trustworthy formal agent interaction specifications, thereby contributing to the improvement of multiagent systems.","sentences":["We investigate the effectiveness of ChatGPT in extracting norms from contracts.","Norms provide a natural way to engineer multiagent systems by capturing how to govern the interactions between two or more autonomous parties.","We extract norms of commitment, prohibition, authorization, and power, along with associated norm elements (the parties involved, antecedents, and consequents) from contracts.","Our investigation reveals ChatGPT's effectiveness and limitations in norm extraction from contracts.","ChatGPT demonstrates promising performance in norm extraction without requiring training or fine-tuning, thus obviating the need for annotated data, which is not generally available in this domain.","However, we found some limitations of ChatGPT in extracting these norms that lead to incorrect norm extractions.","The limitations include oversight of crucial details, hallucination, incorrect parsing of conjunctions, and empty norm elements.","Enhanced norm extraction from contracts can foster the development of more transparent and trustworthy formal agent interaction specifications, thereby contributing to the improvement of multiagent systems."],"url":"http://arxiv.org/abs/2404.02269v1","category":"cs.CL"}
{"created":"2024-04-02 19:37:58","title":"OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment","abstract":"The task of motion prediction is pivotal for autonomous driving systems, providing crucial data to choose a vehicle behavior strategy within its surroundings. Existing motion prediction techniques primarily focus on predicting the future trajectory of each agent in the scene individually, utilizing its past trajectory data. In this paper, we introduce an end-to-end neural network methodology designed to predict the future behaviors of all dynamic objects in the environment. This approach leverages the occupancy map and the scene's motion flow. We are investigatin various alternatives for constructing a deep encoder-decoder model called OFMPNet. This model uses a sequence of bird's-eye-view road images, occupancy grid, and prior motion flow as input data. The encoder of the model can incorporate transformer, attention-based, or convolutional units. The decoder considers the use of both convolutional modules and recurrent blocks. Additionally, we propose a novel time-weighted motion flow loss, whose application has shown a substantial decrease in end-point error. Our approach has achieved state-of-the-art results on the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1% and an AUC of 76.75% on Flow-Grounded Occupancy.","sentences":["The task of motion prediction is pivotal for autonomous driving systems, providing crucial data to choose a vehicle behavior strategy within its surroundings.","Existing motion prediction techniques primarily focus on predicting the future trajectory of each agent in the scene individually, utilizing its past trajectory data.","In this paper, we introduce an end-to-end neural network methodology designed to predict the future behaviors of all dynamic objects in the environment.","This approach leverages the occupancy map and the scene's motion flow.","We are investigatin various alternatives for constructing a deep encoder-decoder model called OFMPNet.","This model uses a sequence of bird's-eye-view road images, occupancy grid, and prior motion flow as input data.","The encoder of the model can incorporate transformer, attention-based, or convolutional units.","The decoder considers the use of both convolutional modules and recurrent blocks.","Additionally, we propose a novel time-weighted motion flow loss, whose application has shown a substantial decrease in end-point error.","Our approach has achieved state-of-the-art results on the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1% and an AUC of 76.75% on Flow-Grounded Occupancy."],"url":"http://arxiv.org/abs/2404.02263v1","category":"cs.CV"}
{"created":"2024-04-02 19:34:22","title":"LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages","abstract":"Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings. By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.","sentences":["Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly.","The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets.","To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation.","Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator.","The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required.","Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation.","Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings.","By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes."],"url":"http://arxiv.org/abs/2404.02261v1","category":"cs.CL"}
{"created":"2024-04-02 19:23:10","title":"$\\texttt{LM}^\\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning","abstract":"Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by $8.1\\%$ on MATH, $7.71\\%$ on JEEBench, and $9.7\\%$ on MedQA problems (code available at https://github.com/LCS2-IIITD/Language_Model_Multiplex).","sentences":["Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning.","Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems.","However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning.","In this paper, we propose LM2 to address these challenges.","LM2 modularizes the decomposition, solution, and verification into three different language models.","The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement.","The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions.","These models are trained to coordinate using policy learning.","Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by $8.1\\%$ on MATH, $7.71\\%$ on JEEBench, and $9.7\\%$ on MedQA problems (code available at https://github.com/LCS2-IIITD/Language_Model_Multiplex)."],"url":"http://arxiv.org/abs/2404.02255v1","category":"cs.CL"}
{"created":"2024-04-02 19:14:23","title":"RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction","abstract":"Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at \\url{https://github.com/YushenLi807/WWW24-RAT}.","sentences":["Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions.","Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction.","To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples.","By retrieving similar samples, we construct augmented input for each target sample.","We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency.","Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios.","The code has been open-sourced at \\url{https://github.com/YushenLi807/WWW24-RAT}."],"url":"http://arxiv.org/abs/2404.02249v1","category":"cs.IR"}
{"created":"2024-04-02 18:45:01","title":"Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning","abstract":"In deep reinforcement learning (RL) research, there has been a concerted effort to design more efficient and productive exploration methods while solving sparse-reward problems. These exploration methods often share common principles (e.g., improving diversity) and implementation details (e.g., intrinsic reward). Prior work found that non-stationary Markov decision processes (MDPs) require exploration to efficiently adapt to changes in the environment with online transfer learning. However, the relationship between specific exploration characteristics and effective transfer learning in deep RL has not been characterized. In this work, we seek to understand the relationships between salient exploration characteristics and improved performance and efficiency in transfer learning. We test eleven popular exploration algorithms on a variety of transfer types -- or ``novelties'' -- to identify the characteristics that positively affect online transfer learning. Our analysis shows that some characteristics correlate with improved performance and efficiency across a wide range of transfer tasks, while others only improve transfer performance with respect to specific environment changes. From our analysis, make recommendations about which exploration algorithm characteristics are best suited to specific transfer situations.","sentences":["In deep reinforcement learning (RL) research, there has been a concerted effort to design more efficient and productive exploration methods while solving sparse-reward problems.","These exploration methods often share common principles (e.g., improving diversity) and implementation details (e.g., intrinsic reward).","Prior work found that non-stationary Markov decision processes (MDPs) require exploration to efficiently adapt to changes in the environment with online transfer learning.","However, the relationship between specific exploration characteristics and effective transfer learning in deep RL has not been characterized.","In this work, we seek to understand the relationships between salient exploration characteristics and improved performance and efficiency in transfer learning.","We test eleven popular exploration algorithms on a variety of transfer types -- or ``novelties'' -- to identify the characteristics that positively affect online transfer learning.","Our analysis shows that some characteristics correlate with improved performance and efficiency across a wide range of transfer tasks, while others only improve transfer performance with respect to specific environment changes.","From our analysis, make recommendations about which exploration algorithm characteristics are best suited to specific transfer situations."],"url":"http://arxiv.org/abs/2404.02235v1","category":"cs.LG"}
{"created":"2024-04-02 18:36:21","title":"\"Against the Void\": An Interview and Survey Study on How Rust Developers Use Unsafe Code","abstract":"The Rust programming language is an increasingly popular choice for systems programming, since it can statically guarantee memory safety without automatic garbage collection. Rust provides its safety guarantees by restricting aliasing and mutability, but many key design patterns, such as cyclic aliasing and multi-language interoperation, must bypass these restrictions. Rust's $\\texttt{unsafe}$ keyword enables features that developers can use to implement these patterns, and the Rust ecosystem includes useful tools for validating whether $\\texttt{unsafe}$ code is used correctly. However, it is unclear if these tools are adequate for all use cases. To understand developers' needs, we conducted a mixed-methods study consisting of semi-structured interviews followed by a survey. We interviewed 19 Rust developers and surveyed 160 developers$\\unicode{x2013}$all of whom engaged with $\\texttt{unsafe}$ code. We found that 77% of survey respondents and a majority of interview participants were motivated to use $\\texttt{unsafe}$ code because they were unaware of a safe alternative. Developers typically followed best-practices such as minimizing and localizing their use of $\\texttt{unsafe}$ code, but only 23% were always certain that their encapsulations were sound. Limited tooling support for inline assembly and foreign function calls prevented developers from validating $\\texttt{unsafe}$ code, and differences between Rust and other languages made foreign functions difficult to encapsulate. Verification tools were underused, and developers rarely audited their dependencies. Our results indicate a pressing need for production-ready tools that can validate the most frequently used $\\texttt{unsafe}$ features.","sentences":["The Rust programming language is an increasingly popular choice for systems programming, since it can statically guarantee memory safety without automatic garbage collection.","Rust provides its safety guarantees by restricting aliasing and mutability, but many key design patterns, such as cyclic aliasing and multi-language interoperation, must bypass these restrictions.","Rust's $\\texttt{unsafe}$ keyword enables features that developers can use to implement these patterns, and the Rust ecosystem includes useful tools for validating whether $\\texttt{unsafe}$ code is used correctly.","However, it is unclear if these tools are adequate for all use cases.","To understand developers' needs, we conducted a mixed-methods study consisting of semi-structured interviews followed by a survey.","We interviewed 19 Rust developers and surveyed 160 developers$\\unicode{x2013}$all of whom engaged with $\\texttt{unsafe}$ code.","We found that 77% of survey respondents and a majority of interview participants were motivated to use $\\texttt{unsafe}$ code because they were unaware of a safe alternative.","Developers typically followed best-practices such as minimizing and localizing their use of $\\texttt{unsafe}$ code, but only 23% were always certain that their encapsulations were sound.","Limited tooling support for inline assembly and foreign function calls prevented developers from validating $\\texttt{unsafe}$ code, and differences between Rust and other languages made foreign functions difficult to encapsulate.","Verification tools were underused, and developers rarely audited their dependencies.","Our results indicate a pressing need for production-ready tools that can validate the most frequently used $\\texttt{unsafe}$ features."],"url":"http://arxiv.org/abs/2404.02230v1","category":"cs.SE"}
{"created":"2024-04-02 18:30:29","title":"OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising","abstract":"Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sight noisy sensor trajectory denoising and prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory prediction accuracy and addressing the challenges of out-of-sight objects, our work significantly contributes to improving the safety and reliability of autonomous driving in complex environments. Our work represents the first initiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new benchmark for future research. The code is available at \\url{https://github.com/Hai-chao-Zhang/OOSTraj}.","sentences":["Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making.","Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data.","Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects.","To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique.","Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories.","This method has demonstrated state-of-the-art performance in out-of-sight noisy sensor trajectory denoising and prediction on the Vi-Fi and JRDB datasets.","By enhancing trajectory prediction accuracy and addressing the challenges of out-of-sight objects, our work significantly contributes to improving the safety and reliability of autonomous driving in complex environments.","Our work represents the first initiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new benchmark for future research.","The code is available at \\url{https://github.com/Hai-chao-Zhang/OOSTraj}."],"url":"http://arxiv.org/abs/2404.02227v1","category":"cs.CV"}
{"created":"2024-04-02 18:27:03","title":"CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement","abstract":"We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.","sentences":["We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework.","It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses.","Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system.","The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished.","Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines."],"url":"http://arxiv.org/abs/2404.02225v1","category":"cs.CV"}
{"created":"2024-04-02 18:17:14","title":"Polarimetric differential imaging with VLT/NACO. A comprehensive PDI pipeline for NACO data (PIPPIN)","abstract":"The observed diversity of exoplanets can possibly be traced back to the planet formation processes. Planet-disk interactions induce sub-structures in the circumstellar disk that can be revealed via scattered light observations. However, a high-contrast imaging technique such as polarimetric differential imaging (PDI) must first be applied to suppress the stellar diffraction halo. In this work we present the PDI PiPelIne for NACO data (PIPPIN), which reduces the archival polarimetric observations made with the NACO instrument at the Very Large Telescope. Prior to this work, such a comprehensive pipeline to reduce polarimetric NACO data did not exist. We identify a total of 243 datasets of 57 potentially young stellar objects observed before NACO's decommissioning. The PIPPIN pipeline applies various levels of instrumental polarisation correction and is capable of reducing multiple observing setups, including half-wave plate or de-rotator usage and wire-grid observations. A novel template-matching method is applied to assess the detection significance of polarised signals in the reduced data. In 22 of the 57 observed targets, we detect polarised light resulting from a scattering of circumstellar dust. The detections exhibit a collection of known sub-structures, including rings, gaps, spirals, shadows, and in- or outflows of material. Since NACO was equipped with a near-infrared wavefront sensor, it made unique polarimetric observations of a number of embedded protostars. This is the first time detections of the Class I objects Elia 2-21 and YLW 16A have been published. Alongside the outlined PIPPIN pipeline, we publish an archive of the reduced data products, thereby improving the accessibility of these data for future studies.","sentences":["The observed diversity of exoplanets can possibly be traced back to the planet formation processes.","Planet-disk interactions induce sub-structures in the circumstellar disk that can be revealed via scattered light observations.","However, a high-contrast imaging technique such as polarimetric differential imaging (PDI) must first be applied to suppress the stellar diffraction halo.","In this work we present the PDI PiPelIne for NACO data (PIPPIN), which reduces the archival polarimetric observations made with the NACO instrument at the Very Large Telescope.","Prior to this work, such a comprehensive pipeline to reduce polarimetric NACO data did not exist.","We identify a total of 243 datasets of 57 potentially young stellar objects observed before NACO's decommissioning.","The PIPPIN pipeline applies various levels of instrumental polarisation correction and is capable of reducing multiple observing setups, including half-wave plate or de-rotator usage and wire-grid observations.","A novel template-matching method is applied to assess the detection significance of polarised signals in the reduced data.","In 22 of the 57 observed targets, we detect polarised light resulting from a scattering of circumstellar dust.","The detections exhibit a collection of known sub-structures, including rings, gaps, spirals, shadows, and in- or outflows of material.","Since NACO was equipped with a near-infrared wavefront sensor, it made unique polarimetric observations of a number of embedded protostars.","This is the first time detections of the Class I objects Elia 2-21 and YLW 16A have been published.","Alongside the outlined PIPPIN pipeline, we publish an archive of the reduced data products, thereby improving the accessibility of these data for future studies."],"url":"http://arxiv.org/abs/2404.02222v1","category":"astro-ph.EP"}
{"created":"2024-04-02 18:05:26","title":"Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices","abstract":"Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing LLM-based hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students' problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students' learning needs.","sentences":["Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving.","However, most existing LLM-based hint systems are limited to one single hint type.","To investigate whether and how different levels of hints can support students' problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity.","We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests.","Adding lower-level hints, like code examples with in-line comments, can better support students.","The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students' learning needs."],"url":"http://arxiv.org/abs/2404.02213v1","category":"cs.HC"}
{"created":"2024-04-02 18:00:42","title":"A Holistic Indicator of Polarization to Measure Online Sexism","abstract":"The online trend of the manosphere and feminist discourse on social networks requires a holistic measure of the level of sexism in an online community. This indicator is important for policymakers and moderators of online communities (e.g., subreddits) and computational social scientists, either to revise moderation strategies based on the degree of sexism or to match and compare the temporal sexism across different platforms and communities with real-time events and infer social scientific insights.   In this paper, we build a model that can provide a comparable holistic indicator of toxicity targeted toward male and female identity and male and female individuals. Despite previous supervised NLP methods that require annotation of toxic comments at the target level (e.g. annotating comments that are specifically toxic toward women) to detect targeted toxic comments, our indicator uses supervised NLP to detect the presence of toxicity and unsupervised word embedding association test to detect the target automatically.   We apply our model to gender discourse communities (e.g., r/TheRedPill, r/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders (i.e., sexism). Our results show that our framework accurately and consistently (93% correlation) measures the level of sexism in a community. We finally discuss how our framework can be generalized in the future to measure qualities other than toxicity (e.g. sentiment, humor) toward general-purpose targets and turn into an indicator of different sorts of polarizations.","sentences":["The online trend of the manosphere and feminist discourse on social networks requires a holistic measure of the level of sexism in an online community.","This indicator is important for policymakers and moderators of online communities (e.g., subreddits) and computational social scientists, either to revise moderation strategies based on the degree of sexism or to match and compare the temporal sexism across different platforms and communities with real-time events and infer social scientific insights.   ","In this paper, we build a model that can provide a comparable holistic indicator of toxicity targeted toward male and female identity and male and female individuals.","Despite previous supervised NLP methods that require annotation of toxic comments at the target level (e.g. annotating comments that are specifically toxic toward women) to detect targeted toxic comments, our indicator uses supervised NLP to detect the presence of toxicity and unsupervised word embedding association test to detect the target automatically.   ","We apply our model to gender discourse communities (e.g., r/TheRedPill, r/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders (i.e., sexism).","Our results show that our framework accurately and consistently (93% correlation) measures the level of sexism in a community.","We finally discuss how our framework can be generalized in the future to measure qualities other than toxicity (e.g. sentiment, humor) toward general-purpose targets and turn into an indicator of different sorts of polarizations."],"url":"http://arxiv.org/abs/2404.02205v1","category":"cs.SI"}
{"created":"2024-04-02 16:48:34","title":"Insights from the Use of Previously Unseen Neural Architecture Search Datasets","abstract":"The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning methods as well as the best results from challenge participants.","sentences":["The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network.","This goes against the hope of removing the need for experts.","Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture.","However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems.","We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract.","These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time.","We present experimentation using standard Deep Learning methods as well as the best results from challenge participants."],"url":"http://arxiv.org/abs/2404.02189v1","category":"cs.LG"}
{"created":"2024-04-02 16:07:27","title":"A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data","abstract":"Crash data is often greatly imbalanced, with the majority of crashes being non-fatal crashes, and only a small number being fatal crashes due to their rarity. Such data imbalance issue poses a challenge for crash severity modeling since it struggles to fit and interpret fatal crash outcomes with very limited samples. Usually, such data imbalance issues are addressed by data resampling methods, such as under-sampling and over-sampling techniques. However, most traditional and deep learning-based data resampling methods, such as synthetic minority oversampling technique (SMOTE) and generative Adversarial Networks (GAN) are designed dedicated to processing continuous variables. Though some resampling methods have improved to handle both continuous and discrete variables, they may have difficulties in dealing with the collapse issue associated with sparse discrete risk factors. Moreover, there is a lack of comprehensive studies that compare the performance of various resampling methods in crash severity modeling. To address the aforementioned issues, the current study proposes a crash data generation method based on the Conditional Tabular GAN. After data balancing, a crash severity model is employed to estimate the performance of classification and interpretation. A comparative study is conducted to assess classification accuracy and distribution consistency of the proposed generation method using a 4-year imbalanced crash dataset collected in Washington State, U.S. Additionally, Monte Carlo simulation is employed to estimate the performance of parameter and probability estimation in both two- and three-class imbalance scenarios. The results indicate that using synthetic data generated by CTGAN-RU for crash severity modeling outperforms using original data or synthetic data generated by other resampling methods.","sentences":["Crash data is often greatly imbalanced, with the majority of crashes being non-fatal crashes, and only a small number being fatal crashes due to their rarity.","Such data imbalance issue poses a challenge for crash severity modeling since it struggles to fit and interpret fatal crash outcomes with very limited samples.","Usually, such data imbalance issues are addressed by data resampling methods, such as under-sampling and over-sampling techniques.","However, most traditional and deep learning-based data resampling methods, such as synthetic minority oversampling technique (SMOTE) and generative Adversarial Networks (GAN) are designed dedicated to processing continuous variables.","Though some resampling methods have improved to handle both continuous and discrete variables, they may have difficulties in dealing with the collapse issue associated with sparse discrete risk factors.","Moreover, there is a lack of comprehensive studies that compare the performance of various resampling methods in crash severity modeling.","To address the aforementioned issues, the current study proposes a crash data generation method based on the Conditional Tabular GAN.","After data balancing, a crash severity model is employed to estimate the performance of classification and interpretation.","A comparative study is conducted to assess classification accuracy and distribution consistency of the proposed generation method using a 4-year imbalanced crash dataset collected in Washington State, U.S. Additionally, Monte Carlo simulation is employed to estimate the performance of parameter and probability estimation in both two- and three-class imbalance scenarios.","The results indicate that using synthetic data generated by CTGAN-RU for crash severity modeling outperforms using original data or synthetic data generated by other resampling methods."],"url":"http://arxiv.org/abs/2404.02187v1","category":"cs.LG"}
{"created":"2024-04-02 15:28:59","title":"What is to be gained by ensemble models in analysis of spectroscopic data?","abstract":"An empirical study was carried out to compare different implementations of ensemble models aimed at improving prediction in spectroscopic data. A wide range of candidate models were fitted to benchmark datasets from regression and classification settings. A statistical analysis using linear mixed model was carried out on prediction performance criteria resulting from model fits over random splits of the data. The results showed that the ensemble classifiers were able to consistently outperform candidate models in our application","sentences":["An empirical study was carried out to compare different implementations of ensemble models aimed at improving prediction in spectroscopic data.","A wide range of candidate models were fitted to benchmark datasets from regression and classification settings.","A statistical analysis using linear mixed model was carried out on prediction performance criteria resulting from model fits over random splits of the data.","The results showed that the ensemble classifiers were able to consistently outperform candidate models in our application"],"url":"http://arxiv.org/abs/2404.02184v1","category":"cs.LG"}
{"created":"2024-04-02 13:37:28","title":"Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization","abstract":"Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.","sentences":["Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development.","However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length.","To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code.","In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase.","A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability.","This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant.","We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater.","Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy."],"url":"http://arxiv.org/abs/2404.02183v1","category":"cs.SE"}
{"created":"2024-04-02 12:44:51","title":"Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database","abstract":"Machine learning (ML) has advanced quickly, particularly throughout the area of health care. The diagnosis of neurodevelopment problems using ML is a very important area of healthcare. Autism spectrum disorder (ASD) is one of the developmental disorders that is growing the fastest globally. The clinical screening tests used to identify autistic symptoms are expensive and time-consuming. But now that ML has been advanced, it's feasible to identify autism early on. Previously, many different techniques have been used in investigations. Still, none of them have produced the anticipated outcomes when it comes to the capacity to predict autistic features utilizing a clinically validated Indian ASD database. Therefore, this study aimed to develop a simple, quick, and inexpensive technique for identifying ASD by using ML. Various machine learning classifiers, including Adaboost (AB), Gradient Boost (GB), Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), Gaussian Naive Bayes (GNB), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM), were used to develop the autism prediction model. The proposed method was tested with records from the AIIMS Modified INDT-ASD (AMI) database, which were collected through an application developed by AIIMS in Delhi, India. Feature engineering has been applied to make the proposed solution easier than already available solutions. Using the proposed model, we succeeded in predicting ASD using a minimized set of 20 questions rather than the 28 questions presented in AMI with promising accuracy. In a comparative evaluation, SVM emerged as the superior model among others, with 100 $\\pm$ 0.05\\% accuracy, higher recall by 5.34\\%, and improved accuracy by 2.22\\%-6.67\\% over RF. We have also introduced a web-based solution supporting both Hindi and English.","sentences":["Machine learning (ML) has advanced quickly, particularly throughout the area of health care.","The diagnosis of neurodevelopment problems using ML is a very important area of healthcare.","Autism spectrum disorder (ASD) is one of the developmental disorders that is growing the fastest globally.","The clinical screening tests used to identify autistic symptoms are expensive and time-consuming.","But now that ML has been advanced, it's feasible to identify autism early on.","Previously, many different techniques have been used in investigations.","Still, none of them have produced the anticipated outcomes when it comes to the capacity to predict autistic features utilizing a clinically validated Indian ASD database.","Therefore, this study aimed to develop a simple, quick, and inexpensive technique for identifying ASD by using ML.","Various machine learning classifiers, including Adaboost (AB), Gradient Boost (GB), Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), Gaussian Naive Bayes (GNB), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM), were used to develop the autism prediction model.","The proposed method was tested with records from the AIIMS Modified INDT-ASD (AMI) database, which were collected through an application developed by AIIMS in Delhi, India.","Feature engineering has been applied to make the proposed solution easier than already available solutions.","Using the proposed model, we succeeded in predicting ASD using a minimized set of 20 questions rather than the 28 questions presented in AMI with promising accuracy.","In a comparative evaluation, SVM emerged as the superior model among others, with 100 $\\pm$ 0.05\\% accuracy, higher recall by 5.34\\%, and improved accuracy by 2.22\\%-6.67\\% over RF.","We have also introduced a web-based solution supporting both Hindi and English."],"url":"http://arxiv.org/abs/2404.02181v1","category":"cs.LG"}
{"created":"2024-04-02 12:08:26","title":"Real, fake and synthetic faces -- does the coin have three sides?","abstract":"With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage. To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images. The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images. Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image. ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes. From our findings, we observe that the investigated deeplearning models found it easier to detect synthetic facial images, with the ViT Patch-16 model performing best on this task with a class-averaged sensitivity, specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%, respectively. This observation was supported by further analysis of various image properties. We saw noticeable differences across the three category of images. This analysis can help us build better algorithms for facial image generation, and also shows that synthetic, deepfake and real face images are indeed three different classes.","sentences":["With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage.","To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images.","The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images.","Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image.","ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes.","From our findings, we observe that the investigated deeplearning models found it easier to detect synthetic facial images, with the ViT Patch-16 model performing best on this task with a class-averaged sensitivity, specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%, respectively.","This observation was supported by further analysis of various image properties.","We saw noticeable differences across the three category of images.","This analysis can help us build better algorithms for facial image generation, and also shows that synthetic, deepfake and real face images are indeed three different classes."],"url":"http://arxiv.org/abs/2404.01878v1","category":"cs.CV"}
{"created":"2024-04-03 17:45:40","title":"Mean-field theory of 1+1D $\\mathbb{Z}_2$ lattice gauge theory with matter","abstract":"Lattice gauge theories (LGTs) provide valuable insights into problems in strongly correlated many-body systems. Confinement which arises when matter is coupled to gauge fields is just one of the open problems, where LGT formalism can explain the underlying mechanism. However, coupling gauge fields to dynamical charges complicates the theoretical and experimental treatment of the problem. Developing a simplified mean-field theory is thus one of the ways to gain new insights into these complicated systems. Here we develop a mean-field theory of a paradigmatic 1+1D $\\mathbb{Z}_2$ lattice gauge theory with superconducting pairing term, the gauged Kitaev chain, by decoupling charge and $\\mathbb{Z}_2$ fields while enforcing the Gauss law on the mean-field level. We first determine the phase diagram of the original model in the context of confinement, which allows us to identify the symmetry-protected topological transition in the Kitaev chain as a confinement transition. We then compute the phase diagram of the effective mean-field theory, which correctly captures the main features of the original LGT. This is furthermore confirmed by the Green's function results and a direct comparison of the ground state energy. This simple LGT can be implemented in state-of-the art cold atom experiments. We thus also consider string-length histograms and the electric field polarization, which are easily accessible quantities in experimental setups and show that they reliably capture the various phases.","sentences":["Lattice gauge theories (LGTs) provide valuable insights into problems in strongly correlated many-body systems.","Confinement which arises when matter is coupled to gauge fields is just one of the open problems, where LGT formalism can explain the underlying mechanism.","However, coupling gauge fields to dynamical charges complicates the theoretical and experimental treatment of the problem.","Developing a simplified mean-field theory is thus one of the ways to gain new insights into these complicated systems.","Here we develop a mean-field theory of a paradigmatic 1+1D $\\mathbb{Z}_2$ lattice gauge theory with superconducting pairing term, the gauged Kitaev chain, by decoupling charge and $\\mathbb{Z}_2$ fields while enforcing the Gauss law on the mean-field level.","We first determine the phase diagram of the original model in the context of confinement, which allows us to identify the symmetry-protected topological transition in the Kitaev chain as a confinement transition.","We then compute the phase diagram of the effective mean-field theory, which correctly captures the main features of the original LGT.","This is furthermore confirmed by the Green's function results and a direct comparison of the ground state energy.","This simple LGT can be implemented in state-of-the art cold atom experiments.","We thus also consider string-length histograms and the electric field polarization, which are easily accessible quantities in experimental setups and show that they reliably capture the various phases."],"url":"http://arxiv.org/abs/2404.02890v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-03 17:32:50","title":"Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?","abstract":"Everyone spends some time waiting every day. HCI research has developed tools for boosting productivity while waiting. However, little is known about how people naturally spend their waiting time. We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks. The aim of this study is to understand the activities people do while waiting and the effect of situational factors. We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities. These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day. Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance. Our findings shed light on future empirical research and system design for time management.","sentences":["Everyone spends some time waiting every day.","HCI research has developed tools for boosting productivity while waiting.","However, little is known about how people naturally spend their waiting time.","We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks.","The aim of this study is to understand the activities people do while waiting and the effect of situational factors.","We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities.","These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day.","Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance.","Our findings shed light on future empirical research and system design for time management."],"url":"http://arxiv.org/abs/2404.02880v1","category":"cs.HC"}
{"created":"2024-04-03 17:21:59","title":"Sensing Resource Allocation Against Data-Poisoning Attacks in Traffic Routing","abstract":"Data-poisoning attacks can disrupt the efficient operations of transportation systems by misdirecting traffic flows via falsified data. One challenge in countering these attacks is to reduce the uncertainties on the types of attacks, such as the distribution of their targets and intensities. We introduce a resource allocation method in transportation networks to detect and distinguish different types of attacks and facilitate efficient traffic routing. The idea is to first cluster different types of attacks based on the corresponding optimal routing strategies, then allocate sensing resources to a subset of network links to distinguish attacks from different clusters via lexicographical mixed-integer programming. We illustrate the application of the proposed method using the Anaheim network, a benchmark model in traffic routing that contains more than 400 nodes and 900 links.","sentences":["Data-poisoning attacks can disrupt the efficient operations of transportation systems by misdirecting traffic flows via falsified data.","One challenge in countering these attacks is to reduce the uncertainties on the types of attacks, such as the distribution of their targets and intensities.","We introduce a resource allocation method in transportation networks to detect and distinguish different types of attacks and facilitate efficient traffic routing.","The idea is to first cluster different types of attacks based on the corresponding optimal routing strategies, then allocate sensing resources to a subset of network links to distinguish attacks from different clusters via lexicographical mixed-integer programming.","We illustrate the application of the proposed method using the Anaheim network, a benchmark model in traffic routing that contains more than 400 nodes and 900 links."],"url":"http://arxiv.org/abs/2404.02876v1","category":"math.OC"}
{"created":"2024-04-03 17:09:25","title":"Gaussian Process Regression with Soft Inequality and Monotonicity Constraints","abstract":"Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems.","sentences":["Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models.","Standard GP regression can lead to an unbounded model in which some points can take infeasible values.","We introduce a new GP method that enforces the physical constraints in a probabilistic manner.","This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC).","QHMC is an efficient way to sample from a broad class of distributions.","Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution.","Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model.","According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems."],"url":"http://arxiv.org/abs/2404.02873v1","category":"stat.ML"}
{"created":"2024-04-03 16:57:26","title":"End-To-End Self-tuning Self-supervised Time Series Anomaly Detection","abstract":"Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type. Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters. In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types.","sentences":["Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc.","A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data.","Modern neural networks have outstanding ability in modeling complex time series.","Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training.","However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels.","Our work aims to fill this gap.","We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end.","It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type.","Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters.","In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types."],"url":"http://arxiv.org/abs/2404.02865v1","category":"cs.LG"}
{"created":"2024-04-03 16:50:52","title":"Uniqueness of the blow-down limit for triple junction problem","abstract":"We prove the uniqueness of $L^1$ blow-down limit at infinity for an entire minimizing solution $u:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$ of a planar Allen-Cahn system with a triple-well potential. Consequently, $u$ can be approximated by a triple junction map at infinity. The proof exploits a careful analysis of energy upper and lower bounds, ensuring that the diffuse interface remains within a small neighborhood of the approximated triple junction at all scales.","sentences":["We prove the uniqueness of $L^1$ blow-down limit at infinity for an entire minimizing solution $u:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$ of a planar Allen-Cahn system with a triple-well potential.","Consequently, $u$ can be approximated by a triple junction map at infinity.","The proof exploits a careful analysis of energy upper and lower bounds, ensuring that the diffuse interface remains within a small neighborhood of the approximated triple junction at all scales."],"url":"http://arxiv.org/abs/2404.02859v1","category":"math.AP"}
{"created":"2024-04-03 16:42:02","title":"Study of complex nitrogen and oxygen-bearing molecules towards the high-mass protostar IRAS 18089$-$1732","abstract":"The observation of oxygen (O)- and nitrogen (N)-bearing molecules gives an idea about the complex prebiotic chemistry in the interstellar medium (ISM). In this article, we present the identification of the rotational emission lines of N-bearing molecules ethyl cyanide (C$_{2}$H$_{5}$CN), cyanoacetylene (HC$_{3}$N), and O-bearing molecules methyl formate (CH$_{3}$OCHO) towards high-mass protostar IRAS 18089$-$1732 using the Atacama Compact Array (ACA). We also detected the emission lines of both N- and O-bearing molecule formamide (NH$_{2}$CHO) in the envelope of IRAS 18089$-$1732. We have detected the $v$ = 0 and 1 states rotational emission lines of CH$_{3}$OCHO. We also detected the two vibrationally excited states of HC$_{3}$N ($v$7 = 1 and $v$7 = 2). The estimated fractional abundances of C$_{2}$H$_{5}$CN, HC$_{3}$N ($v$7 = 1), HC$_{3}$N ($v$7 = 2), and NH$_{2}$CHO towards the IRAS 18089$-$1732 are (1.40$\\pm$0.5)$\\times$10$^{-10}$, (7.5$\\pm$0.7)$\\times$10$^{-11}$, (3.1$\\pm$0.4)$\\times$10$^{-11}$, and (6.25$\\pm$0.82)$\\times$10$^{-11}$. Similarly, the estimated fractional abundances of CH$_{3}$OCHO ($v$ = 0) and CH$_{3}$OCHO ($v$ = 1) are (1.90$\\pm$0.9)$\\times$10$^{-9}$ and (8.90$\\pm$0.8)$\\times$10$^{-10}$, respectively. We also created the integrated emission maps of the detected molecules, and the observed molecules may have originated from the extended envelope of the protostar. We show that C$_{2}$H$_{5}$CN and HC$_{3}$N are most probably formed via the subsequential hydrogenation of the CH$_{2}$CHCN and the reaction between C$_{2}$H$_{2}$ and CN on the grain surface of IRAS 18089$-$1732. We found that NH$_{2}$CHO is probably produced due to the reaction between NH$_{2}$ and H$_{2}$CO in the gas phase. Similarly, CH$_{3}$OCHO is possibly created via the reaction between radical CH$_{3}$O and radical HCO on the grain surface of IRAS 18089$-$1732.","sentences":["The observation of oxygen (O)- and nitrogen (N)-bearing molecules gives an idea about the complex prebiotic chemistry in the interstellar medium (ISM).","In this article, we present the identification of the rotational emission lines of N-bearing molecules ethyl cyanide (C$_{2}$H$_{5}$CN), cyanoacetylene (HC$_{3}$N), and O-bearing molecules methyl formate (CH$_{3}$OCHO) towards high-mass protostar IRAS 18089$-$1732 using the Atacama Compact Array (ACA).","We also detected the emission lines of both N- and O-bearing molecule formamide (NH$_{2}$CHO) in the envelope of IRAS 18089$-$1732.","We have detected the $v$ = 0 and 1 states rotational emission lines of CH$_{3}$OCHO.","We also detected the two vibrationally excited states of HC$_{3}$N ($v$7 = 1 and $v$7 = 2).","The estimated fractional abundances of C$_{2}$H$_{5}$CN, HC$_{3}$N ($v$7 = 1), HC$_{3}$N ($v$7 = 2), and NH$_{2}$CHO towards the IRAS 18089$-$1732 are (1.40$\\pm$0.5)$\\times$10$^{-10}$, (7.5$\\pm$0.7)$\\times$10$^{-11}$, (3.1$\\pm$0.4)$\\times$10$^{-11}$, and (6.25$\\pm$0.82)$\\times$10$^{-11}$. Similarly, the estimated fractional abundances of CH$_{3}$OCHO ($v$ = 0) and CH$_{3}$OCHO ($v$ = 1) are (1.90$\\pm$0.9)$\\times$10$^{-9}$ and (8.90$\\pm$0.8)$\\times$10$^{-10}$, respectively.","We also created the integrated emission maps of the detected molecules, and the observed molecules may have originated from the extended envelope of the protostar.","We show that C$_{2}$H$_{5}$CN and HC$_{3}$N are most probably formed via the subsequential hydrogenation of the CH$_{2}$CHCN and the reaction between C$_{2}$H$_{2}$ and CN on the grain surface of IRAS 18089$-$1732.","We found that NH$_{2}$CHO is probably produced due to the reaction between NH$_{2}$ and H$_{2}$CO in the gas phase.","Similarly, CH$_{3}$OCHO is possibly created via the reaction between radical CH$_{3}$O and radical HCO on the grain surface of IRAS 18089$-$1732."],"url":"http://arxiv.org/abs/2404.02857v1","category":"astro-ph.GA"}
{"created":"2024-04-03 16:37:38","title":"An Information Bottleneck Approach for Markov Model Construction","abstract":"Markov state models (MSMs) are valuable for studying dynamics of protein conformational changes via statistical analysis of molecular dynamics (MD) simulations. In MSMs, the complex configuration space is coarse-grained into conformational states, with the dynamics modeled by a series of Markovian transitions among these states at discrete lag times. Constructing the Markovian model at a specific lag time requires state defined without significant internal energy barriers, enabling internal dynamics relaxation within the lag time. This process coarse grains time and space, integrating out rapid motions within metastable states. This work introduces a continuous embedding approach for molecular conformations using the state predictive information bottleneck (SPIB), which unifies dimensionality reduction and state space partitioning via a continuous, machine learned basis set. Without explicit optimization of VAMP-based scores, SPIB demonstrates state-of-the-art performance in identifying slow dynamical processes and constructing predictive multi-resolution Markovian models. When applied to mini-proteins trajectories, SPIB showcases unique advantages compared to competing methods. It automatically adjusts the number of metastable states based on a specified minimal time resolution, eliminating the need for manual tuning. While maintaining efficacy in dynamical properties, SPIB excels in accurately distinguishing metastable states and capturing numerous well-populated macrostates. Furthermore, SPIB's ability to learn a low-dimensional continuous embedding of the underlying MSMs enhances the interpretation of dynamic pathways. Accordingly, we propose SPIB as an easy-to-implement methodology for end-to-end MSM construction.","sentences":["Markov state models (MSMs) are valuable for studying dynamics of protein conformational changes via statistical analysis of molecular dynamics (MD) simulations.","In MSMs, the complex configuration space is coarse-grained into conformational states, with the dynamics modeled by a series of Markovian transitions among these states at discrete lag times.","Constructing the Markovian model at a specific lag time requires state defined without significant internal energy barriers, enabling internal dynamics relaxation within the lag time.","This process coarse grains time and space, integrating out rapid motions within metastable states.","This work introduces a continuous embedding approach for molecular conformations using the state predictive information bottleneck (SPIB), which unifies dimensionality reduction and state space partitioning via a continuous, machine learned basis set.","Without explicit optimization of VAMP-based scores, SPIB demonstrates state-of-the-art performance in identifying slow dynamical processes and constructing predictive multi-resolution Markovian models.","When applied to mini-proteins trajectories, SPIB showcases unique advantages compared to competing methods.","It automatically adjusts the number of metastable states based on a specified minimal time resolution, eliminating the need for manual tuning.","While maintaining efficacy in dynamical properties, SPIB excels in accurately distinguishing metastable states and capturing numerous well-populated macrostates.","Furthermore, SPIB's ability to learn a low-dimensional continuous embedding of the underlying MSMs enhances the interpretation of dynamic pathways.","Accordingly, we propose SPIB as an easy-to-implement methodology for end-to-end MSM construction."],"url":"http://arxiv.org/abs/2404.02856v1","category":"physics.bio-ph"}
{"created":"2024-04-03 16:36:21","title":"Axisymmetric steady Navier-Stokes flows under suction","abstract":"We prove the existence of solutions for the axisymmetric steady Navier-Stokes system around an infinite cylinder under external forces. The solutions are constructed to be decaying at the horizontal infinity, despite an analogue of the Stokes paradox for the linearized system, and having neither periodicity nor decay in the vertical direction. The proof is based on perturbation of the nonlinear system around a suction flow. The class of functions in this paper, which is a subspace of the space of Fourier transformed vector finite Radon measures, is inspired by Giga-Saal (2013) treating rotating boundary layers.","sentences":["We prove the existence of solutions for the axisymmetric steady Navier-Stokes system around an infinite cylinder under external forces.","The solutions are constructed to be decaying at the horizontal infinity, despite an analogue of the Stokes paradox for the linearized system, and having neither periodicity nor decay in the vertical direction.","The proof is based on perturbation of the nonlinear system around a suction flow.","The class of functions in this paper, which is a subspace of the space of Fourier transformed vector finite Radon measures, is inspired by Giga-Saal (2013) treating rotating boundary layers."],"url":"http://arxiv.org/abs/2404.02854v1","category":"math.AP"}
{"created":"2024-04-03 15:58:17","title":"Anti-Coulomb interaction between charges in a dielectric medium","abstract":"The free energy of ion solvation can be decomposed into enthalpic and entropic contributions. This helps to understand the connection between the dielectric properties and the underlying forces. We present a simple linear-response model of screened charge interactions that provides an alternative understanding of solvation barriers. Moreover, it explains the ``anti-Coulomb'' interactions (attraction between like-charged ions and repulsion between opposite-charged ions) observed in both simulations and experiments. We show that this is a universal behavior associated to the non-local response function of any dielectric or metallic system.","sentences":["The free energy of ion solvation can be decomposed into enthalpic and entropic contributions.","This helps to understand the connection between the dielectric properties and the underlying forces.","We present a simple linear-response model of screened charge interactions that provides an alternative understanding of solvation barriers.","Moreover, it explains the ``anti-Coulomb'' interactions (attraction between like-charged ions and repulsion between opposite-charged ions) observed in both simulations and experiments.","We show that this is a universal behavior associated to the non-local response function of any dielectric or metallic system."],"url":"http://arxiv.org/abs/2404.02824v1","category":"cond-mat.soft"}
{"created":"2024-04-03 15:43:47","title":"Optimal distributed control with stability guarantees by training a network of neural closed-loop maps","abstract":"This paper proposes a novel approach to improve the performance of distributed nonlinear control systems while preserving stability by leveraging Deep Neural Networks (DNNs). We build upon the Neural System Level Synthesis (Neur-SLS) framework and introduce a method to parameterize stabilizing control policies that are distributed across a network topology. A distinctive feature is that we iteratively minimize an arbitrary control cost function through an unconstrained optimization algorithm, all while preserving the stability of the overall network architecture by design. This is achieved through two key steps. First, we establish a method to parameterize interconnected Recurrent Equilibrium Networks (RENs) that guarantees a bounded $\\mathcal{L}_2$ gain at the network level. This ensures stability. Second, we demonstrate how the information flow within the network is preserved, enabling a fully distributed implementation where each subsystem only communicates with its neighbors. To showcase the effectiveness of our approach, we present a simulation of a distributed formation control problem for a fleet of vehicles. The simulation demonstrates how the proposed neural controller enables the vehicles to maintain a desired formation while navigating obstacles and avoiding collisions, all while guaranteeing network stability.","sentences":["This paper proposes a novel approach to improve the performance of distributed nonlinear control systems while preserving stability by leveraging Deep Neural Networks (DNNs).","We build upon the Neural System Level Synthesis (Neur-SLS) framework and introduce a method to parameterize stabilizing control policies that are distributed across a network topology.","A distinctive feature is that we iteratively minimize an arbitrary control cost function through an unconstrained optimization algorithm, all while preserving the stability of the overall network architecture by design.","This is achieved through two key steps.","First, we establish a method to parameterize interconnected Recurrent Equilibrium Networks (RENs) that guarantees a bounded $\\mathcal{L}_2$ gain at the network level.","This ensures stability.","Second, we demonstrate how the information flow within the network is preserved, enabling a fully distributed implementation where each subsystem only communicates with its neighbors.","To showcase the effectiveness of our approach, we present a simulation of a distributed formation control problem for a fleet of vehicles.","The simulation demonstrates how the proposed neural controller enables the vehicles to maintain a desired formation while navigating obstacles and avoiding collisions, all while guaranteeing network stability."],"url":"http://arxiv.org/abs/2404.02820v1","category":"math.OC"}
{"created":"2024-04-03 15:31:49","title":"Wideband Beamforming for Near-Field Communications with Circular Arrays","abstract":"The beamforming performance of the uniform circular array (UCA) in near-field wideband communication systems is investigated. Compared to uniform linear array (ULA), UCA exhibits uniform effective array aperture in all directions, thus enabling more users to benefit from near-field communications. In this paper, the unique beam squint effect in near-field wideband UCA systems is comprehensively analyzed in both the distance and angular domains. It is rigorously demonstrated that the beam focal point only exists at a specific frequency in wideband UCA systems, resulting in significant beamforming loss. To alleviate this unique beam squint effect, the true-time delay (TTD)-based beamforming architecture is exploited. In particular, two wideband beamforming optimization approaches leveraging TTD units are proposed. 1) Analytical approach: In this approach, the phase shifters (PSs) and the time delay of TTD units are designed based on the analytical formula for beamforming gain. Following this design, the minimum number of TTD units required to achieve a predetermined beamforming gain is quantified. 2) Joint-optimization approach: In this method, the PSs and the TTD units are jointly optimized under practical maximum delay constraints to approximate the optimal unconstrained analog beamformer. Specifically, an efficient alternating optimization algorithm is proposed, where the PSs and the TTD units are alternately updated using either the closed-form solution or the low-complexity linear search approach. Extensive numerical results demonstrate that 1) the proposed beamforming schemes effectively mitigate the beam squint effect, and 2) the joint-optimization approach outperforms the analytical approach in terms of array gain and achievable spectral efficiency.","sentences":["The beamforming performance of the uniform circular array (UCA) in near-field wideband communication systems is investigated.","Compared to uniform linear array (ULA), UCA exhibits uniform effective array aperture in all directions, thus enabling more users to benefit from near-field communications.","In this paper, the unique beam squint effect in near-field wideband UCA systems is comprehensively analyzed in both the distance and angular domains.","It is rigorously demonstrated that the beam focal point only exists at a specific frequency in wideband UCA systems, resulting in significant beamforming loss.","To alleviate this unique beam squint effect, the true-time delay (TTD)-based beamforming architecture is exploited.","In particular, two wideband beamforming optimization approaches leveraging TTD units are proposed.","1) Analytical approach: In this approach, the phase shifters (PSs) and the time delay of TTD units are designed based on the analytical formula for beamforming gain.","Following this design, the minimum number of TTD units required to achieve a predetermined beamforming gain is quantified.","2) Joint-optimization approach: In this method, the PSs and the TTD units are jointly optimized under practical maximum delay constraints to approximate the optimal unconstrained analog beamformer.","Specifically, an efficient alternating optimization algorithm is proposed, where the PSs and the TTD units are alternately updated using either the closed-form solution or the low-complexity linear search approach.","Extensive numerical results demonstrate that 1) the proposed beamforming schemes effectively mitigate the beam squint effect, and 2) the joint-optimization approach outperforms the analytical approach in terms of array gain and achievable spectral efficiency."],"url":"http://arxiv.org/abs/2404.02811v1","category":"cs.IT"}
{"created":"2024-04-03 15:20:24","title":"Efficient Multi-Vector Dense Retrieval Using Bit Vectors","abstract":"Dense retrieval techniques employ pre-trained large language models to build a high-dimensional representation of queries and passages. These representations compute the relevance of a passage w.r.t. to a query using efficient similarity measures. In this line, multi-vector representations show improved effectiveness at the expense of a one-order-of-magnitude increase in memory footprint and query latency by encoding queries and documents on a per-token level. Recently, PLAID has tackled these problems by introducing a centroid-based term representation to reduce the memory impact of multi-vector systems. By exploiting a centroid interaction mechanism, PLAID filters out non-relevant documents, thus reducing the cost of the successive ranking stages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit vectors'' (EMVB), a novel framework for efficient query processing in multi-vector dense retrieval. First, EMVB employs a highly efficient pre-filtering step of passages using optimized bit vectors. Second, the computation of the centroid interaction happens column-wise, exploiting SIMD instructions, thus reducing its latency. Third, EMVB leverages Product Quantization (PQ) to reduce the memory footprint of storing vector representations while jointly allowing for fast late interaction. Fourth, we introduce a per-document term filtering method that further improves the efficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB is up to 2.8x faster while reducing the memory footprint by 1.8x with no loss in retrieval accuracy compared to PLAID.","sentences":["Dense retrieval techniques employ pre-trained large language models to build a high-dimensional representation of queries and passages.","These representations compute the relevance of a passage w.r.t.","to a query using efficient similarity measures.","In this line, multi-vector representations show improved effectiveness at the expense of a one-order-of-magnitude increase in memory footprint and query latency by encoding queries and documents on a per-token level.","Recently, PLAID has tackled these problems by introducing a centroid-based term representation to reduce the memory impact of multi-vector systems.","By exploiting a centroid interaction mechanism, PLAID filters out non-relevant documents, thus reducing the cost of the successive ranking stages.","This paper proposes ``Efficient Multi-Vector dense retrieval with Bit vectors'' (EMVB), a novel framework for efficient query processing in multi-vector dense retrieval.","First, EMVB employs a highly efficient pre-filtering step of passages using optimized bit vectors.","Second, the computation of the centroid interaction happens column-wise, exploiting SIMD instructions, thus reducing its latency.","Third, EMVB leverages Product Quantization (PQ) to reduce the memory footprint of storing vector representations while jointly allowing for fast late interaction.","Fourth, we introduce a per-document term filtering method that further improves the efficiency of the last step.","Experiments on MS MARCO and LoTTE show that EMVB is up to 2.8x faster while reducing the memory footprint by 1.8x with no loss in retrieval accuracy compared to PLAID."],"url":"http://arxiv.org/abs/2404.02805v1","category":"cs.IR"}
{"created":"2024-04-03 15:03:46","title":"Quantum enhanced mechanical rotation sensing using wavefront photonic gears","abstract":"Quantum metrology leverages quantum correlations for enhanced parameter estimation. Recently, structured light enabled increased resolution and sensitivity in quantum metrology systems. However, lossy and complex setups impacting photon flux, hinder true quantum advantage while using high dimensional structured light. We introduce a straightforward mechanical rotation quantum sensing mechanism, employing high-dimensional structured light and a compact high-flux (45,000 coincidence counts per second) N00N state source with N=2. The system utilizes two opposite spiral phase plates with topological charge of up to l=16 that convert mechanical rotation into wavefront phase shifts, and exhibit a 16-fold enhanced super-resolution and 25-fold enhanced sensitivity between different topological charges, while retaining the acquisition times and with negligible change in coincidence count. Furthermore, the high photon flux enables to detect mechanical angular acceleration in real-time. Our approach paves the way for highly sensitive quantum measurements, applicable to various interferometric schemes.","sentences":["Quantum metrology leverages quantum correlations for enhanced parameter estimation.","Recently, structured light enabled increased resolution and sensitivity in quantum metrology systems.","However, lossy and complex setups impacting photon flux, hinder true quantum advantage while using high dimensional structured light.","We introduce a straightforward mechanical rotation quantum sensing mechanism, employing high-dimensional structured light and a compact high-flux (45,000 coincidence counts per second)","N00N state source with N=2.","The system utilizes two opposite spiral phase plates with topological charge of up to l=16 that convert mechanical rotation into wavefront phase shifts, and exhibit a 16-fold enhanced super-resolution and 25-fold enhanced sensitivity between different topological charges, while retaining the acquisition times and with negligible change in coincidence count.","Furthermore, the high photon flux enables to detect mechanical angular acceleration in real-time.","Our approach paves the way for highly sensitive quantum measurements, applicable to various interferometric schemes."],"url":"http://arxiv.org/abs/2404.02797v1","category":"quant-ph"}
{"created":"2024-04-03 14:59:14","title":"xGASS: The scatter of the HI-halo mass relation of central galaxies","abstract":"Empirical studies of the relationship between baryonic matter in galaxies and the gravitational potential of their host halos are important to constrain our theoretical framework for galaxy formation and evolution. One such relation, between the atomic hydrogen (HI) mass of central galaxies ($M_{\\rm{HI,c}}$) and the total mass of their host halos ($M_{\\rm{halo}}$), has attracted significant interest in the last few years. In this work, we use the extended GALEX Arecibo SDSS Survey to examine the scatter of the HI-halo mass relation for a representative sample of central galaxies. Our findings reveal a flat median relation at $\\rm{log}_{10}$$(M_{\\rm{HI,c}}/\\rm{M}_{\\odot}) \\approx 9.40$, across $11.1 < \\rm{log}_{10}$$(M_{\\rm{halo}}/\\rm{M_{\\odot}}) < 14.1$. This flat relation stems from the statistical dominance of star-forming, disc galaxies at low $M_{\\rm{halo}}$ in combination with the increasing prevalence of passive, high stellar-concentration systems at higher $M_{\\rm{halo}}$. The scatter of this relation and the stellar specific angular momentum of centrals have a strong link (Spearman's rank correlation coefficient $\\geq 0.5$). Comparisons with simulations suggest that the kinematic state of host halos may be primarily driving this scatter. Our findings highlight that the HI-halo mass parameter space is too complex to be completely represented by simple median or average relations and we show that tensions with previous works are most likely due to selection biases. We recommend that future observational studies, and their comparisons with theoretical models, bin central galaxies also by their secondary properties to enable a statistically robust understanding of the processes regulating the cold gas content within central galaxies of dark-matter halos.","sentences":["Empirical studies of the relationship between baryonic matter in galaxies and the gravitational potential of their host halos are important to constrain our theoretical framework for galaxy formation and evolution.","One such relation, between the atomic hydrogen (HI) mass of central galaxies ($M_{\\rm{HI,c}}$) and the total mass of their host halos ($M_{\\rm{halo}}$), has attracted significant interest in the last few years.","In this work, we use the extended GALEX Arecibo SDSS Survey to examine the scatter of the HI-halo mass relation for a representative sample of central galaxies.","Our findings reveal a flat median relation at $\\rm{log}_{10}$$(M_{\\rm{HI,c}}/\\rm{M}_{\\odot})","\\approx 9.40$, across $11.1 < \\rm{log}_{10}$$(M_{\\rm{halo}}/\\rm{M_{\\odot}})","< 14.1$.","This flat relation stems from the statistical dominance of star-forming, disc galaxies at low $M_{\\rm{halo}}$ in combination with the increasing prevalence of passive, high stellar-concentration systems at higher $M_{\\rm{halo}}$. The scatter of this relation and the stellar specific angular momentum of centrals have a strong link (Spearman's rank correlation coefficient $\\geq 0.5$).","Comparisons with simulations suggest that the kinematic state of host halos may be primarily driving this scatter.","Our findings highlight that the HI-halo mass parameter space is too complex to be completely represented by simple median or average relations and we show that tensions with previous works are most likely due to selection biases.","We recommend that future observational studies, and their comparisons with theoretical models, bin central galaxies also by their secondary properties to enable a statistically robust understanding of the processes regulating the cold gas content within central galaxies of dark-matter halos."],"url":"http://arxiv.org/abs/2404.02793v1","category":"astro-ph.GA"}
{"created":"2024-04-03 14:56:06","title":"Mixed-encoding one-photon-interference quantum secure direct communication","abstract":"Quantum secure direct communication (QSDC) guarantees both the security and reliability of information transmission using quantum states. One-photon-interference QSDC (OPI-QSDC) is a technique that enhances the transmission distance and ensures secure point-to-point information transmission, but it requires complex phase locking technology. This paper proposes a mixed-encoding one-photon-interference QSDC (MO-QSDC) protocol that removes the need for phase locking technology. Numerical simulations demonstrate that the MO-QSDC protocol could also beat the PLOB bound.","sentences":["Quantum secure direct communication (QSDC) guarantees both the security and reliability of information transmission using quantum states.","One-photon-interference QSDC (OPI-QSDC) is a technique that enhances the transmission distance and ensures secure point-to-point information transmission, but it requires complex phase locking technology.","This paper proposes a mixed-encoding one-photon-interference QSDC (MO-QSDC) protocol that removes the need for phase locking technology.","Numerical simulations demonstrate that the MO-QSDC protocol could also beat the PLOB bound."],"url":"http://arxiv.org/abs/2404.02787v1","category":"quant-ph"}
{"created":"2024-04-03 14:54:59","title":"Minimizing the Number of Tardy Jobs and Maximal Tardiness on a Single Machine is NP-hard","abstract":"This paper resolves a long-standing open question in bicriteria scheduling regarding the complexity of a single machine scheduling problem which combines the number of tardy jobs and the maximal tardiness criteria. We use the lexicographic approach with the maximal tardiness being the primary criterion. Accordingly, the objective is to find, among all solutions minimizing the maximal tardiness, the one which has the minimum number of tardy jobs. The complexity of this problem has been open for over thirty years, and has been known since then to be one of the most challenging open questions in multicriteria scheduling. We resolve this question by proving that the problem is strongly NP-hard. We also prove that the problem is at least weakly NP-hard when we switch roles between the two criteria (i.e., when the number of tardy jobs is the primary criterion). Finally, we provide hardness results for two other approaches (constraint and a priori approaches) to deal with these two criteria.","sentences":["This paper resolves a long-standing open question in bicriteria scheduling regarding the complexity of a single machine scheduling problem which combines the number of tardy jobs and the maximal tardiness criteria.","We use the lexicographic approach with the maximal tardiness being the primary criterion.","Accordingly, the objective is to find, among all solutions minimizing the maximal tardiness, the one which has the minimum number of tardy jobs.","The complexity of this problem has been open for over thirty years, and has been known since then to be one of the most challenging open questions in multicriteria scheduling.","We resolve this question by proving that the problem is strongly NP-hard.","We also prove that the problem is at least weakly NP-hard when we switch roles between the two criteria (i.e., when the number of tardy jobs is the primary criterion).","Finally, we provide hardness results for two other approaches (constraint and a priori approaches) to deal with these two criteria."],"url":"http://arxiv.org/abs/2404.02784v1","category":"cs.DS"}
{"created":"2024-04-03 14:36:29","title":"Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators","abstract":"This paper considers the implicit Euler discretization of Levant's arbitrary order robust exact differentiator in presence of sampled measurements. Existing implicit discretizations of that differentiator are shown to exhibit either unbounded bias errors or, surprisingly, discretization chattering despite the use of the implicit discretization. A new, proper implicit discretization that exhibits neither of these two detrimental effects is proposed by computing the differentiator's outputs as appropriately designed linear combinations of its state variables. A numerical differentiator implementation is discussed and closed-form stability conditions for arbitrary differentiation orders are given. The influence of bounded measurement noise and numerical approximation errors is formally analyzed. Numerical simulations confirm the obtained results.","sentences":["This paper considers the implicit Euler discretization of Levant's arbitrary order robust exact differentiator in presence of sampled measurements.","Existing implicit discretizations of that differentiator are shown to exhibit either unbounded bias errors or, surprisingly, discretization chattering despite the use of the implicit discretization.","A new, proper implicit discretization that exhibits neither of these two detrimental effects is proposed by computing the differentiator's outputs as appropriately designed linear combinations of its state variables.","A numerical differentiator implementation is discussed and closed-form stability conditions for arbitrary differentiation orders are given.","The influence of bounded measurement noise and numerical approximation errors is formally analyzed.","Numerical simulations confirm the obtained results."],"url":"http://arxiv.org/abs/2404.02770v1","category":"math.NA"}
{"created":"2024-04-03 14:19:35","title":"Closing the Implementation Gap in MC: Fully Chemical Synchronization and Detection for Cellular Receivers","abstract":"In the context of the Internet of Bio-Nano Things (IoBNT), nano-devices are envisioned to perform complex tasks collaboratively, i.e., by communicating with each other. One candidate for the implementation of such devices are engineered cells due to their inherent biocompatibility. However, because each engineered cell has only little computational capabilities, transmitter and receiver (RX) functionalities can afford only limited complexity. In this paper, we propose a simple, yet modular, architecture for a cellular RX that is capable of processing a stream of observed symbols using chemical reaction networks. Furthermore, we propose two specific detector implementations for the RX. The first detector is based on a machine learning model that is trained offline, i.e., before the cellular RX is deployed. The second detector utilizes pilot symbol-based training and is therefore able to continuously adapt to changing channel conditions online, i.e., after deployment. To coordinate the different chemical processing steps involved in symbol detection, the proposed cellular RX leverages an internal chemical timer. Furthermore, the RX is synchronized with the transmitter via external, i.e., extracellular, signals. Finally, the proposed architecture is validated using theoretical analysis and stochastic simulations. The presented results confirm the feasibility of both proposed implementations and reveal that the proposed online learning-based RX is able to perform reliable detection even in initially unknown or slowly changing channels. By its modular design and exclusively chemical implementation, the proposed RX contributes towards the realization of versatile and biocompatible nano-scale communication networks for IoBNT applications narrowing the existing implementation gap in cellular molecular communication (MC).","sentences":["In the context of the Internet of Bio-Nano Things (IoBNT), nano-devices are envisioned to perform complex tasks collaboratively, i.e., by communicating with each other.","One candidate for the implementation of such devices are engineered cells due to their inherent biocompatibility.","However, because each engineered cell has only little computational capabilities, transmitter and receiver (RX) functionalities can afford only limited complexity.","In this paper, we propose a simple, yet modular, architecture for a cellular RX that is capable of processing a stream of observed symbols using chemical reaction networks.","Furthermore, we propose two specific detector implementations for the RX.","The first detector is based on a machine learning model that is trained offline, i.e., before the cellular RX is deployed.","The second detector utilizes pilot symbol-based training and is therefore able to continuously adapt to changing channel conditions online, i.e., after deployment.","To coordinate the different chemical processing steps involved in symbol detection, the proposed cellular RX leverages an internal chemical timer.","Furthermore, the RX is synchronized with the transmitter via external, i.e., extracellular, signals.","Finally, the proposed architecture is validated using theoretical analysis and stochastic simulations.","The presented results confirm the feasibility of both proposed implementations and reveal that the proposed online learning-based RX is able to perform reliable detection even in initially unknown or slowly changing channels.","By its modular design and exclusively chemical implementation, the proposed RX contributes towards the realization of versatile and biocompatible nano-scale communication networks for IoBNT applications narrowing the existing implementation gap in cellular molecular communication (MC)."],"url":"http://arxiv.org/abs/2404.02765v1","category":"cs.ET"}
{"created":"2024-04-03 14:16:15","title":"Impact and Integration of Mini Photovoltaic Systems on Electric Power Distribution Grids","abstract":"This work analyzes the impact of varying concentrations mini-photovoltaic (MPV) systems, often referred to as balcony power plants, on the stability and control of the low-voltage (LV) grid. By local energy use and potentially reversing meter operation, we focus on how these MPV systems transform grid dynamics and elucidate consumer participation in the energy transition. We scrutinize the effects of these systems on power quality, power loss, transformer loading, and the functioning of other inverter-based voltage-regulating distributed energy resources (DER). Owing to the rise in renewable output from MPVs, the emerging bidirectional energy flow poses challenges for distribution grids abundant with DERs. Our case studies, featuring sensitivity analysis and comparison of distributed and decentralized DER control strategies, highlight that autonomous inverters are essential for providing ancillary services. With the growing use of battery energy storage (BES) systems in LV grids for these services, the need for adaptable DER control strategies becomes increasingly evident.","sentences":["This work analyzes the impact of varying concentrations mini-photovoltaic (MPV) systems, often referred to as balcony power plants, on the stability and control of the low-voltage (LV) grid.","By local energy use and potentially reversing meter operation, we focus on how these MPV systems transform grid dynamics and elucidate consumer participation in the energy transition.","We scrutinize the effects of these systems on power quality, power loss, transformer loading, and the functioning of other inverter-based voltage-regulating distributed energy resources (DER).","Owing to the rise in renewable output from MPVs, the emerging bidirectional energy flow poses challenges for distribution grids abundant with DERs.","Our case studies, featuring sensitivity analysis and comparison of distributed and decentralized DER control strategies, highlight that autonomous inverters are essential for providing ancillary services.","With the growing use of battery energy storage (BES) systems in LV grids for these services, the need for adaptable DER control strategies becomes increasingly evident."],"url":"http://arxiv.org/abs/2404.02763v1","category":"eess.SY"}
{"created":"2024-04-03 14:05:50","title":"Gate-tunable subband degeneracy in semiconductor nanowires","abstract":"Degeneracy and symmetry have a profound relation in quantum systems. Here, we report gate-tunable subband degeneracy in PbTe nanowires with a nearly symmetric cross-sectional shape. The degeneracy is revealed in electron transport by the absence of a quantized plateau. Utilizing a dual gate design, we can apply an electric field to lift the degeneracy, reflected as emergence of the plateau. This degeneracy and its tunable lifting were challenging to observe in previous nanowire experiments, possibly due to disorder. Numerical simulations can qualitatively capture our observation, shedding light on device parameters for future applications.","sentences":["Degeneracy and symmetry have a profound relation in quantum systems.","Here, we report gate-tunable subband degeneracy in PbTe nanowires with a nearly symmetric cross-sectional shape.","The degeneracy is revealed in electron transport by the absence of a quantized plateau.","Utilizing a dual gate design, we can apply an electric field to lift the degeneracy, reflected as emergence of the plateau.","This degeneracy and its tunable lifting were challenging to observe in previous nanowire experiments, possibly due to disorder.","Numerical simulations can qualitatively capture our observation, shedding light on device parameters for future applications."],"url":"http://arxiv.org/abs/2404.02760v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 13:53:03","title":"Investigating the Relation Between Problem Hardness and QUBO Properties","abstract":"Combinatorial optimization problems, integral to various scientific and industrial applications, often vary significantly in their complexity and computational difficulty. Transforming such problems into Quadratic Unconstrained Binary Optimization (QUBO) has regained considerable research attention in recent decades due to the central role of QUBO in Quantum Annealing. This work aims to shed some light on the relationship between the problems' properties. In particular, we examine how the spectral gap of the QUBO formulation correlates with the original problem, since it has an impact on how efficiently it can be solved on quantum computers. We analyze two well-known problems from Machine Learning, namely Clustering and Support Vector Machine (SVM) training, regarding the spectral gaps of their respective QUBO counterparts. An empirical evaluation provides interesting insights, showing that the spectral gap of Clustering QUBO instances positively correlates with data separability, while for SVM QUBO the opposite is true.","sentences":["Combinatorial optimization problems, integral to various scientific and industrial applications, often vary significantly in their complexity and computational difficulty.","Transforming such problems into Quadratic Unconstrained Binary Optimization (QUBO) has regained considerable research attention in recent decades due to the central role of QUBO in Quantum Annealing.","This work aims to shed some light on the relationship between the problems' properties.","In particular, we examine how the spectral gap of the QUBO formulation correlates with the original problem, since it has an impact on how efficiently it can be solved on quantum computers.","We analyze two well-known problems from Machine Learning, namely Clustering and Support Vector Machine (SVM) training, regarding the spectral gaps of their respective QUBO counterparts.","An empirical evaluation provides interesting insights, showing that the spectral gap of Clustering QUBO instances positively correlates with data separability, while for SVM QUBO the opposite is true."],"url":"http://arxiv.org/abs/2404.02751v1","category":"quant-ph"}
{"created":"2024-04-03 13:39:59","title":"IEEE VIS Workshop on Visualization for Climate Action and Sustainability","abstract":"This first workshop on visualization for climate action and sustainability aims to explore and consolidate the role of data visualization in accelerating action towards addressing the current environmental crisis. Given the urgency and impact of the environmental crisis, we ask how our skills, research methods, and innovations can help by empowering people and organizations. We believe visualization holds an enormous power to aid understanding, decision making, communication, discussion, participation, education, and exploration of complex topics around climate action and sustainability. Hence, this workshop invites submissions and discussion around these topics with the goal of establishing a visible and actionable link between these fields and their respective stakeholders. The workshop solicits work-in-progress and research papers as well as pictorials and interactive demos from the whole range of visualization research (dashboards, interactive spaces, scientific visualization, storytelling, visual analytics, explainability etc.), within the context of environmentalism (climate science, sustainability, energy, circular economy, biodiversity, etc.) and across a range of scenarios from public awareness and understanding, visual analysis, expert decision making, science communication, personal decision making etc. After presentations of submissions, the workshop will feature dedicated discussion groups around data driven interactive experiences for the public, and tools for personal and professional decision making.","sentences":["This first workshop on visualization for climate action and sustainability aims to explore and consolidate the role of data visualization in accelerating action towards addressing the current environmental crisis.","Given the urgency and impact of the environmental crisis, we ask how our skills, research methods, and innovations can help by empowering people and organizations.","We believe visualization holds an enormous power to aid understanding, decision making, communication, discussion, participation, education, and exploration of complex topics around climate action and sustainability.","Hence, this workshop invites submissions and discussion around these topics with the goal of establishing a visible and actionable link between these fields and their respective stakeholders.","The workshop solicits work-in-progress and research papers as well as pictorials and interactive demos from the whole range of visualization research (dashboards, interactive spaces, scientific visualization, storytelling, visual analytics, explainability etc.), within the context of environmentalism (climate science, sustainability, energy, circular economy, biodiversity, etc.) and across a range of scenarios from public awareness and understanding, visual analysis, expert decision making, science communication, personal decision making etc.","After presentations of submissions, the workshop will feature dedicated discussion groups around data driven interactive experiences for the public, and tools for personal and professional decision making."],"url":"http://arxiv.org/abs/2404.02743v1","category":"cs.HC"}
{"created":"2024-04-03 13:35:13","title":"Entanglement structures from modified IR geometry","abstract":"We investigate a new proposal connecting the geometry at various radial scales in asymptotic AdS spacetime with entanglement structure at corresponding real-space length scales of the boundary theory. With this proposal, the bulk IR geometry encodes the long-scale entanglement structure of the dual quantum system. We consider two distinct types of IR geometries, namely the spherical case and the hyperbolic case, which are intimately related to the physics of differential entropy and brane-world holography separately. We explore the corresponding change in the dual long-scale entanglement structures, utilizing the tools of the Ryu-Takayanagi formula, conditional mutual information, and partial entanglement entropy. The results indicate that modifying the IR geometry leads to a redistribution of entanglement at scales longer than a critical length determined by the location of the IR region, with the two modified IR geometries corresponding to two opposite ways of redistribution. Furthermore, we establish the maximum amount of entanglement that can be modified, which is proportional to the area of the IR region.","sentences":["We investigate a new proposal connecting the geometry at various radial scales in asymptotic AdS spacetime with entanglement structure at corresponding real-space length scales of the boundary theory.","With this proposal, the bulk IR geometry encodes the long-scale entanglement structure of the dual quantum system.","We consider two distinct types of IR geometries, namely the spherical case and the hyperbolic case, which are intimately related to the physics of differential entropy and brane-world holography separately.","We explore the corresponding change in the dual long-scale entanglement structures, utilizing the tools of the Ryu-Takayanagi formula, conditional mutual information, and partial entanglement entropy.","The results indicate that modifying the IR geometry leads to a redistribution of entanglement at scales longer than a critical length determined by the location of the IR region, with the two modified IR geometries corresponding to two opposite ways of redistribution.","Furthermore, we establish the maximum amount of entanglement that can be modified, which is proportional to the area of the IR region."],"url":"http://arxiv.org/abs/2404.02737v1","category":"hep-th"}
{"created":"2024-04-03 13:34:40","title":"Floquet topological transitions in 2D Su-Schrieffer-Heeger model: interplay between time reversal symmetry breaking and dimerization","abstract":"We theoretically study the 2D Su-Schrieffer-Heeger model in the context of Floquet topological insulators (FTIs). FTIs are systems which undergo topological phase transitions, governed by Chern numbers, as a result of time reversal symmetry (TRS) breaking by a time periodic process. In our proposed model, the condition of TRS breaking is achieved by circularly polarized light irradiation. We analytically show that TRS breaking is forbidden in the absence of second order neighbors hopping. In the absence of light irradiation, we identify a symmetry-protected degeneracy and prove the appearance of a flat band along a specific direction in the momentum space. Furthermore, we employ a novel method to show that the four unit cell atoms, in the absence of irradiation, can be interpreted as conserved spin states. With the breaking of TRS via light irradiation, these spin states are no longer conserved, leading to the emergence of chiral edge states. We also show how the interplay between the TRS breaking and dimerization leads to a complex phase diagram. The validity of our findings is substantiated through Chern numbers, spectral properties, localization of chiral edge states and simulations of quantum Hall transport. Our model is suitable not only for condensed matter (materials), but also for cold gases trapped in optical lattices or electric circuits.","sentences":["We theoretically study the 2D Su-Schrieffer-Heeger model in the context of Floquet topological insulators (FTIs).","FTIs are systems which undergo topological phase transitions, governed by Chern numbers, as a result of time reversal symmetry (TRS) breaking by a time periodic process.","In our proposed model, the condition of TRS breaking is achieved by circularly polarized light irradiation.","We analytically show that TRS breaking is forbidden in the absence of second order neighbors hopping.","In the absence of light irradiation, we identify a symmetry-protected degeneracy and prove the appearance of a flat band along a specific direction in the momentum space.","Furthermore, we employ a novel method to show that the four unit cell atoms, in the absence of irradiation, can be interpreted as conserved spin states.","With the breaking of TRS via light irradiation, these spin states are no longer conserved, leading to the emergence of chiral edge states.","We also show how the interplay between the TRS breaking and dimerization leads to a complex phase diagram.","The validity of our findings is substantiated through Chern numbers, spectral properties, localization of chiral edge states and simulations of quantum Hall transport.","Our model is suitable not only for condensed matter (materials), but also for cold gases trapped in optical lattices or electric circuits."],"url":"http://arxiv.org/abs/2404.02735v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 13:34:36","title":"Renormalization of Scalar and Fermion Interacting Field Theory for Arbitrary Loop: Heat-Kernel Approach","abstract":"We outline a proposal, based on the Heat-Kernel method, to compute 1PI effective action up to any loop order for quantum field theory with scalar and fermion fields. We algebraically extract the divergences associated with the composite operators without explicitly performing any momentum loop integral. We perform this analysis explicitly for one and two-loop cases and pave the way for three-loop as well. Using our prescription we compute the two-loop counter terms for a theory containing higher mass dimensional effective operators that are polynomial in fields for two different cases: (i) real singlet scalar, and (ii) complex fermion-scalar interacting theories. We also discuss how the minimal Heat-Kernel fails to deal with the effective operators involving derivatives. We explicitly compute the one-loop counter terms for such a case within an $O(n)$ symmetric scalar theory employing a non-minimal Heat-Kernel. Our method computes the counter terms of the composite operators directly and is also useful for extracting infrared divergence in massless limits.","sentences":["We outline a proposal, based on the Heat-Kernel method, to compute 1PI effective action up to any loop order for quantum field theory with scalar and fermion fields.","We algebraically extract the divergences associated with the composite operators without explicitly performing any momentum loop integral.","We perform this analysis explicitly for one and two-loop cases and pave the way for three-loop as well.","Using our prescription we compute the two-loop counter terms for a theory containing higher mass dimensional effective operators that are polynomial in fields for two different cases: (i) real singlet scalar, and (ii) complex fermion-scalar interacting theories.","We also discuss how the minimal Heat-Kernel fails to deal with the effective operators involving derivatives.","We explicitly compute the one-loop counter terms for such a case within an $O(n)$ symmetric scalar theory employing a non-minimal Heat-Kernel.","Our method computes the counter terms of the composite operators directly and is also useful for extracting infrared divergence in massless limits."],"url":"http://arxiv.org/abs/2404.02734v1","category":"hep-th"}
{"created":"2024-04-03 13:28:28","title":"Extending direct data-driven predictive control towards systems with finite control sets","abstract":"Although classical model predictive control with finite control sets (FCS-MPC) is quite a popular control method, particularly in the realm of power electronics systems, its direct data-driven predictive control (FCS-DPC) counterpart has received relatively limited attention. In this paper, we introduce a novel reformulation of a commonly used DPC scheme that allows for the application of a modified sphere decoding algorithm, known for its efficiency and prominence in FCS-MPC applications. We test the reformulation on a popular electrical drive example and compare the computation times of sphere decoding FCS-DPC with an enumeration-based and a MIQP method.","sentences":["Although classical model predictive control with finite control sets (FCS-MPC) is quite a popular control method, particularly in the realm of power electronics systems, its direct data-driven predictive control (FCS-DPC) counterpart has received relatively limited attention.","In this paper, we introduce a novel reformulation of a commonly used DPC scheme that allows for the application of a modified sphere decoding algorithm, known for its efficiency and prominence in FCS-MPC applications.","We test the reformulation on a popular electrical drive example and compare the computation times of sphere decoding FCS-DPC with an enumeration-based and a MIQP method."],"url":"http://arxiv.org/abs/2404.02727v1","category":"eess.SY"}
{"created":"2024-04-03 13:25:37","title":"Shareability of steering in 2-producible states","abstract":"Quantum steering is the phenomenon whereby one party (Alice) proves entanglement by \"steering'' the system of another party (Bob) into distinct ensembles of states, by performing different measurements on her subsystem. Here, we investigate steering in a network scenario involving $n$ parties, who each perform local measurements on part of a global quantum state, that is produced using only two-party entangled states, and mixing with ancillary separable states. We introduce three scenarios which can be straightforwardly implemented in standard quantum optics architecture, which we call random $\\frac{n}{2}$-pair entanglement, random pair entanglement and semi-random pair entanglement. We study steerability of the states across two-party marginals which arise in the three scenarios, and derive analytically the necessary and sufficient steering criteria for different sets of measurement settings. Strikingly, using the semi-random pair entanglement construction, one party can steer every one of the $n-1$ other parties, for arbitrarily large $n$, using only two measurements. Finally, exploiting symmetry, we study various small network configurations (three or four parties) in the three scenarios, under different measurements and produced by different two-party entangled states.","sentences":["Quantum steering is the phenomenon whereby one party (Alice) proves entanglement by \"steering'' the system of another party (Bob) into distinct ensembles of states, by performing different measurements on her subsystem.","Here, we investigate steering in a network scenario involving $n$ parties, who each perform local measurements on part of a global quantum state, that is produced using only two-party entangled states, and mixing with ancillary separable states.","We introduce three scenarios which can be straightforwardly implemented in standard quantum optics architecture, which we call random $\\frac{n}{2}$-pair entanglement, random pair entanglement and semi-random pair entanglement.","We study steerability of the states across two-party marginals which arise in the three scenarios, and derive analytically the necessary and sufficient steering criteria for different sets of measurement settings.","Strikingly, using the semi-random pair entanglement construction, one party can steer every one of the $n-1$ other parties, for arbitrarily large $n$, using only two measurements.","Finally, exploiting symmetry, we study various small network configurations (three or four parties) in the three scenarios, under different measurements and produced by different two-party entangled states."],"url":"http://arxiv.org/abs/2404.02725v1","category":"quant-ph"}
{"created":"2024-04-03 13:23:32","title":"Deterministic Identification Codes for Fading Channels","abstract":"Many communication applications incorporate event-triggered behavior, where the conventional Shannon capacity may not effectively gauge performance. Consequently, we advocate for the concept of identification capacity as a more suitable metric for assessing these systems. We consider deterministic identification codes for the Gaussian AWGN, the slow fading, and the fast fading channels with power constraints. We prove lower bounds on capacities for the slow and the fast fading channels with side information for a wide range of fading distributions. Additionally, we present the code construction with efficient encoding which achieves the lower bound on capacity both for the slow and the fast fading channels. At last, we prove the same lower bound on the capacity of the fast fading channel without side information, i.e. the same lower bound holds even when the receiver doesn't know the fading coefficients. As a result we show that compared with Shannon's message transmission paradigm we achieved completely different capacity scaling for deterministic identification codes for all relevant fading channels.","sentences":["Many communication applications incorporate event-triggered behavior, where the conventional Shannon capacity may not effectively gauge performance.","Consequently, we advocate for the concept of identification capacity as a more suitable metric for assessing these systems.","We consider deterministic identification codes for the Gaussian AWGN, the slow fading, and the fast fading channels with power constraints.","We prove lower bounds on capacities for the slow and the fast fading channels with side information for a wide range of fading distributions.","Additionally, we present the code construction with efficient encoding which achieves the lower bound on capacity both for the slow and the fast fading channels.","At last, we prove the same lower bound on the capacity of the fast fading channel without side information, i.e. the same lower bound holds even when the receiver doesn't know the fading coefficients.","As a result we show that compared with Shannon's message transmission paradigm we achieved completely different capacity scaling for deterministic identification codes for all relevant fading channels."],"url":"http://arxiv.org/abs/2404.02723v1","category":"cs.IT"}
{"created":"2024-04-03 13:22:46","title":"Towards a unifying framework for data-driven predictive control with quadratic regularization","abstract":"Data-driven predictive control (DPC) has recently gained popularity as an alternative to model predictive control (MPC). Amidst the surge in proposed DPC frameworks, upon closer inspection, many of these frameworks are more closely related (or perhaps even equivalent) to each other than it may first appear. We argue for a more formal characterization of these relationships so that results can be freely transferred from one framework to another, rather than being uniquely attributed to a particular framework. We demonstrate this idea by examining the connection between $\\gamma$-DDPC and the original DeePC formulation.","sentences":["Data-driven predictive control (DPC) has recently gained popularity as an alternative to model predictive control (MPC).","Amidst the surge in proposed DPC frameworks, upon closer inspection, many of these frameworks are more closely related (or perhaps even equivalent) to each other than it may first appear.","We argue for a more formal characterization of these relationships so that results can be freely transferred from one framework to another, rather than being uniquely attributed to a particular framework.","We demonstrate this idea by examining the connection between $\\gamma$-DDPC and the original DeePC formulation."],"url":"http://arxiv.org/abs/2404.02721v1","category":"eess.SY"}
{"created":"2024-04-03 13:22:25","title":"Light-quark mass dependence of the $\u039b(1405)$ resonance","abstract":"We present the light-quark mass dependence of the $\\Lambda(1405)$ resonance at leading order in a renormalizable framework of covariant chiral effective field theory. The meson-baryon scattering amplitudes, which are obtained by solving the scattering equation within time-ordered perturbation theory, follow the quark mass trajectory of the Coordinated Lattice Simulations consortium. At $M_\\pi\\approx 200$ MeV and $M_K\\approx 487$ MeV, our parameter-free prediction of $\\Lambda(1405)$ poles is consistent with the recent lattice results of BaSc Collaboration [Phys. Rev. Lett. 132, 051901 (2024)]. Varying the pion mass from $135$ MeV to $400$ MeV, we present the evolution of double-pole positions of $\\Lambda(1405)$: the higher pole remains a resonance around the $\\bar{K}N$ threshold; whereas the lower pole undergoes a transition from resonance to a virtual state, and ultimately to a bound state of the $\\pi\\Sigma$ system, which could serve as a prediction of the forthcoming lattice QCD simulations.","sentences":["We present the light-quark mass dependence of the $\\Lambda(1405)$ resonance at leading order in a renormalizable framework of covariant chiral effective field theory.","The meson-baryon scattering amplitudes, which are obtained by solving the scattering equation within time-ordered perturbation theory, follow the quark mass trajectory of the Coordinated Lattice Simulations consortium.","At $M_\\pi\\approx 200$ MeV and $M_K\\approx 487$ MeV, our parameter-free prediction of $\\Lambda(1405)$ poles is consistent with the recent lattice results of BaSc Collaboration [Phys.","Rev. Lett.","132, 051901 (2024)].","Varying the pion mass from $135$ MeV to $400$ MeV, we present the evolution of double-pole positions of $\\Lambda(1405)$: the higher pole remains a resonance around the $\\bar{K}N$ threshold; whereas the lower pole undergoes a transition from resonance to a virtual state, and ultimately to a bound state of the $\\pi\\Sigma$ system, which could serve as a prediction of the forthcoming lattice QCD simulations."],"url":"http://arxiv.org/abs/2404.02720v1","category":"hep-ph"}
{"created":"2024-04-03 13:13:55","title":"Quantum conjugate gradient method using the positive-side quantum eigenvalue transformation","abstract":"Quantum algorithms are still challenging to solve linear systems of equations on real devices. This challenge arises from the need for deep circuits and numerous ancilla qubits. We introduce the quantum conjugate gradient (QCG) method using the quantum eigenvalue transformation (QET). The circuit depth of this algorithm depends on the square root of the coefficient matrix's condition number $\\kappa$, representing a square root improvement compared to the previous quantum algorithms. The number of ancilla qubits is constant, similar to other QET-based algorithms. Additionally, to implement the QCG method efficiently, we devise a QET-based technique that uses only the positive side of the polynomial (denoted by $P(x)$ for $x\\in[0,1]$). We conduct numerical experiments by applying our algorithm to the one-dimensional Poisson equation and successfully solve it. Based on the numerical results, our algorithm significantly improves circuit depth, outperforming another QET-based algorithm by three to four orders of magnitude.","sentences":["Quantum algorithms are still challenging to solve linear systems of equations on real devices.","This challenge arises from the need for deep circuits and numerous ancilla qubits.","We introduce the quantum conjugate gradient (QCG) method using the quantum eigenvalue transformation (QET).","The circuit depth of this algorithm depends on the square root of the coefficient matrix's condition number $\\kappa$, representing a square root improvement compared to the previous quantum algorithms.","The number of ancilla qubits is constant, similar to other QET-based algorithms.","Additionally, to implement the QCG method efficiently, we devise a QET-based technique that uses only the positive side of the polynomial (denoted by $P(x)$ for $x\\in[0,1]$).","We conduct numerical experiments by applying our algorithm to the one-dimensional Poisson equation and successfully solve it.","Based on the numerical results, our algorithm significantly improves circuit depth, outperforming another QET-based algorithm by three to four orders of magnitude."],"url":"http://arxiv.org/abs/2404.02713v1","category":"quant-ph"}
{"created":"2024-04-03 13:08:11","title":"Certification of multi-qubit quantum systems with temporal inequalities","abstract":"Demonstrating contextual correlations in quantum theory through the violation of a non-contextuality inequality necessarily needs some ``contexts\" and thus assumes some compatibility relations between the measurements. As a result, any self-testing protocol based on the maximal violation of such inequality is not free from such assumptions. In this work, we propose temporal inequalities derived from non-contextuality inequalities for multi-qubit systems without assuming any compatibility relations among the measurements. We demonstrate that the new inequalities can be maximally violated via a sequential measurement scenario. Moreover, using the maximal violation of these temporal inequalities we are able to certify multi-qubit graph states and the measurements.","sentences":["Demonstrating contextual correlations in quantum theory through the violation of a non-contextuality inequality necessarily needs some ``contexts\" and thus assumes some compatibility relations between the measurements.","As a result, any self-testing protocol based on the maximal violation of such inequality is not free from such assumptions.","In this work, we propose temporal inequalities derived from non-contextuality inequalities for multi-qubit systems without assuming any compatibility relations among the measurements.","We demonstrate that the new inequalities can be maximally violated via a sequential measurement scenario.","Moreover, using the maximal violation of these temporal inequalities we are able to certify multi-qubit graph states and the measurements."],"url":"http://arxiv.org/abs/2404.02709v1","category":"quant-ph"}
{"created":"2024-04-03 12:20:51","title":"The VoicePrivacy 2024 Challenge Evaluation Plan","abstract":"The task of the challenge is to develop a voice anonymization system for speech data which conceals the speaker's voice identity while protecting linguistic content and emotional states. The organizers provide development and evaluation datasets and evaluation scripts, as well as baseline anonymization systems and a list of training resources formed on the basis of the participants' requests. Participants apply their developed anonymization systems, run evaluation scripts and submit evaluation results and anonymized speech data to the organizers. Results will be presented at a workshop held in conjunction with Interspeech 2024 to which all participants are invited to present their challenge systems and to submit additional workshop papers.","sentences":["The task of the challenge is to develop a voice anonymization system for speech data which conceals the speaker's voice identity while protecting linguistic content and emotional states.","The organizers provide development and evaluation datasets and evaluation scripts, as well as baseline anonymization systems and a list of training resources formed on the basis of the participants' requests.","Participants apply their developed anonymization systems, run evaluation scripts and submit evaluation results and anonymized speech data to the organizers.","Results will be presented at a workshop held in conjunction with Interspeech 2024 to which all participants are invited to present their challenge systems and to submit additional workshop papers."],"url":"http://arxiv.org/abs/2404.02677v1","category":"eess.AS"}
{"created":"2024-04-03 12:06:01","title":"RS-Mamba for Large Remote Sensing Image Dense Prediction","abstract":"The spatial resolution of remote sensing images is becoming increasingly higher, posing challenges in handling large very-high-resolution (VHR) remote sensing images for dense prediction tasks. Models based on convolutional neural networks are limited in their ability to model global features of remote sensing images due to local convolution operations. Transformer based models, despite their global modeling capabilities, face computational challenges with large VHR images due to their quadratic complexity. The common practice of cropping large images into smaller patches leads to a significant loss of contextual information. To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in VHR remote sensing. RSM is designed to model global features of remote sensing images with linear complexity, enabling it to process large VHR images effectively. It employs an omnidirectional selective scan module to globally model the images in multiple directions, capturing large spatial features from various directions. Experiments on semantic segmentation and change detection tasks across various objects demonstrate the effectiveness of RSM. With simple model architecture and training approach, RSM achieves state-of-the-art performance on the dense prediction tasks of VHR remote sensing. The code for this work will be available at https://github.com/walking-shadow/Official_Remote_Sensing_Mamba.","sentences":["The spatial resolution of remote sensing images is becoming increasingly higher, posing challenges in handling large very-high-resolution (VHR)","remote sensing images for dense prediction tasks.","Models based on convolutional neural networks are limited in their ability to model global features of remote sensing images due to local convolution operations.","Transformer based models, despite their global modeling capabilities, face computational challenges with large VHR images due to their quadratic complexity.","The common practice of cropping large images into smaller patches leads to a significant loss of contextual information.","To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in VHR remote sensing.","RSM is designed to model global features of remote sensing images with linear complexity, enabling it to process large VHR images effectively.","It employs an omnidirectional selective scan module to globally model the images in multiple directions, capturing large spatial features from various directions.","Experiments on semantic segmentation and change detection tasks across various objects demonstrate the effectiveness of RSM.","With simple model architecture and training approach, RSM achieves state-of-the-art performance on the dense prediction tasks of VHR remote sensing.","The code for this work will be available at https://github.com/walking-shadow/Official_Remote_Sensing_Mamba."],"url":"http://arxiv.org/abs/2404.02668v1","category":"cs.CV"}
{"created":"2024-04-03 12:05:43","title":"Entropic pulling and diffusion diode","abstract":"Biological environments at micrometer scales and below are often crowded, and experience incessant stochastic thermal fluctuations. The presence of membranes/pores, and multiple biological entities in a constricted space can make the damping/diffusion inhomogeneous. This effect of inhomogeneity is presented by the diffusion becoming coordinate-dependent. In this paper, we analyze the consequence of inhomogeneity-induced coordinate-dependent diffusion on Brownian systems in thermal equilibrium under the It\\^o's interpretation. We argue that the presence of coordinate-dependent diffusion under It\\^o's formulation gives rise to an effective diffusion potential that can have substantial contribution to system's transport. Alternatively, we relate this to the existence of an emergent force of entropic origin that dictates the transport near interfaces.","sentences":["Biological environments at micrometer scales and below are often crowded, and experience incessant stochastic thermal fluctuations.","The presence of membranes/pores, and multiple biological entities in a constricted space can make the damping/diffusion inhomogeneous.","This effect of inhomogeneity is presented by the diffusion becoming coordinate-dependent.","In this paper, we analyze the consequence of inhomogeneity-induced coordinate-dependent diffusion on Brownian systems in thermal equilibrium under the It\\^o's interpretation.","We argue that the presence of coordinate-dependent diffusion under It\\^o's formulation gives rise to an effective diffusion potential that can have substantial contribution to system's transport.","Alternatively, we relate this to the existence of an emergent force of entropic origin that dictates the transport near interfaces."],"url":"http://arxiv.org/abs/2404.02667v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 12:05:30","title":"Nestings of BIBDs with block size four","abstract":"In a nesting of a balanced incomplete block design (or BIBD), we wish to add a point (the \\emph{nested point}) to every block of a $(v,k,\\lambda)$-BIBD in such a way that we end up with a partial $(v,k+1,\\lambda+1)$-BIBD. In the case where the partial $(v,k+1,\\lambda+1)$-BIBD is in fact a $(v,k+1,\\lambda+1)$-BIBD, we have a \\emph{perfect nesting}. We show that a nesting is perfect if and only if $k = 2 \\lambda + 1$.   Perfect nestings were previously known to exist in the case of Steiner triple systems (i.e., $(v,3,1)$-BIBDs) when $v \\equiv 1 \\bmod 6$, as well as for some symmetric BIBDs. Here we study nestings of $(v,4,1)$-BIBDs, which are not perfect nestings. We prove that there is a nested $(v,4,1)$-BIBD if and only if $v \\equiv 1 \\text{ or } 4 \\bmod 12$, $v \\geq 13$. This is accomplished by a variety of direct and recursive constructions.","sentences":["In a nesting of a balanced incomplete block design (or BIBD), we wish to add a point (the \\emph{nested point}) to every block of a $(v,k,\\lambda)$-BIBD in such a way that we end up with a partial $(v,k+1,\\lambda+1)$-BIBD.","In the case where the partial $(v,k+1,\\lambda+1)$-BIBD is in fact a $(v,k+1,\\lambda+1)$-BIBD, we have a \\emph{perfect nesting}.","We show that a nesting is perfect if and only if $k = 2 \\lambda + 1$.   Perfect nestings were previously known to exist in the case of Steiner triple systems (i.e., $(v,3,1)$-BIBDs)","when $v \\equiv 1 \\bmod 6$, as well as for some symmetric BIBDs",".","Here we study nestings of $(v,4,1)$-BIBDs, which are not perfect nestings.","We prove that there is a nested $(v,4,1)$-BIBD if and only if $v \\equiv 1 \\text{ or } 4 \\bmod 12$, $v \\geq 13$.","This is accomplished by a variety of direct and recursive constructions."],"url":"http://arxiv.org/abs/2404.02666v1","category":"math.CO"}
{"created":"2024-04-03 12:01:58","title":"Computing some Principal Value integrals without Residues and Applications on Hilbert Transform and Fourier Transform","abstract":"This article proposes a new approach in the treatment of the Hilbert transform and some cases of the Fourier transform whose improper integrals are principal values. This approach may be useful for teaching these issues to undergraduate engineering students. Traditional literature of Complex Analysis deals with these transformation integrals with the Cauchy-Goursat theorem and the residues calculation technique. In this new approach, instead of residues, we use an intuitive result about complex line integrals of continuous complex functions that resembles the delta of Dirac.","sentences":["This article proposes a new approach in the treatment of the Hilbert transform and some cases of the Fourier transform whose improper integrals are principal values.","This approach may be useful for teaching these issues to undergraduate engineering students.","Traditional literature of Complex Analysis deals with these transformation integrals with the Cauchy-Goursat theorem and the residues calculation technique.","In this new approach, instead of residues, we use an intuitive result about complex line integrals of continuous complex functions that resembles the delta of Dirac."],"url":"http://arxiv.org/abs/2404.02664v1","category":"math-ph"}
{"created":"2024-04-03 11:50:56","title":"Terahertz channel modeling based on surface sensing characteristics","abstract":"The dielectric properties of environmental surfaces, including walls, floors and the ground, etc., play a crucial role in shaping the accuracy of terahertz (THz) channel modeling, thereby directly impacting the effectiveness of communication systems. Traditionally, acquiring these properties has relied on methods such as terahertz time-domain spectroscopy (THz-TDS) or vector network analyzers (VNA), demanding rigorous sample preparation and entailing a significant expenditure of time. However, such measurements are not always feasible, particularly in novel and uncharacterized scenarios. In this work, we propose a new approach for channel modeling that leverages the inherent sensing capabilities of THz channels. By comparing the results obtained through channel sensing with that derived from THz-TDS measurements, we demonstrate the method's ability to yield dependable surface property information. The application of this approach in both a miniaturized cityscape scenario and an indoor environment has shown consistency with experimental measurements, thereby verifying its effectiveness in real-world settings.","sentences":["The dielectric properties of environmental surfaces, including walls, floors and the ground, etc., play a crucial role in shaping the accuracy of terahertz (THz) channel modeling, thereby directly impacting the effectiveness of communication systems.","Traditionally, acquiring these properties has relied on methods such as terahertz time-domain spectroscopy (THz-TDS) or vector network analyzers (VNA), demanding rigorous sample preparation and entailing a significant expenditure of time.","However, such measurements are not always feasible, particularly in novel and uncharacterized scenarios.","In this work, we propose a new approach for channel modeling that leverages the inherent sensing capabilities of THz channels.","By comparing the results obtained through channel sensing with that derived from THz-TDS measurements, we demonstrate the method's ability to yield dependable surface property information.","The application of this approach in both a miniaturized cityscape scenario and an indoor environment has shown consistency with experimental measurements, thereby verifying its effectiveness in real-world settings."],"url":"http://arxiv.org/abs/2404.02661v1","category":"physics.app-ph"}
{"created":"2024-04-03 11:19:25","title":"Ultrastable lasers: investigations of crystalline mirrors and closed cycle cooling at 124 K","abstract":"We have investigated crystalline AlGaAs/GaAs optical coatings with three ultra-stable cavities operating at 4 K, 16 K, 124 K and 297 K. The response of the resonance frequencies of cavities to variations in optical power indicates effects beyond the photo-thermo-optic effect observed in dielectric coatings. These effects are strongly dependent on the intensity of the intracavity light at 1.5~\\textmu m. When the rear side of the mirrors is illuminated with external light, we observe a prominent photo-modified birefringence for photon energies above the GaAs bandgap, which points to a possible mechanism relating our observations to the semiconductor properties of the coatings. Separately, we also present a low maintenance evolution of our 124 K silicon cavity system where the liquid nitrogen based cooling system is replaced with closed cycle cooling from a pulse-tube cryo-cooler.","sentences":["We have investigated crystalline AlGaAs/GaAs optical coatings with three ultra-stable cavities operating at 4 K, 16 K, 124 K and 297 K. The response of the resonance frequencies of cavities to variations in optical power indicates effects beyond the photo-thermo-optic effect observed in dielectric coatings.","These effects are strongly dependent on the intensity of the intracavity light at 1.5~\\textmu m. When the rear side of the mirrors is illuminated with external light, we observe a prominent photo-modified birefringence for photon energies above the GaAs bandgap, which points to a possible mechanism relating our observations to the semiconductor properties of the coatings.","Separately, we also present a low maintenance evolution of our 124 K silicon cavity system where the liquid nitrogen based cooling system is replaced with closed cycle cooling from a pulse-tube cryo-cooler."],"url":"http://arxiv.org/abs/2404.02647v1","category":"physics.optics"}
{"created":"2024-04-03 11:15:56","title":"One Stack to Rule them All: To Drive Automated Vehicles, and Reach for the 4th level","abstract":"Most automated driving functions are designed for a specific task or vehicle. Most often, the underlying architecture is fixed to specific algorithms to increase performance. Therefore, it is not possible to deploy new modules and algorithms easily. In this paper, we present our automated driving stack which combines both scalability and adaptability. Due to the modular design, our stack allows for a fast integration and testing of novel and state-of-the-art research approaches. Furthermore, it is flexible to be used for our different testing vehicles, including modified EasyMile EZ10 shuttles and different passenger cars. These vehicles differ in multiple ways, e.g. sensor setups, control systems, maximum speed, or steering angle limitations. Finally, our stack is deployed in real world environments, including passenger transport in urban areas. Our stack includes all components needed for operating an autonomous vehicle, including localization, perception, planning, controller, and additional safety modules. Our stack is developed, tested, and evaluated in real world traffic in multiple test sites, including the Test Area Autonomous Driving Baden-W\\\"urttemberg.","sentences":["Most automated driving functions are designed for a specific task or vehicle.","Most often, the underlying architecture is fixed to specific algorithms to increase performance.","Therefore, it is not possible to deploy new modules and algorithms easily.","In this paper, we present our automated driving stack which combines both scalability and adaptability.","Due to the modular design, our stack allows for a fast integration and testing of novel and state-of-the-art research approaches.","Furthermore, it is flexible to be used for our different testing vehicles, including modified EasyMile EZ10 shuttles and different passenger cars.","These vehicles differ in multiple ways, e.g. sensor setups, control systems, maximum speed, or steering angle limitations.","Finally, our stack is deployed in real world environments, including passenger transport in urban areas.","Our stack includes all components needed for operating an autonomous vehicle, including localization, perception, planning, controller, and additional safety modules.","Our stack is developed, tested, and evaluated in real world traffic in multiple test sites, including the Test Area Autonomous Driving Baden-W\\\"urttemberg."],"url":"http://arxiv.org/abs/2404.02645v1","category":"cs.RO"}
{"created":"2024-04-03 11:07:05","title":"Goal-oriented time adaptivity for port-Hamiltonian systems","abstract":"Port-Hamiltonian systems provide an energy-based modeling paradigm for dynamical input-state-output systems. At their core, they fulfill an energy balance relating stored, dissipated and supplied energy. To accurately resolve this energy balance in time discretizations, we propose an adaptive grid refinement technique based on a posteriori error estimation. The evaluation of the error estimator includes the computation of adjoint sensitivities. To interpret this adjoint equation as a backwards-in-time equation, we show piecewise weak differentiability of the dual variable. Then, leveraging dissipativity of the port-Hamiltonian dynamics, we present a parallelizable approximation of the underlying adjoint system in the spirit of a block-Jacobi method to efficiently compute error indicators. We illustrate the performance of the proposed scheme by means of numerical experiments showing that it yields a smaller violation of the energy balance when compared to uniform refinements and traditional step-size controlled time stepping.","sentences":["Port-Hamiltonian systems provide an energy-based modeling paradigm for dynamical input-state-output systems.","At their core, they fulfill an energy balance relating stored, dissipated and supplied energy.","To accurately resolve this energy balance in time discretizations, we propose an adaptive grid refinement technique based on a posteriori error estimation.","The evaluation of the error estimator includes the computation of adjoint sensitivities.","To interpret this adjoint equation as a backwards-in-time equation, we show piecewise weak differentiability of the dual variable.","Then, leveraging dissipativity of the port-Hamiltonian dynamics, we present a parallelizable approximation of the underlying adjoint system in the spirit of a block-Jacobi method to efficiently compute error indicators.","We illustrate the performance of the proposed scheme by means of numerical experiments showing that it yields a smaller violation of the energy balance when compared to uniform refinements and traditional step-size controlled time stepping."],"url":"http://arxiv.org/abs/2404.02641v1","category":"math.NA"}
{"created":"2024-04-03 11:02:32","title":"Selecting High-Dimensional Representations of Physical Systems by Reweighted Diffusion Maps","abstract":"Constructing reduced representations of high-dimensional systems is a fundamental problem in physical chemistry. Many unsupervised machine learning methods can automatically find such low-dimensional representations. However, an often overlooked problem is what high-dimensional representation should be used to describe systems before dimensionality reduction. Here, we address this issue using a recently developed method called reweighted diffusion map [J. Chem. Theory Comput. 2022, 18, 7179-7192]. We show how high-dimensional representations can be quantitatively selected by exploring the spectral decomposition of Markov transition matrices built from data obtained from standard or enhanced sampling atomistic simulations. We demonstrate the performance of the method in several high-dimensional examples.","sentences":["Constructing reduced representations of high-dimensional systems is a fundamental problem in physical chemistry.","Many unsupervised machine learning methods can automatically find such low-dimensional representations.","However, an often overlooked problem is what high-dimensional representation should be used to describe systems before dimensionality reduction.","Here, we address this issue using a recently developed method called reweighted diffusion map [J. Chem.","Theory Comput.","2022, 18, 7179-7192].","We show how high-dimensional representations can be quantitatively selected by exploring the spectral decomposition of Markov transition matrices built from data obtained from standard or enhanced sampling atomistic simulations.","We demonstrate the performance of the method in several high-dimensional examples."],"url":"http://arxiv.org/abs/2404.02639v1","category":"physics.chem-ph"}
{"created":"2024-04-03 10:35:06","title":"GPU acceleration of ab initio simulations of large-scale identical particles based on path integral molecular dynamics","abstract":"Path integral Monte Carlo (PIMC) and path integral molecular dynamics (PIMD) provide the golden standard for the ab initio simulations of identical particles. In this work, we achieved significant GPU acceleration based on PIMD, which is equivalent to PIMC in the ab initio simulations, and developed an open-source PIMD code repository that does not rely on any other third party library. Numerical experiments show that for a system of 1600 interacting identical bosons in a harmonic trap, using a single GPU and a single CPU, it only takes two hours to achieve satisfactory simulation accuracy. With the increase of the number of identical particles, the advantage of GPU acceleration over CPU becomes more obvious, making it possible to simulate tens of thousands of identical particles from first principles using a single GPU. For example, for a system of 10000 non-interacting bosons, numerical experiments show that it takes 23 hours to obtain a simulation that is highly consistent with the exact results. Our study shows that GPU acceleration can lay a solid foundation for the wide application of PIMD simulations for extremely large-scale identical particle quantum systems with more than 10,000 particles. Numerical experiments show that a 24GB GPU can simulate up to 40000 identical particles from first principles, and the GPU acceleration leads to a roughly linear relationship between the computation time and the number of identical particles. In addition, we have also successfully implemented simulations for fictitious identical particle thermodynamics using GPU to overcome the Fermion sign problem, which makes it promising to efficiently and accurately simulate tens of thousands of fermions based on GPU.","sentences":["Path integral Monte Carlo (PIMC) and path integral molecular dynamics (PIMD) provide the golden standard for the ab initio simulations of identical particles.","In this work, we achieved significant GPU acceleration based on PIMD, which is equivalent to PIMC in the ab initio simulations, and developed an open-source PIMD code repository that does not rely on any other third party library.","Numerical experiments show that for a system of 1600 interacting identical bosons in a harmonic trap, using a single GPU and a single CPU, it only takes two hours to achieve satisfactory simulation accuracy.","With the increase of the number of identical particles, the advantage of GPU acceleration over CPU becomes more obvious, making it possible to simulate tens of thousands of identical particles from first principles using a single GPU.","For example, for a system of 10000 non-interacting bosons, numerical experiments show that it takes 23 hours to obtain a simulation that is highly consistent with the exact results.","Our study shows that GPU acceleration can lay a solid foundation for the wide application of PIMD simulations for extremely large-scale identical particle quantum systems with more than 10,000 particles.","Numerical experiments show that a 24GB GPU can simulate up to 40000 identical particles from first principles, and the GPU acceleration leads to a roughly linear relationship between the computation time and the number of identical particles.","In addition, we have also successfully implemented simulations for fictitious identical particle thermodynamics using GPU to overcome the Fermion sign problem, which makes it promising to efficiently and accurately simulate tens of thousands of fermions based on GPU."],"url":"http://arxiv.org/abs/2404.02628v1","category":"physics.comp-ph"}
{"created":"2024-04-03 10:30:36","title":"Polyvalent Machine-Learned Potential for Cobalt: from Bulk to Nanoparticles","abstract":"We present the development of a quadratic Spectral Neighbor Analysis Potential (q-SNAP) for ferromagnetic cobalt and its applications to bulk phases, surfaces, and nanoparticles. Trained on Density Functional Theory calculations using the Perdew-Burke-Ernzerhof (DFT-PBE) functional, this machine-learned potential enables simulations of large systems over extended time scales across a wide range of temperatures and pressures at near DFT accuracy. It is validated by closely reproducing the phonon dispersions of hexagonal close-packed (hcp) and face-centered cubic (fcc) Co, surface energies, and the relative stability of nanoparticles of various shapes. Thermal expansion and the melting point of Co computed with this potential are close to experimental values. Furthermore, this machine-learned potential goes beyond the capabilities of simpler N-body potentials by capturing nuanced properties such as vacancy formation energies on nanoparticle vertices. This accuracy and versatility make the potential suitable for a wide range of applications, including catalysis.","sentences":["We present the development of a quadratic Spectral Neighbor Analysis Potential (q-SNAP) for ferromagnetic cobalt and its applications to bulk phases, surfaces, and nanoparticles.","Trained on Density Functional Theory calculations using the Perdew-Burke-Ernzerhof (DFT-PBE) functional, this machine-learned potential enables simulations of large systems over extended time scales across a wide range of temperatures and pressures at near DFT accuracy.","It is validated by closely reproducing the phonon dispersions of hexagonal close-packed (hcp) and face-centered cubic (fcc) Co, surface energies, and the relative stability of nanoparticles of various shapes.","Thermal expansion and the melting point of Co computed with this potential are close to experimental values.","Furthermore, this machine-learned potential goes beyond the capabilities of simpler N-body potentials by capturing nuanced properties such as vacancy formation energies on nanoparticle vertices.","This accuracy and versatility make the potential suitable for a wide range of applications, including catalysis."],"url":"http://arxiv.org/abs/2404.02626v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 10:25:20","title":"Self-similar intermediate asymptotics for first-order mean field games","abstract":"We study the intermediate asymptotic behavior of solutions to the first-order mean field games system with a local coupling, when the initial density is a compactly supported function on the real line, and the coupling is of power type. Addressing a question that was left open in arXiv:2308.00314, we prove that the solutions converge to the self-similar profile. We proceed by analyzing a continuous rescaling of the solution, and identifying an appropriate Lyapunov functional. We identify a critical value for the parameter of the coupling, which determines the qualitative behavior of the functional, and the well-posedness of the infinite horizon system. Accordingly, we also establish, in the subcritical and critical cases, a second convergence result which characterizes the behavior of the full solution as the time horizon approaches infinity. We also prove the corresponding results for the mean field planning problem. A large part of our analysis and methodology apply just as well to arbitrary dimensions. As such, this work is a major step towards settling these questions in the higher-dimensional setting.","sentences":["We study the intermediate asymptotic behavior of solutions to the first-order mean field games system with a local coupling, when the initial density is a compactly supported function on the real line, and the coupling is of power type.","Addressing a question that was left open in arXiv:2308.00314, we prove that the solutions converge to the self-similar profile.","We proceed by analyzing a continuous rescaling of the solution, and identifying an appropriate Lyapunov functional.","We identify a critical value for the parameter of the coupling, which determines the qualitative behavior of the functional, and the well-posedness of the infinite horizon system.","Accordingly, we also establish, in the subcritical and critical cases, a second convergence result which characterizes the behavior of the full solution as the time horizon approaches infinity.","We also prove the corresponding results for the mean field planning problem.","A large part of our analysis and methodology apply just as well to arbitrary dimensions.","As such, this work is a major step towards settling these questions in the higher-dimensional setting."],"url":"http://arxiv.org/abs/2404.02623v1","category":"math.AP"}
{"created":"2024-04-03 10:19:53","title":"Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals","abstract":"This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning graph structures from nodal signals. Our key contribution lies in modeling the signals as Gaussian and stationary on the graph, enabling the development of a graph-learning formulation that combines the strengths of graphical lasso with a more encompassing model. Specifically, we assume that the precision matrix can take any polynomial form of the sought graph, allowing for increased flexibility in modeling nodal relationships. Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the graph and precision matrices, and (ii) characterize its convergence. We evaluate the performance of PGL through comprehensive numerical simulations using both synthetic and real data, demonstrating its superiority over several alternatives. Overall, this approach presents a significant advancement in graph learning and holds promise for various applications in graph-aware signal analysis and beyond.","sentences":["This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning graph structures from nodal signals.","Our key contribution lies in modeling the signals as Gaussian and stationary on the graph, enabling the development of a graph-learning formulation that combines the strengths of graphical lasso with a more encompassing model.","Specifically, we assume that the precision matrix can take any polynomial form of the sought graph, allowing for increased flexibility in modeling nodal relationships.","Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the graph and precision matrices, and (ii) characterize its convergence.","We evaluate the performance of PGL through comprehensive numerical simulations using both synthetic and real data, demonstrating its superiority over several alternatives.","Overall, this approach presents a significant advancement in graph learning and holds promise for various applications in graph-aware signal analysis and beyond."],"url":"http://arxiv.org/abs/2404.02621v1","category":"eess.SP"}
{"created":"2024-04-03 10:13:18","title":"Adjusting Interpretable Dimensions in Embedding Space with Human Judgments","abstract":"Embedding spaces contain interpretable dimensions indicating gender, formality in style, or even object properties. This has been observed multiple times. Such interpretable dimensions are becoming valuable tools in different areas of study, from social science to neuroscience. The standard way to compute these dimensions uses contrasting seed words and computes difference vectors over them. This is simple but does not always work well. We combine seed-based vectors with guidance from human ratings of where words fall along a specific dimension, and evaluate on predicting both object properties like size and danger, and the stylistic properties of formality and complexity. We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well.","sentences":["Embedding spaces contain interpretable dimensions indicating gender, formality in style, or even object properties.","This has been observed multiple times.","Such interpretable dimensions are becoming valuable tools in different areas of study, from social science to neuroscience.","The standard way to compute these dimensions uses contrasting seed words and computes difference vectors over them.","This is simple but does not always work well.","We combine seed-based vectors with guidance from human ratings of where words fall along a specific dimension, and evaluate on predicting both object properties like size and danger, and the stylistic properties of formality and complexity.","We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well."],"url":"http://arxiv.org/abs/2404.02619v1","category":"cs.CL"}
{"created":"2024-04-03 10:08:55","title":"Neural Radiance Fields with Torch Units","abstract":"Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction methods widely used in industrial applications. Although prevalent methods achieve considerable improvements in small-scale scenes, accomplishing reconstruction in complex and large-scale scenes is still challenging. First, the background in complex scenes shows a large variance among different views. Second, the current inference pattern, $i.e.$, a pixel only relies on an individual camera ray, fails to capture contextual information. To solve these problems, we propose to enlarge the ray perception field and build up the sample points interactions. In this paper, we design a novel inference pattern that encourages a single camera ray possessing more contextual information, and models the relationship among sample points on each camera ray. To hold contextual information,a camera ray in our proposed method can render a patch of pixels simultaneously. Moreover, we replace the MLP in neural radiance field models with distance-aware convolutions to enhance the feature propagation among sample points from the same camera ray. To summarize, as a torchlight, a ray in our proposed method achieves rendering a patch of image. Thus, we call the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF show that the Torch-NeRF exhibits excellent performance.","sentences":["Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction methods widely used in industrial applications.","Although prevalent methods achieve considerable improvements in small-scale scenes, accomplishing reconstruction in complex and large-scale scenes is still challenging.","First, the background in complex scenes shows a large variance among different views.","Second, the current inference pattern, $i.e.$, a pixel only relies on an individual camera ray, fails to capture contextual information.","To solve these problems, we propose to enlarge the ray perception field and build up the sample points interactions.","In this paper, we design a novel inference pattern that encourages a single camera ray possessing more contextual information, and models the relationship among sample points on each camera ray.","To hold contextual information,a camera ray in our proposed method can render a patch of pixels simultaneously.","Moreover, we replace the MLP in neural radiance field models with distance-aware convolutions to enhance the feature propagation among sample points from the same camera ray.","To summarize, as a torchlight, a ray in our proposed method achieves rendering a patch of image.","Thus, we call the proposed method, Torch-NeRF.","Extensive experiments on KITTI-360 and LLFF show that the Torch-NeRF exhibits excellent performance."],"url":"http://arxiv.org/abs/2404.02617v1","category":"cs.CV"}
{"created":"2024-04-03 09:55:15","title":"LightFAt: Mitigating Control-flow Explosion via Lightweight PMU-based Control-flow Attestation","abstract":"With the continuous evolution of computational devices, more and more applications are being executed remotely. The applications operate on a wide spectrum of devices, ranging from IoT nodes with low computational capabilities to large cloud providers with high capabilities. Remote execution often deals with sensitive data or executes proprietary software. Hence, the challenge of ensuring that the code execution will not be compromised rises. Remote Attestation deals with this challenge. It ensures the code is executed in a non-compromised environment by calculating a potentially large sequence of cryptographic hash values. Each hash calculation is computationally intensive and over a large sequence the overhead becomes extremely high. In this work, we propose LightFAt: a Lightweight Control Flow Attestation scheme. Instead of relying on the expensive cryptographic hash calculation, LightFAt leverages the readings from the processor's Performance Monitor Unit (PMU) in conjunction with a lightweight unsupervised machine learning (ML) classifier to detect whether a target application's control flow is compromised, hence improving the system's security. On the verifier's side, LightFAt reaches a detection accuracy of over 95%, with low false-negative and false-positive rates.","sentences":["With the continuous evolution of computational devices, more and more applications are being executed remotely.","The applications operate on a wide spectrum of devices, ranging from IoT nodes with low computational capabilities to large cloud providers with high capabilities.","Remote execution often deals with sensitive data or executes proprietary software.","Hence, the challenge of ensuring that the code execution will not be compromised rises.","Remote Attestation deals with this challenge.","It ensures the code is executed in a non-compromised environment by calculating a potentially large sequence of cryptographic hash values.","Each hash calculation is computationally intensive and over a large sequence the overhead becomes extremely high.","In this work, we propose LightFAt: a Lightweight Control Flow Attestation scheme.","Instead of relying on the expensive cryptographic hash calculation, LightFAt leverages the readings from the processor's Performance Monitor Unit (PMU) in conjunction with a lightweight unsupervised machine learning (ML) classifier to detect whether a target application's control flow is compromised, hence improving the system's security.","On the verifier's side, LightFAt reaches a detection accuracy of over 95%, with low false-negative and false-positive rates."],"url":"http://arxiv.org/abs/2404.02608v1","category":"cs.CR"}
{"created":"2024-04-03 09:13:26","title":"Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages","abstract":"Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data. Our approach improved on the MultiATIS++ benchmark, a primary multi-language SLU dataset, in the cloud scenario using an mBERT model. Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation does not require changes in the production architecture of SLU. Additionally, our pipeline is slot-type independent: it does not require any slot definitions or examples.","sentences":["Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant.","In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data.","Our approach improved on the MultiATIS++ benchmark, a primary multi-language SLU dataset, in the cloud scenario using an mBERT model.","Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF).","In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method.","Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation does not require changes in the production architecture of SLU.","Additionally, our pipeline is slot-type independent: it does not require any slot definitions or examples."],"url":"http://arxiv.org/abs/2404.02588v1","category":"cs.CL"}
{"created":"2024-04-03 09:11:51","title":"Impact of Monoatomic Vacancies in 2D Materials on the Performance of Magnetic Tunnel Junction Devices: Insights from Configurations and Interface Interactions","abstract":"We investigate the impact of monoatomic vacancies in 2D materials on the performance of magnetic tunnel junction (MTJ) devices using first-principles calculations within Density Functional Theory (DFT). Specifically, we analyze the influence on hexagonal boron nitride (hBN) with various layer configurations, uncovering distinct transmission probability patterns. Transmission calculations were conducted using the Landauer-B\\\"uttiker formula employing the Non-Equilibrium Green's Function (NEGF) method. In the Ni/hBN(V$_B$)-hBN/Ni system, a significant reduction in transmission probability was observed compared to non-vacancy configurations. However, when two hBN vacancies were considered, creating the Ni/hBN(V$_B$)-hBN(V$_B$)/Ni MTJ system, a new transmission channel mediated by vacancy localized states emerged. The introduction of a monoatomic boron vacancy in the middle hBN layer of the Ni/3hBN/Ni system revealed nuanced effects on the transmission probability, highlighting alterations in the spin minority and majority channels. Additionally, we explore the monoatomic vacancy in the graphene layer in the Ni/hBN-Gr-hBN/Ni MTJ, uncovering a unique transmission channel influenced by the proximity effect. Our findings suggest that the creation of monoatomic vacancies on the insulator barrier of 2D materials induces distinctive characteristics shaped by the interaction between the surface state of the electrode and the localized state of the monoatomic vacancy layer in the MTJ system.","sentences":["We investigate the impact of monoatomic vacancies in 2D materials on the performance of magnetic tunnel junction (MTJ) devices using first-principles calculations within Density Functional Theory (DFT).","Specifically, we analyze the influence on hexagonal boron nitride (hBN) with various layer configurations, uncovering distinct transmission probability patterns.","Transmission calculations were conducted using the Landauer-B\\\"uttiker formula employing the Non-Equilibrium Green's Function (NEGF) method.","In the Ni/hBN(V$_B$)-hBN/Ni system, a significant reduction in transmission probability was observed compared to non-vacancy configurations.","However, when two hBN vacancies were considered, creating the Ni/hBN(V$_B$)-hBN(V$_B$)/Ni MTJ system, a new transmission channel mediated by vacancy localized states emerged.","The introduction of a monoatomic boron vacancy in the middle hBN layer of the Ni/3hBN/Ni system revealed nuanced effects on the transmission probability, highlighting alterations in the spin minority and majority channels.","Additionally, we explore the monoatomic vacancy in the graphene layer in the Ni/hBN-Gr-hBN/Ni MTJ, uncovering a unique transmission channel influenced by the proximity effect.","Our findings suggest that the creation of monoatomic vacancies on the insulator barrier of 2D materials induces distinctive characteristics shaped by the interaction between the surface state of the electrode and the localized state of the monoatomic vacancy layer in the MTJ system."],"url":"http://arxiv.org/abs/2404.02586v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 08:53:42","title":"Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering","abstract":"We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.","sentences":["We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility.","Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage.","A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities.","This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy.","This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task.","Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism.","This iterative and adaptable process enables the agent to learn a desired optimal policy.","Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency."],"url":"http://arxiv.org/abs/2404.02577v1","category":"cs.LG"}
{"created":"2024-04-03 08:48:54","title":"Learning with errors based dynamic encryption that discloses residue signal for anomaly detection","abstract":"Anomaly detection is a protocol that detects integrity attacks on control systems by comparing the residue signal with a threshold. Implementing anomaly detection on encrypted control systems has been a challenge because it is hard to detect an anomaly from the encrypted residue signal without the secret key. In this paper, we propose a dynamic encryption scheme for a linear system that automatically discloses the residue signal. The initial state and the input are encrypted based on the zero-dynamics of the system, so that the effect of encryption on the residue signal remains identically zero. The proposed scheme is shown to be secure in the sense that no other information than the residue signal is disclosed. Furthermore, we demonstrate a method of utilizing the disclosed residue signal to operate an observer-based controller over encrypted data for an infinite time horizon without re-encryption.","sentences":["Anomaly detection is a protocol that detects integrity attacks on control systems by comparing the residue signal with a threshold.","Implementing anomaly detection on encrypted control systems has been a challenge because it is hard to detect an anomaly from the encrypted residue signal without the secret key.","In this paper, we propose a dynamic encryption scheme for a linear system that automatically discloses the residue signal.","The initial state and the input are encrypted based on the zero-dynamics of the system, so that the effect of encryption on the residue signal remains identically zero.","The proposed scheme is shown to be secure in the sense that no other information than the residue signal is disclosed.","Furthermore, we demonstrate a method of utilizing the disclosed residue signal to operate an observer-based controller over encrypted data for an infinite time horizon without re-encryption."],"url":"http://arxiv.org/abs/2404.02574v1","category":"eess.SY"}
{"created":"2024-04-03 08:46:56","title":"Wenzhou TE: a first-principles calculated thermoelectric materials database","abstract":"Since the implementation of the Materials Genome Project by the Obama administration in the United States, the development of various computational materials databases has fundamentally expanded the choices of industries such as materials and energy. In the field of thermoelectric materials, the thermoelectric figure of merit ZT quantifies the performance of the material. From the viewpoint of calculations for vast materials, the ZT values are not easily obtained due to their computational complexity. Here, we show how to build a database of thermoelectric materials based on first-principles calculations for the electronic and heat transport of materials. Firstly, the initial structures are classified according to the values of bandgap and other basic properties using the clustering algorithm K-means in machine learning, and high-throughput first principles calculations are carried out for narrow-bandgap semiconductors which exhibiting potential thermoelectric application. The present framework of calculations mainly includes deformation potential module, electrical transport performance module, mechanical and thermodynamic properties module. We have also set up a search webpage for the calculated database of thermoelectric materials, providing searching and viewing the related physical properties of materials. Our work may inspire the construction of more computational databases of first-principle thermoelectric materials and accelerate research progress in the field of thermoelectrics.","sentences":["Since the implementation of the Materials Genome Project by the Obama administration in the United States, the development of various computational materials databases has fundamentally expanded the choices of industries such as materials and energy.","In the field of thermoelectric materials, the thermoelectric figure of merit ZT quantifies the performance of the material.","From the viewpoint of calculations for vast materials, the ZT values are not easily obtained due to their computational complexity.","Here, we show how to build a database of thermoelectric materials based on first-principles calculations for the electronic and heat transport of materials.","Firstly, the initial structures are classified according to the values of bandgap and other basic properties using the clustering algorithm K-means in machine learning, and high-throughput first principles calculations are carried out for narrow-bandgap semiconductors which exhibiting potential thermoelectric application.","The present framework of calculations mainly includes deformation potential module, electrical transport performance module, mechanical and thermodynamic properties module.","We have also set up a search webpage for the calculated database of thermoelectric materials, providing searching and viewing the related physical properties of materials.","Our work may inspire the construction of more computational databases of first-principle thermoelectric materials and accelerate research progress in the field of thermoelectrics."],"url":"http://arxiv.org/abs/2404.02571v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 08:44:51","title":"MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness","abstract":"This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to detect semantic relatedness of two sentences in a given target language without access to direct supervision (i.e. zero-shot cross-lingual transfer). To this end, we focus on different source language selection strategies on two different pre-trained languages models: XLM-R and Furina. We experiment with 1) single-source transfer and select source languages based on typological similarity, 2) augmenting English training data with the two nearest-neighbor source languages, and 3) multi-source transfer where we compare selecting on all training languages against languages from the same family. We further study machine translation-based data augmentation and the impact of script differences. Our submission achieved the first place in the C8 (Kinyarwanda) test set.","sentences":["This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness (STR), on Track C: Cross-lingual.","The task aims to detect semantic relatedness of two sentences in a given target language without access to direct supervision (i.e. zero-shot cross-lingual transfer).","To this end, we focus on different source language selection strategies on two different pre-trained languages models: XLM-R and Furina.","We experiment with 1) single-source transfer and select source languages based on typological similarity, 2) augmenting English training data with the two nearest-neighbor source languages, and 3) multi-source transfer where we compare selecting on all training languages against languages from the same family.","We further study machine translation-based data augmentation and the impact of script differences.","Our submission achieved the first place in the C8 (Kinyarwanda) test set."],"url":"http://arxiv.org/abs/2404.02570v1","category":"cs.CL"}
{"created":"2024-04-03 08:40:33","title":"Fusing Multi-sensor Input with State Information on TinyML Brains for Autonomous Nano-drones","abstract":"Autonomous nano-drones (~10 cm in diameter), thanks to their ultra-low power TinyML-based brains, are capable of coping with real-world environments. However, due to their simplified sensors and compute units, they are still far from the sense-and-act capabilities shown in their bigger counterparts. This system paper presents a novel deep learning-based pipeline that fuses multi-sensorial input (i.e., low-resolution images and 8x8 depth map) with the robot's state information to tackle a human pose estimation task. Thanks to our design, the proposed system -- trained in simulation and tested on a real-world dataset -- improves a state-unaware State-of-the-Art baseline by increasing the R^2 regression metric up to 0.10 on the distance's prediction.","sentences":["Autonomous nano-drones (~10 cm in diameter), thanks to their ultra-low power TinyML-based brains, are capable of coping with real-world environments.","However, due to their simplified sensors and compute units, they are still far from the sense-and-act capabilities shown in their bigger counterparts.","This system paper presents a novel deep learning-based pipeline that fuses multi-sensorial input (i.e., low-resolution images and 8x8 depth map) with the robot's state information to tackle a human pose estimation task.","Thanks to our design, the proposed system -- trained in simulation and tested on a real-world dataset -- improves a state-unaware State-of-the-Art baseline by increasing the R^2 regression metric up to 0.10 on the distance's prediction."],"url":"http://arxiv.org/abs/2404.02567v1","category":"cs.RO"}
{"created":"2024-04-03 08:35:26","title":"Effect of constraint relaxation on dynamic critical phenomena in minimum vertex cover problem","abstract":"The effects of constraint relaxation on dynamic critical phenomena in the Minimum Vertex Cover (MVC) problem on Erd\\H{o}s-R\\'enyi random graphs are investigated using Markov chain Monte Carlo simulations. Following our previous work that revealed the reduction of the critical temperature by constraint relaxation based on the penalty function method, this study focuses on investigating the critical properties of the relaxation time along its phase boundary. It is found that the dynamical correlation function of MVC with respect to the problem size and the constraint strength follows a universal scaling function. The analysis shows that the relaxation time decreases as the constraints are relaxed. This decrease is more pronounced for the critical amplitude than for the critical exponent, and this result is interpreted in terms of the system's microscopic energy barriers due to the constraint relaxation.","sentences":["The effects of constraint relaxation on dynamic critical phenomena in the Minimum Vertex Cover (MVC) problem on Erd\\H{o}s-R\\'enyi random graphs are investigated using Markov chain Monte Carlo simulations.","Following our previous work that revealed the reduction of the critical temperature by constraint relaxation based on the penalty function method, this study focuses on investigating the critical properties of the relaxation time along its phase boundary.","It is found that the dynamical correlation function of MVC with respect to the problem size and the constraint strength follows a universal scaling function.","The analysis shows that the relaxation time decreases as the constraints are relaxed.","This decrease is more pronounced for the critical amplitude than for the critical exponent, and this result is interpreted in terms of the system's microscopic energy barriers due to the constraint relaxation."],"url":"http://arxiv.org/abs/2404.02564v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 08:33:08","title":"Representation Alignment Contrastive Regularization for Multi-Object Tracking","abstract":"Achieving high-performance in multi-object tracking algorithms heavily relies on modeling spatio-temporal relationships during the data association stage. Mainstream approaches encompass rule-based and deep learning-based methods for spatio-temporal relationship modeling. While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs. This work aims to simplify deep learning-based spatio-temporal relationship models and introduce interpretability into features for data association. Specifically, a lightweight single-layer transformer encoder is utilized to model spatio-temporal relationships. To make features more interpretative, two contrastive regularization losses based on representation alignment are proposed, derived from spatio-temporal consistency rules. By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow. Experimental results showcase that our model enhances the majority of existing tracking networks' performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs.","sentences":["Achieving high-performance in multi-object tracking algorithms heavily relies on modeling spatio-temporal relationships during the data association stage.","Mainstream approaches encompass rule-based and deep learning-based methods for spatio-temporal relationship modeling.","While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs.","This work aims to simplify deep learning-based spatio-temporal relationship models and introduce interpretability into features for data association.","Specifically, a lightweight single-layer transformer encoder is utilized to model spatio-temporal relationships.","To make features more interpretative, two contrastive regularization losses based on representation alignment are proposed, derived from spatio-temporal consistency rules.","By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow.","Experimental results showcase that our model enhances the majority of existing tracking networks' performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs."],"url":"http://arxiv.org/abs/2404.02562v1","category":"cs.CV"}
{"created":"2024-04-03 08:26:30","title":"Crystallographic dependence of Field Evaporation Energy Barrier in metals using Field Evaporation Energy Loss Spectroscopy mapping","abstract":"Atom probe tomography data is composed of a list of coordinates of the reconstructed atoms in the probed volume. The elemental identity of each atom is derived from time-of-flight mass spectrometry, with no local energetic or chemical information readily available within the mass spectrum. Here, we used a new data processing technique referred to as field evaporation energy loss spectroscopy (FEELS), which analyses the tails of mass peaks. FEELS was used to extract critical energetic parameters that characterize the field evaporation process, which are related to the binding energy of atoms to the surface under intense electrostatic field and dependent of the path followed by the departing atoms during the field evaporation process. We focused our study on different pure face centered cubic metals (Al, Ni, Rh). We demonstrate that the energetic parameters extracted from mass spectra can be mapped in 2D with nanometric resolution. A dependence on the considered crystallographic planes is observed, with sets of planes of low Miller indices showing a lower sensitivity to the intensity of the electric field, which indicates a lower effective attachment energy. The temperature is also an important parameter in particular for Al, which we attribute to an energetic transition between two paths of field evaporation between 25K and 60K close to (002) pole at the specimen's surface. This paper shows that the complex information that can be retrieved from the measured energy loss of surface atoms is important both instrumentally and fundamentally.","sentences":["Atom probe tomography data is composed of a list of coordinates of the reconstructed atoms in the probed volume.","The elemental identity of each atom is derived from time-of-flight mass spectrometry, with no local energetic or chemical information readily available within the mass spectrum.","Here, we used a new data processing technique referred to as field evaporation energy loss spectroscopy (FEELS), which analyses the tails of mass peaks.","FEELS was used to extract critical energetic parameters that characterize the field evaporation process, which are related to the binding energy of atoms to the surface under intense electrostatic field and dependent of the path followed by the departing atoms during the field evaporation process.","We focused our study on different pure face centered cubic metals (Al, Ni, Rh).","We demonstrate that the energetic parameters extracted from mass spectra can be mapped in 2D with nanometric resolution.","A dependence on the considered crystallographic planes is observed, with sets of planes of low Miller indices showing a lower sensitivity to the intensity of the electric field, which indicates a lower effective attachment energy.","The temperature is also an important parameter in particular for Al, which we attribute to an energetic transition between two paths of field evaporation between 25K and 60K close to (002) pole at the specimen's surface.","This paper shows that the complex information that can be retrieved from the measured energy loss of surface atoms is important both instrumentally and fundamentally."],"url":"http://arxiv.org/abs/2404.02557v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 08:21:03","title":"Optimal Riemannian metric for Poincar{\u00e9} inequalities and how to ideally precondition Langevin dymanics","abstract":"Poincar{\\'e} inequality is a fundamental property that rises naturally in different branches of mathematics. The associated Poincar{\\'e} constant plays a central role in many applications, since it governs the convergence of various practical algorithms. For instance, the convergence rate of the Langevin dynamics is exactly given by the Poincar{\\'e} constant. This paper investigates a Riemannian version of Poincar{\\'e} inequality where a positive definite weighting matrix field (i.e. a Riemannian metric) is introduced to improve the Poincar{\\'e} constant, and therefore the performances of the associated algorithm. Assuming the underlying measure is a \\emph{moment measure}, we show that an optimal metric exists and the resulting Poincar{\\'e} constant is 1. We demonstrate that such optimal metric is necessarily a \\emph{Stein kernel}, offering a novel perspective on these complex but central mathematical objects that are hard to obtain in practice. We further discuss how to numerically obtain the optimal metric by deriving an implementable optimization algorithm. The resulting method is illustrated on a few simple but nontrivial examples, where solutions are revealed to be rather sophisticated. We also demonstrate how to design efficient Langevin-based sampling schemes by utilizing the precomputed optimal metric as a preconditioner.","sentences":["Poincar{\\'e} inequality is a fundamental property that rises naturally in different branches of mathematics.","The associated Poincar{\\'e} constant plays a central role in many applications, since it governs the convergence of various practical algorithms.","For instance, the convergence rate of the Langevin dynamics is exactly given by the Poincar{\\'e} constant.","This paper investigates a Riemannian version of Poincar{\\'e} inequality where a positive definite weighting matrix field (i.e. a Riemannian metric) is introduced to improve the Poincar{\\'e} constant, and therefore the performances of the associated algorithm.","Assuming the underlying measure is a \\emph{moment measure}, we show that an optimal metric exists and the resulting Poincar{\\'e} constant is 1.","We demonstrate that such optimal metric is necessarily a \\emph{Stein kernel}, offering a novel perspective on these complex but central mathematical objects that are hard to obtain in practice.","We further discuss how to numerically obtain the optimal metric by deriving an implementable optimization algorithm.","The resulting method is illustrated on a few simple but nontrivial examples, where solutions are revealed to be rather sophisticated.","We also demonstrate how to design efficient Langevin-based sampling schemes by utilizing the precomputed optimal metric as a preconditioner."],"url":"http://arxiv.org/abs/2404.02554v1","category":"math.PR"}
{"created":"2024-04-03 07:51:57","title":"A continuous approach of modeling tumorigenesis and axons regulation for the pancreatic cancer","abstract":"The pancreatic innervation undergoes dynamic remodeling during the development of pancreatic ductal adenocarcinoma (PDAC). Denervation experiments have shown that different types of axons can exert either pro- or anti-tumor effects, but conflicting results exist in the literature, leaving the overall influence of the nervous system on PDAC incompletely understood. To address this gap, we propose a continuous mathematical model of nerve-tumor interactions that allows in silico simulation of denervation at different phases of tumor development. This model takes into account the pro- or anti-tumor properties of different types of axons (sympathetic or sensory) and their distinct remodeling dynamics during PDAC development. We observe a \"shift effect\" where an initial pro-tumor effect of sympathetic axon denervation is later outweighed by the anti-tumor effect of sensory axon denervation, leading to a transition from an overall protective to a deleterious role of the nervous system on PDAC tumorigenesis. Our model also highlights the importance of the impact of sympathetic axon remodeling dynamics on tumor progression. These findings may guide strategies targeting the nervous system to improve PDAC treatment.","sentences":["The pancreatic innervation undergoes dynamic remodeling during the development of pancreatic ductal adenocarcinoma (PDAC).","Denervation experiments have shown that different types of axons can exert either pro- or anti-tumor effects, but conflicting results exist in the literature, leaving the overall influence of the nervous system on PDAC incompletely understood.","To address this gap, we propose a continuous mathematical model of nerve-tumor interactions that allows in silico simulation of denervation at different phases of tumor development.","This model takes into account the pro- or anti-tumor properties of different types of axons (sympathetic or sensory) and their distinct remodeling dynamics during PDAC development.","We observe a \"shift effect\" where an initial pro-tumor effect of sympathetic axon denervation is later outweighed by the anti-tumor effect of sensory axon denervation, leading to a transition from an overall protective to a deleterious role of the nervous system on PDAC tumorigenesis.","Our model also highlights the importance of the impact of sympathetic axon remodeling dynamics on tumor progression.","These findings may guide strategies targeting the nervous system to improve PDAC treatment."],"url":"http://arxiv.org/abs/2404.02539v1","category":"math.AP"}
{"created":"2024-04-03 07:33:57","title":"Computationally Efficient Unsupervised Deep Learning for Robust Joint AP Clustering and Beamforming Design in Cell-Free Systems","abstract":"In this paper, we consider robust joint access point (AP) clustering and beamforming design with imperfect channel state information (CSI) in cell-free systems. Specifically, we jointly optimize AP clustering and beamforming with imperfect CSI to simultaneously maximize the worst-case sum rate and minimize the number of AP clustering under power constraint and the sparsity constraint of AP clustering. By transformations, the semi-infinite constraints caused by the imperfect CSI are converted into more tractable forms for facilitating a computationally efficient unsupervised deep learning algorithm. In addition, to further reduce the computational complexity, a computationally effective unsupervised deep learning algorithm is proposed to implement robust joint AP clustering and beamforming design with imperfect CSI in cell-free systems. Numerical results demonstrate that the proposed unsupervised deep learning algorithm achieves a higher worst-case sum rate under a smaller number of AP clustering with computational efficiency.","sentences":["In this paper, we consider robust joint access point (AP) clustering and beamforming design with imperfect channel state information (CSI) in cell-free systems.","Specifically, we jointly optimize AP clustering and beamforming with imperfect CSI to simultaneously maximize the worst-case sum rate and minimize the number of AP clustering under power constraint and the sparsity constraint of AP clustering.","By transformations, the semi-infinite constraints caused by the imperfect CSI are converted into more tractable forms for facilitating a computationally efficient unsupervised deep learning algorithm.","In addition, to further reduce the computational complexity, a computationally effective unsupervised deep learning algorithm is proposed to implement robust joint AP clustering and beamforming design with imperfect CSI in cell-free systems.","Numerical results demonstrate that the proposed unsupervised deep learning algorithm achieves a higher worst-case sum rate under a smaller number of AP clustering with computational efficiency."],"url":"http://arxiv.org/abs/2404.02531v1","category":"cs.IT"}
{"created":"2024-04-03 07:16:43","title":"On a conjecture concerning the Fisher--Widom line and the line of vanishing excess isothermal compressibility in simple fluids","abstract":"In the statistical mechanics approach to liquid-state theory, understanding the role of the intermolecular potential in determining thermodynamic and structural properties is crucial. The Fisher--Widom (FW) line, which separates regions in the temperature vs density plane where the decay of the total correlation function is monotonic or oscillatory, provides insights into the dominance of the attractive or repulsive part of the interactions. Stopper et al. have recently conjectured [J. Chem. Phys. \\textbf{151}, 014501 (2019)] that the line of vanishing excess isothermal compressibility approximates the FW line in simple fluids. Here, we investigate this conjecture using the Jagla potential and also explore the line of vanishing excess pressure. We employ theoretical approximations and Monte Carlo simulations to study one-dimensional and three-dimensional systems. While exact results for the one-dimensional case do not support the conjecture, our Monte Carlo simulations for the three-dimensional fluid validate it. Our findings not only contribute to the understanding of the relationship between the three transition lines but also provide valuable insights into the thermodynamic and structural behaviour of simple fluids.","sentences":["In the statistical mechanics approach to liquid-state theory, understanding the role of the intermolecular potential in determining thermodynamic and structural properties is crucial.","The Fisher--Widom (FW) line, which separates regions in the temperature vs density plane where the decay of the total correlation function is monotonic or oscillatory, provides insights into the dominance of the attractive or repulsive part of the interactions.","Stopper et al. have recently conjectured [J. Chem.","Phys. \\textbf{151}, 014501 (2019)] that the line of vanishing excess isothermal compressibility approximates the FW line in simple fluids.","Here, we investigate this conjecture using the Jagla potential and also explore the line of vanishing excess pressure.","We employ theoretical approximations and Monte Carlo simulations to study one-dimensional and three-dimensional systems.","While exact results for the one-dimensional case do not support the conjecture, our Monte Carlo simulations for the three-dimensional fluid validate it.","Our findings not only contribute to the understanding of the relationship between the three transition lines but also provide valuable insights into the thermodynamic and structural behaviour of simple fluids."],"url":"http://arxiv.org/abs/2404.02520v1","category":"cond-mat.soft"}
{"created":"2024-04-03 07:10:18","title":"HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras","abstract":"Three-dimensional perception from multi-view cameras is a crucial component in autonomous driving systems, which involves multiple tasks like 3D object detection and bird's-eye-view (BEV) semantic segmentation. To improve perception precision, large image encoders, high-resolution images, and long-term temporal inputs have been adopted in recent 3D perception models, bringing remarkable performance gains. However, these techniques are often incompatible in training and inference scenarios due to computational resource constraints. Besides, modern autonomous driving systems prefer to adopt an end-to-end framework for multi-task 3D perception, which can simplify the overall system architecture and reduce the implementation complexity. However, conflict between tasks often arises when optimizing multiple tasks jointly within an end-to-end 3D perception model. To alleviate these issues, we present an end-to-end framework named HENet for multi-task 3D perception in this paper. Specifically, we propose a hybrid image encoding network, using a large image encoder for short-term frames and a small image encoder for long-term temporal frames. Then, we introduce a temporal feature integration module based on the attention mechanism to fuse the features of different frames extracted by the two aforementioned hybrid image encoders. Finally, according to the characteristics of each perception task, we utilize BEV features of different grid sizes, independent BEV encoders, and task decoders for different tasks. Experimental results show that HENet achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, including 3D object detection and BEV semantic segmentation. The source code and models will be released at https://github.com/VDIGPKU/HENet.","sentences":["Three-dimensional perception from multi-view cameras is a crucial component in autonomous driving systems, which involves multiple tasks like 3D object detection and bird's-eye-view (BEV) semantic segmentation.","To improve perception precision, large image encoders, high-resolution images, and long-term temporal inputs have been adopted in recent 3D perception models, bringing remarkable performance gains.","However, these techniques are often incompatible in training and inference scenarios due to computational resource constraints.","Besides, modern autonomous driving systems prefer to adopt an end-to-end framework for multi-task 3D perception, which can simplify the overall system architecture and reduce the implementation complexity.","However, conflict between tasks often arises when optimizing multiple tasks jointly within an end-to-end 3D perception model.","To alleviate these issues, we present an end-to-end framework named HENet for multi-task 3D perception in this paper.","Specifically, we propose a hybrid image encoding network, using a large image encoder for short-term frames and a small image encoder for long-term temporal frames.","Then, we introduce a temporal feature integration module based on the attention mechanism to fuse the features of different frames extracted by the two aforementioned hybrid image encoders.","Finally, according to the characteristics of each perception task, we utilize BEV features of different grid sizes, independent BEV encoders, and task decoders for different tasks.","Experimental results show that HENet achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, including 3D object detection and BEV semantic segmentation.","The source code and models will be released at https://github.com/VDIGPKU/HENet."],"url":"http://arxiv.org/abs/2404.02517v1","category":"cs.CV"}
{"created":"2024-04-03 07:09:24","title":"On-the-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves","abstract":"By-tree information gathering is an essential task in precision agriculture achieved by ground mobile sensors, but it can be time- and labor-intensive. In this paper we present an algorithmic framework to perform real-time and on-the-go detection of trees and key geometric characteristics (namely, width and height) with wheeled mobile robots in the field. Our method is based on the fusion of 2D domain-specific data (normalized difference vegetation index [NDVI] acquired via a red-green-near-infrared [RGN] camera) and 3D LiDAR point clouds, via a customized tree landmark association and parameter estimation algorithm. The proposed system features a multi-modal and entropy-based landmark correspondences approach, integrated into an underlying Kalman filter system to recognize the surrounding trees and jointly estimate their spatial and vegetation-based characteristics. Realistic simulated tests are used to evaluate our proposed algorithm's behavior in a variety of settings. Physical experiments in agricultural fields help validate our method's efficacy in acquiring accurate by-tree information on-the-go and in real-time by employing only onboard computational and sensing resources.","sentences":["By-tree information gathering is an essential task in precision agriculture achieved by ground mobile sensors, but it can be time- and labor-intensive.","In this paper we present an algorithmic framework to perform real-time and on-the-go detection of trees and key geometric characteristics (namely, width and height) with wheeled mobile robots in the field.","Our method is based on the fusion of 2D domain-specific data (normalized difference vegetation index","[NDVI] acquired via a red-green-near-infrared [RGN] camera) and 3D LiDAR point clouds, via a customized tree landmark association and parameter estimation algorithm.","The proposed system features a multi-modal and entropy-based landmark correspondences approach, integrated into an underlying Kalman filter system to recognize the surrounding trees and jointly estimate their spatial and vegetation-based characteristics.","Realistic simulated tests are used to evaluate our proposed algorithm's behavior in a variety of settings.","Physical experiments in agricultural fields help validate our method's efficacy in acquiring accurate by-tree information on-the-go and in real-time by employing only onboard computational and sensing resources."],"url":"http://arxiv.org/abs/2404.02516v1","category":"cs.RO"}
{"created":"2024-04-03 06:57:45","title":"Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages","abstract":"With the primary focus on evaluating the effectiveness of large language models for automatic reference-less translation assessment, this work presents our experiments on mimicking human direct assessment to evaluate the quality of translations in English and Indian languages. We constructed a translation evaluation task where we performed zero-shot learning, in-context example-driven learning, and fine-tuning of large language models to provide a score out of 100, where 100 represents a perfect translation and 1 represents a poor translation. We compared the performance of our trained systems with existing methods such as COMET, BERT-Scorer, and LABSE, and found that the LLM-based evaluator (LLaMA-2-13B) achieves a comparable or higher overall correlation with human judgments for the considered Indian language pairs.","sentences":["With the primary focus on evaluating the effectiveness of large language models for automatic reference-less translation assessment, this work presents our experiments on mimicking human direct assessment to evaluate the quality of translations in English and Indian languages.","We constructed a translation evaluation task where we performed zero-shot learning, in-context example-driven learning, and fine-tuning of large language models to provide a score out of 100, where 100 represents a perfect translation and 1 represents a poor translation.","We compared the performance of our trained systems with existing methods such as COMET, BERT-Scorer, and LABSE, and found that the LLM-based evaluator (LLaMA-2-13B) achieves a comparable or higher overall correlation with human judgments for the considered Indian language pairs."],"url":"http://arxiv.org/abs/2404.02512v1","category":"cs.CL"}
{"created":"2024-04-03 06:55:59","title":"Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach","abstract":"In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges. In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost. This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations. To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ and $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\\varepsilon^2)$ for both settings given a stochastic gradient oracle with variance $\\sigma^2$. Compared with the prior work \\cite{wai-fw-2017}, our framework relaxes the assumption of the optimal solution being a strict interior point of the feasible set and enjoys wider applicability for large-scale training using a stochastic gradient oracle. We also demonstrate the efficiency of our algorithms with various numerical experiments.","sentences":["In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges.","In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost.","This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations.","To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ and $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\\varepsilon^2)$ for both settings given a stochastic gradient oracle with variance $\\sigma^2$. Compared with the prior work \\cite{wai-fw-2017}, our framework relaxes the assumption of the optimal solution being a strict interior point of the feasible set and enjoys wider applicability for large-scale training using a stochastic gradient oracle.","We also demonstrate the efficiency of our algorithms with various numerical experiments."],"url":"http://arxiv.org/abs/2404.02511v1","category":"math.OC"}
{"created":"2024-04-03 06:53:48","title":"Utilizing Quantum Processor for the Analysis of Strongly Correlated Materials","abstract":"This study introduces a systematic approach for analyzing strongly correlated systems by adapting the conventional quantum cluster method to a quantum circuit model. We have developed a more concise formula for calculating the cluster's Green's function, requiring only real-number computations on the quantum circuit instead of complex ones. This approach is inherently more suited to quantum circuits, which primarily yield statistical probabilities. As an illustrative example, we explored the Hubbard model on a 2D lattice. The ground state is determined utilizing Xiaohong, a superconducting quantum processor equipped with 66 qubits, supplied by QuantumCTek Co., Ltd. Subsequently, we employed the circuit model to compute the real-time retarded Green's function for the cluster, which is then used to determine the lattice Green's function. We conducted an examination of the band structure in the insulator phase of the lattice system. This preliminary investigation lays the groundwork for exploring a wealth of innovative physics within the field of condensed matter physics.","sentences":["This study introduces a systematic approach for analyzing strongly correlated systems by adapting the conventional quantum cluster method to a quantum circuit model.","We have developed a more concise formula for calculating the cluster's Green's function, requiring only real-number computations on the quantum circuit instead of complex ones.","This approach is inherently more suited to quantum circuits, which primarily yield statistical probabilities.","As an illustrative example, we explored the Hubbard model on a 2D lattice.","The ground state is determined utilizing Xiaohong, a superconducting quantum processor equipped with 66 qubits, supplied by QuantumCTek Co., Ltd.","Subsequently, we employed the circuit model to compute the real-time retarded Green's function for the cluster, which is then used to determine the lattice Green's function.","We conducted an examination of the band structure in the insulator phase of the lattice system.","This preliminary investigation lays the groundwork for exploring a wealth of innovative physics within the field of condensed matter physics."],"url":"http://arxiv.org/abs/2404.02509v1","category":"quant-ph"}
{"created":"2024-04-03 06:45:48","title":"Nonlinear Corner States in Topologically Nontrivial Kagome Lattice","abstract":"We investigate a higher-order topological insulator (HOTI) under strong nonlinearity, focusing on the existence and stability of high-amplitude corner states, which can find applications in optics, acoustics, elastodynamics, and other wave-based systems. Our study centers on a breathing Kagome lattice composed of point masses and springs known to exhibit edge and corner states in its linear regime. By introducing onsite cubic nonlinearity, we analyze its impact on both edge and corner states. The nonlinear continuation of the corner state unveils stable high-amplitude corner states within the lattice, featuring non-zero displacements at even sites from the corner -- a characteristic absent in the linear limit. Interestingly, the nonlinear continuation of the edge state reveals its transformation into distinct families of high-amplitude corner states via two pitchfork bifurcations. While some states maintain stability, others become unstable through real instability and Neimark-Sacker bifurcation. These unstable corner states dissipate their energy into the edges and the bulk over an extended period, as corroborated by long-time dynamical simulations. Consequently, our study provides insights into achieving significant energy localization at the corners of HOTIs through various classes of nonlinear states.","sentences":["We investigate a higher-order topological insulator (HOTI) under strong nonlinearity, focusing on the existence and stability of high-amplitude corner states, which can find applications in optics, acoustics, elastodynamics, and other wave-based systems.","Our study centers on a breathing Kagome lattice composed of point masses and springs known to exhibit edge and corner states in its linear regime.","By introducing onsite cubic nonlinearity, we analyze its impact on both edge and corner states.","The nonlinear continuation of the corner state unveils stable high-amplitude corner states within the lattice, featuring non-zero displacements at even sites from the corner -- a characteristic absent in the linear limit.","Interestingly, the nonlinear continuation of the edge state reveals its transformation into distinct families of high-amplitude corner states via two pitchfork bifurcations.","While some states maintain stability, others become unstable through real instability and Neimark-Sacker bifurcation.","These unstable corner states dissipate their energy into the edges and the bulk over an extended period, as corroborated by long-time dynamical simulations.","Consequently, our study provides insights into achieving significant energy localization at the corners of HOTIs through various classes of nonlinear states."],"url":"http://arxiv.org/abs/2404.02504v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 06:38:16","title":"Nonlinear integral extension of PID control with improved convergence of perturbed second-order dynamic systems","abstract":"Nonlinear extension of the integral part of PID feedback control is proposed for the perturbed second-order systems. For the matched constant perturbations, the global asymptotic stability is shown, and for Lipschitz perturbations an ultimately bounded output error is guaranteed. The second-order system plants can also be expanded by an additional (parasitic) actuator dynamics with low-pass characteristics. The proposed nonlinear control is proven to outperform its linear (PID) benchmarking counterpart during the settling phase, i.e. at convergence of the residual output error. An experimental case study of the second-order system with an additional actuator dynamics and considerable perturbation is demonstrated to confirm and benchmark the control performance.","sentences":["Nonlinear extension of the integral part of PID feedback control is proposed for the perturbed second-order systems.","For the matched constant perturbations, the global asymptotic stability is shown, and for Lipschitz perturbations an ultimately bounded output error is guaranteed.","The second-order system plants can also be expanded by an additional (parasitic) actuator dynamics with low-pass characteristics.","The proposed nonlinear control is proven to outperform its linear (PID) benchmarking counterpart during the settling phase, i.e. at convergence of the residual output error.","An experimental case study of the second-order system with an additional actuator dynamics and considerable perturbation is demonstrated to confirm and benchmark the control performance."],"url":"http://arxiv.org/abs/2404.02502v1","category":"math.OC"}
{"created":"2024-04-03 05:48:40","title":"A Dean-Kawasaki equation for reaction diffusion systems driven by Poisson noise","abstract":"We derive a stochastic partial differential equation that describes the fluctuating behaviour of reaction-diffusion systems of N particles, undergoing Markovian, unary reactions. This generalises the work of Dean [J. Phys. A: Math. and Gen., 29 (24), L613, (1996)] through the inclusion of random Poisson fields. Our approach is based on weak interactions, which has the dual benefit that the resulting equations asymptotically converge (in the N to infinity limit) on a variation of a McKean- Vlasov diffusion, whilst still being related to the case of Dean-like strong interactions via a trivial rescaling. Various examples are presented, alongside a discussion of possible extensions to more complicated reaction schemes.","sentences":["We derive a stochastic partial differential equation that describes the fluctuating behaviour of reaction-diffusion systems of N particles, undergoing Markovian, unary reactions.","This generalises the work of Dean [J. Phys.","A: Math. and Gen., 29 (24), L613, (1996)] through the inclusion of random Poisson fields.","Our approach is based on weak interactions, which has the dual benefit that the resulting equations asymptotically converge (in the N to infinity limit) on a variation of a McKean- Vlasov diffusion, whilst still being related to the case of Dean-like strong interactions via a trivial rescaling.","Various examples are presented, alongside a discussion of possible extensions to more complicated reaction schemes."],"url":"http://arxiv.org/abs/2404.02487v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 05:47:55","title":"Joint Optimization on Uplink OFDMA and MU-MIMO for IEEE 802.11ax: Deep Hierarchical Reinforcement Learning Approach","abstract":"This letter tackles a joint user scheduling, frequency resource allocation (USRA), multi-input-multi-output mode selection (MIMO MS) between single-user MIMO and multi-user (MU) MIMO, and MU-MIMO user selection problem, integrating uplink orthogonal frequency division multiple access (OFDMA) in IEEE 802.11ax. Specifically, we focus on \\textit{unsaturated traffic conditions} where users' data demands fluctuate. In unsaturated traffic conditions, considering packet volumes per user introduces a combinatorial problem, requiring the simultaneous optimization of MU-MIMO user selection and RA along the time-frequency-space axis. Consequently, dealing with the combinatorial nature of this problem, characterized by a large cardinality of unknown variables, poses a challenge that conventional optimization methods find nearly impossible to address. In response, this letter proposes an approach with deep hierarchical reinforcement learning (DHRL) to solve the joint problem. Rather than simply adopting off-the-shelf DHRL, we \\textit{tailor} the DHRL to the joint USRA and MS problem, thereby significantly improving the convergence speed and throughput. Extensive simulation results show that the proposed algorithm achieves significantly improved throughput compared to the existing schemes under various unsaturated traffic conditions.","sentences":["This letter tackles a joint user scheduling, frequency resource allocation (USRA), multi-input-multi-output mode selection (MIMO MS) between single-user MIMO and multi-user (MU) MIMO, and MU-MIMO user selection problem, integrating uplink orthogonal frequency division multiple access (OFDMA) in IEEE 802.11ax.","Specifically, we focus on \\textit{unsaturated traffic conditions} where users' data demands fluctuate.","In unsaturated traffic conditions, considering packet volumes per user introduces a combinatorial problem, requiring the simultaneous optimization of MU-MIMO user selection and RA along the time-frequency-space axis.","Consequently, dealing with the combinatorial nature of this problem, characterized by a large cardinality of unknown variables, poses a challenge that conventional optimization methods find nearly impossible to address.","In response, this letter proposes an approach with deep hierarchical reinforcement learning (DHRL) to solve the joint problem.","Rather than simply adopting off-the-shelf DHRL, we \\textit{tailor} the DHRL to the joint USRA and MS problem, thereby significantly improving the convergence speed and throughput.","Extensive simulation results show that the proposed algorithm achieves significantly improved throughput compared to the existing schemes under various unsaturated traffic conditions."],"url":"http://arxiv.org/abs/2404.02486v1","category":"eess.SY"}
{"created":"2024-04-03 05:44:22","title":"Dirac fermions collimation in heterostructures based on tilted Dirac cone materials","abstract":"This paper aims to theoretically analyze the behavior of Dirac fermions in tilted Dirac cone material, particularly those that have diffused a barrier potential.Our results show that the degree of tilt in the y-direction can lead to different collimations of the Dirac fermion beams relative to the Fermi and confinement surfaces. To study the transmission probability, we exploited our results numerically, taking into account the various configurations of the system and the different external and internal physical parameters by characterizing the behavior of fermionic transport in a proposed heterostructure. Our findings lay the groundwork for developing controllable electronic devices utilizing Dirac fermion collimation, governed by the tilt parameter, enabling precise manipulation and enhanced functionality.","sentences":["This paper aims to theoretically analyze the behavior of Dirac fermions in tilted Dirac cone material, particularly those that have diffused a barrier potential.","Our results show that the degree of tilt in the y-direction can lead to different collimations of the Dirac fermion beams relative to the Fermi and confinement surfaces.","To study the transmission probability, we exploited our results numerically, taking into account the various configurations of the system and the different external and internal physical parameters by characterizing the behavior of fermionic transport in a proposed heterostructure.","Our findings lay the groundwork for developing controllable electronic devices utilizing Dirac fermion collimation, governed by the tilt parameter, enabling precise manipulation and enhanced functionality."],"url":"http://arxiv.org/abs/2404.02485v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 05:42:27","title":"Speed, power and cost implications for GPU acceleration of Computational Fluid Dynamics on HPC systems","abstract":"Computational Fluid Dynamics (CFD) is the simulation of fluid flow undertaken with the use of computational hardware. The underlying equations are computationally challenging to solve and necessitate high performance computing (HPC) to resolve in a practical timeframe when a reasonable level of fidelity is required. The simulations are memory intensive, having previously been limited to central processing unit (CPU) solvers, as graphics processing unit (GPU) video random access memory (VRAM) was insufficient. However, with recent developments in GPU design and increases to VRAM, GPU acceleration of CPU solved workflows is now possible. At HPC scale however, many operational details are still unknown. This paper utilizes ANSYS Fluent, a leading commercial code in CFD, to investigate the compute speed, power consumption and service unit (SU) cost considerations for the GPU acceleration of CFD workflows on HPC architectures. To provide a comprehensive analysis, different CPU architectures, and GPUs have been assessed. It is seen that GPU compute speed is faster, however, the initialisation speed, power and cost performance is less clear cut. Whilst the larger A100 cards perform well with respect to power consumption, this is not observed for the V100 cards. In situations where more than one GPU is required, their adoption may not be beneficial from a power or cost perspective.","sentences":["Computational Fluid Dynamics (CFD) is the simulation of fluid flow undertaken with the use of computational hardware.","The underlying equations are computationally challenging to solve and necessitate high performance computing (HPC) to resolve in a practical timeframe when a reasonable level of fidelity is required.","The simulations are memory intensive, having previously been limited to central processing unit (CPU) solvers, as graphics processing unit (GPU) video random access memory (VRAM) was insufficient.","However, with recent developments in GPU design and increases to VRAM, GPU acceleration of CPU solved workflows is now possible.","At HPC scale however, many operational details are still unknown.","This paper utilizes ANSYS Fluent, a leading commercial code in CFD, to investigate the compute speed, power consumption and service unit (SU) cost considerations for the GPU acceleration of CFD workflows on HPC architectures.","To provide a comprehensive analysis, different CPU architectures, and GPUs have been assessed.","It is seen that GPU compute speed is faster, however, the initialisation speed, power and cost performance is less clear cut.","Whilst the larger A100 cards perform well with respect to power consumption, this is not observed for the V100 cards.","In situations where more than one GPU is required, their adoption may not be beneficial from a power or cost perspective."],"url":"http://arxiv.org/abs/2404.02482v1","category":"cs.DC"}
{"created":"2024-04-03 05:20:07","title":"Mobile user experience from the lens of project-based learning","abstract":"This paper presents an overview of mobile application projects conducted at the RMIT University as a part of the Learning and Teaching activities within Bachelor and Master programs, in collaboration with industrial partners. We discuss the lessons learned over eight years of teaching the corresponding courses and compare the results of our student project to the trends summarised in the recently published approached from other universities and countries.","sentences":["This paper presents an overview of mobile application projects conducted at the RMIT University as a part of the Learning and Teaching activities within Bachelor and Master programs, in collaboration with industrial partners.","We discuss the lessons learned over eight years of teaching the corresponding courses and compare the results of our student project to the trends summarised in the recently published approached from other universities and countries."],"url":"http://arxiv.org/abs/2404.02470v1","category":"cs.SE"}
{"created":"2024-04-03 05:19:04","title":"Full orbital solutions in pre-main sequence high-order multiple systems: GG Tau Ab and UX Tau B","abstract":"High-order multiple (triple and beyond) systems are relatively common. Their interaction with circumstellar and circumbinary material can have a large impact on the formation and evolution of planetary systems and depends on their orbital properties. GG\\,Tau and UX\\,Tau are two pre-main sequence high-order multiple systems in which the tightest pair has a projected separation of $\\approx5$--20\\,au. Characterizing precisely their orbits is crucial to establish their long-term stability, to predict the dynamics and evolution of circumstellar matter, and to evaluate the potential for planet formation in such systems. We combine existing astrometric measurements with previously unpublished high-resolution observations of the GG\\,Tau\\,Ab and UX\\,Tau\\,B pairs and perform Keplerian orbital fits. For GG\\,Tau\\,Ab the data presented here represent the first detection of orbital motion. For both systems they yield dramatic increases in orbital coverage ($\\gtrsim60\\%$ and $\\approx100\\%$ for UX\\,Tau\\,B and GG\\,Tau\\,Ab, for orbital periods of $\\approx32$ and $\\approx8$\\,yr, respectively) and allow us to obtain well-constrained orbital fits, including dynamical masses with $\\lesssim10\\%$ and $\\lesssim7\\%$ random and systematic uncertainties. We find that both GG\\,Tau\\,A and UX\\,Tau\\,A--B likely form stable hierarchical systems, although one possible deprojection solution for GG\\,Tau is strongly misaligned and could experience von Zeipel-Lidov-Kozai oscillations. We further find that the UX\\,Tau\\,B orbit is much more eccentric than the GG\\,Tau\\,Ab one, possibly explaining the lack of circumstellar material in the former. The newly-determined orbits revive the question of the dynamical fate of gas and dust in these two hierarchical systems and should spur new dedicated simulations to assess the long-term evolution of the systems and the dynamical perturbations imposed by the close binaries they host.","sentences":["High-order multiple (triple and beyond) systems are relatively common.","Their interaction with circumstellar and circumbinary material can have a large impact on the formation and evolution of planetary systems and depends on their orbital properties.","GG\\,Tau and UX\\,Tau are two pre-main sequence high-order multiple systems in which the tightest pair has a projected separation of $\\approx5$--20\\,au.","Characterizing precisely their orbits is crucial to establish their long-term stability, to predict the dynamics and evolution of circumstellar matter, and to evaluate the potential for planet formation in such systems.","We combine existing astrometric measurements with previously unpublished high-resolution observations of the GG\\,Tau\\,Ab and UX\\,Tau\\,B pairs and perform Keplerian orbital fits.","For GG\\,Tau\\,Ab the data presented here represent the first detection of orbital motion.","For both systems they yield dramatic increases in orbital coverage ($\\gtrsim60\\%$ and $\\approx100\\%$ for UX\\,Tau\\,B and GG\\,Tau\\,Ab, for orbital periods of $\\approx32$ and $\\approx8$\\,yr, respectively) and allow us to obtain well-constrained orbital fits, including dynamical masses with $\\lesssim10\\%$ and $\\lesssim7\\%$ random and systematic uncertainties.","We find that both GG\\,Tau\\,A and UX\\,Tau\\,A--B likely form stable hierarchical systems, although one possible deprojection solution for GG\\,Tau is strongly misaligned and could experience von Zeipel-Lidov-Kozai oscillations.","We further find that the UX\\,Tau\\,B orbit is much more eccentric than the GG\\,Tau\\,Ab one, possibly explaining the lack of circumstellar material in the former.","The newly-determined orbits revive the question of the dynamical fate of gas and dust in these two hierarchical systems and should spur new dedicated simulations to assess the long-term evolution of the systems and the dynamical perturbations imposed by the close binaries they host."],"url":"http://arxiv.org/abs/2404.02469v1","category":"astro-ph.SR"}
{"created":"2024-04-03 05:08:46","title":"DiffFit: Visually-Guided Differentiable Fitting of Molecule Structures to Cryo-EM Map","abstract":"We introduce DiffFit, a differentiable algorithm for fitting protein atomistic structures into experimental reconstructed Cryo-Electron Microscopy (cryo-EM) volume map. This process is essential in structural biology to semi-automatically reconstruct large meso-scale models of complex protein assemblies and complete cellular structures that are based on measured cryo-EM data. Current approaches require manual fitting in 3D that already results in approximately aligned structures followed by an automated fine-tuning of the alignment. With our DiffFit approach, we enable domain scientists to automatically fit new structures and visualize the fitting results for inspection and interactive revision. Our fitting begins with differentiable 3D rigid transformations of the protein atom coordinates, followed by sampling the density values at its atom coordinates from the target cryo-EM volume. To ensure a meaningful correlation between the sampled densities and the protein structure, we propose a novel loss function based on a multi-resolution volume-array approach and the exploitation of the negative space. Such loss function serves as a critical metric for assessing the fitting quality, ensuring both fitting accuracy and improved visualization of the results. We assessed the placement quality of DiffFit with several large, realistic datasets and found its quality to be superior to that of previous methods. We further evaluated our method in two use cases. First, we demonstrate its use in the process of automating the integration of known composite structures into larger protein complexes. Second, we show that it facilitates the fitting of predicted protein domains into volume densities to aid researchers in the identification of unknown proteins. We open-sourced (github.com/nanovis/DiffFitViewer) DiffFit as a plugin in ChimeraX. All supplemental materials are available at osf.io/5tx4q.","sentences":["We introduce DiffFit, a differentiable algorithm for fitting protein atomistic structures into experimental reconstructed Cryo-Electron Microscopy (cryo-EM) volume map.","This process is essential in structural biology to semi-automatically reconstruct large meso-scale models of complex protein assemblies and complete cellular structures that are based on measured cryo-EM data.","Current approaches require manual fitting in 3D that already results in approximately aligned structures followed by an automated fine-tuning of the alignment.","With our DiffFit approach, we enable domain scientists to automatically fit new structures and visualize the fitting results for inspection and interactive revision.","Our fitting begins with differentiable 3D rigid transformations of the protein atom coordinates, followed by sampling the density values at its atom coordinates from the target cryo-EM volume.","To ensure a meaningful correlation between the sampled densities and the protein structure, we propose a novel loss function based on a multi-resolution volume-array approach and the exploitation of the negative space.","Such loss function serves as a critical metric for assessing the fitting quality, ensuring both fitting accuracy and improved visualization of the results.","We assessed the placement quality of DiffFit with several large, realistic datasets and found its quality to be superior to that of previous methods.","We further evaluated our method in two use cases.","First, we demonstrate its use in the process of automating the integration of known composite structures into larger protein complexes.","Second, we show that it facilitates the fitting of predicted protein domains into volume densities to aid researchers in the identification of unknown proteins.","We open-sourced (github.com/nanovis/DiffFitViewer) DiffFit as a plugin in ChimeraX. All supplemental materials are available at osf.io/5tx4q."],"url":"http://arxiv.org/abs/2404.02465v2","category":"q-bio.QM"}
{"created":"2024-04-03 05:05:43","title":"Spin-NeuroMem: A Low-Power Neuromorphic Associative Memory Design Based on Spintronic Devices","abstract":"Biologically-inspired computing models have made significant progress in recent years, but the conventional von Neumann architecture is inefficient for the large-scale matrix operations and massive parallelism required by these models. This paper presents Spin-NeuroMem, a low-power circuit design of Hopfield network for the function of associative memory. Spin-NeuroMem is equipped with energy-efficient spintronic synapses which utilize magnetic tunnel junctions (MTJs) to store weight matrices of multiple associative memories. The proposed synapse design achieves as low as 17.4% power consumption compared to the state-of-the-art synapse designs. Spin-NeuroMem also encompasses a novel voltage converter with 60% less transistor usage for effective Hopfield network computation. In addition, we propose an associative memory simulator for the first time, which achieves a 5.05Mx speedup with a comparable associative memory effect. By harnessing the potential of spintronic devices, this work sheds light on the development of energy-efficient and scalable neuromorphic computing systems. The source code will be publicly available after the manuscript is reviewed.","sentences":["Biologically-inspired computing models have made significant progress in recent years, but the conventional von Neumann architecture is inefficient for the large-scale matrix operations and massive parallelism required by these models.","This paper presents Spin-NeuroMem, a low-power circuit design of Hopfield network for the function of associative memory.","Spin-NeuroMem is equipped with energy-efficient spintronic synapses which utilize magnetic tunnel junctions (MTJs) to store weight matrices of multiple associative memories.","The proposed synapse design achieves as low as 17.4% power consumption compared to the state-of-the-art synapse designs.","Spin-NeuroMem also encompasses a novel voltage converter with 60% less transistor usage for effective Hopfield network computation.","In addition, we propose an associative memory simulator for the first time, which achieves a 5.05Mx speedup with a comparable associative memory effect.","By harnessing the potential of spintronic devices, this work sheds light on the development of energy-efficient and scalable neuromorphic computing systems.","The source code will be publicly available after the manuscript is reviewed."],"url":"http://arxiv.org/abs/2404.02463v1","category":"cs.AR"}
{"created":"2024-04-03 05:04:55","title":"A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability","abstract":"Self-supervised learning shows promise in harnessing extensive unlabeled data, but it also confronts significant privacy concerns, especially in vision. In this paper, we aim to perform membership inference on visual self-supervised models in a more realistic setting: self-supervised training method and details are unknown for an adversary when attacking as he usually faces a black-box system in practice. In this setting, considering that self-supervised model could be trained by completely different self-supervised paradigms, e.g., masked image modeling and contrastive learning, with complex training details, we propose a unified membership inference method called PartCrop. It is motivated by the shared part-aware capability among models and stronger part response on the training data. Specifically, PartCrop crops parts of objects in an image to query responses with the image in representation space. We conduct extensive attacks on self-supervised models with different training protocols and structures using three widely used image datasets. The results verify the effectiveness and generalization of PartCrop. Moreover, to defend against PartCrop, we evaluate two common approaches, i.e., early stop and differential privacy, and propose a tailored method called shrinking crop scale range. The defense experiments indicate that all of them are effective. Our code is available at https://github.com/JiePKU/PartCrop","sentences":["Self-supervised learning shows promise in harnessing extensive unlabeled data, but it also confronts significant privacy concerns, especially in vision.","In this paper, we aim to perform membership inference on visual self-supervised models in a more realistic setting: self-supervised training method and details are unknown for an adversary when attacking as he usually faces a black-box system in practice.","In this setting, considering that self-supervised model could be trained by completely different self-supervised paradigms, e.g., masked image modeling and contrastive learning, with complex training details, we propose a unified membership inference method called PartCrop.","It is motivated by the shared part-aware capability among models and stronger part response on the training data.","Specifically, PartCrop crops parts of objects in an image to query responses with the image in representation space.","We conduct extensive attacks on self-supervised models with different training protocols and structures using three widely used image datasets.","The results verify the effectiveness and generalization of PartCrop.","Moreover, to defend against PartCrop, we evaluate two common approaches, i.e., early stop and differential privacy, and propose a tailored method called shrinking crop scale range.","The defense experiments indicate that all of them are effective.","Our code is available at https://github.com/JiePKU/PartCrop"],"url":"http://arxiv.org/abs/2404.02462v1","category":"cs.CV"}
{"created":"2024-04-03 05:02:46","title":"A coarse-grained description of anharmonic lattice environments affecting the quantum dynamics of charge carriers","abstract":"Lattice softness has a significant impact on charge carrier dynamics in condensed matter systems, contributing to the emergence of various properties and functions. Examples include the remarkable carrier lifetimes and defect tolerances of hybrid organic-inorganic perovskites. Recent studies suggest the contribution of quartic anharmonicity of the lattice vibrations. The quartic anharmonicity can be discussed with a double-well potential, and the transition between the two minima can be coarse-grained as a two-state jump stochastic process. Such a stochastic approach is typically employed to describe dynamic fluctuations introduced into a system by two-state transitions in the surroundings. To investigate charge transport in materials, however, it is crucial to describe not only the fluctuations but also the dynamic lattice distortion associated with charge transport. Therefore, there is a need for a theory to describe the charge carrier dynamics proceeding alongside the lattice distortion dynamics. In this study, we present a theory that describes quantum dynamics under the influence of an environment with two stable states, termed a bistable environment. The theory describes the effects of fluctuations and dissipation induced from the bistable environment in a reasonable manner, and the effects exhibit a different temperature dependence than the widely employed Gaussian environment. The physical implication of this temperature dependence is provided in terms of the environmental dynamics. The results of this study are expected to provide a step forward in describing charge carrier dynamics in materials with lattice softness and pronounced lattice anharmonicity, e.g., hybrid organic-inorganic perovskites. Moreover, these findings represent an advancement in our understanding of and capacity to predict and control the physical properties and functions of these materials.","sentences":["Lattice softness has a significant impact on charge carrier dynamics in condensed matter systems, contributing to the emergence of various properties and functions.","Examples include the remarkable carrier lifetimes and defect tolerances of hybrid organic-inorganic perovskites.","Recent studies suggest the contribution of quartic anharmonicity of the lattice vibrations.","The quartic anharmonicity can be discussed with a double-well potential, and the transition between the two minima can be coarse-grained as a two-state jump stochastic process.","Such a stochastic approach is typically employed to describe dynamic fluctuations introduced into a system by two-state transitions in the surroundings.","To investigate charge transport in materials, however, it is crucial to describe not only the fluctuations but also the dynamic lattice distortion associated with charge transport.","Therefore, there is a need for a theory to describe the charge carrier dynamics proceeding alongside the lattice distortion dynamics.","In this study, we present a theory that describes quantum dynamics under the influence of an environment with two stable states, termed a bistable environment.","The theory describes the effects of fluctuations and dissipation induced from the bistable environment in a reasonable manner, and the effects exhibit a different temperature dependence than the widely employed Gaussian environment.","The physical implication of this temperature dependence is provided in terms of the environmental dynamics.","The results of this study are expected to provide a step forward in describing charge carrier dynamics in materials with lattice softness and pronounced lattice anharmonicity, e.g., hybrid organic-inorganic perovskites.","Moreover, these findings represent an advancement in our understanding of and capacity to predict and control the physical properties and functions of these materials."],"url":"http://arxiv.org/abs/2404.02459v1","category":"physics.chem-ph"}
{"created":"2024-04-03 05:02:10","title":"Network-Aware and Welfare-Maximizing Dynamic Pricing for Energy Sharing","abstract":"The proliferation of behind-the-meter (BTM) distributed energy resources (DER) within the electrical distribution network presents significant supply and demand flexibilities, but also introduces operational challenges such as voltage spikes and reverse power flows. In response, this paper proposes a network-aware dynamic pricing framework tailored for energy-sharing coalitions that aggregate small, but ubiquitous, BTM DER downstream of a distribution system operator's (DSO) revenue meter that adopts a generic net energy metering (NEM) tariff. By formulating a Stackelberg game between the energy-sharing market leader and its prosumers, we show that the dynamic pricing policy induces the prosumers toward a network-safe operation and decentrally maximizes the energy-sharing social welfare. The dynamic pricing mechanism involves a combination of a locational {\\em ex-ante} dynamic price and an {\\em ex-post} allocation, both of which are functions of the energy sharing's BTM DER. The {\\em ex-post} allocation is proportionate to the price differential between the DSO NEM price and the energy sharing locational price. Simulation results using real DER data and the IEEE 13-bus test systems illustrate the dynamic nature of network-aware pricing at each bus, and its impact on voltage.","sentences":["The proliferation of behind-the-meter (BTM) distributed energy resources (DER) within the electrical distribution network presents significant supply and demand flexibilities, but also introduces operational challenges such as voltage spikes and reverse power flows.","In response, this paper proposes a network-aware dynamic pricing framework tailored for energy-sharing coalitions that aggregate small, but ubiquitous, BTM DER downstream of a distribution system operator's (DSO) revenue meter that adopts a generic net energy metering (NEM) tariff.","By formulating a Stackelberg game between the energy-sharing market leader and its prosumers, we show that the dynamic pricing policy induces the prosumers toward a network-safe operation and decentrally maximizes the energy-sharing social welfare.","The dynamic pricing mechanism involves a combination of a locational {\\em ex-ante} dynamic price and an {\\em ex-post} allocation, both of which are functions of the energy sharing's BTM DER.","The {\\em ex-post} allocation is proportionate to the price differential between the DSO NEM price and the energy sharing locational price.","Simulation results using real DER data and the IEEE 13-bus test systems illustrate the dynamic nature of network-aware pricing at each bus, and its impact on voltage."],"url":"http://arxiv.org/abs/2404.02458v1","category":"eess.SY"}
{"created":"2024-04-03 04:59:28","title":"RS3Mamba: Visual State Space Model for Remote Sensing Images Semantic Segmentation","abstract":"Semantic segmentation of remote sensing images is a fundamental task in geoscience research. However, there are some significant shortcomings for the widely used convolutional neural networks (CNNs) and Transformers. The former is limited by its insufficient long-range modeling capabilities, while the latter is hampered by its computational complexity. Recently, a novel visual state space (VSS) model represented by Mamba has emerged, capable of modeling long-range relationships with linear computability. In this work, we propose a novel dual-branch network named remote sensing images semantic segmentation Mamba (RS3Mamba) to incorporate this innovative technology into remote sensing tasks. Specifically, RS3Mamba utilizes VSS blocks to construct an auxiliary branch, providing additional global information to convolution-based main branch. Moreover, considering the distinct characteristics of the two branches, we introduce a collaborative completion module (CCM) to enhance and fuse features from the dual-encoder. Experimental results on two widely used datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness and potential of the proposed RS3Mamba. To the best of our knowledge, this is the first vision Mamba specifically designed for remote sensing images semantic segmentation. The source code will be made available at https://github.com/sstary/SSRS.","sentences":["Semantic segmentation of remote sensing images is a fundamental task in geoscience research.","However, there are some significant shortcomings for the widely used convolutional neural networks (CNNs) and Transformers.","The former is limited by its insufficient long-range modeling capabilities, while the latter is hampered by its computational complexity.","Recently, a novel visual state space (VSS) model represented by Mamba has emerged, capable of modeling long-range relationships with linear computability.","In this work, we propose a novel dual-branch network named remote sensing images semantic segmentation Mamba (RS3Mamba) to incorporate this innovative technology into remote sensing tasks.","Specifically, RS3Mamba utilizes VSS blocks to construct an auxiliary branch, providing additional global information to convolution-based main branch.","Moreover, considering the distinct characteristics of the two branches, we introduce a collaborative completion module (CCM) to enhance and fuse features from the dual-encoder.","Experimental results on two widely used datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness and potential of the proposed RS3Mamba.","To the best of our knowledge, this is the first vision Mamba specifically designed for remote sensing images semantic segmentation.","The source code will be made available at https://github.com/sstary/SSRS."],"url":"http://arxiv.org/abs/2404.02457v1","category":"cs.CV"}
{"created":"2024-04-03 03:59:44","title":"Universality of Efimov states in highly mass-imbalanced cold-atom mixtures with van der Waals and dipole interactions","abstract":"We study three-body systems in a mass-imbalanced two-component cold-atom mixture, and investigate the three-body parameter of their Efimov states for both bosonic and fermionic systems, with a major focus on the Er-Er-Li Efimov states. For a system interacting solely via van der Waals interactions, the van der Waals universality of the three-body parameter is analytically derived using the quantum defect theory. With the addition of a perturbative dipole interaction between the heavy atoms, the three-body parameters of the bosonic and fermionic Efimov states are found to behave differently. When the dipole interaction is as strong as the van der Waals interaction, corresponding to realistic Er-Er-Li Efimov states, we show that the van der Waals universality persists once the effects of the non-perturbative dipole interaction are renormalized into the s-wave and p-wave scattering parameters between the heavy atoms. For a dipole interaction much stronger than the van der Waals interaction, we find that the universality of the Efimov states can be alternatively characterized by a quasi-one-dimensional scattering parameter due to a strong anisotropic deformation of the Efimov wavefunctions. Our work thus clarifies the interplay of isotropic and anisotropic forces in the universality of the Efimov states. Based on the renormalized van der Waals universality, the three-body parameter is estimated for specific isotopes of Er-Li cold-atom mixtures.","sentences":["We study three-body systems in a mass-imbalanced two-component cold-atom mixture, and investigate the three-body parameter of their Efimov states for both bosonic and fermionic systems, with a major focus on the Er-Er-Li Efimov states.","For a system interacting solely via van der Waals interactions, the van der Waals universality of the three-body parameter is analytically derived using the quantum defect theory.","With the addition of a perturbative dipole interaction between the heavy atoms, the three-body parameters of the bosonic and fermionic Efimov states are found to behave differently.","When the dipole interaction is as strong as the van der Waals interaction, corresponding to realistic Er-Er-Li Efimov states, we show that the van der Waals universality persists once the effects of the non-perturbative dipole interaction are renormalized into the s-wave and p-wave scattering parameters between the heavy atoms.","For a dipole interaction much stronger than the van der Waals interaction, we find that the universality of the Efimov states can be alternatively characterized by a quasi-one-dimensional scattering parameter due to a strong anisotropic deformation of the Efimov wavefunctions.","Our work thus clarifies the interplay of isotropic and anisotropic forces in the universality of the Efimov states.","Based on the renormalized van der Waals universality, the three-body parameter is estimated for specific isotopes of Er-Li cold-atom mixtures."],"url":"http://arxiv.org/abs/2404.02441v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-03 03:57:46","title":"A neuroergonomics model to evaluating nuclear power plants operators' performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model","abstract":"Operators experience complicated physiological and psychological states when exposed to extreme heat stress, which can impair cognitive function and decrease performance significantly, ultimately leading to severe secondary disasters. Therefore, there is an urgent need for a feasible technique to identify their abnormal states to enhance the reliability of human-cybernetics systems. With the advancement of deep learning in physiological modeling, a model for evaluating operators' performance driven by electrocardiogram (ECG) and functional near-infrared spectroscopy (fNIRS) was proposed, demonstrating high ecological validity. The model fused a convolutional neural network (CNN) backbone and a graph attention network (GAT) backbone to extract discriminative features from ECG time-frequency spectrums and fNIRS prefrontal cortex (PFC) network respectively with deeper neuroscience domain knowledge, and eventually achieved 0.90 AUC. Results supported that handcrafted features extracted by specialized neuroscience methods can alleviate overfitting. Inspired by the small-world nature of the brain network, the fNIRS PFC network was organized as an undirected graph and embedded by GAT. It is proven to perform better in information aggregation and delivery compared to a simple non-linear transformation. The model provides a potential neuroergonomics application for evaluating the human state in vital human-cybernetics systems under industry 5.0 scenarios.","sentences":["Operators experience complicated physiological and psychological states when exposed to extreme heat stress, which can impair cognitive function and decrease performance significantly, ultimately leading to severe secondary disasters.","Therefore, there is an urgent need for a feasible technique to identify their abnormal states to enhance the reliability of human-cybernetics systems.","With the advancement of deep learning in physiological modeling, a model for evaluating operators' performance driven by electrocardiogram (ECG) and functional near-infrared spectroscopy (fNIRS) was proposed, demonstrating high ecological validity.","The model fused a convolutional neural network (CNN) backbone and a graph attention network (GAT) backbone to extract discriminative features from ECG time-frequency spectrums and fNIRS prefrontal cortex (PFC) network respectively with deeper neuroscience domain knowledge, and eventually achieved 0.90 AUC.","Results supported that handcrafted features extracted by specialized neuroscience methods can alleviate overfitting.","Inspired by the small-world nature of the brain network, the fNIRS PFC network was organized as an undirected graph and embedded by GAT.","It is proven to perform better in information aggregation and delivery compared to a simple non-linear transformation.","The model provides a potential neuroergonomics application for evaluating the human state in vital human-cybernetics systems under industry 5.0 scenarios."],"url":"http://arxiv.org/abs/2404.02439v1","category":"cs.HC"}
{"created":"2024-04-03 03:53:37","title":"From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives","abstract":"In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in \"prediction-powered inference\" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our approach in handling transportability issues. multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like GPT-4-32k or a less accurate predictor like KNN. Our findings demonstrate the practical importance of inference correction for public health decision-making and suggests that if inference tasks are the end goal, having a small amount of contextually relevant, high quality labeled data is essential regardless of the NLP algorithm.","sentences":["In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD).","VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD.","Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths).","In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques.","This method, which we call multiPPI++, extends recent work in \"prediction-powered inference\" to multinomial classification.","We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our approach in handling transportability issues.","multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like GPT-4-32k or a less accurate predictor like KNN.","Our findings demonstrate the practical importance of inference correction for public health decision-making and suggests that if inference tasks are the end goal, having a small amount of contextually relevant, high quality labeled data is essential regardless of the NLP algorithm."],"url":"http://arxiv.org/abs/2404.02438v1","category":"cs.CL"}
{"created":"2024-04-03 03:41:15","title":"A fast cosine transformation accelerated method for predicting effective thermal conductivity","abstract":"Predicting effective thermal conductivity by solving a Partial Differential Equation (PDE) defined on a high-resolution Representative Volume Element (RVE) is a computationally intensive task. In this paper, we tackle the task by proposing an efficient and implementation-friendly computational method that can fully leverage the computing power offered by hardware accelerators, namely, graphical processing units (GPUs). We first employ the Two-Point Flux-Approximation scheme to discretize the PDE and then utilize the preconditioned conjugate gradient method to solve the resulting algebraic linear system. The construction of the preconditioner originates from FFT-based homogenization methods, and an engineered linear programming technique is utilized to determine the homogeneous reference parameters. The fundamental observation presented in this paper is that the preconditioner system can be effectively solved using multiple Fast Cosine Transformations (FCT) and parallel tridiagonal matrix solvers. Regarding the fact that default multiple FCTs are unavailable on the CUDA platform, we detail how to derive FCTs from FFTs with nearly optimal memory usage. Numerical experiments including the stability comparison with standard preconditioners are conducted for 3D RVEs. Our performance reports indicate that the proposed method can achieve a $5$-fold acceleration on the GPU platform over the pure CPU platform and solve the problems with $512^3$ degrees of freedom and reasonable contrast ratios in less than $30$ seconds.","sentences":["Predicting effective thermal conductivity by solving a Partial Differential Equation (PDE) defined on a high-resolution Representative Volume Element (RVE) is a computationally intensive task.","In this paper, we tackle the task by proposing an efficient and implementation-friendly computational method that can fully leverage the computing power offered by hardware accelerators, namely, graphical processing units (GPUs).","We first employ the Two-Point Flux-Approximation scheme to discretize the PDE and then utilize the preconditioned conjugate gradient method to solve the resulting algebraic linear system.","The construction of the preconditioner originates from FFT-based homogenization methods, and an engineered linear programming technique is utilized to determine the homogeneous reference parameters.","The fundamental observation presented in this paper is that the preconditioner system can be effectively solved using multiple Fast Cosine Transformations (FCT) and parallel tridiagonal matrix solvers.","Regarding the fact that default multiple FCTs are unavailable on the CUDA platform, we detail how to derive FCTs from FFTs with nearly optimal memory usage.","Numerical experiments including the stability comparison with standard preconditioners are conducted for 3D RVEs.","Our performance reports indicate that the proposed method can achieve a $5$-fold acceleration on the GPU platform over the pure CPU platform and solve the problems with $512^3","$ degrees of freedom and reasonable contrast ratios in less than $30$ seconds."],"url":"http://arxiv.org/abs/2404.02433v1","category":"math.NA"}
{"created":"2024-04-03 03:41:13","title":"GNSS Spoofing Detection by Crowdsourcing Double Differential Pseudorange Spatial Distribution","abstract":"It is widely known that spoofing is a major threat that adversely impacts the reliability and accuracy of GNSS applications. In this study, a crowdsourcing double differential pseudorange spatial (D2SP) random set is constructed and the distribution of the set is derived.Based on the variance of the D2SP set, a tri-level hypothesis detection algorithm is designed to classify spoofing-free, fully-spoofed, and partially-spoofed cases in the region of interest (ROI).It does not require the prior knowledge of the truth positions or relative distances of the receivers.Simulation test results show that the proposed D2SP spoofing detection method has the advantages of lower computational complexity and higher tolerance for multipath errors compared with the generalized likelihood ratio test (GLRT) method that is the current mainstream spoofing detection algorithm based on multiple receivers' differential pseudoranges.Moreover, it also shows better flexibility for different sizes of ROI and numbers of the crowdsourcing receivers.","sentences":["It is widely known that spoofing is a major threat that adversely impacts the reliability and accuracy of GNSS applications.","In this study, a crowdsourcing double differential pseudorange spatial (D2SP) random set is constructed and the distribution of the set is derived.","Based on the variance of the D2SP set, a tri-level hypothesis detection algorithm is designed to classify spoofing-free, fully-spoofed, and partially-spoofed cases in the region of interest (ROI).It does not require the prior knowledge of the truth positions or relative distances of the receivers.","Simulation test results show that the proposed D2SP spoofing detection method has the advantages of lower computational complexity and higher tolerance for multipath errors compared with the generalized likelihood ratio test (GLRT) method that is the current mainstream spoofing detection algorithm based on multiple receivers' differential pseudoranges.","Moreover, it also shows better flexibility for different sizes of ROI and numbers of the crowdsourcing receivers."],"url":"http://arxiv.org/abs/2404.02432v1","category":"eess.SP"}
{"created":"2024-04-03 03:35:01","title":"Ab initio exploration of short-pitch skyrmion materials: Role of orbital frustration","abstract":"In recent years, the skyrmion lattice phase with a short lattice constant has attracted attention due to its high skyrmion density, making it a promising option for achieving high-density storage memory and for observing novel phenomena like the quantized topological Hall effect. Unlike conventional non-centrosymmetric systems where the Dzyaloshinsky-Moriya interaction plays a crucial role, the short pitch skyrmion phase requires a quadratic magnetic interaction $J(q)$ with a peak at finite-$Q$, and weak easy-axis magnetic anisotropy is also critical. Thus, conducting first-principles evaluations is essential for understanding the formation mechanism as well as for promoting the discovery of new skyrmion materials. In this {\\it Perspective}, we focus on recent developments of the first-principles evaluations of these properties and apply them to the prototype systems Gd$T_2X_2$ and Eu$T_2X_2$, where $T$ denotes a transition metal and $X$ represents Si or Ge. In particular, based on the spin density functional theory with the Hubbard correction combined with the Liechtenstein method in the Wannier tight-binding model formalism, we first show that the Hubbard $U$ and Hund's coupling is essential to stabilize a skyrmion lattice state by enhancing the easy-axis anisotropy. We then discuss mechanisms of finite-$Q$ instability and show that competition among Gd-5$d$ orbitals determines whether ferromagnetism or a finite-$Q$ structure is favored in Gd$T_2$Si$_2$ with $T=$ Fe and Ru. Our systematic calculations reveal that GdRu$_2$$X_2$, GdOs$_2$$X_2$, and GdRe$_2X_2$ are promising, while GdAg$_2X_2$, GdAu$_2X_2$, and EuAg$_2X_2$ are possible candidates as the skyrmion host materials. Analysis based on a spin spiral calculation for the candidate materials is also presented.","sentences":["In recent years, the skyrmion lattice phase with a short lattice constant has attracted attention due to its high skyrmion density, making it a promising option for achieving high-density storage memory and for observing novel phenomena like the quantized topological Hall effect.","Unlike conventional non-centrosymmetric systems where the Dzyaloshinsky-Moriya interaction plays a crucial role, the short pitch skyrmion phase requires a quadratic magnetic interaction $J(q)$ with a peak at finite-$Q$, and weak easy-axis magnetic anisotropy is also critical.","Thus, conducting first-principles evaluations is essential for understanding the formation mechanism as well as for promoting the discovery of new skyrmion materials.","In this {\\it Perspective}, we focus on recent developments of the first-principles evaluations of these properties and apply them to the prototype systems Gd$T_2X_2$ and Eu$T_2X_2$, where $T$ denotes a transition metal and $X$ represents Si or Ge.","In particular, based on the spin density functional theory with the Hubbard correction combined with the Liechtenstein method in the Wannier tight-binding model formalism, we first show that the Hubbard $U$ and Hund's coupling is essential to stabilize a skyrmion lattice state by enhancing the easy-axis anisotropy.","We then discuss mechanisms of finite-$Q$ instability and show that competition among Gd-5$d$ orbitals determines whether ferromagnetism or a finite-$Q$ structure is favored in Gd$T_2$Si$_2$ with $T=$ Fe and Ru.","Our systematic calculations reveal that GdRu$_2$$X_2$, GdOs$_2$$X_2$, and GdRe$_2X_2$ are promising, while GdAg$_2X_2$, GdAu$_2X_2$, and EuAg$_2X_2$ are possible candidates as the skyrmion host materials.","Analysis based on a spin spiral calculation for the candidate materials is also presented."],"url":"http://arxiv.org/abs/2404.02428v1","category":"cond-mat.str-el"}
{"created":"2024-04-03 03:33:14","title":"Novel_Authentication_Protocols_Tailored_for_Ambient_IoT_Devices_in_3GPP_5G_Networks","abstract":"AIoT devices have attracted significant attention within the 3GPP organization. These devices, distinguished from conventional IoT devices, do not rely on additional batteries or have extremely small battery capacities, offering features such as low cost, easy deployment, and maintenance-free operation. Authentication and secure transmission are fundamental security requirements for AIoT devices. However, existing standard security mechanisms are not specifically designed for AIoT devices due to their complex key hierarchies and multi-round interactions, making them unsuitable. Besides, AIoT devices would have more various communication topologies. Therefore, we propose dedicated ultra-lightweight access authentication protocols based on various technologies and algorithms to serve as a forward-looking reference for future research and standardization. Analysis and simulation experiments using chips that closely resemble real AIoT devices, demonstrate that the existing standard protocols are indeed not suitable for such devices, and our protocols outperform existing standard protocols in terms of computational time and energy consumption. After the successful execution of proposed protocols, they can achieve secure transmission of application data, striking a balance between performance and security.","sentences":["AIoT devices have attracted significant attention within the 3GPP organization.","These devices, distinguished from conventional IoT devices, do not rely on additional batteries or have extremely small battery capacities, offering features such as low cost, easy deployment, and maintenance-free operation.","Authentication and secure transmission are fundamental security requirements for AIoT devices.","However, existing standard security mechanisms are not specifically designed for AIoT devices due to their complex key hierarchies and multi-round interactions, making them unsuitable.","Besides, AIoT devices would have more various communication topologies.","Therefore, we propose dedicated ultra-lightweight access authentication protocols based on various technologies and algorithms to serve as a forward-looking reference for future research and standardization.","Analysis and simulation experiments using chips that closely resemble real AIoT devices, demonstrate that the existing standard protocols are indeed not suitable for such devices, and our protocols outperform existing standard protocols in terms of computational time and energy consumption.","After the successful execution of proposed protocols, they can achieve secure transmission of application data, striking a balance between performance and security."],"url":"http://arxiv.org/abs/2404.02425v1","category":"cs.CR"}
{"created":"2024-04-03 02:57:33","title":"The first CCD photometric studies of the member eclipsing binary ZTFJ015003.88+534734.1 in the newly discovered young open cluster UBC 188","abstract":"We present the first CCD observations of an eclipsing binary, ZTFJ015003.88+534734.1, which is a member in the open star cluster UBC 188. The observations were taken by the 1.88 m telescope at the Kottamia Astronomical Observatory (KAO) in SDSS griz bands. The latest version of the Wilson- Devinney (W-D) code was employed for photometric analysis and light curve modeling of the eclipsing binary. The results indicate that the binary system is in an over-contact configuration. The mass of the primary star (M1) is determined to be 1.293 Msun, and the mass of the secondary star (M2) is directly derived from the system's estimated mass ratio (q= M2/M1) as 0.340 times the solar mass (Msun). We investigated the color-magnitude diagram and the membership probability of the open cluster UBC 188 using the Gaia DR3 data. We determined the membership probability of the eclipsing binary ZTFJ015003.88+534734.1 using the pyUPMASK algorithm and found that its membership probability is one.","sentences":["We present the first CCD observations of an eclipsing binary, ZTFJ015003.88+534734.1, which is a member in the open star cluster UBC 188.","The observations were taken by the 1.88 m telescope at the Kottamia Astronomical Observatory (KAO) in SDSS griz bands.","The latest version of the Wilson- Devinney (W-D) code was employed for photometric analysis and light curve modeling of the eclipsing binary.","The results indicate that the binary system is in an over-contact configuration.","The mass of the primary star (M1) is determined to be 1.293 Msun, and the mass of the secondary star (M2) is directly derived from the system's estimated mass ratio (q= M2/M1) as 0.340 times the solar mass (Msun).","We investigated the color-magnitude diagram and the membership probability of the open cluster UBC 188 using the Gaia DR3 data.","We determined the membership probability of the eclipsing binary ZTFJ015003.88+534734.1 using the pyUPMASK algorithm and found that its membership probability is one."],"url":"http://arxiv.org/abs/2404.02419v1","category":"astro-ph.SR"}
{"created":"2024-04-03 02:51:10","title":"One-loop contributions for $A^0 \\rightarrow \\ell \\bar{\\ell} V$ with $\\ell \\equiv e, \u03bc$ and $V\\equiv \u03b3, Z$ in Higgs Extensions of the Standard Model","abstract":"We present one-loop formulas for the decay of CP-odd Higgs $A^0 \\rightarrow \\ell \\bar{\\ell} V$ with $\\ell \\equiv e, \\mu$ and $V\\equiv \\gamma, Z$ in Higgs Extensions of the Standard Model, considering two higgs doublet model with a complex (and real) scalar, two higgs doublet model as well as triplet higgs model. Analytic results for one-loop amplitudes are expressed in terms of Passarino-Veltman functions following the standard notations of {\\tt LoopTools}. As a result, physical results can be generated numerically by using the package. In phenomenological results, the total decay widths and the differential decay rates with respect to the invariant mass of lepton pair are analyzed for two typical models such as two higgs doublet model and triplet higgs model.","sentences":["We present one-loop formulas for the decay of CP-odd Higgs $A^0 \\rightarrow \\ell \\bar{\\ell} V$ with $\\ell \\equiv e, \\mu$ and $V\\equiv \\gamma, Z$ in Higgs Extensions of the Standard Model, considering two higgs doublet model with a complex (and real) scalar, two higgs doublet model as well as triplet higgs model.","Analytic results for one-loop amplitudes are expressed in terms of Passarino-Veltman functions following the standard notations of {\\tt LoopTools}.","As a result, physical results can be generated numerically by using the package.","In phenomenological results, the total decay widths and the differential decay rates with respect to the invariant mass of lepton pair are analyzed for two typical models such as two higgs doublet model and triplet higgs model."],"url":"http://arxiv.org/abs/2404.02417v1","category":"hep-ph"}
{"created":"2024-04-03 02:45:56","title":"Revealing kinetically tuned atomic pathways for interfacial strain relaxation","abstract":"Strain at interfaces may profoundly impact the microstructure and properties of materials; thus, it is a major consideration when designing and engineering materials. Dislocation formation is a commonly known mechanism to release mismatch strain at solid-solid interfaces. However, it is still unclear about how materials accommodate interfacial strain under drastically accelerated structural transformation kinetics, since it is extremely challenging to directly observe the atomic structure evolution of fast-propagating interfaces. Utilizing liquid phase transmission electron microscopy (TEM), we have achieved atomic-scale imaging of hydrogen-induced phase transformations of palladium nanocrystals with different transformation speeds. Our observation reveals that the fast phase transformation occurs with an expanded interface of mixed $\\alpha$- and $\\beta$-$\\mathrm{PdH}_x$ phases, and tilting of (020) planes to accommodate mismatch strain. In contrast, slow phase transformations lead to sharp interfaces with slipping misfit dislocations. Our kinetic Monte Carlo simulations show that fast phase transformation pushes the system far-from-equilibrium, generically roughening the interface; however, a smooth boundary minimizes strain near-equilibrium. Unveiling the atomic pathways of transformations from near-equilibrium to far-from-equilibrium, which was previously possible only computationally, this work holds significant implications for engineering microstructure of materials through modulating solid-solid transformations in a wide range of kinetics.","sentences":["Strain at interfaces may profoundly impact the microstructure and properties of materials; thus, it is a major consideration when designing and engineering materials.","Dislocation formation is a commonly known mechanism to release mismatch strain at solid-solid interfaces.","However, it is still unclear about how materials accommodate interfacial strain under drastically accelerated structural transformation kinetics, since it is extremely challenging to directly observe the atomic structure evolution of fast-propagating interfaces.","Utilizing liquid phase transmission electron microscopy (TEM), we have achieved atomic-scale imaging of hydrogen-induced phase transformations of palladium nanocrystals with different transformation speeds.","Our observation reveals that the fast phase transformation occurs with an expanded interface of mixed $\\alpha$- and $\\beta$-$\\mathrm{PdH}_x$ phases, and tilting of (020) planes to accommodate mismatch strain.","In contrast, slow phase transformations lead to sharp interfaces with slipping misfit dislocations.","Our kinetic Monte Carlo simulations show that fast phase transformation pushes the system far-from-equilibrium, generically roughening the interface; however, a smooth boundary minimizes strain near-equilibrium.","Unveiling the atomic pathways of transformations from near-equilibrium to far-from-equilibrium, which was previously possible only computationally, this work holds significant implications for engineering microstructure of materials through modulating solid-solid transformations in a wide range of kinetics."],"url":"http://arxiv.org/abs/2404.02416v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 02:38:49","title":"A simple lower bound for the complexity of estimating partition functions on a quantum computer","abstract":"We study the complexity of estimating the partition function ${\\mathsf{Z}}(\\beta)=\\sum_{x\\in\\chi} e^{-\\beta H(x)}$ for a Gibbs distribution characterized by the Hamiltonian $H(x)$. We provide a simple and natural lower bound for quantum algorithms that solve this task by relying on reflections through the coherent encoding of Gibbs states. Our primary contribution is a $\\Omega(1/\\epsilon)$ lower bound for the number of reflections needed to estimate the partition function with a quantum algorithm. We also prove a $\\Omega(1/\\epsilon^2)$ query lower bound for classical algorithms. The proofs are based on a reduction from the problem of estimating the Hamming weight of an unknown binary string.","sentences":["We study the complexity of estimating the partition function ${\\mathsf{Z}}(\\beta)=\\sum_{x\\in\\chi} e^{-\\beta H(x)}$ for a Gibbs distribution characterized by the Hamiltonian $H(x)$. We provide a simple and natural lower bound for quantum algorithms that solve this task by relying on reflections through the coherent encoding of Gibbs states.","Our primary contribution is a $\\Omega(1/\\epsilon)$ lower bound for the number of reflections needed to estimate the partition function with a quantum algorithm.","We also prove a $\\Omega(1/\\epsilon^2)$ query lower bound for classical algorithms.","The proofs are based on a reduction from the problem of estimating the Hamming weight of an unknown binary string."],"url":"http://arxiv.org/abs/2404.02414v1","category":"quant-ph"}
{"created":"2024-04-03 02:14:15","title":"Effect of the Source toSubstrate Distance on Structural, Optoelectronic, and Thermoelectric Properties of Zinc Sulfide Thin Films","abstract":"Zinc sulfide ZnS thin films with variable structural, optical, electrical, and thermoelectric properties were obtained by changing the source to substrate SSD distance in the physical vaporthermal coating PVTC system. The films crystallized into a zinc blede cubic structure with 111 preferred orientation.","sentences":["Zinc sulfide ZnS thin films with variable structural, optical, electrical, and thermoelectric properties were obtained by changing the source to substrate SSD distance in the physical vaporthermal coating PVTC system.","The films crystallized into a zinc blede cubic structure with 111 preferred orientation."],"url":"http://arxiv.org/abs/2404.02404v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 02:06:50","title":"Extended Wannier-Stark ladder and particle-pair Bloch oscillations in dimerized non-Hermitian systems","abstract":"In the Hermitian regime, the Wannier-Stark ladder characterizes the eigenstates of an electron in a periodic potential with an applied static electric field. In this work, we extend this concept to the complex regime for a periodic non-Hermitian system under a linear potential. We show that although the energy levels can be complex, they are still equally spaced by a real Bloch frequency. This ensures single-particle Bloch oscillations with a damping (or growing) rate. The system can also support standard two-particle Bloch oscillations under certain conditions. We propose two types of dimerized non-Hermitian systems to demonstrate our results. In addition, we also propose a scheme to demonstrate the results of electron-pair dynamics in a single-particle 2D $\\mathcal{PT}$-symmetric square lattice.","sentences":["In the Hermitian regime, the Wannier-Stark ladder characterizes the eigenstates of an electron in a periodic potential with an applied static electric field.","In this work, we extend this concept to the complex regime for a periodic non-Hermitian system under a linear potential.","We show that although the energy levels can be complex, they are still equally spaced by a real Bloch frequency.","This ensures single-particle Bloch oscillations with a damping (or growing) rate.","The system can also support standard two-particle Bloch oscillations under certain conditions.","We propose two types of dimerized non-Hermitian systems to demonstrate our results.","In addition, we also propose a scheme to demonstrate the results of electron-pair dynamics in a single-particle 2D $\\mathcal{PT}$-symmetric square lattice."],"url":"http://arxiv.org/abs/2404.02399v1","category":"quant-ph"}
{"created":"2024-04-03 01:42:30","title":"Optimal Batch Allocation for Wireless Federated Learning","abstract":"Federated learning aims to construct a global model that fits the dataset distributed across local devices without direct access to private data, leveraging communication between a server and the local devices. In the context of a practical communication scheme, we study the completion time required to achieve a target performance. Specifically, we analyze the number of iterations required for federated learning to reach a specific optimality gap from a minimum global loss. Subsequently, we characterize the time required for each iteration under two fundamental multiple access schemes: time-division multiple access (TDMA) and random access (RA). We propose a step-wise batch allocation, demonstrated to be optimal for TDMA-based federated learning systems. Additionally, we show that the non-zero batch gap between devices provided by the proposed step-wise batch allocation significantly reduces the completion time for RA-based learning systems. Numerical evaluations validate these analytical results through real-data experiments, highlighting the remarkable potential for substantial completion time reduction.","sentences":["Federated learning aims to construct a global model that fits the dataset distributed across local devices without direct access to private data, leveraging communication between a server and the local devices.","In the context of a practical communication scheme, we study the completion time required to achieve a target performance.","Specifically, we analyze the number of iterations required for federated learning to reach a specific optimality gap from a minimum global loss.","Subsequently, we characterize the time required for each iteration under two fundamental multiple access schemes: time-division multiple access (TDMA) and random access (RA).","We propose a step-wise batch allocation, demonstrated to be optimal for TDMA-based federated learning systems.","Additionally, we show that the non-zero batch gap between devices provided by the proposed step-wise batch allocation significantly reduces the completion time for RA-based learning systems.","Numerical evaluations validate these analytical results through real-data experiments, highlighting the remarkable potential for substantial completion time reduction."],"url":"http://arxiv.org/abs/2404.02395v1","category":"cs.LG"}
{"created":"2024-04-03 01:32:31","title":"Backdoor Attack on Multilingual Machine Translation","abstract":"While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages. Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.","sentences":["While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities.","Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages.","Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs.","This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings.","Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages."],"url":"http://arxiv.org/abs/2404.02393v1","category":"cs.CL"}
{"created":"2024-04-03 01:31:41","title":"Low-resource neural machine translation with morphological modeling","abstract":"Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated and shown to increase translation performance in low-resource settings. We evaluate our proposed solution on Kinyarwanda - English translation using public-domain parallel text. Our final models achieve competitive performance in relation to large multi-lingual models. We hope that our results will motivate more use of explicit morphological information and the proposed model and data augmentations in low-resource NMT.","sentences":["Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages.","However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words.","In this work, we propose a framework-solution for modeling complex morphology in low-resource settings.","A two-tier transformer architecture is chosen to encode morphological information at the inputs.","At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance.","An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages.","Several data augmentation techniques are evaluated and shown to increase translation performance in low-resource settings.","We evaluate our proposed solution on Kinyarwanda - English translation using public-domain parallel text.","Our final models achieve competitive performance in relation to large multi-lingual models.","We hope that our results will motivate more use of explicit morphological information and the proposed model and data augmentations in low-resource NMT."],"url":"http://arxiv.org/abs/2404.02392v1","category":"cs.CL"}
{"created":"2024-04-03 01:29:30","title":"APC2Mesh: Bridging the gap from occluded building fa\u00e7ades to full 3D models","abstract":"The benefits of having digital twins of urban buildings are numerous. However, a major difficulty encountered in their creation from airborne LiDAR point clouds is the effective means of accurately reconstructing significant occlusions amidst point density variations and noise. To bridge the noise/sparsity/occlusion gap and generate high fidelity 3D building models, we propose APC2Mesh which integrates point completion into a 3D reconstruction pipeline, enabling the learning of dense geometrically accurate representation of buildings. Specifically, we leveraged complete points generated from occluded ones as input to a linearized skip attention-based deformation network for 3D mesh reconstruction. In our experiments, conducted on 3 different scenes, we demonstrate that: (1) APC2Mesh delivers comparatively superior results, indicating its efficacy in handling the challenges of occluded airborne building points of diverse styles and complexities. (2) The combination of point completion with typical deep learning-based 3D point cloud reconstruction methods offers a direct and effective solution for reconstructing significantly occluded airborne building points. As such, this neural integration holds promise for advancing the creation of digital twins for urban buildings with greater accuracy and fidelity.","sentences":["The benefits of having digital twins of urban buildings are numerous.","However, a major difficulty encountered in their creation from airborne LiDAR point clouds is the effective means of accurately reconstructing significant occlusions amidst point density variations and noise.","To bridge the noise/sparsity/occlusion gap and generate high fidelity 3D building models, we propose APC2Mesh which integrates point completion into a 3D reconstruction pipeline, enabling the learning of dense geometrically accurate representation of buildings.","Specifically, we leveraged complete points generated from occluded ones as input to a linearized skip attention-based deformation network for 3D mesh reconstruction.","In our experiments, conducted on 3 different scenes, we demonstrate that: (1) APC2Mesh delivers comparatively superior results, indicating its efficacy in handling the challenges of occluded airborne building points of diverse styles and complexities.","(2) The combination of point completion with typical deep learning-based 3D point cloud reconstruction methods offers a direct and effective solution for reconstructing significantly occluded airborne building points.","As such, this neural integration holds promise for advancing the creation of digital twins for urban buildings with greater accuracy and fidelity."],"url":"http://arxiv.org/abs/2404.02391v1","category":"cs.CV"}
{"created":"2024-04-03 01:13:05","title":"CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation","abstract":"Deep Neural Networks (DNNs) are widely used for visual classification tasks, but their complex computation process and black-box nature hinder decision transparency and interpretability. Class activation maps (CAMs) and recent variants provide ways to visually explain the DNN decision-making process by displaying 'attention' heatmaps of the DNNs. Nevertheless, the CAM explanation only offers relative attention information, that is, on an attention heatmap, we can interpret which image region is more or less important than the others. However, these regions cannot be meaningfully compared across classes, and the contribution of each region to the model's class prediction is not revealed. To address these challenges that ultimately lead to better DNN Interpretation, in this paper, we propose CAPE, a novel reformulation of CAM that provides a unified and probabilistically meaningful assessment of the contributions of image regions. We quantitatively and qualitatively compare CAPE with state-of-the-art CAM methods on CUB and ImageNet benchmark datasets to demonstrate enhanced interpretability. We also test on a cytology imaging dataset depicting a challenging Chronic Myelomonocytic Leukemia (CMML) diagnosis problem. Code is available at: https://github.com/AIML-MED/CAPE.","sentences":["Deep Neural Networks (DNNs) are widely used for visual classification tasks, but their complex computation process and black-box nature hinder decision transparency and interpretability.","Class activation maps (CAMs) and recent variants provide ways to visually explain the DNN decision-making process by displaying 'attention' heatmaps of the DNNs.","Nevertheless, the CAM explanation only offers relative attention information, that is, on an attention heatmap, we can interpret which image region is more or less important than the others.","However, these regions cannot be meaningfully compared across classes, and the contribution of each region to the model's class prediction is not revealed.","To address these challenges that ultimately lead to better DNN Interpretation, in this paper, we propose CAPE, a novel reformulation of CAM that provides a unified and probabilistically meaningful assessment of the contributions of image regions.","We quantitatively and qualitatively compare CAPE with state-of-the-art CAM methods on CUB and ImageNet benchmark datasets to demonstrate enhanced interpretability.","We also test on a cytology imaging dataset depicting a challenging Chronic Myelomonocytic Leukemia (CMML) diagnosis problem.","Code is available at: https://github.com/AIML-MED/CAPE."],"url":"http://arxiv.org/abs/2404.02388v2","category":"cs.CV"}
{"created":"2024-04-03 00:58:26","title":"Inline AI: Open-source Deep Learning Inference for Cardiac MR","abstract":"Cardiac Magnetic Resonance (CMR) is established as a non-invasive imaging technique for evaluation of heart function, anatomy, and myocardial tissue characterization. Quantitative biomarkers are central for diagnosis and management of heart disease. Deep learning (DL) is playing an ever more important role in extracting these quantitative measures from CMR images. While many researchers have reported promising results in training and evaluating models, model deployment into the imaging workflow is less explored.   A new imaging AI framework, the InlineAI, was developed and open-sourced. The main innovation is to enable the model inference inline as a part of imaging computation, instead of as an offline post-processing step and to allow users to plug in their models. We demonstrate the system capability on three applications: long-axis CMR cine landmark detection, short-axis CMR cine analysis of function and anatomy, and quantitative perfusion mapping.   The InlineAI allowed models to be deployed into imaging workflow in a streaming manner directly on the scanner. The model was loaded and inference on incoming images were performed while the data acquisition was ongoing, and results were sent back to scanner. Several biomarkers were extracted from model outputs in the demonstrated applications and reported as curves and tabular values. All processes are full automated. the model inference was completed within 6-45s after the end of imaging data acquisition.","sentences":["Cardiac Magnetic Resonance (CMR) is established as a non-invasive imaging technique for evaluation of heart function, anatomy, and myocardial tissue characterization.","Quantitative biomarkers are central for diagnosis and management of heart disease.","Deep learning (DL) is playing an ever more important role in extracting these quantitative measures from CMR images.","While many researchers have reported promising results in training and evaluating models, model deployment into the imaging workflow is less explored.   ","A new imaging AI framework, the InlineAI, was developed and open-sourced.","The main innovation is to enable the model inference inline as a part of imaging computation, instead of as an offline post-processing step and to allow users to plug in their models.","We demonstrate the system capability on three applications: long-axis CMR cine landmark detection, short-axis CMR cine analysis of function and anatomy, and quantitative perfusion mapping.   ","The InlineAI allowed models to be deployed into imaging workflow in a streaming manner directly on the scanner.","The model was loaded and inference on incoming images were performed while the data acquisition was ongoing, and results were sent back to scanner.","Several biomarkers were extracted from model outputs in the demonstrated applications and reported as curves and tabular values.","All processes are full automated.","the model inference was completed within 6-45s after the end of imaging data acquisition."],"url":"http://arxiv.org/abs/2404.02384v1","category":"eess.IV"}
{"created":"2024-04-03 00:55:12","title":"Performance Analysis and ISI Mitigation with Imperfect Transmitter in Molecular Communication","abstract":"In molecular communication (MC), molecules are released from the transmitter to convey information. This paper considers a realistic molecule shift keying (MoSK) scenario with two species of molecule in two reservoirs, where the molecules are harvested from the environment and placed into different reservoirs, which are purified by exchanging molecules between the reservoirs. This process consumes energy, and for a reasonable energy cost, the reservoirs cannot be pure; thus, our MoSK transmitter is imperfect, releasing mixtures of both molecules for every symbol, resulting in inter-symbol interference (ISI). To mitigate ISI, the properties of the receiver are analyzed and a detection method based on the ratio of different molecules is proposed. Theoretical and simulation results are provided, showing that with the increase of energy cost, the system achieves better performance. The good performance of the proposed detection scheme is also demonstrated.","sentences":["In molecular communication (MC), molecules are released from the transmitter to convey information.","This paper considers a realistic molecule shift keying (MoSK) scenario with two species of molecule in two reservoirs, where the molecules are harvested from the environment and placed into different reservoirs, which are purified by exchanging molecules between the reservoirs.","This process consumes energy, and for a reasonable energy cost, the reservoirs cannot be pure; thus, our MoSK transmitter is imperfect, releasing mixtures of both molecules for every symbol, resulting in inter-symbol interference (ISI).","To mitigate ISI, the properties of the receiver are analyzed and a detection method based on the ratio of different molecules is proposed.","Theoretical and simulation results are provided, showing that with the increase of energy cost, the system achieves better performance.","The good performance of the proposed detection scheme is also demonstrated."],"url":"http://arxiv.org/abs/2404.02383v1","category":"cs.IT"}
{"created":"2024-04-03 00:51:07","title":"Imaging transformer for MRI denoising with the SNR unit training: enabling generalization across field-strengths, imaging contrasts, and anatomy","abstract":"The ability to recover MRI signal from noise is key to achieve fast acquisition, accurate quantification, and high image quality. Past work has shown convolutional neural networks can be used with abundant and paired low and high-SNR images for training. However, for applications where high-SNR data is difficult to produce at scale (e.g. with aggressive acceleration, high resolution, or low field strength), training a new denoising network using a large quantity of high-SNR images can be infeasible.   In this study, we overcome this limitation by improving the generalization of denoising models, enabling application to many settings beyond what appears in the training data. Specifically, we a) develop a training scheme that uses complex MRIs reconstructed in the SNR units (i.e., the images have a fixed noise level, SNR unit training) and augments images with realistic noise based on coil g-factor, and b) develop a novel imaging transformer (imformer) to handle 2D, 2D+T, and 3D MRIs in one model architecture. Through empirical evaluation, we show this combination improves performance compared to CNN models and improves generalization, enabling a denoising model to be used across field-strengths, image contrasts, and anatomy.","sentences":["The ability to recover MRI signal from noise is key to achieve fast acquisition, accurate quantification, and high image quality.","Past work has shown convolutional neural networks can be used with abundant and paired low and high-SNR images for training.","However, for applications where high-SNR data is difficult to produce at scale (e.g. with aggressive acceleration, high resolution, or low field strength), training a new denoising network using a large quantity of high-SNR images can be infeasible.   ","In this study, we overcome this limitation by improving the generalization of denoising models, enabling application to many settings beyond what appears in the training data.","Specifically, we a) develop a training scheme that uses complex MRIs reconstructed in the SNR units (i.e., the images have a fixed noise level, SNR unit training) and augments images with realistic noise based on coil g-factor, and b) develop a novel imaging transformer (imformer) to handle 2D, 2D+T, and 3D MRIs in one model architecture.","Through empirical evaluation, we show this combination improves performance compared to CNN models and improves generalization, enabling a denoising model to be used across field-strengths, image contrasts, and anatomy."],"url":"http://arxiv.org/abs/2404.02382v1","category":"eess.IV"}
{"created":"2024-04-03 00:46:08","title":"Balian-Bloch Wave Invariants for Nearly Degenerate Orbits","abstract":"This paper is part I of a series in which we aim to show that the singular support of the wave trace and the length spectrum of a smooth, strictly convex, and bounded planar billiard table are generally distinct objects. We derive an asymptotic trace formula for the regularized resolvent which is dual to the wave trace and contains the same information. To do this, we consider a class of periodic orbits which have nearly degenerate Poincar\\'e maps and study their leading order behavior as the deformation parameter goes to zero, generating large coefficients in the wave trace. We also keep careful track of the Maslov indices, which will allow us to match contributions of opposite signs in our subsequent paper. Each cancellation of coefficients in the resolvent trace corresponds to making the wave trace one degree smoother. The resolvent based approach is due to Balian and Bloch and was significantly expanded upon by Zelditch in a foundational series of papers [Zel09], [Zel04a], [Zel04c] and [Zel00].","sentences":["This paper is part I of a series in which we aim to show that the singular support of the wave trace and the length spectrum of a smooth, strictly convex, and bounded planar billiard table are generally distinct objects.","We derive an asymptotic trace formula for the regularized resolvent which is dual to the wave trace and contains the same information.","To do this, we consider a class of periodic orbits which have nearly degenerate Poincar\\'e maps and study their leading order behavior as the deformation parameter goes to zero, generating large coefficients in the wave trace.","We also keep careful track of the Maslov indices, which will allow us to match contributions of opposite signs in our subsequent paper.","Each cancellation of coefficients in the resolvent trace corresponds to making the wave trace one degree smoother.","The resolvent based approach is due to Balian and Bloch and was significantly expanded upon by Zelditch in a foundational series of papers","[Zel09], [Zel04a], [Zel04c] and [Zel00]."],"url":"http://arxiv.org/abs/2404.02381v1","category":"math.SP"}
{"created":"2024-04-03 00:45:51","title":"A spectral investigation of criticality and crossover effects in two and three dimensions: Short timescales with small systems in minute random matrices","abstract":"Random matrix theory, particularly using matrices akin to the Wishart ensemble, has proven successful in elucidating the thermodynamic characteristics of critical behavior in spin systems across varying interaction ranges. This paper explores the applicability of such methods in investigating critical phenomena and the crossover to tricritical points within the Blume-Capel model. Through an analysis of eigenvalue mean, dispersion, and extrema statistics, we demonstrate the efficacy of these spectral techniques in characterizing critical points in both two and three dimensions. Crucially, we propose a significant modification to this spectral approach, which emerges as a versatile tool for studying critical phenomena. Unlike traditional methods that eschew diagonalization, our method excels in handling short timescales and small system sizes, widening the scope of inquiry into critical behavior.","sentences":["Random matrix theory, particularly using matrices akin to the Wishart ensemble, has proven successful in elucidating the thermodynamic characteristics of critical behavior in spin systems across varying interaction ranges.","This paper explores the applicability of such methods in investigating critical phenomena and the crossover to tricritical points within the Blume-Capel model.","Through an analysis of eigenvalue mean, dispersion, and extrema statistics, we demonstrate the efficacy of these spectral techniques in characterizing critical points in both two and three dimensions.","Crucially, we propose a significant modification to this spectral approach, which emerges as a versatile tool for studying critical phenomena.","Unlike traditional methods that eschew diagonalization, our method excels in handling short timescales and small system sizes, widening the scope of inquiry into critical behavior."],"url":"http://arxiv.org/abs/2404.02380v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 00:19:36","title":"Detection and Mitigation of Cyberattacks on Volt-Var Control","abstract":"Cyberattacks are becoming more frequent, and attackers can use different mechanisms, such as denial of service (DoS) and false data injection (FDI). Furthermore, multiple attack types can be launched simultaneously, known as hybrid attacks, to cause more damage. Volt-Var control algorithms are widely used in the distribution system to maintain the voltage within a nominal range. This work uses an artificial neural network (ANN)-based method to detect and mitigate hybrid cyberattacks on the Volt-VAr control algorithm.","sentences":["Cyberattacks are becoming more frequent, and attackers can use different mechanisms, such as denial of service (DoS) and false data injection (FDI).","Furthermore, multiple attack types can be launched simultaneously, known as hybrid attacks, to cause more damage.","Volt-Var control algorithms are widely used in the distribution system to maintain the voltage within a nominal range.","This work uses an artificial neural network (ANN)-based method to detect and mitigate hybrid cyberattacks on the Volt-VAr control algorithm."],"url":"http://arxiv.org/abs/2404.02374v1","category":"eess.SY"}
{"created":"2024-04-03 00:13:23","title":"Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis","abstract":"In the era of the internet and smart devices, the detection of malware has become crucial for system security. Malware authors increasingly employ obfuscation techniques to evade advanced security solutions, making it challenging to detect and eliminate threats. Obfuscated malware, adept at hiding itself, poses a significant risk to various platforms, including computers, mobile devices, and IoT devices. Conventional methods like heuristic-based or signature-based systems struggle against this type of malware, as it leaves no discernible traces on the system. In this research, we propose a simple and cost-effective obfuscated malware detection system through memory dump analysis, utilizing diverse machine-learning algorithms. The study focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world scenarios and assess memory-based obfuscated malware detection. We evaluate the effectiveness of machine learning algorithms, such as decision trees, ensemble methods, and neural networks, in detecting obfuscated malware within memory dumps. Our analysis spans multiple malware categories, providing insights into algorithmic strengths and limitations. By offering a comprehensive assessment of machine learning algorithms for obfuscated malware detection through memory analysis, this paper contributes to ongoing efforts to enhance cybersecurity and fortify digital ecosystems against evolving and sophisticated malware threats. The source code is made open-access for reproducibility and future research endeavours. It can be accessed at https://bit.ly/MalMemCode.","sentences":["In the era of the internet and smart devices, the detection of malware has become crucial for system security.","Malware authors increasingly employ obfuscation techniques to evade advanced security solutions, making it challenging to detect and eliminate threats.","Obfuscated malware, adept at hiding itself, poses a significant risk to various platforms, including computers, mobile devices, and IoT devices.","Conventional methods like heuristic-based or signature-based systems struggle against this type of malware, as it leaves no discernible traces on the system.","In this research, we propose a simple and cost-effective obfuscated malware detection system through memory dump analysis, utilizing diverse machine-learning algorithms.","The study focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world scenarios and assess memory-based obfuscated malware detection.","We evaluate the effectiveness of machine learning algorithms, such as decision trees, ensemble methods, and neural networks, in detecting obfuscated malware within memory dumps.","Our analysis spans multiple malware categories, providing insights into algorithmic strengths and limitations.","By offering a comprehensive assessment of machine learning algorithms for obfuscated malware detection through memory analysis, this paper contributes to ongoing efforts to enhance cybersecurity and fortify digital ecosystems against evolving and sophisticated malware threats.","The source code is made open-access for reproducibility and future research endeavours.","It can be accessed at https://bit.ly/MalMemCode."],"url":"http://arxiv.org/abs/2404.02372v1","category":"cs.CR"}
{"created":"2024-04-03 00:10:03","title":"Piecewise Contractions","abstract":"We study piecewise injective, but not necessarily globally injective, contracting maps on a compact subset of \\(\\bR^d\\). We prove that generically the attractor and the set of discontinuities are disjoint, and hence the attractor consists of periodic orbits. In addition, we prove that piecewise injective contractions are generically topologically stable.","sentences":["We study piecewise injective, but not necessarily globally injective, contracting maps on a compact subset of \\(\\bR^d\\).","We prove that generically the attractor and the set of discontinuities are disjoint, and hence the attractor consists of periodic orbits.","In addition, we prove that piecewise injective contractions are generically topologically stable."],"url":"http://arxiv.org/abs/2404.02371v1","category":"math.DS"}
{"created":"2024-04-02 23:57:58","title":"Astrophotonics: recent and future developments","abstract":"Astrophotonics is a burgeoning field that lies at the interface of photonics and modern astronomical instrumentation. Here we provide a pedagogical review of basic photonic functions that enable modern instruments, and give an overview of recent and future applications. Traditionally, optical fibres have been used in innovative ways to vastly increase the multiplex advantage of an astronomical instrument, e.g. the ability to observe hundreds or thousands of stars simultaneously. But modern instruments are using many new photonic functions, some emerging from the telecom industry, and others specific to the demands of adaptive optics systems on modern telescopes. As telescopes continue to increase in size, we look to a future where instruments exploit the properties of individual photons. In particular, we envisage telescopes and interferometers that build on international developments in quantum networks, the so-called quantum internet. With the aid of entangled photons and quantum logic gates, the new infrastructures seek to preserve the photonic state and timing of individual photons over a coherent network.","sentences":["Astrophotonics is a burgeoning field that lies at the interface of photonics and modern astronomical instrumentation.","Here we provide a pedagogical review of basic photonic functions that enable modern instruments, and give an overview of recent and future applications.","Traditionally, optical fibres have been used in innovative ways to vastly increase the multiplex advantage of an astronomical instrument, e.g. the ability to observe hundreds or thousands of stars simultaneously.","But modern instruments are using many new photonic functions, some emerging from the telecom industry, and others specific to the demands of adaptive optics systems on modern telescopes.","As telescopes continue to increase in size, we look to a future where instruments exploit the properties of individual photons.","In particular, we envisage telescopes and interferometers that build on international developments in quantum networks, the so-called quantum internet.","With the aid of entangled photons and quantum logic gates, the new infrastructures seek to preserve the photonic state and timing of individual photons over a coherent network."],"url":"http://arxiv.org/abs/2404.02368v1","category":"astro-ph.IM"}
{"created":"2024-04-02 23:38:19","title":"The modified Korteweg--de Vries limit of the Ablowitz--Ladik system","abstract":"For slowly-varying initial data, solutions to the Ablowitz-Ladik system have been proven to converge to solutions of the cubic Schr\\\"odinger equation. In this paper we show that in the continuum limit, solutions to the Ablowitz-Ladik system with $H^1$ initial data may also converge to solutions of the modified Korteweg--de Vries equation. To exhibit this new limiting behavior, it suffices that the initial data is supported near the inflection points of the dispersion relation associated with the Ablowitz-Ladik system.   Our arguments employ harmonic analysis tools, Strichartz estimates, and the conservation of mass and energy. Correspondingly, they are applicable beyond the completely integrable models of greatest interest to us.","sentences":["For slowly-varying initial data, solutions to the Ablowitz-Ladik system have been proven to converge to solutions of the cubic Schr\\\"odinger equation.","In this paper we show that in the continuum limit, solutions to the Ablowitz-Ladik system with $H^1$ initial data may also converge to solutions of the modified Korteweg--de Vries equation.","To exhibit this new limiting behavior, it suffices that the initial data is supported near the inflection points of the dispersion relation associated with the Ablowitz-Ladik system.   ","Our arguments employ harmonic analysis tools, Strichartz estimates, and the conservation of mass and energy.","Correspondingly, they are applicable beyond the completely integrable models of greatest interest to us."],"url":"http://arxiv.org/abs/2404.02366v1","category":"math.AP"}
{"created":"2024-04-02 23:16:15","title":"FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction","abstract":"The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures. Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra. Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability. We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum. Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a tool for retrieval-based MS2C.","sentences":["The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures.","Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage.","Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra.","Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability.","We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum.","Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a tool for retrieval-based MS2C."],"url":"http://arxiv.org/abs/2404.02360v1","category":"cs.LG"}
{"created":"2024-04-02 23:05:56","title":"Attribution Regularization for Multimodal Paradigms","abstract":"Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach aims to mitigate the issue of unimodal dominance and improve the performance of multimodal machine learning systems. Through extensive experimentation and evaluation, the effectiveness and generalizability of the proposed technique will be assessed. The findings of this research project have the potential to significantly contribute to the advancement of multimodal machine learning and facilitate its application in various domains, including multimedia analysis, human-computer interaction, and embodied AI research.","sentences":["Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes.","However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information.","Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance.","This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions.","The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved.","By leveraging this regularization term, the proposed approach aims to mitigate the issue of unimodal dominance and improve the performance of multimodal machine learning systems.","Through extensive experimentation and evaluation, the effectiveness and generalizability of the proposed technique will be assessed.","The findings of this research project have the potential to significantly contribute to the advancement of multimodal machine learning and facilitate its application in various domains, including multimedia analysis, human-computer interaction, and embodied AI research."],"url":"http://arxiv.org/abs/2404.02359v1","category":"cs.LG"}
{"created":"2024-04-02 23:03:16","title":"Solving the $KP$ problem with the Global Cartan Decomposition","abstract":"Geometric methods have useful application for solving problems in a range of quantum information disciplines, including the synthesis of time-optimal unitaries in quantum control. In particular, the use of Cartan decompositions to solve problems in optimal control, especially lambda systems, has given rise to a range of techniques for solving the so-called $KP$-problem, where target unitaries belong to a semi-simple Lie group manifold $G$ whose Lie algebra admits a $\\mathfrak{g}=\\mathfrak{k} \\oplus \\mathfrak{p}$ decomposition and time-optimal solutions are represented by subRiemannian geodesics synthesised via a distribution of generators in $\\mathfrak{p}$. In this paper, we propose a new method utilising global Cartan decompositions $G=KAK$ of symmetric spaces $G/K$ for generating time-optimal unitaries for targets $-iX \\in [\\frak{p},\\frak{p}] \\subset \\frak{k}$ with controls $-iH(t) \\in \\frak{p}$. Target unitaries are parametrised as $U=kac$ where $k,c \\in K$ and $a = e^{i\\Theta}$ with $\\Theta \\in \\frak{a}$. We show that the assumption of $d\\Theta=0$ equates to the corresponding time-optimal unitary control problem being able to be solved analytically using variational techniques. We identify how such control problems correspond to the holonomies of a compact globally Riemannian symmetric space, where local translations are generated by $\\mathfrak{p}$ and local rotations are generated by $[\\mathfrak{p},\\mathfrak{p}]$.","sentences":["Geometric methods have useful application for solving problems in a range of quantum information disciplines, including the synthesis of time-optimal unitaries in quantum control.","In particular, the use of Cartan decompositions to solve problems in optimal control, especially lambda systems, has given rise to a range of techniques for solving the so-called $KP$-problem, where target unitaries belong to a semi-simple Lie group manifold $G$ whose Lie algebra admits a $\\mathfrak{g}=\\mathfrak{k} \\oplus \\mathfrak{p}$ decomposition and time-optimal solutions are represented by subRiemannian geodesics synthesised via a distribution of generators in $\\mathfrak{p}$. In this paper, we propose a new method utilising global Cartan decompositions $G=KAK$ of symmetric spaces $G/K$ for generating time-optimal unitaries for targets $-iX \\in","[\\frak{p},\\frak{p}] \\subset \\frak{k}$ with controls $-iH(t) \\in \\frak{p}$. Target unitaries are parametrised as $U=kac$ where $k,c \\in K$ and $a = e^{i\\Theta}$ with $\\Theta \\in \\frak{a}$. We show that the assumption of $d\\Theta=0$ equates to the corresponding time-optimal unitary control problem being able to be solved analytically using variational techniques.","We identify how such control problems correspond to the holonomies of a compact globally Riemannian symmetric space, where local translations are generated by $\\mathfrak{p}$ and local rotations are generated by $[\\mathfrak{p},\\mathfrak{p}]$."],"url":"http://arxiv.org/abs/2404.02358v1","category":"quant-ph"}
{"created":"2024-04-02 23:03:00","title":"NetSmith: An Optimization Framework for Machine-Discovered Network Topologies","abstract":"Over the past few decades, network topology design for general purpose, shared memory multicores has been primarily driven by human experts who use their insights to arrive at network designs that balance the competing goals of performance requirements (e.g., latency, bandwidth) and cost constraints (e.g., router radix, router counts). On the other hand, there have been automatic NoC synthesis methods for SoCs to optimize for application-specific communication and objectives such as resource usage or power. Unfortunately, these techniques do not lend themselves to the general-purpose context, where directly applying these previous NoC synthesis techniques in the general-purpose context yields poor results, even worse than expert-designed networks. We design and develop an automatic network design methodology - NetSmith - to design networks for general-purpose, shared memory multicores that comprehensively outperform expert-designed networks.   We employ NetSmith in the context of interposer networks for chiplet-based systems where there has been significant recent work on network topology design (e.g., Kite, Butter Donut, Double Butterfly). NetSmith generated topologies are capable of achieving significantly higher throughput (50% to 75% higher) while also reducing average hop count by 8% to 13.5%) than previous expert-designed and synthesized networks. Full system simulations using PARSEC benchmarks demonstrate that the improved network performance translates to improved application performance with up to 11% mean speedup over previous NoI topologies.","sentences":["Over the past few decades, network topology design for general purpose, shared memory multicores has been primarily driven by human experts who use their insights to arrive at network designs that balance the competing goals of performance requirements (e.g., latency, bandwidth) and cost constraints (e.g., router radix, router counts).","On the other hand, there have been automatic NoC synthesis methods for SoCs to optimize for application-specific communication and objectives such as resource usage or power.","Unfortunately, these techniques do not lend themselves to the general-purpose context, where directly applying these previous NoC synthesis techniques in the general-purpose context yields poor results, even worse than expert-designed networks.","We design and develop an automatic network design methodology - NetSmith - to design networks for general-purpose, shared memory multicores that comprehensively outperform expert-designed networks.   ","We employ NetSmith in the context of interposer networks for chiplet-based systems where there has been significant recent work on network topology design (e.g., Kite, Butter Donut, Double Butterfly).","NetSmith generated topologies are capable of achieving significantly higher throughput (50% to 75% higher) while also reducing average hop count by 8% to 13.5%) than previous expert-designed and synthesized networks.","Full system simulations using PARSEC benchmarks demonstrate that the improved network performance translates to improved application performance with up to 11% mean speedup over previous NoI topologies."],"url":"http://arxiv.org/abs/2404.02357v1","category":"cs.AR"}
{"created":"2024-04-03 17:51:20","title":"Automated Transparency: A Legal and Empirical Analysis of the Digital Services Act Transparency Database","abstract":"The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.","sentences":["The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency.","Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (`statements of reasons' - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study.","SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023.","The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance.","This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises.","We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices.","In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices."],"url":"http://arxiv.org/abs/2404.02894v1","category":"cs.CY"}
{"created":"2024-04-03 17:33:21","title":"Linear Attention Sequence Parallelism","abstract":"Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.","sentences":["Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU.","However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models.","In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models.","Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP.","We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters.","Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches.","We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes.","LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster.","The code is available at https://github.com/OpenNLPLab/LASP."],"url":"http://arxiv.org/abs/2404.02882v1","category":"cs.LG"}
{"created":"2024-04-03 17:32:52","title":"On computing approximate Lewis weights","abstract":"In this note we provide and analyze a simple method that given an $n \\times d$ matrix, outputs approximate $\\ell_p$-Lewis weights, a natural measure of the importance of the rows with respect to the $\\ell_p$ norm, for $p \\geq 2$. More precisely, we provide a simple post-processing procedure that turns natural one-sided approximate $\\ell_p$-Lewis weights into two-sided approximations. When combined with a simple one-sided approximation algorithm presented by Lee (PhD thesis, `16) this yields an algorithm for computing two-sided approximations of the $\\ell_p$-Lewis weights of an $n \\times d$-matrix using $\\mathrm{poly}(d,p)$ approximate leverage score computations. While efficient high-accuracy algorithms for approximating $\\ell_p$-Lewis had been established previously by Fazel, Lee, Padmanabhan and Sidford (SODA `22), the simple structure and approximation tolerance of our algorithm may make it of use for different applications.","sentences":["In this note we provide and analyze a simple method that given an $n \\times d$ matrix, outputs approximate $\\ell_p$-Lewis weights, a natural measure of the importance of the rows with respect to the $\\ell_p$ norm, for $p \\geq 2$.","More precisely, we provide a simple post-processing procedure that turns natural one-sided approximate $\\ell_p$-Lewis weights into two-sided approximations.","When combined with a simple one-sided approximation algorithm presented by Lee (PhD thesis, `16) this yields an algorithm for computing two-sided approximations of the $\\ell_p$-Lewis weights of an $n \\times d$-matrix using $\\mathrm{poly}(d,p)$ approximate leverage score computations.","While efficient high-accuracy algorithms for approximating $\\ell_p$-Lewis had been established previously by Fazel, Lee, Padmanabhan and Sidford (SODA `22), the simple structure and approximation tolerance of our algorithm may make it of use for different applications."],"url":"http://arxiv.org/abs/2404.02881v1","category":"cs.DS"}
{"created":"2024-04-03 17:08:23","title":"A mean-field model of optimal investment","abstract":"We establish the existence and uniqueness of the equilibrium for a stochastic mean-field game of optimal investment. The analysis covers both finite and infinite time horizons, and the mean-field interaction of the representative company with a mass of identical and indistinguishable firms is modeled through the time-dependent price at which the produced good is sold. At equilibrium, this price is given in terms of a nonlinear function of the expected (optimally controlled) production capacity of the representative company at each time. The proof of the existence and uniqueness of the mean-field equilibrium relies on a priori estimates and the study of nonlinear integral equations, but employs different techniques for the finite and infinite horizon cases. Additionally, we investigate the deterministic counterpart of the mean-field game under study.","sentences":["We establish the existence and uniqueness of the equilibrium for a stochastic mean-field game of optimal investment.","The analysis covers both finite and infinite time horizons, and the mean-field interaction of the representative company with a mass of identical and indistinguishable firms is modeled through the time-dependent price at which the produced good is sold.","At equilibrium, this price is given in terms of a nonlinear function of the expected (optimally controlled) production capacity of the representative company at each time.","The proof of the existence and uniqueness of the mean-field equilibrium relies on a priori estimates and the study of nonlinear integral equations, but employs different techniques for the finite and infinite horizon cases.","Additionally, we investigate the deterministic counterpart of the mean-field game under study."],"url":"http://arxiv.org/abs/2404.02871v1","category":"math.OC"}
{"created":"2024-04-03 16:57:16","title":"Discovery of universal phonon thermal Hall effect in crystals","abstract":"Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field. While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggest that phonons play a significant role. However, the mechanism behind phonon THE is still unknown. Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductor: SrTiO$_3$, SiO$_2$ (quartz), MgO and Si. While the presence of antiferrodistortive domains in SrTiO$_3$ and chiral phonons in SiO$_2$ may complicate the interpretation of THE, the striking observations of THE in the trivial insulator MgO and high-purity intrinsic semiconductor Si demonstrate that phonon THE is a universal property of crystals. Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |$\\kappa_{xy}$| $\\sim$ $\\kappa_{xx}^2$. Our results experimentally discover a fundamental physics of phonons in magnetic field, which must come from the direct coupling between atom vibrations and the field. Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered.","sentences":["Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field.","While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggest that phonons play a significant role.","However, the mechanism behind phonon THE is still unknown.","Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductor:","SrTiO$_3$, SiO$_2$ (quartz), MgO and Si.","While the presence of antiferrodistortive domains in SrTiO$_3$ and chiral phonons in SiO$_2$ may complicate the interpretation of THE, the striking observations of THE in the trivial insulator MgO and high-purity intrinsic semiconductor Si demonstrate that phonon THE is a universal property of crystals.","Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |$\\kappa_{xy}$| $\\sim$ $\\kappa_{xx}^2$. Our results experimentally discover a fundamental physics of phonons in magnetic field, which must come from the direct coupling between atom vibrations and the field.","Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered."],"url":"http://arxiv.org/abs/2404.02863v1","category":"cond-mat.str-el"}
{"created":"2024-04-03 16:53:32","title":"Pre-equilibrium Photon and Dilepton Production","abstract":"We use QCD kinetic theory to compute photon and dilepton production in the chemically equilibrating out-of-equilibrium quark-gluon plasma created in the early stages of high-energy heavy-ion collisions. We derive universal scaling functions for the pre-equilibrium spectra of photons and dileptons. These scaling functions can be used to make realistic predictions for the pre-equilibrium emission and consequently establish the significance of the pre-equilibrium phase for the production of electromagnetic probes in heavy-ion collisions.","sentences":["We use QCD kinetic theory to compute photon and dilepton production in the chemically equilibrating out-of-equilibrium quark-gluon plasma created in the early stages of high-energy heavy-ion collisions.","We derive universal scaling functions for the pre-equilibrium spectra of photons and dileptons.","These scaling functions can be used to make realistic predictions for the pre-equilibrium emission and consequently establish the significance of the pre-equilibrium phase for the production of electromagnetic probes in heavy-ion collisions."],"url":"http://arxiv.org/abs/2404.02861v1","category":"hep-ph"}
{"created":"2024-04-03 16:53:24","title":"Spin alignment of $K^\\ast$ induced by strange-baryon density inhomogeneity","abstract":"The difference between the spin alignments of $K^\\ast$ and those of $\\phi$ at the low collision energies is a puzzle raised by the recent experiments. Unlike $\\phi$ meson, $K^\\ast$, carrying a unit strange charge, should react to strange chemical potential $\\mu_S$. In this paper, we shall first convince you that $\\mu_S$ is not small in a brayon-rich medium for keeping strange neutrality, and then derive the spin alignment induced by the gradient of $\\mu_S$, and hence of baryon chemical potential $\\mu_B$, using linear response theory, with the transport coefficients expressed, without any approximation, in terms of the $K^\\ast$'s in-medium spectral properties by employing Ward-Takahashi identity. It turns out that such an effect applies mainly to the particles whose longitudinal and transverse modes diverge, and induces only the local spin alignment in a static medium. The magnitudes of these coefficients will be further estimated under the quasi-particle approximation.","sentences":["The difference between the spin alignments of $K^\\ast$ and those of $\\phi$ at the low collision energies is a puzzle raised by the recent experiments.","Unlike $\\phi$ meson, $K^\\ast$, carrying a unit strange charge, should react to strange chemical potential $\\mu_S$. In this paper, we shall first convince you that $\\mu_S$ is not small in a brayon-rich medium for keeping strange neutrality, and then derive the spin alignment induced by the gradient of $\\mu_S$, and hence of baryon chemical potential $\\mu_B$, using linear response theory, with the transport coefficients expressed, without any approximation, in terms of the $K^\\ast$'s in-medium spectral properties by employing Ward-Takahashi identity.","It turns out that such an effect applies mainly to the particles whose longitudinal and transverse modes diverge, and induces only the local spin alignment in a static medium.","The magnitudes of these coefficients will be further estimated under the quasi-particle approximation."],"url":"http://arxiv.org/abs/2404.02860v1","category":"nucl-th"}
{"created":"2024-04-03 16:37:16","title":"Tight stability bounds for entropic Brenier maps","abstract":"Entropic Brenier maps are regularized analogues of Brenier maps (optimal transport maps) which converge to Brenier maps as the regularization parameter shrinks. In this work, we prove quantitative stability bounds between entropic Brenier maps under variations of the target measure. In particular, when all measures have bounded support, we establish the optimal Lipschitz constant for the mapping from probability measures to entropic Brenier maps. This provides an exponential improvement to a result of Carlier, Chizat, and Laborde (2024). As an application, we prove near-optimal bounds for the stability of semi-discrete \\emph{unregularized} Brenier maps for a family of discrete target measures.","sentences":["Entropic Brenier maps are regularized analogues of Brenier maps (optimal transport maps) which converge to Brenier maps as the regularization parameter shrinks.","In this work, we prove quantitative stability bounds between entropic Brenier maps under variations of the target measure.","In particular, when all measures have bounded support, we establish the optimal Lipschitz constant for the mapping from probability measures to entropic Brenier maps.","This provides an exponential improvement to a result of Carlier, Chizat, and Laborde (2024).","As an application, we prove near-optimal bounds for the stability of semi-discrete \\emph{unregularized} Brenier maps for a family of discrete target measures."],"url":"http://arxiv.org/abs/2404.02855v1","category":"math.PR"}
{"created":"2024-04-03 16:33:48","title":"Domination number of modular product graphs","abstract":"The modular product $G\\diamond H$ of graphs $G$ and $H$ is a graph on vertex set $V(G)\\times V(H)$. Two vertices $(g,h)$ and $(g^{\\prime},h^{\\prime})$ of $G\\diamond H$ are adjacent if $g=g^{\\prime}$ and $hh^{\\prime}\\in E(H)$, or $gg^{\\prime}\\in E(G)$ and $h=h^{\\prime}$, or $gg^{\\prime}\\in E(G)$ and $hh^{\\prime}\\in E(H)$, or (for $g\\neq g^{\\prime}$ and $h\\neq h^{\\prime}$) $gg^{\\prime}\\notin E(G)$ and $hh^{\\prime}\\notin E(H)$. A set $D\\subseteq V(G)$ is a dominating set of $G$ if every vertex outside of $D$ contains a neighbor in $D$. A set $D\\subseteq V(G)$ is a total dominating set of $G$ if every vertex of $G$ contains a neighbor in $D$. The domination number $\\gamma(G)$ (resp. total domination number $\\gamma_{t}(G)$) of $G$ is the minimum cardinality of a dominating set (resp. total dominating set) of $G$. In this work we give several upper and lower bounds for $\\gamma(G\\diamond H)$ in terms of $\\gamma(G),$ $\\gamma(H)$, $\\gamma_{t}(\\overline{G})$ and $\\gamma _{t}(\\overline{H})$, where $\\overline{G}$ is the complement graph of $G$. Further, we fully describe graphs where $\\gamma(G\\diamond H)=k$ for $k\\in\\{1,2,3\\}$. Several conditions on $G$ and $H$ under which $\\gamma (G\\diamond H)$ is at most $4$ and $5$ are also given. A new type of simultaneous domination $\\bar{\\gamma}(G)$, defined as the smallest number of vertices that dominates $G$ and totally dominates the complement of $G,$ emerged as useful and we believe it could be of independent interest. We conclude the paper by proposing few directions for possible further research.","sentences":["The modular product $G\\diamond H$ of graphs $G$ and $H$ is a graph on vertex set $V(G)\\times V(H)$. Two vertices $(g,h)$ and $(g^{\\prime},h^{\\prime})$ of $G\\diamond H$ are adjacent if $g=g^{\\prime}$ and $hh^{\\prime}\\in E(H)$, or $gg^{\\prime}\\in E(G)$ and $h=h^{\\prime}$, or $gg^{\\prime}\\in E(G)$ and $hh^{\\prime}\\in E(H)$, or (for $g\\neq g^{\\prime}$ and $h\\neq h^{\\prime}$) $gg^{\\prime}\\notin E(G)$ and $hh^{\\prime}\\notin E(H)$. A set $D\\subseteq V(G)$ is a dominating set of $G$ if every vertex outside of $D$ contains a neighbor in $D$. A set $D\\subseteq V(G)$ is a total dominating set of $G$ if every vertex of $G$ contains a neighbor in $D$. The domination number $\\gamma(G)$ (resp.","total domination number $\\gamma_{t}(G)$) of $G$ is the minimum cardinality of a dominating set (resp.","total dominating set) of $G$.","In this work we give several upper and lower bounds for $\\gamma(G\\diamond H)$ in terms of $\\gamma(G),$ $\\gamma(H)$, $\\gamma_{t}(\\overline{G})$ and $\\gamma _{t}(\\overline{H})$, where $\\overline{G}$ is the complement graph of $G$. Further, we fully describe graphs where $\\gamma(G\\diamond H)=k$ for $k\\in\\{1,2,3\\}$. Several conditions on $G$ and $H$ under which $\\gamma (G\\diamond H)$ is at most $4$ and $5$ are also given.","A new type of simultaneous domination $\\bar{\\gamma}(G)$, defined as the smallest number of vertices that dominates $G$ and totally dominates the complement of $G,$ emerged as useful and we believe it could be of independent interest.","We conclude the paper by proposing few directions for possible further research."],"url":"http://arxiv.org/abs/2404.02853v1","category":"math.CO"}
{"created":"2024-04-03 16:33:42","title":"Toward Inference-optimal Mixture-of-Expert Large Language Models","abstract":"Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.","sentences":["Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers.","Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens?","We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree.","Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time.","We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss.","We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training.","On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget."],"url":"http://arxiv.org/abs/2404.02852v1","category":"cs.LG"}
{"created":"2024-04-03 16:21:01","title":"Mixed volumes of zonoids and the absolute value of the Grassmannian (Extended Abstract)","abstract":"Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments. We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map. We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3). Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve.","sentences":["Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments.","We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map.","We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3).","Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve."],"url":"http://arxiv.org/abs/2404.02842v1","category":"math.CO"}
{"created":"2024-04-03 16:03:59","title":"Legendre Transformation under Micro Canonical Ensemble","abstract":"The Legendre transformation is a crucial tool in theoretical physics, known for its symmetry, especially when applied to multivariate functions. In statistical mechanics, ensembles represent the central focus. Leveraging the dimensionless aspect of Legendre transformation, this paper explores the transformation process from the entropy characteristic function of microcanonical ensembles to the analogous definition of partition function transformation. Additionally, it derives characteristic functions, partition functions, and establishes their interrelations, along with deriving corresponding thermodynamic formulas for various ensembles. This streamlined approach sheds light on the fundamental principles of statistical mechanics and underscores the symmetry inherent in Legendre transformation.","sentences":["The Legendre transformation is a crucial tool in theoretical physics, known for its symmetry, especially when applied to multivariate functions.","In statistical mechanics, ensembles represent the central focus.","Leveraging the dimensionless aspect of Legendre transformation, this paper explores the transformation process from the entropy characteristic function of microcanonical ensembles to the analogous definition of partition function transformation.","Additionally, it derives characteristic functions, partition functions, and establishes their interrelations, along with deriving corresponding thermodynamic formulas for various ensembles.","This streamlined approach sheds light on the fundamental principles of statistical mechanics and underscores the symmetry inherent in Legendre transformation."],"url":"http://arxiv.org/abs/2404.02829v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-03 15:37:02","title":"GPU-Accelerated RSF Level Set Evolution for Large-Scale Microvascular Segmentation","abstract":"Microvascular networks are challenging to model because these structures are currently near the diffraction limit for most advanced three-dimensional imaging modalities, including confocal and light sheet microscopy. This makes semantic segmentation difficult, because individual components of these networks fluctuate within the confines of individual pixels. Level set methods are ideally suited to solve this problem by providing surface and topological constraints on the resulting model, however these active contour techniques are extremely time intensive and impractical for terabyte-scale images. We propose a reformulation and implementation of the region-scalable fitting (RSF) level set model that makes it amenable to three-dimensional evaluation using both single-instruction multiple data (SIMD) and single-program multiple-data (SPMD) parallel processing. This enables evaluation of the level set equation on independent regions of the data set using graphics processing units (GPUs), making large-scale segmentation of high-resolution networks practical and inexpensive.   We tested this 3D parallel RSF approach on multiple data sets acquired using state-of-the-art imaging techniques to acquire microvascular data, including micro-CT, light sheet fluorescence microscopy (LSFM) and milling microscopy. To assess the performance and accuracy of the RSF model, we conducted a Monte-Carlo-based validation technique to compare results to other segmentation methods. We also provide a rigorous profiling to show the gains in processing speed leveraging parallel hardware. This study showcases the practical application of the RSF model, emphasizing its utility in the challenging domain of segmenting large-scale high-topology network structures with a particular focus on building microvascular models.","sentences":["Microvascular networks are challenging to model because these structures are currently near the diffraction limit for most advanced three-dimensional imaging modalities, including confocal and light sheet microscopy.","This makes semantic segmentation difficult, because individual components of these networks fluctuate within the confines of individual pixels.","Level set methods are ideally suited to solve this problem by providing surface and topological constraints on the resulting model, however these active contour techniques are extremely time intensive and impractical for terabyte-scale images.","We propose a reformulation and implementation of the region-scalable fitting (RSF) level set model that makes it amenable to three-dimensional evaluation using both single-instruction multiple data (SIMD) and single-program multiple-data (SPMD) parallel processing.","This enables evaluation of the level set equation on independent regions of the data set using graphics processing units (GPUs), making large-scale segmentation of high-resolution networks practical and inexpensive.   ","We tested this 3D parallel RSF approach on multiple data sets acquired using state-of-the-art imaging techniques to acquire microvascular data, including micro-CT, light sheet fluorescence microscopy (LSFM) and milling microscopy.","To assess the performance and accuracy of the RSF model, we conducted a Monte-Carlo-based validation technique to compare results to other segmentation methods.","We also provide a rigorous profiling to show the gains in processing speed leveraging parallel hardware.","This study showcases the practical application of the RSF model, emphasizing its utility in the challenging domain of segmenting large-scale high-topology network structures with a particular focus on building microvascular models."],"url":"http://arxiv.org/abs/2404.02813v1","category":"eess.IV"}
{"created":"2024-04-03 15:23:31","title":"The ALMA Legacy survey of Class 0/I disks in Corona australis, Aquila, chaMaeleon, oPhiuchus north, Ophiuchus, Serpens (CAMPOS). I. Evolution of Protostellar disk radii","abstract":"We surveyed nearly all the embedded protostars in seven nearby clouds (Corona Australis, Aquila, Chamaeleon I & II, Ophiuchus North, Ophiuchus, Serpens) with the Atacama Large Millimeter/submillimeter Array at 1.3mm observations with a resolution of 0.1\". This survey detected 184 protostellar disks, 90 of which were observed at a resolution of 14-18 au, making it one of the most comprehensive high-resolution disk samples across various protostellar evolutionary stages to date. Our key findings include the detection of new annular substructures in two Class I and two Flat-spectrum sources, while 21 embedded protostars exhibit distinct asymmetries or substructures in their disks. We find that protostellar disks have a substantially large variability in their radii across all classes. In particular, the fraction of large disks with sizes above 40 au decreases with the protostellar evolutionary stage. Compiling the literature data, we discovered an increasing trend of the gas disk radii to dust disk radii ratio ($R_{\\rm gas,Kep}/R_{\\rm mm}$) with increasing bolometric temperature (${\\rm T}_{\\rm bol}$). Our results indicate that the dust and gas disk radii decouple during the early Class I stage. We find that Class 0 dust disk size resembled the gas disk size, enabling a direct comparison between models and observational data at the earliest stages of protostellar evolution. We show that the distribution of radii in the 52 Class 0 disks in our sample is in high tension with various disk formation models, indicating that protostellar disk formation remains an unsolved question.","sentences":["We surveyed nearly all the embedded protostars in seven nearby clouds (Corona Australis, Aquila, Chamaeleon I & II, Ophiuchus North, Ophiuchus, Serpens) with the Atacama Large Millimeter/submillimeter Array at 1.3mm observations with a resolution of 0.1\".","This survey detected 184 protostellar disks, 90 of which were observed at a resolution of 14-18 au, making it one of the most comprehensive high-resolution disk samples across various protostellar evolutionary stages to date.","Our key findings include the detection of new annular substructures in two Class I and two Flat-spectrum sources, while 21 embedded protostars exhibit distinct asymmetries or substructures in their disks.","We find that protostellar disks have a substantially large variability in their radii across all classes.","In particular, the fraction of large disks with sizes above 40 au decreases with the protostellar evolutionary stage.","Compiling the literature data, we discovered an increasing trend of the gas disk radii to dust disk radii ratio ($R_{\\rm gas,Kep}/R_{\\rm mm}$) with increasing bolometric temperature (${\\rm T}_{\\rm bol}$).","Our results indicate that the dust and gas disk radii decouple during the early Class I stage.","We find that Class 0 dust disk size resembled the gas disk size, enabling a direct comparison between models and observational data at the earliest stages of protostellar evolution.","We show that the distribution of radii in the 52 Class 0 disks in our sample is in high tension with various disk formation models, indicating that protostellar disk formation remains an unsolved question."],"url":"http://arxiv.org/abs/2404.02809v1","category":"astro-ph.SR"}
{"created":"2024-04-03 14:37:00","title":"Forming Large Patterns with Local Robots in the OBLOT Model","abstract":"In the arbitrary pattern formation problem, $n$ autonomous, mobile robots must form an arbitrary pattern $P \\subseteq \\mathbb{R}^2$. The (deterministic) robots are typically assumed to be indistinguishable, disoriented, and unable to communicate. An important distinction is whether robots have memory and/or a limited viewing range. Previous work managed to form $P$ under a natural symmetry condition if robots have no memory but an unlimited viewing range [22] or if robots have a limited viewing range but memory [25]. In the latter case, $P$ is only formed in a shrunk version that has constant diameter.   Without memory and with limited viewing range, forming arbitrary patterns remains an open problem. We provide a partial solution by showing that $P$ can be formed under the same symmetry condition if the robots' initial diameter is $\\leq 1$. Our protocol partitions $P$ into rotation-symmetric components and exploits the initial mutual visibility to form one cluster per component. Using a careful placement of the clusters and their robots, we show that a cluster can move in a coordinated way through its component while drawing $P$ by dropping one robot per pattern coordinate.","sentences":["In the arbitrary pattern formation problem, $n$ autonomous, mobile robots must form an arbitrary pattern $P \\subseteq \\mathbb{R}^2$.","The (deterministic) robots are typically assumed to be indistinguishable, disoriented, and unable to communicate.","An important distinction is whether robots have memory and/or a limited viewing range.","Previous work managed to form $P$ under a natural symmetry condition if robots have no memory but an unlimited viewing range","[22] or if robots have a limited viewing range but memory [25].","In the latter case, $P$ is only formed in a shrunk version that has constant diameter.   ","Without memory and with limited viewing range, forming arbitrary patterns remains an open problem.","We provide a partial solution by showing that $P$ can be formed under the same symmetry condition if the robots' initial diameter is $\\leq 1$.","Our protocol partitions $P$ into rotation-symmetric components and exploits the initial mutual visibility to form one cluster per component.","Using a careful placement of the clusters and their robots, we show that a cluster can move in a coordinated way through its component while drawing $P$ by dropping one robot per pattern coordinate."],"url":"http://arxiv.org/abs/2404.02771v1","category":"cs.RO"}
{"created":"2024-04-03 14:24:35","title":"(Non-)Extendability of Abel-Jacobi Maps","abstract":"We investigate the \"natural\" locus of definition of Abel-Jacobi maps. In particular, we show that, for a proper, geometrically reduced curve C -- not necessarily smooth -- the Abel-Jacobi map from the smooth locus C^{sm} into the Jacobian of C does not extend to any larger (separated, geometrically reduced) curve containing C^{sm} except under certain particular circumstances which we describe explicitly. As a consequence, we deduce that the Abel-Jacobi map has closed image except in certain explicitly described circumstances, and that it is always a closed embedding for irreducible curves not isomorphic to P^1.","sentences":["We investigate the \"natural\" locus of definition of Abel-Jacobi maps.","In particular, we show that, for a proper, geometrically reduced curve C -- not necessarily smooth -- the Abel-Jacobi map from the smooth locus C^{sm} into the Jacobian of C does not extend to any larger (separated, geometrically reduced) curve containing C^{sm} except under certain particular circumstances which we describe explicitly.","As a consequence, we deduce that the Abel-Jacobi map has closed image except in certain explicitly described circumstances, and that it is always a closed embedding for irreducible curves not isomorphic to P^1."],"url":"http://arxiv.org/abs/2404.02766v1","category":"math.AG"}
{"created":"2024-04-03 13:50:57","title":"Direct in-situ observations of wave-induced floe collisions in the deeper Marginal Ice Zone","abstract":"Ocean waves propagating through the Marginal Ice Zone (MIZ) and the pack ice are strongly attenuated. This attenuation is critical for protecting sea ice from energetic wave events that could otherwise lead to sea ice break-up and dislocation over large areas. Despite the importance of waves-in-ice attenuation, the exact physical mechanisms involved, and their relative importance, are still uncertain. Here we present for the first time direct in situ measurements of floe-floe interactions under the influence of waves, including collisions between adjacent floes. The collision events we report are aligned with the incoming wave direction, and phase-locked to the wave signal, which indicates that the individual collisions we detect are wave-induced. The observations demonstrate that wave attenuation by wave-induced floe-floe collisions, which have been studied in idealized laboratory and field experiments in the outer MIZ, are indeed a likely source of wave energy dissipation deep inside the MIZ as well.","sentences":["Ocean waves propagating through the Marginal Ice Zone (MIZ) and the pack ice are strongly attenuated.","This attenuation is critical for protecting sea ice from energetic wave events that could otherwise lead to sea ice break-up and dislocation over large areas.","Despite the importance of waves-in-ice attenuation, the exact physical mechanisms involved, and their relative importance, are still uncertain.","Here we present for the first time direct in situ measurements of floe-floe interactions under the influence of waves, including collisions between adjacent floes.","The collision events we report are aligned with the incoming wave direction, and phase-locked to the wave signal, which indicates that the individual collisions we detect are wave-induced.","The observations demonstrate that wave attenuation by wave-induced floe-floe collisions, which have been studied in idealized laboratory and field experiments in the outer MIZ, are indeed a likely source of wave energy dissipation deep inside the MIZ as well."],"url":"http://arxiv.org/abs/2404.02750v1","category":"physics.ao-ph"}
{"created":"2024-04-03 13:25:34","title":"AdS_2/CFT_1 at finite density and holographic aspects of 2D black holes","abstract":"In the present thesis, we study various models of 2D dilaton gravity known as JT gravity coupled with non-trivial gauge interactions (for instance $SU(2)$ Yang-Mills, quartic interactions between $U(1)$ gauge fields and Modmax interactions etc.). In particular, we investigate the effects of the non-trivial gauge couplings on the thermal properties of black holes and wormholes in two dimensions and compute various physical observables like entropy, free energy, etc. Further, we examine the possibilities of the Hawking-Page transition and wormhole to black hole phase transition in two dimensions. In addition, we also study the transformation properties of boundary stress-energy tensor under the diffeomorphism and $U(1)$ gauge transformation and hence compute the central charge associated with the 1D boundary theory.","sentences":["In the present thesis, we study various models of 2D dilaton gravity known as JT gravity coupled with non-trivial gauge interactions (for instance $SU(2)$ Yang-Mills, quartic interactions between $U(1)$ gauge fields and Modmax interactions etc.).","In particular, we investigate the effects of the non-trivial gauge couplings on the thermal properties of black holes and wormholes in two dimensions and compute various physical observables like entropy, free energy, etc.","Further, we examine the possibilities of the Hawking-Page transition and wormhole to black hole phase transition in two dimensions.","In addition, we also study the transformation properties of boundary stress-energy tensor under the diffeomorphism and $U(1)$ gauge transformation and hence compute the central charge associated with the 1D boundary theory."],"url":"http://arxiv.org/abs/2404.02724v1","category":"hep-th"}
{"created":"2024-04-03 13:01:26","title":"Transformation of $p$-gradient flows to $p'$-gradient flows in metric spaces","abstract":"We explicitly construct parameter transformations between gradient flows in metric spaces, called curves of maximal slope, having different exponents when the associated function satisfies a suitable convexity condition. These transformations induce the uniqueness of gradient flows for all exponents under a natural assumption which is satisfied in many examples. We also prove the regularizing effects of gradient flows. To establish these results, we directly deal with gradient flows instead of using variational discrete approximations which are often used in the study of gradient flows.","sentences":["We explicitly construct parameter transformations between gradient flows in metric spaces, called curves of maximal slope, having different exponents when the associated function satisfies a suitable convexity condition.","These transformations induce the uniqueness of gradient flows for all exponents under a natural assumption which is satisfied in many examples.","We also prove the regularizing effects of gradient flows.","To establish these results, we directly deal with gradient flows instead of using variational discrete approximations which are often used in the study of gradient flows."],"url":"http://arxiv.org/abs/2404.02703v1","category":"math.AP"}
{"created":"2024-04-03 12:37:39","title":"Elementary methods provide more replicable results in microbial differential abundance analysis","abstract":"Differential abundance analysis is a key component of microbiome studies. It focuses on the task of assessing the magnitude and statistical significance of differences in microbial abundances between conditions. While dozens of methods for differential abundance analysis exist, they have been reported to produce remarkably discordant results. Currently, there is no consensus on the preferred methods. While correctness of results in differential abundance analysis is an ambiguous concept that cannot be evaluated without employing simulated data, we argue that consistency of results across datasets should be considered as an essential quality of a well-performing method. We compared the performance of 13 differential abundance analysis methods employing datasets from multiple (N = 54) taxonomic profiling studies based on 16S rRNA gene or shotgun sequencing. For each method, we examined how the results replicated between random partitions of each dataset and between datasets from independent studies. While certain methods showed good consistency, some widely used methods were observed to make a substantial number of conflicting findings. Overall, the highest consistency without unnecessary reduction in sensitivity was attained by analyzing total sum scaling (TSS) normalized counts with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression (MaAsLin2). Comparable performance was also attained by analyzing presence/absence of taxa with logistic regression. In conclusion, while numerous sophisticated methods for differential abundance analysis have been developed, elementary methods seem to provide more consistent results without unnecessarily compromising sensitivity. We therefore suggest that the elementary methods should be preferred in microbial differential abundance analysis when replicability needs to be emphasized.","sentences":["Differential abundance analysis is a key component of microbiome studies.","It focuses on the task of assessing the magnitude and statistical significance of differences in microbial abundances between conditions.","While dozens of methods for differential abundance analysis exist, they have been reported to produce remarkably discordant results.","Currently, there is no consensus on the preferred methods.","While correctness of results in differential abundance analysis is an ambiguous concept that cannot be evaluated without employing simulated data, we argue that consistency of results across datasets should be considered as an essential quality of a well-performing method.","We compared the performance of 13 differential abundance analysis methods employing datasets from multiple (N = 54) taxonomic profiling studies based on 16S rRNA gene or shotgun sequencing.","For each method, we examined how the results replicated between random partitions of each dataset and between datasets from independent studies.","While certain methods showed good consistency, some widely used methods were observed to make a substantial number of conflicting findings.","Overall, the highest consistency without unnecessary reduction in sensitivity was attained by analyzing total sum scaling (TSS) normalized counts with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression (MaAsLin2).","Comparable performance was also attained by analyzing presence/absence of taxa with logistic regression.","In conclusion, while numerous sophisticated methods for differential abundance analysis have been developed, elementary methods seem to provide more consistent results without unnecessarily compromising sensitivity.","We therefore suggest that the elementary methods should be preferred in microbial differential abundance analysis when replicability needs to be emphasized."],"url":"http://arxiv.org/abs/2404.02691v1","category":"stat.AP"}
{"created":"2024-04-03 12:30:07","title":"Testing Independence Between High-Dimensional Random Vectors Using Rank-Based Max-Sum Tests","abstract":"In this paper, we address the problem of testing independence between two high-dimensional random vectors. Our approach involves a series of max-sum tests based on three well-known classes of rank-based correlations. These correlation classes encompass several popular rank measures, including Spearman's $\\rho$, Kendall's $\\tau$, Hoeffding's D, Blum-Kiefer-Rosenblatt's R and Bergsma-Dassios-Yanagimoto's $\\tau^*$.The key advantages of our proposed tests are threefold: (1) they do not rely on specific assumptions about the distribution of random vectors, which flexibility makes them available across various scenarios; (2) they can proficiently manage non-linear dependencies between random vectors, a critical aspect in high-dimensional contexts; (3) they have robust performance, regardless of whether the alternative hypothesis is sparse or dense.Notably, our proposed tests demonstrate significant advantages in various scenarios, which is suggested by extensive numerical results and an empirical application in RNA microarray analysis.","sentences":["In this paper, we address the problem of testing independence between two high-dimensional random vectors.","Our approach involves a series of max-sum tests based on three well-known classes of rank-based correlations.","These correlation classes encompass several popular rank measures, including Spearman's $\\rho$, Kendall's $\\tau$, Hoeffding's D, Blum-Kiefer-Rosenblatt's R and Bergsma-Dassios-Yanagimoto's $\\tau^*$.The key advantages of our proposed tests are threefold: (1) they do not rely on specific assumptions about the distribution of random vectors, which flexibility makes them available across various scenarios; (2) they can proficiently manage non-linear dependencies between random vectors, a critical aspect in high-dimensional contexts; (3) they have robust performance, regardless of whether the alternative hypothesis is sparse or dense.","Notably, our proposed tests demonstrate significant advantages in various scenarios, which is suggested by extensive numerical results and an empirical application in RNA microarray analysis."],"url":"http://arxiv.org/abs/2404.02685v1","category":"stat.ME"}
{"created":"2024-04-03 12:19:40","title":"A Framework for a High Throughput Screening Method to Assess Polymer/Plasticizer Miscibility","abstract":"Polymer composite materials require softening to reduce their glass transition temperature and improve processability. To this end, plasticizers, which are small organic molecules, are added to the polymer matrix. The miscibility of these plasticizers has a large impact on their effectiveness and therefore their interactions with the polymer matrix must be carefully considered. Many plasticizer characteristics, including their size, topology and flexibility, can impact their miscibility and, because of the exponentially large numbers of plasticizers, the current trial-and-error approach is very ineffective. In this work we show that using molecular simulations of a small dataset of 48 plasticizers, it is possible to identify topological and thermodynamic descriptors that are proxy for their miscibility. Using ad-hoc molecular dynamics simulation set-ups that are relatively computationally inexpensive, we establish correlations between the plasticizers' topology, internal flexibility, thermodynamics of aggregation and their degree of miscibility and use these descriptors to classify the molecules as miscible or immiscible. With all available data we also construct a decision tree model which achieves a F1 score of 0.86 +/- 0.01 with repeated, stratified 5-fold cross-validation, indicating that this machine learning method is a promising route to fully automate the screening. By evaluating the individual performance of the descriptors, we show this procedure enables a 10-fold reduction of the test space and provides the basis for the development of workflows which can efficiently screen thousands of plasticizers with a variety of features.","sentences":["Polymer composite materials require softening to reduce their glass transition temperature and improve processability.","To this end, plasticizers, which are small organic molecules, are added to the polymer matrix.","The miscibility of these plasticizers has a large impact on their effectiveness and therefore their interactions with the polymer matrix must be carefully considered.","Many plasticizer characteristics, including their size, topology and flexibility, can impact their miscibility and, because of the exponentially large numbers of plasticizers, the current trial-and-error approach is very ineffective.","In this work we show that using molecular simulations of a small dataset of 48 plasticizers, it is possible to identify topological and thermodynamic descriptors that are proxy for their miscibility.","Using ad-hoc molecular dynamics simulation set-ups that are relatively computationally inexpensive, we establish correlations between the plasticizers' topology, internal flexibility, thermodynamics of aggregation and their degree of miscibility and use these descriptors to classify the molecules as miscible or immiscible.","With all available data we also construct a decision tree model which achieves a F1 score of 0.86 +/- 0.01 with repeated, stratified 5-fold cross-validation, indicating that this machine learning method is a promising route to fully automate the screening.","By evaluating the individual performance of the descriptors, we show this procedure enables a 10-fold reduction of the test space and provides the basis for the development of workflows which can efficiently screen thousands of plasticizers with a variety of features."],"url":"http://arxiv.org/abs/2404.02676v1","category":"cond-mat.soft"}
{"created":"2024-04-03 12:18:07","title":"Enhancement in phase sensitivity of SU(1,1) interferometer with Kerr state seeding","abstract":"A coherent seeded SU(1,1) interferometer provides a prominent technique in the field of precision measurement. We theoretically study the phase sensitivity of SU(1,1) interferometer with Kerr state seeding under single intensity and homodyne detection schemes. To find the lower bound in this case we calculate the quantum Cram\\'er-Rao bound using the quantum Fisher information technique. We found that, under some conditions, the Kerr seeding performs better in phase sensitivity compared to the well-known vacuum and coherent seeded case. We expect that the Kerr state might act as an alternative non-classical state in the field of quantum information and sensing technologies.","sentences":["A coherent seeded SU(1,1) interferometer provides a prominent technique in the field of precision measurement.","We theoretically study the phase sensitivity of SU(1,1) interferometer with Kerr state seeding under single intensity and homodyne detection schemes.","To find the lower bound in this case we calculate the quantum Cram\\'er-Rao bound using the quantum Fisher information technique.","We found that, under some conditions, the Kerr seeding performs better in phase sensitivity compared to the well-known vacuum and coherent seeded case.","We expect that the Kerr state might act as an alternative non-classical state in the field of quantum information and sensing technologies."],"url":"http://arxiv.org/abs/2404.02674v1","category":"quant-ph"}
{"created":"2024-04-03 11:21:23","title":"On the Importance of Uncertainty in Decision-Making with Large Language Models","abstract":"We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.","sentences":["We investigate the role of uncertainty in decision-making problems with natural language as input.","For such tasks, using Large Language Models as agents has become the norm.","However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task.","We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text.","As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward.","We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy.","We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets.","We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies.","These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs."],"url":"http://arxiv.org/abs/2404.02649v1","category":"cs.LG"}
{"created":"2024-04-03 11:04:27","title":"Vacuum instability in QED with an asymmetric x-step. New example of exactly solvable case","abstract":"We present a new exactly solvable case in strong-field QED with one-dimensional step potential (x-step). The corresponding x-step is given by an analytic asymmetric with respect to the axis x reflection function. The step can be considered as a certain analytic \"deformation\" of the symmetric Sauter field. Moreover, it can be treated as a new regularization of the Klein step field. We study the vacuum instability caused by this x-step in the framework of a nonperturbative approach to strong-field QED. Exact solutions of the Dirac equation used in the corresponding nonperturbative calculations, are represented in the form of stationary plane waves with special left and right asymptotics and identified as components of initial and final wave packets of particles. We show that in spite of the fact that the symmetry with respect to positive and negative bands of energies is broken, distribution of created pairs and other physical quantities can be expressed via elementary functions. We consider the processes of transmission and reflection in the ranges of the stable vacuum and study physical quantities specifying the vacuum instability. We find the differential mean numbers of electron-positron pairs created from the vacuum, the components of current density and energy-momentum tensor of the created electrons and positrons leaving the area of the strong field under consideration. Besides, we study the particular case of the particle creation due to a weakly inhomogeneous electric field and obtain explicitly the total number, the current density and energy-momentum tensor of created particles. Unlike the symmetric case of the Sauter field the asymmetric form of the field under consideration causes the energy density and longitudinal pressure of created electrons to be not equal to the energy density and longitudinal pressure of created positrons.","sentences":["We present a new exactly solvable case in strong-field QED with one-dimensional step potential (x-step).","The corresponding x-step is given by an analytic asymmetric with respect to the axis x reflection function.","The step can be considered as a certain analytic \"deformation\" of the symmetric Sauter field.","Moreover, it can be treated as a new regularization of the Klein step field.","We study the vacuum instability caused by this x-step in the framework of a nonperturbative approach to strong-field QED.","Exact solutions of the Dirac equation used in the corresponding nonperturbative calculations, are represented in the form of stationary plane waves with special left and right asymptotics and identified as components of initial and final wave packets of particles.","We show that in spite of the fact that the symmetry with respect to positive and negative bands of energies is broken, distribution of created pairs and other physical quantities can be expressed via elementary functions.","We consider the processes of transmission and reflection in the ranges of the stable vacuum and study physical quantities specifying the vacuum instability.","We find the differential mean numbers of electron-positron pairs created from the vacuum, the components of current density and energy-momentum tensor of the created electrons and positrons leaving the area of the strong field under consideration.","Besides, we study the particular case of the particle creation due to a weakly inhomogeneous electric field and obtain explicitly the total number, the current density and energy-momentum tensor of created particles.","Unlike the symmetric case of the Sauter field the asymmetric form of the field under consideration causes the energy density and longitudinal pressure of created electrons to be not equal to the energy density and longitudinal pressure of created positrons."],"url":"http://arxiv.org/abs/2404.02640v1","category":"hep-th"}
{"created":"2024-04-03 10:46:10","title":"An Inexact Regularized Proximal Newton Method without Line Search","abstract":"In this paper, we introduce an inexact regularized proximal Newton method (IRPNM) that does not require any line search. The method is designed to minimize the sum of a twice continuously differentiable function $f$ and a convex (possibly non-smooth and extended-valued) function $\\varphi$. Instead of controlling a step size by a line search procedure, we update the regularization parameter in a suitable way, based on the success of the previous iteration. The global convergence of the sequence of iterations and its superlinear convergence rate under a local H\\\"olderian error bound assumption are shown. Notably, these convergence results are obtained without requiring a global Lipschitz property for $ \\nabla f $, which, to the best of the authors' knowledge, is a novel contribution for proximal Newton methods. To highlight the efficiency of our approach, we provide numerical comparisons with an IRPNM using a line search globalization and a modern FISTA-type method.","sentences":["In this paper, we introduce an inexact regularized proximal Newton method (IRPNM) that does not require any line search.","The method is designed to minimize the sum of a twice continuously differentiable function $f$ and a convex (possibly non-smooth and extended-valued) function $\\varphi$. Instead of controlling a step size by a line search procedure, we update the regularization parameter in a suitable way, based on the success of the previous iteration.","The global convergence of the sequence of iterations and its superlinear convergence rate under a local H\\\"olderian error bound assumption are shown.","Notably, these convergence results are obtained without requiring a global Lipschitz property for $ \\nabla f $, which, to the best of the authors' knowledge, is a novel contribution for proximal Newton methods.","To highlight the efficiency of our approach, we provide numerical comparisons with an IRPNM using a line search globalization and a modern FISTA-type method."],"url":"http://arxiv.org/abs/2404.02635v1","category":"math.OC"}
{"created":"2024-04-03 10:38:05","title":"Ensemble Deep Learning for enhanced seismic data reconstruction","abstract":"Seismic data often contain gaps due to various obstacles in the investigated area and recording instrument failures. Deep learning techniques offer promising solutions for reconstructing missing data parts by leveraging existing information. However, self-supervised methods frequently struggle with capturing under-represented features such as weaker events, crossing dips, and higher frequencies. To address these challenges, we propose a novel ensemble deep model along with a tailored self-supervised training approach for reconstructing seismic data with consecutive missing traces. Our model comprises two branches of U-nets, each fed from distinct data transformation modules aimed at amplifying under-represented features and promoting diversity among learners. Our loss function minimizes relative errors at the outputs of individual branches and the entire model, ensuring accurate reconstruction of various features while maintaining overall data integrity. Additionally, we employ masking while training to enhance sample diversity and memory efficiency. Application on two benchmark synthetic datasets and two real datasets demonstrates improved accuracy compared to a conventional U-net, successfully reconstructing weak events, diffractions, higher frequencies, and reflections obscured by groundroll. However, our method requires a threefold of training time compared to a simple U-net. An implementation of our method with TensorFlow is also made available.","sentences":["Seismic data often contain gaps due to various obstacles in the investigated area and recording instrument failures.","Deep learning techniques offer promising solutions for reconstructing missing data parts by leveraging existing information.","However, self-supervised methods frequently struggle with capturing under-represented features such as weaker events, crossing dips, and higher frequencies.","To address these challenges, we propose a novel ensemble deep model along with a tailored self-supervised training approach for reconstructing seismic data with consecutive missing traces.","Our model comprises two branches of U-nets, each fed from distinct data transformation modules aimed at amplifying under-represented features and promoting diversity among learners.","Our loss function minimizes relative errors at the outputs of individual branches and the entire model, ensuring accurate reconstruction of various features while maintaining overall data integrity.","Additionally, we employ masking while training to enhance sample diversity and memory efficiency.","Application on two benchmark synthetic datasets and two real datasets demonstrates improved accuracy compared to a conventional U-net, successfully reconstructing weak events, diffractions, higher frequencies, and reflections obscured by groundroll.","However, our method requires a threefold of training time compared to a simple U-net.","An implementation of our method with TensorFlow is also made available."],"url":"http://arxiv.org/abs/2404.02632v1","category":"physics.geo-ph"}
{"created":"2024-04-03 10:36:08","title":"Effector: A Python package for regional explanations","abstract":"Global feature effect methods explain a model outputting one plot per feature. The plot shows the average effect of the feature on the output, like the effect of age on the annual income. However, average effects may be misleading when derived from local effects that are heterogeneous, i.e., they significantly deviate from the average. To decrease the heterogeneity, regional effects provide multiple plots per feature, each representing the average effect within a specific subspace. For interpretability, subspaces are defined as hyperrectangles defined by a chain of logical rules, like age's effect on annual income separately for males and females and different levels of professional experience. We introduce Effector, a Python library dedicated to regional feature effects. Effector implements well-established global effect methods, assesses the heterogeneity of each method and, based on that, provides regional effects. Effector automatically detects subspaces where regional effects have reduced heterogeneity. All global and regional effect methods share a common API, facilitating comparisons between them. Moreover, the library's interface is extensible so new methods can be easily added and benchmarked. The library has been thoroughly tested, ships with many tutorials (https://xai-effector.github.io/) and is available under an open-source license at PyPi (https://pypi.org/project/effector/) and Github (https://github.com/givasile/effector).","sentences":["Global feature effect methods explain a model outputting one plot per feature.","The plot shows the average effect of the feature on the output, like the effect of age on the annual income.","However, average effects may be misleading when derived from local effects that are heterogeneous, i.e., they significantly deviate from the average.","To decrease the heterogeneity, regional effects provide multiple plots per feature, each representing the average effect within a specific subspace.","For interpretability, subspaces are defined as hyperrectangles defined by a chain of logical rules, like age's effect on annual income separately for males and females and different levels of professional experience.","We introduce Effector, a Python library dedicated to regional feature effects.","Effector implements well-established global effect methods, assesses the heterogeneity of each method and, based on that, provides regional effects.","Effector automatically detects subspaces where regional effects have reduced heterogeneity.","All global and regional effect methods share a common API, facilitating comparisons between them.","Moreover, the library's interface is extensible so new methods can be easily added and benchmarked.","The library has been thoroughly tested, ships with many tutorials (https://xai-effector.github.io/) and is available under an open-source license at PyPi (https://pypi.org/project/effector/) and Github (https://github.com/givasile/effector)."],"url":"http://arxiv.org/abs/2404.02629v1","category":"cs.LG"}
{"created":"2024-04-03 08:56:23","title":"Quantum computing approach to realistic ESG-friendly stock portfolios","abstract":"Finding an optimal balance between risk and returns in investment portfolios is a central challenge in quantitative finance, often addressed through Markowitz portfolio theory (MPT). While traditional portfolio optimization is carried out in a continuous fashion, as if stocks could be bought in fractional increments, practical implementations often resort to approximations, as fractional stocks are typically not tradeable. While these approximations are effective for large investment budgets, they deteriorate as budgets decrease. To alleviate this issue, a discrete Markowitz portfolio theory (DMPT) with finite budgets and integer stock weights can be formulated, but results in a non-polynomial (NP)-hard problem. Recent progress in quantum processing units (QPUs), including quantum annealers, makes solving DMPT problems feasible. Our study explores portfolio optimization on quantum annealers, establishing a mapping between continuous and discrete Markowitz portfolio theories. We find that correctly normalized discrete portfolios converge to continuous solutions as budgets increase. Our DMPT implementation provides efficient frontier solutions, outperforming traditional rounding methods, even for moderate budgets. Responding to the demand for environmentally and socially responsible investments, we enhance our discrete portfolio optimization with ESG (environmental, social, governance) ratings for EURO STOXX 50 index stocks. We introduce a utility function incorporating ESG ratings to balance risk, return, and ESG-friendliness, and discuss implications for ESG-aware investors.","sentences":["Finding an optimal balance between risk and returns in investment portfolios is a central challenge in quantitative finance, often addressed through Markowitz portfolio theory (MPT).","While traditional portfolio optimization is carried out in a continuous fashion, as if stocks could be bought in fractional increments, practical implementations often resort to approximations, as fractional stocks are typically not tradeable.","While these approximations are effective for large investment budgets, they deteriorate as budgets decrease.","To alleviate this issue, a discrete Markowitz portfolio theory (DMPT) with finite budgets and integer stock weights can be formulated, but results in a non-polynomial (NP)-hard problem.","Recent progress in quantum processing units (QPUs), including quantum annealers, makes solving DMPT problems feasible.","Our study explores portfolio optimization on quantum annealers, establishing a mapping between continuous and discrete Markowitz portfolio theories.","We find that correctly normalized discrete portfolios converge to continuous solutions as budgets increase.","Our DMPT implementation provides efficient frontier solutions, outperforming traditional rounding methods, even for moderate budgets.","Responding to the demand for environmentally and socially responsible investments, we enhance our discrete portfolio optimization with ESG (environmental, social, governance) ratings for EURO STOXX 50 index stocks.","We introduce a utility function incorporating ESG ratings to balance risk, return, and ESG-friendliness, and discuss implications for ESG-aware investors."],"url":"http://arxiv.org/abs/2404.02582v1","category":"q-fin.PM"}
{"created":"2024-04-03 08:51:45","title":"Interpretation of the horizontal beam response near the third integer resonance","abstract":"The beam response to an external periodic excitation delivers relevant information about the optics, tune distribution and stability of a circulating beam in a storage ring. In this contribution the horizontal beam response to the excitation (transfer function) under conditions typical for slow extraction is presented for a coasting beam. The resulting spectrum exhibits a splitting behaviour. The single particle dynamics is discussed and an interpretation based on simulation results is presented.","sentences":["The beam response to an external periodic excitation delivers relevant information about the optics, tune distribution and stability of a circulating beam in a storage ring.","In this contribution the horizontal beam response to the excitation (transfer function) under conditions typical for slow extraction is presented for a coasting beam.","The resulting spectrum exhibits a splitting behaviour.","The single particle dynamics is discussed and an interpretation based on simulation results is presented."],"url":"http://arxiv.org/abs/2404.02576v1","category":"physics.acc-ph"}
{"created":"2024-04-03 07:04:16","title":"Small diffusivity asymptotics for a linear parabolic SPDE in two space dimensions","abstract":"We consider parameter estimation of the reaction term for a second order linear parabolic stochastic partial differential equation in two space dimensions driven by a $Q$-Wiener process under small diffusivity. We first construct an estimator of the reaction parameter based on continuous spatio-temporal data, and then derive an estimator of the reaction parameter based on high frequency spatio-temporal data by discretizing the estimator based on the continuous data. We show that the estimators have consistency and asymptotic normality. Furthermore, we give simulation results of the estimator based on high frequency data.","sentences":["We consider parameter estimation of the reaction term for a second order linear parabolic stochastic partial differential equation in two space dimensions driven by a $Q$-Wiener process under small diffusivity.","We first construct an estimator of the reaction parameter based on continuous spatio-temporal data, and then derive an estimator of the reaction parameter based on high frequency spatio-temporal data by discretizing the estimator based on the continuous data.","We show that the estimators have consistency and asymptotic normality.","Furthermore, we give simulation results of the estimator based on high frequency data."],"url":"http://arxiv.org/abs/2404.02513v1","category":"math.ST"}
{"created":"2024-04-03 06:47:55","title":"Adaptation of the Phase Distance Correlation Periodogram to Account for Measurement Uncertainties","abstract":"We present an improvement of the phase distance correlation (PDC) periodogram to account for uncertainties in the time-series data. The PDC periodogram, which we introduced in previous papers, is based on the statistical concept of distance correlation. By viewing each measurement and its accompanying error estimate as a probability distribution, we use the concept of energy distance to design a distance function (metric) between measurement-uncertainty pairs. We use this metric as the basis for the PDC periodogram, instead of the simple absolute difference. We demonstrate the periodogram performance using both simulated and real-life data. This adaptation makes the PDC periodogram much more useful, and it can be helpful for the exploration of large time-resolved astronomical databases, from Gaia radial velocity and photometry data releases to those of smaller surveys, such as APOGEE and LAMOST. We have made a public GitHub repository with a Python implementation of the new tools available to the community.","sentences":["We present an improvement of the phase distance correlation (PDC) periodogram to account for uncertainties in the time-series data.","The PDC periodogram, which we introduced in previous papers, is based on the statistical concept of distance correlation.","By viewing each measurement and its accompanying error estimate as a probability distribution, we use the concept of energy distance to design a distance function (metric) between measurement-uncertainty pairs.","We use this metric as the basis for the PDC periodogram, instead of the simple absolute difference.","We demonstrate the periodogram performance using both simulated and real-life data.","This adaptation makes the PDC periodogram much more useful, and it can be helpful for the exploration of large time-resolved astronomical databases, from Gaia radial velocity and photometry data releases to those of smaller surveys, such as APOGEE and LAMOST.","We have made a public GitHub repository with a Python implementation of the new tools available to the community."],"url":"http://arxiv.org/abs/2404.02506v1","category":"astro-ph.IM"}
{"created":"2024-04-03 05:58:53","title":"Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment","abstract":"The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality.","sentences":["The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora.","This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models.","To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models.","This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking.","We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages.","In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality."],"url":"http://arxiv.org/abs/2404.02490v1","category":"cs.CL"}
{"created":"2024-04-03 04:40:57","title":"Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations","abstract":"Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language.","sentences":["Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss.","To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language.","In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT).","The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language.","Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning.","Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language."],"url":"http://arxiv.org/abs/2404.02452v1","category":"cs.CL"}
{"created":"2024-04-03 04:00:38","title":"Data-driven Optimization for Drone Delivery Service Planning with Online Demand","abstract":"In this study, we develop an innovative data-driven optimization approach to solve the drone delivery service planning problem with online demand. Drone-based logistics are expected to improve operations by enhancing flexibility and reducing congestion effects induced by last-mile deliveries. With rising digitalization and urbanization, however, logistics service providers are constantly grappling with the challenge of uncertain real-time demand. This study investigates the problem of planning drone delivery service through an urban air traffic network to fulfil online and stochastic demand. Customer requests, if accepted, generate profit and are serviced by individual drone flights as per request origins, destinations and time windows. We cast this stochastic optimization problem as a Markov decision process. We present a novel data-driven optimization approach which generates predictive prescriptions of parameters of a surrogate optimization formulation. Our solution method consists of synthesizing training data via lookahead simulations to train a supervised machine learning model for predicting relative link priority based on the state of the network. This knowledge is then leveraged to selectively create weighted reserve capacity in the network and via a surrogate objective function that controls the trade-off between reserve capacity and profit maximization to maximize the cumulative profit earned. Using numerical experiments based on benchmarking transportation networks, the resulting data-driven optimization policy is shown to outperform a myopic policy. Sensitivity analyses on learning parameters reveal insights into the design of efficient policies for drone delivery service planning with online demand.","sentences":["In this study, we develop an innovative data-driven optimization approach to solve the drone delivery service planning problem with online demand.","Drone-based logistics are expected to improve operations by enhancing flexibility and reducing congestion effects induced by last-mile deliveries.","With rising digitalization and urbanization, however, logistics service providers are constantly grappling with the challenge of uncertain real-time demand.","This study investigates the problem of planning drone delivery service through an urban air traffic network to fulfil online and stochastic demand.","Customer requests, if accepted, generate profit and are serviced by individual drone flights as per request origins, destinations and time windows.","We cast this stochastic optimization problem as a Markov decision process.","We present a novel data-driven optimization approach which generates predictive prescriptions of parameters of a surrogate optimization formulation.","Our solution method consists of synthesizing training data via lookahead simulations to train a supervised machine learning model for predicting relative link priority based on the state of the network.","This knowledge is then leveraged to selectively create weighted reserve capacity in the network and via a surrogate objective function that controls the trade-off between reserve capacity and profit maximization to maximize the cumulative profit earned.","Using numerical experiments based on benchmarking transportation networks, the resulting data-driven optimization policy is shown to outperform a myopic policy.","Sensitivity analyses on learning parameters reveal insights into the design of efficient policies for drone delivery service planning with online demand."],"url":"http://arxiv.org/abs/2404.02442v1","category":"math.OC"}
{"created":"2024-04-03 03:34:43","title":"In-situ tunable giant electrical anisotropy in a grating gated AlGaN/GaN two-dimensional electron gas","abstract":"Materials with in-plane electrical anisotropy have great potential for designing artificial synaptic devices. However, natural materials with strong intrinsic in-plane electrical anisotropy are rare. We introduce a simple strategy to produce extremely large electrical anisotropy via grating gating of a semiconductor two-dimensional electron gas (2DEG) of AlGaN/GaN. We show that periodically modulated electric potential in the 2DEG induces in-plane electrical anisotropy, which is significantly enhanced in a magnetic field, leading to an ultra large electrical anisotropy. This is induced by a giant positive magnetoresistance and a giant negative magnetoresistance under two orthogonally oriented in-plane current flows, respectively. This giant electrical anisotropy is in-situ tunable by tailoring both the grating gate voltage and the magnetic field. Our semiconductor device with controllable giant electrical anisotropy will stimulate new device applications, such as multi-terminal memtransistors and bionic synapses.","sentences":["Materials with in-plane electrical anisotropy have great potential for designing artificial synaptic devices.","However, natural materials with strong intrinsic in-plane electrical anisotropy are rare.","We introduce a simple strategy to produce extremely large electrical anisotropy via grating gating of a semiconductor two-dimensional electron gas (2DEG) of AlGaN/","GaN.","We show that periodically modulated electric potential in the 2DEG induces in-plane electrical anisotropy, which is significantly enhanced in a magnetic field, leading to an ultra large electrical anisotropy.","This is induced by a giant positive magnetoresistance and a giant negative magnetoresistance under two orthogonally oriented in-plane current flows, respectively.","This giant electrical anisotropy is in-situ tunable by tailoring both the grating gate voltage and the magnetic field.","Our semiconductor device with controllable giant electrical anisotropy will stimulate new device applications, such as multi-terminal memtransistors and bionic synapses."],"url":"http://arxiv.org/abs/2404.02427v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-03 03:27:01","title":"RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation","abstract":"Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair. Moreover, while parameter-efficient LoRA finetuning has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models. To tackle these challenges, we propose to Repair Sparse Vision-Language Models via Sparse Cross-modality Adaptation (RESSA). RESSA utilizes cross-modality finetuning to enhance task-specific performance and facilitate knowledge distillation from original dense models. Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models. Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3\\% improvement under 2:4 sparsity and a remarkable 47.6\\% enhancement under unstructured 70\\% sparsity.","sentences":["Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks.","However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios.","While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs.","To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance.","For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair.","Moreover, while parameter-efficient LoRA finetuning has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models.","To tackle these challenges, we propose to Repair Sparse Vision-Language Models via Sparse Cross-modality Adaptation (RESSA).","RESSA utilizes cross-modality finetuning to enhance task-specific performance and facilitate knowledge distillation from original dense models.","Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models.","Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3\\% improvement under 2:4 sparsity and a remarkable 47.6\\% enhancement under unstructured 70\\% sparsity."],"url":"http://arxiv.org/abs/2404.02424v1","category":"cs.LG"}
{"created":"2024-04-03 03:24:19","title":"Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data","abstract":"Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.","sentences":["Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks.","In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt.","In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL.","Our solution targets the low resource setting, i.e., when only 4 examples per class are available.","Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier.","Experimental results show that our approach leads to competitive results on multiple text classification datasets."],"url":"http://arxiv.org/abs/2404.02422v1","category":"cs.CL"}
{"created":"2024-04-03 02:26:16","title":"A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion","abstract":"Diffusion models have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions. However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning. To resolve this, we propose a unified framework utilizing diffusion inversion that enables multi-level editing capabilities for co-speech gesture generation without re-training. The method takes advantage of two key capabilities of invertible diffusion models. The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise. This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions. The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory. With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization. Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing.","sentences":["Diffusion models have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions.","However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning.","To resolve this, we propose a unified framework utilizing diffusion inversion that enables multi-level editing capabilities for co-speech gesture generation without re-training.","The method takes advantage of two key capabilities of invertible diffusion models.","The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise.","This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions.","The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory.","With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization.","Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing."],"url":"http://arxiv.org/abs/2404.02411v1","category":"cs.HC"}
{"created":"2024-04-03 02:21:46","title":"CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models","abstract":"Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev","sentences":["Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models.","This could present a significant obstacle for language community members and linguists to use NLP tools.","This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models.","CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data.","We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework.","Code is available at https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev"],"url":"http://arxiv.org/abs/2404.02408v1","category":"cs.CL"}
{"created":"2024-04-03 02:01:23","title":"Factors Affecting Terahertz Emission from InGaN Quantum Wells under Ultrafast Excitation","abstract":"InGaN quantum wells (QWs) grown on c-plane sapphire substrate experience strain due to the lattice mismatch. The strain generates a strong piezoelectric field in QWs that contributes to THz emission under ultrafast excitation. Physical parameters such as QW width, period number, and Indium concentration can affect the strength of the piezoelectric field and result in THz emission. Experimental parameters such as pump fluence, laser energy, excitation power, pump polarization angle, and incident angle can be tuned to further optimize the THz emission. This review summarizes the effects of physical and experimental parameters of THz emission on InGaN QWs. Comparison and relationship between photoluminescence properties and THz emission in QWs are given, which further explains the origin of THz emission in InGaN QWs.","sentences":["InGaN quantum wells (QWs) grown on c-plane sapphire substrate experience strain due to the lattice mismatch.","The strain generates a strong piezoelectric field in QWs that contributes to THz emission under ultrafast excitation.","Physical parameters such as QW width, period number, and Indium concentration can affect the strength of the piezoelectric field and result in THz emission.","Experimental parameters such as pump fluence, laser energy, excitation power, pump polarization angle, and incident angle can be tuned to further optimize the THz emission.","This review summarizes the effects of physical and experimental parameters of THz emission on InGaN QWs.","Comparison and relationship between photoluminescence properties and THz emission in QWs are given, which further explains the origin of THz emission in InGaN QWs."],"url":"http://arxiv.org/abs/2404.02398v1","category":"physics.optics"}
{"created":"2024-04-03 00:59:56","title":"Entropy production and efficiency enhancement in quantum Otto engines operating at negative temperatures","abstract":"Cyclic classical and quantum thermal machines show higher efficiency when the strokes are carried out quasi-statically. Recent theoretical and experimental work on figures of merit for thermal machines show that they have an advantage when operating in environments with negative temperatures. In an experimental proof of concept [Phys. Rev. Lett. 122, 240602 (2019)], it was shown that quantum Otto engines operating at negative temperatures can exhibit a behavior in which the faster the cycle is carried out, the higher the efficiency. In this work, we make use of the concept of entropy production and friction work to explain this counterintuitive behavior, and we show that it only occurs when reservoirs have negative temperatures.","sentences":["Cyclic classical and quantum thermal machines show higher efficiency when the strokes are carried out quasi-statically.","Recent theoretical and experimental work on figures of merit for thermal machines show that they have an advantage when operating in environments with negative temperatures.","In an experimental proof of concept [Phys. Rev. Lett.","122, 240602 (2019)]",", it was shown that quantum Otto engines operating at negative temperatures can exhibit a behavior in which the faster the cycle is carried out, the higher the efficiency.","In this work, we make use of the concept of entropy production and friction work to explain this counterintuitive behavior, and we show that it only occurs when reservoirs have negative temperatures."],"url":"http://arxiv.org/abs/2404.02385v1","category":"quant-ph"}
{"created":"2024-04-03 00:44:55","title":"Diamond principles and Tukey-top ultrafilters on a countable set","abstract":"We provide two types of guessing principles for ultrafilter ($\\diamondsuit^{-}_{\\lambda}(U), \\ \\diamondsuit^p_\\lambda(U)$) on $\\omega$ which form subclasses of Tukey-top ultrafilters, and construct such ultrafilters in $ZFC$. These constructions are essentially different from Isbell's construction \\cite{Isbell65} of Tukey-top ultrafilters. We prove using the Borel-Cantelli Lemma that full guessing is not possible and rule out several stronger guessing principles e.g. we prove that no Dodd-sound ultrafilters exist on $\\omega$. We then apply these guessing principles to force a $q$-point which is Tukey-top (answering a question from \\cite{Benhanou/Dobrinen23}), and prove that the class of ultrafilters which satisfy $\\neg\\diamondsuit^{-}_\\lambda$ is closed under Fubini sum. Finally, we show that $\\diamondsuit^{-}_\\lambda$ and $\\diamondsuit^p_\\lambda$ can be separated.","sentences":["We provide two types of guessing principles for ultrafilter ($\\diamondsuit^{-}_{\\lambda}(U), \\ \\diamondsuit^p_\\lambda(U)$) on $\\omega$ which form subclasses of Tukey-top ultrafilters, and construct such ultrafilters in $ZFC$. These constructions are essentially different from Isbell's construction \\cite{Isbell65} of Tukey-top ultrafilters.","We prove using the Borel-Cantelli Lemma that full guessing is not possible and rule out several stronger guessing principles e.g. we prove that no Dodd-sound ultrafilters exist on $\\omega$. We then apply these guessing principles to force a $q$-point which is Tukey-top (answering a question from \\cite{Benhanou/Dobrinen23}), and prove that the class of ultrafilters which satisfy $\\neg\\diamondsuit^{-}_\\lambda$ is closed under Fubini sum.","Finally, we show that $\\diamondsuit^{-}_\\lambda$ and $\\diamondsuit^p_\\lambda$ can be separated."],"url":"http://arxiv.org/abs/2404.02379v1","category":"math.LO"}
{"created":"2024-04-03 00:41:19","title":"Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation","abstract":"We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions. Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation. The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated SGD under the strong growth condition. In this special case, our analysis reduces the dependence on the strong growth constant from $\\rho$ to $\\sqrt{\\rho}$ as compared to prior work. This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for SGD.","sentences":["We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions.","Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation.","The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated SGD under the strong growth condition.","In this special case, our analysis reduces the dependence on the strong growth constant from $\\rho$ to $\\sqrt{\\rho}$ as compared to prior work.","This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for SGD."],"url":"http://arxiv.org/abs/2404.02378v1","category":"math.OC"}
{"created":"2024-04-03 00:19:12","title":"Size-Mass Relations for Simulated Low-Mass Galaxies: Mock Imaging versus Intrinsic Properties","abstract":"The observationally-inferred size versus stellar-mass relationship for low-mass galaxies provides an important test for galaxy formation models. However, the relationship relies on assumptions that relate observed luminosity profiles to underlying stellar mass profiles. We use the Feedback in Realistic Environments (FIRE-2) simulations of low-mass galaxies to explore how the predicted size-mass relation (SMR) changes depending on whether one uses star-particle counts directly or mock observations. We reproduce the SMR found in the ELVES survey remarkably well only when we infer stellar masses and sizes using mock surface brightness images and the same color-inferred mass-to-light ratio (CMLR) used in deriving the observed relation. However, when we use star particles to directly infer stellar masses and half-mass radii, we find that our galaxies are too large and obey a SMR with too little scatter compared to observations. The reason for this discrepancy between the \"true\" galaxy size and mass and those derived in the mock observation approach is twofold. First, our simulated galaxies have higher and more varied MLRs at a fixed color than those commonly-adopted because their star-formation-histories are more temporally extended and not well represented by exponential star formation models. Using a standard CMLR therefore tends to underestimate their stellar masses compared to their true, simulated values. Second, our galaxies have radially increasing MLR gradients. Using a single MLR tends to under-predict the mass in the outer regions. Similarly, the true half-mass radius is larger than the half-light radius because the light is more concentrated than the mass. If our simulations are accurate representations of the real universe, then the relationship between galaxy size and stellar mass is even tighter for low-mass galaxies than is commonly inferred from observed relations.","sentences":["The observationally-inferred size versus stellar-mass relationship for low-mass galaxies provides an important test for galaxy formation models.","However, the relationship relies on assumptions that relate observed luminosity profiles to underlying stellar mass profiles.","We use the Feedback in Realistic Environments (FIRE-2) simulations of low-mass galaxies to explore how the predicted size-mass relation (SMR) changes depending on whether one uses star-particle counts directly or mock observations.","We reproduce the SMR found in the ELVES survey remarkably well only when we infer stellar masses and sizes using mock surface brightness images and the same color-inferred mass-to-light ratio (CMLR) used in deriving the observed relation.","However, when we use star particles to directly infer stellar masses and half-mass radii, we find that our galaxies are too large and obey a SMR with too little scatter compared to observations.","The reason for this discrepancy between the \"true\" galaxy size and mass and those derived in the mock observation approach is twofold.","First, our simulated galaxies have higher and more varied MLRs at a fixed color than those commonly-adopted because their star-formation-histories are more temporally extended and not well represented by exponential star formation models.","Using a standard CMLR therefore tends to underestimate their stellar masses compared to their true, simulated values.","Second, our galaxies have radially increasing MLR gradients.","Using a single MLR tends to under-predict the mass in the outer regions.","Similarly, the true half-mass radius is larger than the half-light radius because the light is more concentrated than the mass.","If our simulations are accurate representations of the real universe, then the relationship between galaxy size and stellar mass is even tighter for low-mass galaxies than is commonly inferred from observed relations."],"url":"http://arxiv.org/abs/2404.02373v1","category":"astro-ph.GA"}
{"created":"2024-04-02 23:34:39","title":"Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds","abstract":"Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with distribution shift (TDS learning), where a learner is given labeled samples from training distribution $\\mathcal{D}$, unlabeled samples from test distribution $\\mathcal{D}'$, and the goal is to output a classifier with low error on $\\mathcal{D}'$ whenever the training samples pass a corresponding test. Their model deviates from all prior work in that no assumptions are made on $\\mathcal{D}'$. Instead, the test must accept (with high probability) when the marginals of the training and test distributions are equal.   Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training distributions and prove a variety of new upper bounds including a $2^{(k/\\epsilon)^{O(1)}} \\mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\\epsilon$ (prior work achieved $d^{(k/\\epsilon)^{O(1)}}$). We work under the mild assumption that the Gaussian training distribution contains at least an $\\epsilon$ fraction of both positive and negative examples ($\\epsilon$-balanced). We also prove the first set of SQ lower-bounds for any TDS learning problem and show (1) the $\\epsilon$-balanced assumption is necessary for $\\mathsf{poly}(d,1/\\epsilon)$-time TDS learning for a single halfspace and (2) a $d^{\\tilde{\\Omega}(\\log 1/\\epsilon)}$ lower bound for the intersection of two general halfspaces, even with the $\\epsilon$-balanced assumption.   Our techniques significantly expand the toolkit for TDS learning. We use dimension reduction and coverings to give efficient algorithms for computing a localized version of discrepancy distance, a key metric from the domain adaptation literature.","sentences":["Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with distribution shift (TDS learning), where a learner is given labeled samples from training distribution $\\mathcal{D}$, unlabeled samples from test distribution $\\mathcal{D}'$, and the goal is to output a classifier with low error on $\\mathcal{D}'$ whenever the training samples pass a corresponding test.","Their model deviates from all prior work in that no assumptions are made on $\\mathcal{D}'$. Instead, the test must accept (with high probability) when the marginals of the training and test distributions are equal.   ","Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training distributions and prove a variety of new upper bounds including a $2^{(k/\\epsilon)^{O(1)}} \\mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\\epsilon$ (prior work achieved $d^{(k/\\epsilon)^{O(1)}}$).","We work under the mild assumption that the Gaussian training distribution contains at least an $\\epsilon$ fraction of both positive and negative examples ($\\epsilon$-balanced).","We also prove the first set of SQ lower-bounds for any TDS learning problem and show (1) the $\\epsilon$-balanced assumption is necessary for $\\mathsf{poly}(d,1/\\epsilon)$-time TDS learning for a single halfspace and (2) a $d^{\\tilde{\\Omega}(\\log 1/\\epsilon)}$ lower bound for the intersection of two general halfspaces, even with the $\\epsilon$-balanced assumption.   ","Our techniques significantly expand the toolkit for TDS learning.","We use dimension reduction and coverings to give efficient algorithms for computing a localized version of discrepancy distance, a key metric from the domain adaptation literature."],"url":"http://arxiv.org/abs/2404.02364v1","category":"cs.DS"}
{"created":"2024-04-02 22:51:58","title":"Thermodynamic formulation of vacuum energy density in flat spacetime and potential implications for the cosmological constant","abstract":"We propose a thermodynamical definition of the vacuum energy density $\\rho_{\\rm vac}$, defined as $\\langle 0| T_{\\mu\\nu} |0\\rangle = - \\rho_{\\rm vac} \\, g_{\\mu\\nu}$, in quantum field theory in flat Minkowski space in $D$ spacetime dimensions, which can be computed in the limit of high temperature, namely in the limit $\\beta = 1/T \\to 0$. It takes the form $\\rho_{\\rm vac} = {\\rm const} \\cdot m^D$ where $m$ is a fundamental mass scale and ${\\rm \"const\"}$ is a computable constant which can be positive or negative. Due to modular invariance $\\rho_{\\rm vac}$ can also be computed in a different non-thermodynamic channel where one spatial dimension is compactifed on a circle of circumference $\\beta$ and we confirm this modularity for free massive theories for both bosons and fermions for $D=2,3,4$. We list various properties of $\\rho_{\\rm vac}$ that are generally required, for instance $\\rho_{\\rm vac}=0$ for conformal field theories, and others, such as the constraint that $\\rho_{\\rm vac}$ has opposite signs for free bosons verses fermions of the same mass, which is related to constraints from supersymmetry. Using the Thermodynamic Bethe Ansatz we compute $\\rho_{\\rm vac}$ exactly for 2 classes of integrable QFT's in $2D$ and interpreting some previously known results. We apply our definition of $\\rho_{\\rm vac}$ to Lattice QCD data with two light quarks (up and down) and one additional massive flavor (the strange quark), and find it is negative, $\\rho_{\\rm vac} \\approx - ( 200 \\, {\\rm MeV} )^4$. Finally we make some remarks on the Cosmological Constant Problem since $\\rho_{\\rm vac}$ is central to any discussion of it.","sentences":["We propose a thermodynamical definition of the vacuum energy density $\\rho_{\\rm vac}$, defined as $\\langle 0| T_{\\mu\\nu} |0\\rangle = - \\rho_{\\rm vac} \\, g_{\\mu\\nu}$, in quantum field theory in flat Minkowski space in $D$ spacetime dimensions, which can be computed in the limit of high temperature, namely in the limit $\\beta = 1/T \\to 0$.","It takes the form $\\rho_{\\rm vac} = {\\rm const} \\cdot m^D$ where $m$ is a fundamental mass scale and ${\\rm \"const\"}$ is a computable constant which can be positive or negative.","Due to modular invariance $\\rho_{\\rm vac}$ can also be computed in a different non-thermodynamic channel where one spatial dimension is compactifed on a circle of circumference $\\beta$ and we confirm this modularity for free massive theories for both bosons and fermions for $D=2,3,4$. We list various properties of $\\rho_{\\rm vac}$ that are generally required, for instance $\\rho_{\\rm vac}=0$ for conformal field theories, and others, such as the constraint that $\\rho_{\\rm vac}$ has opposite signs for free bosons verses fermions of the same mass, which is related to constraints from supersymmetry.","Using the Thermodynamic Bethe Ansatz we compute $\\rho_{\\rm vac}$ exactly for 2 classes of integrable QFT's in $2D$ and interpreting some previously known results.","We apply our definition of $\\rho_{\\rm vac}$ to Lattice QCD data with two light quarks (up and down) and one additional massive flavor (the strange quark), and find it is negative, $\\rho_{\\rm vac} \\approx - ( 200 \\, {\\rm MeV} )^4$.","Finally we make some remarks on the Cosmological Constant Problem since $\\rho_{\\rm vac}$ is central to any discussion of it."],"url":"http://arxiv.org/abs/2404.02350v1","category":"hep-th"}
{"created":"2024-04-02 22:37:22","title":"Improved model-free bounds for multi-asset options using option-implied information and deep learning","abstract":"We consider the computation of model-free bounds for multi-asset options in a setting that combines dependence uncertainty with additional information on the dependence structure. More specifically, we consider the setting where the marginal distributions are known and partial information, in the form of known prices for multi-asset options, is also available in the market. We provide a fundamental theorem of asset pricing in this setting, as well as a superhedging duality that allows to transform the maximization problem over probability measures in a more tractable minimization problem over trading strategies. The latter is solved using a penalization approach combined with a deep learning approximation using artificial neural networks. The numerical method is fast and the computational time scales linearly with respect to the number of traded assets. We finally examine the significance of various pieces of additional information. Empirical evidence suggests that \"relevant\" information, i.e. prices of derivatives with the same payoff structure as the target payoff, are more useful that other information, and should be prioritized in view of the trade-off between accuracy and computational efficiency.","sentences":["We consider the computation of model-free bounds for multi-asset options in a setting that combines dependence uncertainty with additional information on the dependence structure.","More specifically, we consider the setting where the marginal distributions are known and partial information, in the form of known prices for multi-asset options, is also available in the market.","We provide a fundamental theorem of asset pricing in this setting, as well as a superhedging duality that allows to transform the maximization problem over probability measures in a more tractable minimization problem over trading strategies.","The latter is solved using a penalization approach combined with a deep learning approximation using artificial neural networks.","The numerical method is fast and the computational time scales linearly with respect to the number of traded assets.","We finally examine the significance of various pieces of additional information.","Empirical evidence suggests that \"relevant\" information, i.e. prices of derivatives with the same payoff structure as the target payoff, are more useful that other information, and should be prioritized in view of the trade-off between accuracy and computational efficiency."],"url":"http://arxiv.org/abs/2404.02343v1","category":"q-fin.PR"}
{"created":"2024-04-02 21:51:41","title":"Integrability of Nonabelian Differential-Difference Equations: the Symmetry Approach","abstract":"We propose a novel approach to tackle integrability problem for evolutionary differential-difference equations (D$\\Delta$Es) on free associative algebras, also referred to as nonabelian D$\\Delta$Es. This approach enables us to derive necessary integrability conditions, determine the integrability of a given equation, and make progress in the classification of integrable nonabelian D$\\Delta$Es. This work involves establishing symbolic representations for the nonabelian difference algebra, difference operators, and formal series, as well as introducing a novel quasi-local extension for the algebra of formal series within the context of symbolic representations. Applying this formalism, we solve the classification problem of integrable skew-symmetric quasi-linear nonabelian equations of orders $(-1,1)$, $(-2,2)$, and $(-3,3)$, consequently revealing some new equations in the process.","sentences":["We propose a novel approach to tackle integrability problem for evolutionary differential-difference equations (D$\\Delta$Es) on free associative algebras, also referred to as nonabelian D$\\Delta$Es.","This approach enables us to derive necessary integrability conditions, determine the integrability of a given equation, and make progress in the classification of integrable nonabelian D$\\Delta$Es.","This work involves establishing symbolic representations for the nonabelian difference algebra, difference operators, and formal series, as well as introducing a novel quasi-local extension for the algebra of formal series within the context of symbolic representations.","Applying this formalism, we solve the classification problem of integrable skew-symmetric quasi-linear nonabelian equations of orders $(-1,1)$, $(-2,2)$, and $(-3,3)$, consequently revealing some new equations in the process."],"url":"http://arxiv.org/abs/2404.02326v1","category":"nlin.SI"}
{"created":"2024-04-02 21:32:31","title":"From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization","abstract":"Understanding and communicating data uncertainty is crucial for making informed decisions in sectors like finance and healthcare. Previous work has explored how to express uncertainty in various modes. For example, uncertainty can be expressed visually with quantile dot plots or linguistically with hedge words and prosody. Our research aims to systematically explore how variations within each mode contribute to communicating uncertainty to the user; this allows us to better understand each mode's affordances and limitations. We completed an exploration of the uncertainty design space based on pilot studies and ran two crowdsourced experiments examining how speech, text, and visualization modes and variants within them impact decision-making with uncertain data. Visualization and text were most effective for rational decision-making, though text resulted in lower confidence. Speech garnered the highest trust despite sometimes leading to risky decisions. Results from these studies indicate meaningful trade-offs among modes of information and encourage exploration of multimodal data representations.","sentences":["Understanding and communicating data uncertainty is crucial for making informed decisions in sectors like finance and healthcare.","Previous work has explored how to express uncertainty in various modes.","For example, uncertainty can be expressed visually with quantile dot plots or linguistically with hedge words and prosody.","Our research aims to systematically explore how variations within each mode contribute to communicating uncertainty to the user; this allows us to better understand each mode's affordances and limitations.","We completed an exploration of the uncertainty design space based on pilot studies and ran two crowdsourced experiments examining how speech, text, and visualization modes and variants within them impact decision-making with uncertain data.","Visualization and text were most effective for rational decision-making, though text resulted in lower confidence.","Speech garnered the highest trust despite sometimes leading to risky decisions.","Results from these studies indicate meaningful trade-offs among modes of information and encourage exploration of multimodal data representations."],"url":"http://arxiv.org/abs/2404.02317v1","category":"cs.HC"}
{"created":"2024-04-02 21:24:13","title":"Evolution of Berry Phase and Half-Metallicity in Cr$_2$Te$_3$ in Response to Strain, Filling, Thickness, and Surface Termination","abstract":"Cr$_2$Te$_3$ is a ferromagnetic, quasi-two-dimensional layered material with perpendicular magnetic anisotropy, strong spin-orbit coupling, and non-trivial band topology. The non-trivial topology results in an intrinsic anomalous Hall conductivity (AHC) that switches sign under filling and biaxial strain. Thin films can exhibit half metallicity. Using density functional theory combined with maximally localized Wannier functions, we reveal the physical origins of the sensitivity of the sign of the AHC to strain and filling, and we determine the effect of surface termination on the half metallicity. We find that thin films terminated on the Te layers are the most energetically stable, but only the thin films terminated on both sides with the partially occupied Cr layers are half metals. In bulk Cr$_2$Te$_3$, the sensitivity of the sign of the AHC to strain and filling results from the complex Fermi surface comprised of three bands. Filling of local minima and bands near anti-crossings alters the local Berry curvature consistent with the negative to positive switching of the AHC. Similarly, strain depopulates a local minimum, shifts a degenerate point closer to the Fermi energy, and causes two spin-orbit split bands to reverse their order. These findings provide a physical understanding of the evolution of the Berry phase, AHC, and half-metallicity in Cr$_2$Te$_3$.","sentences":["Cr$_2$Te$_3$ is a ferromagnetic, quasi-two-dimensional layered material with perpendicular magnetic anisotropy, strong spin-orbit coupling, and non-trivial band topology.","The non-trivial topology results in an intrinsic anomalous Hall conductivity (AHC) that switches sign under filling and biaxial strain.","Thin films can exhibit half metallicity.","Using density functional theory combined with maximally localized Wannier functions, we reveal the physical origins of the sensitivity of the sign of the AHC to strain and filling, and we determine the effect of surface termination on the half metallicity.","We find that thin films terminated on the Te layers are the most energetically stable, but only the thin films terminated on both sides with the partially occupied Cr layers are half metals.","In bulk Cr$_2$Te$_3$, the sensitivity of the sign of the AHC to strain and filling results from the complex Fermi surface comprised of three bands.","Filling of local minima and bands near anti-crossings alters the local Berry curvature consistent with the negative to positive switching of the AHC.","Similarly, strain depopulates a local minimum, shifts a degenerate point closer to the Fermi energy, and causes two spin-orbit split bands to reverse their order.","These findings provide a physical understanding of the evolution of the Berry phase, AHC, and half-metallicity in Cr$_2$Te$_3$."],"url":"http://arxiv.org/abs/2404.02315v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 21:19:50","title":"Optimal combination of composite likelihoods using approximate Bayesian computation with application to state-space models","abstract":"Composite likelihood provides approximate inference when the full likelihood is intractable and sub-likelihood functions of marginal events can be evaluated relatively easily. It has been successfully applied for many complex models. However, its wider application is limited by two issues. First, weight selection of marginal likelihood can have a significant impact on the information efficiency and is currently an open question. Second, calibrated Bayesian inference with composite likelihood requires curvature adjustment which is difficult for dependent data. This work shows that approximate Bayesian computation (ABC) can properly address these two issues by using multiple composite score functions as summary statistics. First, the summary-based posterior distribution gives the optimal Godambe information among a wide class of estimators defined by linear combinations of estimating functions. Second, to make ABC computationally feasible for models where marginal likelihoods have no closed form, a novel approach is proposed to estimate all simulated marginal scores using a Monte Carlo sample with size N. Sufficient conditions are given for the additional noise to be negligible with N fixed as the data size n goes to infinity, and the computational cost is O(n). Third, asymptotic properties of ABC with summary statistics having heterogeneous convergence rates is derived, and an adaptive scheme to choose the component composite scores is proposed. Numerical studies show that the new method significantly outperforms the existing Bayesian composite likelihood methods, and the efficiency of adaptively combined composite scores well approximates the efficiency of particle MCMC using the full likelihood.","sentences":["Composite likelihood provides approximate inference when the full likelihood is intractable and sub-likelihood functions of marginal events can be evaluated relatively easily.","It has been successfully applied for many complex models.","However, its wider application is limited by two issues.","First, weight selection of marginal likelihood can have a significant impact on the information efficiency and is currently an open question.","Second, calibrated Bayesian inference with composite likelihood requires curvature adjustment which is difficult for dependent data.","This work shows that approximate Bayesian computation (ABC) can properly address these two issues by using multiple composite score functions as summary statistics.","First, the summary-based posterior distribution gives the optimal Godambe information among a wide class of estimators defined by linear combinations of estimating functions.","Second, to make ABC computationally feasible for models where marginal likelihoods have no closed form, a novel approach is proposed to estimate all simulated marginal scores using a Monte Carlo sample with size N. Sufficient conditions are given for the additional noise to be negligible with N fixed as the data size n goes to infinity, and the computational cost is O(n).","Third, asymptotic properties of ABC with summary statistics having heterogeneous convergence rates is derived, and an adaptive scheme to choose the component composite scores is proposed.","Numerical studies show that the new method significantly outperforms the existing Bayesian composite likelihood methods, and the efficiency of adaptively combined composite scores well approximates the efficiency of particle MCMC using the full likelihood."],"url":"http://arxiv.org/abs/2404.02313v1","category":"stat.ME"}
{"created":"2024-04-02 21:10:46","title":"A Survey of Web Content Control for Generative AI","abstract":"The groundbreaking advancements around generative AI have recently caused a wave of concern culminating in a row of lawsuits, including high-profile actions against Stability AI and OpenAI. This situation of legal uncertainty has sparked a broad discussion on the rights of content creators and publishers to protect their intellectual property on the web. European as well as US law already provides rough guidelines, setting a direction for technical solutions to regulate web data use. In this course, researchers and practitioners have worked on numerous web standards and opt-out formats that empower publishers to keep their data out of the development of generative AI models. The emerging AI/ML opt-out protocols are valuable in regards to data sovereignty, but again, it creates an adverse situation for a site owners who are overwhelmed by the multitude of recent ad hoc standards to consider. In our work, we want to survey the different proposals, ideas and initiatives, and provide a comprehensive legal and technical background in the context of the current discussion on web publishers control.","sentences":["The groundbreaking advancements around generative AI have recently caused a wave of concern culminating in a row of lawsuits, including high-profile actions against Stability AI and OpenAI.","This situation of legal uncertainty has sparked a broad discussion on the rights of content creators and publishers to protect their intellectual property on the web.","European as well as US law already provides rough guidelines, setting a direction for technical solutions to regulate web data use.","In this course, researchers and practitioners have worked on numerous web standards and opt-out formats that empower publishers to keep their data out of the development of generative AI models.","The emerging AI/ML opt-out protocols are valuable in regards to data sovereignty, but again, it creates an adverse situation for a site owners who are overwhelmed by the multitude of recent ad hoc standards to consider.","In our work, we want to survey the different proposals, ideas and initiatives, and provide a comprehensive legal and technical background in the context of the current discussion on web publishers control."],"url":"http://arxiv.org/abs/2404.02309v1","category":"cs.IR"}
{"created":"2024-04-02 20:55:39","title":"CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks","abstract":"Graph neural networks have been shown successful in recent years. While different GNN architectures and training systems have been developed, GNN training on large-scale real-world graphs still remains challenging. Existing distributed systems load the entire graph in memory for graph partitioning, requiring a huge memory space to process large graphs and thus hindering GNN training on such large graphs using commodity workstations. In this paper, we propose CATGNN, a cost-efficient and scalable distributed GNN training system which focuses on scaling GNN training to billion-scale or larger graphs under limited computational resources. Among other features, it takes a stream of edges as input, instead of loading the entire graph in memory, for partitioning. We also propose a novel streaming partitioning algorithm named SPRING for distributed GNN training. We verify the correctness and effectiveness of CATGNN with SPRING on 16 open datasets. In particular, we demonstrate that CATGNN can handle the largest publicly available dataset with limited memory, which would have been infeasible without increasing the memory space. SPRING also outperforms state-of-the-art partitioning algorithms significantly, with a 50% reduction in replication factor on average.","sentences":["Graph neural networks have been shown successful in recent years.","While different GNN architectures and training systems have been developed, GNN training on large-scale real-world graphs still remains challenging.","Existing distributed systems load the entire graph in memory for graph partitioning, requiring a huge memory space to process large graphs and thus hindering GNN training on such large graphs using commodity workstations.","In this paper, we propose CATGNN, a cost-efficient and scalable distributed GNN training system which focuses on scaling GNN training to billion-scale or larger graphs under limited computational resources.","Among other features, it takes a stream of edges as input, instead of loading the entire graph in memory, for partitioning.","We also propose a novel streaming partitioning algorithm named SPRING for distributed GNN training.","We verify the correctness and effectiveness of CATGNN with SPRING on 16 open datasets.","In particular, we demonstrate that CATGNN can handle the largest publicly available dataset with limited memory, which would have been infeasible without increasing the memory space.","SPRING also outperforms state-of-the-art partitioning algorithms significantly, with a 50% reduction in replication factor on average."],"url":"http://arxiv.org/abs/2404.02300v1","category":"cs.LG"}
{"created":"2024-04-02 20:49:40","title":"Output Feedback Periodic Event-triggered Control of Coupled $2\\times 2$ Linear Hyperbolic PDEs","abstract":"This article introduces an observer-based periodic event-triggered control (PETC) strategy for boundary control of a system characterized by $2\\times2$ linear hyperbolic partial differential equations (PDEs). An anti-collocated actuation and sensing configuration is considered, and an exponentially convergent observer for state estimation from boundary data is designed. Initially, a continuous-time dynamic event-triggering mechanism requiring constant monitoring of the triggering function is developed. This mechanism is subsequently adapted into a periodic event-triggering scheme, which necessitates only periodic monitoring to identify when the control input needs updating. The underlying control approach is the PDE backstepping boundary control, implemented in a zero-order hold manner between events. This result marks a substantial improvement over conventional observer-based continuous-time event-triggered control for linear coupled hyperbolic PDEs by removing the requirement for constant monitoring of the triggering function. With the triggering function evaluated periodically, the closed-loop system is inherently free from Zeno behavior. It is demonstrated that under the proposed PETC, the closed-loop system globally exponentially converges to zero in the spatial $L^2$ norm. A simulation study illustrating the theoretical results is presented.","sentences":["This article introduces an observer-based periodic event-triggered control (PETC) strategy for boundary control of a system characterized by $2\\times2$ linear hyperbolic partial differential equations (PDEs).","An anti-collocated actuation and sensing configuration is considered, and an exponentially convergent observer for state estimation from boundary data is designed.","Initially, a continuous-time dynamic event-triggering mechanism requiring constant monitoring of the triggering function is developed.","This mechanism is subsequently adapted into a periodic event-triggering scheme, which necessitates only periodic monitoring to identify when the control input needs updating.","The underlying control approach is the PDE backstepping boundary control, implemented in a zero-order hold manner between events.","This result marks a substantial improvement over conventional observer-based continuous-time event-triggered control for linear coupled hyperbolic PDEs by removing the requirement for constant monitoring of the triggering function.","With the triggering function evaluated periodically, the closed-loop system is inherently free from Zeno behavior.","It is demonstrated that under the proposed PETC, the closed-loop system globally exponentially converges to zero in the spatial $L^2$ norm.","A simulation study illustrating the theoretical results is presented."],"url":"http://arxiv.org/abs/2404.02298v1","category":"math.OC"}
{"created":"2024-04-02 20:48:20","title":"Kaon mixing beyond the standard model with physical masses","abstract":"We present non-perturbative results for beyond the standard model kaon mixing matrix elements in the isospin symmetric limit ($m_u=m_d$) of QCD, including a complete estimate of all dominant sources of systematic error. Our results are obtained from numerical simulations of lattice QCD with $N_f = 2+1$ flavours of dynamical domain wall fermions. For the first time, these quantities are simulated directly at the physical pion mass $m_\\pi$~$\\sim$~$139\\,\\mathrm{MeV}$ for two different lattice spacings. We include data at three lattice spacings in the range $a = 0.11 $ - $ 0.07\\,\\mathrm{fm}$ and with pion masses ranging from the physical value up to 450$\\,\\mathrm{MeV}$. Compared to our earlier work, we have added both direct calculations at physical quark masses and a third lattice spacing making the removal of discretisation effects significantly more precise and eliminating the need for any significant mass extrapolation beyond the range of simulated data. We renormalise the lattice operators non-perturbatively using RI-SMOM off-shell schemes. These schemes eliminate the need to model and subtract non-perturbative pion poles that arises in the RI-MOM scheme and, since the calculations are performed with domain wall fermions, the unphysical mixing between chirality sectors is suppressed. Our results for the bag parameters in the $\\overline{\\mathrm{MS}}$ scheme at $3\\,\\mathrm{GeV}$ are $B_K~\\equiv~\\mathcal{B}_1 = 0.5240(17)(54)$, $\\mathcal{B}_2 = 0.4794(25)(35)$, $\\mathcal{B}_3 = 0.746(13)(17)$, $\\mathcal{B}_4 = 0.897(02)(10)$ and $\\mathcal{B}_5 = 0.6882(78)(94)$, where the first error is from lattice uncertainties and the second is the uncertainty due to the perturbative matching to $\\overline{\\mathrm{MS}}$.","sentences":["We present non-perturbative results for beyond the standard model kaon mixing matrix elements in the isospin symmetric limit ($m_u=m_d$) of QCD, including a complete estimate of all dominant sources of systematic error.","Our results are obtained from numerical simulations of lattice QCD with $N_f = 2+1$ flavours of dynamical domain wall fermions.","For the first time, these quantities are simulated directly at the physical pion mass $m_\\pi$~$\\sim$~$139\\,\\mathrm{MeV}$ for two different lattice spacings.","We include data at three lattice spacings in the range $a = 0.11 $ - $ 0.07\\,\\mathrm{fm}$ and with pion masses ranging from the physical value up to 450$\\,\\mathrm{MeV}$. Compared to our earlier work, we have added both direct calculations at physical quark masses and a third lattice spacing making the removal of discretisation effects significantly more precise and eliminating the need for any significant mass extrapolation beyond the range of simulated data.","We renormalise the lattice operators non-perturbatively using RI-SMOM off-shell schemes.","These schemes eliminate the need to model and subtract non-perturbative pion poles that arises in the RI-MOM scheme and, since the calculations are performed with domain wall fermions, the unphysical mixing between chirality sectors is suppressed.","Our results for the bag parameters in the $\\overline{\\mathrm{MS}}$ scheme at $3\\,\\mathrm{GeV}$ are $B_K~\\equiv~\\mathcal{B}_1 = 0.5240(17)(54)$, $\\mathcal{B}_2 = 0.4794(25)(35)$, $\\mathcal{B}_3 = 0.746(13)(17)$, $\\mathcal{B}_4 = 0.897(02)(10)$ and $\\mathcal{B}_5 = 0.6882(78)(94)$, where the first error is from lattice uncertainties and the second is the uncertainty due to the perturbative matching to $\\overline{\\mathrm{MS}}$."],"url":"http://arxiv.org/abs/2404.02297v1","category":"hep-lat"}
{"created":"2024-04-02 20:44:52","title":"Licking the plate: dusty star-forming galaxies buried in the ALMA calibration data","abstract":"Deep, unbiased surveys are essential to decipher the cosmic evolution of galaxies. The submillimetre (submm) and millimetre (mm) windows complement the UV/optical waveband and are key to revealing the cold and dusty Universe. Traditional ways of conducting deep surveys resort to either lensed fields or target small areas for ultra-long integrations. These surveys have greatly advanced our understanding of dusty star-forming galaxies (DSFGs), but are susceptible to lensing uncertainties and cosmic variance and will be expensive to expand. Here, we summarise our recent multi-wavelength survey of DSFGs in the vicinity of ALMA's calibrators: the ALMACAL survey. These fields have accumulated many hundreds of hours of on-source time, reaching depths and effective areas that are competitive with bespoke cosmological surveys. We summarise the multi-wavelength number counts from ALMACAL and the resolved fraction of the Cosmic Infrared Background (CIB) from submm to mm wavelengths. Meanwhile, combining all available ALMA observations in each field results in impressive frequency coverage, which often yields the redshifts of these DSFGs. The ALMACAL survey has demonstrated the scientific value of calibration scans for all submm/mm and radio telescopes, existing and planned.","sentences":["Deep, unbiased surveys are essential to decipher the cosmic evolution of galaxies.","The submillimetre (submm) and millimetre (mm) windows complement the UV/optical waveband and are key to revealing the cold and dusty Universe.","Traditional ways of conducting deep surveys resort to either lensed fields or target small areas for ultra-long integrations.","These surveys have greatly advanced our understanding of dusty star-forming galaxies (DSFGs), but are susceptible to lensing uncertainties and cosmic variance and will be expensive to expand.","Here, we summarise our recent multi-wavelength survey of DSFGs in the vicinity of ALMA's calibrators: the ALMACAL survey.","These fields have accumulated many hundreds of hours of on-source time, reaching depths and effective areas that are competitive with bespoke cosmological surveys.","We summarise the multi-wavelength number counts from ALMACAL and the resolved fraction of the Cosmic Infrared Background (CIB) from submm to mm wavelengths.","Meanwhile, combining all available ALMA observations in each field results in impressive frequency coverage, which often yields the redshifts of these DSFGs.","The ALMACAL survey has demonstrated the scientific value of calibration scans for all submm/mm and radio telescopes, existing and planned."],"url":"http://arxiv.org/abs/2404.02293v1","category":"astro-ph.GA"}
{"created":"2024-04-02 20:43:28","title":"Elasto-inertial rectification of oscillatory flow in an elastic tube","abstract":"The interaction between deformable surfaces and oscillatory driving is known to yield complex secondary time-averaged flows due to inertial and elastic nonlinearities. Here, we revisit the problem of oscillatory flow in a cylindrical tube with a deformable wall, and analyze it under a long-wave }theory for small deformations, but for arbitrary Womersley numbers. We find that the oscillatory pressure does not vary linearly along the length of a deformable channel, but instead decays exponentially with spatial oscillations. We show that this decay occurs over an elasto-visco-inertial length scale that depends on the material properties of the fluid and the elastic walls, the geometry of the system, and the frequency of the oscillatory flow, but is independent of the amplitude of deformation. Inertial and geometric nonlinearities associated with the elastic deformation of the channel drive a time-averaged secondary flow. We quantify this flow using numerical solutions of our perturbation theory, and gain insight into these solutions with analytic approximations. The theory identifies a complex non-monotonic dependence of the time-averaged flux on the elastic compliance and inertia, including a reversal of the flow. Finally, we show that our analytic theory is in excellent quantitative agreement with the three-dimensional direct numerical simulations of \\citet{pande2023oscillatory}.","sentences":["The interaction between deformable surfaces and oscillatory driving is known to yield complex secondary time-averaged flows due to inertial and elastic nonlinearities.","Here, we revisit the problem of oscillatory flow in a cylindrical tube with a deformable wall, and analyze it under a long-wave }theory for small deformations, but for arbitrary Womersley numbers.","We find that the oscillatory pressure does not vary linearly along the length of a deformable channel, but instead decays exponentially with spatial oscillations.","We show that this decay occurs over an elasto-visco-inertial length scale that depends on the material properties of the fluid and the elastic walls, the geometry of the system, and the frequency of the oscillatory flow, but is independent of the amplitude of deformation.","Inertial and geometric nonlinearities associated with the elastic deformation of the channel drive a time-averaged secondary flow.","We quantify this flow using numerical solutions of our perturbation theory, and gain insight into these solutions with analytic approximations.","The theory identifies a complex non-monotonic dependence of the time-averaged flux on the elastic compliance and inertia, including a reversal of the flow.","Finally, we show that our analytic theory is in excellent quantitative agreement with the three-dimensional direct numerical simulations of \\citet{pande2023oscillatory}."],"url":"http://arxiv.org/abs/2404.02292v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 20:23:02","title":"APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models","abstract":"Dexterous manipulation, particularly adept coordinating and grasping, constitutes a fundamental and indispensable capability for robots, facilitating the emulation of human-like behaviors. Integrating this capability into robots empowers them to supplement and even supplant humans in undertaking increasingly intricate tasks in both daily life and industrial settings. Unfortunately, contemporary methodologies encounter serious challenges in devising manipulation trajectories owing to the intricacies of tasks, the expansive robotic manipulation space, and dynamic obstacles. We propose a novel approach, APEX, to address all these difficulties by introducing a collision-free latent diffusion model for both robotic motion planning and manipulation. Firstly, we simplify the complexity of real-life ambidextrous dual-arm robotic manipulation tasks by abstracting them as aligning two vectors. Secondly, we devise latent diffusion models to produce a variety of robotic manipulation trajectories. Furthermore, we integrate obstacle information utilizing a classifier-guidance technique, thereby guaranteeing both the feasibility and safety of the generated manipulation trajectories. Lastly, we validate our proposed algorithm through extensive experiments conducted on the hardware platform of ambidextrous dual-arm robots. Our algorithm consistently generates successful and seamless trajectories across diverse tasks, surpassing conventional robotic motion planning algorithms. These results carry significant implications for the future design of diffusion robots, enhancing their capability to tackle more intricate robotic manipulation tasks with increased efficiency and safety. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/apex-dual-arm/home.","sentences":["Dexterous manipulation, particularly adept coordinating and grasping, constitutes a fundamental and indispensable capability for robots, facilitating the emulation of human-like behaviors.","Integrating this capability into robots empowers them to supplement and even supplant humans in undertaking increasingly intricate tasks in both daily life and industrial settings.","Unfortunately, contemporary methodologies encounter serious challenges in devising manipulation trajectories owing to the intricacies of tasks, the expansive robotic manipulation space, and dynamic obstacles.","We propose a novel approach, APEX, to address all these difficulties by introducing a collision-free latent diffusion model for both robotic motion planning and manipulation.","Firstly, we simplify the complexity of real-life ambidextrous dual-arm robotic manipulation tasks by abstracting them as aligning two vectors.","Secondly, we devise latent diffusion models to produce a variety of robotic manipulation trajectories.","Furthermore, we integrate obstacle information utilizing a classifier-guidance technique, thereby guaranteeing both the feasibility and safety of the generated manipulation trajectories.","Lastly, we validate our proposed algorithm through extensive experiments conducted on the hardware platform of ambidextrous dual-arm robots.","Our algorithm consistently generates successful and seamless trajectories across diverse tasks, surpassing conventional robotic motion planning algorithms.","These results carry significant implications for the future design of diffusion robots, enhancing their capability to tackle more intricate robotic manipulation tasks with increased efficiency and safety.","Complete video demonstrations of our experiments can be found in https://sites.google.com/view/apex-dual-arm/home."],"url":"http://arxiv.org/abs/2404.02284v1","category":"cs.RO"}
{"created":"2024-04-02 20:17:44","title":"Integrating representative and non-representative survey data for efficient inference","abstract":"Non-representative surveys are commonly used and widely available but suffer from selection bias that generally cannot be entirely eliminated using weighting techniques. Instead, we propose a Bayesian method to synthesize longitudinal representative unbiased surveys with non-representative biased surveys by estimating the degree of selection bias over time. We show using a simulation study that synthesizing biased and unbiased surveys together out-performs using the unbiased surveys alone, even if the selection bias may evolve in a complex manner over time. Using COVID-19 vaccination data, we are able to synthesize two large sample biased surveys with an unbiased survey to reduce uncertainty in now-casting and inference estimates while simultaneously retaining the empirical credible interval coverage. Ultimately, we are able to conceptually obtain the properties of a large sample unbiased survey if the assumed unbiased survey, used to anchor the estimates, is unbiased for all time-points.","sentences":["Non-representative surveys are commonly used and widely available but suffer from selection bias that generally cannot be entirely eliminated using weighting techniques.","Instead, we propose a Bayesian method to synthesize longitudinal representative unbiased surveys with non-representative biased surveys by estimating the degree of selection bias over time.","We show using a simulation study that synthesizing biased and unbiased surveys together out-performs using the unbiased surveys alone, even if the selection bias may evolve in a complex manner over time.","Using COVID-19 vaccination data, we are able to synthesize two large sample biased surveys with an unbiased survey to reduce uncertainty in now-casting and inference estimates while simultaneously retaining the empirical credible interval coverage.","Ultimately, we are able to conceptually obtain the properties of a large sample unbiased survey if the assumed unbiased survey, used to anchor the estimates, is unbiased for all time-points."],"url":"http://arxiv.org/abs/2404.02283v1","category":"stat.ME"}
{"created":"2024-04-02 20:15:43","title":"Smooth Deep Saliency","abstract":"In this work, we investigate methods to reduce the noise in deep saliency maps coming from convolutional downsampling, with the purpose of explaining how a deep learning model detects tumors in scanned histological tissue samples. Those methods make the investigated models more interpretable for gradient-based saliency maps, computed in hidden layers. We test our approach on different models trained for image classification on ImageNet1K, and models trained for tumor detection on Camelyon16 and in-house real-world digital pathology scans of stained tissue samples. Our results show that the checkerboard noise in the gradient gets reduced, resulting in smoother and therefore easier to interpret saliency maps.","sentences":["In this work, we investigate methods to reduce the noise in deep saliency maps coming from convolutional downsampling, with the purpose of explaining how a deep learning model detects tumors in scanned histological tissue samples.","Those methods make the investigated models more interpretable for gradient-based saliency maps, computed in hidden layers.","We test our approach on different models trained for image classification on ImageNet1K, and models trained for tumor detection on Camelyon16 and in-house real-world digital pathology scans of stained tissue samples.","Our results show that the checkerboard noise in the gradient gets reduced, resulting in smoother and therefore easier to interpret saliency maps."],"url":"http://arxiv.org/abs/2404.02282v1","category":"cs.CV"}
{"created":"2024-04-02 19:52:06","title":"Comparing angles in Euclid's Elements","abstract":"The exposition in Euclid's Elements contains an obvious gap (seemingly unnoticed by most commentators): he often compares not just angles, but *groups* of angles, and at the same time he avoids summing angles (and considering angles greater than $\\pi$), and does not say what such a comparison of groups could mean. We discuss the problem and suggest a possible interpretation that could make Euclid's exposition consistent.","sentences":["The exposition in Euclid's Elements contains an obvious gap (seemingly unnoticed by most commentators): he often compares not just angles, but *groups* of angles, and at the same time he avoids summing angles (and considering angles greater than $\\pi$), and does not say what such a comparison of groups could mean.","We discuss the problem and suggest a possible interpretation that could make Euclid's exposition consistent."],"url":"http://arxiv.org/abs/2404.02272v1","category":"math.HO"}
{"created":"2024-04-02 19:50:36","title":"Postprocessing of point predictions for probabilistic forecasting of electricity prices: Diversity matters","abstract":"Operational decisions relying on predictive distributions of electricity prices can result in significantly higher profits compared to those based solely on point forecasts. However, the majority of models developed in both academic and industrial settings provide only point predictions. To address this, we examine three postprocessing methods for converting point forecasts into probabilistic ones: Quantile Regression Averaging, Conformal Prediction, and the recently introduced Isotonic Distributional Regression. We find that while IDR demonstrates the most varied performance, combining its predictive distributions with those of the other two methods results in an improvement of ca. 7.5% compared to a benchmark model with normally distributed errors, over a 4.5-year test period in the German power market spanning the COVID pandemic and the war in Ukraine. Remarkably, the performance of this combination is at par with state-of-the-art Distributional Deep Neural Networks.","sentences":["Operational decisions relying on predictive distributions of electricity prices can result in significantly higher profits compared to those based solely on point forecasts.","However, the majority of models developed in both academic and industrial settings provide only point predictions.","To address this, we examine three postprocessing methods for converting point forecasts into probabilistic ones:","Quantile Regression Averaging, Conformal Prediction, and the recently introduced Isotonic Distributional Regression.","We find that while IDR demonstrates the most varied performance, combining its predictive distributions with those of the other two methods results in an improvement of ca.","7.5% compared to a benchmark model with normally distributed errors, over a 4.5-year test period in the German power market spanning the COVID pandemic and the war in Ukraine.","Remarkably, the performance of this combination is at par with state-of-the-art Distributional Deep Neural Networks."],"url":"http://arxiv.org/abs/2404.02270v1","category":"q-fin.ST"}
{"created":"2024-04-02 19:39:37","title":"The Identity Problem in virtually solvable matrix groups over algebraic numbers","abstract":"The Tits alternative states that a finitely generated matrix group either contains a nonabelian free subgroup $F_2$, or it is virtually solvable. This paper considers two decision problems in virtually solvable matrix groups: the Identity Problem (does a given finitely generated subsemigroup contain the identity matrix?), and the Group Problem (is a given finitely generated subsemigroup a group?). We show that both problems are decidable in virtually solvable matrix groups over the field of algebraic numbers $\\overline{\\mathbb{Q}}$. Our proof also extends the decidability result for nilpotent groups by Bodart, Ciobanu, Metcalfe and Shaffrir, and the decidability result for metabelian groups by Dong (STOC'24). Since the Identity Problem and the Group Problem are known to be undecidable in matrix groups containing $F_2 \\times F_2$, our result significantly reduces the decidability gap for both decision problems.","sentences":["The Tits alternative states that a finitely generated matrix group either contains a nonabelian free subgroup $F_2$, or it is virtually solvable.","This paper considers two decision problems in virtually solvable matrix groups: the Identity Problem (does a given finitely generated subsemigroup contain the identity matrix?), and the Group Problem (is a given finitely generated subsemigroup a group?).","We show that both problems are decidable in virtually solvable matrix groups over the field of algebraic numbers $\\overline{\\mathbb{Q}}$. Our proof also extends the decidability result for nilpotent groups by Bodart, Ciobanu, Metcalfe and Shaffrir, and the decidability result for metabelian groups by Dong (STOC'24).","Since the Identity Problem and the Group Problem are known to be undecidable in matrix groups containing $F_2 \\times F_2$, our result significantly reduces the decidability gap for both decision problems."],"url":"http://arxiv.org/abs/2404.02264v1","category":"math.GR"}
{"created":"2024-04-02 19:21:28","title":"On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning","abstract":"In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for \"typical\" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how \"organic\" the average-case separation is. Would it be encountered in practice? To this end, we prove that under natural conditions, any given computational separation between average-case unimodal and multimodal learning tasks implies a corresponding cryptographic key agreement protocol. We suggest to interpret this as evidence that very strong computational advantages of multimodal learning may arise infrequently in practice, since they exist only for the \"pathological\" case of inherently cryptographic distributions. However, this does not apply to possible (super-polynomial) statistical advantages.","sentences":["In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation).","Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4).","Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning.","In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   ","In this paper, we give a stronger average-case computational separation, where for \"typical\" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy.","We then question how \"organic\" the average-case separation is.","Would it be encountered in practice?","To this end, we prove that under natural conditions, any given computational separation between average-case unimodal and multimodal learning tasks implies a corresponding cryptographic key agreement protocol.","We suggest to interpret this as evidence that very strong computational advantages of multimodal learning may arise infrequently in practice, since they exist only for the \"pathological\" case of inherently cryptographic distributions.","However, this does not apply to possible (super-polynomial) statistical advantages."],"url":"http://arxiv.org/abs/2404.02254v1","category":"stat.ML"}
{"created":"2024-04-02 19:08:50","title":"A recipe for eccentricity and inclination damping for partial gap opening planets in 3D disks","abstract":"In a previous paper we showed that, like the migration speed, the eccentricity damping efficiency is modulated linearly by the depth of the partial gap a planet carves in the disk surface density profile, resulting in less efficient $e$-damping compared to the prescription commonly used in population synthesis works. Here, we extend our analysis to 3D, refining our $e$-damping formula and studying how the inclination damping efficiency is also affected. We perform high resolution 3D locally isothermal hydrodynamical simulations of planets with varying masses embedded in disks with varying aspect ratios and viscosities. We extract the gap profile and orbital damping timescales for fixed eccentricities and inclinations up to the disk scale height. The limit in gap depths below which vortices appear, in the low-viscosity case, happens roughly at the transition between classical type-I and type-II migration regimes. The orbital damping timescales can be described by two linear trends with a break around gap depths $\\sim80\\%$ and with slopes and intercepts depending on the eccentricity and inclination. These trends are understood on physical grounds and are reproduced by simple fitting formulas whose error is within the typically uncertainty of type-I torque formulas. Thus, our recipes for the gap depth and orbital damping efficiencies yield a simple description for planet-disk interactions to use in N-body codes in the case of partial gap opening planets that is consistent with high-resolution 3D hydro-simulations. Finally, we show examples of how our novel orbital damping prescription can affect the outcome of population synthesis experiments.","sentences":["In a previous paper we showed that, like the migration speed, the eccentricity damping efficiency is modulated linearly by the depth of the partial gap a planet carves in the disk surface density profile, resulting in less efficient $e$-damping compared to the prescription commonly used in population synthesis works.","Here, we extend our analysis to 3D, refining our $e$-damping formula and studying how the inclination damping efficiency is also affected.","We perform high resolution 3D locally isothermal hydrodynamical simulations of planets with varying masses embedded in disks with varying aspect ratios and viscosities.","We extract the gap profile and orbital damping timescales for fixed eccentricities and inclinations up to the disk scale height.","The limit in gap depths below which vortices appear, in the low-viscosity case, happens roughly at the transition between classical type-I and type-II migration regimes.","The orbital damping timescales can be described by two linear trends with a break around gap depths $\\sim80\\%$ and with slopes and intercepts depending on the eccentricity and inclination.","These trends are understood on physical grounds and are reproduced by simple fitting formulas whose error is within the typically uncertainty of type-I torque formulas.","Thus, our recipes for the gap depth and orbital damping efficiencies yield a simple description for planet-disk interactions to use in N-body codes in the case of partial gap opening planets that is consistent with high-resolution 3D hydro-simulations.","Finally, we show examples of how our novel orbital damping prescription can affect the outcome of population synthesis experiments."],"url":"http://arxiv.org/abs/2404.02247v1","category":"astro-ph.EP"}
{"created":"2024-04-02 19:06:01","title":"A Novel Approach to Reduce Derivative Costs in Variational Quantum Algorithms","abstract":"We present a detailed numerical study of an alternative approach, named Quantum Non-Demolition Measurement (QNDM), to efficiently estimate the gradients or the Hessians of a quantum observable. This is a key step and a resource-demanding task when we want to minimize the cost function associated with a quantum observable. In our detailed analysis, we account for all the resources needed to implement the QNDM approach with a fixed accuracy and compare them to the current state-of-the-art method. We find that the QNDM approach is more efficient, i.e. it needs fewer resources, in evaluating the derivatives of a cost function.These advantages are already clear in small dimensional systems and are likely to increase for practical implementations and more realistic situations. Since the vast majority of the Variational Quantum Algorithms can be formulated in the discussed framework, our results can have significant implications in quantum optimization algorithms and make the QNDM approach a valuable alternative to implement Variational Quantum Algorithms on near-term quantum computers.","sentences":["We present a detailed numerical study of an alternative approach, named Quantum Non-Demolition Measurement (QNDM), to efficiently estimate the gradients or the Hessians of a quantum observable.","This is a key step and a resource-demanding task when we want to minimize the cost function associated with a quantum observable.","In our detailed analysis, we account for all the resources needed to implement the QNDM approach with a fixed accuracy and compare them to the current state-of-the-art method.","We find that the QNDM approach is more efficient, i.e. it needs fewer resources, in evaluating the derivatives of a cost function.","These advantages are already clear in small dimensional systems and are likely to increase for practical implementations and more realistic situations.","Since the vast majority of the Variational Quantum Algorithms can be formulated in the discussed framework, our results can have significant implications in quantum optimization algorithms and make the QNDM approach a valuable alternative to implement Variational Quantum Algorithms on near-term quantum computers."],"url":"http://arxiv.org/abs/2404.02245v1","category":"quant-ph"}
{"created":"2024-04-02 18:59:39","title":"Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better","abstract":"Diffusion Models (DM) and Consistency Models (CM) are two types of popular generative models with good generation quality on various tasks. When training DM and CM, intermediate weight checkpoints are not fully utilized and only the last converged checkpoint is used. In this work, we find that high-quality model weights often lie in a basin which cannot be reached by SGD but can be obtained by proper checkpoint averaging. Based on these observations, we propose LCSC, a simple but effective and efficient method to enhance the performance of DM and CM, by combining checkpoints along the training trajectory with coefficients deduced from evolutionary search. We demonstrate the value of LCSC through two use cases: $\\textbf{(a) Reducing training cost.}$ With LCSC, we only need to train DM/CM with fewer number of iterations and/or lower batch sizes to obtain comparable sample quality with the fully trained model. For example, LCSC achieves considerable training speedups for CM (23$\\times$ on CIFAR-10 and 15$\\times$ on ImageNet-64). $\\textbf{(b) Enhancing pre-trained models.}$ Assuming full training is already done, LCSC can further improve the generation quality or speed of the final converged models. For example, LCSC achieves better performance using 1 number of function evaluation (NFE) than the base model with 2 NFE on consistency distillation, and decreases the NFE of DM from 15 to 9 while maintaining the generation quality on CIFAR-10. Our code is available at https://github.com/imagination-research/LCSC.","sentences":["Diffusion Models (DM) and Consistency Models (CM) are two types of popular generative models with good generation quality on various tasks.","When training DM and CM, intermediate weight checkpoints are not fully utilized and only the last converged checkpoint is used.","In this work, we find that high-quality model weights often lie in a basin which cannot be reached by SGD but can be obtained by proper checkpoint averaging.","Based on these observations, we propose LCSC, a simple but effective and efficient method to enhance the performance of DM and CM, by combining checkpoints along the training trajectory with coefficients deduced from evolutionary search.","We demonstrate the value of LCSC through two use cases: $\\textbf{(a) Reducing training cost.}$ With LCSC, we only need to train DM/CM with fewer number of iterations and/or lower batch sizes to obtain comparable sample quality with the fully trained model.","For example, LCSC achieves considerable training speedups for CM (23$\\times$ on CIFAR-10 and 15$\\times$ on ImageNet-64).","$\\textbf{(b) Enhancing pre-trained models.}$ Assuming full training is already done, LCSC can further improve the generation quality or speed of the final converged models.","For example, LCSC achieves better performance using 1 number of function evaluation (NFE) than the base model with 2 NFE on consistency distillation, and decreases the NFE of DM from 15 to 9 while maintaining the generation quality on CIFAR-10.","Our code is available at https://github.com/imagination-research/LCSC."],"url":"http://arxiv.org/abs/2404.02241v1","category":"cs.CV"}
{"created":"2024-04-02 18:48:33","title":"Finite speed of sound effects on asymmetry in multibubble cavitation","abstract":"Three-dimensional direct numerical simulations (DNS) are used to revisit the experiments on multibubble cavitation performed by Bremond et al. (https://doi.org/10.1063/1.2396922, Phys. Fluids 18, 121505 (2006), https://doi.org/10.1103/PhysRevLett.96.224501, Phys. Rev. Lett. 96, 224501 (2006)). In particular, we aim at understanding the asymmetry observed therein during the expansion and collapse of bubble clusters subjected to a pressure pulse. Our numerical simulations suggest that the asymmetry is due to the force applied by the imposed pressure pulse and it is a consequence of the finite effective speed of sound in the liquid. By comparing our numerical results to the experiments, we found that the effective speed of sound under the experimental conditions was smaller than that of degassed water due to microbubbles in the system which resulted from prior cavitation experiments in the same setup. The estimated values of the effective speed of sound are consistent with those derived from the classical theory of wave propagation in liquids with small amounts of gas. To support this theory, we also present evidence of tiny bubbles remaining in the liquid bulk as a result of the fragmentation of large bubbles during the prior cavitation experiments. Furthermore, we find that this asymmetry also alters the direction of the liquid jet generated during the last stages of bubble collapse.","sentences":["Three-dimensional direct numerical simulations (DNS) are used to revisit the experiments on multibubble cavitation performed by Bremond et al. (https://doi.org/10.1063/1.2396922, Phys.","Fluids 18, 121505 (2006), https://doi.org/10.1103/PhysRevLett.96.224501, Phys. Rev. Lett.","96, 224501 (2006)).","In particular, we aim at understanding the asymmetry observed therein during the expansion and collapse of bubble clusters subjected to a pressure pulse.","Our numerical simulations suggest that the asymmetry is due to the force applied by the imposed pressure pulse and it is a consequence of the finite effective speed of sound in the liquid.","By comparing our numerical results to the experiments, we found that the effective speed of sound under the experimental conditions was smaller than that of degassed water due to microbubbles in the system which resulted from prior cavitation experiments in the same setup.","The estimated values of the effective speed of sound are consistent with those derived from the classical theory of wave propagation in liquids with small amounts of gas.","To support this theory, we also present evidence of tiny bubbles remaining in the liquid bulk as a result of the fragmentation of large bubbles during the prior cavitation experiments.","Furthermore, we find that this asymmetry also alters the direction of the liquid jet generated during the last stages of bubble collapse."],"url":"http://arxiv.org/abs/2404.02237v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 18:44:53","title":"Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models","abstract":"Friction is one of the cruxes of hydrodynamic modeling; flood conditions are highly sensitive to the Friction Factors (FFs) used to calculate momentum losses. However, empirical FFs are challenging to measure because they require laboratory experiments. Flood models often rely on surrogate observations (such as land use) to estimate FFs, introducing uncertainty. This research presents a laboratory-trained Deep Neural Network (DNN), trained using flume experiments with data augmentation techniques, to measure Manning's n based on Point Cloud data. The DNN was deployed on real-world lidar Point Clouds to directly measure Manning's n under regulatory and extreme storm events, showing improved prediction capabilities in both 1D and 2D hydrodynamic models. For 1D models, the lidar values decreased differences with regulatory models for in-channel water depth when compared to land cover values. For 1D/2D coupled models, the lidar values produced better agreement with flood extents measured from airborne imagery, while better matching flood insurance claim data for Hurricane Harvey. In both 1D and 1D/2D coupled models, lidar resulted in better agreement with validation gauges. For these reasons, the lidar measurements of Manning's n were found to improve both regulatory models and forecasts for extreme storm events, while simultaneously providing a pathway to standardize the measurement of FFs. Changing FFs significantly affected fluvial and pluvial flood models, while surge flooding was generally unaffected. Downstream flow conditions were found to change the importance of FFs to fluvial models, advancing the literature of friction in flood models. This research introduces a reliable, repeatable, and readily-accessible avenue to measure high-resolution FFs based on 3D point clouds, improving flood prediction, and removing uncertainty from hydrodynamic modeling.","sentences":["Friction is one of the cruxes of hydrodynamic modeling; flood conditions are highly sensitive to the Friction Factors (FFs) used to calculate momentum losses.","However, empirical FFs are challenging to measure because they require laboratory experiments.","Flood models often rely on surrogate observations (such as land use) to estimate FFs, introducing uncertainty.","This research presents a laboratory-trained Deep Neural Network (DNN), trained using flume experiments with data augmentation techniques, to measure Manning's n based on Point Cloud data.","The DNN was deployed on real-world lidar Point Clouds to directly measure Manning's n under regulatory and extreme storm events, showing improved prediction capabilities in both 1D and 2D hydrodynamic models.","For 1D models, the lidar values decreased differences with regulatory models for in-channel water depth when compared to land cover values.","For 1D/2D coupled models, the lidar values produced better agreement with flood extents measured from airborne imagery, while better matching flood insurance claim data for Hurricane Harvey.","In both 1D and 1D/2D coupled models, lidar resulted in better agreement with validation gauges.","For these reasons, the lidar measurements of Manning's n were found to improve both regulatory models and forecasts for extreme storm events, while simultaneously providing a pathway to standardize the measurement of FFs.","Changing FFs significantly affected fluvial and pluvial flood models, while surge flooding was generally unaffected.","Downstream flow conditions were found to change the importance of FFs to fluvial models, advancing the literature of friction in flood models.","This research introduces a reliable, repeatable, and readily-accessible avenue to measure high-resolution FFs based on 3D point clouds, improving flood prediction, and removing uncertainty from hydrodynamic modeling."],"url":"http://arxiv.org/abs/2404.02234v1","category":"cs.LG"}
{"created":"2024-04-02 18:23:37","title":"Semigroups of linear transformations whose restrictions belong to a general linear group","abstract":"Let $V$ be a vector space and $U$ a fixed subspace of $V$. We denote the semigroup of all linear transformations on $V$ under composition of functions by $L(V)$. In this paper, we study the semigroup of all linear transformations on $V$ whose restrictions belong to the general linear group $GL(U)$, denoted by $L_{GL(U)}(V)$. More precisely, we consider the subsemigroup   \\[   L_{GL(U)}(V)=\\{\\alpha\\in L(V):\\alpha|_U\\in GL(U)\\}   \\]   of $L(V)$. In this work, Green's relations and ideals of this semigroup are described. Then we also determine the minimal ideal and the set of all minimal idempotents of it. Moreover, we establish an isomorphism theorem when $V$ is a finite dimensional vector space over a finite field. Finally, we find its generating set.","sentences":["Let $V$ be a vector space and $U$ a fixed subspace of $V$. We denote the semigroup of all linear transformations on $V$ under composition of functions by $L(V)$. In this paper, we study the semigroup of all linear transformations on $V$ whose restrictions belong to the general linear group $GL(U)$, denoted by $L_{GL(U)}(V)$. More precisely, we consider the subsemigroup   \\[   L_{GL(U)}(V)=\\{\\alpha\\in L(V):\\alpha|_U\\in GL(U)\\}   \\]   of $L(V)$. In this work, Green's relations and ideals of this semigroup are described.","Then we also determine the minimal ideal and the set of all minimal idempotents of it.","Moreover, we establish an isomorphism theorem when $V$ is a finite dimensional vector space over a finite field.","Finally, we find its generating set."],"url":"http://arxiv.org/abs/2404.02224v1","category":"math.RA"}
{"created":"2024-04-02 18:11:55","title":"A shared compilation stack for distributed-memory parallelism in stencil DSLs","abstract":"Domain Specific Languages (DSLs) increase programmer productivity and provide high performance. Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current- and next-generation supercomputers. The convenience and performance of DSLs come with significant development and maintenance costs. The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale. By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community. We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations. We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem.","sentences":["Domain Specific Languages (DSLs) increase programmer productivity and provide high performance.","Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current-","and next-generation supercomputers.","The convenience and performance of DSLs come with significant development and maintenance costs.","The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale.","By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community.","We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations.","We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem."],"url":"http://arxiv.org/abs/2404.02218v1","category":"cs.DC"}
{"created":"2024-04-02 18:03:10","title":"No need to know: astrophysics-free gravitational-wave cosmology","abstract":"Gravitational waves from merging compact objects encode direct information about the luminosity distance to the binary. When paired with a redshift measurement, this enables standard-siren cosmology: a Hubble diagram can be constructed to directly probe the Universe's expansion. This can be done in the absence of electromagnetic measurements as features in the mass distribution of GW sources provide self-calibrating redshift measurements without the need for a definite or probabilistic host galaxy association. This technique has thus far only been applied with simple parametric representations of the mass distribution. However, the use of an inaccurate representation leads to biases in the cosmological inference, an acute problem given the current uncertainties in true source population. Furthermore, it is commonly presumed that the form of the mass distribution must be known a priori to obtain unbiased measurements of cosmological parameters in this fashion. Here, we demonstrate that spectral sirens can accurately infer cosmological parameters without such prior assumptions. We apply a flexible, non-parametric model for the mass distribution of compact binaries to a simulated catalog of 1,000 gravitational-wave events, consistent with expectations for the next LVK observing run. We find that, despite our model's flexibility, both the source mass model and cosmological parameters are correctly reconstructed. We predict a $5.8\\%$ measurement of $H_0$, keeping all other cosmological parameters fixed, and a $6.4\\%$ measurement of $H(z=0.9)$ when fitting for multiple cosmological parameters ($1\\sigma$ uncertainties). This astrophysically-agnostic spectral siren technique will be essential to arrive at precise and unbiased cosmological constraints from GW source populations.","sentences":["Gravitational waves from merging compact objects encode direct information about the luminosity distance to the binary.","When paired with a redshift measurement, this enables standard-siren cosmology: a Hubble diagram can be constructed to directly probe the Universe's expansion.","This can be done in the absence of electromagnetic measurements as features in the mass distribution of GW sources provide self-calibrating redshift measurements without the need for a definite or probabilistic host galaxy association.","This technique has thus far only been applied with simple parametric representations of the mass distribution.","However, the use of an inaccurate representation leads to biases in the cosmological inference, an acute problem given the current uncertainties in true source population.","Furthermore, it is commonly presumed that the form of the mass distribution must be known a priori to obtain unbiased measurements of cosmological parameters in this fashion.","Here, we demonstrate that spectral sirens can accurately infer cosmological parameters without such prior assumptions.","We apply a flexible, non-parametric model for the mass distribution of compact binaries to a simulated catalog of 1,000 gravitational-wave events, consistent with expectations for the next LVK observing run.","We find that, despite our model's flexibility, both the source mass model and cosmological parameters are correctly reconstructed.","We predict a $5.8\\%$ measurement of $H_0$, keeping all other cosmological parameters fixed, and a $6.4\\%$ measurement of $H(z=0.9)$ when fitting for multiple cosmological parameters ($1\\sigma$ uncertainties).","This astrophysically-agnostic spectral siren technique will be essential to arrive at precise and unbiased cosmological constraints from GW source populations."],"url":"http://arxiv.org/abs/2404.02210v1","category":"astro-ph.CO"}
{"created":"2024-04-02 18:02:59","title":"Positive topological entropy for the Standard Map","abstract":"We show that for the standard map family, for all values of the parameter, except one, the mapping has positive topological entropy. The main tool is the following result.   Let $S$ be a compact connected orientable surface and $f:S \\rightarrow S$ an area preserving orientation preserving $C \\e 1$ diffeomorphism of $S$. Assume that $U$ is an invariant domain of $S$ such that $fr_S{U}$ has a finite number of connected components.   Let $b$ be a regular ideal boundary point of $U$ which is fixed under the induced action by $f$ on the ideal boundary of $U$, and let $\\hat{f}:C(b) \\rightarrow C(b)$ the homeomorphism on the corresponding circle of prime ends.   Let $Z(b)$ be the impression of $b$ in $S$ and assume that all fixed points of $f$ in $Z(b)$ are non degenerate.   If there exists a fixed prime end $e \\in C(b)$ then we know the following.   $\\left(1\\right)$ If $p$ is the principal point of $e$ then $p$ is also a fixed point of $Z(b)$ and $p$ is a saddle.   $\\left(2\\right)$ $C(b)$ has a finite number of fixed prime ends and there exists a finite singular covering $ \\phi :C(b) \\rightarrow Z(b)$, which is a semiconjugacy between the mapping of prime ends on $C(b)$ and the restriction of $f$ to $Z(b)$. In particular, $Z(b)$ is the connected union of finitely many saddle connections and the corresponding saddles.   This can be seen as a two dimensional generalization of the dynamics of homeomorphisms of the circle with fixed points.","sentences":["We show that for the standard map family, for all values of the parameter, except one, the mapping has positive topological entropy.","The main tool is the following result.   ","Let $S$ be a compact connected orientable surface and $f:S \\rightarrow S$ an area preserving orientation preserving $C \\e 1$ diffeomorphism of $S$. Assume that $U$ is an invariant domain of $S$ such that $fr_S{U}$ has a finite number of connected components.   ","Let $b$ be a regular ideal boundary point of $U$ which is fixed under the induced action by $f$ on the ideal boundary of $U$, and let $\\hat{f}:C(b)","\\rightarrow C(b)$ the homeomorphism on the corresponding circle of prime ends.   ","Let $Z(b)$ be the impression of $b$ in $S$ and assume that all fixed points of $f$ in $Z(b)$ are non degenerate.   ","If there exists a fixed prime end $e \\in C(b)$ then we know the following.   ","$\\left(1\\right)$ If $p$ is the principal point of $e$ then $p$ is also a fixed point of $Z(b)$ and $p$ is a saddle.   ","$\\left(2\\right)$ $C(b)$ has a finite number of fixed prime ends and there exists a finite singular covering $ \\phi :C(b)","\\rightarrow Z(b)$, which is a semiconjugacy between the mapping of prime ends on $C(b)$ and the restriction of $f$ to $Z(b)$. In particular, $Z(b)$ is the connected union of finitely many saddle connections and the corresponding saddles.   ","This can be seen as a two dimensional generalization of the dynamics of homeomorphisms of the circle with fixed points."],"url":"http://arxiv.org/abs/2404.02209v1","category":"math.DS"}
{"created":"2024-04-02 18:02:51","title":"Traversable wormholes and light rings","abstract":"Ultracompact objects (UCOs) are horizonless compact objects that present light rings (LRs) - circular photon orbits. As a result, they could be black hole mimickers. Some years ago, Cunha et al. established a theorem stating that, under general assumptions, UCOs formed from smooth, quasi-Minkowski initial data, must have at least a pair of LRs, one of which must be stable. These stable LRs are supposed to trigger a non-linear instability in spacetime, potentially weakening UCOs' ability to replicate black hole phenomenology. However, this LR theorem does not extend to wormholes, which represent topologically nontrivial spacetimes. We address the wormhole case by proving the following theorem: a stationary, axisymmetric, asymptotically flat, traversable wormhole in 1+3 dimensions, connecting two different asymptotic regions, has at least one standard LR for each rotation sense. Thus, any (such) wormhole is an UCO. By filling this gap, our results not only broaden the horizon of knowledge on UCOs but also highlight their potential to closely mimic black hole phenomenology.","sentences":["Ultracompact objects (UCOs) are horizonless compact objects that present light rings (LRs) - circular photon orbits.","As a result, they could be black hole mimickers.","Some years ago, Cunha et al. established a theorem stating that, under general assumptions, UCOs formed from smooth, quasi-Minkowski initial data, must have at least a pair of LRs, one of which must be stable.","These stable LRs are supposed to trigger a non-linear instability in spacetime, potentially weakening UCOs' ability to replicate black hole phenomenology.","However, this LR theorem does not extend to wormholes, which represent topologically nontrivial spacetimes.","We address the wormhole case by proving the following theorem: a stationary, axisymmetric, asymptotically flat, traversable wormhole in 1+3 dimensions, connecting two different asymptotic regions, has at least one standard LR for each rotation sense.","Thus, any (such) wormhole is an UCO.","By filling this gap, our results not only broaden the horizon of knowledge on UCOs but also highlight their potential to closely mimic black hole phenomenology."],"url":"http://arxiv.org/abs/2404.02208v1","category":"gr-qc"}
{"created":"2024-04-02 18:00:02","title":"Superrotations at Spacelike Infinity","abstract":"We propose a consistent set of boundary conditions for gravity in asymptotically flat spacetime at spacelike infinity, which yields an enhancement of the Bondi-Metzner-Sachs group with smooth superrotations and new subleading symmetries. These boundary conditions are obtained by allowing fluctuations of the boundary structure which are responsible for divergences in the symplectic form, and a renormalisation procedure is required to obtain finite canonical generators. The latter are then made integrable by incorporating boundary terms into the symplectic structure, which naturally derive from a linearised spin-two boundary field on a curved background with positive cosmological constant. Finally, we show that the canonical generators form a non-linear algebra under the Poisson bracket and verify the consistency of this structure with the Jacobi identity.","sentences":["We propose a consistent set of boundary conditions for gravity in asymptotically flat spacetime at spacelike infinity, which yields an enhancement of the Bondi-Metzner-Sachs group with smooth superrotations and new subleading symmetries.","These boundary conditions are obtained by allowing fluctuations of the boundary structure which are responsible for divergences in the symplectic form, and a renormalisation procedure is required to obtain finite canonical generators.","The latter are then made integrable by incorporating boundary terms into the symplectic structure, which naturally derive from a linearised spin-two boundary field on a curved background with positive cosmological constant.","Finally, we show that the canonical generators form a non-linear algebra under the Poisson bracket and verify the consistency of this structure with the Jacobi identity."],"url":"http://arxiv.org/abs/2404.02197v1","category":"hep-th"}
{"created":"2024-04-02 18:00:00","title":"A Consistent Cosmic Shear Analysis in Harmonic and Real Space","abstract":"Recent cosmic shear analyses have exhibited inconsistencies of up to $1\\sigma$ between the inferred cosmological parameters when analyzing summary statistics in real space versus harmonic space. In this paper, we demonstrate the consistent measurement and analysis of cosmic shear two-point functions in harmonic and real space using the $i${\\sc Master} algorithm. This algorithm provides a consistent prescription to model the survey window effects and scale cuts in both real space (due to observational systematics) and harmonic space (due to model limitations), resulting in a consistent estimation of the cosmic shear power spectrum from both harmonic and real space estimators. We show that the $i$\\textsc{Master} algorithm gives consistent results using measurements from the HSC Y1 mock shape catalogs in both real and harmonic space, resulting in consistent inferences of $S_8=\\sigma_8(\\Omega_m/0.3)^{0.5}$. This method provides an unbiased estimate of the cosmic shear power spectrum, and $S_8$ inference that has a correlation coefficient of 0.997 between analyses using measurements in real space and harmonic space. We observe the mean difference between the two inferred $S_8$ values to be 0.0004, far below the observed difference of 0.042 for the published HSC Y1 analyses and well below the statistical uncertainties. While the notation employed in this paper is specific to photometric galaxy surveys, the methods are equally applicable and can be extended to spectroscopic galaxy surveys, intensity mapping, and CMB surveys.","sentences":["Recent cosmic shear analyses have exhibited inconsistencies of up to $1\\sigma$ between the inferred cosmological parameters when analyzing summary statistics in real space versus harmonic space.","In this paper, we demonstrate the consistent measurement and analysis of cosmic shear two-point functions in harmonic and real space using the $i${\\sc Master} algorithm.","This algorithm provides a consistent prescription to model the survey window effects and scale cuts in both real space (due to observational systematics) and harmonic space (due to model limitations), resulting in a consistent estimation of the cosmic shear power spectrum from both harmonic and real space estimators.","We show that the $i$\\textsc{Master} algorithm gives consistent results using measurements from the HSC Y1 mock shape catalogs in both real and harmonic space, resulting in consistent inferences of $S_8=\\sigma_8(\\Omega_m/0.3)^{0.5}$. This method provides an unbiased estimate of the cosmic shear power spectrum, and $S_8$ inference that has a correlation coefficient of 0.997 between analyses using measurements in real space and harmonic space.","We observe the mean difference between the two inferred $S_8$ values to be 0.0004, far below the observed difference of 0.042 for the published HSC Y1 analyses and well below the statistical uncertainties.","While the notation employed in this paper is specific to photometric galaxy surveys, the methods are equally applicable and can be extended to spectroscopic galaxy surveys, intensity mapping, and CMB surveys."],"url":"http://arxiv.org/abs/2404.02190v1","category":"astro-ph.CO"}
{"created":"2024-04-02 17:58:24","title":"From Seaweed to Security: The Emergence of Alginate in Compromising IoT Fingerprint Sensors","abstract":"The increasing integration of capacitive fingerprint recognition sensors in IoT devices presents new challenges in digital forensics, particularly in the context of advanced fingerprint spoofing. Previous research has highlighted the effectiveness of materials such as latex and silicone in deceiving biometric systems. In this study, we introduce Alginate, a biopolymer derived from brown seaweed, as a novel material with the potential for spoofing IoT-specific capacitive fingerprint sensors. Our research uses Alginate and cutting-edge image recognition techniques to unveil a nuanced IoT vulnerability that raises significant security and privacy concerns. Our proof-of-concept experiments employed authentic fingerprint molds to create Alginate replicas, which exhibited remarkable visual and tactile similarities to real fingerprints. The conductivity and resistivity properties of Alginate, closely resembling human skin, make it a subject of interest in the digital forensics field, especially regarding its ability to spoof IoT device sensors. This study calls upon the digital forensics community to develop advanced anti-spoofing strategies to protect the evolving IoT infrastructure against such sophisticated threats.","sentences":["The increasing integration of capacitive fingerprint recognition sensors in IoT devices presents new challenges in digital forensics, particularly in the context of advanced fingerprint spoofing.","Previous research has highlighted the effectiveness of materials such as latex and silicone in deceiving biometric systems.","In this study, we introduce Alginate, a biopolymer derived from brown seaweed, as a novel material with the potential for spoofing IoT-specific capacitive fingerprint sensors.","Our research uses Alginate and cutting-edge image recognition techniques to unveil a nuanced IoT vulnerability that raises significant security and privacy concerns.","Our proof-of-concept experiments employed authentic fingerprint molds to create Alginate replicas, which exhibited remarkable visual and tactile similarities to real fingerprints.","The conductivity and resistivity properties of Alginate, closely resembling human skin, make it a subject of interest in the digital forensics field, especially regarding its ability to spoof IoT device sensors.","This study calls upon the digital forensics community to develop advanced anti-spoofing strategies to protect the evolving IoT infrastructure against such sophisticated threats."],"url":"http://arxiv.org/abs/2404.02150v1","category":"cs.CR"}
{"created":"2024-04-02 17:57:04","title":"Multiparametric quantification and visualization of liver fat using ultrasound","abstract":"Objectives- Several ultrasound measures have shown promise for assessment of steatosis compared to traditional B-scan, however clinicians may be required to integrate information across the parameters. Here, we propose an integrated multiparametric approach, enabling simple clinical assessment of key information from combined ultrasound parameters. Methods- We have measured 13 parameters related to ultrasound and shear wave elastography. These were measured in 30 human subjects under a study of liver fat. The 13 individual measures are assessed for their predictive value using independent magnetic resonance imaging-derived proton density fat fraction (MRI-PDFF) measurements as a reference standard. In addition, a comprehensive and fine-grain analysis is made of all possible combinations of sub-sets of these parameters to determine if any subset can be efficiently combined to predict fat fraction. Results- We found that as few as four key parameters related to ultrasound propagation are sufficient to generate a linear multiparametric parameter with a correlation against MRI-PDFF values of greater than 0.93. This optimal combination was found to have a classification area under the curve (AUC) approaching 1.0 when applying a threshold for separating steatosis grade zero from higher classes. Furthermore, a strategy is developed for applying local estimates of fat content as a color overlay to produce a visual impression of the extent and distribution of fat within the liver. Conclusion- In principle, this approach can be applied to most clinical ultrasound systems to provide the clinician and patient with a rapid and inexpensive estimate of liver fat content.","sentences":["Objectives- Several ultrasound measures have shown promise for assessment of steatosis compared to traditional B-scan, however clinicians may be required to integrate information across the parameters.","Here, we propose an integrated multiparametric approach, enabling simple clinical assessment of key information from combined ultrasound parameters.","Methods-","We have measured 13 parameters related to ultrasound and shear wave elastography.","These were measured in 30 human subjects under a study of liver fat.","The 13 individual measures are assessed for their predictive value using independent magnetic resonance imaging-derived proton density fat fraction (MRI-PDFF) measurements as a reference standard.","In addition, a comprehensive and fine-grain analysis is made of all possible combinations of sub-sets of these parameters to determine if any subset can be efficiently combined to predict fat fraction.","Results- We found that as few as four key parameters related to ultrasound propagation are sufficient to generate a linear multiparametric parameter with a correlation against MRI-PDFF values of greater than 0.93.","This optimal combination was found to have a classification area under the curve (AUC) approaching 1.0 when applying a threshold for separating steatosis grade zero from higher classes.","Furthermore, a strategy is developed for applying local estimates of fat content as a color overlay to produce a visual impression of the extent and distribution of fat within the liver.","Conclusion-","In principle, this approach can be applied to most clinical ultrasound systems to provide the clinician and patient with a rapid and inexpensive estimate of liver fat content."],"url":"http://arxiv.org/abs/2404.02143v1","category":"physics.med-ph"}
{"created":"2024-04-02 17:53:28","title":"Robustly estimating heterogeneity in factorial data using Rashomon Partitions","abstract":"Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like geometry. RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.","sentences":["Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates?","How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics?","Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool).","Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions.","Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science.","We develop an alternative perspective, called Rashomon Partition Sets (RPSs).","Each item in the RPS partitions the space of covariates using a tree-like geometry.","RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates.","This prior is the $\\ell_0$ prior, which we show is minimax optimal.","Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS.","We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS.","Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques.","We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance."],"url":"http://arxiv.org/abs/2404.02141v1","category":"stat.ME"}
{"created":"2024-04-02 17:51:53","title":"Lensed Type Ia Supernova \"Encore\" at z=2: The First Instance of Two Multiply-Imaged Supernovae in the Same Host Galaxy","abstract":"A bright ($m_{\\rm F150W,AB}$=24 mag), $z=1.95$ supernova (SN) candidate was discovered in JWST/NIRCam imaging acquired on 2023 November 17. The SN is quintuply-imaged as a result of strong gravitational lensing by a foreground galaxy cluster, detected in three locations, and remarkably is the second lensed SN found in the same host galaxy. The previous lensed SN was called \"Requiem\", and therefore the new SN is named \"Encore\". This makes the MACS J0138.0$-$2155 cluster the first known system to produce more than one multiply-imaged SN. Moreover, both SN Requiem and SN Encore are Type Ia SNe (SNe Ia), making this the most distant case of a galaxy hosting two SNe Ia. Using parametric host fitting, we determine the probability of detecting two SNe Ia in this host galaxy over a $\\sim10$ year window to be $\\approx3\\%$. These observations have the potential to yield a Hubble Constant ($H_0$) measurement with $\\sim10\\%$ precision, only the third lensed SN capable of such a result, using the three visible images of the SN. Both SN Requiem and SN Encore have a fourth image that is expected to appear within a few years of $\\sim2030$, providing an unprecedented baseline for time-delay cosmography.","sentences":["A bright ($m_{\\rm F150W,AB}$=24 mag), $z=1.95$ supernova (SN) candidate was discovered in JWST/NIRCam imaging acquired on 2023 November 17.","The SN is quintuply-imaged as a result of strong gravitational lensing by a foreground galaxy cluster, detected in three locations, and remarkably is the second lensed SN found in the same host galaxy.","The previous lensed SN was called \"Requiem\", and therefore the new SN is named \"Encore\".","This makes the MACS J0138.0$-$2155 cluster the first known system to produce more than one multiply-imaged SN.","Moreover, both SN Requiem and SN Encore are Type Ia SNe (SNe Ia), making this the most distant case of a galaxy hosting two SNe Ia. Using parametric host fitting, we determine the probability of detecting two SNe Ia in this host galaxy over a $\\sim10$ year window to be $\\approx3\\%$. These observations have the potential to yield a Hubble Constant ($H_0$) measurement with $\\sim10\\%$ precision, only the third lensed SN capable of such a result, using the three visible images of the SN.","Both SN Requiem and SN Encore have a fourth image that is expected to appear within a few years of $\\sim2030$, providing an unprecedented baseline for time-delay cosmography."],"url":"http://arxiv.org/abs/2404.02139v2","category":"astro-ph.CO"}
{"created":"2024-04-02 17:40:29","title":"ViTamin: Designing Scalable Vision Models in the Vision-Language Era","abstract":"Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).","sentences":["Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community.","The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs.","However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder.","Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs.","Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased.","In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework.","We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes.","To this end, we introduce ViTamin, a new vision models tailored for VLMs.","ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme.","ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models.","When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B)."],"url":"http://arxiv.org/abs/2404.02132v2","category":"cs.CV"}
{"created":"2024-04-02 17:28:55","title":"On regularity and rigidity of $2\\times 2$ differential inclusions into non-elliptic curves","abstract":"We study differential inclusions $Du\\in \\Pi$ in an open set $\\Omega\\subset\\mathbb R^2$, where $\\Pi\\subset \\mathbb R^{2\\times 2}$ is a compact connected $C^2$ curve without rank-one connections, but non-elliptic: tangent lines to $\\Pi$ may have rank-one connections, so that classical regularity and rigidity results do not apply. For a wide class of such curves $\\Pi$, we show that $Du$ is locally Lipschitz outside a discrete set, and is rigidly characterized around each singularity. Moreover, in the partially elliptic case where at least one tangent line to $\\Pi$ has no rank-one connections, or under some topological restrictions on the tangent bundle of $\\Pi$, there are no singularities. This goes well beyond previously known particular cases related to Burgers' equation and to the Aviles-Giga functional. The key is the identification and appropriate use of a general underlying structure: an infinite family of conservation laws, called entropy productions in reference to the theory of scalar conservation laws.","sentences":["We study differential inclusions $Du\\in \\Pi$ in an open set $\\Omega\\subset\\mathbb R^2$, where $\\Pi\\subset \\mathbb R^{2\\times 2}$ is a compact connected $C^2$ curve without rank-one connections, but non-elliptic: tangent lines to $\\Pi$ may have rank-one connections, so that classical regularity and rigidity results do not apply.","For a wide class of such curves $\\Pi$, we show that $Du$ is locally Lipschitz outside a discrete set, and is rigidly characterized around each singularity.","Moreover, in the partially elliptic case where at least one tangent line to $\\Pi$ has no rank-one connections, or under some topological restrictions on the tangent bundle of $\\Pi$, there are no singularities.","This goes well beyond previously known particular cases related to Burgers' equation and to the Aviles-Giga functional.","The key is the identification and appropriate use of a general underlying structure: an infinite family of conservation laws, called entropy productions in reference to the theory of scalar conservation laws."],"url":"http://arxiv.org/abs/2404.02121v1","category":"math.AP"}
{"created":"2024-04-02 17:26:22","title":"Gas kinematics and dynamics of Carina Pillars: A case study of G287.76-0.87","abstract":"We study the kinematics of a pillar, namely G287.76-0.87, using three rotational lines of $^{12}$CO(5-4), $^{12}$CO(8-7), $^{12}$CO(11-10), and a fine structure line of [OI] $63\\,\\mu$m Southern Carina observed by SOFIA/GREAT. This pillar is irradiated by the associated massive star cluster Trumpler 16, which includes $\\eta$~Carina. Our analysis shows that the relative velocity of the pillar with respect to this ionization source is small, $\\sim 1\\,\\rm km\\,s^{-1}$, and the gas motion in the tail is more turbulent than in the head. We also performed analytical calculations to estimate the gas column density in local thermal equilibrium (LTE) conditions, which yields $N_{\\rm CO}$ as $(\\sim 0.2 -5)\\times 10^{17}\\,\\rm cm^{-2}$. We further constrain the gas's physical properties in non-LTE conditions using RADEX. The non-LTE estimations result in $n_{\\rm H_{2}} \\simeq 10^{5}\\,\\rm cm^{-3}$ and $N_{\\rm CO} \\simeq 10^{16}\\,\\rm cm^{-2}$. We found that the thermal pressure within the G287.76-0.87 pillar is sufficiently high to make it stable for the surrounding hot gas and radiation feedback if the winds are not active. While they are active, stellar winds from the clustered stars sculpt the surrounding molecular cloud into pillars within the giant bubble around $\\eta$~Carina.","sentences":["We study the kinematics of a pillar, namely G287.76-0.87, using three rotational lines of $^{12}$CO(5-4), $^{12}$CO(8-7), $^{12}$CO(11-10), and a fine structure line of [OI] $63\\,\\mu$m Southern Carina observed by SOFIA/GREAT.","This pillar is irradiated by the associated massive star cluster Trumpler 16, which includes $\\eta$~Carina.","Our analysis shows that the relative velocity of the pillar with respect to this ionization source is small, $\\sim 1\\,\\rm km\\,s^{-1}$, and the gas motion in the tail is more turbulent than in the head.","We also performed analytical calculations to estimate the gas column density in local thermal equilibrium (LTE) conditions, which yields $N_{\\rm CO}$ as $(\\sim 0.2 -5)\\times","10^{17}\\,\\rm cm^{-2}$.","We further constrain the gas's physical properties in non-LTE conditions using RADEX.","The non-LTE estimations result in $n_{\\rm H_{2}} \\simeq 10^{5}\\,\\rm cm^{-3}$ and $N_{\\rm CO} \\simeq 10^{16}\\,\\rm","cm^{-2}$.","We found that the thermal pressure within the G287.76-0.87 pillar is sufficiently high to make it stable for the surrounding hot gas and radiation feedback if the winds are not active.","While they are active, stellar winds from the clustered stars sculpt the surrounding molecular cloud into pillars within the giant bubble around $\\eta$~Carina."],"url":"http://arxiv.org/abs/2404.02119v1","category":"astro-ph.GA"}
{"created":"2024-04-02 17:13:22","title":"Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL","abstract":"In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform poorly when restricted to one-percent tuning, whereas several algorithmic mitigations designed to maintain network plasticity perform surprising well. In addition, we find that properties designed to measure the network's ability to learn continually indeed correlate with performance under one-percent tuning.","sentences":["In continual or lifelong reinforcement learning access to the environment should be limited.","If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime.","The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent.","This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies.","In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning.","We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains.","We find both methods generally perform poorly when restricted to one-percent tuning, whereas several algorithmic mitigations designed to maintain network plasticity perform surprising well.","In addition, we find that properties designed to measure the network's ability to learn continually indeed correlate with performance under one-percent tuning."],"url":"http://arxiv.org/abs/2404.02113v1","category":"cs.LG"}
{"created":"2024-04-02 17:13:04","title":"ImageNot: A contrast with ImageNet preserves model rankings","abstract":"We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects. We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet. This is true when training models from scratch or fine-tuning them. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.","sentences":["We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects.","We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet.","This is true when training models from scratch or fine-tuning them.","Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets.","We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes.","Our work demonstrates a surprising degree of external validity in the relative performance of image classification models.","This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset."],"url":"http://arxiv.org/abs/2404.02112v1","category":"cs.LG"}
{"created":"2024-04-02 17:12:53","title":"Risk-Aware Real-Time Task Allocation for Stochastic Multi-Agent Systems under STL Specifications","abstract":"This paper addresses the control synthesis of heterogeneous stochastic linear multi-agent systems with real-time allocation of signal temporal logic (STL) specifications. Based on previous work, we decompose specifications into sub-specifications on the individual agent level. To leverage the efficiency of task allocation, a heuristic filter evaluates potential task allocation based on STL robustness. Subsequently, an auctioning algorithm determines the definite allocation of specifications. Finally, a control strategy is synthesized for each agent-specification pair using tube-based Model Predictive Control (MPC), ensuring provable probabilistic satisfaction. We demonstrate the efficacy of the proposed methods using a multi-bus scenario that highlights a promising extension to autonomous driving applications like crossing an intersection.","sentences":["This paper addresses the control synthesis of heterogeneous stochastic linear multi-agent systems with real-time allocation of signal temporal logic (STL) specifications.","Based on previous work, we decompose specifications into sub-specifications on the individual agent level.","To leverage the efficiency of task allocation, a heuristic filter evaluates potential task allocation based on STL robustness.","Subsequently, an auctioning algorithm determines the definite allocation of specifications.","Finally, a control strategy is synthesized for each agent-specification pair using tube-based Model Predictive Control (MPC), ensuring provable probabilistic satisfaction.","We demonstrate the efficacy of the proposed methods using a multi-bus scenario that highlights a promising extension to autonomous driving applications like crossing an intersection."],"url":"http://arxiv.org/abs/2404.02111v1","category":"eess.SY"}
{"created":"2024-04-02 17:08:23","title":"Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average Reward Markov Decision Processes","abstract":"We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes. The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\\tilde{\\mathcal{O}}(T^{3/4})$.","sentences":["We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes.","The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\\tilde{\\mathcal{O}}(T^{3/4})$."],"url":"http://arxiv.org/abs/2404.02108v1","category":"cs.LG"}
{"created":"2024-04-02 17:04:45","title":"Neural Ordinary Differential Equation based Sequential Image Registration for Dynamic Characterization","abstract":"Deformable image registration (DIR) is crucial in medical image analysis, enabling the exploration of biological dynamics such as organ motions and longitudinal changes in imaging. Leveraging Neural Ordinary Differential Equations (ODE) for registration, this extension work discusses how this framework can aid in the characterization of sequential biological processes. Utilizing the Neural ODE's ability to model state derivatives with neural networks, our Neural Ordinary Differential Equation Optimization-based (NODEO) framework considers voxels as particles within a dynamic system, defining deformation fields through the integration of neural differential equations. This method learns dynamics directly from data, bypassing the need for physical priors, making it exceptionally suitable for medical scenarios where such priors are unavailable or inapplicable. Consequently, the framework can discern underlying dynamics and use sequence data to regularize the transformation trajectory. We evaluated our framework on two clinical datasets: one for cardiac motion tracking and another for longitudinal brain MRI analysis. Demonstrating its efficacy in both 2D and 3D imaging scenarios, our framework offers flexibility and model agnosticism, capable of managing image sequences and facilitating label propagation throughout these sequences. This study provides a comprehensive understanding of how the Neural ODE-based framework uniquely benefits the image registration challenge.","sentences":["Deformable image registration (DIR) is crucial in medical image analysis, enabling the exploration of biological dynamics such as organ motions and longitudinal changes in imaging.","Leveraging Neural Ordinary Differential Equations (ODE) for registration, this extension work discusses how this framework can aid in the characterization of sequential biological processes.","Utilizing the Neural ODE's ability to model state derivatives with neural networks, our Neural Ordinary Differential Equation Optimization-based (NODEO) framework considers voxels as particles within a dynamic system, defining deformation fields through the integration of neural differential equations.","This method learns dynamics directly from data, bypassing the need for physical priors, making it exceptionally suitable for medical scenarios where such priors are unavailable or inapplicable.","Consequently, the framework can discern underlying dynamics and use sequence data to regularize the transformation trajectory.","We evaluated our framework on two clinical datasets: one for cardiac motion tracking and another for longitudinal brain MRI analysis.","Demonstrating its efficacy in both 2D and 3D imaging scenarios, our framework offers flexibility and model agnosticism, capable of managing image sequences and facilitating label propagation throughout these sequences.","This study provides a comprehensive understanding of how the Neural ODE-based framework uniquely benefits the image registration challenge."],"url":"http://arxiv.org/abs/2404.02106v1","category":"cs.CV"}
{"created":"2024-04-02 17:03:40","title":"Quantum Hall effect in a CVD-grown oxide","abstract":"Two-dimensional electron systems (2DES) are promising for investigating correlated quantum phenomena. In particular, 2D oxides provide a platform that can host various quantum phases such as quantized Hall effect, superconductivity, or magnetism. The realization of such quantum phases in 2D oxides heavily relies on dedicated heterostructure growths. Here we show the integer quantum Hall effect achieved in chemical vapor deposition grown Bi2O2Se - a representative member of a more accessible oxide family. A single or few sub-band 2DES can be prepared in thin films of Bi2O2Se, where the film thickness acts as the sole design parameter and the sub-band occupation is determined by the electric field effect. This new oxide platform exhibits characteristic advantages in structural flexibility due to its layered nature, making it suitable for scalable growth. The unique small mass distinguishes Bi2O2Se from other high-mobility oxides, providing a new platform for exploring quantum Hall physics in 2D oxides.","sentences":["Two-dimensional electron systems (2DES) are promising for investigating correlated quantum phenomena.","In particular, 2D oxides provide a platform that can host various quantum phases such as quantized Hall effect, superconductivity, or magnetism.","The realization of such quantum phases in 2D oxides heavily relies on dedicated heterostructure growths.","Here we show the integer quantum Hall effect achieved in chemical vapor deposition grown Bi2O2Se - a representative member of a more accessible oxide family.","A single or few sub-band 2DES can be prepared in thin films of Bi2O2Se, where the film thickness acts as the sole design parameter and the sub-band occupation is determined by the electric field effect.","This new oxide platform exhibits characteristic advantages in structural flexibility due to its layered nature, making it suitable for scalable growth.","The unique small mass distinguishes Bi2O2Se from other high-mobility oxides, providing a new platform for exploring quantum Hall physics in 2D oxides."],"url":"http://arxiv.org/abs/2404.02104v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-02 16:43:39","title":"High-dimensional covariance regression with application to co-expression QTL detection","abstract":"While covariance matrices have been widely studied in many scientific fields, relatively limited progress has been made on estimating conditional covariances that permits a large covariance matrix to vary with high-dimensional subject-level covariates. In this paper, we present a new sparse multivariate regression framework that models the covariance matrix as a function of subject-level covariates. In the context of co-expression quantitative trait locus (QTL) studies, our method can be used to determine if and how gene co-expressions vary with genetic variations. To accommodate high-dimensional responses and covariates, we stipulate a combined sparsity structure that encourages covariates with non-zero effects and edges that are modulated by these covariates to be simultaneously sparse. We approach parameter estimation with a blockwise coordinate descent algorithm, and investigate the $\\ell_2$ convergence rate of the estimated parameters. In addition, we propose a computationally efficient debiased inference procedure for uncertainty quantification. The efficacy of the proposed method is demonstrated through numerical experiments and an application to a gene co-expression network study with brain cancer patients.","sentences":["While covariance matrices have been widely studied in many scientific fields, relatively limited progress has been made on estimating conditional covariances that permits a large covariance matrix to vary with high-dimensional subject-level covariates.","In this paper, we present a new sparse multivariate regression framework that models the covariance matrix as a function of subject-level covariates.","In the context of co-expression quantitative trait locus (QTL) studies, our method can be used to determine if and how gene co-expressions vary with genetic variations.","To accommodate high-dimensional responses and covariates, we stipulate a combined sparsity structure that encourages covariates with non-zero effects and edges that are modulated by these covariates to be simultaneously sparse.","We approach parameter estimation with a blockwise coordinate descent algorithm, and investigate the $\\ell_2$ convergence rate of the estimated parameters.","In addition, we propose a computationally efficient debiased inference procedure for uncertainty quantification.","The efficacy of the proposed method is demonstrated through numerical experiments and an application to a gene co-expression network study with brain cancer patients."],"url":"http://arxiv.org/abs/2404.02093v1","category":"stat.ME"}
{"created":"2024-04-02 16:31:07","title":"On the model uncertainties for the predicted muon content of extensive air showers","abstract":"Motivated by the excess of the muon content of cosmic ray induced extensive air showers (EAS), relative to EAS modeling, observed by the Pierre Auger Observatory, and by the tension between Auger data and air shower simulations on the maximal muon production depth $X^{\\mu}_{\\max}$, we investigate the possibility to modify the corresponding EAS simulation results, within the Standard Model of particle physics. We start by specifying the kinematic range for secondary hadron production, which is of relevance for such predictions. We further investigate the impact on the predicted EAS muon number and on $X^{\\mu}_{\\max}$ of various modifications of the treatment of hadronic interactions, in the framework of the QGSJET-III model, in particular the model calibration to accelerator data, the amount of the \"glue\" in the pion, and the energy dependence of the pion exchange process. None of the considered modifications of the model allowed us to enhance the EAS muon content by more than 10\\%. On the other hand, for the maximal muon production depth, some of the studied modifications of particle production give rise up to $\\sim 10$ g/cm$^2$ larger $X^{\\mu}_{\\max}$ values, which increases the difference with Auger observations.","sentences":["Motivated by the excess of the muon content of cosmic ray induced extensive air showers (EAS), relative to EAS modeling, observed by the Pierre Auger Observatory, and by the tension between Auger data and air shower simulations on the maximal muon production depth $X^{\\mu}_{\\max}$, we investigate the possibility to modify the corresponding EAS simulation results, within the Standard Model of particle physics.","We start by specifying the kinematic range for secondary hadron production, which is of relevance for such predictions.","We further investigate the impact on the predicted EAS muon number and on $X^{\\mu}_{\\max}$ of various modifications of the treatment of hadronic interactions, in the framework of the QGSJET-III model, in particular the model calibration to accelerator data, the amount of the \"glue\" in the pion, and the energy dependence of the pion exchange process.","None of the considered modifications of the model allowed us to enhance the EAS muon content by more than 10\\%.","On the other hand, for the maximal muon production depth, some of the studied modifications of particle production give rise up to $\\sim 10$ g/cm$^2$ larger $X^{\\mu}_{\\max}$ values, which increases the difference with Auger observations."],"url":"http://arxiv.org/abs/2404.02085v1","category":"hep-ph"}
{"created":"2024-04-02 16:29:28","title":"A Stabilized Parametric Finite Element Method for Surface Diffusion with an Arbitrary Surface Energy","abstract":"We proposed a structure-preserving stabilized parametric finite element method (SPFEM) for the evolution of closed curves under anisotropic surface diffusion with an arbitrary surface energy $\\hat{\\gamma}(\\theta)$. By introducing a non-negative stabilizing function $k(\\theta)$ depending on $\\hat{\\gamma}(\\theta)$, we obtained a novel stabilized conservative weak formulation for the anisotropic surface diffusion. A SPFEM is presented for the discretization of this weak formulation. We construct a comprehensive framework to analyze and prove the unconditional energy stability of the SPFEM under a very mild condition on $\\hat{\\gamma}(\\theta)$. This method can be applied to simulate solid-state dewetting of thin films with arbitrary surface energies, which are characterized by anisotropic surface diffusion and contact line migration. Extensive numerical results are reported to demonstrate the efficiency, accuracy and structure-preserving properties of the proposed SPFEM with anisotropic surface energies $\\hat{\\gamma}(\\theta)$ arising from different applications.","sentences":["We proposed a structure-preserving stabilized parametric finite element method (SPFEM) for the evolution of closed curves under anisotropic surface diffusion with an arbitrary surface energy $\\hat{\\gamma}(\\theta)$. By introducing a non-negative stabilizing function $k(\\theta)$ depending on $\\hat{\\gamma}(\\theta)$, we obtained a novel stabilized conservative weak formulation for the anisotropic surface diffusion.","A SPFEM is presented for the discretization of this weak formulation.","We construct a comprehensive framework to analyze and prove the unconditional energy stability of the SPFEM under a very mild condition on $\\hat{\\gamma}(\\theta)$. This method can be applied to simulate solid-state dewetting of thin films with arbitrary surface energies, which are characterized by anisotropic surface diffusion and contact line migration.","Extensive numerical results are reported to demonstrate the efficiency, accuracy and structure-preserving properties of the proposed SPFEM with anisotropic surface energies $\\hat{\\gamma}(\\theta)$ arising from different applications."],"url":"http://arxiv.org/abs/2404.02083v1","category":"math.NA"}
{"created":"2024-04-02 16:19:46","title":"Granular aqueous suspensions with controlled inter-particular friction and adhesion","abstract":"We present a simple route to obtain large quantities of suspensions of non-Brownian particles with stimuli-responsive surface properties to study the relation between their flow and interparticle interactions. We perform an alkaline hydrolysis reaction on poly(methyl methacrylate) (PMMA) particles to obtain poly(sodium methacrylate) (PMAA-Na) particles. We characterize the quasi-static macroscopic frictional response of their aqueous suspensions using a rotating drum. The suspensions are frictionless when the particles are dispersed in pure water. We relate this state to the presence of electrosteric repulsion between the charged surfaces of the ionized PMAA-Na particles in water. Then we add monovalent and multivalent ions (Na+, Ca2+, La3+) and we observe that the suspensions become frictional whatever the valency. For divalent and trivalent ions, the quasi-static avalanche angle {\\theta}c at large ionic strength is greater than that of frictional PMMA particles in water, suggesting the presence of adhesion. Finally, a decrease in the pH of the suspending solution leads to a transition between a frictionless plateau and a frictional one. We perform Atomic Force Microscopy (AFM) to relate our macroscopic observations to the surface features of the particles. In particular, we show that the increase in friction in the presence of multivalent ions or under acidic conditions is driven by a nanoscopic phase separation and the bundling of polyelectrolyte chains at the surface of the particle. Our results highlight the importance of surface interactions in the rheology of granular suspensions. Our particles provide a simple, yet flexible platform to study frictional suspension flows.","sentences":["We present a simple route to obtain large quantities of suspensions of non-Brownian particles with stimuli-responsive surface properties to study the relation between their flow and interparticle interactions.","We perform an alkaline hydrolysis reaction on poly(methyl methacrylate) (PMMA) particles to obtain poly(sodium methacrylate) (PMAA-Na) particles.","We characterize the quasi-static macroscopic frictional response of their aqueous suspensions using a rotating drum.","The suspensions are frictionless when the particles are dispersed in pure water.","We relate this state to the presence of electrosteric repulsion between the charged surfaces of the ionized PMAA-Na particles in water.","Then we add monovalent and multivalent ions (Na+, Ca2+, La3+) and we observe that the suspensions become frictional whatever the valency.","For divalent and trivalent ions, the quasi-static avalanche angle {\\theta}c at large ionic strength is greater than that of frictional PMMA particles in water, suggesting the presence of adhesion.","Finally, a decrease in the pH of the suspending solution leads to a transition between a frictionless plateau and a frictional one.","We perform Atomic Force Microscopy (AFM) to relate our macroscopic observations to the surface features of the particles.","In particular, we show that the increase in friction in the presence of multivalent ions or under acidic conditions is driven by a nanoscopic phase separation and the bundling of polyelectrolyte chains at the surface of the particle.","Our results highlight the importance of surface interactions in the rheology of granular suspensions.","Our particles provide a simple, yet flexible platform to study frictional suspension flows."],"url":"http://arxiv.org/abs/2404.02071v1","category":"cond-mat.soft"}
{"created":"2024-04-02 16:13:29","title":"Data availability and requirements relevant for the Ariel space mission and other exoplanet atmosphere applications","abstract":"The goal of this white paper is to provide a snapshot of the data availability and data needs primarily for the Ariel space mission, but also for related atmospheric studies of exoplanets and brown dwarfs. It covers the following data-related topics: molecular and atomic line lists, line profiles, computed cross-sections and opacities, collision-induced absorption and other continuum data, optical properties of aerosols and surfaces, atmospheric chemistry, UV photodissociation and photoabsorption cross-sections, and standards in the description and format of such data. These data aspects are discussed by addressing the following questions for each topic, based on the experience of the \"data-provider\" and \"data-user\" communities: (1) what are the types and sources of currently available data, (2) what work is currently in progress, and (3) what are the current and anticipated data needs. We present a GitHub platform for Ariel-related data, with the goal to provide a go-to place for both data-users and data-providers, for the users to make requests for their data needs and for the data-providers to link to their available data. Our aim throughout the paper is to provide practical information on existing sources of data whether in databases, theoretical, or literature sources.","sentences":["The goal of this white paper is to provide a snapshot of the data availability and data needs primarily for the Ariel space mission, but also for related atmospheric studies of exoplanets and brown dwarfs.","It covers the following data-related topics: molecular and atomic line lists, line profiles, computed cross-sections and opacities, collision-induced absorption and other continuum data, optical properties of aerosols and surfaces, atmospheric chemistry, UV photodissociation and photoabsorption cross-sections, and standards in the description and format of such data.","These data aspects are discussed by addressing the following questions for each topic, based on the experience of the \"data-provider\" and \"data-user\" communities: (1) what are the types and sources of currently available data, (2) what work is currently in progress, and (3) what are the current and anticipated data needs.","We present a GitHub platform for Ariel-related data, with the goal to provide a go-to place for both data-users and data-providers, for the users to make requests for their data needs and for the data-providers to link to their available data.","Our aim throughout the paper is to provide practical information on existing sources of data whether in databases, theoretical, or literature sources."],"url":"http://arxiv.org/abs/2404.02188v1","category":"astro-ph.IM"}
{"created":"2024-04-03 16:23:37","title":"Cross-Modal Conditioned Reconstruction for Language-guided Medical Image Segmentation","abstract":"Recent developments underscore the potential of textual information in enhancing learning models for a deeper understanding of medical visual semantics. However, language-guided medical image segmentation still faces a challenging issue. Previous works employ implicit and ambiguous architectures to embed textual information. This leads to segmentation results that are inconsistent with the semantics represented by the language, sometimes even diverging significantly. To this end, we propose a novel cross-modal conditioned Reconstruction for Language-guided Medical Image Segmentation (RecLMIS) to explicitly capture cross-modal interactions, which assumes that well-aligned medical visual features and medical notes can effectively reconstruct each other. We introduce conditioned interaction to adaptively predict patches and words of interest. Subsequently, they are utilized as conditioning factors for mutual reconstruction to align with regions described in the medical notes. Extensive experiments demonstrate the superiority of our RecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+ dataset and achieving an average increase of 1.89% mIoU for cross-domain tests on our QATA-CoV19 dataset. Simultaneously, we achieve a relative reduction of 20.2% in parameter count and a 55.5% decrease in computational load. The code will be available at https://github.com/ShashankHuang/RecLMIS.","sentences":["Recent developments underscore the potential of textual information in enhancing learning models for a deeper understanding of medical visual semantics.","However, language-guided medical image segmentation still faces a challenging issue.","Previous works employ implicit and ambiguous architectures to embed textual information.","This leads to segmentation results that are inconsistent with the semantics represented by the language, sometimes even diverging significantly.","To this end, we propose a novel cross-modal conditioned Reconstruction for Language-guided Medical Image Segmentation (RecLMIS) to explicitly capture cross-modal interactions, which assumes that well-aligned medical visual features and medical notes can effectively reconstruct each other.","We introduce conditioned interaction to adaptively predict patches and words of interest.","Subsequently, they are utilized as conditioning factors for mutual reconstruction to align with regions described in the medical notes.","Extensive experiments demonstrate the superiority of our RecLMIS, surpassing LViT by 3.74% mIoU on the publicly available MosMedData+ dataset and achieving an average increase of 1.89% mIoU for cross-domain tests on our QATA-CoV19 dataset.","Simultaneously, we achieve a relative reduction of 20.2% in parameter count and a 55.5% decrease in computational load.","The code will be available at https://github.com/ShashankHuang/RecLMIS."],"url":"http://arxiv.org/abs/2404.02845v1","category":"cs.CV"}
{"created":"2024-04-03 15:19:40","title":"Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations","abstract":"In this note, we extend the analysis for the residual-based a posteriori error estimators in the energy norm defined for the algebraic flux correction (AFC) schemes [Jha20.CAMWA] to the newly proposed algebraic stabilization schemes [JK21.NM, Kn23.NA]. Numerical simulations on adaptively refined grids are performed in two dimensions showing the higher efficiency of an algebraic stabilization with similar accuracy compared with an AFC scheme.","sentences":["In this note, we extend the analysis for the residual-based a posteriori error estimators in the energy norm defined for the algebraic flux correction (AFC) schemes [Jha20.CAMWA] to the newly proposed algebraic stabilization schemes [JK21.NM, Kn23.NA].","Numerical simulations on adaptively refined grids are performed in two dimensions showing the higher efficiency of an algebraic stabilization with similar accuracy compared with an AFC scheme."],"url":"http://arxiv.org/abs/2404.02804v1","category":"math.NA"}
{"created":"2024-04-03 14:39:47","title":"FPT: Feature Prompt Tuning for Few-shot Readability Assessment","abstract":"Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features. Also, our proposed model significantly outperforms the large language model gpt-3.5-turbo-16k in most cases. Our proposed method establishes a new architecture for prompt tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks.","sentences":["Prompt-based methods have achieved promising results in most few-shot text classification tasks.","However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential.","Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.","To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT).","Specifically, we extract linguistic features from the text and embed them into trainable soft prompts.","Further, we devise a new loss function to calibrate the similarity ranking order between categories.","Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features.","Also, our proposed model significantly outperforms the large language model gpt-3.5-turbo-16k in most cases.","Our proposed method establishes a new architecture for prompt tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks."],"url":"http://arxiv.org/abs/2404.02772v1","category":"cs.CL"}
{"created":"2024-04-03 14:33:49","title":"Locking-free hybrid high-order method for linear elasticity","abstract":"The hybrid-high order (HHO) scheme has many successful applications including linear elasticity as the first step towards computational solid mechanics. The striking advantage is the simplicity among other higher-order nonconforming schemes and its geometric flexibility as a polytopal method on the expanse of a parameter-free refined stabilization. The classical suggestion of a locking-free HHO discretization requires a split of the the reconstruction terms with an additional reconstruction of the divergence operator that might be motivated by the Stokes equations for the robust approximation in the incompressible limit, when one Lam\\'e parameter $\\lambda\\to\\infty$ becomes very large. This paper utilizes just one reconstruction operator for the linear Green strain and therefore does not rely on a split in deviatoric and spherical behavior. The a priori error analysis provides quasi-best approximation with $\\lambda$-independent equivalence constants. The reliable and (up to data oscillations) efficient a posteriori error estimates are stabilization-free and $\\lambda$-robust. The error analysis is carried out on simplicial meshes to allow conforming piecewise polynomials finite elements in the kernel of the stabilization terms. Numerical benchmarks provide empirical evidence for optimal convergence rates of the a posteriori error estimator in some associated adaptive mesh-refining algorithm also in the incompressible limit.","sentences":["The hybrid-high order (HHO) scheme has many successful applications including linear elasticity as the first step towards computational solid mechanics.","The striking advantage is the simplicity among other higher-order nonconforming schemes and its geometric flexibility as a polytopal method on the expanse of a parameter-free refined stabilization.","The classical suggestion of a locking-free HHO discretization requires a split of the the reconstruction terms with an additional reconstruction of the divergence operator that might be motivated by the Stokes equations for the robust approximation in the incompressible limit, when one Lam\\'e parameter $\\lambda\\to\\infty$ becomes very large.","This paper utilizes just one reconstruction operator for the linear Green strain and therefore does not rely on a split in deviatoric and spherical behavior.","The a priori error analysis provides quasi-best approximation with $\\lambda$-independent equivalence constants.","The reliable and (up to data oscillations) efficient a posteriori error estimates are stabilization-free and $\\lambda$-robust.","The error analysis is carried out on simplicial meshes to allow conforming piecewise polynomials finite elements in the kernel of the stabilization terms.","Numerical benchmarks provide empirical evidence for optimal convergence rates of the a posteriori error estimator in some associated adaptive mesh-refining algorithm also in the incompressible limit."],"url":"http://arxiv.org/abs/2404.02768v1","category":"math.NA"}
{"created":"2024-04-03 14:29:42","title":"Fast Diffusion Model For Seismic Data Noise Attenuation","abstract":"Noise is one of the primary sources of interference in seismic exploration. Many authors have proposed various methods to remove noise from seismic data; however, in the face of strong noise conditions, satisfactory results are often not achievable. In recent years, methods based on diffusion models have been applied to the task of strong noise processing in seismic data. However, due to iterative computations, the computational efficiency of diffusion-based methods is much lower than conventional methods. To address this issue, we propose using an improved Bayesian equation for iterations, removing the stochastic terms from the computation. Additionally, we proposed a new normalization method adapted to the diffusion model. Through various improvements, on synthetic datasets and field datasets, our proposed method achieves significantly better noise attenuation effects compared to the benchmark methods, while also achieving a several-fold increase in computational speed. We employ transfer learning to demonstrate the robustness of our proposed method on open-source synthetic seismic data and validate on open-source field data sets. Finally, we open-sourced the code to promote the development of high-precision and efficient seismic exploration work.","sentences":["Noise is one of the primary sources of interference in seismic exploration.","Many authors have proposed various methods to remove noise from seismic data; however, in the face of strong noise conditions, satisfactory results are often not achievable.","In recent years, methods based on diffusion models have been applied to the task of strong noise processing in seismic data.","However, due to iterative computations, the computational efficiency of diffusion-based methods is much lower than conventional methods.","To address this issue, we propose using an improved Bayesian equation for iterations, removing the stochastic terms from the computation.","Additionally, we proposed a new normalization method adapted to the diffusion model.","Through various improvements, on synthetic datasets and field datasets, our proposed method achieves significantly better noise attenuation effects compared to the benchmark methods, while also achieving a several-fold increase in computational speed.","We employ transfer learning to demonstrate the robustness of our proposed method on open-source synthetic seismic data and validate on open-source field data sets.","Finally, we open-sourced the code to promote the development of high-precision and efficient seismic exploration work."],"url":"http://arxiv.org/abs/2404.02767v1","category":"physics.geo-ph"}
{"created":"2024-04-03 10:25:45","title":"Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition","abstract":"Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN). In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions. In this paper, we propose self-attention GCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets. We utilize spatial self-attention module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal self-attention module to examine correlations between frames of a node. These two are followed by multi-scale convolution network with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors. They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier.","sentences":["Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN).","In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions.","In this paper, we propose self-attention GCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets.","We utilize spatial self-attention module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal self-attention module to examine correlations between frames of a node.","These two are followed by multi-scale convolution network with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors.","They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier."],"url":"http://arxiv.org/abs/2404.02624v1","category":"cs.CV"}
{"created":"2024-04-03 07:27:33","title":"Large Language Model for Vulnerability Detection and Repair: Literature Review and Roadmap","abstract":"The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous recent studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, covering 36 papers published at 21 distinct venues. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of challenges that still need to be tackled considering existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.","sentences":["The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair.","Numerous recent studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks.","Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair.","In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs.","The review encompasses research work from leading SE, AI, and Security conferences and journals, covering 36 papers published at 21 distinct venues.","By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair.","Based on our findings, we have identified a series of challenges that still need to be tackled considering existing studies.","Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors."],"url":"http://arxiv.org/abs/2404.02525v1","category":"cs.SE"}
{"created":"2024-04-03 05:04:06","title":"On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study","abstract":"This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training. One such domain is IoT applications. Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions. More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional supervised deep neural networks (DNNs). We also demonstrate its superior convergence over supervised solutions. Our findings highlight the advantages of vibration-based FMs (and FM-inspired selfsupervised models in general) in terms of inference robustness, runtime efficiency, and model adaptation (via fine-tuning) in resource-limited IoT settings.","sentences":["This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications.","A case study is presented featuring a vehicle classification application using acoustic and seismic sensing.","The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training.","One such domain is IoT applications.","Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data.","The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions.","More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional supervised deep neural networks (DNNs).","We also demonstrate its superior convergence over supervised solutions.","Our findings highlight the advantages of vibration-based FMs (and FM-inspired selfsupervised models in general) in terms of inference robustness, runtime efficiency, and model adaptation (via fine-tuning) in resource-limited IoT settings."],"url":"http://arxiv.org/abs/2404.02461v1","category":"cs.LG"}
{"created":"2024-04-03 02:27:03","title":"Quantitative Hydrodynamic Stability for Couette Flow on Unbounded Domains with Navier Boundary Conditions","abstract":"We prove a stability threshold theorem for 2D Navier-Stokes on three unbounded domains: the whole plane $\\mathbb{R} \\times \\mathbb{R}$, the half plane $\\mathbb{R} \\times [0,\\infty)$ with Navier boundary conditions, and the infinite channel $\\mathbb{R} \\times [-1, 1]$ with Navier boundary conditions. Starting with the Couette shear flow, we consider initial perturbations $\\omega_{in}$ which are of size $\\nu^{1/2}(1+\\ln(1/\\nu)^{1/2})^{-1}$ in an anisotropic Sobolev space with an additional low frequency control condition for the planar cases. We then demonstrate that such perturbations exhibit inviscid damping of the velocity, as well as enhanced dissipation at $x$-frequencies $|k| \\gg \\nu$ with decay time-scale $O(\\nu^{-1/3}|k|^{-2/3})$. On the plane and half-plane, we show Taylor dispersion for $x$-frequencies $|k| \\ll \\nu$ with decay time-scale $O(\\nu |k|^{-2})$, while on the channel we show low frequency dispersion for $|k| \\ll \\nu$ with decay time-scale $O(\\nu^{-1})$. Generalizing the work of arXiv:2311.00141 done on $\\mathbb{T} \\times [-1,1]$, the key contribution of this paper is to perform new nonlinear computations at low frequencies with wave number $|k| \\lesssim \\nu$ and at intermediate frequencies with wave number $\\nu \\lesssim |k| \\leq 1$, and to provide the first enhanced dissipation result for a fully-nonlinear shear flow on an unbounded $x$-domain.","sentences":["We prove a stability threshold theorem for 2D Navier-Stokes on three unbounded domains: the whole plane $\\mathbb{R} \\times \\mathbb{R}$, the half plane $\\mathbb{R} \\times","[0,\\infty)$ with Navier boundary conditions, and the infinite channel $\\mathbb{R} \\times","[-1, 1]$ with Navier boundary conditions.","Starting with the Couette shear flow, we consider initial perturbations $\\omega_{in}$ which are of size $\\nu^{1/2}(1+\\ln(1/\\nu)^{1/2})^{-1}$ in an anisotropic Sobolev space with an additional low frequency control condition for the planar cases.","We then demonstrate that such perturbations exhibit inviscid damping of the velocity, as well as enhanced dissipation at $x$-frequencies $|k| \\gg \\nu$ with decay time-scale $O(\\nu^{-1/3}|k|^{-2/3})$. On the plane and half-plane, we show Taylor dispersion for $x$-frequencies $|k| \\ll \\nu$ with decay time-scale $O(\\nu |k|^{-2})$, while on the channel we show low frequency dispersion for $|k| \\ll \\nu$ with decay time-scale $O(\\nu^{-1})$. Generalizing the work of arXiv:2311.00141 done on $\\mathbb{T} \\times","[-1,1]$, the key contribution of this paper is to perform new nonlinear computations at low frequencies with wave number $|k| \\lesssim \\nu$ and at intermediate frequencies with wave number $\\nu \\lesssim |k| \\leq 1$, and to provide the first enhanced dissipation result for a fully-nonlinear shear flow on an unbounded $x$-domain."],"url":"http://arxiv.org/abs/2404.02412v1","category":"math.AP"}
{"created":"2024-04-03 02:16:30","title":"TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression","abstract":"In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on hand-crafted components in query-based detectors for temporal action detection (TAD). Despite significant advancements towards an end-to-end framework in object detection, query-based detectors have been limited in achieving full end-to-end modeling in TAD. To address this issue, we propose \\modelname{}, a full end-to-end temporal action detection transformer that integrates time-aligned coordinate expression. We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment. Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set. Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors. Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular benchmark datasets. Code is available at: https://github.com/Dotori-HJ/TE-TAD","sentences":["In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on hand-crafted components in query-based detectors for temporal action detection (TAD).","Despite significant advancements towards an end-to-end framework in object detection, query-based detectors have been limited in achieving full end-to-end modeling in TAD.","To address this issue, we propose \\modelname{}, a full end-to-end temporal action detection transformer that integrates time-aligned coordinate expression.","We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment.","Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set.","Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors.","Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular benchmark datasets.","Code is available at: https://github.com/Dotori-HJ/TE-TAD"],"url":"http://arxiv.org/abs/2404.02405v2","category":"cs.CV"}
{"created":"2024-04-03 01:19:15","title":"Locally homogeneous RCD spaces","abstract":"The goal of this note is to demonstrate how existing results can be adapted to establish the following result: A locally metric measure homogeneous $\\mathrm{RCD}(K,N)$ space is isometric to, after multiplying a positive constant to the reference measure, a smooth Riemannian manifold with the Riemannian volume measure.","sentences":["The goal of this note is to demonstrate how existing results can be adapted to establish the following result: A locally metric measure homogeneous $\\mathrm{RCD}(K,N)$ space is isometric to, after multiplying a positive constant to the reference measure, a smooth Riemannian manifold with the Riemannian volume measure."],"url":"http://arxiv.org/abs/2404.02390v1","category":"math.DG"}
{"created":"2024-04-02 23:19:02","title":"Task-priority Intermediated Hierarchical Distributed Policies: Reinforcement Learning of Adaptive Multi-robot Cooperative Transport","abstract":"Multi-robot cooperative transport is crucial in logistics, housekeeping, and disaster response. However, it poses significant challenges in environments where objects of various weights are mixed and the number of robots and objects varies. This paper presents Task-priority Intermediated Hierarchical Distributed Policies (TIHDP), a multi-agent Reinforcement Learning (RL) framework that addresses these challenges through a hierarchical policy structure. TIHDP consists of three layers: task allocation policy (higher layer), dynamic task priority (intermediate layer), and robot control policy (lower layer). Whereas the dynamic task priority layer can manipulate the priority of any object to be transported by receiving global object information and communicating with other robots, the task allocation and robot control policies are restricted by local observations/actions so that they are not affected by changes in the number of objects and robots. Through simulations and real-robot demonstrations, TIHDP shows promising adaptability and performance of the learned multi-robot cooperative transport, even in environments with varying numbers of robots and objects. Video is available at https://youtu.be/Rmhv5ovj0xM","sentences":["Multi-robot cooperative transport is crucial in logistics, housekeeping, and disaster response.","However, it poses significant challenges in environments where objects of various weights are mixed and the number of robots and objects varies.","This paper presents Task-priority Intermediated Hierarchical Distributed Policies (TIHDP), a multi-agent Reinforcement Learning (RL) framework that addresses these challenges through a hierarchical policy structure.","TIHDP consists of three layers: task allocation policy (higher layer), dynamic task priority (intermediate layer), and robot control policy (lower layer).","Whereas the dynamic task priority layer can manipulate the priority of any object to be transported by receiving global object information and communicating with other robots, the task allocation and robot control policies are restricted by local observations/actions so that they are not affected by changes in the number of objects and robots.","Through simulations and real-robot demonstrations, TIHDP shows promising adaptability and performance of the learned multi-robot cooperative transport, even in environments with varying numbers of robots and objects.","Video is available at https://youtu.be/Rmhv5ovj0xM"],"url":"http://arxiv.org/abs/2404.02362v1","category":"cs.RO"}
{"created":"2024-04-02 21:51:23","title":"Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods","abstract":"Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors. However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD. In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions. Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task. The robots then execute the task using sensorimotor actions and reinforcement learning (RL) policies when required. A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration. In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes. We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains. The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations.","sentences":["Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors.","However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD.","In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions.","Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task.","The robots then execute the task using sensorimotor actions and reinforcement learning (RL) policies when required.","A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration.","In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes.","We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains.","The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations."],"url":"http://arxiv.org/abs/2404.02324v1","category":"cs.RO"}
{"created":"2024-04-02 21:33:57","title":"ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models","abstract":"Incorporating language comprehension into robotic operations unlocks significant advancements in robotics, but also presents distinct challenges, particularly in executing spatially oriented tasks like pattern formation. This paper introduces ZeroCAP, a novel system that integrates large language models with multi-robot systems for zero-shot context aware pattern formation. Grounded in the principles of language-conditioned robotics, ZeroCAP leverages the interpretative power of language models to translate natural language instructions into actionable robotic configurations. This approach combines the synergy of vision-language models, cutting-edge segmentation techniques and shape descriptors, enabling the realization of complex, context-driven pattern formations in the realm of multi robot coordination. Through extensive experiments, we demonstrate the systems proficiency in executing complex context aware pattern formations across a spectrum of tasks, from surrounding and caging objects to infilling regions. This not only validates the system's capability to interpret and implement intricate context-driven tasks but also underscores its adaptability and effectiveness across varied environments and scenarios. More details about this work are available at: https://sites.google.com/view/zerocap/home","sentences":["Incorporating language comprehension into robotic operations unlocks significant advancements in robotics, but also presents distinct challenges, particularly in executing spatially oriented tasks like pattern formation.","This paper introduces ZeroCAP, a novel system that integrates large language models with multi-robot systems for zero-shot context aware pattern formation.","Grounded in the principles of language-conditioned robotics, ZeroCAP leverages the interpretative power of language models to translate natural language instructions into actionable robotic configurations.","This approach combines the synergy of vision-language models, cutting-edge segmentation techniques and shape descriptors, enabling the realization of complex, context-driven pattern formations in the realm of multi robot coordination.","Through extensive experiments, we demonstrate the systems proficiency in executing complex context aware pattern formations across a spectrum of tasks, from surrounding and caging objects to infilling regions.","This not only validates the system's capability to interpret and implement intricate context-driven tasks but also underscores its adaptability and effectiveness across varied environments and scenarios.","More details about this work are available at: https://sites.google.com/view/zerocap/home"],"url":"http://arxiv.org/abs/2404.02318v1","category":"cs.RO"}
{"created":"2024-04-02 20:32:32","title":"Federated Multi-Agent Mapping for Planetary Exploration","abstract":"In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiveness for real-world deployment in multi-agent exploration scenarios.","sentences":["In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge.","Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning.","FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints.","Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations.","We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures.","This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers.","We rigorously evaluate this approach, demonstrating its effectiveness for real-world deployment in multi-agent exploration scenarios."],"url":"http://arxiv.org/abs/2404.02289v1","category":"cs.RO"}
{"created":"2024-04-02 20:23:10","title":"LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP","abstract":"In a recent, strongly emergent literature on few-shot CLIP adaptation, Linear Probe (LP) has been often reported as a weak baseline. This has motivated intensive research building convoluted prompt learning or feature adaptation strategies. In this work, we propose and examine from convex-optimization perspectives a generalization of the standard LP baseline, in which the linear classifier weights are learnable functions of the text embedding, with class-wise multipliers blending image and text knowledge. As our objective function depends on two types of variables, i.e., the class visual prototypes and the learnable blending parameters, we propose a computationally efficient block coordinate Majorize-Minimize (MM) descent algorithm. In our full-batch MM optimizer, which we coin LP++, step sizes are implicit, unlike standard gradient descent practices where learning rates are intensively searched over validation sets. By examining the mathematical properties of our loss (e.g., Lipschitz gradient continuity), we build majorizing functions yielding data-driven learning rates and derive approximations of the loss's minima, which provide data-informed initialization of the variables. Our image-language objective function, along with these non-trivial optimization insights and ingredients, yields, surprisingly, highly competitive few-shot CLIP performances. Furthermore, LP++ operates in black-box, relaxes intensive validation searches for the optimization hyper-parameters, and runs orders-of-magnitudes faster than state-of-the-art few-shot CLIP adaptation methods. Our code is available at: \\url{https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline.git}.","sentences":["In a recent, strongly emergent literature on few-shot CLIP adaptation, Linear Probe (LP) has been often reported as a weak baseline.","This has motivated intensive research building convoluted prompt learning or feature adaptation strategies.","In this work, we propose and examine from convex-optimization perspectives a generalization of the standard LP baseline, in which the linear classifier weights are learnable functions of the text embedding, with class-wise multipliers blending image and text knowledge.","As our objective function depends on two types of variables, i.e., the class visual prototypes and the learnable blending parameters, we propose a computationally efficient block coordinate Majorize-Minimize (MM) descent algorithm.","In our full-batch MM optimizer, which we coin LP++, step sizes are implicit, unlike standard gradient descent practices where learning rates are intensively searched over validation sets.","By examining the mathematical properties of our loss (e.g., Lipschitz gradient continuity), we build majorizing functions yielding data-driven learning rates and derive approximations of the loss's minima, which provide data-informed initialization of the variables.","Our image-language objective function, along with these non-trivial optimization insights and ingredients, yields, surprisingly, highly competitive few-shot CLIP performances.","Furthermore, LP++ operates in black-box, relaxes intensive validation searches for the optimization hyper-parameters, and runs orders-of-magnitudes faster than state-of-the-art few-shot CLIP adaptation methods.","Our code is available at: \\url{https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline.git}."],"url":"http://arxiv.org/abs/2404.02285v1","category":"cs.CV"}
{"created":"2024-04-02 19:40:37","title":"Continuous Sculpting: Persistent Swarm Shape Formation Adaptable to Local Environmental Changes","abstract":"Despite their growing popularity, swarms of robots remain limited by the operating time of each individual. We present algorithms which allow a human to sculpt a swarm of robots into a shape that persists in space perpetually, independent of onboard energy constraints such as batteries. Robots generate a path through a shape such that robots cycle in and out of the shape. Robots inside the shape react to human initiated changes and adapt the path through the shape accordingly. Robots outside the shape recharge and return to the shape so that the shape can persist indefinitely. The presented algorithms communicate shape changes throughout the swarm using message passing and robot motion. These algorithms enable the swarm to persist through any arbitrary changes to the shape. We describe these algorithms in detail and present their performance in simulation and on a swarm of mobile robots. The result is a swarm behavior more suitable for extended duration, dynamic shape-based tasks in applications such as agriculture and emergency response.","sentences":["Despite their growing popularity, swarms of robots remain limited by the operating time of each individual.","We present algorithms which allow a human to sculpt a swarm of robots into a shape that persists in space perpetually, independent of onboard energy constraints such as batteries.","Robots generate a path through a shape such that robots cycle in and out of the shape.","Robots inside the shape react to human initiated changes and adapt the path through the shape accordingly.","Robots outside the shape recharge and return to the shape so that the shape can persist indefinitely.","The presented algorithms communicate shape changes throughout the swarm using message passing and robot motion.","These algorithms enable the swarm to persist through any arbitrary changes to the shape.","We describe these algorithms in detail and present their performance in simulation and on a swarm of mobile robots.","The result is a swarm behavior more suitable for extended duration, dynamic shape-based tasks in applications such as agriculture and emergency response."],"url":"http://arxiv.org/abs/2404.02265v1","category":"cs.RO"}
{"created":"2024-04-02 19:07:48","title":"Matrix-weighted estimates beyond Calder\u00f3n-Zygmund theory","abstract":"We investigate matrix-weighted bounds for the linear non-kernel operators considered by F. Bernicot, D. Frey, and S. Petermichl. First, we dominate these operators by bilinear convex body sparse forms, applying a recent general principle due to T. Hyt\\\"onen. Then, we use this domination to derive matrix-weighted bounds, adapting arguments of F. Nazarov, S. Petermichl, S. Treil, and A. Volberg. Our requirements on the weight are formulated in terms of two-exponent matrix Muckenhoupt conditions, which surprisingly exhibit a rich structure that is absent in the scalar case. Consequently, we deduce that our matrix-weighted bounds improve the ones obtained recently by A. Laukkarinen.","sentences":["We investigate matrix-weighted bounds for the linear non-kernel operators considered by F. Bernicot, D. Frey, and S. Petermichl.","First, we dominate these operators by bilinear convex body sparse forms, applying a recent general principle due to T. Hyt\\\"onen.","Then, we use this domination to derive matrix-weighted bounds, adapting arguments of F. Nazarov, S. Petermichl, S. Treil, and A. Volberg.","Our requirements on the weight are formulated in terms of two-exponent matrix Muckenhoupt conditions, which surprisingly exhibit a rich structure that is absent in the scalar case.","Consequently, we deduce that our matrix-weighted bounds improve the ones obtained recently by A. Laukkarinen."],"url":"http://arxiv.org/abs/2404.02246v1","category":"math.CA"}
{"created":"2024-04-02 18:52:28","title":"Proximal Oracles for Optimization and Sampling","abstract":"We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density). In particular, we study two specific settings where the convex objective/potential function is either semi-smooth or in composite form as the finite sum of semi-smooth components. To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution. A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method. We establish the iteration-complexity of the proximal map in both semi-smooth and composite settings. We further propose an adaptive proximal bundle method for non-smooth optimization. The proposed method is universal since it does not need any problem parameters as input. Additionally, we develop a proximal sampling oracle that resembles the proximal map in optimization and establish its complexity using a novel technique (a modified Gaussian integral). Finally, we combine this proximal sampling oracle and ASF to obtain a Markov chain Monte Carlo method with non-asymptotic complexity bounds for sampling in semi-smooth and composite settings.","sentences":["We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density).","In particular, we study two specific settings where the convex objective/potential function is either semi-smooth or in composite form as the finite sum of semi-smooth components.","To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution.","A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method.","We establish the iteration-complexity of the proximal map in both semi-smooth and composite settings.","We further propose an adaptive proximal bundle method for non-smooth optimization.","The proposed method is universal since it does not need any problem parameters as input.","Additionally, we develop a proximal sampling oracle that resembles the proximal map in optimization and establish its complexity using a novel technique (a modified Gaussian integral).","Finally, we combine this proximal sampling oracle and ASF to obtain a Markov chain Monte Carlo method with non-asymptotic complexity bounds for sampling in semi-smooth and composite settings."],"url":"http://arxiv.org/abs/2404.02239v1","category":"math.OC"}
{"created":"2024-04-02 18:14:33","title":"Semantic Information Theory in a feedback-control Kuramoto Model","abstract":"Semantic Information Theory (SIT) offers a new approach to evaluating the information architecture of complex systems. In this study we describe the steps required to {\\it operationalize} SIT via its application to dynamical problems. Our road map has four steps: (1) separating the dynamical system into agent-environment sub-systems; (2) choosing an appropriate coarse graining and quantifying correlations; (3) identifying a measure of viability; (4) implementing a scrambling protocol and measuring the semantic content. We apply the road map to a model inspired by the neural dynamics of epileptic seizures whereby an agent (a control process) attempts to maintain an environment (a base process) in a desynchronized state. The synchronization dynamics is studied through the well-known Kuramoto model of phase synchronization. Our application of SIT to this problem reveals new features of both semantic information and the Kuramoto model. For the latter we find articulating the correlational structure for agent and environment(the oscillators), allows us to cast the model in in a novel computational (information theoretic) perspective, where the agent-environment dynamics can be thought of as analyzing a communication channel. For the former we find that all the information in our system is semantic. This is in contrast to previous SIT studies of foragers in which semantic thresholds where seen above which no further semantic content was obtained.","sentences":["Semantic Information Theory (SIT) offers a new approach to evaluating the information architecture of complex systems.","In this study we describe the steps required to {\\it operationalize} SIT via its application to dynamical problems.","Our road map has four steps: (1) separating the dynamical system into agent-environment sub-systems; (2) choosing an appropriate coarse graining and quantifying correlations; (3) identifying a measure of viability; (4) implementing a scrambling protocol and measuring the semantic content.","We apply the road map to a model inspired by the neural dynamics of epileptic seizures whereby an agent (a control process) attempts to maintain an environment (a base process) in a desynchronized state.","The synchronization dynamics is studied through the well-known Kuramoto model of phase synchronization.","Our application of SIT to this problem reveals new features of both semantic information and the Kuramoto model.","For the latter we find articulating the correlational structure for agent and environment(the oscillators), allows us to cast the model in in a novel computational (information theoretic) perspective, where the agent-environment dynamics can be thought of as analyzing a communication channel.","For the former we find that all the information in our system is semantic.","This is in contrast to previous SIT studies of foragers in which semantic thresholds where seen above which no further semantic content was obtained."],"url":"http://arxiv.org/abs/2404.02221v1","category":"nlin.PS"}
{"created":"2024-04-02 17:58:49","title":"Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration","abstract":"All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation. The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives. We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks. Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training. This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights. Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours. To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples. We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models. The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet.","sentences":["All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation.","The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives.","We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks.","Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training.","This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights.","Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours.","To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples.","We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models.","The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet."],"url":"http://arxiv.org/abs/2404.02154v1","category":"cs.CV"}
{"created":"2024-04-02 17:58:03","title":"Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models","abstract":"Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively. In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated. Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models. Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts.","sentences":["Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images.","However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly.","Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively.","In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation.","Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated.","Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes.","Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models.","Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts."],"url":"http://arxiv.org/abs/2404.02148v1","category":"cs.CV"}
{"created":"2024-04-02 17:27:51","title":"DEMO: Dose Exploration, Monitoring, and Optimization Using a Biological Mediator for Clinical Outcomes","abstract":"Phase 1-2 designs provide a methodological advance over phase 1 designs for dose finding by using both clinical response and toxicity. A phase 1-2 trial still may fail to select a truly optimal dose. because early response is not a perfect surrogate for long term therapeutic success. To address this problem, a generalized phase 1-2 design first uses a phase 1-2 design's components to identify a set of candidate doses, adaptively randomizes patients among the candidates, and after longer follow up selects a dose to maximize long-term success rate. In this paper, we extend this paradigm by proposing a design that exploits an early treatment-related, real-valued biological outcome, such as pharmacodynamic activity or an immunological effect, that may act as a mediator between dose and clinical outcomes, including tumor response, toxicity, and survival time. We assume multivariate dose-outcome models that include effects appearing in causal pathways from dose to the clinical outcomes. Bayesian model selection is used to identify and eliminate biologically inactive doses. At the end of the trial, a therapeutically optimal dose is chosen from the set of doses that are acceptably safe, clinically effective, and biologically active to maximize restricted mean survival time. Results of a simulation study show that the proposed design may provide substantial improvements over designs that ignore the biological variable.","sentences":["Phase 1-2 designs provide a methodological advance over phase 1 designs for dose finding by using both clinical response and toxicity.","A phase 1-2 trial still may fail to select a truly optimal dose.","because early response is not a perfect surrogate for long term therapeutic success.","To address this problem, a generalized phase 1-2 design first uses a phase 1-2 design's components to identify a set of candidate doses, adaptively randomizes patients among the candidates, and after longer follow up selects a dose to maximize long-term success rate.","In this paper, we extend this paradigm by proposing a design that exploits an early treatment-related, real-valued biological outcome, such as pharmacodynamic activity or an immunological effect, that may act as a mediator between dose and clinical outcomes, including tumor response, toxicity, and survival time.","We assume multivariate dose-outcome models that include effects appearing in causal pathways from dose to the clinical outcomes.","Bayesian model selection is used to identify and eliminate biologically inactive doses.","At the end of the trial, a therapeutically optimal dose is chosen from the set of doses that are acceptably safe, clinically effective, and biologically active to maximize restricted mean survival time.","Results of a simulation study show that the proposed design may provide substantial improvements over designs that ignore the biological variable."],"url":"http://arxiv.org/abs/2404.02120v1","category":"stat.AP"}
{"created":"2024-04-02 17:00:11","title":"CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems","abstract":"Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG. CLAPNQ is publicly available at https://github.com/primeqa/clapnq","sentences":["Retrieval Augmented Generation (RAG) has become a popular application for large language models.","It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations.","While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary.","We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline.","ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline.","The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous.","RAG models must adapt to these properties to be successful at ClapNQ.","We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG.","CLAPNQ is publicly available at https://github.com/primeqa/clapnq"],"url":"http://arxiv.org/abs/2404.02103v1","category":"cs.CL"}
{"created":"2024-04-02 16:52:41","title":"CameraCtrl: Enabling Camera Control for Text-to-Video Generation","abstract":"Controllability plays a crucial role in video generation since it allows users to create desired content. However, existing models largely overlooked the precise control of camera pose that serves as a cinematic language to express deeper narrative nuances. To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for text-to-video(T2V) models. After precisely parameterizing the camera trajectory, a plug-and-play camera module is then trained on a T2V model, leaving others untouched. Additionally, a comprehensive study on the effect of various datasets is also conducted, suggesting that videos with diverse camera distribution and similar appearances indeed enhance controllability and generalization. Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise and domain-adaptive camera control, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs. Our project website is at: https://hehao13.github.io/projects-CameraCtrl/.","sentences":["Controllability plays a crucial role in video generation since it allows users to create desired content.","However, existing models largely overlooked the precise control of camera pose that serves as a cinematic language to express deeper narrative nuances.","To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for text-to-video(T2V) models.","After precisely parameterizing the camera trajectory, a plug-and-play camera module is then trained on a T2V model, leaving others untouched.","Additionally, a comprehensive study on the effect of various datasets is also conducted, suggesting that videos with diverse camera distribution and similar appearances indeed enhance controllability and generalization.","Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise and domain-adaptive camera control, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs.","Our project website is at: https://hehao13.github.io/projects-CameraCtrl/."],"url":"http://arxiv.org/abs/2404.02101v1","category":"cs.CV"}
{"created":"2024-04-02 16:30:12","title":"Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images","abstract":"Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets. To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning. Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain. Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability. In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks. Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets.","sentences":["Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets.","To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning.","Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain.","Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability.","In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks.","Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets."],"url":"http://arxiv.org/abs/2404.02084v1","category":"cs.CV"}
{"created":"2024-04-02 16:25:35","title":"Coherent Control of an Optical Quantum Dot Using Phonons and Photons","abstract":"Genuine quantum-mechanical effects are readily observable in modern optomechanical systems comprising bosonic (``classical\") optical resonators. Such systems have enabled laser-cooling of mesoscopic objects, generation of squeezed light, and the conversion of photons between microwave and optical modes. Here we demonstrate unique advantages of optical two-level systems, or qubits, for optomechanics. The qubit state can be coherently controlled using an immense variety of driving schemes including both phonons and resonant or detuned photons. We experimentally demonstrate this using charge-controlled InAs quantum dots (QDs) in surface-acoustic-wave resonators. Time-correlated single-photon counting measurements reveal the control of QD population dynamics using engineered optical pulses and mechanical motion. As a first example, we show how this can improve signal-to-background scattering in microwave-to-optical transduction processes. Specifically, we tailor the scheme so that mechanically assisted photon scattering is enhanced over the direct detuned photon scattering from the optical system. These differences are greatly amplified by strategic temporal pulse shaping. Quantum-mechanical calculations show good agreement with our experimental results and provide guidance for adapting these schemes to small phonon occupancies relevant for microwave-to-optical quantum transduction.","sentences":["Genuine quantum-mechanical effects are readily observable in modern optomechanical systems comprising bosonic (``classical\") optical resonators.","Such systems have enabled laser-cooling of mesoscopic objects, generation of squeezed light, and the conversion of photons between microwave and optical modes.","Here we demonstrate unique advantages of optical two-level systems, or qubits, for optomechanics.","The qubit state can be coherently controlled using an immense variety of driving schemes including both phonons and resonant or detuned photons.","We experimentally demonstrate this using charge-controlled InAs quantum dots (QDs) in surface-acoustic-wave resonators.","Time-correlated single-photon counting measurements reveal the control of QD population dynamics using engineered optical pulses and mechanical motion.","As a first example, we show how this can improve signal-to-background scattering in microwave-to-optical transduction processes.","Specifically, we tailor the scheme so that mechanically assisted photon scattering is enhanced over the direct detuned photon scattering from the optical system.","These differences are greatly amplified by strategic temporal pulse shaping.","Quantum-mechanical calculations show good agreement with our experimental results and provide guidance for adapting these schemes to small phonon occupancies relevant for microwave-to-optical quantum transduction."],"url":"http://arxiv.org/abs/2404.02079v1","category":"quant-ph"}
{"created":"2024-04-02 16:20:02","title":"EGTR: Extracting Graph from Transformer for Scene Graph Generation","abstract":"Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects. By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction. We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at https://github.com/naver-ai/egtr.","sentences":["Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects.","After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied.","However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected.","We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder.","By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head.","Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects.","By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves.","Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction.","We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets.","Our code is publicly available at https://github.com/naver-ai/egtr."],"url":"http://arxiv.org/abs/2404.02072v2","category":"cs.CV"}
{"created":"2024-04-02 16:07:44","title":"On the stability of $\\ddot x(t)+\u03b1(t)\\dot x(t)+\u03b2(t) x(t)=0$","abstract":"Our main goal is to understand the stability of second order linear homogeneous differential equations $\\ddot x(t)+\\alpha(t)\\dot x(t)+\\beta(t)x(t)=0$ for $C^0$-generic values of the variable parameters $\\alpha(t)$ and $\\beta(t)$. For that we embed the problem into the framework of the general theory of continuous-time linear cocycles induced by the random ODE $\\ddot x(t)+\\alpha(\\varphi^t(\\omega))\\dot x(t)+\\beta(\\varphi^t(\\omega))x(t)=0$, where the coefficients $\\alpha$ and $\\beta$ evolve along the $\\varphi^t$-orbit for $\\omega\\in M$, and $\\varphi^t: M\\to M$ is a flow defined on a compact Hausdorff space $M$ preserving a probability measure $\\mu$. Considering $y=\\dot x$, the above random ODE can be rewritten as $\\dot X=A(\\varphi^t (\\omega))X$, with $X=(x,y)^\\top$, having a kinetic linear cocycle as fundamental solution.   We prove that for a $C^0$-generic choice of parameters $\\alpha$ and $\\beta$ and for $\\mu$-almost all $\\omega\\in M$ either the Lyapunov exponents of the linear cocycle are equal ($\\lambda_1(\\omega)=\\lambda_2(\\omega)$), or else the orbit of $\\omega$ displays a dominated splitting. Applying to dissipative systems ($\\alpha<0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)<0$, attesting the stability of the solution of the random ODE above, or else the orbit of $\\omega$ displays a dominated splitting. Applying to frictionless systems ($\\alpha=0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)=0$, attesting the asymptotic neutrality of the solution of the random ODE above, or else the orbit of $\\omega$ displays a hyperbolic splitting attesting the \\emph{uniform} instability of the solution of the ODE above. This last result implies also an analog result for the 1-d continuous aperiodic Schr\\\"odinger equation. Furthermore, all results hold for $L^\\infty$-generic parameters $\\alpha$ and $\\beta$.","sentences":["Our main goal is to understand the stability of second order linear homogeneous differential equations $\\ddot x(t)+\\alpha(t)\\dot x(t)+\\beta(t)x(t)=0$ for $C^0$-generic values of the variable parameters $\\alpha(t)$ and $\\beta(t)$. For that we embed the problem into the framework of the general theory of continuous-time linear cocycles induced by the random ODE $\\ddot x(t)+\\alpha(\\varphi^t(\\omega))\\dot x(t)+\\beta(\\varphi^t(\\omega))x(t)=0$, where the coefficients $\\alpha$ and $\\beta$ evolve along the $\\varphi^t$-orbit for $\\omega\\in M$, and $\\varphi^t: M\\to M$ is a flow defined on a compact Hausdorff space $M$ preserving a probability measure $\\mu$. Considering $y=\\dot x$, the above random ODE can be rewritten as $\\dot X=A(\\varphi^t (\\omega))X$, with $X=(x,y)^\\top$, having a kinetic linear cocycle as fundamental solution.   ","We prove that for a $C^0$-generic choice of parameters $\\alpha$ and $\\beta$ and for $\\mu$-almost all $\\omega\\in M$ either the Lyapunov exponents of the linear cocycle are equal ($\\lambda_1(\\omega)=\\lambda_2(\\omega)$), or else the orbit of $\\omega$ displays a dominated splitting.","Applying to dissipative systems ($\\alpha<0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)<0$, attesting the stability of the solution of the random ODE above, or else the orbit of $\\omega$ displays a dominated splitting.","Applying to frictionless systems ($\\alpha=0$) we obtain a dichotomy: either $\\lambda_1(\\omega)=\\lambda_2(\\omega)=0$, attesting the asymptotic neutrality of the solution of the random ODE above, or else the orbit of $\\omega$ displays a hyperbolic splitting attesting the \\emph{uniform} instability of the solution of the ODE above.","This last result implies also an analog result for the 1-d continuous aperiodic Schr\\\"odinger equation.","Furthermore, all results hold for $L^\\infty$-generic parameters $\\alpha$ and $\\beta$."],"url":"http://arxiv.org/abs/2404.02066v1","category":"math.DS"}
{"created":"2024-04-02 15:58:36","title":"IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT","abstract":"Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training.   Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that \"parameter efficiency represents overall efficiency\". TPME provides more comprehensive insights into practical efficiency comparisons between different methods. Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN. We release our codes and other materials at https://github.com/jjGenAILab/IISAN.","sentences":["Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities.","While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed.","Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   ","IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT.","More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks.","Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT.","This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training.   ","Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that \"parameter efficiency represents overall efficiency\".","TPME provides more comprehensive insights into practical efficiency comparisons between different methods.","Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN.","We release our codes and other materials at https://github.com/jjGenAILab/IISAN."],"url":"http://arxiv.org/abs/2404.02059v1","category":"cs.IR"}
{"created":"2024-04-02 15:41:07","title":"Small-scale magnetohydrodynamic dynamos: from deterministic chaos to turbulence","abstract":"It is shown, using results of numerical simulations, and geophysical and solar observations, that the transition from deterministic chaos to hard turbulence in the magnetic field generated by the small-scale MHD dynamos occurs through a randomization process. This randomization process has been described using the notion of distributed chaos and the main parameter of distributed chaos has been used for quantifying the degree of randomization. The dissipative (Loitsianskii and Birkhoff-Saffman integrals) and ideal (magnetic helicity) magnetohydrodynamic invariants control the randomization process and determine the degree of randomization in different MHD flows, directly or through the Kolmogorov-Iroshnikov phenomenology (the magneto-inertial range of scales as a precursor of hard turbulence). Despite the considerable differences in the scales and physical parameters, the results of numerical simulations are in quantitative agreement with the geophysical and solar observations in the frames of this approach. The Hall magnetohydrodynamic dynamo has been also briefly discussed in this context.","sentences":["It is shown, using results of numerical simulations, and geophysical and solar observations, that the transition from deterministic chaos to hard turbulence in the magnetic field generated by the small-scale MHD dynamos occurs through a randomization process.","This randomization process has been described using the notion of distributed chaos and the main parameter of distributed chaos has been used for quantifying the degree of randomization.","The dissipative (Loitsianskii and Birkhoff-Saffman integrals) and ideal (magnetic helicity) magnetohydrodynamic invariants control the randomization process and determine the degree of randomization in different MHD flows, directly or through the Kolmogorov-Iroshnikov phenomenology (the magneto-inertial range of scales as a precursor of hard turbulence).","Despite the considerable differences in the scales and physical parameters, the results of numerical simulations are in quantitative agreement with the geophysical and solar observations in the frames of this approach.","The Hall magnetohydrodynamic dynamo has been also briefly discussed in this context."],"url":"http://arxiv.org/abs/2404.02049v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 15:34:52","title":"SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation","abstract":"We present a new self-supervised approach, SelfPose3d, for estimating 3d poses of multiple persons from multiple camera views. Unlike current state-of-the-art fully-supervised methods, our approach does not require any 2d or 3d ground-truth poses and uses only the multi-view input images from a calibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d human pose estimator. We propose two self-supervised learning objectives: self-supervised person localization in 3d space and self-supervised 3d pose estimation. We achieve self-supervised 3d person localization by training the model on synthetically generated 3d points, serving as 3d person root positions, and on the projected root-heatmaps in all the views. We then model the 3d poses of all the localized persons with a bottleneck representation, map them onto all views obtaining 2d joints, and render them using 2d Gaussian heatmaps in an end-to-end differentiable manner. Afterwards, we use the corresponding 2d joints and heatmaps from the pseudo 2d poses for learning. To alleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive supervision attention mechanism to guide the self-supervision. Our experiments and analysis on three public benchmark datasets, including Panoptic, Shelf, and Campus, show the effectiveness of our approach, which is comparable to fully-supervised methods. Code is available at \\url{https://github.com/CAMMA-public/SelfPose3D}","sentences":["We present a new self-supervised approach, SelfPose3d, for estimating 3d poses of multiple persons from multiple camera views.","Unlike current state-of-the-art fully-supervised methods, our approach does not require any 2d or 3d ground-truth poses and uses only the multi-view input images from a calibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d human pose estimator.","We propose two self-supervised learning objectives: self-supervised person localization in 3d space and self-supervised 3d pose estimation.","We achieve self-supervised 3d person localization by training the model on synthetically generated 3d points, serving as 3d person root positions, and on the projected root-heatmaps in all the views.","We then model the 3d poses of all the localized persons with a bottleneck representation, map them onto all views obtaining 2d joints, and render them using 2d Gaussian heatmaps in an end-to-end differentiable manner.","Afterwards, we use the corresponding 2d joints and heatmaps from the pseudo 2d poses for learning.","To alleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive supervision attention mechanism to guide the self-supervision.","Our experiments and analysis on three public benchmark datasets, including Panoptic, Shelf, and Campus, show the effectiveness of our approach, which is comparable to fully-supervised methods.","Code is available at \\url{https://github.com/CAMMA-public/SelfPose3D}"],"url":"http://arxiv.org/abs/2404.02041v1","category":"cs.CV"}
{"created":"2024-04-02 15:14:29","title":"Infrared nanosensors of pico- to micro-newton forces","abstract":"Mechanical force is an essential feature for many physical and biological processes.1-12 Remote measurement of mechanical signals with high sensitivity and spatial resolution is needed for diverse applications, including robotics,13 biophysics,14-20 energy storage,21-24 and medicine.25-27 Nanoscale luminescent force sensors excel at measuring piconewton forces,28-32 while larger sensors have proven powerful in probing micronewton forces.33,34 However, large gaps remain in the force magnitudes that can be probed remotely from subsurface or interfacial sites, and no individual, non-invasive sensor is capable of measuring over the large dynamic range needed to understand many systems.35,36 Here, we demonstrate Tm3+-doped avalanching nanoparticle37 force sensors that can be addressed remotely by deeply penetrating near-infrared (NIR) light and can detect piconewton to micronewton forces with a dynamic range spanning more than four orders of magnitude. Using atomic force microscopy coupled with single-nanoparticle optical spectroscopy, we characterize the mechanical sensitivity of the photon avalanching process and reveal its exceptional force responsiveness. By manipulating the Tm3+ concentrations and energy transfer within the nanosensors, we demonstrate different optical force-sensing modalities, including mechanobrightening and mechanochromism. The adaptability of these nanoscale optical force sensors, along with their multiscale sensing capability, enable operation in the dynamic and versatile environments present in real-world, complex structures spanning biological organisms to nanoelectromechanical systems (NEMS).","sentences":["Mechanical force is an essential feature for many physical and biological processes.1-12 Remote measurement of mechanical signals with high sensitivity and spatial resolution is needed for diverse applications, including robotics,13 biophysics,14-20 energy storage,21-24 and","medicine.25-27 Nanoscale luminescent force sensors excel at measuring piconewton forces,28-32 while larger sensors have proven powerful in probing micronewton forces.33,34","However, large gaps remain in the force magnitudes that can be probed remotely from subsurface or interfacial sites, and no individual, non-invasive sensor is capable of measuring over the large dynamic range needed to understand many systems.35,36 Here, we demonstrate Tm3+-doped avalanching nanoparticle37 force sensors that can be addressed remotely by deeply penetrating near-infrared (NIR) light and can detect piconewton to micronewton forces with a dynamic range spanning more than four orders of magnitude.","Using atomic force microscopy coupled with single-nanoparticle optical spectroscopy, we characterize the mechanical sensitivity of the photon avalanching process and reveal its exceptional force responsiveness.","By manipulating the Tm3+ concentrations and energy transfer within the nanosensors, we demonstrate different optical force-sensing modalities, including mechanobrightening and mechanochromism.","The adaptability of these nanoscale optical force sensors, along with their multiscale sensing capability, enable operation in the dynamic and versatile environments present in real-world, complex structures spanning biological organisms to nanoelectromechanical systems (NEMS)."],"url":"http://arxiv.org/abs/2404.02026v1","category":"physics.optics"}
{"created":"2024-04-02 15:12:35","title":"On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty","abstract":"Continuous-time adaptive controllers for systems with a matched uncertainty often comprise an online parameter estimator and a corresponding parameterized controller to cancel the uncertainty. However, such methods are often unimplementable, as they depend on an unobserved estimation error. We consider the equivalent discrete-time setting with a causal information structure. We propose a novel, online proximal point method-based adaptive controller, that under a weak persistence of excitation (PE) condition is asymptotically stable and achieves finite regret, scaling only with the time required to fulfill the PE condition. We show the same also for the widely-used recursive least squares with exponential forgetting controller under a stronger PE condition.","sentences":["Continuous-time adaptive controllers for systems with a matched uncertainty often comprise an online parameter estimator and a corresponding parameterized controller to cancel the uncertainty.","However, such methods are often unimplementable, as they depend on an unobserved estimation error.","We consider the equivalent discrete-time setting with a causal information structure.","We propose a novel, online proximal point method-based adaptive controller, that under a weak persistence of excitation (PE) condition is asymptotically stable and achieves finite regret, scaling only with the time required to fulfill the PE condition.","We show the same also for the widely-used recursive least squares with exponential forgetting controller under a stronger PE condition."],"url":"http://arxiv.org/abs/2404.02023v1","category":"eess.SY"}
{"created":"2024-04-02 15:01:14","title":"Brownian Particles and Matter Waves","abstract":"In view of the remarkable progress in micro-rheology to monitor the random motion of Brownian particles with size as small as few nanometers, in association that de Broglie matter waves have been experimentally observed for large molecules of comparable nanometer size; we examine whether Brownian particles can manifest a particle-wave duality without employing a priori arguments from quantum decoherence. First, we examine the case where Brownian particles are immersed in a memoryless viscous fluid with a time-independent diffusion coefficient; and the requirement for the Brownian particles to manifest a particle-wave duality leads to the untenable result that the diffusion coefficient has to be proportional to the inverse time; therefore, diverging at early times. This finding agrees with past conclusions--that quantum mechanics is not equivalent to a Markovian diffusion process. Next, we examine the case where the Brownian particle is trapped in a harmonic potential well with and without dissipation. Both solutions of the Fokker-Plank equation for the case with dissipation, and of the Schrodinger equation for the case without dissipation lead to the same physically acceptable result-that for the Brownian particle to manifest a particle-wave duality, its mean kinetic energy needs to be half the ground-state energy of the quantum harmonic oscillator. Our one-dimensional calculations show that for this to happen, the trapping needs to be very strong so that a Brownian nanoparticle needs to be embedded in an extremely stiff solid.","sentences":["In view of the remarkable progress in micro-rheology to monitor the random motion of Brownian particles with size as small as few nanometers, in association that de Broglie matter waves have been experimentally observed for large molecules of comparable nanometer size; we examine whether Brownian particles can manifest a particle-wave duality without employing a priori arguments from quantum decoherence.","First, we examine the case where Brownian particles are immersed in a memoryless viscous fluid with a time-independent diffusion coefficient; and the requirement for the Brownian particles to manifest a particle-wave duality leads to the untenable result that the diffusion coefficient has to be proportional to the inverse time; therefore, diverging at early times.","This finding agrees with past conclusions--that quantum mechanics is not equivalent to a Markovian diffusion process.","Next, we examine the case where the Brownian particle is trapped in a harmonic potential well with and without dissipation.","Both solutions of the Fokker-Plank equation for the case with dissipation, and of the Schrodinger equation for the case without dissipation lead to the same physically acceptable result-that for the Brownian particle to manifest a particle-wave duality, its mean kinetic energy needs to be half the ground-state energy of the quantum harmonic oscillator.","Our one-dimensional calculations show that for this to happen, the trapping needs to be very strong so that a Brownian nanoparticle needs to be embedded in an extremely stiff solid."],"url":"http://arxiv.org/abs/2404.02016v1","category":"quant-ph"}
{"created":"2024-04-02 14:56:43","title":"MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving","abstract":"Large language models (LLMs) have demonstrated remarkable performance, and organizations are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally formulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal colocations and maximize utilization. MuxServe designs a unified resource manager to enable flexible and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99\\%$ SLO attainment.","sentences":["Large language models (LLMs) have demonstrated remarkable performance, and organizations are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search.","However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs.","In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving.","The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources.","MuxServe formally formulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal colocations and maximize utilization.","MuxServe designs a unified resource manager to enable flexible and efficient multiplexing.","Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99\\%$ SLO attainment."],"url":"http://arxiv.org/abs/2404.02015v1","category":"cs.DC"}
{"created":"2024-04-02 14:40:04","title":"DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning","abstract":"Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.","sentences":["Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction.","For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history.","Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective.","Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision.","To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning.","This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making.","Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations.","We also reconstruct a dual-level instruction for adaptation to the dual-level alignment.","As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities.","Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN."],"url":"http://arxiv.org/abs/2404.01994v1","category":"cs.CV"}
{"created":"2024-04-02 14:26:18","title":"Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection","abstract":"Unsupervised Domain Adaptation (UDA) has shown significant advancements in object detection under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles. To address this problem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant domain shift across contexts. Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target domain. Comprehensive experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K, SHIFT, and ACDC datasets, respectively. Code is available at https://github.com/jichengyuan/Cooperitive_Students.","sentences":["Unsupervised Domain Adaptation (UDA) has shown significant advancements in object detection under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles.","To address this problem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant domain shift across contexts.","Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target domain.","Comprehensive experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K, SHIFT, and ACDC datasets, respectively.","Code is available at https://github.com/jichengyuan/Cooperitive_Students."],"url":"http://arxiv.org/abs/2404.01988v2","category":"cs.CV"}
{"created":"2024-04-02 14:17:32","title":"Least Squares Inference for Data with Network Dependency","abstract":"We address the inference problem concerning regression coefficients in a classical linear regression model using least squares estimates. The analysis is conducted under circumstances where network dependency exists across units in the sample. Neglecting the dependency among observations may lead to biased estimation of the asymptotic variance and often inflates the Type I error in coefficient inference. In this paper, we first establish a central limit theorem for the ordinary least squares estimate, with a verifiable dependence condition alongside corresponding neighborhood growth conditions. Subsequently, we propose a consistent estimator for the asymptotic variance of the estimated coefficients, which employs a data-driven method to balance the bias-variance trade-off. We find that the optimal tuning depends on the linear hypothesis under consideration and must be chosen adaptively. The presented theory and methods are illustrated and supported by numerical experiments and a data example.","sentences":["We address the inference problem concerning regression coefficients in a classical linear regression model using least squares estimates.","The analysis is conducted under circumstances where network dependency exists across units in the sample.","Neglecting the dependency among observations may lead to biased estimation of the asymptotic variance and often inflates the Type I error in coefficient inference.","In this paper, we first establish a central limit theorem for the ordinary least squares estimate, with a verifiable dependence condition alongside corresponding neighborhood growth conditions.","Subsequently, we propose a consistent estimator for the asymptotic variance of the estimated coefficients, which employs a data-driven method to balance the bias-variance trade-off.","We find that the optimal tuning depends on the linear hypothesis under consideration and must be chosen adaptively.","The presented theory and methods are illustrated and supported by numerical experiments and a data example."],"url":"http://arxiv.org/abs/2404.01977v1","category":"stat.ME"}
{"created":"2024-04-02 13:54:22","title":"Bi-LORA: A Vision-Language Approach for Synthetic Image Detection","abstract":"Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.","sentences":["Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images.","While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts.","This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs).","We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images.","The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2).","Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs.","The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models.","The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT."],"url":"http://arxiv.org/abs/2404.01959v1","category":"cs.CV"}
{"created":"2024-04-02 13:54:05","title":"MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels","abstract":"Human activity recognition (HAR) will be an essential function of various emerging applications. However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements. In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase. From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage. With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality. Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples. Extensive experiments on eight public multimodal datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal data.","sentences":["Human activity recognition (HAR) will be an essential function of various emerging applications.","However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements.","In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase.","From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage.","With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality.","Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples.","Extensive experiments on eight public multimodal datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal data."],"url":"http://arxiv.org/abs/2404.01958v1","category":"cs.LG"}
{"created":"2024-04-02 13:41:22","title":"Event-assisted Low-Light Video Object Segmentation","abstract":"In the realm of video object segmentation (VOS), the challenge of operating under low-light conditions persists, resulting in notably degraded image quality and compromised accuracy when comparing query and memory frames for similarity computation. Event cameras, characterized by their high dynamic range and ability to capture motion information of objects, offer promise in enhancing object visibility and aiding VOS methods under such low-light conditions. This paper introduces a pioneering framework tailored for low-light VOS, leveraging event camera data to elevate segmentation accuracy. Our approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion (ACMF) module, aimed at extracting pertinent features while fusing image and event modalities to mitigate noise interference, and the Event-Guided Memory Matching (EGMM) module, designed to rectify the issue of inaccurate matching prevalent in low-light settings. Additionally, we present the creation of a synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset, encompassing frames and events. Experimental evaluations corroborate the efficacy of our method across both datasets, affirming its effectiveness in low-light scenarios.","sentences":["In the realm of video object segmentation (VOS), the challenge of operating under low-light conditions persists, resulting in notably degraded image quality and compromised accuracy when comparing query and memory frames for similarity computation.","Event cameras, characterized by their high dynamic range and ability to capture motion information of objects, offer promise in enhancing object visibility and aiding VOS methods under such low-light conditions.","This paper introduces a pioneering framework tailored for low-light VOS, leveraging event camera data to elevate segmentation accuracy.","Our approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion (ACMF) module, aimed at extracting pertinent features while fusing image and event modalities to mitigate noise interference, and the Event-Guided Memory Matching (EGMM) module, designed to rectify the issue of inaccurate matching prevalent in low-light settings.","Additionally, we present the creation of a synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset, encompassing frames and events.","Experimental evaluations corroborate the efficacy of our method across both datasets, affirming its effectiveness in low-light scenarios."],"url":"http://arxiv.org/abs/2404.01945v1","category":"cs.CV"}
{"created":"2024-04-02 13:27:28","title":"PREGO: online mistake detection in PRocedural EGOcentric videos","abstract":"Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen. This capability has a wide range of applications across various fields, such as manufacturing and healthcare. The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures. However, no technique can currently detect open-set procedural mistakes online. We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos. PREGO is based on an online action recognition component to model the current action, and a symbolic reasoning module to predict the next actions. Mistake detection is performed by comparing the recognized current action with the expected future one. We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online benchmarking of procedural mistake detection to establish suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets, respectively.","sentences":["Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen.","This capability has a wide range of applications across various fields, such as manufacturing and healthcare.","The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures.","However, no technique can currently detect open-set procedural mistakes online.","We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos.","PREGO is based on an online action recognition component to model the current action, and a symbolic reasoning module to predict the next actions.","Mistake detection is performed by comparing the recognized current action with the expected future one.","We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online benchmarking of procedural mistake detection to establish suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets, respectively."],"url":"http://arxiv.org/abs/2404.01933v1","category":"cs.CV"}
{"created":"2024-04-02 13:23:54","title":"Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies","abstract":"We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains. We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage. We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them. Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee. In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior. Moreover, we discover a new parameter of adaptive selection policies, which we term the \"maximal gain ratio\". We show that this parameter is strictly less restrictive than the greedy approximation parameter that has been used in previous approximation guarantees, and show that it can be used to provide stronger approximation guarantees than previous results. In particular, we show that the maximal gain ratio is never larger than the greedy approximation factor of a policy, and that it can be considerably smaller. This provides a new insight into the properties that make a policy useful for adaptive combinatorial maximization.","sentences":["We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains.","We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage.","We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them.","Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee.","In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior.","Moreover, we discover a new parameter of adaptive selection policies, which we term the \"maximal gain ratio\".","We show that this parameter is strictly less restrictive than the greedy approximation parameter that has been used in previous approximation guarantees, and show that it can be used to provide stronger approximation guarantees than previous results.","In particular, we show that the maximal gain ratio is never larger than the greedy approximation factor of a policy, and that it can be considerably smaller.","This provides a new insight into the properties that make a policy useful for adaptive combinatorial maximization."],"url":"http://arxiv.org/abs/2404.01930v1","category":"cs.LG"}
{"created":"2024-04-02 13:14:32","title":"Hyperviscosity stabilisation of the RBF-FD solution to natural convection","abstract":"The numerical stability of fluid flow is an important topic in computational fluid dynamics as fluid flow simulations usually become numerically unstable in the turbulent regime. Many mesh-based methods have already established numerical dissipation procedures that dampen the effects of the unstable advection term. When it comes to meshless methods, the prominent stabilisation scheme is hyperviscosity. It introduces numerical dissipation in the form of a higher-order Laplacian operator. Many papers have already discussed the general effects of hyperviscosity and its parameters. However, hyperviscosity in flow problems has not yet been analyzed in depth. In this paper, we discuss the effects of hyperviscosity on natural convection flow problems as we approach the turbulent regime.","sentences":["The numerical stability of fluid flow is an important topic in computational fluid dynamics as fluid flow simulations usually become numerically unstable in the turbulent regime.","Many mesh-based methods have already established numerical dissipation procedures that dampen the effects of the unstable advection term.","When it comes to meshless methods, the prominent stabilisation scheme is hyperviscosity.","It introduces numerical dissipation in the form of a higher-order Laplacian operator.","Many papers have already discussed the general effects of hyperviscosity and its parameters.","However, hyperviscosity in flow problems has not yet been analyzed in depth.","In this paper, we discuss the effects of hyperviscosity on natural convection flow problems as we approach the turbulent regime."],"url":"http://arxiv.org/abs/2404.01919v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 12:49:03","title":"Nonlinear stability for active suspensions","abstract":"This paper is devoted to the nonlinear analysis of a kinetic model introduced by Saintillan and Shelley to describe suspensions of active rodlike particles in viscous flows. We investigate the stability of the constant state $\\Psi(t,x,p) = \\frac{1}{4\\pi} $ corresponding to a distribution of particles that is homogeneous in space (variable $x \\in \\mathbb{T}^3$) and uniform in orientation (variable $p \\in \\mathbb{S}^2$). We prove its nonlinear stability under the optimal condition of linearized spectral stability, without any addition of spatial diffusion. The mathematical novelty and difficulty compared to previous linear studies comes from the presence of a quasilinear term in $x$ due to nonlinear convection. A key feature of our work, which we hope to be of independent interest, is an analysis of enhanced dissipation and mixing properties of the advection diffusion operator $$\\partial_t + (p + u(t,x)) \\cdot \\nabla_x - \\nu \\Delta_p $$ on $\\mathbb{T}^3 \\times \\mathbb{S}^2$ for a given appropriately small vector field $u$.","sentences":["This paper is devoted to the nonlinear analysis of a kinetic model introduced by Saintillan and Shelley to describe suspensions of active rodlike particles in viscous flows.","We investigate the stability of the constant state $\\Psi(t,x,p) = \\frac{1}{4\\pi} $ corresponding to a distribution of particles that is homogeneous in space (variable $x \\in \\mathbb{T}^3$) and uniform in orientation (variable $p \\in \\mathbb{S}^2$).","We prove its nonlinear stability under the optimal condition of linearized spectral stability, without any addition of spatial diffusion.","The mathematical novelty and difficulty compared to previous linear studies comes from the presence of a quasilinear term in $x$ due to nonlinear convection.","A key feature of our work, which we hope to be of independent interest, is an analysis of enhanced dissipation and mixing properties of the advection diffusion operator $$\\partial_t + (p + u(t,x))","\\cdot \\nabla_x - \\nu \\Delta_p $$ on $\\mathbb{T}^3 \\times \\mathbb{S}^2$ for a given appropriately small vector field $u$."],"url":"http://arxiv.org/abs/2404.01906v1","category":"math.AP"}
{"created":"2024-04-02 12:15:25","title":"Scene Adaptive Sparse Transformer for Event-based Object Detection","abstract":"While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse Transformers. However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency. Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate. To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST). SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1). Code: https://github.com/Peterande/SAST","sentences":["While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras.","Image-based works attempt to reduce these costs by introducing sparse Transformers.","However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency.","Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate.","To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST).","SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead.","Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost.","The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1).","Code: https://github.com/Peterande/SAST"],"url":"http://arxiv.org/abs/2404.01882v1","category":"cs.CV"}
{"created":"2024-04-02 11:55:50","title":"Fast and Adaptive Questionnaires for Voting Advice Applications","abstract":"The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions. We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy. Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version.","sentences":["The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires.","To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire.","However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%.","To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters.","Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations.","Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions.","We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy.","Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version."],"url":"http://arxiv.org/abs/2404.01872v1","category":"cs.LG"}
{"created":"2024-04-02 11:41:22","title":"Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems","abstract":"Generating simulated training data needed for constructing sufficiently accurate surrogate models to be used for efficient optimization or parameter identification can incur a huge computational effort in the offline phase. We consider a fully adaptive greedy approach to the computational design of experiments problem using gradient-enhanced Gaussian process regression as surrogates. Designs are incrementally defined by solving an optimization problem for accuracy given a certain computational budget. We address not only the choice of evaluation points but also of required simulation accuracy, both of values and gradients of the forward model. Numerical results show a significant reduction of the computational effort compared to just position-adaptive and static designs as well as a clear benefit of including gradient information into the surrogate training.","sentences":["Generating simulated training data needed for constructing sufficiently accurate surrogate models to be used for efficient optimization or parameter identification can incur a huge computational effort in the offline phase.","We consider a fully adaptive greedy approach to the computational design of experiments problem using gradient-enhanced Gaussian process regression as surrogates.","Designs are incrementally defined by solving an optimization problem for accuracy given a certain computational budget.","We address not only the choice of evaluation points but also of required simulation accuracy, both of values and gradients of the forward model.","Numerical results show a significant reduction of the computational effort compared to just position-adaptive and static designs as well as a clear benefit of including gradient information into the surrogate training."],"url":"http://arxiv.org/abs/2404.01864v1","category":"math.NA"}
{"created":"2024-04-02 11:03:13","title":"Semi-Supervised Domain Adaptation for Wildfire Detection","abstract":"Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change. In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised Domain Adaptation for object detection, accompanied by a corresponding dataset designed for use by both academics and industries. Our dataset encompasses 30 times more diverse labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection. Inspired by CoordConv, we propose a robust baseline, Location-Aware Object Detection for Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires. With only using 1% target domain labeled data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset. Our dataset is available at https://github.com/BloomBerry/LADA.","sentences":["Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change.","In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised Domain Adaptation for object detection, accompanied by a corresponding dataset designed for use by both academics and industries.","Our dataset encompasses 30 times more diverse labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection.","Inspired by CoordConv, we propose a robust baseline, Location-Aware Object Detection for Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires.","With only using 1% target domain labeled data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset.","Our dataset is available at https://github.com/BloomBerry/LADA."],"url":"http://arxiv.org/abs/2404.01842v1","category":"cs.CV"}
{"created":"2024-04-02 10:34:46","title":"Mathematical modeling and numerical multigoal-oriented a posteriori error control and adaptivity for a stationary, nonlinear, coupled flow temperature model with temperature dependent density","abstract":"In this work, we develop adaptive schemes using goal-oriented error control for a highly nonlinear flow temperature model with temperature dependent density. The dual-weighted residual method for computing error indicators to steer mesh refinement and solver control is employed. The error indicators are used to employ adaptive algorithms, which are substantiated with several numerical tests. Therein, error reductions and effectivity indices are consulted to establish the robustness and efficiency of our framework.","sentences":["In this work, we develop adaptive schemes using goal-oriented error control for a highly nonlinear flow temperature model with temperature dependent density.","The dual-weighted residual method for computing error indicators to steer mesh refinement and solver control is employed.","The error indicators are used to employ adaptive algorithms, which are substantiated with several numerical tests.","Therein, error reductions and effectivity indices are consulted to establish the robustness and efficiency of our framework."],"url":"http://arxiv.org/abs/2404.01823v1","category":"math.NA"}
{"created":"2024-04-02 10:03:23","title":"EventSleep: Sleep Activity Recognition with Event Cameras","abstract":"Event cameras are a promising technology for activity recognition in dark environments due to their unique properties. However, real event camera datasets under low-lighting conditions are still scarce, which also limits the number of approaches to solve these kind of problems, hindering the potential of this technology in many applications. We present EventSleep, a new dataset and methodology to address this gap and study the suitability of event cameras for a very relevant medical application: sleep monitoring for sleep disorders analysis. The dataset contains synchronized event and infrared recordings emulating common movements that happen during the sleep, resulting in a new challenging and unique dataset for activity recognition in dark environments. Our novel pipeline is able to achieve high accuracy under these challenging conditions and incorporates a Bayesian approach (Laplace ensembles) to increase the robustness in the predictions, which is fundamental for medical applications. Our work is the first application of Bayesian neural networks for event cameras, the first use of Laplace ensembles in a realistic problem, and also demonstrates for the first time the potential of event cameras in a new application domain: to enhance current sleep evaluation procedures. Our activity recognition results highlight the potential of event cameras under dark conditions, and its capacity and robustness for sleep activity recognition, and open problems as the adaptation of event data pre-processing techniques to dark environments.","sentences":["Event cameras are a promising technology for activity recognition in dark environments due to their unique properties.","However, real event camera datasets under low-lighting conditions are still scarce, which also limits the number of approaches to solve these kind of problems, hindering the potential of this technology in many applications.","We present EventSleep, a new dataset and methodology to address this gap and study the suitability of event cameras for a very relevant medical application: sleep monitoring for sleep disorders analysis.","The dataset contains synchronized event and infrared recordings emulating common movements that happen during the sleep, resulting in a new challenging and unique dataset for activity recognition in dark environments.","Our novel pipeline is able to achieve high accuracy under these challenging conditions and incorporates a Bayesian approach (Laplace ensembles) to increase the robustness in the predictions, which is fundamental for medical applications.","Our work is the first application of Bayesian neural networks for event cameras, the first use of Laplace ensembles in a realistic problem, and also demonstrates for the first time the potential of event cameras in a new application domain: to enhance current sleep evaluation procedures.","Our activity recognition results highlight the potential of event cameras under dark conditions, and its capacity and robustness for sleep activity recognition, and open problems as the adaptation of event data pre-processing techniques to dark environments."],"url":"http://arxiv.org/abs/2404.01801v1","category":"cs.CV"}
{"created":"2024-04-02 09:56:04","title":"Long Time Propagation of Chaos in Total Variation Distance for Mean Field Interacting Particle System","abstract":"In this paper, a general result on the long time quantitative propagation of chaos in total variation distance for mean field interacting particle system driven by general L\\'{e}vy noise is derived, where the non-interacting drift is assumed to be dissipative in long distance and the initial distribution of interacting particle system converges to that of the limit equation in $L^1$-Wasserstein distance. Moreover, by using the method of coupling, the results are applied to mean field interacting particle system driven by Brownian motion and $\\alpha(\\alpha>1)$-stable noise respectively.","sentences":["In this paper, a general result on the long time quantitative propagation of chaos in total variation distance for mean field interacting particle system driven by general L\\'{e}vy noise is derived, where the non-interacting drift is assumed to be dissipative in long distance and the initial distribution of interacting particle system converges to that of the limit equation in $L^1$-Wasserstein distance.","Moreover, by using the method of coupling, the results are applied to mean field interacting particle system driven by Brownian motion and $\\alpha(\\alpha>1)$-stable noise respectively."],"url":"http://arxiv.org/abs/2404.01795v1","category":"math.PR"}
{"created":"2024-04-02 09:53:20","title":"Super-Resolution Analysis for Landfill Waste Classification","abstract":"Illegal landfills are a critical issue due to their environmental, economic, and public health impacts. This study leverages aerial imagery for environmental crime monitoring. While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images. Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains. Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills. We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, necessitating careful threshold fine-tuning.","sentences":["Illegal landfills are a critical issue due to their environmental, economic, and public health impacts.","This study leverages aerial imagery for environmental crime monitoring.","While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images.","Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains.","Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills.","We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, necessitating careful threshold fine-tuning."],"url":"http://arxiv.org/abs/2404.01790v1","category":"cs.CV"}
{"created":"2024-04-02 09:31:14","title":"Class-Incremental Few-Shot Event Detection","abstract":"Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD.","sentences":["Event detection is one of the fundamental tasks in information extraction and knowledge graph.","However, a realistic event detection system often needs to deal with new event classes constantly.","These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances.","Therefore, this paper proposes a new task, called class-incremental few-shot event detection.","Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting.","To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD.","Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation.","On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism.","Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD."],"url":"http://arxiv.org/abs/2404.01767v1","category":"cs.CL"}
{"created":"2024-04-02 09:01:21","title":"Atom-Level Optical Chemical Structure Recognition with Limited Supervision","abstract":"Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development. Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images. To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision. Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds. Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision. Through rigorous and extensive benchmarking, we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction.","sentences":["Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development.","Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images.","To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision.","Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds.","Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision.","Through rigorous and extensive benchmarking, we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction."],"url":"http://arxiv.org/abs/2404.01743v1","category":"cs.CV"}
{"created":"2024-04-02 08:55:56","title":"A Posteriori Single- and Multi-Goal Error Control and Adaptivity for Partial Differential Equations","abstract":"This work reviews goal-oriented a posteriori error control, adaptivity and solver control for finite element approximations to boundary and initial-boundary value problems for stationary and non-stationary partial differential equations, respectively. In particular, coupled field problems with different physics may require simultaneously the accurate evaluation of several quantities of interest, which is achieved with multi-goal oriented error control. Sensitivity measures are obtained by solving an adjoint problem. Error localization is achieved with the help of a partition-of-unity. We also review and extend theoretical results for efficiency and reliability by employing a saturation assumption. The resulting adaptive algorithms allow to balance discretization and non-linear iteration errors, and are demonstrated for four applications: Poisson's problem, non-linear elliptic boundary value problems, stationary incompressible Navier-Stokes equations, and regularized parabolic $p$-Laplace initial-boundary value problems. Therein, different finite element discretizations in two different software libraries are utilized, which are partially accompanied with open-source implementations on GitHub.","sentences":["This work reviews goal-oriented a posteriori error control, adaptivity and solver control for finite element approximations to boundary and initial-boundary value problems for stationary and non-stationary partial differential equations, respectively.","In particular, coupled field problems with different physics may require simultaneously the accurate evaluation of several quantities of interest, which is achieved with multi-goal oriented error control.","Sensitivity measures are obtained by solving an adjoint problem.","Error localization is achieved with the help of a partition-of-unity.","We also review and extend theoretical results for efficiency and reliability by employing a saturation assumption.","The resulting adaptive algorithms allow to balance discretization and non-linear iteration errors, and are demonstrated for four applications: Poisson's problem, non-linear elliptic boundary value problems, stationary incompressible Navier-Stokes equations, and regularized parabolic $p$-Laplace initial-boundary value problems.","Therein, different finite element discretizations in two different software libraries are utilized, which are partially accompanied with open-source implementations on GitHub."],"url":"http://arxiv.org/abs/2404.01738v1","category":"math.NA"}
{"created":"2024-04-02 08:53:00","title":"Nonparametric efficient causal estimation of the intervention-specific expected number of recurrent events with continuous-time targeted maximum likelihood and highly adaptive lasso estimation","abstract":"Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account. In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework. In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process. We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter. We illustrate the methods in a simulation study.","sentences":["Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account.","In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework.","In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process.","We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter.","We illustrate the methods in a simulation study."],"url":"http://arxiv.org/abs/2404.01736v1","category":"stat.ME"}
{"created":"2024-04-02 08:33:21","title":"Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge","abstract":"We focus on the generalization ability of the 6-DoF grasp detection method in this paper. While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures. To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences. More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping. For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios. Extensive experiments conducted on the GraspNet-1billion benchmark demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method.","sentences":["We focus on the generalization ability of the 6-DoF grasp detection method in this paper.","While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures.","To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences.","More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping.","For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios.","Extensive experiments conducted on the GraspNet-1billion benchmark demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method."],"url":"http://arxiv.org/abs/2404.01727v1","category":"cs.RO"}
{"created":"2024-04-02 08:21:16","title":"Disentangled Pre-training for Human-Object Interaction Detection","abstract":"Detecting human-object interaction (HOI) has long been limited by the amount of supervised data available. Recent approaches address this issue by pre-training according to pseudo-labels, which align object regions with HOI triplets parsed from image captions. However, pseudo-labeling is tricky and noisy, making HOI pre-training a complex process. Therefore, we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem. First, DP-HOI utilizes object detection and action recognition datasets to pre-train the detection and interaction decoder layers, respectively. Then, we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task. This facilitates efficient knowledge transfer. Specifically, the detection decoder identifies reliable human instances in each action recognition dataset image, generates one corresponding query, and feeds it into the interaction decoder for verb classification. Next, we combine the human instance verb predictions in the same image and impose image-level supervision. The DP-HOI structure can be easily adapted to the HOI detection task, enabling effective model parameter initialization. Therefore, it significantly enhances the performance of existing HOI detection models on a broad range of rare categories. The code and pre-trained weight are available at https://github.com/xingaoli/DP-HOI.","sentences":["Detecting human-object interaction (HOI) has long been limited by the amount of supervised data available.","Recent approaches address this issue by pre-training according to pseudo-labels, which align object regions with HOI triplets parsed from image captions.","However, pseudo-labeling is tricky and noisy, making HOI pre-training a complex process.","Therefore, we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem.","First, DP-HOI utilizes object detection and action recognition datasets to pre-train the detection and interaction decoder layers, respectively.","Then, we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task.","This facilitates efficient knowledge transfer.","Specifically, the detection decoder identifies reliable human instances in each action recognition dataset image, generates one corresponding query, and feeds it into the interaction decoder for verb classification.","Next, we combine the human instance verb predictions in the same image and impose image-level supervision.","The DP-HOI structure can be easily adapted to the HOI detection task, enabling effective model parameter initialization.","Therefore, it significantly enhances the performance of existing HOI detection models on a broad range of rare categories.","The code and pre-trained weight are available at https://github.com/xingaoli/DP-HOI."],"url":"http://arxiv.org/abs/2404.01725v1","category":"cs.CV"}
{"created":"2024-04-02 08:07:38","title":"AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation","abstract":"Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs. However, their practical applicability is often hampered by poor efficiency, stemming from the requirement of thousands or hundreds of sampling steps. Inspired by the efficient text-to-image approach adversarial diffusion distillation (ADD), we design AddSR to address this issue by incorporating the ideas of both distillation and ControlNet. Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost. Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for distillation. Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD. Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR).","sentences":["Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs.","However, their practical applicability is often hampered by poor efficiency, stemming from the requirement of thousands or hundreds of sampling steps.","Inspired by the efficient text-to-image approach adversarial diffusion distillation (ADD), we design AddSR to address this issue by incorporating the ideas of both distillation and ControlNet.","Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost.","Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for distillation.","Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD.","Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR)."],"url":"http://arxiv.org/abs/2404.01717v2","category":"cs.CV"}
{"created":"2024-04-02 07:57:17","title":"Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning","abstract":"Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.","sentences":["Training deep neural networks is a challenging task.","In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning.","Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like.","Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased.","Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset."],"url":"http://arxiv.org/abs/2404.01714v1","category":"cs.LG"}
{"created":"2024-04-02 07:49:08","title":"Upsample Guidance: Scale Up Diffusion Models without Training","abstract":"Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models. We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment.","sentences":["Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio.","However, they encounter difficulties in directly generating high-resolution samples.","Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages.","These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work.","In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process.","Remarkably, this technique does not necessitate any additional training or relying on external models.","We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models.","We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment."],"url":"http://arxiv.org/abs/2404.01709v1","category":"cs.CV"}
{"created":"2024-04-02 07:10:16","title":"Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot","abstract":"As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds.","sentences":["As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot.","This includes combining data from several modalities together with the context of the situation and background knowledge.","Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data.","In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup).","We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations.","Then we implement and evaluate the model on the real setup.","In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification.","For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds."],"url":"http://arxiv.org/abs/2404.01702v1","category":"cs.HC"}
{"created":"2024-04-02 06:39:06","title":"Emergent Simplicities in the Living Histories of Individual Cells","abstract":"Organisms maintain the status quo, holding key physiological variables constant to within an acceptable tolerance, and yet adapt with precision and plasticity to dynamic changes in externalities. What organizational principles ensure such exquisite yet robust control of systems-level \"state variables\" in complex systems with an extraordinary number of moving parts and fluctuating variables? Here we focus on these issues in the specific context of intra- and intergenerational life histories of individual bacterial cells, whose biographies are precisely charted via high-precision dynamic experiments using the SChemostat technology. We highlight intra- and intergenerational scaling laws and other \"emergent simplicities\" revealed by these high-precision data. In turn, these facilitate a principled route to dimensional reduction of the problem, and serve as essential building blocks for phenomenological and mechanistic theory. Parameter-free data-theory matches for multiple organisms validate theory frameworks, and explicate the systems physics of stochastic homeostasis and adaptation.","sentences":["Organisms maintain the status quo, holding key physiological variables constant to within an acceptable tolerance, and yet adapt with precision and plasticity to dynamic changes in externalities.","What organizational principles ensure such exquisite yet robust control of systems-level \"state variables\" in complex systems with an extraordinary number of moving parts and fluctuating variables?","Here we focus on these issues in the specific context of intra- and intergenerational life histories of individual bacterial cells, whose biographies are precisely charted via high-precision dynamic experiments using the SChemostat technology.","We highlight intra- and intergenerational scaling laws and other \"emergent simplicities\" revealed by these high-precision data.","In turn, these facilitate a principled route to dimensional reduction of the problem, and serve as essential building blocks for phenomenological and mechanistic theory.","Parameter-free data-theory matches for multiple organisms validate theory frameworks, and explicate the systems physics of stochastic homeostasis and adaptation."],"url":"http://arxiv.org/abs/2404.01682v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-02 06:24:21","title":"A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification","abstract":"Hyperspectral image (HSI) classification techniques have been intensively studied and a variety of models have been developed. However, these HSI classification models are confined to pocket models and unrealistic ways of datasets partitioning. The former limits the generalization performance of the model and the latter is partitioned leads to inflated model evaluation metrics, which results in plummeting model performance in the real world. Therefore, we propose a universal knowledge embedded contrastive learning framework (KnowCL) for supervised, unsupervised, and semisupervised HSI classification, which largely closes the gap of HSI classification models between pocket models and standard vision backbones. We present a new HSI processing pipeline in conjunction with a range of data transformation and augmentation techniques that provide diverse data representations and realistic data partitioning. The proposed framework based on this pipeline is compatible with all kinds of backbones and can fully exploit labeled and unlabeled samples with expected training time. Furthermore, we design a new loss function, which can adaptively fuse the supervised loss and unsupervised loss, enhancing the learning performance. This proposed new classification paradigm shows great potentials in exploring for HSI classification technology. The code can be accessed at https://github.com/quanweiliu/KnowCL.","sentences":["Hyperspectral image (HSI) classification techniques have been intensively studied and a variety of models have been developed.","However, these HSI classification models are confined to pocket models and unrealistic ways of datasets partitioning.","The former limits the generalization performance of the model and the latter is partitioned leads to inflated model evaluation metrics, which results in plummeting model performance in the real world.","Therefore, we propose a universal knowledge embedded contrastive learning framework (KnowCL) for supervised, unsupervised, and semisupervised HSI classification, which largely closes the gap of HSI classification models between pocket models and standard vision backbones.","We present a new HSI processing pipeline in conjunction with a range of data transformation and augmentation techniques that provide diverse data representations and realistic data partitioning.","The proposed framework based on this pipeline is compatible with all kinds of backbones and can fully exploit labeled and unlabeled samples with expected training time.","Furthermore, we design a new loss function, which can adaptively fuse the supervised loss and unsupervised loss, enhancing the learning performance.","This proposed new classification paradigm shows great potentials in exploring for HSI classification technology.","The code can be accessed at https://github.com/quanweiliu/KnowCL."],"url":"http://arxiv.org/abs/2404.01673v1","category":"cs.CV"}
{"created":"2024-04-02 06:09:42","title":"Nonreciprocal interactions in crowd dynamics: investigating the impact of moving threats on pedestrian speed preferences","abstract":"Nonreciprocal interaction crowd systems, such as human-human, human-vehicle, and human-robot systems, often have serious impacts on pedestrian safety and social order. A more comprehensive understanding of these systems is needed to optimize system stability and efficiency. Despite the importance of these interactions, empirical research in this area remains limited. Thus, in our study we explore this underresearched area, focusing on scenarios where nonreciprocity plays a critical role, such as mass stabbings, which pose a substantial risk to public safety. We conducted the first experiments on this system and analysed high-accuracy data obtained from these experiments. The extent of the direct threat zone is determined by the speed of the moving threat and the radius of danger occurrence. We further categorize potential threats into direct, adjacent, and rear-view zones, quantifying the level of threat for pedestrians. Our study revealed that a pedestrian's desired velocity correlated positively with potential threat intensity, increasing until near the direct threat zone. An emerging steady state is observed when escape routes are blocked by moving threats. This deviation affects the density-velocity relationship, making it distinct from the general relationship. This deviation signifies unique pedestrian behaviour in the presence of moving threats. Additionally, the rate of change in the angle for pedestrian motion in various desired directions is synchronized. This indicates the emergence of collective intelligence in nonreciprocal interaction crowd systems. As a result, our study may constitute a pioneering step towards understanding nonreciprocal interactions in crowd systems through laboratory experiments. These findings may enhance pedestrian safety and inform not only government crowd management strategies but also individual self-protection measures.","sentences":["Nonreciprocal interaction crowd systems, such as human-human, human-vehicle, and human-robot systems, often have serious impacts on pedestrian safety and social order.","A more comprehensive understanding of these systems is needed to optimize system stability and efficiency.","Despite the importance of these interactions, empirical research in this area remains limited.","Thus, in our study we explore this underresearched area, focusing on scenarios where nonreciprocity plays a critical role, such as mass stabbings, which pose a substantial risk to public safety.","We conducted the first experiments on this system and analysed high-accuracy data obtained from these experiments.","The extent of the direct threat zone is determined by the speed of the moving threat and the radius of danger occurrence.","We further categorize potential threats into direct, adjacent, and rear-view zones, quantifying the level of threat for pedestrians.","Our study revealed that a pedestrian's desired velocity correlated positively with potential threat intensity, increasing until near the direct threat zone.","An emerging steady state is observed when escape routes are blocked by moving threats.","This deviation affects the density-velocity relationship, making it distinct from the general relationship.","This deviation signifies unique pedestrian behaviour in the presence of moving threats.","Additionally, the rate of change in the angle for pedestrian motion in various desired directions is synchronized.","This indicates the emergence of collective intelligence in nonreciprocal interaction crowd systems.","As a result, our study may constitute a pioneering step towards understanding nonreciprocal interactions in crowd systems through laboratory experiments.","These findings may enhance pedestrian safety and inform not only government crowd management strategies but also individual self-protection measures."],"url":"http://arxiv.org/abs/2404.01664v1","category":"physics.soc-ph"}
{"created":"2024-04-02 06:07:35","title":"CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models","abstract":"Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.","sentences":["Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.","Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.","Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset.","We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback.","This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory.","In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors.","Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs."],"url":"http://arxiv.org/abs/2404.01663v1","category":"cs.CL"}
{"created":"2024-04-02 05:44:50","title":"Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization","abstract":"Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model's performance in its original corpus and domain.","sentences":["Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus.","However, real-world knowledge is not static; it updates and evolves continually.","Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate.","In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains.","In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains.","We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus.","We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training.","Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model's performance in its original corpus and domain."],"url":"http://arxiv.org/abs/2404.01652v1","category":"cs.CL"}
{"created":"2024-04-02 05:34:33","title":"Test-Time Model Adaptation with Only Forward Passes","abstract":"Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C. The source code will be released.","sentences":["Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts.","However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration.","In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported.","To address this, we propose a test-time Forward-Only Adaptation (FOA) method.","In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy.","To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy.","Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance.","Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C. The source code will be released."],"url":"http://arxiv.org/abs/2404.01650v1","category":"cs.LG"}
{"created":"2024-04-02 05:19:27","title":"A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection","abstract":"Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models. (2) CT-scan contains large number of out-of-distribution (OOD) slices. The crucial features may only be present in specific spatial regions and slices of the entire CT scan. How can we effectively figure out where these are located? To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan. It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence and boosting performance. As a result, the experiments demonstrate the promising performance of our model using a simple EfficientNet-2D (E2D) model, even with only 1% of the training data. The efficacy of our approach has been validated on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024. Our source code will be made available.","sentences":["Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models.","(2) CT-scan contains large number of out-of-distribution (OOD) slices.","The crucial features may only be present in specific spatial regions and slices of the entire CT scan.","How can we effectively figure out where these are located?","To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan.","It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally.","Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence and boosting performance.","As a result, the experiments demonstrate the promising performance of our model using a simple EfficientNet-2D (E2D) model, even with only 1% of the training data.","The efficacy of our approach has been validated on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024.","Our source code will be made available."],"url":"http://arxiv.org/abs/2404.01643v1","category":"eess.IV"}
{"created":"2024-04-02 04:30:53","title":"SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed & Timed Transport with Recycled Entropies","abstract":"With the rapid growth of machine learning (ML) workloads in datacenters, existing congestion control (CC) algorithms fail to deliver the required performance at scale. ML traffic is bursty and bulk-synchronous and thus requires quick reaction and strong fairness. We show that existing CC algorithms that use delay as a main signal react too slowly and are not always fair. We design SMaRTT, a simple sender-based CC algorithm that combines delay, ECN, and optional packet trimming for fast and precise window adjustments. At the core of SMaRTT lies the novel QuickAdapt algorithm that accurately estimates the bandwidth at the receiver. We show how to combine SMaRTT with a new per-packet traffic load-balancing algorithm called REPS to effectively reroute packets around congested hotspots as well as flaky or failing links. Our evaluation shows that SMaRTT alone outperforms EQDS, Swift, BBR, and MPRDMA by up to 50% on modern datacenter networks.","sentences":["With the rapid growth of machine learning (ML) workloads in datacenters, existing congestion control (CC) algorithms fail to deliver the required performance at scale.","ML traffic is bursty and bulk-synchronous and thus requires quick reaction and strong fairness.","We show that existing CC algorithms that use delay as a main signal react too slowly and are not always fair.","We design SMaRTT, a simple sender-based CC algorithm that combines delay, ECN, and optional packet trimming for fast and precise window adjustments.","At the core of SMaRTT lies the novel QuickAdapt algorithm that accurately estimates the bandwidth at the receiver.","We show how to combine SMaRTT with a new per-packet traffic load-balancing algorithm called REPS to effectively reroute packets around congested hotspots as well as flaky or failing links.","Our evaluation shows that SMaRTT alone outperforms EQDS, Swift, BBR, and MPRDMA by up to 50% on modern datacenter networks."],"url":"http://arxiv.org/abs/2404.01630v1","category":"cs.NI"}
{"created":"2024-04-02 04:22:07","title":"AAA: an Adaptive Mechanism for Locally Differential Private Mean Estimation","abstract":"Local differential privacy (LDP) is a strong privacy standard that has been adopted by popular software systems. The main idea is that each individual perturbs their own data locally, and only submits the resulting noisy version to a data aggregator. Although much effort has been devoted to computing various types of aggregates and building machine learning applications under LDP, research on fundamental perturbation mechanisms has not achieved significant improvement in recent years. Towards a more refined result utility, existing works mainly focus on improving the worst-case guarantee. However, this approach does not necessarily promise a better average performance given the fact that the data in practice obey a certain distribution, which is not known beforehand.   In this paper, we propose the advanced adaptive additive (AAA) mechanism, which is a distribution-aware approach that addresses the average utility and tackles the classical mean estimation problem. AAA is carried out in a two-step approach: first, as the global data distribution is not available beforehand, the data aggregator selects a random subset of individuals to compute a (noisy) quantized data descriptor; then, the data aggregator collects data from the remaining individuals, which are perturbed in a distribution-aware fashion. The perturbation involved in the latter step is obtained by solving an optimization problem, which is formulated with the data descriptor obtained in the former step and the desired properties of task-determined utilities. We provide rigorous privacy proofs, utility analyses, and extensive experiments comparing AAA with state-of-the-art mechanisms. The evaluation results demonstrate that the AAA mechanism consistently outperforms existing solutions with a clear margin in terms of result utility, on a wide range of privacy constraints and real-world and synthetic datasets.","sentences":["Local differential privacy (LDP) is a strong privacy standard that has been adopted by popular software systems.","The main idea is that each individual perturbs their own data locally, and only submits the resulting noisy version to a data aggregator.","Although much effort has been devoted to computing various types of aggregates and building machine learning applications under LDP, research on fundamental perturbation mechanisms has not achieved significant improvement in recent years.","Towards a more refined result utility, existing works mainly focus on improving the worst-case guarantee.","However, this approach does not necessarily promise a better average performance given the fact that the data in practice obey a certain distribution, which is not known beforehand.   ","In this paper, we propose the advanced adaptive additive (AAA) mechanism, which is a distribution-aware approach that addresses the average utility and tackles the classical mean estimation problem.","AAA is carried out in a two-step approach: first, as the global data distribution is not available beforehand, the data aggregator selects a random subset of individuals to compute a (noisy) quantized data descriptor; then, the data aggregator collects data from the remaining individuals, which are perturbed in a distribution-aware fashion.","The perturbation involved in the latter step is obtained by solving an optimization problem, which is formulated with the data descriptor obtained in the former step and the desired properties of task-determined utilities.","We provide rigorous privacy proofs, utility analyses, and extensive experiments comparing AAA with state-of-the-art mechanisms.","The evaluation results demonstrate that the AAA mechanism consistently outperforms existing solutions with a clear margin in terms of result utility, on a wide range of privacy constraints and real-world and synthetic datasets."],"url":"http://arxiv.org/abs/2404.01625v2","category":"cs.CR"}
{"created":"2024-04-02 03:44:25","title":"Multi-Robot Collaborative Navigation with Formation Adaptation","abstract":"Multi-robot collaborative navigation is an essential ability where teamwork and synchronization are keys. In complex and uncertain environments, adaptive formation is vital, as rigid formations prove to be inadequate. The ability of robots to dynamically adjust their formation enables navigation through unpredictable spaces, maintaining cohesion, and effectively responding to environmental challenges. In this paper, we introduce a novel approach that uses bi-level learning framework. Specifically, we use graph learning at a high level for group coordination and reinforcement learning for individual navigation. We innovate by integrating a spring-damper model within the reinforcement learning reward mechanism, addressing the rigidity of traditional formation control methods. During execution, our approach enables a team of robots to successfully navigate challenging environments, maintain a desired formation shape, and dynamically adjust their formation scale based on environmental information. We conduct extensive experiments to evaluate our approach across three distinct formation scenarios in multi-robot navigation: circle, line, and wedge. Experimental results show that our approach achieves promising results and scalability on multi-robot navigation with formation adaptation.","sentences":["Multi-robot collaborative navigation is an essential ability where teamwork and synchronization are keys.","In complex and uncertain environments, adaptive formation is vital, as rigid formations prove to be inadequate.","The ability of robots to dynamically adjust their formation enables navigation through unpredictable spaces, maintaining cohesion, and effectively responding to environmental challenges.","In this paper, we introduce a novel approach that uses bi-level learning framework.","Specifically, we use graph learning at a high level for group coordination and reinforcement learning for individual navigation.","We innovate by integrating a spring-damper model within the reinforcement learning reward mechanism, addressing the rigidity of traditional formation control methods.","During execution, our approach enables a team of robots to successfully navigate challenging environments, maintain a desired formation shape, and dynamically adjust their formation scale based on environmental information.","We conduct extensive experiments to evaluate our approach across three distinct formation scenarios in multi-robot navigation: circle, line, and wedge.","Experimental results show that our approach achieves promising results and scalability on multi-robot navigation with formation adaptation."],"url":"http://arxiv.org/abs/2404.01618v1","category":"cs.RO"}
{"created":"2024-04-02 03:43:55","title":"LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models","abstract":"We present LLM-ABR, the first system that utilizes the generative capabilities of large language models (LLMs) to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics. Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to design key components such as states and neural network architectures. We evaluate LLM-ABR across diverse network settings, including broadband, satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms.","sentences":["We present LLM-ABR, the first system that utilizes the generative capabilities of large language models (LLMs) to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics.","Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to design key components such as states and neural network architectures.","We evaluate LLM-ABR across diverse network settings, including broadband, satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms."],"url":"http://arxiv.org/abs/2404.01617v1","category":"cs.NI"}
{"created":"2024-04-02 03:39:06","title":"Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration","abstract":"Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors. The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time. Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts. These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams.","sentences":["Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each.","In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies.","User trust is essential to creating and maintaining these collaborative relationships.","Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors.","The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time.","Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts.","These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams."],"url":"http://arxiv.org/abs/2404.01615v1","category":"cs.HC"}
{"created":"2024-04-02 03:21:06","title":"Distributed and Rate-Adaptive Feature Compression","abstract":"We study the problem of distributed and rate-adaptive feature compression for linear regression. A set of distributed sensors collect disjoint features of regressor data. A fusion center is assumed to contain a pretrained linear regression model, trained on a dataset of the entire uncompressed data. At inference time, the sensors compress their observations and send them to the fusion center through communication-constrained channels, whose rates can change with time. Our goal is to design a feature compression {scheme} that can adapt to the varying communication constraints, while maximizing the inference performance at the fusion center. We first obtain the form of optimal quantizers assuming knowledge of underlying regressor data distribution. Under a practically reasonable approximation, we then propose a distributed compression scheme which works by quantizing a one-dimensional projection of the sensor data. We also propose a simple adaptive scheme for handling changes in communication constraints. We demonstrate the effectiveness of the distributed adaptive compression scheme through simulated experiments.","sentences":["We study the problem of distributed and rate-adaptive feature compression for linear regression.","A set of distributed sensors collect disjoint features of regressor data.","A fusion center is assumed to contain a pretrained linear regression model, trained on a dataset of the entire uncompressed data.","At inference time, the sensors compress their observations and send them to the fusion center through communication-constrained channels, whose rates can change with time.","Our goal is to design a feature compression {scheme} that can adapt to the varying communication constraints, while maximizing the inference performance at the fusion center.","We first obtain the form of optimal quantizers assuming knowledge of underlying regressor data distribution.","Under a practically reasonable approximation, we then propose a distributed compression scheme which works by quantizing a one-dimensional projection of the sensor data.","We also propose a simple adaptive scheme for handling changes in communication constraints.","We demonstrate the effectiveness of the distributed adaptive compression scheme through simulated experiments."],"url":"http://arxiv.org/abs/2404.02179v1","category":"cs.IT"}
{"created":"2024-04-02 02:39:17","title":"Extremum-Seeking Action Selection for Accelerating Policy Optimization","abstract":"Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment. Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments.","sentences":["Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance.","Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence.","In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning.","We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC).","On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal.","Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment.","Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments."],"url":"http://arxiv.org/abs/2404.01598v1","category":"cs.LG"}
{"created":"2024-04-02 02:31:13","title":"Language Model Guided Interpretable Video Action Reasoning","abstract":"While neural networks have excelled in video action recognition tasks, their black-box nature often obscures the understanding of their decision-making processes. Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human reasoning. These models, however, usually fall short in performance compared to their black-box counterparts. In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR). LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models. In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models. Using the logical reasoning captured by the language model, we steer the training of the video model. This integrated approach not only improves the video model's adaptability to different domains but also boosts its overall performance. Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework. The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.","sentences":["While neural networks have excelled in video action recognition tasks, their black-box nature often obscures the understanding of their decision-making processes.","Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human reasoning.","These models, however, usually fall short in performance compared to their black-box counterparts.","In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR).","LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models.","In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models.","Using the logical reasoning captured by the language model, we steer the training of the video model.","This integrated approach not only improves the video model's adaptability to different domains but also boosts its overall performance.","Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework.","The code of LaIAR is available at https://github.com/NingWang2049/LaIAR."],"url":"http://arxiv.org/abs/2404.01591v1","category":"cs.CV"}
{"created":"2024-04-02 02:17:50","title":"Diffusion Deepfake","abstract":"Recent progress in generative AI, primarily through diffusion models, presents significant challenges for real-world deepfake detection. The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes. Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art diffusion models as other datasets are less diverse and low in quality. Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets. Our strategic dataset creation not only challenge the deepfake detectors but also sets a new benchmark for more evaluation. Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of diffusion deepfakes, limiting their practical utility. To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods. This involves expanding the diversity of both manipulation techniques and image domains. Our findings underscore that increasing training data diversity results in improved generalizability. Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity. This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model's adaptability to both easy and challenging samples. Extensive experiments on both existing and newly proposed benchmarks demonstrate that our model optimization approach surpasses prior alternatives significantly.","sentences":["Recent progress in generative AI, primarily through diffusion models, presents significant challenges for real-world deepfake detection.","The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes.","Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art diffusion models as other datasets are less diverse and low in quality.","Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets.","Our strategic dataset creation not only challenge the deepfake detectors but also sets a new benchmark for more evaluation.","Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of diffusion deepfakes, limiting their practical utility.","To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods.","This involves expanding the diversity of both manipulation techniques and image domains.","Our findings underscore that increasing training data diversity results in improved generalizability.","Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity.","This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model's adaptability to both easy and challenging samples.","Extensive experiments on both existing and newly proposed benchmarks demonstrate that our model optimization approach surpasses prior alternatives significantly."],"url":"http://arxiv.org/abs/2404.01579v1","category":"cs.CV"}
{"created":"2024-04-02 02:05:17","title":"DCP and VarDis: An Ad-Hoc Protocol Stack for Dynamic Swarms and Formations of Drones -- Extended Version","abstract":"Recently, swarms or formations of drones have received increased interest both in the literature and in applications. To dynamically adapt to their operating environment, swarm members need to communicate wirelessly for control and coordination tasks. One fundamental communication pattern required for basic safety purposes, such as collision avoidance, is beaconing, where drones frequently transmit information about their position, speed, heading, and other operational data to a local neighbourhood, using a local broadcast service. In this paper, we propose and analyse a protocol stack which allows to use the recurring-beaconing primitive for additional purposes. In particular, we propose the VarDis (Variable Dissemination) protocol, which creates the abstraction of variables to which all members of a drone swarm have (read) access, and which can naturally be used for centralized control of a swarm, amongst other applications. We describe the involved protocols and provide a mainly simulation-based performance analysis of VarDis.","sentences":["Recently, swarms or formations of drones have received increased interest both in the literature and in applications.","To dynamically adapt to their operating environment, swarm members need to communicate wirelessly for control and coordination tasks.","One fundamental communication pattern required for basic safety purposes, such as collision avoidance, is beaconing, where drones frequently transmit information about their position, speed, heading, and other operational data to a local neighbourhood, using a local broadcast service.","In this paper, we propose and analyse a protocol stack which allows to use the recurring-beaconing primitive for additional purposes.","In particular, we propose the VarDis (Variable Dissemination) protocol, which creates the abstraction of variables to which all members of a drone swarm have (read) access, and which can naturally be used for centralized control of a swarm, amongst other applications.","We describe the involved protocols and provide a mainly simulation-based performance analysis of VarDis."],"url":"http://arxiv.org/abs/2404.01570v1","category":"cs.NI"}
{"created":"2024-04-03 17:53:32","title":"Comment on \"Machine learning conservation laws from differential equations\"","abstract":"In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the author. However, their derivation contained six serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed.","sentences":["In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark","[2, 3], without citing the author.","However, their derivation contained six serious errors, causing both their method and result to be incorrect.","In this Comment, those errors are reviewed."],"url":"http://arxiv.org/abs/2404.02896v1","category":"cs.LG"}
{"created":"2024-04-03 17:52:28","title":"Renormalized energy of proper maps and conformal geodesics","abstract":"We introduce a certain renormalized energy of proper maps between conformally compact Einstein manifolds, which is then used to give a holographic description of conformal geodesics on the boundary at infinity, in a way deeply inspired by a work of Fine and Herfray on renormalized area minimization and conformal geodesics.","sentences":["We introduce a certain renormalized energy of proper maps between conformally compact Einstein manifolds, which is then used to give a holographic description of conformal geodesics on the boundary at infinity, in a way deeply inspired by a work of Fine and Herfray on renormalized area minimization and conformal geodesics."],"url":"http://arxiv.org/abs/2404.02895v1","category":"math.DG"}
{"created":"2024-04-03 17:49:41","title":"MODNO: Multi Operator Learning With Distributed Neural Operators","abstract":"The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset. Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method. Our results demonstrate enhanced efficiency and satisfactory accuracy. Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning. This highlights another MOL's potential to bolster operator learning.","sentences":["The study of operator learning involves the utilization of neural networks to approximate operators.","Traditionally, the focus has been on single-operator learning (SOL).","However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL).","In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs.","Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).","The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset.","Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method.","Our results demonstrate enhanced efficiency and satisfactory accuracy.","Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning.","This highlights another MOL's potential to bolster operator learning."],"url":"http://arxiv.org/abs/2404.02892v1","category":"cs.LG"}
{"created":"2024-04-03 17:42:22","title":"Learning Quadrupedal Locomotion via Differentiable Simulation","abstract":"The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional Reinforcement Learning (RL) methods. While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce. This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients. The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact. Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact simulation. We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients.","sentences":["The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional Reinforcement Learning (RL) methods.","While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce.","This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients.","The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact.","Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact simulation.","We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients."],"url":"http://arxiv.org/abs/2404.02887v1","category":"cs.RO"}
{"created":"2024-04-03 17:38:04","title":"Stability of multiphase mean curvature flow beyond circular topology changes","abstract":"We prove a weak-strong uniqueness principle for varifold-BV solutions to planar multiphase mean curvature flow beyond a circular topology change: Assuming that there exists a classical solution with an interface that becomes increasingly circular and shrinks to a point, any varifold-BV solution with the same initial interface must coincide with it, and any varifold-BV solution with similar initial data must undergo the same type of topology change. Our result illustrates the robustness of the relative energy method for establishing weak-strong uniqueness principles for interface evolution equations, showing that it may also be applied beyond certain topological changes.","sentences":["We prove a weak-strong uniqueness principle for varifold-BV solutions to planar multiphase mean curvature flow beyond a circular topology change: Assuming that there exists a classical solution with an interface that becomes increasingly circular and shrinks to a point, any varifold-BV solution with the same initial interface must coincide with it, and any varifold-BV solution with similar initial data must undergo the same type of topology change.","Our result illustrates the robustness of the relative energy method for establishing weak-strong uniqueness principles for interface evolution equations, showing that it may also be applied beyond certain topological changes."],"url":"http://arxiv.org/abs/2404.02884v1","category":"math.AP"}
{"created":"2024-04-03 17:16:37","title":"Subconductance states in a semimicroscopic model for a tetrameric pore","abstract":"A physical model for a structured tetrameric pore is studied. The pore is modeled as a device composed of four subunits, each one exhibiting two possible states (open and closed). The pore is located within a membrane that separates two reservoirs with ionic solutions. All variables of the model follow physical dynamical equations accounting for the internal structure of the pore, derived from a single energy functional and supplemented with thermal noises. An extensive study of the resulting ionic intensity is performed for different values of the control parameters, mainly membrane potential and reservoir ion concentrations. Two possible physical devices are studied: voltage-gated (including a voltage sensor in each subunit) and non-voltage-gated pores. The ionic flux through the pore exhibits several distinct dynamical configurations, in particular subconductance states, which indicate very different dynamical internal states of the subunits. Such subconductance states become much easier to observe in sensorless pores. These results are compared with available experimental data on tetrameric K channels and analytical predictions.","sentences":["A physical model for a structured tetrameric pore is studied.","The pore is modeled as a device composed of four subunits, each one exhibiting two possible states (open and closed).","The pore is located within a membrane that separates two reservoirs with ionic solutions.","All variables of the model follow physical dynamical equations accounting for the internal structure of the pore, derived from a single energy functional and supplemented with thermal noises.","An extensive study of the resulting ionic intensity is performed for different values of the control parameters, mainly membrane potential and reservoir ion concentrations.","Two possible physical devices are studied: voltage-gated (including a voltage sensor in each subunit) and non-voltage-gated pores.","The ionic flux through the pore exhibits several distinct dynamical configurations, in particular subconductance states, which indicate very different dynamical internal states of the subunits.","Such subconductance states become much easier to observe in sensorless pores.","These results are compared with available experimental data on tetrameric K channels and analytical predictions."],"url":"http://arxiv.org/abs/2404.02875v1","category":"physics.bio-ph"}
{"created":"2024-04-03 16:58:03","title":"Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds","abstract":"Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.","sentences":["Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers.","The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\").","The added noise helps prevent reconstruction of the inputs from the noisy features.","Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise.","Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds.","Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification.","The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes.","Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet.","In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features.","Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification."],"url":"http://arxiv.org/abs/2404.02866v1","category":"cs.LG"}
{"created":"2024-04-03 13:39:29","title":"LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis","abstract":"Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at https://github.com/ispc-lab/LiDAR4D.","sentences":["Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored.","Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds.","In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis.","In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner.","Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency.","For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns.","Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction.","Codes are available at https://github.com/ispc-lab/LiDAR4D."],"url":"http://arxiv.org/abs/2404.02742v1","category":"cs.CV"}
{"created":"2024-04-03 12:40:23","title":"Stable patterns in the Lugiato-Lefever equation with a confined vortex pump","abstract":"We introduce a model of a passive optical cavity based on a novel variety of the two-dimensional Lugiato-Lefever equation, with a localized pump carrying intrinsic vorticity S, and the cubic or cubic-quintic nonlinearity. Up to S = 5, stable confined vortex-ring states (vortex pixels) are produced by means of a variational approximation and in a numerical form. Surprisingly, vast stability areas of the vortex states are found, for both the self-focusing and defocusing signs of the nonlinearity, in the plane of the pump and loss parameters. When the vortex-rings are unstable, they are destroyed by azimuthal perturbations which break the axial symmetry. The results suggest new possibilities for mode manipulations in passive nonlinear photonic media by means of appropriately designed pump beams.","sentences":["We introduce a model of a passive optical cavity based on a novel variety of the two-dimensional Lugiato-Lefever equation, with a localized pump carrying intrinsic vorticity S, and the cubic or cubic-quintic nonlinearity.","Up to S = 5, stable confined vortex-ring states (vortex pixels) are produced by means of a variational approximation and in a numerical form.","Surprisingly, vast stability areas of the vortex states are found, for both the self-focusing and defocusing signs of the nonlinearity, in the plane of the pump and loss parameters.","When the vortex-rings are unstable, they are destroyed by azimuthal perturbations which break the axial symmetry.","The results suggest new possibilities for mode manipulations in passive nonlinear photonic media by means of appropriately designed pump beams."],"url":"http://arxiv.org/abs/2404.02693v1","category":"physics.optics"}
{"created":"2024-04-03 12:25:00","title":"A weak-strong uniqueness principle for the Mullins-Sekerka equation","abstract":"We establish a weak-strong uniqueness principle for the two-phase Mullins-Sekerka equation in the plane: As long as a classical solution to the evolution problem exists, any weak De Giorgi type varifold solution (see for this notion the recent work of Stinson and the second author, Arch. Ration. Mech. Anal. 248, 8, 2024) must coincide with it. In particular, in the absence of geometric singularities such weak solutions do not introduce a mechanism for (unphysical) non-uniqueness. We also derive a stability estimate with respect to changes in the data. Our method is based on the notion of relative entropies for interface evolution problems, a reduction argument to a perturbative graph setting (which is the only step in our argument exploiting in an essential way the planar setting), and a stability analysis in this perturbative regime relying crucially on the gradient flow structure of the Mullins-Sekerka equation.","sentences":["We establish a weak-strong uniqueness principle for the two-phase Mullins-Sekerka equation in the plane: As long as a classical solution to the evolution problem exists, any weak De Giorgi type varifold solution (see for this notion the recent work of Stinson and the second author, Arch.","Ration.","Mech.","Anal.","248, 8, 2024) must coincide with it.","In particular, in the absence of geometric singularities such weak solutions do not introduce a mechanism for (unphysical) non-uniqueness.","We also derive a stability estimate with respect to changes in the data.","Our method is based on the notion of relative entropies for interface evolution problems, a reduction argument to a perturbative graph setting (which is the only step in our argument exploiting in an essential way the planar setting), and a stability analysis in this perturbative regime relying crucially on the gradient flow structure of the Mullins-Sekerka equation."],"url":"http://arxiv.org/abs/2404.02682v1","category":"math.AP"}
{"created":"2024-04-03 11:55:18","title":"Oberwolfach Report: Scalar Curvature Stability","abstract":"Although scalar curvature is the simplest curvature invariant, our understanding of scalar curvature has not matured to the same level as Ricci or sectional curvature. Despite this fact, many rigidity phenomenon have been established which give some of the strongest insights into scalar curvature. Important examples include Geroch's conjecture, the positive mass theorem, and Llarull's theorem. In order to further understand scalar curvature we ask corresponding geometric stability questions, where the hypotheses of the rigidity phenomenon are relaxed, and one would like to show that Riemannian manifolds which satisfy the relaxed conditions are close to the rigid objects in some topology. In this note we will survey what is known for scalar curvature stability, discuss what the questions are in this area, and introduce important tools which have been useful so far.","sentences":["Although scalar curvature is the simplest curvature invariant, our understanding of scalar curvature has not matured to the same level as Ricci or sectional curvature.","Despite this fact, many rigidity phenomenon have been established which give some of the strongest insights into scalar curvature.","Important examples include Geroch's conjecture, the positive mass theorem, and Llarull's theorem.","In order to further understand scalar curvature we ask corresponding geometric stability questions, where the hypotheses of the rigidity phenomenon are relaxed, and one would like to show that Riemannian manifolds which satisfy the relaxed conditions are close to the rigid objects in some topology.","In this note we will survey what is known for scalar curvature stability, discuss what the questions are in this area, and introduce important tools which have been useful so far."],"url":"http://arxiv.org/abs/2404.02662v1","category":"math.DG"}
{"created":"2024-04-03 11:47:20","title":"A Satellite Band Selection Framework for Amazon Forest Deforestation Detection Task","abstract":"The conservation of tropical forests is a topic of significant social and ecological relevance due to their crucial role in the global ecosystem. Unfortunately, deforestation and degradation impact millions of hectares annually, necessitating government or private initiatives for effective forest monitoring. This study introduces a novel framework that employs the Univariate Marginal Distribution Algorithm (UMDA) to select spectral bands from Landsat-8 satellite, optimizing the representation of deforested areas. This selection guides a semantic segmentation architecture, DeepLabv3+, enhancing its performance. Experimental results revealed several band compositions that achieved superior balanced accuracy compared to commonly adopted combinations for deforestation detection, utilizing segment classification via a Support Vector Machine (SVM). Moreover, the optimal band compositions identified by the UMDA-based approach improved the performance of the DeepLabv3+ architecture, surpassing state-of-the-art approaches compared in this study. The observation that a few selected bands outperform the total contradicts the data-driven paradigm prevalent in the deep learning field. Therefore, this suggests an exception to the conventional wisdom that 'more is always better'.","sentences":["The conservation of tropical forests is a topic of significant social and ecological relevance due to their crucial role in the global ecosystem.","Unfortunately, deforestation and degradation impact millions of hectares annually, necessitating government or private initiatives for effective forest monitoring.","This study introduces a novel framework that employs the Univariate Marginal Distribution Algorithm (UMDA) to select spectral bands from Landsat-8 satellite, optimizing the representation of deforested areas.","This selection guides a semantic segmentation architecture, DeepLabv3+, enhancing its performance.","Experimental results revealed several band compositions that achieved superior balanced accuracy compared to commonly adopted combinations for deforestation detection, utilizing segment classification via a Support Vector Machine (SVM).","Moreover, the optimal band compositions identified by the UMDA-based approach improved the performance of the DeepLabv3+ architecture, surpassing state-of-the-art approaches compared in this study.","The observation that a few selected bands outperform the total contradicts the data-driven paradigm prevalent in the deep learning field.","Therefore, this suggests an exception to the conventional wisdom that 'more is always better'."],"url":"http://arxiv.org/abs/2404.02659v1","category":"cs.CV"}
{"created":"2024-04-03 10:52:23","title":"Sibyll$^{\\bigstar}$","abstract":"In the last decade, an increasing number of datasets have revealed a consistent discrepancy between the number of muons measured in ultra-high-energy extensive air showers (EAS) and the numbers predicted by simulations. This gap persists despite incorporating Large Hadron Collider (LHC) data into the tuning of current hadronic interaction models, leading to the phenomenon often termed the ''muon puzzle''. To gain a deeper understanding of the potential origins of this muon puzzle, we have developed Sibyll$^{\\bigstar}$, a series of phenomenologically modified versions of Sibyll 2.3d. In these models, we have increased muon production by altering $\\rho^0$, baryon-antibaryon pair, or kaon production in hadronic multiparticle production processes. These variants remain within bounds from provided by accelerator measurements, including those from the LHC and fixed-target experiments, notably NA49 and NA61, showing a level of consistency comparable to Sibyll 2.3d. Our findings show that these modifications can increase the muon count in EAS by up to 35%, while minimally affecting the depth of shower maximum ($X_{\\rm max}$) and other shower variables. Additionally, we assess the impact of these modifications on various observables, including inclusive muon and neutrino fluxes and the multiplicities of muon bundles in deep underground and water/ice Cherenkov detectors. We aim for at least one of these model variants to offer a more accurate representation of EAS data at the highest energies, thereby enhancing the quality of Monte Carlo predictions used in training neural networks. This improvement is crucial for achieving more reliable data analyses and interpretations.","sentences":["In the last decade, an increasing number of datasets have revealed a consistent discrepancy between the number of muons measured in ultra-high-energy extensive air showers (EAS) and the numbers predicted by simulations.","This gap persists despite incorporating Large Hadron Collider (LHC) data into the tuning of current hadronic interaction models, leading to the phenomenon often termed the ''muon puzzle''.","To gain a deeper understanding of the potential origins of this muon puzzle, we have developed Sibyll$^{\\bigstar}$, a series of phenomenologically modified versions of Sibyll 2.3d.","In these models, we have increased muon production by altering $\\rho^0$, baryon-antibaryon pair, or kaon production in hadronic multiparticle production processes.","These variants remain within bounds from provided by accelerator measurements, including those from the LHC and fixed-target experiments, notably NA49 and NA61, showing a level of consistency comparable to Sibyll 2.3d.","Our findings show that these modifications can increase the muon count in EAS by up to 35%, while minimally affecting the depth of shower maximum ($X_{\\rm max}$) and other shower variables.","Additionally, we assess the impact of these modifications on various observables, including inclusive muon and neutrino fluxes and the multiplicities of muon bundles in deep underground and water/ice Cherenkov detectors.","We aim for at least one of these model variants to offer a more accurate representation of EAS data at the highest energies, thereby enhancing the quality of Monte Carlo predictions used in training neural networks.","This improvement is crucial for achieving more reliable data analyses and interpretations."],"url":"http://arxiv.org/abs/2404.02636v1","category":"hep-ph"}
{"created":"2024-04-03 10:37:56","title":"Two-Stage Super-Resolution Simulation Method for Three-Dimensional Flow Fields Around Buildings for Real-Time Prediction of Urban Micrometeorology","abstract":"A two-stage super-resolution simulation method is proposed for building-resolving micrometeorology simulations, which considerably reduces the computation time while maintaining accuracy. The first stage employs a convolutional neural network (CNN) to correct large-scale flows above buildings in the input of low-resolution (LR) simulation results. The second stage uses another CNN to reconstruct small-scale flows between buildings from the output of the first stage, resulting in high-resolution (HR) inferences. The CNNs are trained using HR simulation data for the second stage and their coarse-grained version for the first stage. This learning approach separates the flow scales to be inferred in each stage. The effectiveness of the proposed method was evaluated using micrometeorological simulations in an actual urban area around Tokyo Station in Japan. The super-resolution simulation successfully inferred HR atmospheric flows, reducing errors by about 50% for air temperature and 60\\% for wind velocity compared to the LR simulations. Furthermore, the two-stage approach allowed for localized HR inferences, reducing GPU memory usage to 12% during the training phase. The total wall-clock time for a 60-min prediction was reduced to about 9.92 min, which was approximately 3.2% (i.e., a 31-fold speedup) of the HR simulation time (309 min). The proposed method demonstrates the feasibility of real-time micrometeorology predictions in urban areas with a combination of physics-based and data-driven models.","sentences":["A two-stage super-resolution simulation method is proposed for building-resolving micrometeorology simulations, which considerably reduces the computation time while maintaining accuracy.","The first stage employs a convolutional neural network (CNN) to correct large-scale flows above buildings in the input of low-resolution (LR) simulation results.","The second stage uses another CNN to reconstruct small-scale flows between buildings from the output of the first stage, resulting in high-resolution (HR) inferences.","The CNNs are trained using HR simulation data for the second stage and their coarse-grained version for the first stage.","This learning approach separates the flow scales to be inferred in each stage.","The effectiveness of the proposed method was evaluated using micrometeorological simulations in an actual urban area around Tokyo Station in Japan.","The super-resolution simulation successfully inferred HR atmospheric flows, reducing errors by about 50% for air temperature and 60\\% for wind velocity compared to the LR simulations.","Furthermore, the two-stage approach allowed for localized HR inferences, reducing GPU memory usage to 12% during the training phase.","The total wall-clock time for a 60-min prediction was reduced to about 9.92 min, which was approximately 3.2% (i.e., a 31-fold speedup) of the HR simulation time (309 min).","The proposed method demonstrates the feasibility of real-time micrometeorology predictions in urban areas with a combination of physics-based and data-driven models."],"url":"http://arxiv.org/abs/2404.02631v1","category":"physics.ao-ph"}
{"created":"2024-04-03 09:55:51","title":"Some properties of a modified Hilbert transform","abstract":"Recently, Steinbach et al. introduced a novel operator $\\mathcal{H}_T: L^2(0,T) \\to L^2(0,T)$, known as the modified Hilbert transform. This operator has shown its significance in space-time formulations related to the heat and wave equations. In this paper, we establish a direct connection between the modified Hilbert transform $\\mathcal{H}_T$ and the canonical Hilbert transform $\\mathcal{H}$. Specifically, we prove the relationship $\\mathcal{H}_T \\varphi = -\\mathcal{H} \\tilde{\\varphi}$, where $\\varphi \\in L^2(0,T)$ and $\\tilde{\\varphi}$ is a suitable extension of $\\varphi$ over the entire $\\mathbb{R}$. By leveraging this crucial result, we derive some properties of $\\mathcal{H}_T$, including a new inversion formula, that emerge as immediate consequences of well-established findings on $\\mathcal{H}$.","sentences":["Recently, Steinbach et al. introduced a novel operator $\\mathcal{H}_T: L^2(0,T) \\to L^2(0,T)$, known as the modified Hilbert transform.","This operator has shown its significance in space-time formulations related to the heat and wave equations.","In this paper, we establish a direct connection between the modified Hilbert transform $\\mathcal{H}_T$ and the canonical Hilbert transform $\\mathcal{H}$. Specifically, we prove the relationship $\\mathcal{H}_T \\varphi = -\\mathcal{H} \\tilde{\\varphi}$, where $\\varphi \\in L^2(0,T)$ and $\\tilde{\\varphi}$ is a suitable extension of $\\varphi$ over the entire $\\mathbb{R}$. By leveraging this crucial result, we derive some properties of $\\mathcal{H}_T$, including a new inversion formula, that emerge as immediate consequences of well-established findings on $\\mathcal{H}$."],"url":"http://arxiv.org/abs/2404.02609v1","category":"math.CA"}
{"created":"2024-04-03 09:53:37","title":"DoubleTES detectors to investigate the CRESST low energy background: results from above-ground prototypes","abstract":"In recent times, the sensitivity of low-mass direct dark matter searches has been limited by unknown low energy backgrounds close to the energy threshold of the experiments known as the low energy excess (LEE). The CRESST experiment utilises advanced cryogenic detectors constructed with different types of crystals equipped with Transition Edge Sensors (TESs) to measure signals of nuclear recoils induced by the scattering of dark matter particles in the detector. In CRESST, this low energy background manifests itself as a steeply rising population of events below 200 eV. A novel detector design named doubleTES using two identical TESs on the target crystal was studied to investigate the hypothesis that the events are sensor-related. We present the first results from two such modules, demonstrating their ability to differentiate between events originating from the crystal's bulk and those occurring in the sensor or in its close proximity.","sentences":["In recent times, the sensitivity of low-mass direct dark matter searches has been limited by unknown low energy backgrounds close to the energy threshold of the experiments known as the low energy excess (LEE).","The CRESST experiment utilises advanced cryogenic detectors constructed with different types of crystals equipped with Transition Edge Sensors (TESs) to measure signals of nuclear recoils induced by the scattering of dark matter particles in the detector.","In CRESST, this low energy background manifests itself as a steeply rising population of events below 200 eV. A novel detector design named doubleTES using two identical TESs on the target crystal was studied to investigate the hypothesis that the events are sensor-related.","We present the first results from two such modules, demonstrating their ability to differentiate between events originating from the crystal's bulk and those occurring in the sensor or in its close proximity."],"url":"http://arxiv.org/abs/2404.02607v1","category":"physics.ins-det"}
{"created":"2024-04-03 09:44:45","title":"Deep learning for flow observables in high energy heavy-ion collisions","abstract":"We demonstrate how deep convolutional neural networks can be trained to predict 2+1 D hydrodynamic simulation results for flow coefficients, mean-transverse-momentum and charged particle multiplicity from the initial energy density profile. We show that this method provides results that are accurate enough, so that one can use neural networks to reliably estimate multi-particle flow correlators. Additionally, we train networks that can take any model parameter as an additional input and demonstrate with a few examples that the accuracy remains good. The usage of neural networks can reduce the computation time needed in performing Bayesian analyses with multi-particle flow correlators by many orders of magnitude.","sentences":["We demonstrate how deep convolutional neural networks can be trained to predict 2+1 D hydrodynamic simulation results for flow coefficients, mean-transverse-momentum and charged particle multiplicity from the initial energy density profile.","We show that this method provides results that are accurate enough, so that one can use neural networks to reliably estimate multi-particle flow correlators.","Additionally, we train networks that can take any model parameter as an additional input and demonstrate with a few examples that the accuracy remains good.","The usage of neural networks can reduce the computation time needed in performing Bayesian analyses with multi-particle flow correlators by many orders of magnitude."],"url":"http://arxiv.org/abs/2404.02602v1","category":"hep-ph"}
{"created":"2024-04-03 09:38:31","title":"Forward self-similar solutions to the MHD equations in the whole space","abstract":"In this paper, we study the existence of forward self-similar solutions to the three-dimensional Magnetohydrodynamic equations (MHD equations) with arbitrarily large self-similar initial data. Using the so called blow-up argument, we establish the necessary a priori estimates. Subsequently, the Leray-Schauder theorem allows us to construct a self-similar solutions of the MHD equations.","sentences":["In this paper, we study the existence of forward self-similar solutions to the three-dimensional Magnetohydrodynamic equations (MHD equations) with arbitrarily large self-similar initial data.","Using the so called blow-up argument, we establish the necessary a priori estimates.","Subsequently, the Leray-Schauder theorem allows us to construct a self-similar solutions of the MHD equations."],"url":"http://arxiv.org/abs/2404.02601v1","category":"math.AP"}
{"created":"2024-04-03 09:19:46","title":"QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection","abstract":"This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.","sentences":["This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection.","Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions.","Implementing a dual-phase training model across distributed clients surpasses existing methods in performance.","QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection."],"url":"http://arxiv.org/abs/2404.02595v1","category":"quant-ph"}
{"created":"2024-04-03 09:09:42","title":"Unsegment Anything by Simulating Deformation","abstract":"Foundation segmentation models, while powerful, pose a significant risk: they enable users to effortlessly extract any objects from any digital content with a single click, potentially leading to copyright infringement or malicious misuse. To mitigate this risk, we introduce a new task \"Anything Unsegmentable\" to grant any image \"the right to be unsegmented\". The ambitious pursuit of the task is to achieve highly transferable adversarial attacks against all prompt-based segmentation models, regardless of model parameterizations and prompts. We highlight the non-transferable and heterogeneous nature of prompt-specific adversarial noises. Our approach focuses on disrupting image encoder features to achieve prompt-agnostic attacks. Intriguingly, targeted feature attacks exhibit better transferability compared to untargeted ones, suggesting the optimal update direction aligns with the image manifold. Based on the observations, we design a novel attack named Unsegment Anything by Simulating Deformation (UAD). Our attack optimizes a differentiable deformation function to create a target deformed image, which alters structural information while preserving achievable feature distance by adversarial example. Extensive experiments verify the effectiveness of our approach, compromising a variety of promptable segmentation models with different architectures and prompt interfaces. We release the code at https://github.com/jiahaolu97/anything-unsegmentable.","sentences":["Foundation segmentation models, while powerful, pose a significant risk: they enable users to effortlessly extract any objects from any digital content with a single click, potentially leading to copyright infringement or malicious misuse.","To mitigate this risk, we introduce a new task \"Anything Unsegmentable\" to grant any image \"the right to be unsegmented\".","The ambitious pursuit of the task is to achieve highly transferable adversarial attacks against all prompt-based segmentation models, regardless of model parameterizations and prompts.","We highlight the non-transferable and heterogeneous nature of prompt-specific adversarial noises.","Our approach focuses on disrupting image encoder features to achieve prompt-agnostic attacks.","Intriguingly, targeted feature attacks exhibit better transferability compared to untargeted ones, suggesting the optimal update direction aligns with the image manifold.","Based on the observations, we design a novel attack named Unsegment Anything by Simulating Deformation (UAD).","Our attack optimizes a differentiable deformation function to create a target deformed image, which alters structural information while preserving achievable feature distance by adversarial example.","Extensive experiments verify the effectiveness of our approach, compromising a variety of promptable segmentation models with different architectures and prompt interfaces.","We release the code at https://github.com/jiahaolu97/anything-unsegmentable."],"url":"http://arxiv.org/abs/2404.02585v1","category":"cs.CV"}
{"created":"2024-04-03 08:33:53","title":"Probing microcavity resonance spectra with intracavity emitters","abstract":"We measure the fluorescence spectrum of broadband emitters in an open optical microcavity with radius of curvature R = 17.7(3) um and finesse F ~= 1000. This geometry enables a combined measurement of emission spectra versus cavity length, which has several benefits over measurements at fixed wavelength or fixed cavity length alone. We demonstrate the role of the optical penetration depths on the cavity modes and provide practical working equations for its analysis. Furthermore, we show the ability to measure the coupling of cavity modes within a small scan range of the cavity length. By measuring these cavity emission spectra as a function of cavity length, we thus obtain a rich and complete picture of the optical microcavity.","sentences":["We measure the fluorescence spectrum of broadband emitters in an open optical microcavity with radius of curvature R = 17.7(3) um and finesse F ~= 1000.","This geometry enables a combined measurement of emission spectra versus cavity length, which has several benefits over measurements at fixed wavelength or fixed cavity length alone.","We demonstrate the role of the optical penetration depths on the cavity modes and provide practical working equations for its analysis.","Furthermore, we show the ability to measure the coupling of cavity modes within a small scan range of the cavity length.","By measuring these cavity emission spectra as a function of cavity length, we thus obtain a rich and complete picture of the optical microcavity."],"url":"http://arxiv.org/abs/2404.02563v1","category":"physics.optics"}
{"created":"2024-04-03 08:13:13","title":"Well-posedness of the obstacle problem for stochastic nonlinear diffusion equations: an entropy formulation","abstract":"In this paper, we establish the existence, uniqueness and stability results for the obstacle problem associated with a degenerate nonlinear diffusion equation perturbed by conservative gradient noise. Our approach revolves round introducing a new entropy formulation for stochastic variational inequalities. As a consequence, we obtain a novel well-posedness result for the obstacle problem of deterministic porous medium equations with nonlinear reaction terms.","sentences":["In this paper, we establish the existence, uniqueness and stability results for the obstacle problem associated with a degenerate nonlinear diffusion equation perturbed by conservative gradient noise.","Our approach revolves round introducing a new entropy formulation for stochastic variational inequalities.","As a consequence, we obtain a novel well-posedness result for the obstacle problem of deterministic porous medium equations with nonlinear reaction terms."],"url":"http://arxiv.org/abs/2404.02547v1","category":"math.PR"}
{"created":"2024-04-03 07:58:24","title":"Global well-posedness for 2D inhomogeneous viscous flows with rough data via dynamic interpolation","abstract":"We consider the evolution of two-dimensional incompressible flows with variable density, only bounded and bounded away from zero. Assuming that the initial velocity belongs to a suitable critical subspace of L^2 , we prove a global-in-time existence and stability result for the initial (boundary) value problem. Our proof relies on new time decay estimates for finite energy weak solutions and on a 'dynamic interpolation' argument. We show that the constructed solutions have a uniformly C^1 flow, which ensures the propagation of geometrical structures in the fluid and guarantees that the Eulerian and Lagrangian formulations of the equations are equivalent. By adopting this latter formulation, we establish the uniqueness of the solutions for prescribed data, and the continuity of the flow map in an energy-like functional framework. In contrast with prior works, our results hold true in the critical regularity setting without any smallness assumption. Our approach uses only elementary tools and applies indistinctly to the cases where the fluid domain is the whole plane, a smooth two-dimensional bounded domain or the torus.","sentences":["We consider the evolution of two-dimensional incompressible flows with variable density, only bounded and bounded away from zero.","Assuming that the initial velocity belongs to a suitable critical subspace of L^2 , we prove a global-in-time existence and stability result for the initial (boundary) value problem.","Our proof relies on new time decay estimates for finite energy weak solutions and on a 'dynamic interpolation' argument.","We show that the constructed solutions have a uniformly C^1 flow, which ensures the propagation of geometrical structures in the fluid and guarantees that the Eulerian and Lagrangian formulations of the equations are equivalent.","By adopting this latter formulation, we establish the uniqueness of the solutions for prescribed data, and the continuity of the flow map in an energy-like functional framework.","In contrast with prior works, our results hold true in the critical regularity setting without any smallness assumption.","Our approach uses only elementary tools and applies indistinctly to the cases where the fluid domain is the whole plane, a smooth two-dimensional bounded domain or the torus."],"url":"http://arxiv.org/abs/2404.02541v1","category":"math.AP"}
{"created":"2024-04-03 07:44:41","title":"Construction of $r$-harmonic submanifolds in spheres","abstract":"We provide a new construction method for $r$-harmonic submanifolds in spheres, i.e. proper isometric $r$-harmonic immersions into spheres. Furthermore, we give new examples of $r$-harmonic spheres in spheres, possibly of high codimension. We determine index and nullity of these isometric $r$-harmonic immersions when $r=2$. Finally, we provide the normal index of these maps when $r=3$.","sentences":["We provide a new construction method for $r$-harmonic submanifolds in spheres, i.e. proper isometric $r$-harmonic immersions into spheres.","Furthermore, we give new examples of $r$-harmonic spheres in spheres, possibly of high codimension.","We determine index and nullity of these isometric $r$-harmonic immersions when $r=2$. Finally, we provide the normal index of these maps when $r=3$."],"url":"http://arxiv.org/abs/2404.02535v1","category":"math.DG"}
{"created":"2024-04-03 06:08:00","title":"A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media","abstract":"Solving high-wavenumber and heterogeneous Helmholtz equations presents a long-standing challenge in scientific computing. In this paper, we introduce a deep learning-enhanced multigrid solver to address this issue. By conducting error analysis on standard multigrid applied to a discrete Helmholtz equation, we devise a strategy to handle errors with different frequencies separately.   For error components with frequencies distant from the wavenumber, we perform simple smoothing based on local operations at different levels to eliminate them.   On the other hand, to address error components with frequencies near the wavenumber, we utilize another multigrid V-cycle to solve an advection-diffusion-reaction (ADR) equation at a coarse scale.   The resulting solver, named Wave-ADR-NS, involves parameters learned through unsupervised training.   Numerical results demonstrate that Wave-ADR-NS effectively resolves heterogeneous 2D Helmholtz equation with wavenumber up to 2000. Comparative experiments against classical multigrid preconditioners and existing deep learning-based multigrid preconditioners reveals the superior performance of Wave-ADR-NS.","sentences":["Solving high-wavenumber and heterogeneous Helmholtz equations presents a long-standing challenge in scientific computing.","In this paper, we introduce a deep learning-enhanced multigrid solver to address this issue.","By conducting error analysis on standard multigrid applied to a discrete Helmholtz equation, we devise a strategy to handle errors with different frequencies separately.   ","For error components with frequencies distant from the wavenumber, we perform simple smoothing based on local operations at different levels to eliminate them.   ","On the other hand, to address error components with frequencies near the wavenumber, we utilize another multigrid V-cycle to solve an advection-diffusion-reaction (ADR) equation at a coarse scale.   ","The resulting solver, named Wave-ADR-NS, involves parameters learned through unsupervised training.   ","Numerical results demonstrate that Wave-ADR-NS effectively resolves heterogeneous 2D Helmholtz equation with wavenumber up to 2000.","Comparative experiments against classical multigrid preconditioners and existing deep learning-based multigrid preconditioners reveals the superior performance of Wave-ADR-NS."],"url":"http://arxiv.org/abs/2404.02493v1","category":"math.NA"}
{"created":"2024-04-03 04:51:20","title":"An implementation of nuclear many-body wave functions by the superposition of localized Gaussians","abstract":"We introduce a new framework for the low-energy nuclear structure calculations, which describes the single-particle wave function as a superposition of localized Gaussians. It is a hybrid of the Hartree-Fock and antisymmetrized molecular dynamics models. In the numerical calculations of oxygen, calcium isotopes and 100Sn, the framework shows its potential by significantly improving upon AMD and yielding the results consistent with or even better than Hartree-Fock(-Bogoliubov) calculations based on harmonic oscillator expansions. In addition to the basic equations, general form of the matrix elements is also given.","sentences":["We introduce a new framework for the low-energy nuclear structure calculations, which describes the single-particle wave function as a superposition of localized Gaussians.","It is a hybrid of the Hartree-Fock and antisymmetrized molecular dynamics models.","In the numerical calculations of oxygen, calcium isotopes and 100Sn, the framework shows its potential by significantly improving upon AMD and yielding the results consistent with or even better than Hartree-Fock(-Bogoliubov) calculations based on harmonic oscillator expansions.","In addition to the basic equations, general form of the matrix elements is also given."],"url":"http://arxiv.org/abs/2404.02455v1","category":"nucl-th"}
{"created":"2024-04-03 04:23:01","title":"Masked Completion via Structured Diffusion with White-Box Transformers","abstract":"Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving a deep transformer-like masked autoencoder architecture, called CRATE-MAE, in which the role of each layer is mathematically fully interpretable: they transform the data distribution to and from a structured representation. Extensive empirical evaluations confirm our analytical insights. CRATE-MAE demonstrates highly promising performance on large-scale imagery datasets while using only ~30% of the parameters compared to the standard masked autoencoder with the same model configuration. The representations learned by CRATE-MAE have explicit structure and also contain semantic meaning. Code is available at https://github.com/Ma-Lab-Berkeley/CRATE .","sentences":["Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks.","These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant.","White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative.","However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification.","In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning.","We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving a deep transformer-like masked autoencoder architecture, called CRATE-MAE, in which the role of each layer is mathematically fully interpretable: they transform the data distribution to and from a structured representation.","Extensive empirical evaluations confirm our analytical insights.","CRATE-MAE demonstrates highly promising performance on large-scale imagery datasets while using only ~30% of the parameters compared to the standard masked autoencoder with the same model configuration.","The representations learned by CRATE-MAE have explicit structure and also contain semantic meaning.","Code is available at https://github.com/Ma-Lab-Berkeley/CRATE ."],"url":"http://arxiv.org/abs/2404.02446v1","category":"cs.LG"}
{"created":"2024-04-03 03:58:21","title":"Designing a Photonic Physically Unclonable Function Having Resilience to Machine Learning Attacks","abstract":"Physically unclonable functions (PUFs) are designed to act as device 'fingerprints.' Given an input challenge, the PUF circuit should produce an unpredictable response for use in situations such as root-of-trust applications and other hardware-level cybersecurity applications. PUFs are typically subcircuits present within integrated circuits (ICs), and while conventional IC PUFs are well-understood, several implementations have proven vulnerable to malicious exploits, including those perpetrated by machine learning (ML)-based attacks. Such attacks can be difficult to prevent because they are often designed to work even when relatively few challenge-response pairs are known in advance. Hence the need for both more resilient PUF designs and analysis of ML-attack susceptibility. Previous work has developed a PUF for photonic integrated circuits (PICs). A PIC PUF not only produces unpredictable responses given manufacturing-introduced tolerances, but is also less prone to electromagnetic radiation eavesdropping attacks than a purely electronic IC PUF. In this work, we analyze the resilience of the proposed photonic PUF when subjected to ML-based attacks. Specifically, we describe a computational PUF model for producing the large datasets required for training ML attacks; we analyze the quality of the model; and we discuss the modeled PUF's susceptibility to ML-based attacks. We find that the modeled PUF generates distributions that resemble uniform white noise, explaining the exhibited resilience to neural-network-based attacks designed to exploit latent relationships between challenges and responses. Preliminary analysis suggests that the PUF exhibits similar resilience to generative adversarial networks, and continued development will show whether more-sophisticated ML approaches better compromise the PUF and -- if so -- how design modifications might improve resilience.","sentences":["Physically unclonable functions (PUFs) are designed to act as device 'fingerprints.'","Given an input challenge, the PUF circuit should produce an unpredictable response for use in situations such as root-of-trust applications and other hardware-level cybersecurity applications.","PUFs are typically subcircuits present within integrated circuits (ICs), and while conventional IC PUFs are well-understood, several implementations have proven vulnerable to malicious exploits, including those perpetrated by machine learning (ML)-based attacks.","Such attacks can be difficult to prevent because they are often designed to work even when relatively few challenge-response pairs are known in advance.","Hence the need for both more resilient PUF designs and analysis of ML-attack susceptibility.","Previous work has developed a PUF for photonic integrated circuits (PICs).","A PIC PUF not only produces unpredictable responses given manufacturing-introduced tolerances, but is also less prone to electromagnetic radiation eavesdropping attacks than a purely electronic IC PUF.","In this work, we analyze the resilience of the proposed photonic PUF when subjected to ML-based attacks.","Specifically, we describe a computational PUF model for producing the large datasets required for training ML attacks; we analyze the quality of the model; and we discuss the modeled PUF's susceptibility to ML-based attacks.","We find that the modeled PUF generates distributions that resemble uniform white noise, explaining the exhibited resilience to neural-network-based attacks designed to exploit latent relationships between challenges and responses.","Preliminary analysis suggests that the PUF exhibits similar resilience to generative adversarial networks, and continued development will show whether more-sophisticated ML approaches better compromise the PUF and -- if so -- how design modifications might improve resilience."],"url":"http://arxiv.org/abs/2404.02440v1","category":"cs.CR"}
{"created":"2024-04-03 01:59:57","title":"Net proton number cumulants from viscous hydro with equation of state including a critical end point","abstract":"In the SMASH-CLVisc-hybrid framework, including SMASH for the initial conditions and the hadronic rescattering stage, and CLVisc for the quark gluon plasma (QGP) evolution, we investigate net baryon number fluctuations via considering the equation of state (EoS) with and without a critical end point (CEP) in the QCD phase transition. Specifically, two distinct QCD EoS are utilized: one with smooth crossover derived from NEOS and another with a critical end point sourced from the rPNJL model. Our results show that non-monotonic behavior of $\\kappa\\sigma^2$ is not observed above the collision energy $7.7 {\\rm GeV}$, nor are there explicit differences between the EoS characterized by crossover and that by CEP. This could be attributed to the significant deviation of the freeze-out line from the location of CEP. It is also found that the pure SMASH result of $\\kappa \\sigma^2$ is positive and close to zero at 3 GeV, which is different from the negative value observed from STAR.","sentences":["In the SMASH-CLVisc-hybrid framework, including SMASH for the initial conditions and the hadronic rescattering stage, and CLVisc for the quark gluon plasma (QGP) evolution, we investigate net baryon number fluctuations via considering the equation of state (EoS) with and without a critical end point (CEP) in the QCD phase transition.","Specifically, two distinct QCD EoS are utilized: one with smooth crossover derived from NEOS and another with a critical end point sourced from the rPNJL model.","Our results show that non-monotonic behavior of $\\kappa\\sigma^2$ is not observed above the collision energy $7.7 {\\rm GeV}$, nor are there explicit differences between the EoS characterized by crossover and that by CEP.","This could be attributed to the significant deviation of the freeze-out line from the location of CEP.","It is also found that the pure SMASH result of $\\kappa \\sigma^2$ is positive and close to zero at 3 GeV, which is different from the negative value observed from STAR."],"url":"http://arxiv.org/abs/2404.02397v1","category":"hep-ph"}
{"created":"2024-04-03 01:02:06","title":"An inversion problem for optical spectrum data via physics-guided machine learning","abstract":"We propose the regularized recurrent inference machine (rRIM), a novel machine-learning approach to solve the challenging problem of deriving the pairing glue function from measured optical spectra. The rRIM incorporates physical principles into both training and inference and affords noise robustness, flexibility with out-of-distribution data, and reduced data requirements. It effectively obtains reliable pairing glue functions from experimental optical spectra and yields promising solutions for similar inverse problems of the Fredholm integral equation of the first kind.","sentences":["We propose the regularized recurrent inference machine (rRIM), a novel machine-learning approach to solve the challenging problem of deriving the pairing glue function from measured optical spectra.","The rRIM incorporates physical principles into both training and inference and affords noise robustness, flexibility with out-of-distribution data, and reduced data requirements.","It effectively obtains reliable pairing glue functions from experimental optical spectra and yields promising solutions for similar inverse problems of the Fredholm integral equation of the first kind."],"url":"http://arxiv.org/abs/2404.02387v1","category":"physics.data-an"}
{"created":"2024-04-03 01:00:45","title":"Exact solution to Maxwell's equations for the infinite ideal solenoid with a time-dependent surface current","abstract":"Very little previous literature has considered the *exact* solution to Maxwell's equations for an infinite ideal cylindrical solenoid with an arbitrary time-dependent azimuthal surface current $K(t) \\hat{\\bf \\phi}$. Most of the previous literature has focused on special cases and has approached the problem by calculating the magnetic vector potential ${\\bf A}$, which requires performing some very complicated surface integrals over the cylinder. In this article, we take a simpler approach and directly tackle Maxwell's equations without ever invoking a vector potential. The high symmetry of the geometry allows us to reduce Maxwell's equations to just two coupled partial differential equations for two functions of two real variables, which can be readily solved numerically. We find the general analytic solution to these PDEs and derive the Green's functions for the electromagnetic fields, which allow us to calculate the fields directly from the surface current $K(t)$. We also briefly discuss a family of exact formal solutions that (the author believes) has not appeared in the previous literature because it corresponds to a current $K(t)$ that does not have a Fourier transform.","sentences":["Very little previous literature has considered the *exact* solution to Maxwell's equations for an infinite ideal cylindrical solenoid with an arbitrary time-dependent azimuthal surface current $K(t) \\hat{\\bf \\phi}$. Most of the previous literature has focused on special cases and has approached the problem by calculating the magnetic vector potential ${\\bf A}$, which requires performing some very complicated surface integrals over the cylinder.","In this article, we take a simpler approach and directly tackle Maxwell's equations without ever invoking a vector potential.","The high symmetry of the geometry allows us to reduce Maxwell's equations to just two coupled partial differential equations for two functions of two real variables, which can be readily solved numerically.","We find the general analytic solution to these PDEs and derive the Green's functions for the electromagnetic fields, which allow us to calculate the fields directly from the surface current $K(t)$. We also briefly discuss a family of exact formal solutions that (the author believes) has not appeared in the previous literature because it corresponds to a current $K(t)$ that does not have a Fourier transform."],"url":"http://arxiv.org/abs/2404.02386v1","category":"physics.class-ph"}
{"created":"2024-04-02 22:54:16","title":"A remark on omega limit sets for non-expansive dynamics","abstract":"In this paper, we study systems of time-invariant ordinary differential equations whose flows are non-expansive with respect to a norm, meaning that the distance between solutions may not increase. Since non-expansiveness (and contractivity) are norm-dependent notions, the topology of $\\omega$-limit sets of solutions may depend on the norm. For example, and at least for systems defined by real-analytic vector fields, the only possible $\\omega$-limit sets of systems that are non-expansive with respect to polyhedral norms (such as $\\ell^p$ norms with $p =1$ or $p=\\infty$) are equilibria. In contrast, for non-expansive systems with respect to Euclidean ($\\ell^2$) norm, other limit sets may arise (such as multi-dimensional tori): for example linear harmonic oscillators are non-expansive (and even isometric) flows, yet have periodic orbits as $\\omega$-limit sets. This paper shows that the Euclidean linear case is what can be expected in general: for flows that are contractive with respect to any strictly convex norm (such as $\\ell^p$ for any $p\\not=1,\\infty$), and if there is at least one bounded solution, then the $\\omega$-limit set of every trajectory is also an omega limit set of a linear time-invariant system.","sentences":["In this paper, we study systems of time-invariant ordinary differential equations whose flows are non-expansive with respect to a norm, meaning that the distance between solutions may not increase.","Since non-expansiveness (and contractivity) are norm-dependent notions, the topology of $\\omega$-limit sets of solutions may depend on the norm.","For example, and at least for systems defined by real-analytic vector fields, the only possible $\\omega$-limit sets of systems that are non-expansive with respect to polyhedral norms (such as $\\ell^p$ norms with $p =1$ or $p=\\infty$) are equilibria.","In contrast, for non-expansive systems with respect to Euclidean ($\\ell^2$) norm, other limit sets may arise (such as multi-dimensional tori): for example linear harmonic oscillators are non-expansive (and even isometric) flows, yet have periodic orbits as $\\omega$-limit sets.","This paper shows that the Euclidean linear case is what can be expected in general: for flows that are contractive with respect to any strictly convex norm (such as $\\ell^p$ for any $p\\not=1,\\infty$), and if there is at least one bounded solution, then the $\\omega$-limit set of every trajectory is also an omega limit set of a linear time-invariant system."],"url":"http://arxiv.org/abs/2404.02352v1","category":"math.DS"}
{"created":"2024-04-02 22:49:16","title":"Correlation and Spectral Density Functions in Mode-Stirred Reverberation -- I. Theory","abstract":"Auto- and cross-spectral density functions for dynamic {random} fields and power are derived. These are based on first- and second-order Pad\\'{e} approximants of correlation functions expanded in terms of spectral moments. The second-order approximant permits a characterization of stir noise observable {at high stir frequencies in the autospectral density}. A relationship between stir imperfection and spectral kurtosis is established. For the latter, lower bounds are established. A novel alternative measure of correlation time for mean-square differentiable fields is introduced as the lag at the first point of inflection in the autocorrelation function. A hierarchy of Pad\\'{e} deviation coefficients is constructed that quantify imperfections of correlations and spectra with increasing accuracy and range of lags. Analytical models of the spectral densities are derived and their asymptotic behaviour is analyzed. The theoretical spectral density for the electric field as an input quantity is compared with that for power as the measurand. For the latter, its inverted-S shape conforms to experimentally observed stir-spectral power densities. The effect of additive noise on the stir autocorrelation and spectral density functions is quantified.","sentences":["Auto- and cross-spectral density functions for dynamic {random} fields and power are derived.","These are based on first- and second-order Pad\\'{e} approximants of correlation functions expanded in terms of spectral moments.","The second-order approximant permits a characterization of stir noise observable {at high stir frequencies in the autospectral density}.","A relationship between stir imperfection and spectral kurtosis is established.","For the latter, lower bounds are established.","A novel alternative measure of correlation time for mean-square differentiable fields is introduced as the lag at the first point of inflection in the autocorrelation function.","A hierarchy of Pad\\'{e} deviation coefficients is constructed that quantify imperfections of correlations and spectra with increasing accuracy and range of lags.","Analytical models of the spectral densities are derived and their asymptotic behaviour is analyzed.","The theoretical spectral density for the electric field as an input quantity is compared with that for power as the measurand.","For the latter, its inverted-S shape conforms to experimentally observed stir-spectral power densities.","The effect of additive noise on the stir autocorrelation and spectral density functions is quantified."],"url":"http://arxiv.org/abs/2404.02347v1","category":"physics.class-ph"}
{"created":"2024-04-02 22:42:33","title":"A lattice Boltzmann approach for acoustic manipulation","abstract":"We employ a lattice Boltzmann method to compute the acoustic radiation force produced by standing waves on a compressible object. Instead of simulating the fluid mechanics equations directly, the proposed method uses a lattice Boltzmann model that reproduces the wave equation, together with a kernel interpolation scheme, to compute the first order perturbations of the pressure and velocity fields on the object's surface and, from them, the acoustic radiation force. The procedure reproduces with excellent accuracy the theoretical expressions by Gor'kov and Wei for the sphere and the disk, respectively, even with a modest number of lattice Boltzmann cells. The proposed method shows to be a promising tool for simulating phenomena where the acoustic radiation force plays a relevant role, like acoustic tweezers or the acoustic manipulation of microswimmers, with applications in medicine and engineering.","sentences":["We employ a lattice Boltzmann method to compute the acoustic radiation force produced by standing waves on a compressible object.","Instead of simulating the fluid mechanics equations directly, the proposed method uses a lattice Boltzmann model that reproduces the wave equation, together with a kernel interpolation scheme, to compute the first order perturbations of the pressure and velocity fields on the object's surface and, from them, the acoustic radiation force.","The procedure reproduces with excellent accuracy the theoretical expressions by Gor'kov and Wei for the sphere and the disk, respectively, even with a modest number of lattice Boltzmann cells.","The proposed method shows to be a promising tool for simulating phenomena where the acoustic radiation force plays a relevant role, like acoustic tweezers or the acoustic manipulation of microswimmers, with applications in medicine and engineering."],"url":"http://arxiv.org/abs/2404.02346v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 22:31:38","title":"A Computational Analysis of Lyric Similarity Perception","abstract":"In musical compositions that include vocals, lyrics significantly contribute to artistic expression. Consequently, previous studies have introduced the concept of a recommendation system that suggests lyrics similar to a user's favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks. However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area. To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception. Results indicated that computational models based on similarities between embeddings from pre-trained BERT-based models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity. This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception about lyric similarity. We anticipate that our findings will enhance the development of similarity-based lyric recommendation systems by offering pseudo-labels for neural network development and introducing objective evaluation metrics.","sentences":["In musical compositions that include vocals, lyrics significantly contribute to artistic expression.","Consequently, previous studies have introduced the concept of a recommendation system that suggests lyrics similar to a user's favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks.","However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area.","To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception.","Results indicated that computational models based on similarities between embeddings from pre-trained BERT-based models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity.","This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception about lyric similarity.","We anticipate that our findings will enhance the development of similarity-based lyric recommendation systems by offering pseudo-labels for neural network development and introducing objective evaluation metrics."],"url":"http://arxiv.org/abs/2404.02342v1","category":"cs.CL"}
{"created":"2024-04-02 22:12:45","title":"No top-heavy stellar initial mass function needed: the ionizing radiation of GS9422 can be powered by a mixture of AGN and stars","abstract":"JWST is producing high-quality rest-frame optical and UV spectra of faint galaxies at $z>4$ for the first time, challenging models of galaxy and stellar populations. One galaxy recently observed at $z=5.943$, GS9422, has nebular line and UV continuum emission that appears to require a high ionizing photon production efficiency. This has been explained with an exotic stellar initial mass function (IMF), 10-30x more top-heavy than a Salpeter IMF (Cameron et al. 2023). Here we suggest an alternate explanation to this exotic IMF. We use a new flexible neural net emulator for CLOUDY, Cue, to infer the shape of the ionizing spectrum directly from the observed emission line fluxes. By describing the ionizing spectrum with a piece-wise power-law, Cue is agnostic to the source of the ionizing photons. Cue finds that the ionizing radiation from GS9422 can be approximated by a double power law characterized by $\\frac{Q_\\mathrm{HeII}}{Q_\\mathrm{H}} = -1.5$, which can be interpreted as a combination of young, metal-poor stars and a low-luminosity active galactic nucleus (AGN) with $F_{\\nu} \\propto \\lambda ^ {2}$ in a 65%/35% ratio. This suggests a significantly lower nebular continuum contribution to the observed UV flux (24%) than a top-heavy IMF ($\\gtrsim80$%), and hence, necessitates a damped Lyman-$\\alpha$ absorber (DLA) to explain the continuum turnover bluewards of $\\sim1400$ Angstrom. While current data cannot rule out either scenario, given the immense impact the proposed top-heavy IMF would have on models of galaxy formation, it is important to propose viable alternative explanations and to further investigate the nature of peculiar high-z nebular emitters.","sentences":["JWST is producing high-quality rest-frame optical and UV spectra of faint galaxies at $z>4$ for the first time, challenging models of galaxy and stellar populations.","One galaxy recently observed at $z=5.943$, GS9422, has nebular line and UV continuum emission that appears to require a high ionizing photon production efficiency.","This has been explained with an exotic stellar initial mass function (IMF), 10-30x more top-heavy than a Salpeter IMF (Cameron et al. 2023).","Here we suggest an alternate explanation to this exotic IMF.","We use a new flexible neural net emulator for CLOUDY, Cue, to infer the shape of the ionizing spectrum directly from the observed emission line fluxes.","By describing the ionizing spectrum with a piece-wise power-law, Cue is agnostic to the source of the ionizing photons.","Cue finds that the ionizing radiation from GS9422 can be approximated by a double power law characterized by $\\frac{Q_\\mathrm{HeII}}{Q_\\mathrm{H}} = -1.5$, which can be interpreted as a combination of young, metal-poor stars and a low-luminosity active galactic nucleus (AGN) with $F_{\\nu} \\propto \\lambda ^","{2}$ in a 65%/35% ratio.","This suggests a significantly lower nebular continuum contribution to the observed UV flux (24%) than a top-heavy IMF ($\\gtrsim80$%), and hence, necessitates a damped Lyman-$\\alpha$ absorber (DLA) to explain the continuum turnover bluewards of $\\sim1400$ Angstrom.","While current data cannot rule out either scenario, given the immense impact the proposed top-heavy IMF would have on models of galaxy formation, it is important to propose viable alternative explanations and to further investigate the nature of peculiar high-z nebular emitters."],"url":"http://arxiv.org/abs/2404.02333v1","category":"astro-ph.GA"}
{"created":"2024-04-02 22:10:08","title":"Unmasking Correlations in Nuclear Cross Sections with Graph Neural Networks","abstract":"In this work, we explore the use of deep learning techniques to learn the relationships between nuclear cross-sections across the chart of isotopes. As a proof of principle, we focus on the neutron-induced reactions in the fast energy regime that are the most important in nuclear science and engineering. We use variational autoencoders (VAEs) and implicit neural representations (INRs) to build a learned feature representation space of nuclear cross sections and reduce the dimensionality of the problem. We then train graph neural networks (GNNs) on the resulting latent space to leverage the topological information encoded in the chart of isotopes and to capture the relationships between cross sections in different nuclei. We find that hypernetworks based on INRs significantly outperforms VAEs in encoding nuclear cross-sections. This superiority is attributed to INR's ability to model complex, varying frequency details, which enables lower prediction errors when combined with GNNs. We also observe that GNN optimization is much more successful when performed in the latent space, whether using INRs or VAEs. However VAEs' continuous representation also allows for direct GNN training in the original input space. We leverage these representational learning techniques and successfully predict cross sections for a 17x17 block of nuclei with high accuracy and precision. These findings suggest that both representation encoding of cross-sections and the prediction task hold significant potential in augmenting nuclear theory models, e.g., providing reliable estimates of covariances of cross sections, including cross-material covariances.","sentences":["In this work, we explore the use of deep learning techniques to learn the relationships between nuclear cross-sections across the chart of isotopes.","As a proof of principle, we focus on the neutron-induced reactions in the fast energy regime that are the most important in nuclear science and engineering.","We use variational autoencoders (VAEs) and implicit neural representations (INRs) to build a learned feature representation space of nuclear cross sections and reduce the dimensionality of the problem.","We then train graph neural networks (GNNs) on the resulting latent space to leverage the topological information encoded in the chart of isotopes and to capture the relationships between cross sections in different nuclei.","We find that hypernetworks based on INRs significantly outperforms VAEs in encoding nuclear cross-sections.","This superiority is attributed to INR's ability to model complex, varying frequency details, which enables lower prediction errors when combined with GNNs.","We also observe that GNN optimization is much more successful when performed in the latent space, whether using INRs or VAEs.","However VAEs' continuous representation also allows for direct GNN training in the original input space.","We leverage these representational learning techniques and successfully predict cross sections for a 17x17 block of nuclei with high accuracy and precision.","These findings suggest that both representation encoding of cross-sections and the prediction task hold significant potential in augmenting nuclear theory models, e.g., providing reliable estimates of covariances of cross sections, including cross-material covariances."],"url":"http://arxiv.org/abs/2404.02332v1","category":"nucl-th"}
{"created":"2024-04-02 21:53:43","title":"Robust Constrained Consensus and Inequality-constrained Distributed Optimization with Guaranteed Differential Privacy and Accurate Convergence","abstract":"We address differential privacy for fully distributed optimization subject to a shared inequality constraint. By co-designing the distributed optimization mechanism and the differential-privacy noise injection mechanism, we propose the first distributed constrained optimization algorithm that can ensure both provable convergence to a global optimal solution and rigorous $\\epsilon$-differential privacy, even when the number of iterations tends to infinity. Our approach does not require the Lagrangian function to be strictly convex/concave, and allows the global objective function to be non-separable. As a byproduct of the co-design, we also propose a new constrained consensus algorithm that can achieve rigorous $\\epsilon$-differential privacy while maintaining accurate convergence, which, to our knowledge, has not been achieved before. Numerical simulation results on a demand response control problem in smart grid confirm the effectiveness of the proposed approach.","sentences":["We address differential privacy for fully distributed optimization subject to a shared inequality constraint.","By co-designing the distributed optimization mechanism and the differential-privacy noise injection mechanism, we propose the first distributed constrained optimization algorithm that can ensure both provable convergence to a global optimal solution and rigorous $\\epsilon$-differential privacy, even when the number of iterations tends to infinity.","Our approach does not require the Lagrangian function to be strictly convex/concave, and allows the global objective function to be non-separable.","As a byproduct of the co-design, we also propose a new constrained consensus algorithm that can achieve rigorous $\\epsilon$-differential privacy while maintaining accurate convergence, which, to our knowledge, has not been achieved before.","Numerical simulation results on a demand response control problem in smart grid confirm the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2404.02327v1","category":"math.OC"}
{"created":"2024-04-02 21:42:25","title":"On Properties of Adjoint Systems for Evolutionary PDEs","abstract":"We investigate the geometric structure of adjoint systems associated with evolutionary partial differential equations at the fully continuous, semi-discrete, and fully discrete levels and the relations between these levels. We show that the adjoint system associated with an evolutionary partial differential equation has an infinite-dimensional Hamiltonian structure, which is useful for connecting the fully continuous, semi-discrete, and fully discrete levels. We subsequently address the question of discretize-then-optimize versus optimize-then-discrete for both semi-discretization and time integration, by characterizing the commutativity of discretize-then-optimize methods versus optimize-then-discretize methods uniquely in terms of an adjoint-variational quadratic conservation law. For Galerkin semi-discretizations and one-step time integration methods in particular, we explicitly construct these commuting methods by using structure-preserving discretization techniques.","sentences":["We investigate the geometric structure of adjoint systems associated with evolutionary partial differential equations at the fully continuous, semi-discrete, and fully discrete levels and the relations between these levels.","We show that the adjoint system associated with an evolutionary partial differential equation has an infinite-dimensional Hamiltonian structure, which is useful for connecting the fully continuous, semi-discrete, and fully discrete levels.","We subsequently address the question of discretize-then-optimize versus optimize-then-discrete for both semi-discretization and time integration, by characterizing the commutativity of discretize-then-optimize methods versus optimize-then-discretize methods uniquely in terms of an adjoint-variational quadratic conservation law.","For Galerkin semi-discretizations and one-step time integration methods in particular, we explicitly construct these commuting methods by using structure-preserving discretization techniques."],"url":"http://arxiv.org/abs/2404.02320v1","category":"math.OC"}
{"created":"2024-04-02 21:05:12","title":"Derivation and analysis of a nonlocal Hele-Shaw-Cahn-Hilliard system for flow in thin heterogeneous layers","abstract":"We derive, through the deterministic homogenization theory in thin domains, a new model consisting of Hele-Shaw equation with memory coupled with the convective Cahn-Hilliard equation. The obtained system, which models in particular tumor growth, is then analyzed and we prove its well-posedness in dimension 2. To achieve our goal, we develop and use the new concept of sigma-convergence in thin heterogeneous media, and we prove some regularity results for the upscaled model.","sentences":["We derive, through the deterministic homogenization theory in thin domains, a new model consisting of Hele-Shaw equation with memory coupled with the convective Cahn-Hilliard equation.","The obtained system, which models in particular tumor growth, is then analyzed and we prove its well-posedness in dimension 2.","To achieve our goal, we develop and use the new concept of sigma-convergence in thin heterogeneous media, and we prove some regularity results for the upscaled model."],"url":"http://arxiv.org/abs/2404.02306v1","category":"math.AP"}
{"created":"2024-04-02 20:58:49","title":"Curvature homogeneous hypersurfaces in space forms","abstract":"We classify curvature homogeneous hypersurfaces in S^4 and H^4. In higher dimesnsion one only has the FKM examples and an isolate one by Tsukada of a hypersurface in H^5.   Besides some simple examples, we show that there exists an isolated hypersurface with a circle of symmetries and and a one parameter family admitting no continuous symmetries. Outside the set of minimal points, which only exists in the case of S^4, every example is locally and up to covers of this form.","sentences":["We classify curvature homogeneous hypersurfaces in S^4 and H^4.","In higher dimesnsion one only has the FKM examples and an isolate one by Tsukada of a hypersurface in H^5.   ","Besides some simple examples, we show that there exists an isolated hypersurface with a circle of symmetries and and a one parameter family admitting no continuous symmetries.","Outside the set of minimal points, which only exists in the case of S^4, every example is locally and up to covers of this form."],"url":"http://arxiv.org/abs/2404.02302v1","category":"math.DG"}
{"created":"2024-04-02 20:12:41","title":"Neural network reconstruction of density and velocity fields from the 2MASS Redshift Survey","abstract":"We reconstruct the 3D matter density and peculiar velocity fields in the local Universe up to a distance of $200\\,h^{-1}\\,\\mathrm{Mpc}$ from the Two-Micron All-Sky Redshift Survey (2MRS), using a neural network (NN). We employ a NN with U-net autoencoder architecture and a weighted mean squared error loss function, trained separately to output either the density or velocity field for a given input grid of galaxy number counts. The NN is trained on mocks derived from the Quijote N-body simulations, incorporating redshift-space distortions (RSD), galaxy bias and selection effects, closely mimicking the characteristics of 2MRS. The trained NN is benchmarked against a standard Wiener filter (WF) on a validation set of mocks, before applying it to 2MRS. The NN reconstructions effectively approximate the mean posterior estimate of the true density and velocity fields conditioned on the observations. They consistently outperform the WF in terms of reconstruction accuracy, and effectively capture the nonlinear relation between velocity and density. The NN-reconstructed bulk flow of the total survey volume exhibits a significant correlation with the true mock bulk flow, demonstrating that the NN is sensitive to information on super-survey scales encoded in the RSD. When applied to 2MRS, the NN successfully recovers the main known clusters, some of which are partially in the Zone of Avoidance. The reconstructed bulk flows in spheres of different radii less than $100\\,h^{-1}\\,\\mathrm{Mpc}$ are in good agreement with a previous 2MRS analysis that required an additional external bulk flow component inferred from directly observed peculiar velocities. The NN-reconstructed peculiar velocity of the Local Group closely matches the observed CMB dipole in amplitude and Galactic latitude, and only deviates by $18^\\circ$ in longitude. The NN-reconstructed fields are publicly available.","sentences":["We reconstruct the 3D matter density and peculiar velocity fields in the local Universe up to a distance of $200\\,h^{-1}\\,\\mathrm{Mpc}$ from the Two-Micron All-Sky Redshift Survey (2MRS), using a neural network (NN).","We employ a NN with U-net autoencoder architecture and a weighted mean squared error loss function, trained separately to output either the density or velocity field for a given input grid of galaxy number counts.","The NN is trained on mocks derived from the Quijote N-body simulations, incorporating redshift-space distortions (RSD), galaxy bias and selection effects, closely mimicking the characteristics of 2MRS.","The trained NN is benchmarked against a standard Wiener filter (WF) on a validation set of mocks, before applying it to 2MRS.","The NN reconstructions effectively approximate the mean posterior estimate of the true density and velocity fields conditioned on the observations.","They consistently outperform the WF in terms of reconstruction accuracy, and effectively capture the nonlinear relation between velocity and density.","The NN-reconstructed bulk flow of the total survey volume exhibits a significant correlation with the true mock bulk flow, demonstrating that the NN is sensitive to information on super-survey scales encoded in the RSD.","When applied to 2MRS, the NN successfully recovers the main known clusters, some of which are partially in the Zone of Avoidance.","The reconstructed bulk flows in spheres of different radii less than $100\\,h^{-1}\\,\\mathrm{Mpc}$ are in good agreement with a previous 2MRS analysis that required an additional external bulk flow component inferred from directly observed peculiar velocities.","The NN-reconstructed peculiar velocity of the Local Group closely matches the observed CMB dipole in amplitude and Galactic latitude, and only deviates by $18^\\circ$ in longitude.","The NN-reconstructed fields are publicly available."],"url":"http://arxiv.org/abs/2404.02278v1","category":"astro-ph.CO"}
{"created":"2024-04-02 19:34:19","title":"On diffusion and transport acting on parameterized moving closed curves in space","abstract":"We investigate the motion of closed, smooth non-self-intersecting curves that evolve in space $\\mathbb{R}^3$. The geometric evolutionary equation for the evolution of the curve is accompanied by a parabolic equation for the scalar quantity evaluated over the evolving curve. We apply the direct Lagrangian approach to describe the geometric flow of 3D curves resulting in a system of degenerate parabolic equations. We prove the local existence and uniqueness of classical H\\\"older smooth solutions to the governing system of nonlinear parabolic equations. A numerical discretization scheme has been constructed using the method of flowing finite volumes. We present several numerical examples of the evolution of curves in 3D with a scalar quantity. In this paper, we analyze the flow of curves with no torsion evolving in rotating and parallel planes. Next, we present examples of the evolution of curves with initially knotted and unknotted curves.","sentences":["We investigate the motion of closed, smooth non-self-intersecting curves that evolve in space $\\mathbb{R}^3$. The geometric evolutionary equation for the evolution of the curve is accompanied by a parabolic equation for the scalar quantity evaluated over the evolving curve.","We apply the direct Lagrangian approach to describe the geometric flow of 3D curves resulting in a system of degenerate parabolic equations.","We prove the local existence and uniqueness of classical H\\\"older smooth solutions to the governing system of nonlinear parabolic equations.","A numerical discretization scheme has been constructed using the method of flowing finite volumes.","We present several numerical examples of the evolution of curves in 3D with a scalar quantity.","In this paper, we analyze the flow of curves with no torsion evolving in rotating and parallel planes.","Next, we present examples of the evolution of curves with initially knotted and unknotted curves."],"url":"http://arxiv.org/abs/2404.02260v1","category":"math.AP"}
{"created":"2024-04-02 19:24:29","title":"Machine learning-based vorticity evolution and superresolution of homogeneous isotropic turbulence using wavelet projection","abstract":"A wavelet-based machine learning method is proposed for predicting the time evolution of homogeneous isotropic turbulence where vortex tubes are preserved. Three-dimensional convolutional neural networks and long short-term memory are trained with a time series of direct numerical simulation (DNS) data of homogeneous isotropic turbulence at the Taylor microscale Reynolds number 92. The predicted results are assessed by using flow visualization of vorticity and statistics, e.g., probability density functions of vorticity and enstrophy spectra. It is found that the predicted results are in good agreement with DNS results. The small-scale flow topology considering the second and third invariant of the velocity gradient tensor likewise shows an approximate match. Furthermore, we apply the pre-trained neural networks to coarse-grained vorticity data using superresolution. It is shown that the superresolved flow field well agrees with the reference DNS field and thus small-scale information and vortex tubes are well regenerated.","sentences":["A wavelet-based machine learning method is proposed for predicting the time evolution of homogeneous isotropic turbulence where vortex tubes are preserved.","Three-dimensional convolutional neural networks and long short-term memory are trained with a time series of direct numerical simulation (DNS) data of homogeneous isotropic turbulence at the Taylor microscale Reynolds number 92.","The predicted results are assessed by using flow visualization of vorticity and statistics, e.g., probability density functions of vorticity and enstrophy spectra.","It is found that the predicted results are in good agreement with DNS results.","The small-scale flow topology considering the second and third invariant of the velocity gradient tensor likewise shows an approximate match.","Furthermore, we apply the pre-trained neural networks to coarse-grained vorticity data using superresolution.","It is shown that the superresolved flow field well agrees with the reference DNS field and thus small-scale information and vortex tubes are well regenerated."],"url":"http://arxiv.org/abs/2404.02256v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 19:08:54","title":"A Fully-Configurable Open-Source Software-Defined Digital Quantized Spiking Neural Core Architecture","abstract":"We introduce QUANTISENC, a fully configurable open-source software-defined digital quantized spiking neural core architecture to advance research in neuromorphic computing. QUANTISENC is designed hierarchically using a bottom-up methodology with multiple neurons in each layer and multiple layers in each core. The number of layers and neurons per layer can be configured via software in a top-down methodology to generate the hardware for a target spiking neural network (SNN) model. QUANTISENC uses leaky integrate and fire neurons (LIF) and current-based excitatory and inhibitory synapses (CUBA). The nonlinear dynamics of a neuron can be configured at run-time via programming its internal control registers. Each neuron performs signed fixed-point arithmetic with user-defined quantization and decimal precision. QUANTISENC supports all-to-all, one-to-one, and Gaussian connections between layers. Its hardware-software interface is integrated with a PyTorch-based SNN simulator. This integration allows to define and train an SNN model in PyTorch and evaluate the hardware performance (e.g., area, power, latency, and throughput) through FPGA prototyping and ASIC design. The hardware-software interface also takes advantage of the layer-based architecture and distributed memory organization of QUANTISENC to enable pipelining by overlapping computations on streaming data. Overall, the proposed software-defined hardware design methodology offers flexibility similar to that of high-level synthesis (HLS), but provides better hardware performance with zero hardware development effort. We evaluate QUANTISENC using three spiking datasets and show its superior performance against state-of the-art designs.","sentences":["We introduce QUANTISENC, a fully configurable open-source software-defined digital quantized spiking neural core architecture to advance research in neuromorphic computing.","QUANTISENC is designed hierarchically using a bottom-up methodology with multiple neurons in each layer and multiple layers in each core.","The number of layers and neurons per layer can be configured via software in a top-down methodology to generate the hardware for a target spiking neural network (SNN) model.","QUANTISENC uses leaky integrate and fire neurons (LIF) and current-based excitatory and inhibitory synapses (CUBA).","The nonlinear dynamics of a neuron can be configured at run-time via programming its internal control registers.","Each neuron performs signed fixed-point arithmetic with user-defined quantization and decimal precision.","QUANTISENC supports all-to-all, one-to-one, and Gaussian connections between layers.","Its hardware-software interface is integrated with a PyTorch-based SNN simulator.","This integration allows to define and train an SNN model in PyTorch and evaluate the hardware performance (e.g., area, power, latency, and throughput) through FPGA prototyping and ASIC design.","The hardware-software interface also takes advantage of the layer-based architecture and distributed memory organization of QUANTISENC to enable pipelining by overlapping computations on streaming data.","Overall, the proposed software-defined hardware design methodology offers flexibility similar to that of high-level synthesis (HLS), but provides better hardware performance with zero hardware development effort.","We evaluate QUANTISENC using three spiking datasets and show its superior performance against state-of the-art designs."],"url":"http://arxiv.org/abs/2404.02248v1","category":"cs.AR"}
{"created":"2024-04-02 18:27:22","title":"Coupling of Branes and Twisted Self-Duality in the Maxwell-Chern-Simons Theory","abstract":"We study three approaches to electric-magnetic duality in the 4-dimensional Maxwell theory coupled to a dyonic point charge and in the 5-dimensional Maxwell-Chern-Simons (MCS) theory coupled to an electric point charge and a magnetic string charge. The three approaches have been developed by Dirac, Bunster and Henneaux, and Pasti, Sorokin and Tonin (PST). In Dirac's formulation, the electric magnetic duality is realized only on the level of the equations of motion. The other two formulations introduce a dual (magnetic) gauge potential to induce manifest twisted self-duality in the action. In particular, we study the relations connecting the three approaches. The main results of this paper are the Bunster-Henneaux and PST formulations of the MCS theory with sources. We compare our result to the PST formulation of 11-dimensional supergravity coupled to the M2- and M5-brane by Bandos, Berkovits, and Sorokin.","sentences":["We study three approaches to electric-magnetic duality in the 4-dimensional Maxwell theory coupled to a dyonic point charge and in the 5-dimensional Maxwell-Chern-Simons (MCS) theory coupled to an electric point charge and a magnetic string charge.","The three approaches have been developed by Dirac, Bunster and Henneaux, and Pasti, Sorokin and Tonin (PST).","In Dirac's formulation, the electric magnetic duality is realized only on the level of the equations of motion.","The other two formulations introduce a dual (magnetic) gauge potential to induce manifest twisted self-duality in the action.","In particular, we study the relations connecting the three approaches.","The main results of this paper are the Bunster-Henneaux and PST formulations of the MCS theory with sources.","We compare our result to the PST formulation of 11-dimensional supergravity coupled to the M2- and M5-brane by Bandos, Berkovits, and Sorokin."],"url":"http://arxiv.org/abs/2404.02226v1","category":"hep-th"}
{"created":"2024-04-02 18:13:41","title":"Late time tail of waves on dynamic asymptotically flat spacetimes of odd space dimensions","abstract":"We introduce a general method for understanding the late time tail for solutions to wave equations on asymptotically flat spacetimes with odd space dimensions. In particular, for a large class of equations, we prove that the precise late time tail is determined by the limits of higher radiation field at future null infinity.   In the setting of stationary linear equations, we recover and generalize the Price law decay rates. In particular, in addition to reproving known results on $(3+1)$-dimensional black holes, this allows one to obtain the sharp decay rate for the wave equation on higher dimensional black hole spacetimes, which exhibits an anomalous rate due to subtle cancellations. More interesting, our method goes beyond the stationary linear case and applies to both equations on dynamical background and nonlinear equations. In this case, our results can be used to show that in general there is a correction to the Price law rates.","sentences":["We introduce a general method for understanding the late time tail for solutions to wave equations on asymptotically flat spacetimes with odd space dimensions.","In particular, for a large class of equations, we prove that the precise late time tail is determined by the limits of higher radiation field at future null infinity.   ","In the setting of stationary linear equations, we recover and generalize the Price law decay rates.","In particular, in addition to reproving known results on $(3+1)$-dimensional black holes, this allows one to obtain the sharp decay rate for the wave equation on higher dimensional black hole spacetimes, which exhibits an anomalous rate due to subtle cancellations.","More interesting, our method goes beyond the stationary linear case and applies to both equations on dynamical background and nonlinear equations.","In this case, our results can be used to show that in general there is a correction to the Price law rates."],"url":"http://arxiv.org/abs/2404.02220v1","category":"gr-qc"}
{"created":"2024-04-02 18:07:52","title":"An $\\infty$-Laplacian for differential forms, and calibrated laminations","abstract":"Motivated by Thurston and Daskalopoulos--Uhlenbeck's approach to Teichm\\\"uller theory, we study the behavior of $q$-harmonic functions and their $p$-harmonic conjugates in the limit as $q \\to 1$, where $1/p + 1/q = 1$. The $1$-Laplacian is already known to give rise to laminations by minimal hypersurfaces; we show that the limiting $p$-harmonic conjugates converge to calibrations $F$ of the laminations. Moreover, we show that the laminations which are calibrated by $F$ are exactly those which arise from the $1$-Laplacian. We also explore the limiting dual problem as a model problem for the optimal Lipschitz extension problem, which exhibits behavior rather unlike the scalar $\\infty$-Laplacian. In a companion work, we will apply the main result of this paper to associate to each class in $H^{d - 1}$ a lamination in a canonical way, and study the duality of the stable norm on $H_{d - 1}$.","sentences":["Motivated by Thurston and Daskalopoulos--Uhlenbeck's approach to Teichm\\\"uller theory, we study the behavior of $q$-harmonic functions and their $p$-harmonic conjugates in the limit as $q \\to 1$, where $1/p + 1/q = 1$.","The $1$-Laplacian is already known to give rise to laminations by minimal hypersurfaces; we show that the limiting $p$-harmonic conjugates converge to calibrations $F$ of the laminations.","Moreover, we show that the laminations which are calibrated by $F$ are exactly those which arise from the $1$-Laplacian.","We also explore the limiting dual problem as a model problem for the optimal Lipschitz extension problem, which exhibits behavior rather unlike the scalar $\\infty$-Laplacian.","In a companion work, we will apply the main result of this paper to associate to each class in $H^{d - 1}$ a lamination in a canonical way, and study the duality of the stable norm on $H_{d - 1}$."],"url":"http://arxiv.org/abs/2404.02215v1","category":"math.AP"}
{"created":"2024-04-02 18:00:02","title":"Normal weak eigenstate thermalization","abstract":"Eigenstate thermalization has been shown to occur for few-body observables in a wide range of nonintegrable interacting models. For intensive observables that are sums of local operators, because of their polynomially vanishing Hilbert-Schmidt norm, weak eigenstate thermalization occurs in quadratic and integrable interacting systems. Here, we unveil a novel weak eigenstate thermalization phenomenon that occurs in quadratic models whose single-particle sector exhibits quantum chaos (quantum-chaotic quadratic models) and in integrable interacting models. In such models, we show that there are few-body observables with a nonvanishing Hilbert-Schmidt norm that are guarrantied to exhibit a polynomially vanishing variance of the diagonal matrix elements, a phenomenon we dub normal weak eigenstate thermalization. For quantum-chaotic quadratic Hamiltonians, we prove that normal weak eigenstate thermalization is a consequence of single-particle eigenstate thermalization, i.e., it can be viewed as a manifestation of quantum chaos at the single-particle level. We report numerical evidence of normal weak eigenstate thermalization for quantum-chaotic quadratic models such as the 3D Anderson model in the delocalized regime and the power-law random banded matrix model, as well as for the integrable interacting spin-$\\frac{1}{2}$ XYZ and XXZ models.","sentences":["Eigenstate thermalization has been shown to occur for few-body observables in a wide range of nonintegrable interacting models.","For intensive observables that are sums of local operators, because of their polynomially vanishing Hilbert-Schmidt norm, weak eigenstate thermalization occurs in quadratic and integrable interacting systems.","Here, we unveil a novel weak eigenstate thermalization phenomenon that occurs in quadratic models whose single-particle sector exhibits quantum chaos (quantum-chaotic quadratic models) and in integrable interacting models.","In such models, we show that there are few-body observables with a nonvanishing Hilbert-Schmidt norm that are guarrantied to exhibit a polynomially vanishing variance of the diagonal matrix elements, a phenomenon we dub normal weak eigenstate thermalization.","For quantum-chaotic quadratic Hamiltonians, we prove that normal weak eigenstate thermalization is a consequence of single-particle eigenstate thermalization, i.e., it can be viewed as a manifestation of quantum chaos at the single-particle level.","We report numerical evidence of normal weak eigenstate thermalization for quantum-chaotic quadratic models such as the 3D Anderson model in the delocalized regime and the power-law random banded matrix model, as well as for the integrable interacting spin-$\\frac{1}{2}$ XYZ and XXZ models."],"url":"http://arxiv.org/abs/2404.02199v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-02 18:00:01","title":"Radiation reaction in weakly magnetized black holes: can the tail term be ignored in the strong field regime?","abstract":"We study radiation from charged particles in circular motion around a Schwarzschild black hole immersed in an asymptotically uniform magnetic field. In curved space, the radiation reaction force is described by the DeWitt-Brehme equation, which includes a complicated, non-local tail term. We show that, contrary to some claims in the literature, this term cannot, in general, be neglected. We account for self-force effects directly by calculating the electromagnetic energy flux at infinity and on the horizon. The radiative field is obtained using black hole perturbation theory. We solve the relevant equations analytically, in the low-frequency and slow-motion approximation, as well as numerically in the general case. Our results show that great care must be taken when neglecting the tail term, which is often fundamental to capture the dynamics of the particle: in fact, it only seems to be negligible when the magnetic force greatly dominates the gravitational force, so that the motion is well described by the Abraham--Lorentz--Dirac equation. We also report a curious \"horizon dominance effect\" that occurs for a radiating particle in a circular orbit around a black hole (emitting either scalar, electromagnetic or gravitational waves): for fixed orbital radius, the fraction of energy that is absorbed by the black hole can be made arbitrarily large by decreasing the particle velocity.","sentences":["We study radiation from charged particles in circular motion around a Schwarzschild black hole immersed in an asymptotically uniform magnetic field.","In curved space, the radiation reaction force is described by the DeWitt-Brehme equation, which includes a complicated, non-local tail term.","We show that, contrary to some claims in the literature, this term cannot, in general, be neglected.","We account for self-force effects directly by calculating the electromagnetic energy flux at infinity and on the horizon.","The radiative field is obtained using black hole perturbation theory.","We solve the relevant equations analytically, in the low-frequency and slow-motion approximation, as well as numerically in the general case.","Our results show that great care must be taken when neglecting the tail term, which is often fundamental to capture the dynamics of the particle: in fact, it only seems to be negligible when the magnetic force greatly dominates the gravitational force, so that the motion is well described by the Abraham--Lorentz--Dirac equation.","We also report a curious \"horizon dominance effect\" that occurs for a radiating particle in a circular orbit around a black hole (emitting either scalar, electromagnetic or gravitational waves): for fixed orbital radius, the fraction of energy that is absorbed by the black hole can be made arbitrarily large by decreasing the particle velocity."],"url":"http://arxiv.org/abs/2404.02195v1","category":"gr-qc"}
{"created":"2024-04-02 17:59:00","title":"Convergence of overlapping domain decomposition methods with PML transmission conditions applied to nontrapping Helmholtz problems","abstract":"We study overlapping Schwarz methods for the Helmholtz equation posed in any dimension with large, real wavenumber and smooth variable wave speed. The radiation condition is approximated by a Cartesian perfectly-matched layer (PML). The domain-decomposition subdomains are overlapping hyperrectangles with Cartesian PMLs at their boundaries. The overlaps of the subdomains and the widths of the PMLs are all taken to be independent of the wavenumber.   For both parallel (i.e., additive) and sequential (i.e., multiplicative) methods, we show that after a specified number of iterations -- depending on the behaviour of the geometric-optic rays -- the error is smooth and smaller than any negative power of the wavenumber. For the parallel method, the specified number of iterations is less than the maximum number of subdomains, counted with their multiplicity, that a geometric-optic ray can intersect.   These results, which are illustrated by numerical experiments, are the first wavenumber-explicit results about convergence of overlapping Schwarz methods for the Helmholtz equation, and the first wavenumber-explicit results about convergence of any domain-decomposition method for the Helmholtz equation with a non-trivial scatterer (here a variable wave speed).","sentences":["We study overlapping Schwarz methods for the Helmholtz equation posed in any dimension with large, real wavenumber and smooth variable wave speed.","The radiation condition is approximated by a Cartesian perfectly-matched layer (PML).","The domain-decomposition subdomains are overlapping hyperrectangles with Cartesian PMLs at their boundaries.","The overlaps of the subdomains and the widths of the PMLs are all taken to be independent of the wavenumber.   ","For both parallel (i.e., additive) and sequential (i.e., multiplicative) methods, we show that after a specified number of iterations -- depending on the behaviour of the geometric-optic rays -- the error is smooth and smaller than any negative power of the wavenumber.","For the parallel method, the specified number of iterations is less than the maximum number of subdomains, counted with their multiplicity, that a geometric-optic ray can intersect.   ","These results, which are illustrated by numerical experiments, are the first wavenumber-explicit results about convergence of overlapping Schwarz methods for the Helmholtz equation, and the first wavenumber-explicit results about convergence of any domain-decomposition method for the Helmholtz equation with a non-trivial scatterer (here a variable wave speed)."],"url":"http://arxiv.org/abs/2404.02156v1","category":"math.NA"}
{"created":"2024-04-02 17:58:57","title":"Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields","abstract":"Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust.","sentences":["Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa.","We call this property alpha invariance.","For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance.","We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling.","We test their behaviors and show our recipe to be more robust."],"url":"http://arxiv.org/abs/2404.02155v1","category":"cs.CV"}
{"created":"2024-04-02 17:58:22","title":"Nambu-Goto equation from three-dimensional gravity","abstract":"We demonstrate that the solutions of three-dimensional gravity obtained by gluing two copies of a spacetime across a junction constituted of a tensile string are in one-to-one correspondence with the solutions of the Nambu-Goto equation in the same spacetime up to a finite number of rigid deformations. The non-linear Nambu-Goto equation satisfied by the average of the embedding coordinates of the junction emerges directly from the junction conditions along with the rigid deformations and corrections due to the tension. Therefore, the equivalence principle generalizes non-trivially to the string. Our results are valid both in three-dimensional flat and AdS spacetimes. In the context of AdS$_3$/CFT$_2$ correspondence, our setup could be used to describe a class of interfaces in the conformal field theory featuring relative time reparametrization at the interface which encodes the solution of the Nambu-Goto equation corresponding to the bulk junction.","sentences":["We demonstrate that the solutions of three-dimensional gravity obtained by gluing two copies of a spacetime across a junction constituted of a tensile string are in one-to-one correspondence with the solutions of the Nambu-Goto equation in the same spacetime up to a finite number of rigid deformations.","The non-linear Nambu-Goto equation satisfied by the average of the embedding coordinates of the junction emerges directly from the junction conditions along with the rigid deformations and corrections due to the tension.","Therefore, the equivalence principle generalizes non-trivially to the string.","Our results are valid both in three-dimensional flat and AdS spacetimes.","In the context of AdS$_3$/CFT$_2$ correspondence, our setup could be used to describe a class of interfaces in the conformal field theory featuring relative time reparametrization at the interface which encodes the solution of the Nambu-Goto equation corresponding to the bulk junction."],"url":"http://arxiv.org/abs/2404.02149v1","category":"hep-th"}
{"created":"2024-04-02 17:49:40","title":"Topic-based Watermarks for LLM-Generated Text","abstract":"Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a \"topic-based watermarking algorithm\" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.","sentences":["Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text.","Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output.","However, current watermarking schemes lack robustness against known attacks against watermarking algorithms.","In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work.","In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a \"topic-based watermarking algorithm\" for LLMs.","The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM.","Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM.","Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm.","Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss."],"url":"http://arxiv.org/abs/2404.02138v1","category":"cs.CR"}
{"created":"2024-04-02 17:48:46","title":"ResNet with Integrated Convolutional Block Attention Module for Ship Classification Using Transfer Learning on Optical Satellite Imagery","abstract":"This study proposes a novel transfer learning framework for effective ship classification using high-resolution optical remote sensing satellite imagery. The framework is based on the deep convolutional neural network model ResNet50 and incorporates the Convolutional Block Attention Module (CBAM) to enhance performance. CBAM enables the model to attend to salient features in the images, allowing it to better discriminate between subtle differences between ships and backgrounds. Furthermore, this study adopts a transfer learning approach tailored for accurately classifying diverse types of ships by fine-tuning a pre-trained model for the specific task. Experimental results demonstrate the efficacy of the proposed framework in ship classification using optical remote sensing imagery, achieving a high classification accuracy of 94% across 5 classes, outperforming existing methods. This research holds potential applications in maritime surveillance and management, illegal fishing detection, and maritime traffic monitoring.","sentences":["This study proposes a novel transfer learning framework for effective ship classification using high-resolution optical remote sensing satellite imagery.","The framework is based on the deep convolutional neural network model ResNet50 and incorporates the Convolutional Block Attention Module (CBAM) to enhance performance.","CBAM enables the model to attend to salient features in the images, allowing it to better discriminate between subtle differences between ships and backgrounds.","Furthermore, this study adopts a transfer learning approach tailored for accurately classifying diverse types of ships by fine-tuning a pre-trained model for the specific task.","Experimental results demonstrate the efficacy of the proposed framework in ship classification using optical remote sensing imagery, achieving a high classification accuracy of 94% across 5 classes, outperforming existing methods.","This research holds potential applications in maritime surveillance and management, illegal fishing detection, and maritime traffic monitoring."],"url":"http://arxiv.org/abs/2404.02135v2","category":"cs.CV"}
{"created":"2024-04-02 17:42:30","title":"Numerical simulation of the Gross-Pitaevskii equation via vortex tracking","abstract":"This paper deals with the numerical simulation of the Gross-Pitaevskii (GP) equation, for which a well-known feature is the appearance of quantized vortices with core size of the order of a small parameter $\\varepsilon$. Without a magnetic field and with suitable initial conditions, these vortices interact, in the singular limit $\\varepsilon\\to0$, through an explicit Hamiltonian dynamics. Using this analytical framework, we develop and analyze a numerical strategy based on the reduced-order Hamiltonian system to efficiently simulate the infinite-dimensional GP equation for small, but finite, $\\varepsilon$. This method allows us to avoid numerical stability issues in solving the GP equation, where small values of $\\varepsilon$ typically require very fine meshes and time steps. We also provide a mathematical justification of our method in terms of rigorous error estimates of the error in the supercurrent, together with numerical illustrations.","sentences":["This paper deals with the numerical simulation of the Gross-Pitaevskii (GP) equation, for which a well-known feature is the appearance of quantized vortices with core size of the order of a small parameter $\\varepsilon$. Without a magnetic field and with suitable initial conditions, these vortices interact, in the singular limit $\\varepsilon\\to0$, through an explicit Hamiltonian dynamics.","Using this analytical framework, we develop and analyze a numerical strategy based on the reduced-order Hamiltonian system to efficiently simulate the infinite-dimensional GP equation for small, but finite, $\\varepsilon$. This method allows us to avoid numerical stability issues in solving the GP equation, where small values of $\\varepsilon$ typically require very fine meshes and time steps.","We also provide a mathematical justification of our method in terms of rigorous error estimates of the error in the supercurrent, together with numerical illustrations."],"url":"http://arxiv.org/abs/2404.02133v1","category":"math.NA"}
{"created":"2024-04-02 17:10:47","title":"Tightening the reins on non-minimal dark sector physics: Interacting Dark Energy with dynamical and non-dynamical equation of state","abstract":"We present a comprehensive reassessment of the state of Interacting Dark Energy (IDE) cosmology, namely models featuring a non-gravitational interaction between Dark Matter (DM) and Dark Energy (DE). To achieve high generality, we extend the dark sector physics by considering two different scenarios: a non-dynamical DE equation of state $w_0\\neq-1$, and a dynamical $w(a)=w_0+w_a(1-a)$. In both cases, we distinguish two different physical regimes resulting from a phantom or quintessence equation of state. To circumvent early-time superhorizon instabilities, the energy-momentum transfer should occur in opposing directions within the two regimes, resulting in distinct phenomenological outcomes. We study quintessence and phantom non-dynamical and dynamical models in light of two independent Cosmic Microwave Background (CMB) experiments - the Planck satellite and the Atacama Cosmology Telescope. We analyze CMB data both independently and in combination with Supernovae (SN) distance moduli measurements from the Pantheon-Plus catalog and Baryon Acoustic Oscillations (BAO) from the SDSS-IV eBOSS survey. Our results update and extend the state-of-the-art analyses, significantly narrowing the parameter space allowed for these models and limiting their overall ability to reconcile cosmological tensions. Although considering different combinations of data leaves some freedom to increase $H_0$ towards the value measured by the SH0ES collaboration, our most constraining dataset (CMB+BAO+SN) indicates that fully reconciling the tension solely within the framework of IDE remains challenging.","sentences":["We present a comprehensive reassessment of the state of Interacting Dark Energy (IDE) cosmology, namely models featuring a non-gravitational interaction between Dark Matter (DM) and Dark Energy (DE).","To achieve high generality, we extend the dark sector physics by considering two different scenarios: a non-dynamical DE equation of state $w_0\\neq-1$, and a dynamical $w(a)=w_0+w_a(1-a)$.","In both cases, we distinguish two different physical regimes resulting from a phantom or quintessence equation of state.","To circumvent early-time superhorizon instabilities, the energy-momentum transfer should occur in opposing directions within the two regimes, resulting in distinct phenomenological outcomes.","We study quintessence and phantom non-dynamical and dynamical models in light of two independent Cosmic Microwave Background (CMB) experiments - the Planck satellite and the Atacama Cosmology Telescope.","We analyze CMB data both independently and in combination with Supernovae (SN) distance moduli measurements from the Pantheon-Plus catalog and Baryon Acoustic Oscillations (BAO) from the SDSS-IV eBOSS survey.","Our results update and extend the state-of-the-art analyses, significantly narrowing the parameter space allowed for these models and limiting their overall ability to reconcile cosmological tensions.","Although considering different combinations of data leaves some freedom to increase $H_0$ towards the value measured by the SH0ES collaboration, our most constraining dataset (CMB+BAO+SN) indicates that fully reconciling the tension solely within the framework of IDE remains challenging."],"url":"http://arxiv.org/abs/2404.02110v1","category":"astro-ph.CO"}
{"created":"2024-04-02 16:46:11","title":"Searching for new physics in the solar system with tetrahedral spacecraft formations","abstract":"Tetrahedral configurations of spacecraft on unperturbed heliocentric orbits allow for highly precise observations of small spatial changes in the gravitational field, especially those affecting the gravity gradient tensor (GGT). The resulting high sensitivity may be used to search for new physics that could manifest itself via deviations from general relativistic behavior yielding a non-vanishing trace[GGT]. We study the feasibility of recovering the trace[GGT] with the sensitivity of O(1e-24 s^(-2)) -- the level where some of the recently proposed cosmological models may have observable effects in the solar system. We consider how local measurements provided by precision laser ranging and atom-wave interferometry can be used for that purpose. We report on a preliminary study of such an experiment and precision that may be reached in measuring the trace[GGT], with the assumption of drag-compensated spacecraft by atom interferometer measurements. For that, we study the dynamical behavior of a tetrahedral formation established by four spacecraft on heliocentric nearby elliptical orbits. We formulate the observational equations to measure the trace[GGT] relying only on the observables available within the formation: laser ranging and the Sagnac interferometry. We demonstrate that Sagnac observable is a mission enabling and allows to measure the angular frequency of the tetrahedral rotation with respect to an inertial reference frame with an accuracy much higher than that available from any other modern navigational techniques. We show that the quality of the science measurements is affected by the changes in tetrahedron's orientation and shape as spacecraft follow their orbits. We present the preliminary mission and instrument requirements needed to measure the trace[GGT] to the required accuracy and demonstrate the feasibility of satisfying the science objectives.","sentences":["Tetrahedral configurations of spacecraft on unperturbed heliocentric orbits allow for highly precise observations of small spatial changes in the gravitational field, especially those affecting the gravity gradient tensor (GGT).","The resulting high sensitivity may be used to search for new physics that could manifest itself via deviations from general relativistic behavior yielding a non-vanishing trace[GGT].","We study the feasibility of recovering the trace[GGT] with the sensitivity of O(1e-24 s^(-2)) -- the level where some of the recently proposed cosmological models may have observable effects in the solar system.","We consider how local measurements provided by precision laser ranging and atom-wave interferometry can be used for that purpose.","We report on a preliminary study of such an experiment and precision that may be reached in measuring the trace[GGT], with the assumption of drag-compensated spacecraft by atom interferometer measurements.","For that, we study the dynamical behavior of a tetrahedral formation established by four spacecraft on heliocentric nearby elliptical orbits.","We formulate the observational equations to measure the trace[GGT] relying only on the observables available within the formation: laser ranging and the Sagnac interferometry.","We demonstrate that Sagnac observable is a mission enabling and allows to measure the angular frequency of the tetrahedral rotation with respect to an inertial reference frame with an accuracy much higher than that available from any other modern navigational techniques.","We show that the quality of the science measurements is affected by the changes in tetrahedron's orientation and shape as spacecraft follow their orbits.","We present the preliminary mission and instrument requirements needed to measure the trace[GGT] to the required accuracy and demonstrate the feasibility of satisfying the science objectives."],"url":"http://arxiv.org/abs/2404.02096v1","category":"gr-qc"}
{"created":"2024-04-02 16:18:49","title":"Asymptotics of resampling without replacement in robust and logistic regression","abstract":"This paper studies the asymptotics of resampling without replacement in the proportional regime where dimension $p$ and sample size $n$ are of the same order. For a given dataset $(\\bm{X},\\bm{y})\\in\\mathbb{R}^{n\\times p}\\times \\mathbb{R}^n$ and fixed subsample ratio $q\\in(0,1)$, the practitioner samples independently of $(\\bm{X},\\bm{y})$ iid subsets $I_1,...,I_M$ of $\\{1,...,n\\}$ of size $q n$ and trains estimators $\\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$ on the corresponding subsets of rows of $(\\bm{X},\\bm{y})$. Understanding the performance of the bagged estimate $\\bm{\\bar{\\beta}} = \\frac1M\\sum_{m=1}^M \\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$, for instance its squared error, requires us to understand correlations between two distinct $\\bm{\\hat{\\beta}}(I_m)$ and $\\bm{\\hat{\\beta}}(I_{m'})$ trained on different subsets $I_m$ and $I_{m'}$. In robust linear regression and logistic regression, we characterize the limit in probability of the correlation between two estimates trained on different subsets of the data. The limit is characterized as the unique solution of a simple nonlinear equation. We further provide data-driven estimators that are consistent for estimating this limit. These estimators of the limiting correlation allow us to estimate the squared error of the bagged estimate $\\bm{\\bar{\\beta}}$, and for instance perform parameter tuning to choose the optimal subsample ratio $q$. As a by-product of the proof argument, we obtain the limiting distribution of the bivariate pair $(\\bm{x}_i^T \\bm{\\hat{\\beta}}(I_m), \\bm{x}_i^T \\bm{\\hat{\\beta}}(I_{m'}))$ for observations $i\\in I_m\\cap I_{m'}$, i.e., for observations used to train both estimates.","sentences":["This paper studies the asymptotics of resampling without replacement in the proportional regime where dimension $p$ and sample size $n$ are of the same order.","For a given dataset $(\\bm{X},\\bm{y})\\in\\mathbb{R}^{n\\times p}\\times \\mathbb{R}^n$ and fixed subsample ratio $q\\in(0,1)$, the practitioner samples independently of $(\\bm{X},\\bm{y})$ iid subsets $I_1,...,I_M$ of $\\{1,...,n\\}$ of size $q n$ and trains estimators $\\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$ on the corresponding subsets of rows of $(\\bm{X},\\bm{y})$. Understanding the performance of the bagged estimate $\\bm{\\bar{\\beta}} = \\frac1M\\sum_{m=1}^M \\bm{\\hat{\\beta}}(I_1),...,\\bm{\\hat{\\beta}}(I_M)$, for instance its squared error, requires us to understand correlations between two distinct $\\bm{\\hat{\\beta}}(I_m)$ and $\\bm{\\hat{\\beta}}(I_{m'})$ trained on different subsets $I_m$ and $I_{m'}$. In robust linear regression and logistic regression, we characterize the limit in probability of the correlation between two estimates trained on different subsets of the data.","The limit is characterized as the unique solution of a simple nonlinear equation.","We further provide data-driven estimators that are consistent for estimating this limit.","These estimators of the limiting correlation allow us to estimate the squared error of the bagged estimate $\\bm{\\bar{\\beta}}$, and for instance perform parameter tuning to choose the optimal subsample ratio $q$. As a by-product of the proof argument, we obtain the limiting distribution of the bivariate pair $(\\bm{x}_i^T \\bm{\\hat{\\beta}}(I_m), \\bm{x}_i^T \\bm{\\hat{\\beta}}(I_{m'}))$ for observations $i\\in I_m\\cap I_{m'}$, i.e., for observations used to train both estimates."],"url":"http://arxiv.org/abs/2404.02070v1","category":"math.ST"}
{"created":"2024-04-02 16:10:29","title":"Using Interpretation Methods for Model Enhancement","abstract":"In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings. Code is available at https://github.com/Chord-Chen-30/UIMER.","sentences":["In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models.","Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales.","However, this intuitive idea has not been fully explored.","In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models.","Our framework is very general in the sense that it can incorporate various interpretation methods.","Previously proposed gradient-based methods can be shown as an instance of our framework.","We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement.","We conduct comprehensive experiments on a variety of tasks.","Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings.","Code is available at https://github.com/Chord-Chen-30/UIMER."],"url":"http://arxiv.org/abs/2404.02068v1","category":"cs.CL"}
{"created":"2024-04-02 16:06:20","title":"Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation","abstract":"Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data. Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data. However, unreliable pseudo-labeling can undermine the semi-supervision processes. In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use graph neural networks to capture structural relationships in Semantic-Level Graphs (SLGs) and Class-Level Graphs (CLGs) to rectify erroneous pseudo-labels. Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels. With the support of proximate pattern information from graphs, MLLC can rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations. We design an end-to-end network to train and perform this effective label corrections mechanism. Experiments demonstrate that MLLC can significantly improve supervised baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets. Specifically, MLLC improves the supervised baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols.","sentences":["Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data.","Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data.","However, unreliable pseudo-labeling can undermine the semi-supervision processes.","In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use graph neural networks to capture structural relationships in Semantic-Level Graphs (SLGs) and Class-Level Graphs (CLGs) to rectify erroneous pseudo-labels.","Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels.","With the support of proximate pattern information from graphs, MLLC can rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations.","We design an end-to-end network to train and perform this effective label corrections mechanism.","Experiments demonstrate that MLLC can significantly improve supervised baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets.","Specifically, MLLC improves the supervised baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols."],"url":"http://arxiv.org/abs/2404.02065v1","category":"cs.CV"}
{"created":"2024-04-02 15:57:27","title":"Uniformity in nonreduced rings via Noetherian operators","abstract":"We prove a differential version of the Artin-Rees lemma with the use of Noetherian differential operators. As a consequence, we obtain several uniformity results for nonreduced rings.","sentences":["We prove a differential version of the Artin-Rees lemma with the use of Noetherian differential operators.","As a consequence, we obtain several uniformity results for nonreduced rings."],"url":"http://arxiv.org/abs/2404.02057v1","category":"math.AC"}
{"created":"2024-04-02 15:51:52","title":"On Hamiltonian Structures of Partial Difference Equations","abstract":"We first introduce the notion of Hamiltonian structure for a partial difference equation. Then we construct some infinite quivers, and realize the discrete KdV equation, the Hirota-Miwa equation and its various reductions as the mutation relations of the corresponding cluster algebras. Finally, we show that the log-canonical Poisson structures associated to these cluster algebras give the Hamiltonian structures or the bihamiltonian structures of these partial difference equations.","sentences":["We first introduce the notion of Hamiltonian structure for a partial difference equation.","Then we construct some infinite quivers, and realize the discrete KdV equation, the Hirota-Miwa equation and its various reductions as the mutation relations of the corresponding cluster algebras.","Finally, we show that the log-canonical Poisson structures associated to these cluster algebras give the Hamiltonian structures or the bihamiltonian structures of these partial difference equations."],"url":"http://arxiv.org/abs/2404.02055v1","category":"math-ph"}
{"created":"2024-04-02 15:49:00","title":"NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation","abstract":"The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB.","sentences":["The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis.","As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target.","Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation.","Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters.","Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen.","This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters.","At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication.","Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB."],"url":"http://arxiv.org/abs/2404.02185v1","category":"cs.CV"}
{"created":"2024-04-02 15:45:39","title":"Classification of superpotentials for cohomogeneity one Ricci solitons","abstract":"We classify superpotentials for the Hamiltonian system corresponding to the cohomogeneity one gradient Ricci soliton equations. Aside from recovering known examples of superpotentials for steady solitons, we find a new superpotential on a specific case of the B\\'erard-Bergery-Calabi ansatz. The latter is used to obtain an explicit formula for a steady complete soliton with an equidistant family of hypersurfaces given by circle bundles over $S^2\\times S^2$. There are no superpotentials in the non-steady case in dimensions greater than 2, even if polynomial coefficients are allowed. We also briefly discuss generalised first integrals and the limitations of some known methods of finding them.","sentences":["We classify superpotentials for the Hamiltonian system corresponding to the cohomogeneity one gradient Ricci soliton equations.","Aside from recovering known examples of superpotentials for steady solitons, we find a new superpotential on a specific case of the B\\'erard-Bergery-Calabi ansatz.","The latter is used to obtain an explicit formula for a steady complete soliton with an equidistant family of hypersurfaces given by circle bundles over $S^2\\times S^2$.","There are no superpotentials in the non-steady case in dimensions greater than 2, even if polynomial coefficients are allowed.","We also briefly discuss generalised first integrals and the limitations of some known methods of finding them."],"url":"http://arxiv.org/abs/2404.02050v1","category":"math.DG"}
{"created":"2024-04-02 15:32:27","title":"Analytic conjugation between planar differential systems and potential systems","abstract":"In this article it is proved that an analytical planar vector field with a non-degenerate center at $(0,0)$ is analytically conjugate, in a neighborhood of $(0,0)$, to a Hamiltonian vector field of the form $y\\frac{\\partial}{\\partial x}-V'(x)\\frac{\\partial}{\\partial y}$, where $V$ is an analytic function defined in a neighborhood of the origin such that $V(0)=V'(0)=0$ and $V''(0)>0.$","sentences":["In this article it is proved that an analytical planar vector field with a non-degenerate center at $(0,0)$ is analytically conjugate, in a neighborhood of $(0,0)$, to a Hamiltonian vector field of the form $y\\frac{\\partial}{\\partial x}-V'(x)\\frac{\\partial}{\\partial y}$, where $V$ is an analytic function defined in a neighborhood of the origin such that $V(0)=V'(0)=0$ and $V''(0)>0.$"],"url":"http://arxiv.org/abs/2404.02036v1","category":"math.DS"}
{"created":"2024-04-02 15:20:25","title":"Exploring Spin Polarization of Heavy Quarks in Magnetic Fields and Hot Medium","abstract":"Relativistic heavy-ion collisions give rise to the formation of both deconfined QCD matter and a strong magnetic field. The spin of heavy quarks is influenced by interactions with the external magnetic field as well as by random scatterings with thermal light partons. The presence of QCD matter comprising charged quarks can extend the lifetime and strength of the magnetic field, thereby enhancing the degree of heavy quark polarization. However, the random scatterings with QCD matter tend to diminish heavy quark polarization. In this study, we utilize the Landau-Lifshitz-Gilbert (LLG) equation to investigate both these contributions. Taking into account the realistic evolutions of medium temperatures and the in-medium magnetic fields at the Relativistic Heavy-Ion Collider (RHIC) and the Large Hadron Collider (LHC), we observe that heavy quark polarization is limited by the short lifetime of the magnetic field and the high temperatures of the medium. Furthermore, we explore the mass dependence of quark polarization, revealing that the polarization degree of strange quarks is much larger than that of charm quarks.","sentences":["Relativistic heavy-ion collisions give rise to the formation of both deconfined QCD matter and a strong magnetic field.","The spin of heavy quarks is influenced by interactions with the external magnetic field as well as by random scatterings with thermal light partons.","The presence of QCD matter comprising charged quarks can extend the lifetime and strength of the magnetic field, thereby enhancing the degree of heavy quark polarization.","However, the random scatterings with QCD matter tend to diminish heavy quark polarization.","In this study, we utilize the Landau-Lifshitz-Gilbert (LLG) equation to investigate both these contributions.","Taking into account the realistic evolutions of medium temperatures and the in-medium magnetic fields at the Relativistic Heavy-Ion Collider (RHIC) and the Large Hadron Collider (LHC), we observe that heavy quark polarization is limited by the short lifetime of the magnetic field and the high temperatures of the medium.","Furthermore, we explore the mass dependence of quark polarization, revealing that the polarization degree of strange quarks is much larger than that of charm quarks."],"url":"http://arxiv.org/abs/2404.02032v1","category":"nucl-th"}
{"created":"2024-04-02 15:16:38","title":"High-energy neutrinos flavour composition as a probe of neutrino magnetic moments","abstract":"Neutrino propagation in the Galactic magnetic field is considered. To describe neutrino flavour and spin oscillations on the galactic scale baselines an approach using wave packets is developed. Evolution equations for the neutrino wave packets in a uniform and non-uniform magnetic field are derived. Analytical expressions for neutrino flavour and spin oscillations probabilities accounting for damping due to wave packet separation are obtained for the case of uniform magnetic field. It is shown that for oscillations on magnetic frequencies $\\omega_i^B = \\mu_i B_\\perp$ the coherence lengths that characterizes the damping scale is proportional to the cube of neutrino average momentum $p_0^3$. Probabilities of flavour and spin oscillations are calculated numerically for neutrino interacting with the non-uniform Galactic magnetic field. Flavour compositions of high-energy neutrino flux coming from the Galactic centre are calculated accounting for neutrino interaction with the magnetic field. It is shown that for neutrino magnetic moments $\\sim 10^{-13} \\mu_B$ and larger these flavour compositions significantly differ from ones predicted by the vacuum neutrino oscillations scenario.","sentences":["Neutrino propagation in the Galactic magnetic field is considered.","To describe neutrino flavour and spin oscillations on the galactic scale baselines an approach using wave packets is developed.","Evolution equations for the neutrino wave packets in a uniform and non-uniform magnetic field are derived.","Analytical expressions for neutrino flavour and spin oscillations probabilities accounting for damping due to wave packet separation are obtained for the case of uniform magnetic field.","It is shown that for oscillations on magnetic frequencies $\\omega_i^B = \\mu_i B_\\perp$ the coherence lengths that characterizes the damping scale is proportional to the cube of neutrino average momentum $p_0^3$.","Probabilities of flavour and spin oscillations are calculated numerically for neutrino interacting with the non-uniform Galactic magnetic field.","Flavour compositions of high-energy neutrino flux coming from the Galactic centre are calculated accounting for neutrino interaction with the magnetic field.","It is shown that for neutrino magnetic moments $\\sim 10^{-13} \\mu_B$ and larger these flavour compositions significantly differ from ones predicted by the vacuum neutrino oscillations scenario."],"url":"http://arxiv.org/abs/2404.02027v1","category":"hep-ph"}
{"created":"2024-04-02 15:09:52","title":"Algebraic structures in Lagrangian Floer cohomology modelled on differential forms","abstract":"We define a structure of an algebra on the Lagrangian Floer cohomology of a Lagrangian submanifold over the quantum cohomology of the ambient symplectic manifold. The structure is analogous to the one defined by Biran-Cornea, but is constructed in the differential forms model. In the spirit of Ganatra and Hugtenburg, we define another such algebra structure using a closed-open map. We show that the two structures coincide. As an application, we show that the module structure for the 2-dimensional Clifford torus is given by multiplication by a Novikov coefficient, similarly to the Biran-Cornea module structure for this case.","sentences":["We define a structure of an algebra on the Lagrangian Floer cohomology of a Lagrangian submanifold over the quantum cohomology of the ambient symplectic manifold.","The structure is analogous to the one defined by Biran-Cornea, but is constructed in the differential forms model.","In the spirit of Ganatra and Hugtenburg, we define another such algebra structure using a closed-open map.","We show that the two structures coincide.","As an application, we show that the module structure for the 2-dimensional Clifford torus is given by multiplication by a Novikov coefficient, similarly to the Biran-Cornea module structure for this case."],"url":"http://arxiv.org/abs/2404.02020v1","category":"math.SG"}
{"created":"2024-04-02 15:09:22","title":"Aspects of the geometry and topology of expanding horizons","abstract":"The aim of this paper is to extend some basic results about marginally outer trapped surfaces to the context of surfaces having general null expansion. Motivated in part by recent work of Chai-Wan, we introduce the notion of $\\mathfrak{g}$-stability for a general closed hypersurface $\\Sigma$ in an ambient initial data set and prove that, under natural energy conditions, $\\Sigma$ has positive Yamabe type, that is, $\\Sigma$ admits a metric of positive scalar curvature, provided $\\Sigma$ is $\\mathfrak{g}$-stable. Similar results are obtained when $\\Sigma$ is embedded in a spacelike, or null, hypersurface of a spacetime satisfying the dominant energy condition. Conditions implying $\\mathfrak{g}$-stability are also discussed. Finally, we obtain a spacetime positive mass theorem for initial data sets with compact boundary $\\Sigma$ of positive null expansion, assuming that the dominant energy condition is sufficiently strict near $\\Sigma$. This extends recent results of Galloway-Lee and Lee-Lesourd-Unger.","sentences":["The aim of this paper is to extend some basic results about marginally outer trapped surfaces to the context of surfaces having general null expansion.","Motivated in part by recent work of Chai-Wan, we introduce the notion of $\\mathfrak{g}$-stability for a general closed hypersurface $\\Sigma$ in an ambient initial data set and prove that, under natural energy conditions, $\\Sigma$ has positive Yamabe type, that is, $\\Sigma$ admits a metric of positive scalar curvature, provided $\\Sigma$ is $\\mathfrak{g}$-stable.","Similar results are obtained when $\\Sigma$ is embedded in a spacelike, or null, hypersurface of a spacetime satisfying the dominant energy condition.","Conditions implying $\\mathfrak{g}$-stability are also discussed.","Finally, we obtain a spacetime positive mass theorem for initial data sets with compact boundary $\\Sigma$ of positive null expansion, assuming that the dominant energy condition is sufficiently strict near $\\Sigma$. This extends recent results of Galloway-Lee and Lee-Lesourd-Unger."],"url":"http://arxiv.org/abs/2404.02019v1","category":"gr-qc"}
{"created":"2024-04-02 14:55:44","title":"Determining the chemical composition of diamagnetic mixed solids via measurements of the magnetic susceptibility","abstract":"Mixed solid compounds are employed in a vast array of applications so an accurate determination of their chemical compositions is of crucial importance. All current characterization methods require specially-treated samples so the availability of a more practical method with similar accuracy should alleviate the quantification process. In this work, we show how the doping concentration $\\delta$ (or isotope concentration) of a mixed solid compound in powdered form, where both parent compounds are diamagnetic, can be obtained from the measurement of the mass magnetization. We exploit the additive nature of the molar magnetic susceptibility $\\chi_{Mol}$ and molar mass to construct two equations with the same two unknowns in the $\\chi_{Mol}$ vs. $\\delta$ space to simultaneously solve $\\chi_{Mol}$ and $\\delta$ of a mixed solid. Eight examples are provided to show the wide applicability of this method: NH$_{4(1-\\delta)}$D$_{4\\delta}$Br (where D = $^2$H), NH$_4$I$_{1-\\delta}$Br$_\\delta$, (NH$_4$H$_2$)$_{1-\\delta}$(ND$_4$D$_2$)$_\\delta$PO$_4$, C$_{48}$H$_{22+6\\delta}$Br$_{6(1-\\delta)}$O$_{32}$Zr$_6$, [creatine]$_{1-\\delta}$[$_D$-glucose]$_\\delta$, [$_L$-glutamic acid]$_{1-\\delta}$[$_L$-leucine]$_\\delta$, [terephthalic acid]$_{1-\\delta}$[trimesic acid]$_\\delta$ and [p-terphenyl]$_{1-\\delta}$[triphenylphosphine]$_\\delta$. Experimental errors of ~1.2% were obtained for $\\delta$ from average sample masses of 16.6 mg in powdered form rendering the presented approach an attractive choice for characterizing the ratios of mixed solids.","sentences":["Mixed solid compounds are employed in a vast array of applications so an accurate determination of their chemical compositions is of crucial importance.","All current characterization methods require specially-treated samples so the availability of a more practical method with similar accuracy should alleviate the quantification process.","In this work, we show how the doping concentration $\\delta$ (or isotope concentration) of a mixed solid compound in powdered form, where both parent compounds are diamagnetic, can be obtained from the measurement of the mass magnetization.","We exploit the additive nature of the molar magnetic susceptibility $\\chi_{Mol}$ and molar mass to construct two equations with the same two unknowns in the $\\chi_{Mol}$ vs. $\\delta$ space to simultaneously solve $\\chi_{Mol}$ and $\\delta$ of a mixed solid.","Eight examples are provided to show the wide applicability of this method: NH$_{4(1-\\delta)}$D$_{4\\delta}$Br (where D = $^2$H), NH$_4$I$_{1-\\delta}$Br$_\\delta$, (NH$_4$H$_2$)$_{1-\\delta}$(ND$_4$D$_2$)$_\\delta$PO$_4$, C$_{48}$H$_{22+6\\delta}$Br$_{6(1-\\delta)}$O$_{32}$Zr$_6$, [creatine]$_{1-\\delta}$[$_D$-glucose]$_\\delta$, [$_L$-glutamic acid]$_{1-\\delta}$[$_L$-leucine]$_\\delta$, [terephthalic acid]$_{1-\\delta}$[trimesic acid]$_\\delta$ and [p-terphenyl]$_{1-\\delta}$[triphenylphosphine]$_\\delta$. Experimental errors of ~1.2% were obtained for $\\delta$ from average sample masses of 16.6 mg in powdered form rendering the presented approach an attractive choice for characterizing the ratios of mixed solids."],"url":"http://arxiv.org/abs/2404.02012v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-02 14:52:24","title":"Singularity formation of vortex sheets in 2D Euler equations using the characteristic mapping method","abstract":"The goal of this numerical study is to get insight into singular solutions of the two-dimensional (2D) Euler equations for non-smooth initial data, in particular for vortex sheets. To this end high resolution computations of vortex layers in 2D incompressible Euler flows are performed using the characteristic mapping method (CMM). This semi-Lagrangian method evolves the flow map using the gradient-augmented level set method (GALS). The semi-group structure of the flow map allows its decomposition into sub-maps (each over a finite time interval), and thus the precision can be controlled by choosing appropriate remapping times. Composing the flow map yields exponential resolution in linear time, a unique feature of CMM, and thus fine scale flow structures can be resolved in great detail. Here the roll-up process of vortex layers is studied varying the thickness of the layer showing its impact on the growth of palinstrophy and possible blow up of absolute vorticity. The curvature of the vortex sheet shows a singular-like behavior. The self-similar structure of the vortex core is investigated in the vanishing thickness limit. Conclusions on the non-uniqueness of weak solutions of 2D Euler for non-smooth initial data are drawn and the presence of flow singularities is revealed tracking them in the complex plane.","sentences":["The goal of this numerical study is to get insight into singular solutions of the two-dimensional (2D) Euler equations for non-smooth initial data, in particular for vortex sheets.","To this end high resolution computations of vortex layers in 2D incompressible Euler flows are performed using the characteristic mapping method (CMM).","This semi-Lagrangian method evolves the flow map using the gradient-augmented level set method (GALS).","The semi-group structure of the flow map allows its decomposition into sub-maps (each over a finite time interval), and thus the precision can be controlled by choosing appropriate remapping times.","Composing the flow map yields exponential resolution in linear time, a unique feature of CMM, and thus fine scale flow structures can be resolved in great detail.","Here the roll-up process of vortex layers is studied varying the thickness of the layer showing its impact on the growth of palinstrophy and possible blow up of absolute vorticity.","The curvature of the vortex sheet shows a singular-like behavior.","The self-similar structure of the vortex core is investigated in the vanishing thickness limit.","Conclusions on the non-uniqueness of weak solutions of 2D Euler for non-smooth initial data are drawn and the presence of flow singularities is revealed tracking them in the complex plane."],"url":"http://arxiv.org/abs/2404.02008v1","category":"physics.flu-dyn"}
{"created":"2024-04-02 14:29:47","title":"Impact of The Newly Revised Gravitational Redshift of X-ray Burster GS 1826-24 on The Equation of State of Supradense Neutron-Rich Matter","abstract":"Thanks to the recent advancement in producing rare isotopes and measuring their masses with unprecedented precision, the updated nuclear masses around the waiting-point nucleus $^{64}$Ge in the rapid-proton capture process have led to a significant revision of the surface gravitational redshift of the neutron star (NS) in GS 1826-24 by re-fitting its X-ray burst light curve ({\\it X. Zhou et al., Nature Physics {\\bf 19}, 1091 (2023)}) using Modules for Experiments in Stellar Astrophysics (MESA). The resulting NS compactness $\\xi$ is between 0.244 and 0.342 at 95\\% confidence level and its upper boundary is significantly smaller than the maximum $\\xi$ previously known. Incorporating this new data within a comprehensive Bayesian statistical framework, we investigate its impact on the Equation of State (EOS) of supradense neutron-rich matter and the required spin frequency for GW190814's minor $m_2$ with mass $2.59\\pm 0.05$M$_{\\odot}$ to be a rotationally stable pulsar. We found that the EOS of high-density symmetric nuclear matter (SNM) has to be softened significantly while the symmetry energy at supersaturation densities stiffened compared to our prior knowledge from earlier analyses using data from both astrophysical observations and terrestrial nuclear experiments. In particular, the skewness $J_0$ characterizing the stiffness of high-density SNM decreases significantly, while the slope $L$, curvature $K_{\\rm{sym}}$, and skewness $J_{\\rm{sym}}$ of nuclear symmetry energy all increase appreciably compared to their fiducial values. We also found that the most probable spin rate for the $m_2$ to be a stable pulsar is very close to its mass-shedding limit once the revised redshift data from GS 1826-24 is considered, making the $m_2$ unlikely the most massive NS observed so far.","sentences":["Thanks to the recent advancement in producing rare isotopes and measuring their masses with unprecedented precision, the updated nuclear masses around the waiting-point nucleus $^{64}$Ge in the rapid-proton capture process have led to a significant revision of the surface gravitational redshift of the neutron star (NS) in GS 1826-24 by re-fitting its X-ray burst light curve ({\\it X. Zhou et al., Nature Physics {\\bf 19}, 1091 (2023)}) using Modules for Experiments in Stellar Astrophysics (MESA).","The resulting NS compactness $\\xi$ is between 0.244 and 0.342 at 95\\% confidence level and its upper boundary is significantly smaller than the maximum $\\xi$ previously known.","Incorporating this new data within a comprehensive Bayesian statistical framework, we investigate its impact on the Equation of State (EOS) of supradense neutron-rich matter and the required spin frequency for GW190814's minor $m_2$ with mass $2.59\\pm 0.05$M$_{\\odot}$ to be a rotationally stable pulsar.","We found that the EOS of high-density symmetric nuclear matter (SNM) has to be softened significantly while the symmetry energy at supersaturation densities stiffened compared to our prior knowledge from earlier analyses using data from both astrophysical observations and terrestrial nuclear experiments.","In particular, the skewness $J_0$ characterizing the stiffness of high-density SNM decreases significantly, while the slope $L$, curvature $K_{\\rm{sym}}$, and skewness $J_{\\rm{sym}}$ of nuclear symmetry energy all increase appreciably compared to their fiducial values.","We also found that the most probable spin rate for the $m_2$ to be a stable pulsar is very close to its mass-shedding limit once the revised redshift data from GS 1826-24 is considered, making the $m_2$ unlikely the most massive NS observed so far."],"url":"http://arxiv.org/abs/2404.01989v1","category":"astro-ph.HE"}
{"created":"2024-04-02 14:20:26","title":"Rigorous derivation of an effective model for coupled Stokes advection, reaction and diffusion with freely evolving microstructure","abstract":"We consider the homogenisation of a coupled Stokes flow and advection-reaction-diffusion problem in a perforated domain with an evolving microstructure of size $\\varepsilon$. Reactions at the boundaries of the microscopic interfaces lead to the formation of a solid layer having a variable, a priori unknown thickness. This results in a growth or shrinkage of the solid phase and, thus, the domain evolution is not known a priori but induced by the advection-reaction-diffusion process. The achievements of this work are the existence and uniqueness of a weak microscopic solution and the rigorous derivation of an effective model for $\\varepsilon \\to 0$, based on $\\varepsilon$-uniform a priori estimates. As a result of the limit passage, the processes on the macroscale are described by an advection-reaction-diffusion problem coupled to Darcy's equation with effective coefficients (porosity, diffusivity and permeability) depending on local cell problems. These local problems are formulated on cells, which depend on the macroscopic position and evolve in time. In particular, the evolution of these cells depends on the macroscopic concentration. Thus, the cell problems (respectively the effective coefficients) are coupled to the macroscopic unknowns and vice versa, leading to a strongly coupled micro-macro model. For pure reactive-diffusive transport coupled with microscopic domain evolution but without advective transport, homogenisation results have recently been presented. We extend these models by advective transport which is driven by the Stokes equation in the a priori unknown evolving pore domain.","sentences":["We consider the homogenisation of a coupled Stokes flow and advection-reaction-diffusion problem in a perforated domain with an evolving microstructure of size $\\varepsilon$. Reactions at the boundaries of the microscopic interfaces lead to the formation of a solid layer having a variable, a priori unknown thickness.","This results in a growth or shrinkage of the solid phase and, thus, the domain evolution is not known a priori but induced by the advection-reaction-diffusion process.","The achievements of this work are the existence and uniqueness of a weak microscopic solution and the rigorous derivation of an effective model for $\\varepsilon \\to 0$, based on $\\varepsilon$-uniform a priori estimates.","As a result of the limit passage, the processes on the macroscale are described by an advection-reaction-diffusion problem coupled to Darcy's equation with effective coefficients (porosity, diffusivity and permeability) depending on local cell problems.","These local problems are formulated on cells, which depend on the macroscopic position and evolve in time.","In particular, the evolution of these cells depends on the macroscopic concentration.","Thus, the cell problems (respectively the effective coefficients) are coupled to the macroscopic unknowns and vice versa, leading to a strongly coupled micro-macro model.","For pure reactive-diffusive transport coupled with microscopic domain evolution but without advective transport, homogenisation results have recently been presented.","We extend these models by advective transport which is driven by the Stokes equation in the a priori unknown evolving pore domain."],"url":"http://arxiv.org/abs/2404.01983v2","category":"math.AP"}
{"created":"2024-04-02 14:19:06","title":"A Simple Ricci Flow Proof of the Uniformization Theorem","abstract":"In this note, we provide a very simple proof of the uniformization theorem of Riemann surfaces by Ricci flow. The argument builds on a refinement of Hamilton's isoperimetric estimate for the Ricci flow on the two-sphere.","sentences":["In this note, we provide a very simple proof of the uniformization theorem of Riemann surfaces by Ricci flow.","The argument builds on a refinement of Hamilton's isoperimetric estimate for the Ricci flow on the two-sphere."],"url":"http://arxiv.org/abs/2404.01980v1","category":"math.DG"}
{"created":"2024-04-02 14:16:57","title":"DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation","abstract":"Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public. Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D convolution to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions. To this end, we propose a Dual-view Supergrid-aware Graph Neural Network (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology). Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data). The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way. Based on the dual-view supergrids, the dual-view implicit correlation encoding module is introduced to learn the correlations between pairwise supergrids. In addition, the dual-view message passing network is introduced to implement the information interaction on the supergrid graphs and images. Extensive experiments on two real-world datasets demonstrate that DSGNN achieves the state-of-the-art performances on the air quality estimation task, outperforming the best baseline by an average of 19.64% in MAE.","sentences":["Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public.","Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D convolution to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions.","To this end, we propose a Dual-view Supergrid-aware Graph Neural Network (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology).","Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data).","The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way.","Based on the dual-view supergrids, the dual-view implicit correlation encoding module is introduced to learn the correlations between pairwise supergrids.","In addition, the dual-view message passing network is introduced to implement the information interaction on the supergrid graphs and images.","Extensive experiments on two real-world datasets demonstrate that DSGNN achieves the state-of-the-art performances on the air quality estimation task, outperforming the best baseline by an average of 19.64% in MAE."],"url":"http://arxiv.org/abs/2404.01975v1","category":"cs.LG"}
{"created":"2024-04-02 14:14:04","title":"Anisotropic quark stars in $f(R,L_m,T)$ gravity","abstract":"We investigate the impact of $f(R,L_m,T)$ gravity on the internal structure of compact stars, expecting this theory to manifest prominently in the high-density cores of such stars. In this study, we begin by considering the algebraic function $f(R,L_m,T) = R + \\alpha T L_m$, where $\\alpha$ represents the matter-geometry coupling constant. We specifically choose the matter Lagrangian density $L_m= -\\rho$ to explore compact stars with anisotropic pressure. To this end, we employ the MIT bag model as an equation of state. We then numerically solve the hydrostatic equilibrium equations to obtain mass-radius relations for quark stars, examining static stability criteria, the adiabatic index, and the speed of sound. Finally, we use recent astrophysical data to constrain the coupling parameter $\\alpha$, which may lead to either larger or smaller masses for quark stars compared to their counterparts in general relativity.","sentences":["We investigate the impact of $f(R,L_m,T)$ gravity on the internal structure of compact stars, expecting this theory to manifest prominently in the high-density cores of such stars.","In this study, we begin by considering the algebraic function $f(R,L_m,T) = R + \\alpha T L_m$, where $\\alpha$ represents the matter-geometry coupling constant.","We specifically choose the matter Lagrangian density $L_m= -\\rho$ to explore compact stars with anisotropic pressure.","To this end, we employ the MIT bag model as an equation of state.","We then numerically solve the hydrostatic equilibrium equations to obtain mass-radius relations for quark stars, examining static stability criteria, the adiabatic index, and the speed of sound.","Finally, we use recent astrophysical data to constrain the coupling parameter $\\alpha$, which may lead to either larger or smaller masses for quark stars compared to their counterparts in general relativity."],"url":"http://arxiv.org/abs/2404.01970v1","category":"gr-qc"}
{"created":"2024-04-02 14:11:55","title":"Analytical photoresponses of gated nanowire photoconductors","abstract":"Low-dimensional photoconductors have extraordinarily high photoresponse and gain, which can be modulated by gate voltages as shown in literature. However, the physics of gate modulation remains elusive. In this work, we investigated the physics of gate modulation in silicon nanowire photoconductors with the analytical photoresponse equations. It was found that the impact of gate voltage varies vastly for nanowires with different size. For the wide nanowires that cannot be pinched off by high gate voltage, we found that the photoresponses are enhanced by at least one order of magnitude due to the gate-induced electric passivation. For narrow nanowires that starts with a pinched-off channel, the gate voltage has no electric passivation effect but increases the potential barrier between source and drain, resulting in a decrease in dark and photo current. For the nanowires with an intermediate size, the channel is continuous but can be pinched off by a high gate voltage. The photoresponsivity and photodetectivity is maximized during the transition from the continuous channel to the pinched-off one. This work provides important insights on how to design high-performance photoconductors.","sentences":["Low-dimensional photoconductors have extraordinarily high photoresponse and gain, which can be modulated by gate voltages as shown in literature.","However, the physics of gate modulation remains elusive.","In this work, we investigated the physics of gate modulation in silicon nanowire photoconductors with the analytical photoresponse equations.","It was found that the impact of gate voltage varies vastly for nanowires with different size.","For the wide nanowires that cannot be pinched off by high gate voltage, we found that the photoresponses are enhanced by at least one order of magnitude due to the gate-induced electric passivation.","For narrow nanowires that starts with a pinched-off channel, the gate voltage has no electric passivation effect but increases the potential barrier between source and drain, resulting in a decrease in dark and photo current.","For the nanowires with an intermediate size, the channel is continuous but can be pinched off by a high gate voltage.","The photoresponsivity and photodetectivity is maximized during the transition from the continuous channel to the pinched-off one.","This work provides important insights on how to design high-performance photoconductors."],"url":"http://arxiv.org/abs/2404.01969v1","category":"physics.app-ph"}
{"created":"2024-04-02 13:56:02","title":"Triharmonic curves in the 3-dimensional Sol space","abstract":"The main aim of this paper is to study triharmonic curves in the 3-dimensional homogeneous space Sol. In the first part of the paper we shall obtain a complete classification of proper triharmonic curves with constant geodesic curvature and torsion. In the final section we shall show that these triharmonic curves form a constant angle with a suitable Killing field of constant length along the curve.","sentences":["The main aim of this paper is to study triharmonic curves in the 3-dimensional homogeneous space Sol.","In the first part of the paper we shall obtain a complete classification of proper triharmonic curves with constant geodesic curvature and torsion.","In the final section we shall show that these triharmonic curves form a constant angle with a suitable Killing field of constant length along the curve."],"url":"http://arxiv.org/abs/2404.01963v1","category":"math.DG"}
{"created":"2024-04-02 13:55:39","title":"Existence of solutions to the generalized dual Minkowski problem","abstract":"Given a real number $q$ and a star body in the $n$-dimensional Euclidean space, the generalized dual curvature measure of a convex body was introduced by Lutwak-Yang-Zhang [43]. The corresponding generalized dual Minkowski problem is studied in this paper. By using variational methods, we solve the generalized dual Minkowski problem for $q<0$, and the even generalized dual Minkowski problem for $0\\leq q\\leq1$. We also obtain a sufficient condition for the existence of solutions to the even generalized dual Minkowski problem for $1<q<n$.","sentences":["Given a real number $q$ and a star body in the $n$-dimensional Euclidean space, the generalized dual curvature measure of a convex body was introduced by Lutwak-Yang-Zhang","[43].","The corresponding generalized dual Minkowski problem is studied in this paper.","By using variational methods, we solve the generalized dual Minkowski problem for $q<0$, and the even generalized dual Minkowski problem for $0\\leq q\\leq1$. We also obtain a sufficient condition for the existence of solutions to the even generalized dual Minkowski problem for $1<q<n$."],"url":"http://arxiv.org/abs/2404.01962v1","category":"math.AP"}
{"created":"2024-04-02 13:49:10","title":"Contact germs and partial differential equations","abstract":"The article introduces contact germs that transform solutions of some partial differential equations into solutions of other equations. Parametric symmetries of differential equations generalizing point and contact symmetries are defined. New transformations and symmetries may depend on derivatives of arbitrary but finite order. The stationary Schr\\\"odinger equations, acoustics and gas dynamics equations are considered as examples.","sentences":["The article introduces contact germs that transform solutions of some partial differential equations into solutions of other equations.","Parametric symmetries of differential equations generalizing point and contact symmetries are defined.","New transformations and symmetries may depend on derivatives of arbitrary but finite order.","The stationary Schr\\\"odinger equations, acoustics and gas dynamics equations are considered as examples."],"url":"http://arxiv.org/abs/2404.01955v1","category":"nlin.SI"}
{"created":"2024-04-02 13:42:46","title":"Unique continuation of Schr\u00f6dinger-type equations for $\\bar\\partial$","abstract":"The purpose of this paper is to study the unique continuation property for a Schr\\\"odinger-type equation $ \\bar\\partial u = Vu$ on a domain in $\\mathbb C^n$, where the solution $u$ may be a scalar function, or a vector-valued function. While simple examples show that the unique continuation property fails in general if the potential $V\\in L^{p}, p<2n$, we first prove that, in the case when $u $ is a scalar function, the unique continuation property holds when $V\\in L_{loc}^{2n}$ and is $\\bar\\partial$-closed. For vector-valued smooth solutions, we establish the unique continuation property either when $V\\in L_{loc}^p $, $ p>2n$ for $n\\ge 3$, or when $V\\in L_{loc}^{2n}$ for $n = 2$. Finally, we discuss the unique continuation property for some special cases where $V\\notin L_{loc}^{2n}$, for instance, $V $ is a constant multiple of $ \\frac{1}{|z|}$.","sentences":["The purpose of this paper is to study the unique continuation property for a Schr\\\"odinger-type equation $ \\bar\\partial u = Vu$ on a domain in $\\mathbb C^n$, where the solution $u$ may be a scalar function, or a vector-valued function.","While simple examples show that the unique continuation property fails in general if the potential $V\\in L^{p}, p<2n$, we first prove that, in the case when $u $ is a scalar function, the unique continuation property holds when $V\\in L_{loc}^{2n}$ and is $\\bar\\partial$-closed.","For vector-valued smooth solutions, we establish the unique continuation property either when $V\\in L_{loc}^p $, $ p>2n$ for $n\\ge 3$, or when $V\\in L_{loc}^{2n}$ for $n = 2$.","Finally, we discuss the unique continuation property for some special cases where $V\\notin L_{loc}^{2n}$, for instance, $V $ is a constant multiple of $ \\frac{1}{|z|}$."],"url":"http://arxiv.org/abs/2404.01947v1","category":"math.CV"}
{"created":"2024-04-02 13:36:03","title":"Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation","abstract":"Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.","sentences":["Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments.","At each navigation step, the agent selects from possible candidate locations and then makes the move.","For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations.","To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost.","To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction.","Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation.","Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.01943v1","category":"cs.CV"}
{"created":"2024-04-02 13:12:40","title":"Particle systems for mean reflected BSDEs with jumps","abstract":"In this paper, we study the mean reflected backward stochastic differential equations with jump (BSDEJs) where the generator only depends on $Y$. We extend the work of Briand and Hibon on the propagation of chaos for mean reflected BSDEs \\cite{briand2021particles} to the jump framework. Besides, we study the reflections for the particle system and obtain the rate of of convergence of the particle system towards the deterministic flat solution to the mean reflected BSDEJ.","sentences":["In this paper, we study the mean reflected backward stochastic differential equations with jump (BSDEJs) where the generator only depends on $Y$. We extend the work of Briand and Hibon on the propagation of chaos for mean reflected BSDEs \\cite{briand2021particles} to the jump framework.","Besides, we study the reflections for the particle system and obtain the rate of of convergence of the particle system towards the deterministic flat solution to the mean reflected BSDEJ."],"url":"http://arxiv.org/abs/2404.01916v1","category":"math.PR"}
{"created":"2024-04-02 12:40:31","title":"Efficient estimation for a smoothing thin plate spline in a two-dimensional space","abstract":"Using a deterministic framework allows us to estimate a function with the purpose of interpolating data in spatial statistics. Radial basis functions are commonly used for scattered data interpolation in a d-dimensional space, however, interpolation problems have to deal with dense matrices. For the case of smoothing thin plate splines, we propose an efficient way to address this problem by compressing the dense matrix by an hierarchical matrix ($\\mathcal{H}$-matrix) and using the conjugate gradient method to solve the linear system of equations. A simulation study was conducted to assess the effectiveness of the spatial interpolation method. The results indicated that employing an $\\mathcal{H}$-matrix along with the conjugate gradient method allows for efficient computations while maintaining a minimal error. We also provide a sensitivity analysis that covers a range of smoothing and compression parameter values, along with a Monte Carlo simulation aimed at quantifying uncertainty in the approximated function. Lastly, we present a comparative study between the proposed approach and thin plate regression using the \"mgcv\" package of the statistical software R. The comparison results demonstrate similar interpolation performance between the two methods.","sentences":["Using a deterministic framework allows us to estimate a function with the purpose of interpolating data in spatial statistics.","Radial basis functions are commonly used for scattered data interpolation in a d-dimensional space, however, interpolation problems have to deal with dense matrices.","For the case of smoothing thin plate splines, we propose an efficient way to address this problem by compressing the dense matrix by an hierarchical matrix ($\\mathcal{H}$-matrix) and using the conjugate gradient method to solve the linear system of equations.","A simulation study was conducted to assess the effectiveness of the spatial interpolation method.","The results indicated that employing an $\\mathcal{H}$-matrix along with the conjugate gradient method allows for efficient computations while maintaining a minimal error.","We also provide a sensitivity analysis that covers a range of smoothing and compression parameter values, along with a Monte Carlo simulation aimed at quantifying uncertainty in the approximated function.","Lastly, we present a comparative study between the proposed approach and thin plate regression using the \"mgcv\" package of the statistical software R. The comparison results demonstrate similar interpolation performance between the two methods."],"url":"http://arxiv.org/abs/2404.01902v1","category":"stat.CO"}
{"created":"2024-04-02 12:40:02","title":"Learning-based model augmentation with LFRs","abstract":"Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems. To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods. These methods have shown better estimation speeds and/or accuracy on unseen data. Among these methods are model augmentation structures. A variety of these structures have been considered in literature, there is however no unifying theory to these. In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure. This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure. Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure. The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper simulation example.","sentences":["Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems.","To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods.","These methods have shown better estimation speeds and/or accuracy on unseen data.","Among these methods are model augmentation structures.","A variety of these structures have been considered in literature, there is however no unifying theory to these.","In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure.","This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure.","Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure.","The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper simulation example."],"url":"http://arxiv.org/abs/2404.01901v1","category":"eess.SY"}
{"created":"2024-04-02 12:34:12","title":"$q$-variational H{\u00f6}rmander functional calculus and Schr{\u00f6}dinger and wave maximal estimates","abstract":"This article is the continuation of the work [DK] where we had proved maximal estimates $$\\left\\|\\sup_{t > 0} |m(tA)f| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f\\|_{L^p(\\Omega,Y)}$$ for sectorial operators $A$ acting on $L^p(\\Omega,Y)$ ($Y$ being a UMD lattice) and admitting a H\\\"ormander functional calculus(a strengthening of the holomorphic $H^\\infty$ calculus to symbols $m$ differentiable on $(0,\\infty)$ in a quantified manner), and $m : (0, \\infty) \\to \\mathbb{C}$ being a H\\\"ormander class symbol with certain decay at $\\infty$.In the present article, we show that under the same conditions as above, the scalar function $t \\mapsto m(tA)f(x,\\omega)$ is of finite $q$-variation with $q > 2$, a.e. $(x,\\omega)$.This extends recent works by [BMSW,HHL,HoMa1,HoMa,JSW,LMX] who have considered among others $m(tA) = e^{-tA}$ the semigroup generated by $-A$.As a consequence, we extend estimates for spherical means in euclidean space from [JSW] to the case of UMD lattice-valued spaces.A second main result yields a maximal estimate $$\\left\\|\\sup_{t > 0} |m(tA) f_t| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f_t\\|_{L^p(\\Omega,Y(\\Lambda^\\beta))}$$ for the same $A$ and similar conditions on $m$ as above but with $f_t$ depending itself on $t$ such that $t \\mapsto f_t(x,\\omega)$ belongs to a Sobolev space $\\Lambda^\\beta$ over $(\\mathbb{R}_+, \\frac{dt}{t})$.We apply this to show a maximal estimate of the Schr\\\"odinger (case $A = -\\Delta$) or wave (case $A = \\sqrt{-\\Delta}$) solution propagator $t \\mapsto \\exp(itA)f$.Then we deduce from it variants of Carleson's problem of pointwise convergence [Car]\\[ \\exp(itA)f(x,\\omega) \\to f(x,\\omega) \\text{ a. e. }(x,\\omega) \\quad (t \\to 0+)\\]for $A$ a Fourier multiplier operator or a differential operator on an open domain $\\Omega \\subseteq \\mathbb{R}^d$ with boundary conditions.","sentences":["This article is the continuation of the work","[DK] where we had proved maximal estimates $$\\left\\|\\sup_{t > 0} |m(tA)f| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f\\|_{L^p(\\Omega,Y)}$$ for sectorial operators $A$ acting on $L^p(\\Omega,Y)$ ($Y$ being a UMD lattice) and admitting a H\\\"ormander functional calculus(a strengthening of the holomorphic $H^\\infty$ calculus to symbols $m$ differentiable on $(0,\\infty)$ in a quantified manner), and $m : (0, \\infty) \\to \\mathbb{C}$ being a H\\\"ormander class symbol with certain decay at $\\infty$.In the present article, we show that under the same conditions as above, the scalar function $t \\mapsto m(tA)f(x,\\omega)$ is of finite $q$-variation with $q > 2$, a.e. $(x,\\omega)$.This extends recent works by [BMSW,HHL,HoMa1,HoMa,JSW,LMX] who have considered among others $m(tA)","= e^{-tA}$ the semigroup generated by $-A$.As a consequence, we extend estimates for spherical means in euclidean space from [JSW] to the case of UMD lattice-valued spaces.","A second main result yields a maximal estimate $$\\left\\|\\sup_{t > 0} |m(tA) f_t| \\right\\|_{L^p(\\Omega,Y)} \\leq C \\|f_t\\|_{L^p(\\Omega,Y(\\Lambda^\\beta))}$$ for the same $A$ and similar conditions on $m$ as above but with $f_t$ depending itself on $t$ such that $t \\mapsto f_t(x,\\omega)$ belongs to a Sobolev space $\\Lambda^\\beta$ over $(\\mathbb{R}_+, \\frac{dt}{t})$.We apply this to show a maximal estimate of the Schr\\\"odinger (case $A = -\\Delta$) or wave (case $A = \\sqrt{-\\Delta}$) solution propagator $t \\mapsto \\exp(itA)f$.Then we deduce from it variants of Carleson's problem of pointwise convergence [Car]\\[ \\exp(itA)f(x,\\omega) \\to f(x,\\omega) \\text{ a. e. }(x,\\omega) \\quad (t \\to 0+)\\]for $A$ a Fourier multiplier operator or a differential operator on an open domain $\\Omega \\subseteq \\mathbb{R}^d$ with boundary conditions."],"url":"http://arxiv.org/abs/2404.01893v1","category":"math.CA"}
{"created":"2024-04-02 12:29:31","title":"Minimize Quantization Output Error with Bias Compensation","abstract":"Quantization is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment. In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision quantization without model fine-tuning. Instead of optimizing the non-convex quantization process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation. We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or fine-tuning. We conduct extensive experiments on Vision Transformer models and Large Language Models, and the results show that our method notably reduces quantization output error, thereby permitting ultra-low-precision post-training quantization and enhancing the task performance of models. Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the perplexity of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in https://github.com/GongCheng1919/bias-compensation.","sentences":["Quantization is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment.","In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision quantization without model fine-tuning.","Instead of optimizing the non-convex quantization process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation.","We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or fine-tuning.","We conduct extensive experiments on Vision Transformer models and Large Language Models, and the results show that our method notably reduces quantization output error, thereby permitting ultra-low-precision post-training quantization and enhancing the task performance of models.","Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the perplexity of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in https://github.com/GongCheng1919/bias-compensation."],"url":"http://arxiv.org/abs/2404.01892v1","category":"cs.CV"}
{"created":"2024-04-02 12:16:35","title":"Comparison of Different Elastic Strain Definitions for Largely Deformed SEI of Chemo-Mechanically Coupled Silicon Battery Particles","abstract":"Amorphous silicon is a highly promising anode material for next-generation lithium-ion batteries. Large volume changes of the silicon particle have a critical effect on the surrounding solid-electrolyte interphase (SEI) due to repeated fracture and healing during cycling. Based on a thermodynamically consistent chemo-elasto-plastic continuum model we investigate the stress development inside the particle and the SEI. Using the example of a particle with SEI, we apply a higher order finite element method together with a variable-step, variable-order time integration scheme on a nonlinear system of partial differential equations. Starting from a single silicon particle setting, the surrounding SEI is added in a first step with the typically used elastic Green--St-Venant (GSV) strain definition for a purely elastic deformation. For this type of deformation, the definition of the elastic strain is crucial to get reasonable simulation results. In case of the elastic GSV strain, the simulation aborts. We overcome the simulation failure by using the definition of the logarithmic Hencky strain. However, the particle remains unaffected by the elastic strain definitions in the particle domain. Compared to GSV, plastic deformation with the Hencky strain is straightforward to take into account. For the plastic SEI deformation, a rate-independent and a rate-dependent plastic deformation are newly introduced and numerically compared for three half cycles for the example of a radial symmetric particle.","sentences":["Amorphous silicon is a highly promising anode material for next-generation lithium-ion batteries.","Large volume changes of the silicon particle have a critical effect on the surrounding solid-electrolyte interphase (SEI) due to repeated fracture and healing during cycling.","Based on a thermodynamically consistent chemo-elasto-plastic continuum model we investigate the stress development inside the particle and the SEI.","Using the example of a particle with SEI, we apply a higher order finite element method together with a variable-step, variable-order time integration scheme on a nonlinear system of partial differential equations.","Starting from a single silicon particle setting, the surrounding SEI is added in a first step with the typically used elastic Green--St-Venant (GSV) strain definition for a purely elastic deformation.","For this type of deformation, the definition of the elastic strain is crucial to get reasonable simulation results.","In case of the elastic GSV strain, the simulation aborts.","We overcome the simulation failure by using the definition of the logarithmic Hencky strain.","However, the particle remains unaffected by the elastic strain definitions in the particle domain.","Compared to GSV, plastic deformation with the Hencky strain is straightforward to take into account.","For the plastic SEI deformation, a rate-independent and a rate-dependent plastic deformation are newly introduced and numerically compared for three half cycles for the example of a radial symmetric particle."],"url":"http://arxiv.org/abs/2404.01884v1","category":"math.NA"}
{"created":"2024-04-02 12:10:03","title":"Monodromy of generalized Lame equations with Darboux-Treibich-Verdier potentials: A universal law","abstract":"The Darboux-Treibich-Verdier (DTV) potential $\\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)$ is well-known as doubly-periodic solutions of the stationary KdV hierarchy (Treibich-Verdier, Duke Math. J. {\\bf 68} (1992), 217-236). In this paper, we study the generalized Lam\\'{e} equation with the DTV potential \\begin{equation*} y^{\\prime \\prime }(z)=\\bigg[ \\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)+B\\bigg] y(z),\\quad n_{k}\\in \\mathbb{N} \\end{equation*} from the monodromy aspect. We prove that the map from $(\\tau, B)$ to the monodromy data $(r,s)$ satisfies a surprising universal law $d\\tau\\wedge dB\\equiv8\\pi^2 dr\\wedge ds.$ Our proof applies Panlev\\'{e} VI equation and modular forms. We also give applications to the algebraic multiplicity of (anti)periodic eigenvalues for the associated Hill operator.","sentences":["The Darboux-Treibich-Verdier (DTV) potential $\\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)$ is well-known as doubly-periodic solutions of the stationary KdV hierarchy (Treibich-Verdier, Duke Math.","J. {\\bf 68} (1992), 217-236).","In this paper, we study the generalized Lam\\'{e} equation with the DTV potential \\begin{equation*} y^{\\prime \\prime }(z)=\\bigg[ \\sum_{k=0}^{3}n_{k}(n_{k}+1)\\wp(z+\\tfrac{ \\omega_{k}}{2};\\tau)+B\\bigg] y(z),\\quad n_{k}\\in \\mathbb{N} \\end{equation*} from the monodromy aspect.","We prove that the map from $(\\tau, B)$ to the monodromy data $(r,s)$ satisfies a surprising universal law $d\\tau\\wedge dB\\equiv8\\pi^2 dr\\wedge ds.$ Our proof applies Panlev\\'{e} VI equation and modular forms.","We also give applications to the algebraic multiplicity of (anti)periodic eigenvalues for the associated Hill operator."],"url":"http://arxiv.org/abs/2404.01879v1","category":"math.CA"}
{"created":"2024-04-02 11:50:03","title":"On the reduction of Linear Parameter-Varying State-Space models","abstract":"This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and benchmarking their capabilities, limitations and performance. The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions. For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection. For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, autoencoders and deep neural network. The comparison reveals the most suitable reduction methods for the different benchmark configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model.","sentences":["This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and benchmarking their capabilities, limitations and performance.","The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions.","For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection.","For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, autoencoders and deep neural network.","The comparison reveals the most suitable reduction methods for the different benchmark configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model."],"url":"http://arxiv.org/abs/2404.01871v1","category":"eess.SY"}
{"created":"2024-04-02 11:44:37","title":"Supervised Autoencoder MLP for Financial Time Series Forecasting","abstract":"This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance. It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios. The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022. Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness. However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning. This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling. The results of this study have substantial policy implications, suggesting that financial institutions and regulators could leverage techniques presented to enhance market stability and investor protection, while also encouraging more informed and strategic investment approaches in various financial sectors.","sentences":["This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance.","It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios.","The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022.","Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness.","However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning.","This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling.","The results of this study have substantial policy implications, suggesting that financial institutions and regulators could leverage techniques presented to enhance market stability and investor protection, while also encouraging more informed and strategic investment approaches in various financial sectors."],"url":"http://arxiv.org/abs/2404.01866v1","category":"q-fin.TR"}
{"created":"2024-04-03 16:10:17","title":"\"Are Adversarial Phishing Webpages a Threat in Reality?\" Understanding the Users' Perception of Adversarial Webpages","abstract":"Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation. Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages. However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing -- the end users. In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD. Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t. unperturbed ones. However, not all adversarial perturbations are equally effective. For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background). We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence. We release our resources.","sentences":["Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation.","Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages.","However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing -- the end users.","In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD.","Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t.","unperturbed ones.","However, not all adversarial perturbations are equally effective.","For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background).","We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence.","We release our resources."],"url":"http://arxiv.org/abs/2404.02832v1","category":"cs.CR"}
{"created":"2024-04-03 15:59:42","title":"BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models","abstract":"This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is available at https://github.com/Ledzy/BAdam.","sentences":["This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver.","BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property.","Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU.","The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO.","Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO.","Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark.","The results demonstrate that BAdam is capable of narrowing the performance gap with Adam.","Our code is available at https://github.com/Ledzy/BAdam."],"url":"http://arxiv.org/abs/2404.02827v1","category":"cs.LG"}
{"created":"2024-04-03 12:56:20","title":"Probing intractable beyond-standard-model parameter spaces armed with Machine Learning","abstract":"This article attempts to summarize the effort by the particle physics community in addressing the tedious work of determining the parameter spaces of beyond-the-standard-model (BSM) scenarios, allowed by data. These spaces, typically associated with a large number of dimensions, especially in the presence of nuisance parameters, suffer from the curse of dimensionality and thus render naive sampling of any kind -- even the computationally inexpensive ones -- ineffective. Over the years, various new sampling (from variations of Markov Chain Monte Carlo (MCMC) to dynamic nested sampling) and machine learning (ML) algorithms have been adopted by the community to alleviate this issue. If not all, we discuss potentially the most important among them and the significance of their results, in detail.","sentences":["This article attempts to summarize the effort by the particle physics community in addressing the tedious work of determining the parameter spaces of beyond-the-standard-model (BSM) scenarios, allowed by data.","These spaces, typically associated with a large number of dimensions, especially in the presence of nuisance parameters, suffer from the curse of dimensionality and thus render naive sampling of any kind -- even the computationally inexpensive ones -- ineffective.","Over the years, various new sampling (from variations of Markov Chain Monte Carlo (MCMC) to dynamic nested sampling) and machine learning (ML) algorithms have been adopted by the community to alleviate this issue.","If not all, we discuss potentially the most important among them and the significance of their results, in detail."],"url":"http://arxiv.org/abs/2404.02698v1","category":"hep-ph"}
{"created":"2024-04-03 12:21:41","title":"Independently Keypoint Learning for Small Object Semantic Correspondence","abstract":"Semantic correspondence remains a challenging task for establishing correspondences between a pair of images with the same category or similar scenes due to the large intra-class appearance. In this paper, we introduce a novel problem called 'Small Object Semantic Correspondence (SOSC).' This problem is challenging due to the close proximity of keypoints associated with small objects, which results in the fusion of these respective features. It is difficult to identify the corresponding key points of the fused features, and it is also difficult to be recognized. To address this challenge, we propose the Keypoint Bounding box-centered Cropping (KBC) method, which aims to increase the spatial separation between keypoints of small objects, thereby facilitating independent learning of these keypoints. The KBC method is seamlessly integrated into our proposed inference pipeline and can be easily incorporated into other methodologies, resulting in significant performance enhancements. Additionally, we introduce a novel framework, named KBCNet, which serves as our baseline model. KBCNet comprises a Cross-Scale Feature Alignment (CSFA) module and an efficient 4D convolutional decoder. The CSFA module is designed to align multi-scale features, enriching keypoint representations by integrating fine-grained features and deep semantic features. Meanwhile, the 4D convolutional decoder, based on efficient 4D convolution, ensures efficiency and rapid convergence. To empirically validate the effectiveness of our proposed methodology, extensive experiments are conducted on three widely used benchmarks: PF-PASCAL, PF-WILLOW, and SPair-71k. Our KBC method demonstrates a substantial performance improvement of 7.5\\% on the SPair-71K dataset, providing compelling evidence of its efficacy.","sentences":["Semantic correspondence remains a challenging task for establishing correspondences between a pair of images with the same category or similar scenes due to the large intra-class appearance.","In this paper, we introduce a novel problem called 'Small Object Semantic Correspondence (SOSC).'","This problem is challenging due to the close proximity of keypoints associated with small objects, which results in the fusion of these respective features.","It is difficult to identify the corresponding key points of the fused features, and it is also difficult to be recognized.","To address this challenge, we propose the Keypoint Bounding box-centered Cropping (KBC) method, which aims to increase the spatial separation between keypoints of small objects, thereby facilitating independent learning of these keypoints.","The KBC method is seamlessly integrated into our proposed inference pipeline and can be easily incorporated into other methodologies, resulting in significant performance enhancements.","Additionally, we introduce a novel framework, named KBCNet, which serves as our baseline model.","KBCNet comprises a Cross-Scale Feature Alignment (CSFA) module and an efficient 4D convolutional decoder.","The CSFA module is designed to align multi-scale features, enriching keypoint representations by integrating fine-grained features and deep semantic features.","Meanwhile, the 4D convolutional decoder, based on efficient 4D convolution, ensures efficiency and rapid convergence.","To empirically validate the effectiveness of our proposed methodology, extensive experiments are conducted on three widely used benchmarks: PF-PASCAL, PF-WILLOW, and SPair-71k.","Our KBC method demonstrates a substantial performance improvement of 7.5\\% on the SPair-71K dataset, providing compelling evidence of its efficacy."],"url":"http://arxiv.org/abs/2404.02678v1","category":"cs.CV"}
{"created":"2024-04-03 12:12:18","title":"Bayesian Bi-level Sparse Group Regressions for Macroeconomic Forecasting","abstract":"We propose a Machine Learning approach for optimal macroeconomic forecasting in a high-dimensional setting with covariates presenting a known group structure. Our model encompasses forecasting settings with many series, mixed frequencies, and unknown nonlinearities. We introduce in time-series econometrics the concept of bi-level sparsity, i.e. sparsity holds at both the group level and within groups, and we assume the true model satisfies this assumption. We propose a prior that induces bi-level sparsity, and the corresponding posterior distribution is demonstrated to contract at the minimax-optimal rate, recover the model parameters, and have a support that includes the support of the model asymptotically. Our theory allows for correlation between groups, while predictors in the same group can be characterized by strong covariation as well as common characteristics and patterns. Finite sample performance is illustrated through comprehensive Monte Carlo experiments and a real-data nowcasting exercise of the US GDP growth rate.","sentences":["We propose a Machine Learning approach for optimal macroeconomic forecasting in a high-dimensional setting with covariates presenting a known group structure.","Our model encompasses forecasting settings with many series, mixed frequencies, and unknown nonlinearities.","We introduce in time-series econometrics the concept of bi-level sparsity, i.e. sparsity holds at both the group level and within groups, and we assume the true model satisfies this assumption.","We propose a prior that induces bi-level sparsity, and the corresponding posterior distribution is demonstrated to contract at the minimax-optimal rate, recover the model parameters, and have a support that includes the support of the model asymptotically.","Our theory allows for correlation between groups, while predictors in the same group can be characterized by strong covariation as well as common characteristics and patterns.","Finite sample performance is illustrated through comprehensive Monte Carlo experiments and a real-data nowcasting exercise of the US GDP growth rate."],"url":"http://arxiv.org/abs/2404.02671v1","category":"econ.EM"}
{"created":"2024-04-03 10:22:35","title":"Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models","abstract":"Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of reasoning with enough structure and regularity to identify and quantify systematic reasoning failures in widely-used models. In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on the entailment label is mediated by the relation between these words). Extending related work on causal analysis of NLP models in different settings, we perform an extensive interventional study on the NLI task to investigate robustness to irrelevant changes and sensitivity to impactful changes of Transformers. The results strongly bolster the fact that similar benchmark accuracy scores may be observed for models that exhibit very different behaviour. Moreover, our methodology reinforces previously suspected biases from a causal perspective, including biases in favour of upward-monotone contexts and ignoring the effects of negation markers.","sentences":["Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems.","However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of reasoning with enough structure and regularity to identify and quantify systematic reasoning failures in widely-used models.","In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context.","In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on the entailment label is mediated by the relation between these words).","Extending related work on causal analysis of NLP models in different settings, we perform an extensive interventional study on the NLI task to investigate robustness to irrelevant changes and sensitivity to impactful changes of Transformers.","The results strongly bolster the fact that similar benchmark accuracy scores may be observed for models that exhibit very different behaviour.","Moreover, our methodology reinforces previously suspected biases from a causal perspective, including biases in favour of upward-monotone contexts and ignoring the effects of negation markers."],"url":"http://arxiv.org/abs/2404.02622v1","category":"cs.CL"}
{"created":"2024-04-03 08:47:40","title":"Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution","abstract":"Knowledge distillation (KD) is a promising yet challenging model compression technique that transfers rich learning representations from a well-performing but cumbersome teacher model to a compact student model. Previous methods for image super-resolution (SR) mostly compare the feature maps directly or after standardizing the dimensions with basic algebraic operations (e.g. average, dot-product). However, the intrinsic semantic differences among feature maps are overlooked, which are caused by the disparate expressive capacity between the networks. This work presents MiPKD, a multi-granularity mixture of prior KD framework, to facilitate efficient SR model through the feature mixture in a unified latent space and stochastic network block mixture. Extensive experiments demonstrate the effectiveness of the proposed MiPKD method.","sentences":["Knowledge distillation (KD) is a promising yet challenging model compression technique that transfers rich learning representations from a well-performing but cumbersome teacher model to a compact student model.","Previous methods for image super-resolution (SR) mostly compare the feature maps directly or after standardizing the dimensions with basic algebraic operations (e.g. average, dot-product).","However, the intrinsic semantic differences among feature maps are overlooked, which are caused by the disparate expressive capacity between the networks.","This work presents MiPKD, a multi-granularity mixture of prior KD framework, to facilitate efficient SR model through the feature mixture in a unified latent space and stochastic network block mixture.","Extensive experiments demonstrate the effectiveness of the proposed MiPKD method."],"url":"http://arxiv.org/abs/2404.02573v1","category":"cs.CV"}
{"created":"2024-04-03 07:31:53","title":"A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality","abstract":"Learning argumentative writing is challenging. Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality essays. To support argumentative writing computationally, one step is to mine the argumentative structure. When combined with automatic essay scoring, interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support. Although studies have shown the usefulness of using information about the argumentative structure for essay scoring, no argument mining corpus with ground-truth essay quality annotations has been published yet. Moreover, none of the existing corpora contain essays written by school students specifically. To fill this research gap, we present a German corpus of 1,320 essays from school students of two age groups. Each essay has been manually annotated for argumentative structure and quality on multiple levels of granularity. We propose baseline approaches to argument mining and essay scoring, and we analyze interactions between both tasks, thereby laying the ground for quality-oriented argumentative writing support.","sentences":["Learning argumentative writing is challenging.","Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality essays.","To support argumentative writing computationally, one step is to mine the argumentative structure.","When combined with automatic essay scoring, interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support.","Although studies have shown the usefulness of using information about the argumentative structure for essay scoring, no argument mining corpus with ground-truth essay quality annotations has been published yet.","Moreover, none of the existing corpora contain essays written by school students specifically.","To fill this research gap, we present a German corpus of 1,320 essays from school students of two age groups.","Each essay has been manually annotated for argumentative structure and quality on multiple levels of granularity.","We propose baseline approaches to argument mining and essay scoring, and we analyze interactions between both tasks, thereby laying the ground for quality-oriented argumentative writing support."],"url":"http://arxiv.org/abs/2404.02529v1","category":"cs.CL"}
{"created":"2024-04-03 06:51:49","title":"Lifelong Event Detection with Embedding Space Separation and Compaction","abstract":"To mitigate forgetting, existing lifelong event detection methods typically maintain a memory module and replay the stored memory data during the learning of a new task. However, the simple combination of memory data and new-task samples can still result in substantial forgetting of previously acquired knowledge, which may occur due to the potential overlap between the feature distribution of new data and the previously learned embedding space. Moreover, the model suffers from overfitting on the few memory samples rather than effectively remembering learned patterns. To address the challenges of forgetting and overfitting, we propose a novel method based on embedding space separation and compaction. Our method alleviates forgetting of previously learned tasks by forcing the feature distribution of new data away from the previous embedding space. It also mitigates overfitting by a memory calibration mechanism that encourages memory data to be close to its prototype to enhance intra-class compactness. In addition, the learnable parameters of the new task are initialized by drawing upon acquired knowledge from the previously learned task to facilitate forward knowledge transfer. With extensive experiments, we demonstrate that our method can significantly outperform previous state-of-the-art approaches.","sentences":["To mitigate forgetting, existing lifelong event detection methods typically maintain a memory module and replay the stored memory data during the learning of a new task.","However, the simple combination of memory data and new-task samples can still result in substantial forgetting of previously acquired knowledge, which may occur due to the potential overlap between the feature distribution of new data and the previously learned embedding space.","Moreover, the model suffers from overfitting on the few memory samples rather than effectively remembering learned patterns.","To address the challenges of forgetting and overfitting, we propose a novel method based on embedding space separation and compaction.","Our method alleviates forgetting of previously learned tasks by forcing the feature distribution of new data away from the previous embedding space.","It also mitigates overfitting by a memory calibration mechanism that encourages memory data to be close to its prototype to enhance intra-class compactness.","In addition, the learnable parameters of the new task are initialized by drawing upon acquired knowledge from the previously learned task to facilitate forward knowledge transfer.","With extensive experiments, we demonstrate that our method can significantly outperform previous state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.02507v1","category":"cs.CL"}
{"created":"2024-04-03 04:21:27","title":"MOPAR: A Model Partitioning Framework for Deep Learning Inference Services on Serverless Platforms","abstract":"With its elastic power and a pay-as-you-go cost model, the deployment of deep learning inference services (DLISs) on serverless platforms is emerging as a prevalent trend. However, the varying resource requirements of different layers in DL models hinder resource utilization and increase costs, when DLISs are deployed as a single function on serverless platforms. To tackle this problem, we propose a model partitioning framework called MOPAR. This work is based on the two resource usage patterns of DLISs: global differences and local similarity, due to the presence of resource dominant (RD) operators and layer stacking. Considering these patterns, MOPAR adopts a hybrid approach that initially divides the DL model vertically into multiple slices composed of similar layers to improve resource efficiency. Slices containing RD operators are further partitioned into multiple sub-slices, enabling parallel optimization to reduce inference latency. Moreover, MOPAR comprehensively employs data compression and share-memory techniques to offset the additional time introduced by communication between slices. We implement a prototype of MOPAR and evaluate its efficacy using four categories of 12 DL models on OpenFaaS and AWS Lambda. The experiment results show that MOPAR can improve the resource efficiency of DLISs by 27.62\\% on average, while reducing latency by about 5.52\\%. Furthermore, based on Lambda's pricing, the cost of running DLISs is reduced by about 2.58 $\\times$ using MOPAR.","sentences":["With its elastic power and a pay-as-you-go cost model, the deployment of deep learning inference services (DLISs) on serverless platforms is emerging as a prevalent trend.","However, the varying resource requirements of different layers in DL models hinder resource utilization and increase costs, when DLISs are deployed as a single function on serverless platforms.","To tackle this problem, we propose a model partitioning framework called MOPAR.","This work is based on the two resource usage patterns of DLISs: global differences and local similarity, due to the presence of resource dominant (RD) operators and layer stacking.","Considering these patterns, MOPAR adopts a hybrid approach that initially divides the DL model vertically into multiple slices composed of similar layers to improve resource efficiency.","Slices containing RD operators are further partitioned into multiple sub-slices, enabling parallel optimization to reduce inference latency.","Moreover, MOPAR comprehensively employs data compression and share-memory techniques to offset the additional time introduced by communication between slices.","We implement a prototype of MOPAR and evaluate its efficacy using four categories of 12 DL models on OpenFaaS and AWS Lambda.","The experiment results show that MOPAR can improve the resource efficiency of DLISs by 27.62\\% on average, while reducing latency by about 5.52\\%.","Furthermore, based on Lambda's pricing, the cost of running DLISs is reduced by about 2.58 $\\times$ using MOPAR."],"url":"http://arxiv.org/abs/2404.02445v1","category":"cs.DC"}
{"created":"2024-04-03 03:44:25","title":"Physics Faculty and Empathy in Academic Spaces","abstract":"The ability to emotionally or intellectually understand another person's thoughts and feelings-empathy-can foster critical connections that facilitate learning and collaboration. We present a case study of physics faculty that examines their experiences empathizing with students, both in and outside of the classroom. We expand on frameworks for understanding the empathy process by identifying key mediating factors, and note various barriers that faculty express as preventing them from taking empathetic action. Our analysis unpacks the mechanisms of communication and contextual information, which play key roles in the empathetic process, with implications for programs that rely on empathy to develop more inclusive STEM academic spaces.","sentences":["The ability to emotionally or intellectually understand another person's thoughts and feelings-empathy-can foster critical connections that facilitate learning and collaboration.","We present a case study of physics faculty that examines their experiences empathizing with students, both in and outside of the classroom.","We expand on frameworks for understanding the empathy process by identifying key mediating factors, and note various barriers that faculty express as preventing them from taking empathetic action.","Our analysis unpacks the mechanisms of communication and contextual information, which play key roles in the empathetic process, with implications for programs that rely on empathy to develop more inclusive STEM academic spaces."],"url":"http://arxiv.org/abs/2404.02434v1","category":"physics.ed-ph"}
{"created":"2024-04-03 02:40:35","title":"What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases","abstract":"Vision-language (VL) models, pretrained on colossal image-text datasets, have attained broad VL competence that is difficult to evaluate. A common belief is that a small number of VL skills underlie the variety of VL tests. In this paper, we perform a large-scale transfer learning experiment aimed at discovering latent VL skills from data. We reveal interesting characteristics that have important implications for test suite design. First, generation tasks suffer from a length bias, suggesting benchmarks should balance tasks with varying output lengths. Second, we demonstrate that factor analysis successfully identifies reasonable yet surprising VL skill factors, suggesting benchmarks could leverage similar analyses for task selection. Finally, we present a new dataset, OLIVE (https://github.com/jq-zh/olive-dataset), which simulates user instructions in the wild and presents challenges dissimilar to all datasets we tested. Our findings contribute to the design of balanced and broad-coverage vision-language evaluation methods.","sentences":["Vision-language (VL) models, pretrained on colossal image-text datasets, have attained broad VL competence that is difficult to evaluate.","A common belief is that a small number of VL skills underlie the variety of VL tests.","In this paper, we perform a large-scale transfer learning experiment aimed at discovering latent VL skills from data.","We reveal interesting characteristics that have important implications for test suite design.","First, generation tasks suffer from a length bias, suggesting benchmarks should balance tasks with varying output lengths.","Second, we demonstrate that factor analysis successfully identifies reasonable yet surprising VL skill factors, suggesting benchmarks could leverage similar analyses for task selection.","Finally, we present a new dataset, OLIVE (https://github.com/jq-zh/olive-dataset), which simulates user instructions in the wild and presents challenges dissimilar to all datasets we tested.","Our findings contribute to the design of balanced and broad-coverage vision-language evaluation methods."],"url":"http://arxiv.org/abs/2404.02415v1","category":"cs.CV"}
{"created":"2024-04-03 02:26:15","title":"TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes","abstract":"Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian's properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios.","sentences":["Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data.","In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis.","TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting.","3D Gaussian's properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features.","During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry.","Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's state-of-the-art (SOTA) performance.","Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios."],"url":"http://arxiv.org/abs/2404.02410v1","category":"cs.CV"}
{"created":"2024-04-03 02:12:29","title":"Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT","abstract":"This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities and a broad understanding of general knowledge, they often lag behind smaller pre-trained models fine-tuned specifically for particular tasks. Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5. These results highlight the significant potential for enhancing LLM performance in the Persian language. This is particularly noteworthy due to the unique attributes of Persian, including its distinct alphabet and writing styles.","sentences":["This paper explores the efficacy of large language models (LLMs) for Persian.","While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question.","We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks.","Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation.","Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains.","To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models.","Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades.","Our findings reveal that while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities and a broad understanding of general knowledge, they often lag behind smaller pre-trained models fine-tuned specifically for particular tasks.","Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5.","These results highlight the significant potential for enhancing LLM performance in the Persian language.","This is particularly noteworthy due to the unique attributes of Persian, including its distinct alphabet and writing styles."],"url":"http://arxiv.org/abs/2404.02403v1","category":"cs.CL"}
{"created":"2024-04-03 01:55:15","title":"Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint","abstract":"Diffusion models have been popular for point cloud generation tasks. Existing works utilize the forward diffusion process to convert the original point distribution into a noise distribution and then learn the reverse diffusion process to recover the point distribution from the noise distribution. However, the reverse diffusion process can produce samples with non-smooth points on the surface because of the ignorance of the point cloud geometric properties. We propose alleviating the problem by incorporating the local smoothness constraint into the diffusion framework for point cloud generation. Experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds, outperforming multiple state-of-the-art methods.","sentences":["Diffusion models have been popular for point cloud generation tasks.","Existing works utilize the forward diffusion process to convert the original point distribution into a noise distribution and then learn the reverse diffusion process to recover the point distribution from the noise distribution.","However, the reverse diffusion process can produce samples with non-smooth points on the surface because of the ignorance of the point cloud geometric properties.","We propose alleviating the problem by incorporating the local smoothness constraint into the diffusion framework for point cloud generation.","Experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds, outperforming multiple state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.02396v1","category":"cs.CV"}
{"created":"2024-04-03 01:36:27","title":"Cohort-Individual Cooperative Learning for Multimodal Cancer Survival Analysis","abstract":"Recently, we have witnessed impressive achievements in cancer survival analysis by integrating multimodal data, e.g., pathology images and genomic profiles. However, the heterogeneity and high dimensionality of these modalities pose significant challenges for extracting discriminative representations while maintaining good generalization. In this paper, we propose a Cohort-individual Cooperative Learning (CCL) framework to advance cancer survival analysis by collaborating knowledge decomposition and cohort guidance. Specifically, first, we propose a Multimodal Knowledge Decomposition (MKD) module to explicitly decompose multimodal knowledge into four distinct components: redundancy, synergy and uniqueness of the two modalities. Such a comprehensive decomposition can enlighten the models to perceive easily overlooked yet important information, facilitating an effective multimodal fusion. Second, we propose a Cohort Guidance Modeling (CGM) to mitigate the risk of overfitting task-irrelevant information. It can promote a more comprehensive and robust understanding of the underlying multimodal data, while avoiding the pitfalls of overfitting and enhancing the generalization ability of the model. By cooperating the knowledge decomposition and cohort guidance methods, we develop a robust multimodal survival analysis model with enhanced discrimination and generalization abilities. Extensive experimental results on five cancer datasets demonstrate the effectiveness of our model in integrating multimodal data for survival analysis.","sentences":["Recently, we have witnessed impressive achievements in cancer survival analysis by integrating multimodal data, e.g., pathology images and genomic profiles.","However, the heterogeneity and high dimensionality of these modalities pose significant challenges for extracting discriminative representations while maintaining good generalization.","In this paper, we propose a Cohort-individual Cooperative Learning (CCL) framework to advance cancer survival analysis by collaborating knowledge decomposition and cohort guidance.","Specifically, first, we propose a Multimodal Knowledge Decomposition (MKD) module to explicitly decompose multimodal knowledge into four distinct components: redundancy, synergy and uniqueness of the two modalities.","Such a comprehensive decomposition can enlighten the models to perceive easily overlooked yet important information, facilitating an effective multimodal fusion.","Second, we propose a Cohort Guidance Modeling (CGM) to mitigate the risk of overfitting task-irrelevant information.","It can promote a more comprehensive and robust understanding of the underlying multimodal data, while avoiding the pitfalls of overfitting and enhancing the generalization ability of the model.","By cooperating the knowledge decomposition and cohort guidance methods, we develop a robust multimodal survival analysis model with enhanced discrimination and generalization abilities.","Extensive experimental results on five cancer datasets demonstrate the effectiveness of our model in integrating multimodal data for survival analysis."],"url":"http://arxiv.org/abs/2404.02394v1","category":"eess.IV"}
{"created":"2024-04-02 22:58:38","title":"Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors","abstract":"Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NPoE effectively defends against a variety of triggers both separately and in trigger mixtures. Due to the versatility of the MoE structure in NPoE, this framework can be further expanded to defend against other attack settings","sentences":["Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance.","Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored.","In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types.","During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers.","At inference time, only the main model is used.","Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NPoE effectively defends against a variety of triggers both separately and in trigger mixtures.","Due to the versatility of the MoE structure in NPoE, this framework can be further expanded to defend against other attack settings"],"url":"http://arxiv.org/abs/2404.02356v1","category":"cs.CL"}
{"created":"2024-04-02 22:37:34","title":"Effective Malware Detection for Embedded Computing Systems with Limited Exposure","abstract":"One of the pivotal security threats for the embedded computing systems is malicious software a.k.a malware. With efficiency and efficacy, Machine Learning (ML) has been widely adopted for malware detection in recent times. Despite being efficient, the existing techniques require a tremendous number of benign and malware samples for training and modeling an efficient malware detector. Furthermore, such constraints limit the detection of emerging malware samples due to the lack of sufficient malware samples required for efficient training. To address such concerns, we introduce a code-aware data generation technique that generates multiple mutated samples of the limitedly seen malware by the devices. Loss minimization ensures that the generated samples closely mimic the limitedly seen malware and mitigate the impractical samples. Such developed malware is further incorporated into the training set to formulate the model that can efficiently detect the emerging malware despite having limited exposure. The experimental results demonstrates that the proposed technique achieves an accuracy of 90% in detecting limitedly seen malware, which is approximately 3x more than the accuracy attained by state-of-the-art techniques.","sentences":["One of the pivotal security threats for the embedded computing systems is malicious software a.k.a malware.","With efficiency and efficacy, Machine Learning (ML) has been widely adopted for malware detection in recent times.","Despite being efficient, the existing techniques require a tremendous number of benign and malware samples for training and modeling an efficient malware detector.","Furthermore, such constraints limit the detection of emerging malware samples due to the lack of sufficient malware samples required for efficient training.","To address such concerns, we introduce a code-aware data generation technique that generates multiple mutated samples of the limitedly seen malware by the devices.","Loss minimization ensures that the generated samples closely mimic the limitedly seen malware and mitigate the impractical samples.","Such developed malware is further incorporated into the training set to formulate the model that can efficiently detect the emerging malware despite having limited exposure.","The experimental results demonstrates that the proposed technique achieves an accuracy of 90% in detecting limitedly seen malware, which is approximately 3x more than the accuracy attained by state-of-the-art techniques."],"url":"http://arxiv.org/abs/2404.02344v1","category":"cs.CR"}
{"created":"2024-04-02 21:51:39","title":"Heat Death of Generative Models in Closed-Loop Learning","abstract":"Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as \"knowledge\", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been only limited theoretical understanding of this process, in part due to the complexity of the deep networks underlying these generative models.   The aim of this paper is to provide insights into this process (that we refer to as \"generative closed-loop learning\") by studying the learning dynamics of generative models that are fed back their own produced content in addition to their original training dataset. The sampling of many of these models can be controlled via a \"temperature\" parameter. Using dynamical systems tools, we show that, unless a sufficient amount of external data is introduced at each iteration, any non-trivial temperature leads the model to asymptotically degenerate. In fact, either the generative distribution collapses to a small set of outputs, or becomes uniform over a large set of outputs.","sentences":["Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.","As generative models become widespread, data they generate is incorporated into shared content through the public web.","This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns.","This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as \"knowledge\", remains stable or collapses.   ","Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating.","Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse).","So far there has been only limited theoretical understanding of this process, in part due to the complexity of the deep networks underlying these generative models.   ","The aim of this paper is to provide insights into this process (that we refer to as \"generative closed-loop learning\") by studying the learning dynamics of generative models that are fed back their own produced content in addition to their original training dataset.","The sampling of many of these models can be controlled via a \"temperature\" parameter.","Using dynamical systems tools, we show that, unless a sufficient amount of external data is introduced at each iteration, any non-trivial temperature leads the model to asymptotically degenerate.","In fact, either the generative distribution collapses to a small set of outputs, or becomes uniform over a large set of outputs."],"url":"http://arxiv.org/abs/2404.02325v1","category":"cs.LG"}
{"created":"2024-04-02 19:48:46","title":"Multi-Objective Bayesian Active Learning for MeV-ultrafast electron diffraction","abstract":"Ultrafast electron diffraction using MeV energy beams(MeV-UED) has enabled unprecedented scientific opportunities in the study of ultrafast structural dynamics in a variety of gas, liquid and solid state systems. Broad scientific applications usually pose different requirements for electron probe properties. Due to the complex, nonlinear and correlated nature of accelerator systems, electron beam property optimization is a time-taking process and often relies on extensive hand-tuning by experienced human operators. Algorithm based efficient online tuning strategies are highly desired. Here, we demonstrate multi-objective Bayesian active learning for speeding up online beam tuning at the SLAC MeV-UED facility. The multi-objective Bayesian optimization algorithm was used for efficiently searching the parameter space and mapping out the Pareto Fronts which give the trade-offs between key beam properties. Such scheme enables an unprecedented overview of the global behavior of the experimental system and takes a significantly smaller number of measurements compared with traditional methods such as a grid scan. This methodology can be applied in other experimental scenarios that require simultaneously optimizing multiple objectives by explorations in high dimensional, nonlinear and correlated systems.","sentences":["Ultrafast electron diffraction using MeV energy beams(MeV-UED) has enabled unprecedented scientific opportunities in the study of ultrafast structural dynamics in a variety of gas, liquid and solid state systems.","Broad scientific applications usually pose different requirements for electron probe properties.","Due to the complex, nonlinear and correlated nature of accelerator systems, electron beam property optimization is a time-taking process and often relies on extensive hand-tuning by experienced human operators.","Algorithm based efficient online tuning strategies are highly desired.","Here, we demonstrate multi-objective Bayesian active learning for speeding up online beam tuning at the SLAC MeV-UED facility.","The multi-objective Bayesian optimization algorithm was used for efficiently searching the parameter space and mapping out the Pareto Fronts which give the trade-offs between key beam properties.","Such scheme enables an unprecedented overview of the global behavior of the experimental system and takes a significantly smaller number of measurements compared with traditional methods such as a grid scan.","This methodology can be applied in other experimental scenarios that require simultaneously optimizing multiple objectives by explorations in high dimensional, nonlinear and correlated systems."],"url":"http://arxiv.org/abs/2404.02268v1","category":"physics.acc-ph"}
{"created":"2024-04-02 19:28:11","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models","abstract":"Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling.","sentences":["Transformer-based language models spread FLOPs uniformly across input sequences.","In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth.","Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer.","The tokens to be processed are determined by the network using a top-$k$ routing mechanism.","Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques.","Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions.","Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level.","Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently.","These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling."],"url":"http://arxiv.org/abs/2404.02258v1","category":"cs.LG"}
{"created":"2024-04-02 19:25:04","title":"SnAG: Scalable and Accurate Video Grounding","abstract":"Temporal grounding of text descriptions in videos is a central problem in vision-language learning and video understanding. Existing methods often prioritize accuracy over scalability -- they have been optimized for grounding only a few text queries within short videos, and fail to scale up to long videos with hundreds of queries. In this paper, we study the effect of cross-modal fusion on the scalability of video grounding models. Our analysis establishes late fusion as a more cost-effective fusion scheme for long-form videos with many text queries. Moreover, it leads us to a novel, video-centric sampling scheme for efficient training. Based on these findings, we present SnAG, a simple baseline for scalable and accurate video grounding. Without bells and whistles, SnAG is 43% more accurate and 1.5x faster than CONE, a state of the art for long-form video grounding on the challenging MAD dataset, while achieving highly competitive results on short videos.","sentences":["Temporal grounding of text descriptions in videos is a central problem in vision-language learning and video understanding.","Existing methods often prioritize accuracy over scalability -- they have been optimized for grounding only a few text queries within short videos, and fail to scale up to long videos with hundreds of queries.","In this paper, we study the effect of cross-modal fusion on the scalability of video grounding models.","Our analysis establishes late fusion as a more cost-effective fusion scheme for long-form videos with many text queries.","Moreover, it leads us to a novel, video-centric sampling scheme for efficient training.","Based on these findings, we present SnAG, a simple baseline for scalable and accurate video grounding.","Without bells and whistles, SnAG is 43% more accurate and 1.5x faster than CONE, a state of the art for long-form video grounding on the challenging MAD dataset, while achieving highly competitive results on short videos."],"url":"http://arxiv.org/abs/2404.02257v1","category":"cs.CV"}
{"created":"2024-04-02 19:03:39","title":"Towards Robust 3D Pose Transfer with Adversarial Learning","abstract":"3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D generation tasks. Previous attempts rely on well-defined parametric human models or skeletal joints as driving pose sources. However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are inevitable, hindering implementations of the real-time applications. This work is driven by the intuition that the robustness of the model can be enhanced by introducing adversarial samples into the training, leading to a more invulnerable model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing. Furthermore, we propose a novel 3D pose Masked Autoencoder (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose). 3D-PoseMAE facilitates learning from the aspect of extrinsic attributes by simultaneously generating adversarial samples that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy. Both qualitative and quantitative studies show that the transferred meshes given by our network result in much better quality. Besides, we demonstrate the strong generalizability of our method on various poses, different domains, and even raw scans. Experimental results also show meaningful insights that the intermediate adversarial samples generated in the training can successfully attack the existing pose transfer models.","sentences":["3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D generation tasks.","Previous attempts rely on well-defined parametric human models or skeletal joints as driving pose sources.","However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are inevitable, hindering implementations of the real-time applications.","This work is driven by the intuition that the robustness of the model can be enhanced by introducing adversarial samples into the training, leading to a more invulnerable model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing.","Furthermore, we propose a novel 3D pose Masked Autoencoder (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose).","3D-PoseMAE facilitates learning from the aspect of extrinsic attributes by simultaneously generating adversarial samples that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy.","Both qualitative and quantitative studies show that the transferred meshes given by our network result in much better quality.","Besides, we demonstrate the strong generalizability of our method on various poses, different domains, and even raw scans.","Experimental results also show meaningful insights that the intermediate adversarial samples generated in the training can successfully attack the existing pose transfer models."],"url":"http://arxiv.org/abs/2404.02242v1","category":"cs.CV"}
{"created":"2024-04-03 16:16:31","title":"Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models","abstract":"This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. These findings highlight the potential of CherryQ for enabling efficient deployment of LLMs by taking advantage of parameter heterogeneity.","sentences":["This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs).","We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact.","This heterogeneity is found to be prevalent across different model families, scales, and types.","Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters.","CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision.","Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance.","Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts.","These findings highlight the potential of CherryQ for enabling efficient deployment of LLMs by taking advantage of parameter heterogeneity."],"url":"http://arxiv.org/abs/2404.02837v1","category":"cs.CL"}
{"created":"2024-04-03 13:41:22","title":"Terraced Compression Method with Automated Threshold Selection for Multidimensional Image Clustering of Heterogeneous Bodies","abstract":"Multispectral transmission imaging provides strong benefits for early breast cancer screening. The frame accumulation method addresses the challenge of low grayscale and signal-to-noise ratio resulting from the strong absorption and scattering of light by breast tissue. This method introduces redundancy in data while improving the grayscale and signal-to-noise ratio of the image. Existing terraced compression algorithms effectively eliminate the data redundancy introduced by frame accumulation but necessitate significant time for manual debugging of threshold values. Hence, this paper proposes an improved terrace compression algorithm. The algorithm necessitates solely the input of the desired heterogeneous body size and autonomously calculates the optimal area threshold and gradient threshold by counting the grayscale and combining its distribution. Experimental acquisition involved multi-wavelength images of heterogeneous bodies exhibiting diverse textures, depths, and thicknesses. Subsequently, the method was applied after pre-processing to determine the thresholds for terraced compression at each wavelength, coupled with a window function for multi-dimensional image clustering. The results illustrate the method's efficacy in detecting and identifying various heterogeneous body types, depths, and thicknesses. This approach is expected to accurately identify the locations and types of breast tumors in the future, thus providing a more dependable tool for early breast cancer screening.","sentences":["Multispectral transmission imaging provides strong benefits for early breast cancer screening.","The frame accumulation method addresses the challenge of low grayscale and signal-to-noise ratio resulting from the strong absorption and scattering of light by breast tissue.","This method introduces redundancy in data while improving the grayscale and signal-to-noise ratio of the image.","Existing terraced compression algorithms effectively eliminate the data redundancy introduced by frame accumulation but necessitate significant time for manual debugging of threshold values.","Hence, this paper proposes an improved terrace compression algorithm.","The algorithm necessitates solely the input of the desired heterogeneous body size and autonomously calculates the optimal area threshold and gradient threshold by counting the grayscale and combining its distribution.","Experimental acquisition involved multi-wavelength images of heterogeneous bodies exhibiting diverse textures, depths, and thicknesses.","Subsequently, the method was applied after pre-processing to determine the thresholds for terraced compression at each wavelength, coupled with a window function for multi-dimensional image clustering.","The results illustrate the method's efficacy in detecting and identifying various heterogeneous body types, depths, and thicknesses.","This approach is expected to accurately identify the locations and types of breast tumors in the future, thus providing a more dependable tool for early breast cancer screening."],"url":"http://arxiv.org/abs/2404.02744v1","category":"eess.IV"}
{"created":"2024-04-03 12:17:49","title":"History Trees and Their Applications","abstract":"In the theoretical study of distributed communication networks, \"history trees\" are a discrete structure that naturally models the concept that anonymous agents become distinguishable upon receiving different sets of messages from neighboring agents. By conveniently organizing temporal information in a systematic manner, history trees have been instrumental in the development of optimal deterministic algorithms for networks that are both anonymous and dynamically evolving.   This note provides an accessible introduction to history trees, drawing comparisons with more traditional structures found in existing literature and reviewing the latest advancements in the applications of history trees, especially within dynamic networks. Furthermore, it expands the theoretical framework of history trees in new directions, also highlighting several open problems for further investigation.","sentences":["In the theoretical study of distributed communication networks, \"history trees\" are a discrete structure that naturally models the concept that anonymous agents become distinguishable upon receiving different sets of messages from neighboring agents.","By conveniently organizing temporal information in a systematic manner, history trees have been instrumental in the development of optimal deterministic algorithms for networks that are both anonymous and dynamically evolving.   ","This note provides an accessible introduction to history trees, drawing comparisons with more traditional structures found in existing literature and reviewing the latest advancements in the applications of history trees, especially within dynamic networks.","Furthermore, it expands the theoretical framework of history trees in new directions, also highlighting several open problems for further investigation."],"url":"http://arxiv.org/abs/2404.02673v1","category":"cs.DC"}
{"created":"2024-04-03 11:42:38","title":"Demonstration of weighted graph optimization on a Rydberg atom array using local light-shifts","abstract":"Neutral atom arrays have emerged as a versatile platform towards scalable quantum computation and optimization. In this paper we present first demonstrations of weighted graph optimization on a Rydberg atom array using annealing with local light-shifts. We verify the ability to prepare weighted graphs in 1D and 2D arrays, including embedding a five vertex non-unit disk graph using nine physical qubits. We find common annealing ramps leading to preparation of the target ground state robustly over a substantial range of different graph weightings. This work provides a route to exploring large-scale optimization of non-planar weighted graphs relevant for solving relevant real-world problems.","sentences":["Neutral atom arrays have emerged as a versatile platform towards scalable quantum computation and optimization.","In this paper we present first demonstrations of weighted graph optimization on a Rydberg atom array using annealing with local light-shifts.","We verify the ability to prepare weighted graphs in 1D and 2D arrays, including embedding a five vertex non-unit disk graph using nine physical qubits.","We find common annealing ramps leading to preparation of the target ground state robustly over a substantial range of different graph weightings.","This work provides a route to exploring large-scale optimization of non-planar weighted graphs relevant for solving relevant real-world problems."],"url":"http://arxiv.org/abs/2404.02658v1","category":"quant-ph"}
{"created":"2024-04-03 09:51:39","title":"A mixed-integer-programming-based Gauss-Seidel method for multi-leader-multi-follower games","abstract":"We design a computational approach to find equilibria in a class of Nash games possessing a hierarchical structure. By using tools from mixed-integer programming and the characterization of variational equilibria in terms of the Karush--Kuhn--Tucker conditions, we propose a mixed-integer game formulation for solving such a challenging instance. Besides providing an equivalent reformulation, we build upon our previous work and design a proximal Gauss--Seidel method with global convergence guarantees for the case in which the game enjoys a potential structure. In addition to proving the convergence of the resulting scheme, we show its performance on a numerical instance of a ride-hail market problem.","sentences":["We design a computational approach to find equilibria in a class of Nash games possessing a hierarchical structure.","By using tools from mixed-integer programming and the characterization of variational equilibria in terms of the Karush--Kuhn--Tucker conditions, we propose a mixed-integer game formulation for solving such a challenging instance.","Besides providing an equivalent reformulation, we build upon our previous work and design a proximal Gauss--Seidel method with global convergence guarantees for the case in which the game enjoys a potential structure.","In addition to proving the convergence of the resulting scheme, we show its performance on a numerical instance of a ride-hail market problem."],"url":"http://arxiv.org/abs/2404.02605v1","category":"math.OC"}
{"created":"2024-04-03 08:18:30","title":"Degree Sequence Optimization and Extremal Degree Enumerators","abstract":"The degree sequence optimization problem is to find a subgraph of a given graph which maximizes the sum of given functions evaluated at the subgraph degrees. Here we study this problem by replacing degree sequences, via suitable nonlinear transformations, by suitable degree enumerators, and we introduce suitable degree enumerator polytopes.   We characterize their vertices, that is, the extremal degree enumerators, for complete graphs and some complete bipartite graphs, and use these characterizations to obtain simpler and faster algorithms for optimization over degree sequences for such graphs.","sentences":["The degree sequence optimization problem is to find a subgraph of a given graph which maximizes the sum of given functions evaluated at the subgraph degrees.","Here we study this problem by replacing degree sequences, via suitable nonlinear transformations, by suitable degree enumerators, and we introduce suitable degree enumerator polytopes.   ","We characterize their vertices, that is, the extremal degree enumerators, for complete graphs and some complete bipartite graphs, and use these characterizations to obtain simpler and faster algorithms for optimization over degree sequences for such graphs."],"url":"http://arxiv.org/abs/2404.02551v1","category":"math.CO"}
{"created":"2024-04-03 06:36:34","title":"Bacterial cell death: Atomistic simulations reveal pore formation as a mode of action of structurally nano engineered star peptide polymers","abstract":"Multidrug resistance (MDR) to conventional antibiotics is one of the most urgent global health threats, necessitating the development of effective and biocompatible antimicrobial agents that are less inclined to provoke resistance. Structurally Nanoengineered Antimicrobial Peptide Polymers (SNAPPs) are a novel and promising class of such alternatives. These star-shaped polymers are made of a dendritic core with multiple arms made of co-peptides with varying amino acid sequences. Through a comprehensive set of in vivo experiments, we (Nature Microbiology, 1, 16162, 2016) showed that SNAPPs with arms made of random blocks of lysine (K) and valine (V) residues exhibit sub-micron M efficacy against Gram-negative and Gram-positive bacteria tested. Cryo-TEM images suggested pore formation by SNAPP with random block co-peptide arms as one of their mode of actions. However, the molecular mechanisms responsible for this mode of action of SNAPP were not fully understood. To address this gap, we employed atomistic molecular dynamics simulation technique to investigate the influence of three different sequences of amino acids, namely 1) alternating block KKV 2) random block and 3) di-block motifs on secondary structure of their arms and SNAPP's overall configuration as well as their interactions with lipid bilayer. We, for the first time identified a step-by-step mechanism through which alternating block and random SNAPPs interact with lipid bilayer and leads to pore formation, hence cell death. These insights provide a strong foundation for further optimization of the chemical structure of SNAPPs for maximum performance against MDR bacteria, therefore offering a promising avenue for addressing antibiotic resistance and development of effective antibacterial agents.","sentences":["Multidrug resistance (MDR) to conventional antibiotics is one of the most urgent global health threats, necessitating the development of effective and biocompatible antimicrobial agents that are less inclined to provoke resistance.","Structurally Nanoengineered Antimicrobial Peptide Polymers (SNAPPs) are a novel and promising class of such alternatives.","These star-shaped polymers are made of a dendritic core with multiple arms made of co-peptides with varying amino acid sequences.","Through a comprehensive set of in vivo experiments, we (Nature Microbiology, 1, 16162, 2016) showed that SNAPPs with arms made of random blocks of lysine (K) and valine (V) residues exhibit sub-micron M efficacy against Gram-negative and Gram-positive bacteria tested.","Cryo-TEM images suggested pore formation by SNAPP with random block co-peptide arms as one of their mode of actions.","However, the molecular mechanisms responsible for this mode of action of SNAPP were not fully understood.","To address this gap, we employed atomistic molecular dynamics simulation technique to investigate the influence of three different sequences of amino acids, namely 1) alternating block KKV 2) random block and 3) di-block motifs on secondary structure of their arms and SNAPP's overall configuration as well as their interactions with lipid bilayer.","We, for the first time identified a step-by-step mechanism through which alternating block and random SNAPPs interact with lipid bilayer and leads to pore formation, hence cell death.","These insights provide a strong foundation for further optimization of the chemical structure of SNAPPs for maximum performance against MDR bacteria, therefore offering a promising avenue for addressing antibiotic resistance and development of effective antibacterial agents."],"url":"http://arxiv.org/abs/2404.02501v2","category":"cond-mat.soft"}
{"created":"2024-04-03 05:42:11","title":"Optimizing traffic signs and lights visibility for the teleoperation of autonomous vehicles through ROI compression","abstract":"Autonomous vehicles are a promising solution to traffic congestion, air pollution, accidents, and wasted time and resources. However, remote driver intervention may be necessary for extreme situations to ensure safe roadside parking or complete remote takeover. In such cases, high-quality real-time video streaming is crucial for practical remote driving. In a preliminary study, we already presented a region of interest (ROI) HEVC data compression where the image was segmented into two categories of ROI and background, allocating more bandwidth to the ROI, yielding an improvement in the visibility of the classes that essential for driving while transmitting the background with lesser quality. However, migrating bandwidth to the large ROI portion of the image doesn't substantially improve the quality of traffic signs and lights. This work categorized the ROIs into either background, weak ROI, or strong ROI. The simulation-based approach uses a photo-realistic driving scenario database created with the Cognata self-driving car simulation platform. We use semantic segmentation to categorize the compression quality of a Coding Tree Unit (CTU) according to each pixel class. A background CTU can contain only sky, trees, vegetation, or building classes. Essentials for remote driving include significant classes such as roads, road marks, cars, and pedestrians. And most importantly, traffic signs and traffic lights. We apply thresholds to decide if the number of pixels in a CTU of a particular category is enough to declare it as belonging to the strong or weak ROI. Then, we allocate the bandwidth according to the CTU categories. Our results show that the perceptual quality of traffic signs, especially textual signs and traffic lights, improves significantly by up to 5.5 dB compared to the only background and foreground partition, while the weak ROI classes at least retain their original quality.","sentences":["Autonomous vehicles are a promising solution to traffic congestion, air pollution, accidents, and wasted time and resources.","However, remote driver intervention may be necessary for extreme situations to ensure safe roadside parking or complete remote takeover.","In such cases, high-quality real-time video streaming is crucial for practical remote driving.","In a preliminary study, we already presented a region of interest (ROI) HEVC data compression where the image was segmented into two categories of ROI and background, allocating more bandwidth to the ROI, yielding an improvement in the visibility of the classes that essential for driving while transmitting the background with lesser quality.","However, migrating bandwidth to the large ROI portion of the image doesn't substantially improve the quality of traffic signs and lights.","This work categorized the ROIs into either background, weak ROI, or strong ROI.","The simulation-based approach uses a photo-realistic driving scenario database created with the Cognata self-driving car simulation platform.","We use semantic segmentation to categorize the compression quality of a Coding Tree Unit (CTU) according to each pixel class.","A background CTU can contain only sky, trees, vegetation, or building classes.","Essentials for remote driving include significant classes such as roads, road marks, cars, and pedestrians.","And most importantly, traffic signs and traffic lights.","We apply thresholds to decide if the number of pixels in a CTU of a particular category is enough to declare it as belonging to the strong or weak ROI.","Then, we allocate the bandwidth according to the CTU categories.","Our results show that the perceptual quality of traffic signs, especially textual signs and traffic lights, improves significantly by up to 5.5 dB compared to the only background and foreground partition, while the weak ROI classes at least retain their original quality."],"url":"http://arxiv.org/abs/2404.02481v1","category":"eess.IV"}
{"created":"2024-04-03 03:44:27","title":"Fast marginalization algorithm for optimizing gravitational wave detection, parameter estimation and sky localization","abstract":"We introduce an algorithm to marginalize the likelihood for a gravitational wave signal from a quasi-circular binary merger over its extrinsic parameters, accounting for the effects of higher harmonics and spin-induced precession. The algorithm takes as input the matched-filtering time series of individual waveform harmonics against the data in all operational detectors, and the covariances of the harmonics. The outputs are the Gaussian likelihood marginalized over extrinsic parameters describing the merger time, location and orientation, along with samples from the conditional posterior of these parameters. Our algorithm exploits the waveform's known analytical dependence on extrinsic parameters to efficiently marginalize over them using a single waveform evaluation. Our current implementation achieves a 10% precision on the marginalized likelihood within $\\approx 50$ ms on a single CPU core and is publicly available through the package `cogwheel`. We discuss applications of this tool for gravitational wave searches involving higher modes or precession, efficient and robust parameter estimation, and generation of sky localization maps in low latency for electromagnetic followup of gravitational-wave alerts. The inclusion of higher modes can improve the distance measurement, providing an advantage over existing low-latency localization methods.","sentences":["We introduce an algorithm to marginalize the likelihood for a gravitational wave signal from a quasi-circular binary merger over its extrinsic parameters, accounting for the effects of higher harmonics and spin-induced precession.","The algorithm takes as input the matched-filtering time series of individual waveform harmonics against the data in all operational detectors, and the covariances of the harmonics.","The outputs are the Gaussian likelihood marginalized over extrinsic parameters describing the merger time, location and orientation, along with samples from the conditional posterior of these parameters.","Our algorithm exploits the waveform's known analytical dependence on extrinsic parameters to efficiently marginalize over them using a single waveform evaluation.","Our current implementation achieves a 10% precision on the marginalized likelihood within $\\approx 50$ ms on a single CPU core and is publicly available through the package `cogwheel`.","We discuss applications of this tool for gravitational wave searches involving higher modes or precession, efficient and robust parameter estimation, and generation of sky localization maps in low latency for electromagnetic followup of gravitational-wave alerts.","The inclusion of higher modes can improve the distance measurement, providing an advantage over existing low-latency localization methods."],"url":"http://arxiv.org/abs/2404.02435v1","category":"gr-qc"}
{"created":"2024-04-02 22:54:48","title":"A faster algorithm for the construction of optimal factoring automata","abstract":"The problem of constructing optimal factoring automata arises in the context of unification factoring for the efficient execution of logic programs. Given an ordered set of $n$ strings of length $m$, the problem is to construct a trie-like tree structure of minimum size in which the leaves in left-to-right order represent the input strings in the given order. Contrary to standard tries, the order in which the characters of a string are encountered can be different on different root-to-leaf paths. Dawson et al. [ACM Trans. Program. Lang. Syst. 18(5):528--563, 1996] gave an algorithm that solves the problem in time $O(n^2 m (n+m))$. In this paper, we present an improved algorithm with running-time $O(n^2m)$.","sentences":["The problem of constructing optimal factoring automata arises in the context of unification factoring for the efficient execution of logic programs.","Given an ordered set of $n$ strings of length $m$, the problem is to construct a trie-like tree structure of minimum size in which the leaves in left-to-right order represent the input strings in the given order.","Contrary to standard tries, the order in which the characters of a string are encountered can be different on different root-to-leaf paths.","Dawson et al.","[ACM Trans.","Program.","Lang.","Syst.","18(5):528--563, 1996] gave an algorithm that solves the problem in time $O(n^2 m (n+m))$.","In this paper, we present an improved algorithm with running-time $O(n^2m)$."],"url":"http://arxiv.org/abs/2404.02354v1","category":"cs.DS"}
{"created":"2024-04-02 21:44:08","title":"Bounds and Limiting Minimizers for a Family of Interaction Energies","abstract":"We study a two parameter family of energy minimization problems for interaction energies $\\mathcal{E}_{\\alpha,\\beta}$ with attractive-repulsive potential $W_{\\alpha,\\beta}$. We develop a concavity principle, which allows us to provide a lower bound on $\\mathcal{E}_{\\alpha,\\beta}$ if there exist $\\beta_0<\\beta<\\beta_1$ with minimizers of $\\mathcal{E}_{\\alpha,\\beta_0}$ and $\\mathcal{E}_{\\alpha,\\beta_1}$ known. In addition to this, we also derive new conclusions about the limiting behaviour of $\\mathcal{E}_{\\alpha,\\beta}$ for $\\beta\\approx 2.$ Finally, we describe a method to show that, for certain values of $(\\alpha,\\beta),$ $\\mathcal{E}_{\\alpha,\\beta}$ cannot be minimized by the uniform distribution over a top-dimensional regular unit simplex. Our results are made possible by two key factors -- recent progress in identifying minimizers of $\\mathcal{E}_{\\alpha,\\beta}$ for a range of $\\alpha$ and $\\beta$, and an analysis of $\\inf\\mathcal{E}_{\\alpha,\\beta}$ as a function on parameter space.","sentences":["We study a two parameter family of energy minimization problems for interaction energies $\\mathcal{E}_{\\alpha,\\beta}$ with attractive-repulsive potential $W_{\\alpha,\\beta}$. We develop a concavity principle, which allows us to provide a lower bound on $\\mathcal{E}_{\\alpha,\\beta}$ if there exist $\\beta_0<\\beta<\\beta_1$ with minimizers of $\\mathcal{E}_{\\alpha,\\beta_0}$ and $\\mathcal{E}_{\\alpha,\\beta_1}$ known.","In addition to this, we also derive new conclusions about the limiting behaviour of $\\mathcal{E}_{\\alpha,\\beta}$ for $\\beta\\approx 2.$","Finally, we describe a method to show that, for certain values of $(\\alpha,\\beta),$ $\\mathcal{E}_{\\alpha,\\beta}$ cannot be minimized by the uniform distribution over a top-dimensional regular unit simplex.","Our results are made possible by two key factors -- recent progress in identifying minimizers of $\\mathcal{E}_{\\alpha,\\beta}$ for a range of $\\alpha$ and $\\beta$, and an analysis of $\\inf\\mathcal{E}_{\\alpha,\\beta}$ as a function on parameter space."],"url":"http://arxiv.org/abs/2404.02322v1","category":"math.OC"}
{"created":"2024-04-02 20:31:56","title":"Generalized saturation game","abstract":"We study the following game version of the generalized graph Tur\\'an problem. For two fixed graphs F and H, two players, Max and Mini, alternately claim unclaimed edges of the complete graph Kn such that the graph G of the claimed edges must remain F-free throughout the game. The game ends when no further edges can be claimed, i.e. when G becomes F-saturated. The H-score of the game is the number of copies of H in G. Max aims to maximize the H-score, while Mini wants to minimize it. The H-score of the game when both players play optimally is denoted by s1(n, #H, F) when Max starts, and by s2(n, #H, F) when Mini starts. We study these values for several natural choices of F and H.","sentences":["We study the following game version of the generalized graph Tur\\'an problem.","For two fixed graphs F and H, two players, Max and Mini, alternately claim unclaimed edges of the complete graph Kn such that the graph G of the claimed edges must remain F-free throughout the game.","The game ends when no further edges can be claimed, i.e. when G becomes F-saturated.","The H-score of the game is the number of copies of H in G. Max aims to maximize the H-score, while Mini wants to minimize it.","The H-score of the game when both players play optimally is denoted by s1(n, #H, F) when Max starts, and by s2(n, #H, F) when Mini starts.","We study these values for several natural choices of F and H."],"url":"http://arxiv.org/abs/2404.02288v1","category":"math.CO"}
{"created":"2024-04-02 20:26:03","title":"Energy Allocation for Multi-User Cooperative Molecular Communication Systems in the Internet of Bio-Nano Things","abstract":"Cooperative molecular communication (MC) is a promising technology for facilitating communication between nanomachines in the Internet of Bio-Nano Things (IoBNT) field. However, the performance of IoBNT is limited by the availability of energy for cooperative MC. This paper presents a novel transmitter design scheme that utilizes molecule movement between reservoirs, creating concentration differences through the consumption of free energy, and encoding information on molecule types. The performance of the transmitter is primarily influenced by energy costs, which directly impact the overall IoBNT system performance. To address this, the paper focuses on optimizing energy allocation in cooperative MC for enhanced transmitter performance. Theoretical analysis is conducted for two transmitters. For scenarios with more than two users, a genetic algorithm is employed in the energy allocation to minimize the total bit error rate (BER). Finally, numerical results show the effectiveness of the proposed energy allocation strategies in the considered cooperative MC system.","sentences":["Cooperative molecular communication (MC) is a promising technology for facilitating communication between nanomachines in the Internet of Bio-Nano Things (IoBNT) field.","However, the performance of IoBNT is limited by the availability of energy for cooperative MC.","This paper presents a novel transmitter design scheme that utilizes molecule movement between reservoirs, creating concentration differences through the consumption of free energy, and encoding information on molecule types.","The performance of the transmitter is primarily influenced by energy costs, which directly impact the overall IoBNT system performance.","To address this, the paper focuses on optimizing energy allocation in cooperative MC for enhanced transmitter performance.","Theoretical analysis is conducted for two transmitters.","For scenarios with more than two users, a genetic algorithm is employed in the energy allocation to minimize the total bit error rate (BER).","Finally, numerical results show the effectiveness of the proposed energy allocation strategies in the considered cooperative MC system."],"url":"http://arxiv.org/abs/2404.02286v1","category":"cs.IT"}
{"created":"2024-04-02 20:13:18","title":"Efficient Implementation of Multi-Controlled Quantum Gates","abstract":"We present an implementation of multi-controlled quantum gates which provides significant reductions of cost compared to state-of-the-art methods. The operator applied on the target qubit is a unitary, special unitary, or the Pauli X operator (Multi-Controlled Toffoli). The required number of ancilla qubits is no larger than one, similarly to known linear cost decompositions. We extend our methods for any number of target qubits, and provide further cost reductions if additional ancilla qubits are available. For each type of multi-controlled gate, we provide implementations for unrestricted (all-to-all) connectivity and for linear-nearest-neighbor. All of the methods use a linear cost of gates from the Clifford+T (fault-tolerant) set. In the context of linear-nearest-neighbor architecture, the cost and depth of our circuits scale linearly irrespective of the position of the qubits on which the gate is applied. Our methods directly improve the compilation process of many quantum algorithms, providing optimized circuits, which will result in a large reduction of errors.","sentences":["We present an implementation of multi-controlled quantum gates which provides significant reductions of cost compared to state-of-the-art methods.","The operator applied on the target qubit is a unitary, special unitary, or the Pauli X operator (Multi-Controlled Toffoli).","The required number of ancilla qubits is no larger than one, similarly to known linear cost decompositions.","We extend our methods for any number of target qubits, and provide further cost reductions if additional ancilla qubits are available.","For each type of multi-controlled gate, we provide implementations for unrestricted (all-to-all) connectivity and for linear-nearest-neighbor.","All of the methods use a linear cost of gates from the Clifford+T (fault-tolerant) set.","In the context of linear-nearest-neighbor architecture, the cost and depth of our circuits scale linearly irrespective of the position of the qubits on which the gate is applied.","Our methods directly improve the compilation process of many quantum algorithms, providing optimized circuits, which will result in a large reduction of errors."],"url":"http://arxiv.org/abs/2404.02279v1","category":"quant-ph"}
{"created":"2024-04-02 19:50:46","title":"High temperature series expansions of S = 1/2 Heisenberg spin models: algorithm to include the magnetic field with optimized complexity","abstract":"This work presents an algorithm for calculating high temperature series expansions (HTSE) of Heisenberg spin models with spin $S=1/2$ in the thermodynamic limit. This algorithm accounts for the presence of a magnetic field. The paper begins with a comprehensive introduction to HTSE and then focuses on identifying the bottlenecks that limit the computation of higher order coefficients. HTSE calculations involve two key steps: graph enumeration on the lattice and trace calculations for each graph. The introduction of a non-zero magnetic field adds complexity to the expansion because previously irrelevant graphs must now be considered: bridged graphs. We present an efficient method to deduce the contribution of these graphs from the contribution of sub-graphs, that drastically reduces the time of calculation for the last order coefficient (in practice increasing by one the order of the series at almost no cost). Previous articles of the authors have utilized HTSE calculations based on this algorithm, but without providing detailed explanations. The complete algorithm is publicly available, as well as the series on many lattice and for various interactions.","sentences":["This work presents an algorithm for calculating high temperature series expansions (HTSE) of Heisenberg spin models with spin $S=1/2$ in the thermodynamic limit.","This algorithm accounts for the presence of a magnetic field.","The paper begins with a comprehensive introduction to HTSE and then focuses on identifying the bottlenecks that limit the computation of higher order coefficients.","HTSE calculations involve two key steps: graph enumeration on the lattice and trace calculations for each graph.","The introduction of a non-zero magnetic field adds complexity to the expansion because previously irrelevant graphs must now be considered: bridged graphs.","We present an efficient method to deduce the contribution of these graphs from the contribution of sub-graphs, that drastically reduces the time of calculation for the last order coefficient (in practice increasing by one the order of the series at almost no cost).","Previous articles of the authors have utilized HTSE calculations based on this algorithm, but without providing detailed explanations.","The complete algorithm is publicly available, as well as the series on many lattice and for various interactions."],"url":"http://arxiv.org/abs/2404.02271v1","category":"cond-mat.str-el"}
{"created":"2024-04-02 19:42:50","title":"Tracking the Mean of a Piecewise Stationary Sequence","abstract":"In this paper we study the problem of tracking the mean of a piecewise stationary sequence of independent random variables. First we consider the case where the transition times are known and show that a direct running average performs the tracking in short time and with high accuracy. We then use a single valued weighted running average with a tunable parameter for the case when transition times are unknown and establish deviation bounds for the tracking accuracy. Our result has applications in choosing the optimal rewards for the multiarmed bandit scenario.","sentences":["In this paper we study the problem of tracking the mean of a piecewise stationary sequence of independent random variables.","First we consider the case where the transition times are known and show that a direct running average performs the tracking in short time and with high accuracy.","We then use a single valued weighted running average with a tunable parameter for the case when transition times are unknown and establish deviation bounds for the tracking accuracy.","Our result has applications in choosing the optimal rewards for the multiarmed bandit scenario."],"url":"http://arxiv.org/abs/2404.02266v1","category":"math.PR"}
{"created":"2024-04-02 19:37:07","title":"Classification and Regression Error Bounds for Inhomogenous Data With Applications to Wireless Networks","abstract":"In this paper, we study classification and regression error bounds for inhomogenous data that are independent but not necessarily identically distributed. First, we consider classification of data in the presence of non-stationary noise and establish ergodic type sufficient conditions that guarantee the achievability of the Bayes error bound, using universal rules. We then perform a similar analysis for $k$-nearest neighbour regression and obtain optimal error bounds for the same. Finally, we illustrate applications of our results in the context of wireless networks.","sentences":["In this paper, we study classification and regression error bounds for inhomogenous data that are independent but not necessarily identically distributed.","First, we consider classification of data in the presence of non-stationary noise and establish ergodic type sufficient conditions that guarantee the achievability of the Bayes error bound, using universal rules.","We then perform a similar analysis for $k$-nearest neighbour regression and obtain optimal error bounds for the same.","Finally, we illustrate applications of our results in the context of wireless networks."],"url":"http://arxiv.org/abs/2404.02262v1","category":"cs.IT"}
{"created":"2024-04-02 18:00:08","title":"James-Stein Estimation in Gaussian Metrology : V2","abstract":"The James-Stein estimator is a biased estimator -- for a finite number of samples its expected value is not the true mean. The maximum-likelihood estimator (MLE), is unbiased and asymptotically optimal. Yet, when estimating the mean of $3$ or more normally-distributed random variables, the James-Stein estimator has a smaller total (expected) error than the MLE. We introduce the James-Stein estimator to the field of quantum metrology, from both the frequentist and Bayesian perspectives. We characterise the effect of quantum phenomena on the James-Stein estimator through the lens of quantum Gaussian sensing, the task of estimating the mean of an unknown multivariate quantum Gaussian state. We find that noiseless entanglement or coherence improves performance of the James-Stein estimator, but diminishes its advantage over the MLE. In the presence of noise, the James-Stein advantage is restored. Quantum effects can also boost the James-Stein advantage. We demonstrate this by investigating multivariate postselective metrology (generalised weak-value amplification), a strategy that uses quantum effects to measure parameters with imperfect detectors. Simply by post-processing measured data differently, our techniques reduce errors in quantum experiments.","sentences":["The James-Stein estimator is a biased estimator -- for a finite number of samples its expected value is not the true mean.","The maximum-likelihood estimator (MLE), is unbiased and asymptotically optimal.","Yet, when estimating the mean of $3$ or more normally-distributed random variables, the James-Stein estimator has a smaller total (expected) error than the MLE.","We introduce the James-Stein estimator to the field of quantum metrology, from both the frequentist and Bayesian perspectives.","We characterise the effect of quantum phenomena on the James-Stein estimator through the lens of quantum Gaussian sensing, the task of estimating the mean of an unknown multivariate quantum Gaussian state.","We find that noiseless entanglement or coherence improves performance of the James-Stein estimator, but diminishes its advantage over the MLE.","In the presence of noise, the James-Stein advantage is restored.","Quantum effects can also boost the James-Stein advantage.","We demonstrate this by investigating multivariate postselective metrology (generalised weak-value amplification), a strategy that uses quantum effects to measure parameters with imperfect detectors.","Simply by post-processing measured data differently, our techniques reduce errors in quantum experiments."],"url":"http://arxiv.org/abs/2404.02203v1","category":"quant-ph"}
{"created":"2024-04-02 17:55:51","title":"Priority-Neutral Matching Lattices Are Not Distributive","abstract":"Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure. However, in school-choice problems, stable matchings are not Pareto optimal for the students. Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching. Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice.   We study the structure of the priority-neutral lattice. Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice. In particular, we show that the priority-neutral lattice need not be distributive. Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answering an open question. This show that many widely-used properties of stable matchings fail for priority-neutral matchings; in particular, the set of priority-neutral matchings cannot be represented by via a partial ordering on a set of rotations. However, by proving a novel structural property of the set of priority-neutral matchings, we also show that not every lattice arises as a priority-neutral lattice, which suggests that the exact nature of the family of priority-neutral lattices may be subtle.","sentences":["Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure.","However, in school-choice problems, stable matchings are not Pareto optimal for the students.","Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching.","Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice.   ","We study the structure of the priority-neutral lattice.","Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice.","In particular, we show that the priority-neutral lattice need not be distributive.","Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answering an open question.","This show that many widely-used properties of stable matchings fail for priority-neutral matchings; in particular, the set of priority-neutral matchings cannot be represented by via a partial ordering on a set of rotations.","However, by proving a novel structural property of the set of priority-neutral matchings, we also show that not every lattice arises as a priority-neutral lattice, which suggests that the exact nature of the family of priority-neutral lattices may be subtle."],"url":"http://arxiv.org/abs/2404.02142v1","category":"econ.TH"}
{"created":"2024-04-02 17:13:32","title":"Counting rational points on the sphere with bounded denominator","abstract":"We give an optimal bound for the remainder when counting the number of rational points on the $n$-dimensional sphere with bounded denominator for any $n\\geq 3$.","sentences":["We give an optimal bound for the remainder when counting the number of rational points on the $n$-dimensional sphere with bounded denominator for any $n\\geq 3$."],"url":"http://arxiv.org/abs/2404.02114v1","category":"math.NT"}
{"created":"2024-04-02 16:40:57","title":"Optimal Bell inequalities for qubit-qudit systems","abstract":"We evaluate the maximal Bell violation for a generic qubit-qudit system, obtaining easily computable expressions in arbitrary qudit dimension. This work generalizes the well-known Horodeckis's result for a qubit-qubit system. We also give simple lower and upper bounds on that violation and study the possibility of improving the amount of Bell-violation by embedding the qudit Hilbert space in one of larger dimension. The results are illustrated with a family of density matrices in the context of a qubit-qutrit system.","sentences":["We evaluate the maximal Bell violation for a generic qubit-qudit system, obtaining easily computable expressions in arbitrary qudit dimension.","This work generalizes the well-known Horodeckis's result for a qubit-qubit system.","We also give simple lower and upper bounds on that violation and study the possibility of improving the amount of Bell-violation by embedding the qudit Hilbert space in one of larger dimension.","The results are illustrated with a family of density matrices in the context of a qubit-qutrit system."],"url":"http://arxiv.org/abs/2404.02092v1","category":"quant-ph"}
{"created":"2024-04-02 16:31:24","title":"Regularity results for almost-minimizers of anisotropic free interface problem with H\u00f6lder dependence on the position","abstract":"We establish regularity results for almost-minimizers of a class of variational problems involving both bulk and interface energies. The bulk energy is of Dirichlet type. The surface energy exhibits anisotropic behaviour and is defined by means of an ellipsoidal density that is H\\\"older continuous with respect to the position variable.","sentences":["We establish regularity results for almost-minimizers of a class of variational problems involving both bulk and interface energies.","The bulk energy is of Dirichlet type.","The surface energy exhibits anisotropic behaviour and is defined by means of an ellipsoidal density that is H\\\"older continuous with respect to the position variable."],"url":"http://arxiv.org/abs/2404.02086v1","category":"math.OC"}
{"created":"2024-04-02 16:28:41","title":"WcDT: World-centric Diffusion Transformer for Traffic Scene Generation","abstract":"In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of diffusion probabilistic models (a.k.a., diffusion models) and transformers. Our proposed framework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference. To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with Transformer (DiT) blocks. Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various transformer-based encoders. The encoded traffic scenes are then decoded by a trajectory decoder to generate multimodal future trajectories. Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving simulation systems.","sentences":["In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of diffusion probabilistic models (a.k.a., diffusion models) and transformers.","Our proposed framework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference.","To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with Transformer (DiT) blocks.","Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various transformer-based encoders.","The encoded traffic scenes are then decoded by a trajectory decoder to generate multimodal future trajectories.","Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving simulation systems."],"url":"http://arxiv.org/abs/2404.02082v1","category":"cs.CV"}
{"created":"2024-04-02 16:26:33","title":"Generic Properties of Conjugate Points in Optimal Control Problems","abstract":"The first part of the paper studies a class of optimal control problems in Bolza form, where the dynamics is linear w.r.t.~the control function. A necessary condition is derived, for the optimality of a trajectory which starts at a conjugate point. The second part is concerned with a classical problem in the Calculus of Variations, with free terminal point. For a generic terminal cost $\\psi\\in \\C^4(\\mathbb{R}^n)$, applying the previous necessary condition we show that the set of conjugate points is contained in the image of an $(n-2)$-dimensional manifold, and has locally bounded $(n-2)$-dimensional Hausdorff measure.","sentences":["The first part of the paper studies a class of optimal control problems in Bolza form, where the dynamics is linear w.r.t.~the control function.","A necessary condition is derived, for the optimality of a trajectory which starts at a conjugate point.","The second part is concerned with a classical problem in the Calculus of Variations, with free terminal point.","For a generic terminal cost $\\psi\\in \\C^4(\\mathbb{R}^n)$, applying the previous necessary condition we show that the set of conjugate points is contained in the image of an $(n-2)$-dimensional manifold, and has locally bounded $(n-2)$-dimensional Hausdorff measure."],"url":"http://arxiv.org/abs/2404.02080v1","category":"math.OC"}
{"created":"2024-04-02 16:23:15","title":"Energy-Optimized Planning in Non-Uniform Wind Fields with Fixed-Wing Aerial Vehicles","abstract":"Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances. However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limitations, potentially raising safety concerns. Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable. Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency. In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields. We study the planner characteristics with synthetic and real-world wind data and compare its performance against baseline cost and path formulations. We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time. The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes.","sentences":["Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances.","However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limitations, potentially raising safety concerns.","Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable.","Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency.","In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields.","We study the planner characteristics with synthetic and real-world wind data and compare its performance against baseline cost and path formulations.","We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time.","The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes."],"url":"http://arxiv.org/abs/2404.02077v1","category":"cs.RO"}
{"created":"2024-04-02 16:01:01","title":"SRG/ART-XC Galactic Bulge deep survey. I. Maximum likelihood source detection algorithm for X-ray surveys","abstract":"We describe an X-ray source detection method entirely based on the maximum likelihood analysis, in application to observations with the ART-XC telescope onboard the Spectrum Roentgen Gamma observatory. The method optimally combines the data taken at different conditions, a situation commonly found in scanning surveys or mosaic observations with a telescope with a significant off-axis PSF distortion. The method can be naturally extended to include additional information from the X-ray photon energies, detector grades, etc. The likelihood-based source detection naturally results in a stable and uniform definition of detection thresholds under different observing conditions (PSF, background level). This greatly simplifies the statistical calibration of the survey needed to, e.g., obtain the $\\log N - \\log S$ distribution of detected sources or their luminosity function. The method can be applied to the data from any imaging X-ray telescope.","sentences":["We describe an X-ray source detection method entirely based on the maximum likelihood analysis, in application to observations with the ART-XC telescope onboard the Spectrum Roentgen Gamma observatory.","The method optimally combines the data taken at different conditions, a situation commonly found in scanning surveys or mosaic observations with a telescope with a significant off-axis PSF distortion.","The method can be naturally extended to include additional information from the X-ray photon energies, detector grades, etc.","The likelihood-based source detection naturally results in a stable and uniform definition of detection thresholds under different observing conditions (PSF, background level).","This greatly simplifies the statistical calibration of the survey needed to, e.g., obtain the $\\log N - \\log S$ distribution of detected sources or their luminosity function.","The method can be applied to the data from any imaging X-ray telescope."],"url":"http://arxiv.org/abs/2404.02061v1","category":"astro-ph.HE"}
{"created":"2024-04-02 15:18:22","title":"Using the Empirical Attainment Function for Analyzing Single-objective Black-box Optimization Algorithms","abstract":"A widely accepted way to assess the performance of iterative black-box optimizers is to analyze their empirical cumulative distribution function (ECDF) of pre-defined quality targets achieved not later than a given runtime. In this work, we consider an alternative approach, based on the empirical attainment function (EAF) and we show that the target-based ECDF is an approximation of the EAF. We argue that the EAF has several advantages over the target-based ECDF. In particular, it does not require defining a priori quality targets per function, captures performance differences more precisely, and enables the use of additional summary statistics that enrich the analysis. We also show that the average area over the convergence curves is a simpler-to-calculate, but equivalent, measure of anytime performance. To facilitate the accessibility of the EAF, we integrate a module to compute it into the IOHanalyzer platform. Finally, we illustrate the use of the EAF via synthetic examples and via the data available for the BBOB suite.","sentences":["A widely accepted way to assess the performance of iterative black-box optimizers is to analyze their empirical cumulative distribution function (ECDF) of pre-defined quality targets achieved not later than a given runtime.","In this work, we consider an alternative approach, based on the empirical attainment function (EAF) and we show that the target-based ECDF is an approximation of the EAF.","We argue that the EAF has several advantages over the target-based ECDF.","In particular, it does not require defining a priori quality targets per function, captures performance differences more precisely, and enables the use of additional summary statistics that enrich the analysis.","We also show that the average area over the convergence curves is a simpler-to-calculate, but equivalent, measure of anytime performance.","To facilitate the accessibility of the EAF, we integrate a module to compute it into the IOHanalyzer platform.","Finally, we illustrate the use of the EAF via synthetic examples and via the data available for the BBOB suite."],"url":"http://arxiv.org/abs/2404.02031v1","category":"math.OC"}
{"created":"2024-04-02 15:17:12","title":"Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework","abstract":"This study presents an innovative approach to portfolio optimization by integrating Transformer models with Generative Adversarial Networks (GANs) within the Black-Litterman (BL) framework. Capitalizing on Transformers' ability to discern long-range dependencies and GANs' proficiency in generating accurate predictive models, our method enhances the generation of refined predictive views for BL portfolio allocations. This fusion of our model with BL's structured method for merging objective views with market equilibrium offers a potent tool for modern portfolio management, outperforming traditional forecasting methods. Our integrated approach not only demonstrates the potential to improve investment decision-making but also contributes a new approach to capture the complexities of financial markets for robust portfolio optimization.","sentences":["This study presents an innovative approach to portfolio optimization by integrating Transformer models with Generative Adversarial Networks (GANs) within the Black-Litterman (BL) framework.","Capitalizing on Transformers' ability to discern long-range dependencies and GANs' proficiency in generating accurate predictive models, our method enhances the generation of refined predictive views for BL portfolio allocations.","This fusion of our model with BL's structured method for merging objective views with market equilibrium offers a potent tool for modern portfolio management, outperforming traditional forecasting methods.","Our integrated approach not only demonstrates the potential to improve investment decision-making but also contributes a new approach to capture the complexities of financial markets for robust portfolio optimization."],"url":"http://arxiv.org/abs/2404.02029v2","category":"cs.CE"}
{"created":"2024-04-02 14:41:42","title":"Specularity Factorization for Low-Light Enhancement","abstract":"We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition. Our model-driven {\\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned. The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion. Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision. Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets. We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet. The code and data is released for reproducibility on the project homepage.","sentences":["We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition.","Our model-driven {\\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned.","The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion.","Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision.","Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets.","We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet.","The code and data is released for reproducibility on the project homepage."],"url":"http://arxiv.org/abs/2404.01998v1","category":"cs.CV"}
{"created":"2024-04-02 13:47:15","title":"Automatic Wood Pith Detector: Local Orientation Estimation and Robust Accumulation","abstract":"A fully automated technique for wood pith detection (APD), relying on the concentric shape of the structure of wood ring slices, is introduced. The method estimates the ring's local orientations using the 2D structure tensor and finds the pith position, optimizing a cost function designed for this problem. We also present a variant (APD-PCL), using the parallel coordinates space, that enhances the method's effectiveness when there are no clear tree ring patterns. Furthermore, refining previous work by Kurdthongmee, a YoloV8 net is trained for pith detection, producing a deep learning-based approach to the same problem (APD-DL). All methods were tested on seven datasets, including images captured under diverse conditions (controlled laboratory settings, sawmill, and forest) and featuring various tree species (Pinus taeda, Douglas fir, Abies alba, and Gleditsia triacanthos). All proposed approaches outperform existing state-of-the-art methods and can be used in CPU-based real-time applications. Additionally, we provide a novel dataset comprising images of gymnosperm and angiosperm species. Dataset and source code are available at http://github.com/hmarichal93/apd.","sentences":["A fully automated technique for wood pith detection (APD), relying on the concentric shape of the structure of wood ring slices, is introduced.","The method estimates the ring's local orientations using the 2D structure tensor and finds the pith position, optimizing a cost function designed for this problem.","We also present a variant (APD-PCL), using the parallel coordinates space, that enhances the method's effectiveness when there are no clear tree ring patterns.","Furthermore, refining previous work by Kurdthongmee, a YoloV8 net is trained for pith detection, producing a deep learning-based approach to the same problem (APD-DL).","All methods were tested on seven datasets, including images captured under diverse conditions (controlled laboratory settings, sawmill, and forest) and featuring various tree species (Pinus taeda, Douglas fir, Abies alba, and Gleditsia triacanthos).","All proposed approaches outperform existing state-of-the-art methods and can be used in CPU-based real-time applications.","Additionally, we provide a novel dataset comprising images of gymnosperm and angiosperm species.","Dataset and source code are available at http://github.com/hmarichal93/apd."],"url":"http://arxiv.org/abs/2404.01952v1","category":"cs.CV"}
{"created":"2024-04-02 13:44:27","title":"Heuristic Optimization of Amplifier Reconfiguration Process for Autonomous Driving Optical Networks","abstract":"We propose a heuristic-based optimization scheme for reliable optical amplifier reconfiguration process in ADON. In the experiment on a commercial testbed, the scheme prevents a 0.48-dB Q-factor degradation and outperforms 97.3% random solutions.","sentences":["We propose a heuristic-based optimization scheme for reliable optical amplifier reconfiguration process in ADON.","In the experiment on a commercial testbed, the scheme prevents a 0.48-dB Q-factor degradation and outperforms 97.3% random solutions."],"url":"http://arxiv.org/abs/2404.01949v1","category":"eess.SY"}
{"created":"2024-04-02 13:43:08","title":"Quantifying Noise of Dynamic Vision Sensor","abstract":"Dynamic visual sensors (DVS) are characterized by a large amount of background activity (BA) noise, which it is mixed with the original (cleaned) sensor signal. The dynamic nature of the signal and the absence in practical application of the ground truth, it clearly makes difficult to distinguish between noise and the cleaned sensor signals using standard image processing techniques. In this letter, a new technique is presented to characterise BA noise derived from the Detrended Fluctuation Analysis (DFA). The proposed technique can be used to address an existing DVS issues, which is how to quantitatively characterised noise and signal without ground truth, and how to derive an optimal denoising filter parameters. The solution of the latter problem is demonstrated for the popular real moving-car dataset.","sentences":["Dynamic visual sensors (DVS) are characterized by a large amount of background activity (BA) noise, which it is mixed with the original (cleaned) sensor signal.","The dynamic nature of the signal and the absence in practical application of the ground truth, it clearly makes difficult to distinguish between noise and the cleaned sensor signals using standard image processing techniques.","In this letter, a new technique is presented to characterise BA noise derived from the Detrended Fluctuation Analysis (DFA).","The proposed technique can be used to address an existing DVS issues, which is how to quantitatively characterised noise and signal without ground truth, and how to derive an optimal denoising filter parameters.","The solution of the latter problem is demonstrated for the popular real moving-car dataset."],"url":"http://arxiv.org/abs/2404.01948v1","category":"cs.CV"}
{"created":"2024-04-02 13:25:16","title":"Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks","abstract":"In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.","sentences":["In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation.","Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task.","However, they are computationally demanding and require careful fine-tuning of the produced outputs.","A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models.","Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment.","Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%.","Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length.","Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language."],"url":"http://arxiv.org/abs/2404.01932v1","category":"cs.RO"}
{"created":"2024-04-02 13:23:21","title":"Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA -- A Semi-Supervised Video Object Detection Method","abstract":"This study aims to establish a computer-aided diagnostic system for lung lesions using bronchoscope endobronchial ultrasound (EBUS) to assist physicians in identifying lesion areas. During EBUS-transbronchial needle aspiration (EBUS-TBNA) procedures, physicians rely on grayscale ultrasound images to determine the location of lesions. However, these images often contain significant noise and can be influenced by surrounding tissues or blood vessels, making interpretation challenging. Previous research has lacked the application of object detection models to EBUS-TBNA, and there has been no well-defined solution for annotating the EBUS-TBNA dataset. In related studies on ultrasound images, although models have been successful in capturing target regions for their respective tasks, their training and predictions have been based on two-dimensional images, limiting their ability to leverage temporal features for improved predictions. This study introduces a three-dimensional image-based object detection model. It utilizes an attention mechanism to capture temporal correlations and we will implements a filtering mechanism to select relevant information from previous frames. Subsequently, a teacher-student model training approach is employed to optimize the model further, leveraging unlabeled data. To mitigate the impact of poor-quality pseudo-labels on the student model, we will add a special Gaussian Mixture Model (GMM) to ensure the quality of pseudo-labels.","sentences":["This study aims to establish a computer-aided diagnostic system for lung lesions using bronchoscope endobronchial ultrasound (EBUS) to assist physicians in identifying lesion areas.","During EBUS-transbronchial needle aspiration (EBUS-TBNA) procedures, physicians rely on grayscale ultrasound images to determine the location of lesions.","However, these images often contain significant noise and can be influenced by surrounding tissues or blood vessels, making interpretation challenging.","Previous research has lacked the application of object detection models to EBUS-TBNA, and there has been no well-defined solution for annotating the EBUS-TBNA dataset.","In related studies on ultrasound images, although models have been successful in capturing target regions for their respective tasks, their training and predictions have been based on two-dimensional images, limiting their ability to leverage temporal features for improved predictions.","This study introduces a three-dimensional image-based object detection model.","It utilizes an attention mechanism to capture temporal correlations and we will implements a filtering mechanism to select relevant information from previous frames.","Subsequently, a teacher-student model training approach is employed to optimize the model further, leveraging unlabeled data.","To mitigate the impact of poor-quality pseudo-labels on the student model, we will add a special Gaussian Mixture Model (GMM) to ensure the quality of pseudo-labels."],"url":"http://arxiv.org/abs/2404.01929v1","category":"eess.IV"}
{"created":"2024-04-02 12:52:26","title":"Optimizing Offload Performance in Heterogeneous MPSoCs","abstract":"Heterogeneous multi-core architectures combine a few \"host\" cores, optimized for single-thread performance, with many small energy-efficient \"accelerator\" cores for data-parallel processing, on a single chip. Offloading a computation to the many-core acceleration fabric introduces a communication and synchronization cost which reduces the speedup attainable on the accelerator, particularly for small and fine-grained parallel tasks. We demonstrate that by co-designing the hardware and offload routines, we can increase the speedup of an offloaded DAXPY kernel by as much as 47.9%. Furthermore, we show that it is possible to accurately model the runtime of an offloaded application, accounting for the offload overheads, with as low as 1% MAPE error, enabling optimal offload decisions under offload execution time constraints.","sentences":["Heterogeneous multi-core architectures combine a few \"host\" cores, optimized for single-thread performance, with many small energy-efficient \"accelerator\" cores for data-parallel processing, on a single chip.","Offloading a computation to the many-core acceleration fabric introduces a communication and synchronization cost which reduces the speedup attainable on the accelerator, particularly for small and fine-grained parallel tasks.","We demonstrate that by co-designing the hardware and offload routines, we can increase the speedup of an offloaded DAXPY kernel by as much as 47.9%.","Furthermore, we show that it is possible to accurately model the runtime of an offloaded application, accounting for the offload overheads, with as low as 1% MAPE error, enabling optimal offload decisions under offload execution time constraints."],"url":"http://arxiv.org/abs/2404.01908v1","category":"cs.AR"}
{"created":"2024-04-02 12:39:44","title":"Automatic Derivation of an Optimal Task Frame for Learning and Controlling Contact-Rich Tasks","abstract":"This study investigates learning from demonstration (LfD) for contact-rich tasks. The procedure for choosing a task frame to express the learned signals for the motion and interaction wrench is often omitted or using expert insight. This article presents a procedure to derive the optimal task frame from motion and wrench data recorded during the demonstration. The procedure is based on two principles that are hypothesized to underpin the control configuration targeted by an expert, and assumes task frame origins and orientations that are fixed to either the world or the robot tool. It is rooted in screw theory, is entirely probabilistic and does not involve any hyperparameters. The procedure was validated by demonstrating several tasks, including surface following and manipulation of articulated objects, showing good agreement between the obtained and the assumed expert task frames. To validate the performance of the learned tasks by a UR10e robot, a constraint-based controller was designed based on the derived task frames and the learned data expressed therein. These experiments showed the effectiveness and versatility of the proposed approach. The task frame derivation approach fills a gap in the state of the art of LfD, bringing LfD for contact-rich tasks closer to practical application.","sentences":["This study investigates learning from demonstration (LfD) for contact-rich tasks.","The procedure for choosing a task frame to express the learned signals for the motion and interaction wrench is often omitted or using expert insight.","This article presents a procedure to derive the optimal task frame from motion and wrench data recorded during the demonstration.","The procedure is based on two principles that are hypothesized to underpin the control configuration targeted by an expert, and assumes task frame origins and orientations that are fixed to either the world or the robot tool.","It is rooted in screw theory, is entirely probabilistic and does not involve any hyperparameters.","The procedure was validated by demonstrating several tasks, including surface following and manipulation of articulated objects, showing good agreement between the obtained and the assumed expert task frames.","To validate the performance of the learned tasks by a UR10e robot, a constraint-based controller was designed based on the derived task frames and the learned data expressed therein.","These experiments showed the effectiveness and versatility of the proposed approach.","The task frame derivation approach fills a gap in the state of the art of LfD, bringing LfD for contact-rich tasks closer to practical application."],"url":"http://arxiv.org/abs/2404.01900v2","category":"cs.RO"}
{"created":"2024-04-02 12:35:48","title":"Damage Location in Mechanical Structures by Multi-Objective Pattern Search","abstract":"We propose a multi-objective global pattern search algorithm for the task of locating and quantifying damage in flexible mechanical structures. This is achieved by identifying eigenfrequencies and eigenmodes from measurements and matching them against the results of a finite element simulation model, which leads to a nonsmooth nonlinear bi-objective parameter estimation problem. A derivative-free optimization algorithm is required since the problem is nonsmooth and also because complex mechanical simulation models are often solved using commercial black-box software. Moreover, the entire set of non-dominated solutions is of interest to practitioners. Most solution approaches published to date are based on meta-heuristics such as genetic algorithms. The proposed multi-objective pattern-search algorithm provides a mathematically well-founded alternative. It features a novel sorting procedure that reduces the complexity in our context. Test runs on two experimental structures with multiple damage scenarios are used to validate the approach. The results demonstrate that the proposed algorithm yields accurate damage locations and requires moderate computational resources. From the engineer's perspective it represents a promising tool for structural health monitoring.","sentences":["We propose a multi-objective global pattern search algorithm for the task of locating and quantifying damage in flexible mechanical structures.","This is achieved by identifying eigenfrequencies and eigenmodes from measurements and matching them against the results of a finite element simulation model, which leads to a nonsmooth nonlinear bi-objective parameter estimation problem.","A derivative-free optimization algorithm is required since the problem is nonsmooth and also because complex mechanical simulation models are often solved using commercial black-box software.","Moreover, the entire set of non-dominated solutions is of interest to practitioners.","Most solution approaches published to date are based on meta-heuristics such as genetic algorithms.","The proposed multi-objective pattern-search algorithm provides a mathematically well-founded alternative.","It features a novel sorting procedure that reduces the complexity in our context.","Test runs on two experimental structures with multiple damage scenarios are used to validate the approach.","The results demonstrate that the proposed algorithm yields accurate damage locations and requires moderate computational resources.","From the engineer's perspective it represents a promising tool for structural health monitoring."],"url":"http://arxiv.org/abs/2404.01896v1","category":"math.OC"}
{"created":"2024-04-02 12:25:16","title":"Enhancing Robot Navigation Efficiency Using Cellular Automata with Active Cells","abstract":"Enhancing robot navigation efficiency is a crucial objective in modern robotics. Robots relying on external navigation systems are often susceptible to electromagnetic interference (EMI) and encounter environmental disturbances, resulting in orientation errors within their surroundings. Therefore, the study employed an internal navigation system to enhance robot navigation efficacy under interference conditions, based on the analysis of the internal parameters and the external signals. This article presents details of the robot's autonomous operation, which allows for setting the robot's trajectory using an embedded map. The robot's navigation process involves counting the number of wheel revolutions as well as adjusting wheel orientation after each straight path section. In this article, an autonomous robot navigation system has been presented that leverages an embedded control navigation map utilising cellular automata with active cells which can effectively navigate in an environment containing various types of obstacles. By analysing the neighbouring cells of the active cell, the cellular environment determines which cell should become active during the robot's next movement step. This approach ensures the robot's independence from external control inputs. Furthermore, the accuracy and speed of the robot's movement have been further enhanced using a hexagonal mosaic for navigation surface mapping. This concept of utilising on cellular automata with active cells has been extended to the navigation of a group of robots on a shared navigation surface, taking into account the intersections of the robots' trajectories over time. To achieve this, a distance control module has been used that records the travelled trajectories in terms of wheel turns and revolutions.","sentences":["Enhancing robot navigation efficiency is a crucial objective in modern robotics.","Robots relying on external navigation systems are often susceptible to electromagnetic interference (EMI) and encounter environmental disturbances, resulting in orientation errors within their surroundings.","Therefore, the study employed an internal navigation system to enhance robot navigation efficacy under interference conditions, based on the analysis of the internal parameters and the external signals.","This article presents details of the robot's autonomous operation, which allows for setting the robot's trajectory using an embedded map.","The robot's navigation process involves counting the number of wheel revolutions as well as adjusting wheel orientation after each straight path section.","In this article, an autonomous robot navigation system has been presented that leverages an embedded control navigation map utilising cellular automata with active cells which can effectively navigate in an environment containing various types of obstacles.","By analysing the neighbouring cells of the active cell, the cellular environment determines which cell should become active during the robot's next movement step.","This approach ensures the robot's independence from external control inputs.","Furthermore, the accuracy and speed of the robot's movement have been further enhanced using a hexagonal mosaic for navigation surface mapping.","This concept of utilising on cellular automata with active cells has been extended to the navigation of a group of robots on a shared navigation surface, taking into account the intersections of the robots' trajectories over time.","To achieve this, a distance control module has been used that records the travelled trajectories in terms of wheel turns and revolutions."],"url":"http://arxiv.org/abs/2404.01885v1","category":"cs.RO"}
{"created":"2024-04-02 11:40:34","title":"Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model","abstract":"Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.","sentences":["Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction.","While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work.","There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information.","2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length.","To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos.","Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information.","Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos.","For better visual perception, we further design a refinement network focusing on missing details of certain areas.","Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations.","Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion."],"url":"http://arxiv.org/abs/2404.01862v1","category":"cs.CV"}
{"created":"2024-04-02 11:03:24","title":"Sketch3D: Style-Consistent Guidance for Sketch-to-3D Generation","abstract":"Recently, image-to-3D approaches have achieved significant results with a natural image as input. However, it is not always possible to access these enriched color input samples in practical applications, where only sketches are available. Existing sketch-to-3D researches suffer from limitations in broad applications due to the challenges of lacking color information and multi-view content. To overcome them, this paper proposes a novel generation paradigm Sketch3D to generate realistic 3D assets with shape aligned with the input sketch and color matching the textual description. Concretely, Sketch3D first instantiates the given sketch in the reference image through the shape-preserving generation process. Second, the reference image is leveraged to deduce a coarse 3D Gaussian prior, and multi-view style-consistent guidance images are generated based on the renderings of the 3D Gaussians. Finally, three strategies are designed to optimize 3D Gaussians, i.e., structural optimization via a distribution transfer mechanism, color optimization with a straightforward MSE loss and sketch similarity optimization with a CLIP-based geometric similarity loss. Extensive visual comparisons and quantitative analysis illustrate the advantage of our Sketch3D in generating realistic 3D assets while preserving consistency with the input.","sentences":["Recently, image-to-3D approaches have achieved significant results with a natural image as input.","However, it is not always possible to access these enriched color input samples in practical applications, where only sketches are available.","Existing sketch-to-3D researches suffer from limitations in broad applications due to the challenges of lacking color information and multi-view content.","To overcome them, this paper proposes a novel generation paradigm Sketch3D to generate realistic 3D assets with shape aligned with the input sketch and color matching the textual description.","Concretely, Sketch3D first instantiates the given sketch in the reference image through the shape-preserving generation process.","Second, the reference image is leveraged to deduce a coarse 3D Gaussian prior, and multi-view style-consistent guidance images are generated based on the renderings of the 3D Gaussians.","Finally, three strategies are designed to optimize 3D Gaussians, i.e., structural optimization via a distribution transfer mechanism, color optimization with a straightforward MSE loss and sketch similarity optimization with a CLIP-based geometric similarity loss.","Extensive visual comparisons and quantitative analysis illustrate the advantage of our Sketch3D in generating realistic 3D assets while preserving consistency with the input."],"url":"http://arxiv.org/abs/2404.01843v1","category":"cs.CV"}
{"created":"2024-04-02 10:59:17","title":"A zonogon approach for computing small polygons of maximum perimeter","abstract":"We derive a mixed integer nonlinear programming formulation for the problem of finding a convex polygon with a given number of vertices that is small (diameter at most one) and has maximum perimeter. The formulation is based on a geometric construction using zonogons. The resulting zonogons can be characterized by an integer code and we study the number of codes that are distinct under the symmetries of the problem. We propose a two-phase computational approach. Phase I comprises the solution of a purely combinatorial problem. Under assumption of Mossinghoff's conjecture, the Phase I problem can be reduced to a Subset-Sum Problem. Without Mossinghoff's conjecture, a generalized Subset-Sum Problem needs to be solved, which consists of picking $n$ non-opposing $2n$-th roots of unity such that their sum is as small as possible. Phase II consists of a non-combinatorial Nonlinear Programming Problem, which can be solved to high accuracy with arbitrary precision Newton-type methods. We provide extensive numerical results including highly accurate solutions for polygons with 64 and 128 vertices.","sentences":["We derive a mixed integer nonlinear programming formulation for the problem of finding a convex polygon with a given number of vertices that is small (diameter at most one) and has maximum perimeter.","The formulation is based on a geometric construction using zonogons.","The resulting zonogons can be characterized by an integer code and we study the number of codes that are distinct under the symmetries of the problem.","We propose a two-phase computational approach.","Phase I comprises the solution of a purely combinatorial problem.","Under assumption of Mossinghoff's conjecture, the Phase I problem can be reduced to a Subset-Sum Problem.","Without Mossinghoff's conjecture, a generalized Subset-Sum Problem needs to be solved, which consists of picking $n$ non-opposing $2n$-th roots of unity such that their sum is as small as possible.","Phase II consists of a non-combinatorial Nonlinear Programming Problem, which can be solved to high accuracy with arbitrary precision Newton-type methods.","We provide extensive numerical results including highly accurate solutions for polygons with 64 and 128 vertices."],"url":"http://arxiv.org/abs/2404.01841v1","category":"math.OC"}
{"created":"2024-04-02 10:44:55","title":"When does Subagging Work?","abstract":"We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform subagging if the size of its individual trees is not optimally chosen. This last result goes against common practice of growing large randomized trees to eliminate bias and then averaging to reduce variance.","sentences":["We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning.","First, we give sufficient conditions for pointwise consistency of trees.","We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance.","While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally.","Second, we compare the performance of subagging to that of trees across different numbers of splits.","We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits.","However, (3) a single tree grown at optimal size can outperform subagging if the size of its individual trees is not optimally chosen.","This last result goes against common practice of growing large randomized trees to eliminate bias and then averaging to reduce variance."],"url":"http://arxiv.org/abs/2404.01832v1","category":"stat.ML"}
{"created":"2024-04-02 10:42:45","title":"Sub-Riemannian optimal synthesis for Carnot groups with the structure of a path geometry","abstract":"This paper explicitly constructs the complete set of optimal sub-Riemannian geodesics starting from a point for certain Carnot groups of step two. These are groups of dimension 2n+1 equipped with a left-invariant distribution of dimension n+1 such that at each point, there is a unique direction defining a nontrivial Lie bracket. A suitable explicit expression of geodesics, together with symmetries of the structure, allows us to identify the cut time and the cut locus by applying the so-called extended Hadamard technique.","sentences":["This paper explicitly constructs the complete set of optimal sub-Riemannian geodesics starting from a point for certain Carnot groups of step two.","These are groups of dimension 2n+1 equipped with a left-invariant distribution of dimension n+1 such that at each point, there is a unique direction defining a nontrivial Lie bracket.","A suitable explicit expression of geodesics, together with symmetries of the structure, allows us to identify the cut time and the cut locus by applying the so-called extended Hadamard technique."],"url":"http://arxiv.org/abs/2404.01831v1","category":"math.DG"}
{"created":"2024-04-02 10:42:44","title":"Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy","abstract":"We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.","sentences":["We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown.","The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy.","When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators.","When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound.","We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods."],"url":"http://arxiv.org/abs/2404.01830v1","category":"stat.ML"}
{"created":"2024-04-02 10:42:22","title":"Synthesizing Control Lyapunov-Value Functions for High-Dimensional Systems Using System Decomposition and Admissible Control Sets","abstract":"Control Lyapunov functions (CLFs) play a vital role in modern control applications, but finding them remains a problem. Recently, the control Lyapunov-value function (CLVF) and robust CLVF have been proposed as solutions for nonlinear time-invariant systems with bounded control and disturbance. However, the CLVF suffers from the ''curse of dimensionality,'' which hinders its application to practical high-dimensional systems. In this paper, we propose a method to decompose systems of a particular coupled nonlinear structure, in order to solve for the CLVF in each low-dimensional subsystem. We then reconstruct the full-dimensional CLVF and provide sufficient conditions for when this reconstruction is exact. Moreover, a point-wise optimal controller can be obtained using a quadratic program. We also show that when the exact reconstruction is impossible, the subsystems' CLVFs and their ``admissible control sets'' can be used to generate a Lipschitz continuous CLF. We provide several numerical examples to validate the theory and show computational efficiency.","sentences":["Control Lyapunov functions (CLFs) play a vital role in modern control applications, but finding them remains a problem.","Recently, the control Lyapunov-value function (CLVF) and robust CLVF have been proposed as solutions for nonlinear time-invariant systems with bounded control and disturbance.","However, the CLVF suffers from the ''curse of dimensionality,'' which hinders its application to practical high-dimensional systems.","In this paper, we propose a method to decompose systems of a particular coupled nonlinear structure, in order to solve for the CLVF in each low-dimensional subsystem.","We then reconstruct the full-dimensional CLVF and provide sufficient conditions for when this reconstruction is exact.","Moreover, a point-wise optimal controller can be obtained using a quadratic program.","We also show that when the exact reconstruction is impossible, the subsystems' CLVFs and their ``admissible control sets'' can be used to generate a Lipschitz continuous CLF.","We provide several numerical examples to validate the theory and show computational efficiency."],"url":"http://arxiv.org/abs/2404.01829v1","category":"math.OC"}
{"created":"2024-04-03 17:58:59","title":"Electron-Phonon Coupling using Many-Body Perturbation Theory: Implementation in the Questaal Electronic Structure Suite","abstract":"The ability to calculate the electron-phonon coupling (e-ph) from first principles is of tremendous interest in materials science, as it provides a non-empirical approach to understand and predict a wide range of phenomena. While this has largely been accomplished in the Kohn-Sham framework of density functional theory (KS-DFT), it is becoming more apparent that standard approximations in KS-DFT can be inaccurate. These discrepancies are often attributed to a non-local potential where more advanced approaches to DFT or many-body perturbation theory have been used. However, a highly reliable and efficient first-principles approach to compute these quantities is still missing. With the goal of realizing a high-fidelity description of e-ph, we present a new field-theoretical methodology, incorporating the seminal work of Baym and Hedin within the quasiparticle self-consistent GW (QSGW) approximation, and the Questaal electronic structure package. We show within the response function framework employed here, that no Pulay corrections are needed to account for a change in the basis functions.","sentences":["The ability to calculate the electron-phonon coupling (e-ph) from first principles is of tremendous interest in materials science, as it provides a non-empirical approach to understand and predict a wide range of phenomena.","While this has largely been accomplished in the Kohn-Sham framework of density functional theory (KS-DFT), it is becoming more apparent that standard approximations in KS-DFT can be inaccurate.","These discrepancies are often attributed to a non-local potential where more advanced approaches to DFT or many-body perturbation theory have been used.","However, a highly reliable and efficient first-principles approach to compute these quantities is still missing.","With the goal of realizing a high-fidelity description of e-ph, we present a new field-theoretical methodology, incorporating the seminal work of Baym and Hedin within the quasiparticle self-consistent GW (QSGW) approximation, and the Questaal electronic structure package.","We show within the response function framework employed here, that no Pulay corrections are needed to account for a change in the basis functions."],"url":"http://arxiv.org/abs/2404.02902v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-03 17:46:38","title":"New Herbig-Haro Objects associated with Embedded Sources","abstract":"We continue to present the results of the Byurakan Narrow Band Imaging Survey (BNBIS). The main goal of this survey is to search for Herbig-Haro (HH) objects and jets in Galactic dark clouds. In this work we present the results of the search in the vicinity of infrared sources that are bright in the WISE survey and embedded in the dark clouds. The survey is performed with the 1 m Schmidt telescope of Byurakan Observatory, lately equipped with a new CCD detector, which allows to obtain one square degree images of the sky in various filters. Narrow-band filters were used to obtain Halpha and [S II] images, and a medium-width filter was used for the continuum imaging. New HH flows and knots were found near six embedded IR sources, which constitutes a significant proportion of the objects observed. At least two of the newly found HH flows (HH 1226 and HH 1227) lie in isolated dark clouds, thus pointing to active star formation in these regions. Other flows are also located in detached and dense globules or filaments. The length of the HH 1228 flow is about 1 pc; it has also a molecular hydrogen counterpart of the same extension. Coordinates, charts, detailed descriptions and distance estimates are provided. The lower limits of bolometric luminosities of the source stars are typical for low-mass young stellar objects.","sentences":["We continue to present the results of the Byurakan Narrow Band Imaging Survey (BNBIS).","The main goal of this survey is to search for Herbig-Haro (HH) objects and jets in Galactic dark clouds.","In this work we present the results of the search in the vicinity of infrared sources that are bright in the WISE survey and embedded in the dark clouds.","The survey is performed with the 1 m Schmidt telescope of Byurakan Observatory, lately equipped with a new CCD detector, which allows to obtain one square degree images of the sky in various filters.","Narrow-band filters were used to obtain Halpha and [S II] images, and a medium-width filter was used for the continuum imaging.","New HH flows and knots were found near six embedded IR sources, which constitutes a significant proportion of the objects observed.","At least two of the newly found HH flows (HH 1226 and HH 1227) lie in isolated dark clouds, thus pointing to active star formation in these regions.","Other flows are also located in detached and dense globules or filaments.","The length of the HH 1228 flow is about 1 pc; it has also a molecular hydrogen counterpart of the same extension.","Coordinates, charts, detailed descriptions and distance estimates are provided.","The lower limits of bolometric luminosities of the source stars are typical for low-mass young stellar objects."],"url":"http://arxiv.org/abs/2404.02891v1","category":"astro-ph.SR"}
{"created":"2024-04-03 17:25:35","title":"Modelling the star-formation activity and ionizing properties of high-redshift galaxies","abstract":"Early results from the JWST observations have reported a surprisingly high number of UV-bright galaxies at $z \\geq 10$, which appears to challenge the theoretical predictions from standard galaxy formation models in the $\\Lambda$CDM framework at these redshifts. To alleviate this tension, several cosmological and astrophysical interpretations have been advanced. However, all of these proposed scenarios carry noteworthy consequences for other large-scale processes in the early Universe, particularly cosmic reionization, since high-redshift galaxies are believed to be the primary ionizing sources during the Epoch of Reionization (EoR). To investigate this, we introduce a semi-analytical model of galaxy formation and evolution that explains the evolving galaxy UV luminosity function (UVLF) over $6 \\lesssim z \\lesssim 15$, and also jointly tracks the time evolution of the globally averaged neutral hydrogen fraction in the intergalactic medium. The model self-consistently accounts for the suppression of star formation in low-mass galaxies due to reionization feedback and is constrained by comparing the model predictions with various observational probes like the UVLF data from HST and JWST, recent measurements of the neutral hydrogen fraction, and the CMB scattering optical depth. Our analysis confirms that a rapid enhancement in the star-formation rate efficiency and/or UV luminosity per stellar mass formed is necessary for consistency with the JWST UVLF estimates at $z \\geq 10$. We further find that it is possible to jointly satisfy the current reionization constraints when the escape fraction is assumed to be halo-mass dependent, requiring higher Lyman-continuum leakage from low-mass galaxies. We also examine the relative contribution of galaxies with different UV luminosities towards the ionizing photon budget for the EoR and investigate the large-scale bias of high-$z$ galaxies.","sentences":["Early results from the JWST observations have reported a surprisingly high number of UV-bright galaxies at $z \\geq 10$, which appears to challenge the theoretical predictions from standard galaxy formation models in the $\\Lambda$CDM framework at these redshifts.","To alleviate this tension, several cosmological and astrophysical interpretations have been advanced.","However, all of these proposed scenarios carry noteworthy consequences for other large-scale processes in the early Universe, particularly cosmic reionization, since high-redshift galaxies are believed to be the primary ionizing sources during the Epoch of Reionization (EoR).","To investigate this, we introduce a semi-analytical model of galaxy formation and evolution that explains the evolving galaxy UV luminosity function (UVLF) over $6 \\lesssim z \\lesssim 15$, and also jointly tracks the time evolution of the globally averaged neutral hydrogen fraction in the intergalactic medium.","The model self-consistently accounts for the suppression of star formation in low-mass galaxies due to reionization feedback and is constrained by comparing the model predictions with various observational probes like the UVLF data from HST and JWST, recent measurements of the neutral hydrogen fraction, and the CMB scattering optical depth.","Our analysis confirms that a rapid enhancement in the star-formation rate efficiency and/or UV luminosity per stellar mass formed is necessary for consistency with the JWST UVLF estimates at $z \\geq 10$.","We further find that it is possible to jointly satisfy the current reionization constraints when the escape fraction is assumed to be halo-mass dependent, requiring higher Lyman-continuum leakage from low-mass galaxies.","We also examine the relative contribution of galaxies with different UV luminosities towards the ionizing photon budget for the EoR and investigate the large-scale bias of high-$z$ galaxies."],"url":"http://arxiv.org/abs/2404.02879v1","category":"astro-ph.GA"}
{"created":"2024-04-03 17:12:47","title":"Planck dust polarization power spectra are consistent with strongly supersonic turbulence","abstract":"The polarization of the Cosmic Microwave Background (CMB) is rich in information but obscured by foreground emission from the Milky Way's interstellar medium (ISM). To uncover relationships between the underlying turbulent ISM and the foreground power spectra, we simulated a suite of driven, magnetized, turbulent models of the ISM, varying the fluid properties via the sonic Mach number, Ms, and magnetic (Alfv\\'en) Mach number, Ma. We measure the power spectra of density ($\\rho$), velocity ($v$), magnetic field ($H$), total projected intensity ($T$), parity-even polarization ($E$), and parity-odd polarization ($B$). We find that the slopes of all six quantities increase with Ms and tend to increase with Ma. By comparing spectral slopes of $E$ and $B$ to those measured by Planck, we infer typical values of Ms and Ma for the ISM. As the fluid velocity increases, the ratio of BB power to EE power increases to approach a constant value near the Planck-observed value of $\\sim 0.5$ for $Ms > 4$, regardless of the magnetic field strength. We also examine correlation-coefficients between projected quantities, and find that $r^{TE}\\approx 0.3$, in agreement with Planck, for appropriate combinations of Ms and Ma. {Finally, we consider parity-violating correlations $r^{TB}$ and $r^{EB}$","sentences":["The polarization of the Cosmic Microwave Background (CMB) is rich in information but obscured by foreground emission from the Milky Way's interstellar medium (ISM).","To uncover relationships between the underlying turbulent ISM and the foreground power spectra, we simulated a suite of driven, magnetized, turbulent models of the ISM, varying the fluid properties via the sonic Mach number, Ms, and magnetic (Alfv\\'en)","Mach number, Ma.","We measure the power spectra of density ($\\rho$), velocity ($v$), magnetic field ($H$), total projected intensity ($T$), parity-even polarization ($E$), and parity-odd polarization ($B$).","We find that the slopes of all six quantities increase with Ms and tend to increase with Ma.","By comparing spectral slopes of $E$ and $B$ to those measured by Planck, we infer typical values of Ms and Ma for the ISM.","As the fluid velocity increases, the ratio of BB power to EE power increases to approach a constant value near the Planck-observed value of $\\sim 0.5$ for $Ms > 4$, regardless of the magnetic field strength.","We also examine correlation-coefficients between projected quantities, and find that $r^{TE}\\approx 0.3$, in agreement with Planck, for appropriate combinations of Ms and Ma.","{Finally, we consider parity-violating correlations $r^{TB}$ and $r^{EB}$"],"url":"http://arxiv.org/abs/2404.02874v1","category":"astro-ph.GA"}
{"created":"2024-04-03 16:19:39","title":"A Survey on Error-Bounded Lossy Compression for Scientific Datasets","abstract":"Error-bounded lossy compression has been effective in significantly reducing the data storage/transfer burden while preserving the reconstructed data fidelity very well. Many error-bounded lossy compressors have been developed for a wide range of parallel and distributed use cases for years. These lossy compressors are designed with distinct compression models and design principles, such that each of them features particular pros and cons. In this paper we provide a comprehensive survey of emerging error-bounded lossy compression techniques for different use cases each involving big data to process. The key contribution is fourfold. (1) We summarize an insightful taxonomy of lossy compression into 6 classic compression models. (2) We provide a comprehensive survey of 10+ commonly used compression components/modules used in error-bounded lossy compressors. (3) We provide a comprehensive survey of 10+ state-of-the-art error-bounded lossy compressors as well as how they combine the various compression modules in their designs. (4) We provide a comprehensive survey of the lossy compression for 10+ modern scientific applications and use-cases. We believe this survey is useful to multiple communities including scientific applications, high-performance computing, lossy compression, and big data.","sentences":["Error-bounded lossy compression has been effective in significantly reducing the data storage/transfer burden while preserving the reconstructed data fidelity very well.","Many error-bounded lossy compressors have been developed for a wide range of parallel and distributed use cases for years.","These lossy compressors are designed with distinct compression models and design principles, such that each of them features particular pros and cons.","In this paper we provide a comprehensive survey of emerging error-bounded lossy compression techniques for different use cases each involving big data to process.","The key contribution is fourfold.","(1) We summarize an insightful taxonomy of lossy compression into 6 classic compression models.","(2) We provide a comprehensive survey of 10+ commonly used compression components/modules used in error-bounded lossy compressors.","(3) We provide a comprehensive survey of 10+ state-of-the-art error-bounded lossy compressors as well as how they combine the various compression modules in their designs.","(4) We provide a comprehensive survey of the lossy compression for 10+ modern scientific applications and use-cases.","We believe this survey is useful to multiple communities including scientific applications, high-performance computing, lossy compression, and big data."],"url":"http://arxiv.org/abs/2404.02840v1","category":"cs.DC"}
{"created":"2024-04-03 16:11:42","title":"More on doubled Hilbert space in double-scaled SYK","abstract":"We continue our study of the doubled Hilbert space formalism of double-scaled SYK model initiated in [arXiv:2401.07403]. We show that the 1-particle Hilbert space introduced by Lin and Stanford in [arXiv:2307.15725] is related to the doubled Hilbert space $\\mathcal{H}_0\\otimes \\mathcal{H}_0$ of the 0-particle Hilbert space $\\mathcal{H}_0$ by some linear isomorphism $\\mathcal{C}$. It turns out that the entangler and disentangler appeared in our previous study are given by the dual (or adjoint) of $\\mathcal{C}^{-1}$ and $\\mathcal{C}$.","sentences":["We continue our study of the doubled Hilbert space formalism of double-scaled SYK model initiated in [arXiv:2401.07403].","We show that the 1-particle Hilbert space introduced by Lin and Stanford in [arXiv:2307.15725] is related to the doubled Hilbert space $\\mathcal{H}_0\\otimes \\mathcal{H}_0$ of the 0-particle Hilbert space $\\mathcal{H}_0$ by some linear isomorphism $\\mathcal{C}$. It turns out that the entangler and disentangler appeared in our previous study are given by the dual (or adjoint) of $\\mathcal{C}^{-1}$ and $\\mathcal{C}$."],"url":"http://arxiv.org/abs/2404.02833v1","category":"hep-th"}
{"created":"2024-04-03 15:37:12","title":"First Light and Reionization Epoch Simulations (FLARES) -- XV: The physical properties of super-massive black holes and their impact on galaxies in the early universe","abstract":"Understanding the co-evolution of super-massive black holes (SMBHs) and their host galaxies remains a key challenge of extragalactic astrophysics, particularly the earliest stages at high-redshift. However, studying SMBHs at high-redshift with cosmological simulations, is challenging due to the large volumes and high-resolution required. Through its innovative simulation strategy, the First Light And Reionisation Epoch Simulations (FLARES) suite of cosmological hydrodynamical zoom simulations allows us to simulate a much wider range of environments which contain SMBHs with masses extending to $M_{\\bullet}>10^{9}\\ M_{\\odot}$ at $z=5$. In this paper, we use FLARES to study the physical properties of SMBHs and their hosts in the early Universe ($5\\le\\, z \\le10$). FLARES predicts a sharply declining density with increasing redshift, decreasing by a factor of 100 over the range $z=5\\to 10$. Comparison between our predicted bolometric luminosity function and pre-\\emph{JWST} observations yield a good match. However, recent \\emph{JWST} observations appear to suggest a larger contribution of SMBHs than previously observed, or predicted by FLARES. Finally, by using a re-simulation with AGN feedback disabled, we explore the impact of AGN feedback on their host galaxies. This reveals that AGN feedback results in a reduction of star formation activity, even at $z>5$, but only in the most massive galaxies. A deeper analysis reveals that AGN are also the cause of suppressed star formation in passive galaxies but that the presence of an AGN doesn't necessarily result in the suppression of star formation.","sentences":["Understanding the co-evolution of super-massive black holes (SMBHs) and their host galaxies remains a key challenge of extragalactic astrophysics, particularly the earliest stages at high-redshift.","However, studying SMBHs at high-redshift with cosmological simulations, is challenging due to the large volumes and high-resolution required.","Through its innovative simulation strategy, the First Light And Reionisation Epoch Simulations (FLARES) suite of cosmological hydrodynamical zoom simulations allows us to simulate a much wider range of environments which contain SMBHs with masses extending to $M_{\\bullet}>10^{9}\\ M_{\\odot}$ at $z=5$. In this paper, we use FLARES to study the physical properties of SMBHs and their hosts in the early Universe ($5\\le\\, z \\le10$).","FLARES predicts a sharply declining density with increasing redshift, decreasing by a factor of 100 over the range $z=5\\to 10$. Comparison between our predicted bolometric luminosity function and pre-\\emph{JWST} observations yield a good match.","However, recent \\emph{JWST} observations appear to suggest a larger contribution of SMBHs than previously observed, or predicted by FLARES.","Finally, by using a re-simulation with AGN feedback disabled, we explore the impact of AGN feedback on their host galaxies.","This reveals that AGN feedback results in a reduction of star formation activity, even at $z>5$, but only in the most massive galaxies.","A deeper analysis reveals that AGN are also the cause of suppressed star formation in passive galaxies but that the presence of an AGN doesn't necessarily result in the suppression of star formation."],"url":"http://arxiv.org/abs/2404.02815v1","category":"astro-ph.GA"}
{"created":"2024-04-03 14:59:13","title":"Numerical relativity simulations of black hole and relativistic jet formation","abstract":"We investigate impacts of stellar rotation and magnetic fields on black hole (BH) formation and its subsequent explosive activities, by conducting axisymmetric radiation-magnetohydrodynamics simulations of gravitational collapse of a 70 $M_\\odot$ star with two-moment multi energy neutrino transport in numerical relativity. Due to its dense stellar structure, all models cannot avoid the eventual BH formation even though a strongly magnetized model experiences the so-called magnetorotational explosion prior to the BH formation. One intriguing phenomenon observed in the strongly magnetized model is the formation of a relativistic jet in the post-BH formation. The relativistic jet is the outcome of a combination of strong magnetic fields and low-density materials above the BH. The jet further enhances the explosion energy beyond $\\sim10^{52}$ erg, which is well exceeding the gravitational overburden ahead of the shock. Our self-consistent supernova models demonstrate that rotating magnetized massive stars at the high-mass end of supernova progenitors could be a potential candidate of hypernova and long gamma-ray burst progenitors.","sentences":["We investigate impacts of stellar rotation and magnetic fields on black hole (BH) formation and its subsequent explosive activities, by conducting axisymmetric radiation-magnetohydrodynamics simulations of gravitational collapse of a 70 $M_\\odot$ star with two-moment multi energy neutrino transport in numerical relativity.","Due to its dense stellar structure, all models cannot avoid the eventual BH formation even though a strongly magnetized model experiences the so-called magnetorotational explosion prior to the BH formation.","One intriguing phenomenon observed in the strongly magnetized model is the formation of a relativistic jet in the post-BH formation.","The relativistic jet is the outcome of a combination of strong magnetic fields and low-density materials above the BH.","The jet further enhances the explosion energy beyond $\\sim10^{52}$ erg, which is well exceeding the gravitational overburden ahead of the shock.","Our self-consistent supernova models demonstrate that rotating magnetized massive stars at the high-mass end of supernova progenitors could be a potential candidate of hypernova and long gamma-ray burst progenitors."],"url":"http://arxiv.org/abs/2404.02792v1","category":"astro-ph.HE"}
{"created":"2024-04-03 14:53:28","title":"AstroSat View of the Neutron Star Low-mass X-Ray Binary GX 5-1","abstract":"We present the spectral and timing study of the bright NS-LMXB GX 5-1 using \\textit{\\textit{AstroSat}/LAXPC} and \\textit{SXT} observations conducted in the year 2018. During the observation, the source traces out the complete HB and NB of the Z-track in the HID. Understanding the spectral and temporal evolution of the source along the 'Z' track can probe the accretion process in the vicinity of a neutron star. Spectral analysis was performed in the 0.7-20 keV energy range for different segments in the HID using a multi-temperature disc black body with an average temperature, $kT_{in} \\sim$0.46 and a thermal Comptonization model. It is found that the optical depth of the corona drops from $\\sim$6.68 in HB to $\\sim$2.74 in NB. The Timing analysis using the LAXPC instrument indicates the presence of quasi-periodic oscillations in HB, NB, and the hard apex of the Z-track. The observed QPO frequencies are similar to the characteristic frequencies of horizontal branch and normal branch oscillations. The HBO frequency increase from $\\sim$12-46 Hz towards the hard apex. The timing studies conducted in soft and hard band indicate the association of HBO and NBO origin with the non-thermal component. Further research could explore the implications of this relationship for understanding the dynamics of accretion onto neutron stars.","sentences":["We present the spectral and timing study of the bright NS-LMXB GX 5-1 using \\textit{\\textit{AstroSat}/LAXPC} and \\textit{SXT} observations conducted in the year 2018.","During the observation, the source traces out the complete HB and NB of the Z-track in the HID.","Understanding the spectral and temporal evolution of the source along the 'Z' track can probe the accretion process in the vicinity of a neutron star.","Spectral analysis was performed in the 0.7-20 keV energy range for different segments in the HID using a multi-temperature disc black body with an average temperature, $kT_{in} \\sim$0.46 and a thermal Comptonization model.","It is found that the optical depth of the corona drops from $\\sim$6.68 in HB to $\\sim$2.74 in NB.","The Timing analysis using the LAXPC instrument indicates the presence of quasi-periodic oscillations in HB, NB, and the hard apex of the Z-track.","The observed QPO frequencies are similar to the characteristic frequencies of horizontal branch and normal branch oscillations.","The HBO frequency increase from $\\sim$12-46 Hz towards the hard apex.","The timing studies conducted in soft and hard band indicate the association of HBO and NBO origin with the non-thermal component.","Further research could explore the implications of this relationship for understanding the dynamics of accretion onto neutron stars."],"url":"http://arxiv.org/abs/2404.02782v1","category":"astro-ph.HE"}
{"created":"2024-04-03 14:43:58","title":"A plethora of long-range neutrino interactions probed by DUNE and T2HK","abstract":"Upcoming neutrino experiments will soon search for new neutrino interactions more thoroughly than ever before, boosting the prospects of extending the Standard Model. In anticipation of this, we forecast the capability of two of the leading long-baseline neutrino oscillation experiments, DUNE and T2HK, to look for new flavor-dependent neutrino interactions with electrons, protons, and neutrons that could affect the transitions between different flavors. We interpret their sensitivity in the context of long-range neutrino interactions, mediated by a new neutral boson lighter than $10^{-10}$ eV, and sourced by the vast amount of nearby and distant matter in the Earth, Moon, Sun, Milky Way, and beyond. For the first time, we explore the sensitivity of DUNE and T2HK to a wide variety of $U(1)^\\prime$ symmetries, built from combinations of lepton and baryon numbers, each of which induces new interactions that affect oscillations differently. We find ample sensitivity: in all cases, DUNE and T2HK may constrain the existence of the new interaction even if it is supremely feeble, may discover it, and, in some cases, may identify the symmetry responsible for it.","sentences":["Upcoming neutrino experiments will soon search for new neutrino interactions more thoroughly than ever before, boosting the prospects of extending the Standard Model.","In anticipation of this, we forecast the capability of two of the leading long-baseline neutrino oscillation experiments, DUNE and T2HK, to look for new flavor-dependent neutrino interactions with electrons, protons, and neutrons that could affect the transitions between different flavors.","We interpret their sensitivity in the context of long-range neutrino interactions, mediated by a new neutral boson lighter than $10^{-10}$ eV, and sourced by the vast amount of nearby and distant matter in the Earth, Moon, Sun, Milky Way, and beyond.","For the first time, we explore the sensitivity of DUNE and T2HK to a wide variety of $U(1)^\\prime$ symmetries, built from combinations of lepton and baryon numbers, each of which induces new interactions that affect oscillations differently.","We find ample sensitivity: in all cases, DUNE and T2HK may constrain the existence of the new interaction even if it is supremely feeble, may discover it, and, in some cases, may identify the symmetry responsible for it."],"url":"http://arxiv.org/abs/2404.02775v1","category":"hep-ph"}
{"created":"2024-04-03 13:43:56","title":"Probing the spatial distribution of gluons within the proton in the coherent vector meson production at large $|t|$","abstract":"The coherent production of vector mesons in photon-hadron interactions is considered one of the most promising observables to probe the QCD dynamics at high energies and the transverse spatial distribution of the gluons in the hadron wave function. In this letter, we perform an exploratory study about the dependence of the transverse momentum distributions, $d\\sigma/dt$, on the model assumed for the proton density profile. We consider a set of non-Gaussian profiles and include them in the forward dipole-proton scattering amplitude associated with the IP-sat model. We demonstrate that the predictions for $d\\sigma/dt$ are similar in the HERA kinematical range, but are very distinct for values of $t$ that will be probed in future colliders. In particular, the presence of diffractive dips in the $t$-distributions associated with the production of $J/\\Psi$ and $\\rho$ mesons is strongly dependent on the model assumed for the density profile. Similar conclusions are also derived when the non-linear effects on the dipole-target interaction are disregarded. Our results indicate that a future study of the coherent vector meson production at large $t$ will be useful to constrain the spatial distribution of gluons within the proton.","sentences":["The coherent production of vector mesons in photon-hadron interactions is considered one of the most promising observables to probe the QCD dynamics at high energies and the transverse spatial distribution of the gluons in the hadron wave function.","In this letter, we perform an exploratory study about the dependence of the transverse momentum distributions, $d\\sigma/dt$, on the model assumed for the proton density profile.","We consider a set of non-Gaussian profiles and include them in the forward dipole-proton scattering amplitude associated with the IP-sat model.","We demonstrate that the predictions for $d\\sigma/dt$ are similar in the HERA kinematical range, but are very distinct for values of $t$ that will be probed in future colliders.","In particular, the presence of diffractive dips in the $t$-distributions associated with the production of $J/\\Psi$ and $\\rho$ mesons is strongly dependent on the model assumed for the density profile.","Similar conclusions are also derived when the non-linear effects on the dipole-target interaction are disregarded.","Our results indicate that a future study of the coherent vector meson production at large $t$ will be useful to constrain the spatial distribution of gluons within the proton."],"url":"http://arxiv.org/abs/2404.02746v1","category":"hep-ph"}
{"created":"2024-04-03 13:39:22","title":"Emergent Elastic Surfaces from Two-Dimensional Dirac Materials","abstract":"Temperature constraints are highly desirable in the experimental setup when seeking the synthesis of new carbon structures. Fluctuations of the Dirac field result in temperature-dependent corrections to the Helfrich-Canham formulation, which governs the classical elasticity of the membrane in equilibrium state. Here, we examine the emergent shapes allowed by the effective model up to quadratic order in Ricci curvature and discuss the constraints required to observe them. We determine the mechanical stability conditions and provide a phase diagram characterized by the appearance of a critical temperature $T_c$ that distinguishes between carbon nanotube and fullerene phases. The observation of minimal and developable surfaces is anticipated, respectively, at the high- and low-temperature regimes.","sentences":["Temperature constraints are highly desirable in the experimental setup when seeking the synthesis of new carbon structures.","Fluctuations of the Dirac field result in temperature-dependent corrections to the Helfrich-Canham formulation, which governs the classical elasticity of the membrane in equilibrium state.","Here, we examine the emergent shapes allowed by the effective model up to quadratic order in Ricci curvature and discuss the constraints required to observe them.","We determine the mechanical stability conditions and provide a phase diagram characterized by the appearance of a critical temperature $T_c$ that distinguishes between carbon nanotube and fullerene phases.","The observation of minimal and developable surfaces is anticipated, respectively, at the high- and low-temperature regimes."],"url":"http://arxiv.org/abs/2404.02741v1","category":"hep-th"}
{"created":"2024-04-03 13:17:47","title":"Diamond on Kurepa trees","abstract":"We introduce a new weak variation of diamond that is meant to only guess the branches of a Kurepa tree. We demonstrate that this variation is considerably weaker than diamond by proving it is compatible with Martin's axiom. We then prove that this principle is nontrivial by showing it may consistently fail.","sentences":["We introduce a new weak variation of diamond that is meant to only guess the branches of a Kurepa tree.","We demonstrate that this variation is considerably weaker than diamond by proving it is compatible with Martin's axiom.","We then prove that this principle is nontrivial by showing it may consistently fail."],"url":"http://arxiv.org/abs/2404.02715v1","category":"math.LO"}
{"created":"2024-04-03 12:58:30","title":"Quantum Mechanical Softening of the Hypertriton Transverse Momentum Spectrum in Heavy-Ion Collisions","abstract":"Understanding the properties of hypernuclei helps to constrain the interaction between hyperon and nucleon, which is known to play an essential role in determining the properties of neutron stars. Experimental measurements have suggested that the hypertriton ($^3_\\Lambda \\text{H}$), the lightest hypernucleus, exhibits a halo structure with a deuteron core encircled by a $\\Lambda$ hyperon at a distance of about 10 fm. This large $\\Lambda-d$ distance in $^3_\\Lambda \\text{H}$ wave function is found to cause a suppressed $^3_\\Lambda \\text{H}$ yield and a softening of its transverse momentum ($p_T$) spectrum in relativistic heavy-ion collisions. Within the coalescence model based on nucleons and $\\Lambda$ hyperons from a microscopic hybrid hydro model with a hadronic afterburner for nuclear cluster production in Pb-Pb collisions at $\\sqrt{s_{NN}}$= 5.02 TeV, we show how this softening of the hypertriton $p_T$ spectrum appears and leads to a significantly smaller mean $p_T$ for $^3_\\Lambda \\text{H}$ than for helium-3 ($^3$He). The latter is opposite to the predictions from the blast-wave model which assumes that $^3_\\Lambda \\text{H}$ and $^3$He are thermally produced at the kinetic freeze-out of heavy ion collisions. The discovered quantum mechanical softening of the (anti-)hypertriton spectrum can be experimentally tested in relativistic heavy-ion collisions at different collision energies and centralities and used to obtain valuable insights to the mechanisms for light (hyper-)nuclei production in these collisions.","sentences":["Understanding the properties of hypernuclei helps to constrain the interaction between hyperon and nucleon, which is known to play an essential role in determining the properties of neutron stars.","Experimental measurements have suggested that the hypertriton ($^3_\\Lambda \\text{H}$), the lightest hypernucleus, exhibits a halo structure with a deuteron core encircled by a $\\Lambda$ hyperon at a distance of about 10 fm.","This large $\\Lambda-d$ distance in $^3_\\Lambda \\text{H}$ wave function is found to cause a suppressed $^3_\\Lambda \\text{H}$ yield and a softening of its transverse momentum ($p_T$) spectrum in relativistic heavy-ion collisions.","Within the coalescence model based on nucleons and $\\Lambda$ hyperons from a microscopic hybrid hydro model with a hadronic afterburner for nuclear cluster production in Pb-Pb collisions at $\\sqrt{s_{NN}}$= 5.02 TeV, we show how this softening of the hypertriton $p_T$ spectrum appears and leads to a significantly smaller mean $p_T$ for $^3_\\Lambda \\text{H}$ than for helium-3 ($^3$He).","The latter is opposite to the predictions from the blast-wave model which assumes that $^3_\\Lambda \\text{H}$ and $^3$He are thermally produced at the kinetic freeze-out of heavy ion collisions.","The discovered quantum mechanical softening of the (anti-)hypertriton spectrum can be experimentally tested in relativistic heavy-ion collisions at different collision energies and centralities and used to obtain valuable insights to the mechanisms for light (hyper-)nuclei production in these collisions."],"url":"http://arxiv.org/abs/2404.02701v2","category":"nucl-th"}
{"created":"2024-04-03 12:22:24","title":"Rendering string diagrams recursively","abstract":"String diagrams are a graphical language used to represent processes that can be composed sequentially or in parallel, which correspond graphically to horizontal or vertical juxtaposition. In this paper we demonstrate how to compute the layout of a string diagram by folding over its algebraic representation in terms of sequential and parallel composition operators. The algebraic representation can be seen as a term of a free monoidal category or a proof tree for a small fragment of linear logic. This contrasts to existing non-compositional approaches that use graph layout techniques. The key innovation is storing the diagrams in binary space-partition trees, maintaining a right-trapezoidal shape for the diagram's outline as an invariant.   We provide an implementation in Haskell, using an existing denotational graphics library called Diagrams. Our renderer also supports adding semantics to diagrams to serve as a compiler, with matrix algebra used as an example.","sentences":["String diagrams are a graphical language used to represent processes that can be composed sequentially or in parallel, which correspond graphically to horizontal or vertical juxtaposition.","In this paper we demonstrate how to compute the layout of a string diagram by folding over its algebraic representation in terms of sequential and parallel composition operators.","The algebraic representation can be seen as a term of a free monoidal category or a proof tree for a small fragment of linear logic.","This contrasts to existing non-compositional approaches that use graph layout techniques.","The key innovation is storing the diagrams in binary space-partition trees, maintaining a right-trapezoidal shape for the diagram's outline as an invariant.   ","We provide an implementation in Haskell, using an existing denotational graphics library called Diagrams.","Our renderer also supports adding semantics to diagrams to serve as a compiler, with matrix algebra used as an example."],"url":"http://arxiv.org/abs/2404.02679v1","category":"math.CT"}
{"created":"2024-04-03 12:09:39","title":"Geometry of Kupershmidt's operators and non-commutative probability theory","abstract":"We have developed a comprehensive framework to calculate the envelope of particular deformations of PostLie algebras. We have applied this framework to operator-valued non-commutative probability by introducing new transforms that enable the computation of conditionally free and conditionally monotone multiplicative convolutions of operator-valued non-commutative distributions.","sentences":["We have developed a comprehensive framework to calculate the envelope of particular deformations of PostLie algebras.","We have applied this framework to operator-valued non-commutative probability by introducing new transforms that enable the computation of conditionally free and conditionally monotone multiplicative convolutions of operator-valued non-commutative distributions."],"url":"http://arxiv.org/abs/2404.02670v1","category":"math.OA"}
{"created":"2024-04-03 10:31:28","title":"Possible X-ray Cocoon Emission from GRB 050709","abstract":"The detection of the short gamma-ray burst (SGRB) 050709 by the HETE-2 satellite opened a new window into understanding the nature of SGRBs, offering clues about their emission mechanism and progenitors, with the crucial aid of optical follow-up observations. Here, we revisit the prompt emission of GRB 050709. Our analysis reveals an initial hard spike ~200 ms long, followed by a subsequent soft tail emission lasting ~300 ms. These components could be common among other SGRBs originating from binary neutron merger events, such as GW/GRB 170817A. Detailed temporal and spectral analyses indicate that the soft tail emission might be attributed to the cocoon formed by the relativistic jet depositing energy into the surrounding material. We find the necessary cocoon parameters at the breakout, as consistent with numerical simulation results. We compared the physical parameters of this cocoon with those of other SGRBs. The relatively higher cocoon pressure and temperature in GRB 050709 may indicate a more on-axis jet compared to GRB 170817A and GRB 150101B.","sentences":["The detection of the short gamma-ray burst (SGRB) 050709 by the HETE-2 satellite opened a new window into understanding the nature of SGRBs, offering clues about their emission mechanism and progenitors, with the crucial aid of optical follow-up observations.","Here, we revisit the prompt emission of GRB 050709.","Our analysis reveals an initial hard spike ~200 ms long, followed by a subsequent soft tail emission lasting ~300 ms.","These components could be common among other SGRBs originating from binary neutron merger events, such as GW/GRB 170817A. Detailed temporal and spectral analyses indicate that the soft tail emission might be attributed to the cocoon formed by the relativistic jet depositing energy into the surrounding material.","We find the necessary cocoon parameters at the breakout, as consistent with numerical simulation results.","We compared the physical parameters of this cocoon with those of other SGRBs.","The relatively higher cocoon pressure and temperature in GRB 050709 may indicate a more on-axis jet compared to GRB 170817A and GRB 150101B."],"url":"http://arxiv.org/abs/2404.02627v1","category":"astro-ph.HE"}
{"created":"2024-04-03 09:57:13","title":"Searches for multi-Z boson productions and anomalous gauge boson couplings at a muon collider","abstract":"Multi-boson productions can be exploited as novel probes either for standard model precision tests or new physics searches, and have become one of those popular topics in the ongoing LHC experiments, and in future collider studies, including those for electron-positron and muon-muon colliders. Here we focus on two examples, i.e., ZZZ direct productions through $\\mu^{+}\\mu^{-}$ annihilation at a 1 TeV muon collider, and ZZ productions through vector boson scattering at a 10 TeV muon collider, with an integrated luminosity of $1-10 \\, \\text{ab}^{-1}$. Various channels are considered, including, such as $ZZZ \\rightarrow 4l2\\nu$ and $ZZZ \\rightarrow 4l + 2 \\text{ jets}$, etc. Expected significance on these multi-Z boson production processes are provided based on a detailed Monte Carlo study and signal background analysis. Sensitives on anomalous gauge boson couplings are also presented.","sentences":["Multi-boson productions can be exploited as novel probes either for standard model precision tests or new physics searches, and have become one of those popular topics in the ongoing LHC experiments, and in future collider studies, including those for electron-positron and muon-muon colliders.","Here we focus on two examples, i.e., ZZZ direct productions through $\\mu^{+}\\mu^{-}$ annihilation at a 1 TeV muon collider, and ZZ productions through vector boson scattering at a 10 TeV muon collider, with an integrated luminosity of $1-10 \\, \\text{ab}^{-1}$. Various channels are considered, including, such as $ZZZ \\rightarrow 4l2\\nu$ and $ZZZ \\rightarrow 4l + 2 \\text{ jets}$, etc. Expected significance on these multi-Z boson production processes are provided based on a detailed Monte Carlo study and signal background analysis.","Sensitives on anomalous gauge boson couplings are also presented."],"url":"http://arxiv.org/abs/2404.02613v1","category":"hep-ex"}
{"created":"2024-04-03 09:52:57","title":"Chiral symmetry restoration at finite temperature in a model with manifest confinement","abstract":"Multiple lattice evidences support the existence of a confining but chirally symmetric regime of QCD above the chiral symmetry restoration crossover at Tch ~ 155 MeV. This regime is characterised by an approximate chiral spin symmetry of the partition function, which is a symmetry of the colour charge and the confining electric part of the QCD Lagrangian. It is traditionally believed that confinement should automatically induce spontaneous breaking of chiral symmetry, which would preclude the existence of a confining but chirally symmetric regime of QCD at high temperatures. We employ a well-known solvable quark model for QCD in 3+1 dimensions that is chirally symmetric and manifestly confining and argue that while confinement indeed induces dynamical breaking of chiral symmetry at T=0, a chiral restoration phase transition takes place at some critical temperature Tch. Above this temperature, the spectrum of the model consists of chirally symmetric hadrons with approximate chiral spin symmetry.","sentences":["Multiple lattice evidences support the existence of a confining but chirally symmetric regime of QCD above the chiral symmetry restoration crossover at Tch ~ 155 MeV.","This regime is characterised by an approximate chiral spin symmetry of the partition function, which is a symmetry of the colour charge and the confining electric part of the QCD Lagrangian.","It is traditionally believed that confinement should automatically induce spontaneous breaking of chiral symmetry, which would preclude the existence of a confining but chirally symmetric regime of QCD at high temperatures.","We employ a well-known solvable quark model for QCD in 3+1 dimensions that is chirally symmetric and manifestly confining and argue that while confinement indeed induces dynamical breaking of chiral symmetry at T=0, a chiral restoration phase transition takes place at some critical temperature Tch.","Above this temperature, the spectrum of the model consists of chirally symmetric hadrons with approximate chiral spin symmetry."],"url":"http://arxiv.org/abs/2404.02606v1","category":"hep-ph"}
{"created":"2024-04-03 09:51:37","title":"NLTE modelling of water-rich exoplanet atmospheres. Cooling and heating rates","abstract":"The hydrogen and water molecules respond very differently to the collisional-radiative processes taking place in planetary atmospheres. Naturally, the question arises whether H2O-rich atmospheres are more (or less) resilient to long-term mass loss than H2-dominated ones if they radiate away the incident stellar energy more (or less) efficiently. If confirmed, the finding would have implications on our understanding of the evolution of sub-Neptune exoplanets. As a key step towards answering this question, we present a non-local thermodynamic equilibrium (NLTE) model of H2O for the atmospheric region where the gas accelerates to escape the planet and conditions relevant to close-in sub-Neptunes. Our exploratory calculations for isothermal gas composed of H2, H2O and e- reveal that: 1) In the pressure region ~1e-2 - 1e-4 dyn cm-2 where the stellar extreme-ultraviolet (XUV) photons are typically deposited in the atmosphere, H2O is in rotational LTE but vibrational NLTE. Vibrational LTE is facilitated by high H2O abundances and fractional ionizations, and we report critical densities for the LTE-NLTE transition; 2) Vibrational cooling may locally dominate over rotational cooling, partly because of the comparatively small opacities of ro-vibrational lines; 3) Even low H2O abundances notably enhance the cooling, foreseeably offsetting some of the stellar heating; 4) Heating due to the deposition of stellar infrared (IR) photons is significant at pressures >=0.1 dyn cm-2. We estimate the contribution of H2O excitation to the internal energy of the gas and speculate on the photodissociation from the excited vibrational states. Ultimately, our findings motivate the consideration of NLTE in the mass loss rate calculations of H2O-rich atmospheres.","sentences":["The hydrogen and water molecules respond very differently to the collisional-radiative processes taking place in planetary atmospheres.","Naturally, the question arises whether H2O-rich atmospheres are more (or less) resilient to long-term mass loss than H2-dominated ones if they radiate away the incident stellar energy more (or less) efficiently.","If confirmed, the finding would have implications on our understanding of the evolution of sub-Neptune exoplanets.","As a key step towards answering this question, we present a non-local thermodynamic equilibrium (NLTE) model of H2O for the atmospheric region where the gas accelerates to escape the planet and conditions relevant to close-in sub-Neptunes.","Our exploratory calculations for isothermal gas composed of H2, H2O and e- reveal that: 1) In the pressure region ~1e-2 - 1e-4 dyn cm-2 where the stellar extreme-ultraviolet (XUV) photons are typically deposited in the atmosphere, H2O is in rotational LTE but vibrational NLTE.","Vibrational LTE is facilitated by high H2O abundances and fractional ionizations, and we report critical densities for the LTE-NLTE transition; 2) Vibrational cooling may locally dominate over rotational cooling, partly because of the comparatively small opacities of ro-vibrational lines; 3) Even low H2O abundances notably enhance the cooling, foreseeably offsetting some of the stellar heating; 4) Heating due to the deposition of stellar infrared (IR) photons is significant at pressures >=0.1 dyn cm-2.","We estimate the contribution of H2O excitation to the internal energy of the gas and speculate on the photodissociation from the excited vibrational states.","Ultimately, our findings motivate the consideration of NLTE in the mass loss rate calculations of H2O-rich atmospheres."],"url":"http://arxiv.org/abs/2404.02604v1","category":"astro-ph.EP"}
{"created":"2024-04-03 08:37:39","title":"Detached Circumstellar Matter as an Explanation for Slowly-Rising Interacting Type Ibc Supernovae","abstract":"Some hydrogen-poor (Type Ibc) supernovae (SNe) are known to have massive circumstellar matter (CSM) that are well detached from the star. Using the open-source code CHIPS, we construct a grid of models of SN Ibc interacting with detached CSM. We find that interaction with detached CSM can produce a slowly rising phase in the light curve seen in some interacting SN Ibc, which is difficult to reproduce by interaction with CSM of a density profile motivated from wind or eruptions that are continuous down to the star. We also show that SNe having double peaks in their light curves with timescales of months (e.g., SN 2022xxf) can be explained by radioactive decay of $^{56}$Ni/$^{56}$Co, followed by interaction with detached CSM.","sentences":["Some hydrogen-poor (Type Ibc) supernovae (SNe) are known to have massive circumstellar matter (CSM) that are well detached from the star.","Using the open-source code CHIPS, we construct a grid of models of SN Ibc interacting with detached CSM.","We find that interaction with detached CSM can produce a slowly rising phase in the light curve seen in some interacting SN Ibc, which is difficult to reproduce by interaction with CSM of a density profile motivated from wind or eruptions that are continuous down to the star.","We also show that SNe having double peaks in their light curves with timescales of months (e.g., SN 2022xxf) can be explained by radioactive decay of $^{56}$Ni/$^{56}$Co, followed by interaction with detached CSM."],"url":"http://arxiv.org/abs/2404.02566v1","category":"astro-ph.HE"}
{"created":"2024-04-03 07:43:59","title":"Holographic Carrollian Conformal Scalars","abstract":"We provide holographic realisations in Minkowski spacetime of a free conformal Carrollian scalar field living at null infinity. To this end, we first show that the electric and magnetic limits of a relativistic conformal scalar are equivalent and we study the representation of the Carroll, Poincar\\'e and BMS algebras that is realised on the resulting solution space. We then realise it as a quotient of the solution space of a free massless scalar in Minkowski spacetime with unusual falloff, in full analogy with the interpretation of Dirac's singleton as a shortened scalar in Anti de Sitter spacetime.","sentences":["We provide holographic realisations in Minkowski spacetime of a free conformal Carrollian scalar field living at null infinity.","To this end, we first show that the electric and magnetic limits of a relativistic conformal scalar are equivalent and we study the representation of the Carroll, Poincar\\'e and BMS algebras that is realised on the resulting solution space.","We then realise it as a quotient of the solution space of a free massless scalar in Minkowski spacetime with unusual falloff, in full analogy with the interpretation of Dirac's singleton as a shortened scalar in Anti de Sitter spacetime."],"url":"http://arxiv.org/abs/2404.02533v1","category":"hep-th"}
{"created":"2024-04-03 04:44:27","title":"Exploring the Connection Between the Normalized Power Prior and Bayesian Hierarchical Models","abstract":"The power prior is a popular class of informative priors for incorporating information from historical data. It involves raising the likelihood for the historical data to a power, which acts as a discounting parameter. When the discounting parameter is modeled as random, the normalized power prior is recommended. Bayesian hierarchical modeling is a widely used method for synthesizing information from different sources, including historical data. In this work, we examine the analytical relationship between the normalized power prior (NPP) and Bayesian hierarchical models (BHM) for \\emph{i.i.d.} normal data. We establish a direct relationship between the prior for the discounting parameter of the NPP and the prior for the variance parameter of the BHM. Such a relationship is first established for the case of a single historical dataset, and then extended to the case with multiple historical datasets with dataset-specific discounting parameters. For multiple historical datasets, we develop and establish theory for the BHM-matching NPP (BNPP) which establishes dependence between the dataset-specific discounting parameters leading to inferences that are identical to the BHM. Establishing this relationship not only justifies the NPP from the perspective of hierarchical modeling, but also provides insight on prior elicitation for the NPP. We present strategies on inducing priors on the discounting parameter based on hierarchical models, and investigate the borrowing properties of the BNPP.","sentences":["The power prior is a popular class of informative priors for incorporating information from historical data.","It involves raising the likelihood for the historical data to a power, which acts as a discounting parameter.","When the discounting parameter is modeled as random, the normalized power prior is recommended.","Bayesian hierarchical modeling is a widely used method for synthesizing information from different sources, including historical data.","In this work, we examine the analytical relationship between the normalized power prior (NPP) and Bayesian hierarchical models (BHM) for \\emph{i.i.d.} normal data.","We establish a direct relationship between the prior for the discounting parameter of the NPP and the prior for the variance parameter of the BHM.","Such a relationship is first established for the case of a single historical dataset, and then extended to the case with multiple historical datasets with dataset-specific discounting parameters.","For multiple historical datasets, we develop and establish theory for the BHM-matching NPP (BNPP) which establishes dependence between the dataset-specific discounting parameters leading to inferences that are identical to the BHM.","Establishing this relationship not only justifies the NPP from the perspective of hierarchical modeling, but also provides insight on prior elicitation for the NPP.","We present strategies on inducing priors on the discounting parameter based on hierarchical models, and investigate the borrowing properties of the BNPP."],"url":"http://arxiv.org/abs/2404.02453v1","category":"stat.ME"}
{"created":"2024-04-03 04:37:14","title":"The Societal Implications of Blockchain Proliferation","abstract":"Blockchain and its distributed ledger technology have far-reaching implications for consumers across the world. Cryptocurrencies like XRP work to solve key issues in the remittance industry, targeting corridors like Mexico where foreign remittance fuels economies. Blockchain's libertarian principles have the potential to change lives in the third world, replacing corrupt infrastructure with trust-based solutions. While this technology can be used to significantly improve lives, it has a wealth of destructive applications. Bitcoin's blockchain and nefarious websites like the Silk Road have fueled an underground market of drugs, money laundering, and terrorism, complicating digital currency legislation. The negative environmental effects of cryptocurrency may also contribute significantly to global climate change. Negatives aside, cryptocurrency still proves to be a valuable commodity in technological development.","sentences":["Blockchain and its distributed ledger technology have far-reaching implications for consumers across the world.","Cryptocurrencies like XRP work to solve key issues in the remittance industry, targeting corridors like Mexico where foreign remittance fuels economies.","Blockchain's libertarian principles have the potential to change lives in the third world, replacing corrupt infrastructure with trust-based solutions.","While this technology can be used to significantly improve lives, it has a wealth of destructive applications.","Bitcoin's blockchain and nefarious websites like the Silk Road have fueled an underground market of drugs, money laundering, and terrorism, complicating digital currency legislation.","The negative environmental effects of cryptocurrency may also contribute significantly to global climate change.","Negatives aside, cryptocurrency still proves to be a valuable commodity in technological development."],"url":"http://arxiv.org/abs/2404.02451v1","category":"cs.CY"}
{"created":"2024-04-03 04:27:07","title":"Entanglement entropy in type II$_1$ von Neumann algebra: examples in Double-Scaled SYK","abstract":"An intriguing feature of type II$_1$ von Neumann algebra is that the entropy of the mixed states is negative. Although the type classification of von Neumann algebra and its consequence in holography have been extensively explored recently, there has not been an explicit calculation of entropy in some physically interesting models with type II$_1$ algebra. In this paper, we study the entanglement entropy $S_n$ of the fixed length state $\\{|n\\rangle\\}$ in Double-Scaled Sachdev-Ye-Kitaev model, which has been recently shown to exhibit type II$_1$ von Neumann algebra. These states furnish an orthogonal basis for 0-particle chord Hilbert space. We systematically study $S_n$ and its R\\'enyi generalizations $S_n^{(m)}$ in various limit of DSSYK model, ranging $q\\in[0,1]$. We obtain exotic analytical expressions for the scaling behavior of $S_n^{(m)}$ at large $n$ for random matrix theory limit ($q=0$) and SYK$_2$ limit ($q=1$), for the former we observe highly non-flat entanglement spectrum. We then dive into triple scaling limits where the fixed chord number states become the geodesic wormholes with definite length connecting left/right AdS$_2$ boundary in Jackiw-Teitelboim gravity. In semi-classical regime, we match the boundary calculation of entanglement entropy with the dilaton value at the center of geodesic, as a nontrivial check of the Ryu-Takayanagi formula.","sentences":["An intriguing feature of type II$_1$ von Neumann algebra is that the entropy of the mixed states is negative.","Although the type classification of von Neumann algebra and its consequence in holography have been extensively explored recently, there has not been an explicit calculation of entropy in some physically interesting models with type II$_1$ algebra.","In this paper, we study the entanglement entropy $S_n$ of the fixed length state $\\{|n\\rangle\\}$ in Double-Scaled Sachdev-Ye-Kitaev model, which has been recently shown to exhibit type II$_1$ von Neumann algebra.","These states furnish an orthogonal basis for 0-particle chord Hilbert space.","We systematically study $S_n$ and its R\\'enyi generalizations $S_n^{(m)}$ in various limit of DSSYK model, ranging $q\\in[0,1]$. We obtain exotic analytical expressions for the scaling behavior of $S_n^{(m)}$ at large $n$ for random matrix theory limit ($q=0$) and SYK$_2$ limit ($q=1$), for the former we observe highly non-flat entanglement spectrum.","We then dive into triple scaling limits where the fixed chord number states become the geodesic wormholes with definite length connecting left/right AdS$_2$ boundary in Jackiw-Teitelboim gravity.","In semi-classical regime, we match the boundary calculation of entanglement entropy with the dilaton value at the center of geodesic, as a nontrivial check of the Ryu-Takayanagi formula."],"url":"http://arxiv.org/abs/2404.02449v1","category":"hep-th"}
{"created":"2024-04-03 04:02:18","title":"Automatic generation of orthogonal multiplet bases in $\\mathrm{SU}(N_c)$ color space","abstract":"We present a software that automatically generates a multiplet color basis for general $2 \\to n$ processes in quantum chromodynamics (QCD). The construction process is guided by the decomposition of the corresponding $\\mathrm{SU}(N_c)$ representation into a direct sum of irreducible representations. The projectors of these irreducible multiplet states are then used to construct a minimal, linearly independent basis. The software achieves highest efficiency through a combination of the Z3 satisfiability modulo theories solver and the symbolic manipulation program FORM. The resulting color basis enables the analytical execution of involved QCD calculations that require color decomposition.","sentences":["We present a software that automatically generates a multiplet color basis for general $2 \\to n$ processes in quantum chromodynamics (QCD).","The construction process is guided by the decomposition of the corresponding $\\mathrm{SU}(N_c)$ representation into a direct sum of irreducible representations.","The projectors of these irreducible multiplet states are then used to construct a minimal, linearly independent basis.","The software achieves highest efficiency through a combination of the Z3 satisfiability modulo theories solver and the symbolic manipulation program FORM.","The resulting color basis enables the analytical execution of involved QCD calculations that require color decomposition."],"url":"http://arxiv.org/abs/2404.02443v1","category":"hep-ph"}
{"created":"2024-04-03 03:37:22","title":"On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons","abstract":"Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism. We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire ``uniquely for each language'' within decoder-only multilingual PLMs. We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (< 5%) between languages. These neurons are mainly distributed in the models' first and last few layers. This trend remains consistent across languages and models. Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.","sentences":["Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities.","However, it is unclear how these models handle multilingualism.","We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire ``uniquely for each language'' within decoder-only multilingual PLMs.","We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (< 5%) between languages.","These neurons are mainly distributed in the models' first and last few layers.","This trend remains consistent across languages and models.","Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation."],"url":"http://arxiv.org/abs/2404.02431v1","category":"cs.CL"}
{"created":"2024-04-03 03:25:16","title":"Radio Scrutiny of the X-ray-Weak Tail of Low-Mass Active Galactic Nuclei: A Novel Signature of High-Eddington Accretion?","abstract":"The supermassive black holes ($M_{\\rm BH} \\sim 10^{6}$$-$$10^{10}~M_\\odot$) that power luminous active galactic nuclei (AGNs), i.e., quasars, generally show a correlation between thermal disk emission in the ultraviolet (UV) and coronal emission in hard X-rays. In contrast, some \"massive\" black holes (mBHs; $M_{\\rm BH} \\sim 10^{5}$$-$$10^{6}~M_\\odot$) in low-mass galaxies present curious X-ray properties with coronal radiative output up to 100$\\times$ weaker than expected. To examine this issue, we present new and archival 10 GHz Very Large Array observations of a sample of high-accretion-rate (Eddington ratios $L_{\\rm bol}/L_{\\rm Edd} > 0.1$), mBH-powered AGNs with Chandra X-ray coverage. Empirical correlations previously revealed in samples of radio-quiet, high-Eddington AGNs indicate that the radio$-$X-ray luminosity ratio, $L_{\\rm R}/L_{\\rm X}$, is approximately constant. Through multiwavelength analysis, we instead find that the X-ray-weaker mBHs in our sample tend toward larger values of $L_{\\rm R}/L_{\\rm X}$ even though they remain radio-quiet per their optical$-$UV properties. This trend results in a tentative but highly intriguing correlation between $L_{\\rm R}/L_{\\rm X}$ and X-ray weakness, which we argue is consistent with a scenario in which X-rays may be preferentially obscured from our line of sight by a \"slim\" accretion disk. We compare this observation to weak emission-line quasars (AGNs with exceptionally weak broad-line emission and a significant X-ray-weak fraction) and conclude by suggesting that our results may offer a new observational signature for finding high-accretion-rate AGNs.","sentences":["The supermassive black holes ($M_{\\rm BH} \\sim 10^{6}$$-$$10^{10}~M_\\odot$) that power luminous active galactic nuclei (AGNs), i.e., quasars, generally show a correlation between thermal disk emission in the ultraviolet (UV) and coronal emission in hard X-rays.","In contrast, some \"massive\" black holes (mBHs; $M_{\\rm BH} \\sim 10^{5}$$-$$10^{6}~M_\\odot$) in low-mass galaxies present curious X-ray properties with coronal radiative output up to 100$\\times$ weaker than expected.","To examine this issue, we present new and archival 10 GHz Very Large Array observations of a sample of high-accretion-rate (Eddington ratios $L_{\\rm bol}/L_{\\rm Edd} > 0.1$), mBH-powered AGNs with Chandra X-ray coverage.","Empirical correlations previously revealed in samples of radio-quiet, high-Eddington AGNs indicate that the radio$-$X-ray luminosity ratio, $L_{\\rm R}/L_{\\rm X}$, is approximately constant.","Through multiwavelength analysis, we instead find that the X-ray-weaker mBHs in our sample tend toward larger values of $L_{\\rm R}/L_{\\rm","X}$","even though they remain radio-quiet per their optical$-$UV properties.","This trend results in a tentative but highly intriguing correlation between $L_{\\rm R}/L_{\\rm X}$ and X-ray weakness, which we argue is consistent with a scenario in which X-rays may be preferentially obscured from our line of sight by a \"slim\" accretion disk.","We compare this observation to weak emission-line quasars (AGNs with exceptionally weak broad-line emission and a significant X-ray-weak fraction) and conclude by suggesting that our results may offer a new observational signature for finding high-accretion-rate AGNs."],"url":"http://arxiv.org/abs/2404.02423v1","category":"astro-ph.GA"}
{"created":"2024-04-03 00:25:29","title":"Efficient ultra-broadband low-resolution astrophotonic spectrographs","abstract":"Broadband low-resolution near-infrared spectrographs in a compact form are crucial for ground- and space-based astronomy and other fields of sensing. Astronomical spectroscopy poses stringent requirements including high efficiency, broad band operation ($>$ 300 nm), and in some cases, polarization insensitivity. We present and compare experimental results from the design, fabrication, and characterization of broadband (1200 - 1650 nm) arrayed waveguide grating (AWG) spectrographs built using the two most promising low-loss platforms - Si$_3$N$_4$ (rectangular waveguides) and doped-SiO$_2$ (square waveguides). These AWGs have a resolving power ($\\lambda/\\Delta\\lambda$) of ~200, a free spectral range of ~ 200-350 nm, and a small footprint of ~ 50-100 mm$^2$. The peak overall (fiber-chip-fiber) efficiency of the doped-SiO$_2$ AWG was ~ 79\\% (1 dB), and it exhibited a negligible polarization-dependent shift compared to the channel spacing. For Si$_3$N$_4$ AWGs, the peak overall efficiency in TE mode was ~ 50\\% (3 dB), and the main loss component was found to be fiber-to-chip coupling losses. These broadband AWGs are key to enabling compact integrations such as multi-object spectrographs or dispersion back-ends for other astrophotonic devices such as photonic lanterns or nulling interferometers.","sentences":["Broadband low-resolution near-infrared spectrographs in a compact form are crucial for ground- and space-based astronomy and other fields of sensing.","Astronomical spectroscopy poses stringent requirements including high efficiency, broad band operation ($>$ 300 nm), and in some cases, polarization insensitivity.","We present and compare experimental results from the design, fabrication, and characterization of broadband (1200 - 1650 nm) arrayed waveguide grating (AWG) spectrographs built using the two most promising low-loss platforms - Si$_3$N$_4$ (rectangular waveguides) and doped-SiO$_2$ (square waveguides).","These AWGs have a resolving power ($\\lambda/\\Delta\\lambda$) of ~200, a free spectral range of ~ 200-350 nm, and a small footprint of ~ 50-100 mm$^2$.","The peak overall (fiber-chip-fiber) efficiency of the doped-SiO$_2$ AWG was ~ 79\\% (1 dB), and it exhibited a negligible polarization-dependent shift compared to the channel spacing.","For Si$_3$N$_4$ AWGs, the peak overall efficiency in TE mode was ~ 50\\% (3 dB), and the main loss component was found to be fiber-to-chip coupling losses.","These broadband AWGs are key to enabling compact integrations such as multi-object spectrographs or dispersion back-ends for other astrophotonic devices such as photonic lanterns or nulling interferometers."],"url":"http://arxiv.org/abs/2404.02376v1","category":"astro-ph.IM"}
{"created":"2024-04-02 23:24:33","title":"A comprehensive X-ray analysis of the massive O-type binary HD93250 over two decades","abstract":"Massive star winds are known to be responsible for X-ray emission arising from wind plasma heated by the strong shocks up to the temperature of 10$^6$--10$^7$ K in case of colliding wind binaries. We have investigated thermal and non-thermal X-ray emission from the massive O-type star HD93250 to unveil its binary orbital parameters independently. To meet our goal, X-ray data obtained with XMM-Newton has been analyzed, spanning over $\\sim$19 years. Additionally, we analyzed NuSTAR observations of HD93250 taken at various epochs. We determined the variability time-scale of the X-ray emission to be 193.8$\\pm$1.3\\,d, in full agreement with the 194.3$\\pm$0.4\\,d period derived from the astrometric orbit. The X-ray spectrum of HD93250 is well explained by a three-temperature thermal plasma emission model with temperatures of 0.26, 1.0, and 3.3 keV. The resulting X-ray flux varies in compliance with the typical colliding wind emission from eccentric massive binaries where it enhances near periastron passage and decreases gradually close to apastron, proportionally with the inverse of the binary separation. The periastron-to-apastron X-ray emission ratio points to an eccentricity range of 0.20-0.25, once again in agreement with the previously determined astrometric orbit. Finally, we did not detect any hard X-ray emission attributable to non-thermal emission above 10 keV. Given the derived plasma temperature, the strong phase-locked variability and the significant over-luminosity in X-rays, we establish that the X-ray emission from HD93250 is dominated by the colliding-wind region. Our results lend support to the idea that X-ray time analysis of massive stars constitutes a relevant tool to investigate their multiplicity and extract relevant information on their basic orbital parameters, such as the period and the eccentricity, independently of any orbital solution derived from usual techniques.","sentences":["Massive star winds are known to be responsible for X-ray emission arising from wind plasma heated by the strong shocks up to the temperature of 10$^6$--10$^7$ K in case of colliding wind binaries.","We have investigated thermal and non-thermal X-ray emission from the massive O-type star HD93250 to unveil its binary orbital parameters independently.","To meet our goal, X-ray data obtained with XMM-Newton has been analyzed, spanning over $\\sim$19 years.","Additionally, we analyzed NuSTAR observations of HD93250 taken at various epochs.","We determined the variability time-scale of the X-ray emission to be 193.8$\\pm$1.3\\,d, in full agreement with the 194.3$\\pm$0.4\\,d period derived from the astrometric orbit.","The X-ray spectrum of HD93250 is well explained by a three-temperature thermal plasma emission model with temperatures of 0.26, 1.0, and 3.3 keV.","The resulting X-ray flux varies in compliance with the typical colliding wind emission from eccentric massive binaries where it enhances near periastron passage and decreases gradually close to apastron, proportionally with the inverse of the binary separation.","The periastron-to-apastron X-ray emission ratio points to an eccentricity range of 0.20-0.25, once again in agreement with the previously determined astrometric orbit.","Finally, we did not detect any hard X-ray emission attributable to non-thermal emission above 10 keV. Given the derived plasma temperature, the strong phase-locked variability and the significant over-luminosity in X-rays, we establish that the X-ray emission from HD93250 is dominated by the colliding-wind region.","Our results lend support to the idea that X-ray time analysis of massive stars constitutes a relevant tool to investigate their multiplicity and extract relevant information on their basic orbital parameters, such as the period and the eccentricity, independently of any orbital solution derived from usual techniques."],"url":"http://arxiv.org/abs/2404.02363v1","category":"astro-ph.SR"}
{"created":"2024-04-02 22:49:36","title":"Kalman filter based localization in hybrid BLE-UWB positioning system","abstract":"In this paper a concept of hybrid Bluetooth Low Energy (BLE) Ultra-wideband (UWB) positioning system is presented. The system is intended to be energy efficient. Low energy BLE unit is used as a primary source of measurement data and for most of the time localization is calculated based on received signal strength (RSS). UWB technology is used less often. Time difference of arrival (TDOA) values measured with UWB radios are periodically used to improve RSS based localization. The paper contains a description of proposed hybrid positioning algorithm. Results of simulations and experiments confirming algorithm's efficiency are also included.","sentences":["In this paper a concept of hybrid Bluetooth Low Energy (BLE) Ultra-wideband (UWB) positioning system is presented.","The system is intended to be energy efficient.","Low energy BLE unit is used as a primary source of measurement data and for most of the time localization is calculated based on received signal strength (RSS).","UWB technology is used less often.","Time difference of arrival (TDOA) values measured with UWB radios are periodically used to improve RSS based localization.","The paper contains a description of proposed hybrid positioning algorithm.","Results of simulations and experiments confirming algorithm's efficiency are also included."],"url":"http://arxiv.org/abs/2404.02349v1","category":"eess.SP"}
{"created":"2024-04-02 22:22:01","title":"Muon g-2 and lepton flavor violation in supersymmetric GUTs","abstract":"We present a class of supersymmetric (SUSY) GUT models that can explain the apparent discrepancy between the SM predictions and experimental values of muon g-2 while providing testable signals for lepton flavor violation in charged lepton decays. Moreover, these models predict LSP neutralino abundance that is compatible with the Planck dark matter bounds. We find that scenarios in the framework of $SU(4)_c\\times SU(2)_L\\times SU(2)_R$ unification, with additional symmetries to explain fermion masses and neutrino oscillations, provide interesting benchmarks for the search of SUSY by correlating a possible manifestation of it in dark matter, rare lepton decays and LHC signals.","sentences":["We present a class of supersymmetric (SUSY) GUT models that can explain the apparent discrepancy between the SM predictions and experimental values of muon g-2 while providing testable signals for lepton flavor violation in charged lepton decays.","Moreover, these models predict LSP neutralino abundance that is compatible with the Planck dark matter bounds.","We find that scenarios in the framework of $SU(4)_c\\times SU(2)_L\\times SU(2)_R$ unification, with additional symmetries to explain fermion masses and neutrino oscillations, provide interesting benchmarks for the search of SUSY by correlating a possible manifestation of it in dark matter, rare lepton decays and LHC signals."],"url":"http://arxiv.org/abs/2404.02337v1","category":"hep-ph"}
{"created":"2024-04-02 22:15:06","title":"cppdlr: Imaginary time calculations using the discrete Lehmann representation","abstract":"We introduce cppdlr, a C++ library implementing the discrete Lehmann representation (DLR) of functions in imaginary time and Matsubara frequency, such as Green's functions and self-energies. The DLR is based on a low-rank approximation of the analytic continuation kernel, and yields a compact and explicit basis consisting of exponentials in imaginary time and simple poles in Matsubara frequency. cppdlr constructs the DLR basis and associated interpolation grids, and implements standard operations. It provides a flexible yet high-level interface, facilitating the incorporation of the DLR into both small-scale applications and existing large-scale software projects.","sentences":["We introduce cppdlr, a C++ library implementing the discrete Lehmann representation (DLR) of functions in imaginary time and Matsubara frequency, such as Green's functions and self-energies.","The DLR is based on a low-rank approximation of the analytic continuation kernel, and yields a compact and explicit basis consisting of exponentials in imaginary time and simple poles in Matsubara frequency.","cppdlr constructs the DLR basis and associated interpolation grids, and implements standard operations.","It provides a flexible yet high-level interface, facilitating the incorporation of the DLR into both small-scale applications and existing large-scale software projects."],"url":"http://arxiv.org/abs/2404.02334v1","category":"physics.comp-ph"}
{"created":"2024-04-02 22:09:18","title":"Replica Wormholes and Quantum Hair","abstract":"We discuss recent applications of Euclidean path integrals to the black hole information problem. In calculations with replica wormholes as the next-to-leading order correction to the Gibbons-Hawking saddlepoint, the radiation density matrix approaches a pure state at late times, following the Page curve. We compare unitary evaporation of black holes (in real time), mediated by calculable quantum hair effects, with the replica wormhole results. Both replica wormhole and quantum hair approaches imply that radiation states are macroscopic superpositions of spacetime backgrounds, invalidating firewall and monogamy of entanglement constructions. Importantly, identification of modes inside the horizon with radiation modes (i.e., large scale nonlocality across the horizon) is not required to provide a physical picture of unitary evaporation. Radiation modes can encode the interior information while still remaining independent degrees of freedom.","sentences":["We discuss recent applications of Euclidean path integrals to the black hole information problem.","In calculations with replica wormholes as the next-to-leading order correction to the Gibbons-Hawking saddlepoint, the radiation density matrix approaches a pure state at late times, following the Page curve.","We compare unitary evaporation of black holes (in real time), mediated by calculable quantum hair effects, with the replica wormhole results.","Both replica wormhole and quantum hair approaches imply that radiation states are macroscopic superpositions of spacetime backgrounds, invalidating firewall and monogamy of entanglement constructions.","Importantly, identification of modes inside the horizon with radiation modes (i.e., large scale nonlocality across the horizon) is not required to provide a physical picture of unitary evaporation.","Radiation modes can encode the interior information while still remaining independent degrees of freedom."],"url":"http://arxiv.org/abs/2404.02331v1","category":"hep-th"}
{"created":"2024-04-02 21:43:53","title":"Exploring Substructure of the Near-Surface Shear Layer of the Sun","abstract":"The gradient of rotation in the near-surface shear layer (NSSL) of the Sun provides valuable insights into the dynamics associated with the solar activity cycle and the dynamo. Results obtained with global oscillation mode-splittings lack resolution near the surface, prompting the use of the local helioseismic ring-diagram method. While the Helioseismic and Magnetic Imager ring-analysis pipeline has been used previously for analyzing this layer, default pipeline parameters limit the accuracy of the near-surface gradients. To address these challenges, we fitted the flow parameters to power spectra averaged over one-year periods at each location, followed by additional averaging over 12 years. We find that the NSSL can be divided into three fairly distinct regions: a deeper, larger region with small shear, steepening towards the surface; a narrow middle layer with a strong shear, with a gradient approximately three times larger; and a layer very close to the surface, where the logarithmic gradient is close to zero but becomes steeper again towards the surface. The middle layer appears to be centered at 3 Mm, but the poor resolution in these layers implies that it is potentially located closer to the surface, around 1.5 Mm deep. While our analysis primarily focused on regions along the central meridian, we also investigated systematic errors at longitudes off the center. The east-west antisymmetric component of the gradient reveals a layer of substantial differences between east and west longitude around at 1.7 Mm, and the amplitude of the differences increases with longitude.","sentences":["The gradient of rotation in the near-surface shear layer (NSSL) of the Sun provides valuable insights into the dynamics associated with the solar activity cycle and the dynamo.","Results obtained with global oscillation mode-splittings lack resolution near the surface, prompting the use of the local helioseismic ring-diagram method.","While the Helioseismic and Magnetic Imager ring-analysis pipeline has been used previously for analyzing this layer, default pipeline parameters limit the accuracy of the near-surface gradients.","To address these challenges, we fitted the flow parameters to power spectra averaged over one-year periods at each location, followed by additional averaging over 12 years.","We find that the NSSL can be divided into three fairly distinct regions: a deeper, larger region with small shear, steepening towards the surface; a narrow middle layer with a strong shear, with a gradient approximately three times larger; and a layer very close to the surface, where the logarithmic gradient is close to zero but becomes steeper again towards the surface.","The middle layer appears to be centered at 3 Mm, but the poor resolution in these layers implies that it is potentially located closer to the surface, around 1.5 Mm deep.","While our analysis primarily focused on regions along the central meridian, we also investigated systematic errors at longitudes off the center.","The east-west antisymmetric component of the gradient reveals a layer of substantial differences between east and west longitude around at 1.7 Mm, and the amplitude of the differences increases with longitude."],"url":"http://arxiv.org/abs/2404.02321v1","category":"astro-ph.SR"}
{"created":"2024-04-02 20:49:50","title":"\"Beam `a la carte\": laser heater shaping for attosecond pulses in a multiplexed x-ray free-electron laser","abstract":"Electron beam shaping allows the control of the temporal properties of x-ray free-electron laser pulses from femtosecond to attosecond timescales. Here we demonstrate the use of a laser heater to shape electron bunches and enable the generation of attosecond x-ray pulses. We demonstrate that this method can be applied in a selective way, shaping a targeted subset of bunches while leaving the remaining bunches unchanged. This experiment enables the delivery of shaped x-ray pulses to multiple undulator beamlines, with pulse properties tailored to specialized scientific applications.","sentences":["Electron beam shaping allows the control of the temporal properties of x-ray free-electron laser pulses from femtosecond to attosecond timescales.","Here we demonstrate the use of a laser heater to shape electron bunches and enable the generation of attosecond x-ray pulses.","We demonstrate that this method can be applied in a selective way, shaping a targeted subset of bunches while leaving the remaining bunches unchanged.","This experiment enables the delivery of shaped x-ray pulses to multiple undulator beamlines, with pulse properties tailored to specialized scientific applications."],"url":"http://arxiv.org/abs/2404.02299v1","category":"physics.acc-ph"}
{"created":"2024-04-02 20:48:08","title":"Quantum Flux and Quantum Ergodicity for Cross Sections","abstract":"For sequences of quantum ergodic eigenfunctions, we define the quantum flux norm associated to a codimension $1$ submanifold $\\Sigma$ of a non-degenerate energy surface. We prove restrictions of eigenfunctions to $\\Sigma$, realized using the quantum flux norm, are quantum ergodic. We compare this result to known results from \\cite{CTZ} in the case of Euclidean domains and hyperfurfaces. As a further application, we consider complexified analytic eigenfunctions and prove a second microlocal analogue of \\cite{CTZ} in that context.","sentences":["For sequences of quantum ergodic eigenfunctions, we define the quantum flux norm associated to a codimension $1$ submanifold $\\Sigma$ of a non-degenerate energy surface.","We prove restrictions of eigenfunctions to $\\Sigma$, realized using the quantum flux norm, are quantum ergodic.","We compare this result to known results from \\cite{CTZ} in the case of Euclidean domains and hyperfurfaces.","As a further application, we consider complexified analytic eigenfunctions and prove a second microlocal analogue of \\cite{CTZ} in that context."],"url":"http://arxiv.org/abs/2404.02296v1","category":"math.AP"}
{"created":"2024-04-02 20:46:35","title":"Large curvature fluctuations from no-scale supergravity with a spectator field","abstract":"We investigate the large curvature perturbations which can lead to the formation of primordial black holes (PBHs) in the context of no-scale supergravity. Our study does not depend on any exotic scenario, such as scalar potentials with inflection points or bulks, and aims to avoid the fine-tuning of model parameters to achieve the formation of PBHs. This formation relies on the quantum fluctuations of a light spectator stochastic field after the inflationary period. Our analysis is based on the SU(2,1)/SU(2)$\\times$U(1) symmetry, considering both the inflaton and the spectator field. Specifically, we examine existing no-scale models with Starobinsky-like scalar potentials that are consistent with observable constraints on inflation from measurements of the cosmic microwave background (CMB). These models involve two chiral fields: the inflaton and the modulus field. We propose a novel role for the modulus field as a spectator field, responsible for generating PBHs. Our hypothesis suggests that while the inflaton field satisfies the CMB constraints of inflation, it is the modulus field acting as the spectator that leads to large curvature perturbations, capable to explain the production of PBHs. Additionally, we prioritize retaining the inflationary constraints from the CMB through the consideration of spectator fluctuations. Therefore, by exploring the relationship between these fields within the framework of the SU(2,1)/SU(2)$\\times$U(1) symmetry, our aim is to unveil their implications for the formation of PBHs.","sentences":["We investigate the large curvature perturbations which can lead to the formation of primordial black holes (PBHs) in the context of no-scale supergravity.","Our study does not depend on any exotic scenario, such as scalar potentials with inflection points or bulks, and aims to avoid the fine-tuning of model parameters to achieve the formation of PBHs.","This formation relies on the quantum fluctuations of a light spectator stochastic field after the inflationary period.","Our analysis is based on the SU(2,1)/SU(2)$\\times$U(1) symmetry, considering both the inflaton and the spectator field.","Specifically, we examine existing no-scale models with Starobinsky-like scalar potentials that are consistent with observable constraints on inflation from measurements of the cosmic microwave background (CMB).","These models involve two chiral fields: the inflaton and the modulus field.","We propose a novel role for the modulus field as a spectator field, responsible for generating PBHs.","Our hypothesis suggests that while the inflaton field satisfies the CMB constraints of inflation, it is the modulus field acting as the spectator that leads to large curvature perturbations, capable to explain the production of PBHs.","Additionally, we prioritize retaining the inflationary constraints from the CMB through the consideration of spectator fluctuations.","Therefore, by exploring the relationship between these fields within the framework of the SU(2,1)/SU(2)$\\times$U(1) symmetry, our aim is to unveil their implications for the formation of PBHs."],"url":"http://arxiv.org/abs/2404.02295v1","category":"astro-ph.CO"}
{"created":"2024-04-02 20:15:37","title":"Avenues for a number density interpretation of dihadron fragmentation functions","abstract":"In this comment, we reassess the underlying physics of the number sum rule for dihadron fragmentation functions. We will argue that, currently, there are no settled constraints on what constitutes a valid number density interpretation for multihadron fragmentation functions. Imposing overly restrictive criteria might lead to misinterpretating the data. Most importantly, and on the basis of phenomenological analyses, the slightly varying definitions used in previous work are not excluded from possessing legitimate number density interpretations (up to the usual issues with ultraviolet divergences and renormalization), so long as they are paired with appropriate factorization theorems. We advocate for further theoretical analyses to be challenged with experimental data, available at JLab or at the future EIC.","sentences":["In this comment, we reassess the underlying physics of the number sum rule for dihadron fragmentation functions.","We will argue that, currently, there are no settled constraints on what constitutes a valid number density interpretation for multihadron fragmentation functions.","Imposing overly restrictive criteria might lead to misinterpretating the data.","Most importantly, and on the basis of phenomenological analyses, the slightly varying definitions used in previous work are not excluded from possessing legitimate number density interpretations (up to the usual issues with ultraviolet divergences and renormalization), so long as they are paired with appropriate factorization theorems.","We advocate for further theoretical analyses to be challenged with experimental data, available at JLab or at the future EIC."],"url":"http://arxiv.org/abs/2404.02281v1","category":"hep-ph"}
