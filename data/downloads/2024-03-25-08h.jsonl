{"created":"2024-03-22 17:59:58","title":"DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data","abstract":"Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks. The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods. To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising diffusion framework coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks. To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled tasks, leading to an improvement in the denoising performance of the different tasks. Extensive quantitative and qualitative experiments demonstrate that the proposed multi-task denoising diffusion model can significantly improve multi-task prediction maps, and outperform the state-of-the-art methods on three challenging multi-task benchmarks, under two different partial-labeling evaluation settings. The code is available at https://prismformore.github.io/diffusionmtl/.","sentences":["Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks.","The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods.","To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising diffusion framework coined as DiffusionMTL.","It designs a joint diffusion and denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks.","To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled tasks, leading to an improvement in the denoising performance of the different tasks.","Extensive quantitative and qualitative experiments demonstrate that the proposed multi-task denoising diffusion model can significantly improve multi-task prediction maps, and outperform the state-of-the-art methods on three challenging multi-task benchmarks, under two different partial-labeling evaluation settings.","The code is available at https://prismformore.github.io/diffusionmtl/."],"url":"http://arxiv.org/abs/2403.15389v1","category":"cs.CV"}
{"created":"2024-03-22 17:59:52","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models","abstract":"Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select the unpruned visual tokens based on their similarity to class tokens and spatial tokens. We then cluster the pruned tokens based on key similarity and merge the clustered tokens with the unpruned tokens to supplement their information. Empirically, when applied to LLaVA-1.5, our approach can compress the visual tokens by 14.4 times on average, and achieve comparable performance across diverse visual question-answering and reasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.","sentences":["Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model.","LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content.","Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly.","However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens.","To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant.","Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance.","We first select the unpruned visual tokens based on their similarity to class tokens and spatial tokens.","We then cluster the pruned tokens based on key similarity and merge the clustered tokens with the unpruned tokens to supplement their information.","Empirically, when applied to LLaVA-1.5, our approach can compress the visual tokens by 14.4 times on average, and achieve comparable performance across diverse visual question-answering and reasoning tasks.","Code and checkpoints are at https://llava-prumerge.github.io/."],"url":"http://arxiv.org/abs/2403.15388v1","category":"cs.CV"}
{"created":"2024-03-22 17:59:43","title":"Production of dark sector particles via resonant positron annihilation on atomic electrons","abstract":"Resonant positron annihilation on atomic electrons provides a powerful method to search for light new particles coupled to $e^+e^-$. Reliable estimates of production rates require a detailed characterization of electron momentum distributions. We describe a general method that harnesses the target material Compton profile to properly include electron velocity effects in resonant annihilation cross-sections. We additionally find that high $Z$ atoms can efficiently act as particle physics accelerators, providing a density of relativistic electrons that allows to extend by several times the experimental mass reach.","sentences":["Resonant positron annihilation on atomic electrons provides a powerful method to search for light new particles coupled to $e^+e^-$. Reliable estimates of production rates require a detailed characterization of electron momentum distributions.","We describe a general method that harnesses the target material Compton profile to properly include electron velocity effects in resonant annihilation cross-sections.","We additionally find that high $Z$ atoms can efficiently act as particle physics accelerators, providing a density of relativistic electrons that allows to extend by several times the experimental mass reach."],"url":"http://arxiv.org/abs/2403.15387v1","category":"hep-ph"}
{"created":"2024-03-22 17:59:37","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","abstract":"Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt. Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency geometry and texture details and struggle to scale to large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts. LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D generates 3D objects in 400ms, and can be further enhanced with fast test-time optimization.","sentences":["Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt.","Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis.","However, they cannot capture high-frequency geometry and texture details and struggle to scale to large prompt sets, so they generalize poorly.","We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set.","Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts.","LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass.","LATTE3D generates 3D objects in 400ms, and can be further enhanced with fast test-time optimization."],"url":"http://arxiv.org/abs/2403.15385v1","category":"cs.CV"}
{"created":"2024-03-22 17:59:01","title":"ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars","abstract":"Real-world applications often require a large gallery of 3D assets that share a consistent theme. While remarkable advances have been made in general 3D content creation from text or image, synthesizing customized 3D assets following the shared theme of input 3D exemplars remains an open and challenging problem. In this work, we present ThemeStation, a novel approach for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D assets based on given few exemplars with two goals: 1) unity for generating 3D assets that thematically align with the given exemplars and 2) diversity for generating 3D assets with a high degree of variations. To this end, we design a two-stage framework that draws a concept image first, followed by a reference-informed 3D modeling stage. We propose a novel dual score distillation (DSD) loss to jointly leverage priors from both the input exemplars and the synthesized concept image. Extensive experiments and user studies confirm that ThemeStation surpasses prior works in producing diverse theme-aware 3D models with impressive quality. ThemeStation also enables various applications such as controllable 3D-to-3D generation.","sentences":["Real-world applications often require a large gallery of 3D assets that share a consistent theme.","While remarkable advances have been made in general 3D content creation from text or image, synthesizing customized 3D assets following the shared theme of input 3D exemplars remains an open and challenging problem.","In this work, we present ThemeStation, a novel approach for theme-aware 3D-to-3D generation.","ThemeStation synthesizes customized 3D assets based on given few exemplars with two goals: 1) unity for generating 3D assets that thematically align with the given exemplars and 2) diversity for generating 3D assets with a high degree of variations.","To this end, we design a two-stage framework that draws a concept image first, followed by a reference-informed 3D modeling stage.","We propose a novel dual score distillation (DSD) loss to jointly leverage priors from both the input exemplars and the synthesized concept image.","Extensive experiments and user studies confirm that ThemeStation surpasses prior works in producing diverse theme-aware 3D models with impressive quality.","ThemeStation also enables various applications such as controllable 3D-to-3D generation."],"url":"http://arxiv.org/abs/2403.15383v1","category":"cs.CV"}
{"created":"2024-03-22 17:58:59","title":"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects","abstract":"We introduce DragAPart, a method that, given an image and a set of drags as input, can generate a new image of the same object in a new state, compatible with the action of the drags. Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. To this end, we start from a pre-trained image generator and fine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce. Combined with a new encoding for the drags and dataset randomization, the new model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding.","sentences":["We introduce DragAPart, a method that, given an image and a set of drags as input, can generate a new image of the same object in a new state, compatible with the action of the drags.","Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer.","We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category.","To this end, we start from a pre-trained image generator and fine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.","Combined with a new encoding for the drags and dataset randomization, the new model generalizes well to real images and different categories.","Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding."],"url":"http://arxiv.org/abs/2403.15382v1","category":"cs.CV"}
{"created":"2024-03-22 17:58:51","title":"Localization for quasi-one-dimensional Dirac operators","abstract":"We consider a random family of Dirac operators on $N$ parallel real lines, modelling for example a graphene nanoribbon. We establish a localization criterion involving properties on the group generated by transfer matrices. In particular, we consider not only the case where this group is the symplectic group but also a strict subgroup of it. We establish under quite general hypotheses that the sum of the Lyapunov exponents and the integrated density of states are H\\\"older continuous. Moreover, for a set of concrete cases where the potentials are on Pauli matrices, we compute the transfer matrices and prove either localization or delocalization, depending on the potential and on the parity of $N$.","sentences":["We consider a random family of Dirac operators on $N$ parallel real lines, modelling for example a graphene nanoribbon.","We establish a localization criterion involving properties on the group generated by transfer matrices.","In particular, we consider not only the case where this group is the symplectic group but also a strict subgroup of it.","We establish under quite general hypotheses that the sum of the Lyapunov exponents and the integrated density of states are H\\\"older continuous.","Moreover, for a set of concrete cases where the potentials are on Pauli matrices, we compute the transfer matrices and prove either localization or delocalization, depending on the potential and on the parity of $N$."],"url":"http://arxiv.org/abs/2403.15381v1","category":"math-ph"}
{"created":"2024-03-22 17:58:45","title":"Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes","abstract":"This article introduces two control frameworks: one for Grid-Following (GFL) inverters aiding Grid-Forming (GFM) inverters in voltage regulation during large contingency events and optimizing power transactions under normal conditions; and another for seamless transitions between grid-tied and grid-isolated setups, managing voltage transient characteristics. In microgrids, GFM inverters regulate voltage, while GFL inverters handle power transactions. The proposed GFL control detects abrupt load/generation changes, adjusting power transactions using local storage to support GFM inverters during contingencies. Additionally, a transition control ensures smooth GFL-GFM shifts, reducing power and voltage fluctuations. Simulation results validate improved voltage regulation during contingencies and enhanced power tracking during slow changes, alongside minimized transient overshoot.","sentences":["This article introduces two control frameworks: one for Grid-Following (GFL) inverters aiding Grid-Forming (GFM) inverters in voltage regulation during large contingency events and optimizing power transactions under normal conditions; and another for seamless transitions between grid-tied and grid-isolated setups, managing voltage transient characteristics.","In microgrids, GFM inverters regulate voltage, while GFL inverters handle power transactions.","The proposed GFL control detects abrupt load/generation changes, adjusting power transactions using local storage to support GFM inverters during contingencies.","Additionally, a transition control ensures smooth GFL-GFM shifts, reducing power and voltage fluctuations.","Simulation results validate improved voltage regulation during contingencies and enhanced power tracking during slow changes, alongside minimized transient overshoot."],"url":"http://arxiv.org/abs/2403.15380v1","category":"eess.SY"}
{"created":"2024-03-22 17:58:16","title":"Long-CLIP: Unlocking the Long-Text Capability of CLIP","abstract":"Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its zero-shot generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic fine-tuning can result in a significant degradation of CLIP's performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient fine-tuning solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long text-image pairs, Long-CLIP has shown the superiority to CLIP for about 20% in long caption text-image retrieval and 6% in traditional text-image retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.","sentences":["Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities.","Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input.","The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20.","This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites.","To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its zero-shot generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks.","Nevertheless, achieving this goal is far from straightforward, as simplistic fine-tuning can result in a significant degradation of CLIP's performance.","Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses.","Accordingly, Long-CLIP introduces an efficient fine-tuning solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features.","With leveraging just one million extra long text-image pairs, Long-CLIP has shown the superiority to CLIP for about 20% in long caption text-image retrieval and 6% in traditional text-image retrieval tasks, e.g., COCO and Flickr30k.","Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner."],"url":"http://arxiv.org/abs/2403.15378v1","category":"cs.CV"}
{"created":"2024-03-22 17:57:42","title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding","abstract":"We introduce InternVideo2, a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue. Our approach employs a progressive training paradigm that unifies the different self- or weakly-supervised learning frameworks of masked video token reconstruction, cross-modal contrastive learning, and next token prediction. Different training stages would guide our model to capture different levels of structure and semantic information through different pretext tasks. At the data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our InternVideo2. Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding benchmarks, highlighting its ability to reason and comprehend long temporal contexts. Code and models are available at https://github.com/OpenGVLab/InternVideo2/.","sentences":["We introduce InternVideo2, a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue.","Our approach employs a progressive training paradigm that unifies the different self- or weakly-supervised learning frameworks of masked video token reconstruction, cross-modal contrastive learning, and next token prediction.","Different training stages would guide our model to capture different levels of structure and semantic information through different pretext tasks.","At the data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions.","This improves the alignment between video and text.","We scale both data and model size for our InternVideo2.","Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks.","Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding benchmarks, highlighting its ability to reason and comprehend long temporal contexts.","Code and models are available at https://github.com/OpenGVLab/InternVideo2/."],"url":"http://arxiv.org/abs/2403.15377v1","category":"cs.CV"}
{"created":"2024-03-22 17:57:20","title":"A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform","abstract":"Experimentation in practical, end-to-end (E2E) next-generation networks deployments is becoming increasingly prevalent and significant in the realm of modern networking and wireless communications research. The prevalence of fifth-generation technology (5G) testbeds and the emergence of developing networks systems, for the purposes of research and testing, focus on the capabilities and features of analytics, intelligence, and automated management using novel testbed designs and architectures, ranging from simple simulations and setups to complex networking systems; however, with the ever-demanding application requirements for modern and future networks, 5G-and-beyond (denoted as 5G+) testbed experimentation can be useful in assessing the creation of large-scale network infrastructures that are capable of supporting E2E virtualized mobile network services. To this end, this paper presents a functional, modular E2E 5G+ system, complete with the integration of a Radio Access Network (RAN) and handling the connection of User Equipment (UE) in real-world scenarios. As well, this paper assesses and evaluates the effectiveness of emulating full network functionalities and capabilities, including a complete description of user-plane data, from UE registrations to communications sequences, and leads to the presentation of a future outlook in powering new experimentation for 6G and next-generation networks.","sentences":["Experimentation in practical, end-to-end (E2E) next-generation networks deployments is becoming increasingly prevalent and significant in the realm of modern networking and wireless communications research.","The prevalence of fifth-generation technology (5G) testbeds and the emergence of developing networks systems, for the purposes of research and testing, focus on the capabilities and features of analytics, intelligence, and automated management using novel testbed designs and architectures, ranging from simple simulations and setups to complex networking systems; however, with the ever-demanding application requirements for modern and future networks, 5G-and-beyond (denoted as 5G+) testbed experimentation can be useful in assessing the creation of large-scale network infrastructures that are capable of supporting E2E virtualized mobile network services.","To this end, this paper presents a functional, modular E2E 5G+ system, complete with the integration of a Radio Access Network (RAN) and handling the connection of User Equipment (UE) in real-world scenarios.","As well, this paper assesses and evaluates the effectiveness of emulating full network functionalities and capabilities, including a complete description of user-plane data, from UE registrations to communications sequences, and leads to the presentation of a future outlook in powering new experimentation for 6G and next-generation networks."],"url":"http://arxiv.org/abs/2403.15376v1","category":"cs.NI"}
{"created":"2024-03-22 17:54:22","title":"Might Normal Nuclear Matter be Quarkyonic?","abstract":"The possibility that nuclear matter might be Quarkyonic is considered. Quarkyonic matter is high baryon density matter that is confined but can be approximately thought of as a filled Fermi sea of quarks surrounded by a shell of nucleons. Here, nuclear matter is described by the IdylliQ sigma model for Quarkyonic matter, generalizing the non-interacting IdylliQ model [Y. Fujimoto et al., Phys. Rev. Lett. 132, 112701 (2024) [arXiv:2306.04304]] to include interactions with a sigma meson and a pion. When such interactions are included, we find that isospin-symmetric nuclear matter binds, with acceptable values of the compressibility and other parameters for nuclear matter at saturation. The energy per nucleon and sound velocity of such matter is computed, and the isospin dependence is determined. Nuclear matter is formed at a density close to but slightly above the density at which Quarkyonic matter forms. Quarkyonic matter predicts a strong depletion of nucleons in normal nuclear matter at low momentum. Such a depletion for nucleon momenta $k \\lesssim 120$ MeV is shown to be consistent with electron scattering data.","sentences":["The possibility that nuclear matter might be Quarkyonic is considered.","Quarkyonic matter is high baryon density matter that is confined but can be approximately thought of as a filled Fermi sea of quarks surrounded by a shell of nucleons.","Here, nuclear matter is described by the IdylliQ sigma model for Quarkyonic matter, generalizing the non-interacting IdylliQ model [Y. Fujimoto et al., Phys.","Rev. Lett.","132, 112701 (2024)","[arXiv:2306.04304]] to include interactions with a sigma meson and a pion.","When such interactions are included, we find that isospin-symmetric nuclear matter binds, with acceptable values of the compressibility and other parameters for nuclear matter at saturation.","The energy per nucleon and sound velocity of such matter is computed, and the isospin dependence is determined.","Nuclear matter is formed at a density close to but slightly above the density at which Quarkyonic matter forms.","Quarkyonic matter predicts a strong depletion of nucleons in normal nuclear matter at low momentum.","Such a depletion for nucleon momenta $k \\lesssim 120$ MeV is shown to be consistent with electron scattering data."],"url":"http://arxiv.org/abs/2403.15375v1","category":"nucl-th"}
{"created":"2024-03-22 17:52:27","title":"Can the NANOGrav observations constrain the geometry of the universe?","abstract":"The theory of inflation provides an elegant explanation for the nearly flat universe observed today, which represents one of the pillars of the standard cosmological model. However, recent studies have reported some deviations from a flat geometry, arguing that a closed universe would be instead favored by observations. Given its central role played in the cosmological context, this paper revisits the issue of spatial curvature in light of the stochastic gravitational wave background signal recently detected by the NANOGrav collaboration. For this purpose, we investigate the primordial gravitational waves generated during inflation and their propagation in the post-inflationary universe. We propose a new parametrization of the gravitational wave power spectrum, taking into account spatial curvature, the tensor-to-scalar ratio and the spectral index of tensor perturbations. Therefore, we compare the theoretical predictions with NANOGrav data to possibly constrain the geometry of the universe. We find that the choice of the priors has a significant effect on the computed posterior distributions. In particular, using flat uniform priors results in $\\Omega_{\\mathcal{K},0}= 0.00 \\pm 0.67$ at the 68\\% confidence level. On the other hand, imposing a Planck prior, we obtain $\\Omega_{\\mathcal{K},0}= -0.05 \\pm 0.17$ at the 68\\% confidence level. This result aligns with the analysis of the cosmic microwave background radiation, and no deviations from a flat universe are found.","sentences":["The theory of inflation provides an elegant explanation for the nearly flat universe observed today, which represents one of the pillars of the standard cosmological model.","However, recent studies have reported some deviations from a flat geometry, arguing that a closed universe would be instead favored by observations.","Given its central role played in the cosmological context, this paper revisits the issue of spatial curvature in light of the stochastic gravitational wave background signal recently detected by the NANOGrav collaboration.","For this purpose, we investigate the primordial gravitational waves generated during inflation and their propagation in the post-inflationary universe.","We propose a new parametrization of the gravitational wave power spectrum, taking into account spatial curvature, the tensor-to-scalar ratio and the spectral index of tensor perturbations.","Therefore, we compare the theoretical predictions with NANOGrav data to possibly constrain the geometry of the universe.","We find that the choice of the priors has a significant effect on the computed posterior distributions.","In particular, using flat uniform priors results in $\\Omega_{\\mathcal{K},0}= 0.00 \\pm 0.67$ at the 68\\% confidence level.","On the other hand, imposing a Planck prior, we obtain $\\Omega_{\\mathcal{K},0}= -0.05 \\pm 0.17$ at the 68\\% confidence level.","This result aligns with the analysis of the cosmic microwave background radiation, and no deviations from a flat universe are found."],"url":"http://arxiv.org/abs/2403.15373v1","category":"astro-ph.CO"}
{"created":"2024-03-22 17:51:57","title":"Machine learning-based compression of quantum many body physics: PCA and autoencoder representation of the vertex function","abstract":"Characterizing complex many-body phases of matter has been a central question in quantum physics for decades. Numerical methods built around approximations of the renormalization group (RG) flow equations have offered reliable and systematically improvable answers to the initial question -- what simple physics drives quantum order and disorder? The flow equations are a very high dimensional set of coupled nonlinear equations whose solution is the two particle vertex function, a function of three continuous momenta that describes particle-particle scattering and encodes much of the low energy physics including whether the system exhibits various forms of long ranged order. In this work, we take a simple and interpretable data-driven approach to the open question of compressing the two-particle vertex. We use PCA and an autoencoder neural network to derive compact, low-dimensional representations of underlying physics for the case of interacting fermions on a lattice. We quantify errors in the representations by multiple metrics and show that a simple linear PCA offers more physical insight and better out-of-distribution (zero-shot) generalization than the nominally more expressive nonlinear models. Even with a modest number of principal components (10 - 20), we find excellent reconstruction of vertex functions across the phase diagram. This result suggests that many other many-body functions may be similarly compressible, potentially allowing for efficient computation of observables. Finally, we identify principal component subspaces that are shared between known phases, offering new physical insight.","sentences":["Characterizing complex many-body phases of matter has been a central question in quantum physics for decades.","Numerical methods built around approximations of the renormalization group (RG) flow equations have offered reliable and systematically improvable answers to the initial question -- what simple physics drives quantum order and disorder?","The flow equations are a very high dimensional set of coupled nonlinear equations whose solution is the two particle vertex function, a function of three continuous momenta that describes particle-particle scattering and encodes much of the low energy physics including whether the system exhibits various forms of long ranged order.","In this work, we take a simple and interpretable data-driven approach to the open question of compressing the two-particle vertex.","We use PCA and an autoencoder neural network to derive compact, low-dimensional representations of underlying physics for the case of interacting fermions on a lattice.","We quantify errors in the representations by multiple metrics and show that a simple linear PCA offers more physical insight and better out-of-distribution (zero-shot) generalization than the nominally more expressive nonlinear models.","Even with a modest number of principal components (10 - 20), we find excellent reconstruction of vertex functions across the phase diagram.","This result suggests that many other many-body functions may be similarly compressible, potentially allowing for efficient computation of observables.","Finally, we identify principal component subspaces that are shared between known phases, offering new physical insight."],"url":"http://arxiv.org/abs/2403.15372v1","category":"cond-mat.str-el"}
{"created":"2024-03-22 17:50:43","title":"Can large language models explore in-context?","abstract":"We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.","sentences":["We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making.","We focus on native performance of existing LLMs, without training interventions.","We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt.","We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii)","All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history.","Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents.","We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings."],"url":"http://arxiv.org/abs/2403.15371v1","category":"cs.LG"}
{"created":"2024-03-22 17:49:11","title":"Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks","abstract":"Detecting a diverse range of objects under various driving scenarios is essential for the effectiveness of autonomous driving systems. However, the real-world data collected often lacks the necessary diversity presenting a long-tail distribution. Although synthetic data has been utilized to overcome this issue by generating virtual scenes, it faces hurdles such as a significant domain gap and the substantial efforts required from 3D artists to create realistic environments. To overcome these challenges, we present ARSim, a fully automated, comprehensive, modular framework designed to enhance real multi-view image data with 3D synthetic objects of interest. The proposed method integrates domain adaptation and randomization strategies to address covariate shift between real and simulated data by inferring essential domain attributes from real data and employing simulation-based randomization for other attributes. We construct a simplified virtual scene using real data and strategically place 3D synthetic assets within it. Illumination is achieved by estimating light distribution from multiple images capturing the surroundings of the vehicle. Camera parameters from real data are employed to render synthetic assets in each frame. The resulting augmented multi-view consistent dataset is used to train a multi-camera perception network for autonomous vehicles. Experimental results on various AV perception tasks demonstrate the superior performance of networks trained on the augmented dataset.","sentences":["Detecting a diverse range of objects under various driving scenarios is essential for the effectiveness of autonomous driving systems.","However, the real-world data collected often lacks the necessary diversity presenting a long-tail distribution.","Although synthetic data has been utilized to overcome this issue by generating virtual scenes, it faces hurdles such as a significant domain gap and the substantial efforts required from 3D artists to create realistic environments.","To overcome these challenges, we present ARSim, a fully automated, comprehensive, modular framework designed to enhance real multi-view image data with 3D synthetic objects of interest.","The proposed method integrates domain adaptation and randomization strategies to address covariate shift between real and simulated data by inferring essential domain attributes from real data and employing simulation-based randomization for other attributes.","We construct a simplified virtual scene using real data and strategically place 3D synthetic assets within it.","Illumination is achieved by estimating light distribution from multiple images capturing the surroundings of the vehicle.","Camera parameters from real data are employed to render synthetic assets in each frame.","The resulting augmented multi-view consistent dataset is used to train a multi-camera perception network for autonomous vehicles.","Experimental results on various AV perception tasks demonstrate the superior performance of networks trained on the augmented dataset."],"url":"http://arxiv.org/abs/2403.15370v1","category":"cs.CV"}
{"created":"2024-03-22 17:38:44","title":"Energy-dependent Boosted Dark Matter from Diffuse Supernova Neutrino Background","abstract":"Diffuse neutrinos from past supernovae in the Universe present us with a unique opportunity to test dark matter (DM) interactions. These neutrinos can scatter and boost the DM particles in the Milky Way halo to relativistic energies allowing us to detect them in terrestrial laboratories. Focusing on generic models of DM-neutrino and electron interactions, mediated by a vector or a scalar boson, we implement energy-dependent scattering cross-sections and perform detailed numerical analysis of DM attenuation due to electron scattering in-medium while propagating towards terrestrial experiments. We set new limits on DM-neutrino and electron interactions for DM with masses in the range $\\sim (0.1, 10^4)~$MeV, using recent data from XENONnT, LUX-ZEPLIN, and PandaX-4T direct detection experiments. We demonstrate that consideration of energy-dependent cross-sections for DM interactions can significantly affect constraints previously derived under the assumption of constant cross-sections, modifying them by multiple orders of magnitude.","sentences":["Diffuse neutrinos from past supernovae in the Universe present us with a unique opportunity to test dark matter (DM) interactions.","These neutrinos can scatter and boost the DM particles in the Milky Way halo to relativistic energies allowing us to detect them in terrestrial laboratories.","Focusing on generic models of DM-neutrino and electron interactions, mediated by a vector or a scalar boson, we implement energy-dependent scattering cross-sections and perform detailed numerical analysis of DM attenuation due to electron scattering in-medium while propagating towards terrestrial experiments.","We set new limits on DM-neutrino and electron interactions for DM with masses in the range $\\sim (0.1, 10^4)~$MeV, using recent data from XENONnT, LUX-ZEPLIN, and PandaX-4T direct detection experiments.","We demonstrate that consideration of energy-dependent cross-sections for DM interactions can significantly affect constraints previously derived under the assumption of constant cross-sections, modifying them by multiple orders of magnitude."],"url":"http://arxiv.org/abs/2403.15367v1","category":"hep-ph"}
{"created":"2024-03-22 17:36:19","title":"Fourier Transform-based Estimators for Data Sketches","abstract":"In this paper we consider the problem of estimating the $f$-moment ($\\sum_{v\\in [n]} (f(\\mathbf{x}(v))-f(0))$) of a dynamic vector $\\mathbf{x}\\in \\mathbb{G}^n$ over some abelian group $(\\mathbb{G},+)$, where the $\\|f\\|_\\infty$ norm is bounded. We propose a simple sketch and new estimation framework based on the \\emph{Fourier transform} of $f$. I.e., we decompose $f$ into a linear combination of homomorphisms $f_1,f_2,\\ldots$ from $(\\mathbb{G},+)$ to $(\\mathbb{C},\\times)$, estimate the $f_k$-moment for each $f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our estimators are asymptotically unbiased and have variance asymptotic to $\\|\\mathbf{x}\\|_0^2 (\\|f\\|_\\infty^2 m^{-1} + \\|\\hat{f}\\|_1^2 m^{-2})$, where the size of the sketch is $O(m\\log n\\log|\\mathbb{G}|)$ bits.   When $\\mathbb{G}=\\mathbb{Z}$ this problem can also be solved using off-the-shelf $\\ell_0$-samplers with space $O(m\\log^2 n)$ bits, which does not obviously generalize to finite groups. As a concrete benchmark, we extend Ganguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work over $\\mathbb{G}$ using $O(m\\log n\\log|\\mathbb{G}|\\log(m\\log n))$ bits.   We give some experimental evidence that the Fourier-based estimation framework is significantly more accurate than sampling-based approaches at the same memory footprint.","sentences":["In this paper we consider the problem of estimating the $f$-moment ($\\sum_{v\\in [n]} (f(\\mathbf{x}(v))-f(0))$) of a dynamic vector $\\mathbf{x}\\in \\mathbb{G}^n$ over some abelian group $(\\mathbb{G},+)$, where the $\\|f\\|_\\infty$ norm is bounded.","We propose a simple sketch and new estimation framework based on the \\emph{Fourier transform} of $f$. I.e., we decompose $f$ into a linear combination of homomorphisms $f_1,f_2,\\ldots$ from $(\\mathbb{G},+)$ to $(\\mathbb{C},\\times)$, estimate the $f_k$-moment for each $f_k$, and synthesize them to obtain an estimate of the $f$-moment.","Our estimators are asymptotically unbiased and have variance asymptotic to $\\|\\mathbf{x}\\|_0^2 (\\|f\\|_\\infty^2 m^{-1} + \\|\\hat{f}\\|_1^2 m^{-2})$, where the size of the sketch is $O(m\\log n\\log|\\mathbb{G}|)$ bits.   ","When $\\mathbb{G}=\\mathbb{Z}$ this problem can also be solved using off-the-shelf $\\ell_0$-samplers with space $O(m\\log^2 n)$ bits, which does not obviously generalize to finite groups.","As a concrete benchmark, we extend Ganguly, Garofalakis, and Rastogi's singleton-detector-based sampler to work over $\\mathbb{G}$ using $O(m\\log n\\log|\\mathbb{G}|\\log(m\\log n))$ bits.   ","We give some experimental evidence that the Fourier-based estimation framework is significantly more accurate than sampling-based approaches at the same memory footprint."],"url":"http://arxiv.org/abs/2403.15366v1","category":"cs.DS"}
{"created":"2024-03-22 17:33:11","title":"A Transfer Attack to Image Watermarks","abstract":"Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.","sentences":["Watermark has been widely deployed by industry to detect AI-generated images.","The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature.","However, the robustness in the no-box setting is much less understood.","In particular, multiple studies claimed that image watermark is robust in such setting.","In this work, we propose a new transfer evasion attack to image watermark in the no-box setting.","Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model.","Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API."],"url":"http://arxiv.org/abs/2403.15365v1","category":"cs.CR"}
{"created":"2024-03-22 17:32:43","title":"Towards Knowledge-Grounded Natural Language Understanding and Generation","abstract":"This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks?   Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection, and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning significantly improves performance. Additionally, it is established that other general forms of knowledge, such as parametric and distilled knowledge, enhance multimodal and multilingual knowledge-intensive tasks. This research shows the tangible benefits of diverse knowledge integration and motivates further exploration in this direction.","sentences":["This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking?","(ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text?","(iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks?   ","Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection, and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks.","In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning significantly improves performance.","Additionally, it is established that other general forms of knowledge, such as parametric and distilled knowledge, enhance multimodal and multilingual knowledge-intensive tasks.","This research shows the tangible benefits of diverse knowledge integration and motivates further exploration in this direction."],"url":"http://arxiv.org/abs/2403.15364v1","category":"cs.CL"}
{"created":"2024-03-22 17:31:21","title":"Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks","abstract":"Higher variability in grid conditions, resulting from growing renewable penetration and increased incidence of extreme weather events, has increased the difficulty of screening for scenarios that may lead to catastrophic cascading failures. Traditional power-flow-based tools for assessing cascading blackout risk are too slow to properly explore the space of possible failures and load/generation patterns. We add to the growing literature of faster graph-neural-network (GNN)-based techniques, developing two novel techniques for the estimation of blackout magnitude from initial grid conditions. First we propose several methods for employing an initial classification step to filter out safe \"non blackout\" scenarios prior to magnitude estimation. Second, using insights from the statistical properties of cascading blackouts, we propose a method for facilitating non-local message passing in our GNN models. We validate these two approaches on a large simulated dataset, and show the potential of both to increase blackout size estimation performance.","sentences":["Higher variability in grid conditions, resulting from growing renewable penetration and increased incidence of extreme weather events, has increased the difficulty of screening for scenarios that may lead to catastrophic cascading failures.","Traditional power-flow-based tools for assessing cascading blackout risk are too slow to properly explore the space of possible failures and load/generation patterns.","We add to the growing literature of faster graph-neural-network (GNN)-based techniques, developing two novel techniques for the estimation of blackout magnitude from initial grid conditions.","First we propose several methods for employing an initial classification step to filter out safe \"non blackout\" scenarios prior to magnitude estimation.","Second, using insights from the statistical properties of cascading blackouts, we propose a method for facilitating non-local message passing in our GNN models.","We validate these two approaches on a large simulated dataset, and show the potential of both to increase blackout size estimation performance."],"url":"http://arxiv.org/abs/2403.15363v1","category":"eess.SY"}
{"created":"2024-03-22 17:26:05","title":"CoLLEGe: Concept Embedding Generation for Large Language Models","abstract":"Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting without task-specific training.","sentences":["Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly.","Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts.","Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models.","In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning.","CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions.","Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining.","We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting without task-specific training."],"url":"http://arxiv.org/abs/2403.15362v1","category":"cs.CL"}
{"created":"2024-03-22 17:19:50","title":"Simulation of radio signals from cosmic-ray cascades in air and ice as observed by in-ice Askaryan radio detectors","abstract":"A new generation of neutrino observatories, including RNO-G and RET, will search for PeV-EeV neutrinos interacting in the ice by detecting radio pulses. Extended air showers propagating into the ice will form an important background and could be a valuable calibration signal. We present results from a Monte-Carlo simulation framework developed to fully simulate radio emission from cosmic-ray particle cascades as observed by in-ice radio detectors in the polar regions. The framework involves a modified version of CoREAS (a module of CORSIKA 7) to simulate in-air radio emission and a GEANT4-based framework for simulating in-ice radio emission from cosmic-ray showers as observed by in-ice antennas. The particles that reach the surface of the polar ice sheet at the end of the CORSIKA 7 simulation are injected into the GEANT4-based shower simulation code that takes the particles and propagates them further into the ice sheet, using an exponential density profile for the ice. The framework takes into account curved ray paths caused by the exponential refractive index profiles of air and ice. We present the framework and discuss some key features of the radio signal and radio shower footprint for in-ice observers.","sentences":["A new generation of neutrino observatories, including RNO-G and RET, will search for PeV-EeV neutrinos interacting in the ice by detecting radio pulses.","Extended air showers propagating into the ice will form an important background and could be a valuable calibration signal.","We present results from a Monte-Carlo simulation framework developed to fully simulate radio emission from cosmic-ray particle cascades as observed by in-ice radio detectors in the polar regions.","The framework involves a modified version of CoREAS (a module of CORSIKA 7) to simulate in-air radio emission and a GEANT4-based framework for simulating in-ice radio emission from cosmic-ray showers as observed by in-ice antennas.","The particles that reach the surface of the polar ice sheet at the end of the CORSIKA 7 simulation are injected into the GEANT4-based shower simulation code that takes the particles and propagates them further into the ice sheet, using an exponential density profile for the ice.","The framework takes into account curved ray paths caused by the exponential refractive index profiles of air and ice.","We present the framework and discuss some key features of the radio signal and radio shower footprint for in-ice observers."],"url":"http://arxiv.org/abs/2403.15358v1","category":"astro-ph.HE"}
{"created":"2024-03-22 17:10:00","title":"Spatial covariant gravity with two degrees of freedom in the presence of an auxiliary scalar field: perturbation analysis","abstract":"We investigate a class of gravity theories respecting only spatial covariance, termed spatially covariant gravity, in the presence of an auxiliary scalar field. We examine the conditions on the Lagrangian required to eliminate scalar degrees of freedom, allowing only two tensorial degrees of freedom to propagate. Instead of strict constraint analysis, in this work, we employ the perturbation method and focus on the necessary conditions to evade the scalar mode at linear order in perturbations around a cosmological background. Starting from a general action and solving the auxiliary perturbation variables in terms of the would-be dynamical scalar mode, we derive the condition to remove its kinetic term, thus ensuring that no scalar mode propagates. As an application of the general condition, we study a polynomial-type Lagrangian as a concrete example, in which all the monomials are spatially covariant scalars containing two derivatives. We find that the auxiliary scalar field plays a nontrivial role, and new terms in the Lagrangian are allowed. Our analysis sheds light on constructing gravity theories with two degrees of freedom in the extended framework of spatially covariant gravity.","sentences":["We investigate a class of gravity theories respecting only spatial covariance, termed spatially covariant gravity, in the presence of an auxiliary scalar field.","We examine the conditions on the Lagrangian required to eliminate scalar degrees of freedom, allowing only two tensorial degrees of freedom to propagate.","Instead of strict constraint analysis, in this work, we employ the perturbation method and focus on the necessary conditions to evade the scalar mode at linear order in perturbations around a cosmological background.","Starting from a general action and solving the auxiliary perturbation variables in terms of the would-be dynamical scalar mode, we derive the condition to remove its kinetic term, thus ensuring that no scalar mode propagates.","As an application of the general condition, we study a polynomial-type Lagrangian as a concrete example, in which all the monomials are spatially covariant scalars containing two derivatives.","We find that the auxiliary scalar field plays a nontrivial role, and new terms in the Lagrangian are allowed.","Our analysis sheds light on constructing gravity theories with two degrees of freedom in the extended framework of spatially covariant gravity."],"url":"http://arxiv.org/abs/2403.15355v1","category":"gr-qc"}
{"created":"2024-03-22 17:08:03","title":"Fully automated workflow for the design of patient-specific orthopaedic implants: application to total knee arthroplasty","abstract":"Arthroplasty is commonly performed to treat joint osteoarthritis, reducing pain and improving mobility. While arthroplasty has known several technical improvements, a significant share of patients are still unsatisfied with their surgery. Personalised arthroplasty improves surgical outcomes however current solutions require delays, making it difficult to integrate in clinical routine. We propose a fully automated workflow to design patient-specific implants, presented for total knee arthroplasty, the most widely performed arthroplasty in the world nowadays.   The proposed pipeline first uses artificial neural networks to segment the proximal and distal extremities of the femur and tibia. Then the full bones are reconstructed using augmented statistical shape models, combining shape and landmarks information. Finally, 77 morphological parameters are computed to design patient-specific implants. The developed workflow has been trained using 91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in terms of accuracy and execution time.   The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$ for the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical landmarks determination. The custom implants fitted the patients' anatomy with $0.6\\pm0.2mm$ accuracy. The whole process from segmentation to implants' design lasted about 5 minutes.   The proposed workflow allows for a fast and reliable personalisation of knee implants, directly from the patient CT image without requiring any manual intervention. It establishes a patient-specific pre-operative planning for TKA in a very short time making it easily available for all patients. Combined with efficient implant manufacturing techniques, this solution could help answer the growing number of arthroplasties while reducing complications and improving the patients' satisfaction.","sentences":["Arthroplasty is commonly performed to treat joint osteoarthritis, reducing pain and improving mobility.","While arthroplasty has known several technical improvements, a significant share of patients are still unsatisfied with their surgery.","Personalised arthroplasty improves surgical outcomes however current solutions require delays, making it difficult to integrate in clinical routine.","We propose a fully automated workflow to design patient-specific implants, presented for total knee arthroplasty, the most widely performed arthroplasty in the world nowadays.   ","The proposed pipeline first uses artificial neural networks to segment the proximal and distal extremities of the femur and tibia.","Then the full bones are reconstructed using augmented statistical shape models, combining shape and landmarks information.","Finally, 77 morphological parameters are computed to design patient-specific implants.","The developed workflow has been trained using 91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in terms of accuracy and execution time.   ","The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$ for the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical landmarks determination.","The custom implants fitted the patients' anatomy with $0.6\\pm0.2mm$ accuracy.","The whole process from segmentation to implants' design lasted about 5 minutes.   ","The proposed workflow allows for a fast and reliable personalisation of knee implants, directly from the patient CT image without requiring any manual intervention.","It establishes a patient-specific pre-operative planning for TKA in a very short time making it easily available for all patients.","Combined with efficient implant manufacturing techniques, this solution could help answer the growing number of arthroplasties while reducing complications and improving the patients' satisfaction."],"url":"http://arxiv.org/abs/2403.15353v1","category":"cs.CV"}
{"created":"2024-03-22 17:06:05","title":"Multi-Review Fusion-in-Context","abstract":"Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize \\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses. This study lays the groundwork for further exploration of modular text generation in the multi-document setting, offering potential improvements in the quality and reliability of generated content. \\footnote{Our benchmark, FuseReviews, including the dataset, evaluation framework and designated leaderboard, can be found at \\url{https://fusereviews.github.io/}.}","sentences":["Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation.","Current end-to-end methods are difficult to control and interpret due to their opaqueness.","Accordingly, recent works have proposed a modular approach, with separate components for each step.","Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting.","Concretely, we formalize \\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content.","A model then needs to generate a coherent passage that includes all and only the target information.","Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment.","Several baseline models exhibit promising outcomes and provide insightful analyses.","This study lays the groundwork for further exploration of modular text generation in the multi-document setting, offering potential improvements in the quality and reliability of generated content.","\\footnote{Our benchmark, FuseReviews, including the dataset, evaluation framework and designated leaderboard, can be found at \\url{https://fusereviews.github.io/}.}"],"url":"http://arxiv.org/abs/2403.15351v1","category":"cs.CL"}
{"created":"2024-03-22 16:51:53","title":"On the validity of the rotating wave approximation for coupled harmonic oscillators","abstract":"In this work we study the validity of the rotating wave approximation of an ideal system composed of two harmonic oscillators evolving with a quadratic Hamiltonian and arbitrarily strong interaction. We solve the dynamics analytically by employing tools from symplectic geometry. We focus on systems with initial Gaussian states and quantify exactly the deviation between the state obtained through the rotating approximation and the state obtained through the full evolution, therefore providing an answer for all values of the coupling. We find that the squeezing present in the full Hamiltonian and in the initial state governs the deviation from the approximated evolution. Furthermore, we also show that the rotating wave approximation is recovered for resonant frequencies and vanishing coupling to frequency ratio. Finally, we give a general proof of the rotating wave approximation and estimate its convergence on Fock states. Applications and potential physical implementations are also discussed.","sentences":["In this work we study the validity of the rotating wave approximation of an ideal system composed of two harmonic oscillators evolving with a quadratic Hamiltonian and arbitrarily strong interaction.","We solve the dynamics analytically by employing tools from symplectic geometry.","We focus on systems with initial Gaussian states and quantify exactly the deviation between the state obtained through the rotating approximation and the state obtained through the full evolution, therefore providing an answer for all values of the coupling.","We find that the squeezing present in the full Hamiltonian and in the initial state governs the deviation from the approximated evolution.","Furthermore, we also show that the rotating wave approximation is recovered for resonant frequencies and vanishing coupling to frequency ratio.","Finally, we give a general proof of the rotating wave approximation and estimate its convergence on Fock states.","Applications and potential physical implementations are also discussed."],"url":"http://arxiv.org/abs/2403.15342v1","category":"quant-ph"}
{"created":"2024-03-22 16:50:56","title":"Collaborative AI Teaming in Unknown Environments via Active Goal Deduction","abstract":"With the advancements of artificial intelligence (AI), we're seeing more scenarios that require AI to work closely with other agents, whose goals and strategies might not be known beforehand. However, existing approaches for training collaborative agents often require defined and known reward signals and cannot address the problem of teaming with unknown agents that often have latent objectives/rewards. In response to this challenge, we propose teaming with unknown agents framework, which leverages kernel density Bayesian inverse learning method for active goal deduction and utilizes pre-trained, goal-conditioned policies to enable zero-shot policy adaptation. We prove that unbiased reward estimates in our framework are sufficient for optimal teaming with unknown agents. We further evaluate the framework of redesigned multi-agent particle and StarCraft II micromanagement environments with diverse unknown agents of different behaviors/rewards. Empirical results demonstrate that our framework significantly advances the teaming performance of AI and unknown agents in a wide range of collaborative scenarios.","sentences":["With the advancements of artificial intelligence (AI), we're seeing more scenarios that require AI to work closely with other agents, whose goals and strategies might not be known beforehand.","However, existing approaches for training collaborative agents often require defined and known reward signals and cannot address the problem of teaming with unknown agents that often have latent objectives/rewards.","In response to this challenge, we propose teaming with unknown agents framework, which leverages kernel density Bayesian inverse learning method for active goal deduction and utilizes pre-trained, goal-conditioned policies to enable zero-shot policy adaptation.","We prove that unbiased reward estimates in our framework are sufficient for optimal teaming with unknown agents.","We further evaluate the framework of redesigned multi-agent particle and StarCraft II micromanagement environments with diverse unknown agents of different behaviors/rewards.","Empirical results demonstrate that our framework significantly advances the teaming performance of AI and unknown agents in a wide range of collaborative scenarios."],"url":"http://arxiv.org/abs/2403.15341v1","category":"cs.AI"}
{"created":"2024-03-22 16:46:23","title":"High Harmonic Generation by Bright Squeezed Vacuum","abstract":"We observe non-perturbative high harmonic generation in solids pumped by a macroscopic quantum state of light, bright squeezed vacuum (BSV), which we generate in a single spatiotemporal mode. Due to its broad photon-number distribution, covering states from $0$ to $2 \\times 10^{13}$ photons per pulse, and sub-cycle electric field fluctuations over $\\pm1\\hbox{V}/\\hbox{\\r{A}}$, BSV provides access to free carrier dynamics within a much broader range of peak intensities than accessible with coherent light. It is also considerably more efficient in the generation of high harmonics than coherent light of the same mean intensity.","sentences":["We observe non-perturbative high harmonic generation in solids pumped by a macroscopic quantum state of light, bright squeezed vacuum (BSV), which we generate in a single spatiotemporal mode.","Due to its broad photon-number distribution, covering states from $0$ to $2 \\times 10^{13}$ photons per pulse, and sub-cycle electric field fluctuations over $\\pm1\\hbox{V}/\\hbox{\\r{A}}$, BSV provides access to free carrier dynamics within a much broader range of peak intensities than accessible with coherent light.","It is also considerably more efficient in the generation of high harmonics than coherent light of the same mean intensity."],"url":"http://arxiv.org/abs/2403.15337v1","category":"quant-ph"}
{"created":"2024-03-22 16:41:45","title":"Dialogue Understandability: Why are we streaming movies with subtitles?","abstract":"Watching movies and TV shows with subtitles enabled is not simply down to audibility or speech intelligibility. A variety of evolving factors related to technological advances, cinema production and social behaviour challenge our perception and understanding. This study seeks to formalise and give context to these influential factors under a wider and novel term referred to as Dialogue Understandability. We propose a working definition for Dialogue Understandability being a listener's capacity to follow the story without undue cognitive effort or concentration being required that impacts their Quality of Experience (QoE). The paper identifies, describes and categorises the factors that influence Dialogue Understandability mapping them over the QoE framework, a media streaming lifecycle, and the stakeholders involved. We then explore available measurement tools in the literature and link them to the factors they could potentially be used for. The maturity and suitability of these tools is evaluated over a set of pilot experiments. Finally, we reflect on the gaps that still need to be filled, what we can measure and what not, future subjective experiments, and new research trends that could help us to fully characterise Dialogue Understandability.","sentences":["Watching movies and TV shows with subtitles enabled is not simply down to audibility or speech intelligibility.","A variety of evolving factors related to technological advances, cinema production and social behaviour challenge our perception and understanding.","This study seeks to formalise and give context to these influential factors under a wider and novel term referred to as Dialogue Understandability.","We propose a working definition for Dialogue Understandability being a listener's capacity to follow the story without undue cognitive effort or concentration being required that impacts their Quality of Experience (QoE).","The paper identifies, describes and categorises the factors that influence Dialogue Understandability mapping them over the QoE framework, a media streaming lifecycle, and the stakeholders involved.","We then explore available measurement tools in the literature and link them to the factors they could potentially be used for.","The maturity and suitability of these tools is evaluated over a set of pilot experiments.","Finally, we reflect on the gaps that still need to be filled, what we can measure and what not, future subjective experiments, and new research trends that could help us to fully characterise Dialogue Understandability."],"url":"http://arxiv.org/abs/2403.15336v1","category":"eess.AS"}
{"created":"2024-03-22 16:40:48","title":"Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy","abstract":"We present a novel approach that aims to address both safety and stability of a haptic teleoperation system within a framework of Haptic Shared Autonomy (HSA). We use Control Barrier Functions (CBFs) to generate the control input that follows the user's input as closely as possible while guaranteeing safety. In the context of stability of the human-in-the-loop system, we limit the force feedback perceived by the user via a small $L_2$-gain, which is achieved by limiting the control and the force feedback via a differential constraint. Specifically, with the property of HSA, we propose two pathways to design the control and the force feedback: Sequential Control Force (SCF) and Joint Control Force (JCF). Both designs can achieve safety and stability but with different responses to the user's commands. We conducted experimental simulations to evaluate and investigate the properties of the designed methods. We also tested the proposed method on a physical quadrotor UAV and a haptic interface.","sentences":["We present a novel approach that aims to address both safety and stability of a haptic teleoperation system within a framework of Haptic Shared Autonomy (HSA).","We use Control Barrier Functions (CBFs) to generate the control input that follows the user's input as closely as possible while guaranteeing safety.","In the context of stability of the human-in-the-loop system, we limit the force feedback perceived by the user via a small $L_2$-gain, which is achieved by limiting the control and the force feedback via a differential constraint.","Specifically, with the property of HSA, we propose two pathways to design the control and the force feedback: Sequential Control Force (SCF) and Joint Control Force (JCF).","Both designs can achieve safety and stability but with different responses to the user's commands.","We conducted experimental simulations to evaluate and investigate the properties of the designed methods.","We also tested the proposed method on a physical quadrotor UAV and a haptic interface."],"url":"http://arxiv.org/abs/2403.15335v1","category":"cs.RO"}
{"created":"2024-03-22 16:35:38","title":"Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization","abstract":"In text-to-image personalization, a timely and crucial challenge is the tendency of generated images overfitting to the biases present in the reference images. We initiate our study with a comprehensive categorization of the biases into background, nearby-object, tied-object, substance (in style re-contextualization), and pose biases. These biases manifest in the generated images due to their entanglement into the subject embedding. This undesired embedding entanglement not only results in the reflection of biases from the reference images into the generated images but also notably diminishes the alignment of the generated images with the given generation prompt. To address this challenge, we propose SID~(Selectively Informative Description), a text description strategy that deviates from the prevalent approach of only characterizing the subject's class identification. SID is generated utilizing multimodal GPT-4 and can be seamlessly integrated into optimization-based models. We present comprehensive experimental results along with analyses of cross-attention maps, subject-alignment, non-subject-disentanglement, and text-alignment.","sentences":["In text-to-image personalization, a timely and crucial challenge is the tendency of generated images overfitting to the biases present in the reference images.","We initiate our study with a comprehensive categorization of the biases into background, nearby-object, tied-object, substance (in style re-contextualization), and pose biases.","These biases manifest in the generated images due to their entanglement into the subject embedding.","This undesired embedding entanglement not only results in the reflection of biases from the reference images into the generated images but also notably diminishes the alignment of the generated images with the given generation prompt.","To address this challenge, we propose SID~(Selectively Informative Description), a text description strategy that deviates from the prevalent approach of only characterizing the subject's class identification.","SID is generated utilizing multimodal GPT-4 and can be seamlessly integrated into optimization-based models.","We present comprehensive experimental results along with analyses of cross-attention maps, subject-alignment, non-subject-disentanglement, and text-alignment."],"url":"http://arxiv.org/abs/2403.15330v1","category":"cs.CV"}
{"created":"2024-03-22 16:30:58","title":"A Technological Perspective on Misuse of Available AI","abstract":"Potential malicious misuse of civilian artificial intelligence (AI) poses serious threats to security on a national and international level. Besides defining autonomous systems from a technological viewpoint and explaining how AI development is characterized, we show how already existing and openly available AI technology could be misused. To underline this, we developed three exemplary use cases of potentially misused AI that threaten political, digital and physical security. The use cases can be built from existing AI technologies and components from academia, the private sector and the developer-community. This shows how freely available AI can be combined into autonomous weapon systems. Based on the use cases, we deduce points of control and further measures to prevent the potential threat through misused AI. Further, we promote the consideration of malicious misuse of civilian AI systems in the discussion on autonomous weapon systems (AWS).","sentences":["Potential malicious misuse of civilian artificial intelligence (AI) poses serious threats to security on a national and international level.","Besides defining autonomous systems from a technological viewpoint and explaining how AI development is characterized, we show how already existing and openly available AI technology could be misused.","To underline this, we developed three exemplary use cases of potentially misused AI that threaten political, digital and physical security.","The use cases can be built from existing AI technologies and components from academia, the private sector and the developer-community.","This shows how freely available AI can be combined into autonomous weapon systems.","Based on the use cases, we deduce points of control and further measures to prevent the potential threat through misused AI.","Further, we promote the consideration of malicious misuse of civilian AI systems in the discussion on autonomous weapon systems (AWS)."],"url":"http://arxiv.org/abs/2403.15325v1","category":"cs.CY"}
{"created":"2024-03-22 16:14:55","title":"From loop quantum gravity to cosmology: the 2-vertex model","abstract":"We show that the U(N)-symmetry reduction of the loop-quantum-gravity truncation known as the 2-vertex model provides the effective improved dynamics of loop quantum cosmology with an arbitrary perfect barotropic fluid content.","sentences":["We show that the U(N)-symmetry reduction of the loop-quantum-gravity truncation known as the 2-vertex model provides the effective improved dynamics of loop quantum cosmology with an arbitrary perfect barotropic fluid content."],"url":"http://arxiv.org/abs/2403.15320v1","category":"gr-qc"}
{"created":"2024-03-22 16:11:53","title":"Global Analysis of LISA Data with Galactic Binaries and Massive Black Hole Binaries","abstract":"The Laser Interferometer Space Antenna (LISA) is a planned space-based observatory to measure gravitational waves in the millihertz frequency band. This frequency band is expected to be dominated by signals from millions of Galactic binaries and tens of merging massive black hole binaries. The LISA Data Challenge 2a is focused on the robust signal extraction from a blend of these two types of gravitational wave signals. Here, we introduce a novel high performance and cost-effective global fit pipeline extracting and characterizing galactic binary and massive black hole binary signals and estimate the noise of the residual. We preform the pipeline in a time-evolving weekly analysis starting with and observation time of 1 week until we reach a full year. As expected we detect more galactic binaries and massive black hole binaries bringing the noise estimate of the residual closer to the instrument noise by each week of additional observation time. Furthermore, we present a novel maximum likelihood estimate-based algorithm for extracting multiple massive black hole binaries. Additionally we demonstrate a massive black hole binary signal extraction with a more accurate LISA response, considering higher harmonic modes, in a noisy data set.","sentences":["The Laser Interferometer Space Antenna (LISA) is a planned space-based observatory to measure gravitational waves in the millihertz frequency band.","This frequency band is expected to be dominated by signals from millions of Galactic binaries and tens of merging massive black hole binaries.","The LISA Data Challenge 2a is focused on the robust signal extraction from a blend of these two types of gravitational wave signals.","Here, we introduce a novel high performance and cost-effective global fit pipeline extracting and characterizing galactic binary and massive black hole binary signals and estimate the noise of the residual.","We preform the pipeline in a time-evolving weekly analysis starting with and observation time of 1 week until we reach a full year.","As expected we detect more galactic binaries and massive black hole binaries bringing the noise estimate of the residual closer to the instrument noise by each week of additional observation time.","Furthermore, we present a novel maximum likelihood estimate-based algorithm for extracting multiple massive black hole binaries.","Additionally we demonstrate a massive black hole binary signal extraction with a more accurate LISA response, considering higher harmonic modes, in a noisy data set."],"url":"http://arxiv.org/abs/2403.15318v1","category":"gr-qc"}
{"created":"2024-03-22 16:11:29","title":"Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection","abstract":"Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization.In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior. Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector's perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI).Moreover, an innovative point-guided self-supervised learning technique is proposed to allow for fully exploiting point priors, even in student models.Extensive experiments on representative nuScenes dataset demonstrate our Point-DETR3D obtains significant improvements compared to previous works. Notably, with only 5% of labeled data, Point-DETR3D achieves over 90% performance of its fully supervised counterpart.","sentences":["Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming.","Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization.","In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points.","To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.","Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior.","Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector's perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI).Moreover, an innovative point-guided self-supervised learning technique is proposed to allow for fully exploiting point priors, even in student models.","Extensive experiments on representative nuScenes dataset demonstrate our Point-DETR3D obtains significant improvements compared to previous works.","Notably, with only 5% of labeled data, Point-DETR3D achieves over 90% performance of its fully supervised counterpart."],"url":"http://arxiv.org/abs/2403.15317v1","category":"cs.CV"}
{"created":"2024-03-22 16:10:38","title":"Ultrasound Imaging based on the Variance of a Diffusion Restoration Model","abstract":"Despite today's prevalence of ultrasound imaging in medicine, ultrasound signal-to-noise ratio is still affected by several sources of noise and artefacts. Moreover, enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. Recently, there has been progress in both model-based and learning-based approaches addressing the problem of ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid reconstruction method combining an ultrasound linear direct model with a learning-based prior coming from a generative Denoising Diffusion model. More specifically, we rely on the unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model (DDRM). Given the nature of multiplicative noise inherent to ultrasound, this paper proposes an empirical model to characterize the stochasticity of diffusion reconstruction of ultrasound images, and shows the interest of its variance as an echogenicity map estimator. We conduct experiments on synthetic, in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging approach in achieving high-quality image reconstructions from single plane-wave acquisitions and in comparison to state-of-the-art methods.","sentences":["Despite today's prevalence of ultrasound imaging in medicine, ultrasound signal-to-noise ratio is still affected by several sources of noise and artefacts.","Moreover, enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation.","Recently, there has been progress in both model-based and learning-based approaches addressing the problem of ultrasound image reconstruction.","Bringing the best from both worlds, we propose a hybrid reconstruction method combining an ultrasound linear direct model with a learning-based prior coming from a generative Denoising Diffusion model.","More specifically, we rely on the unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model (DDRM).","Given the nature of multiplicative noise inherent to ultrasound, this paper proposes an empirical model to characterize the stochasticity of diffusion reconstruction of ultrasound images, and shows the interest of its variance as an echogenicity map estimator.","We conduct experiments on synthetic, in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging approach in achieving high-quality image reconstructions from single plane-wave acquisitions and in comparison to state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.15316v1","category":"eess.IV"}
{"created":"2024-03-22 16:06:05","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","abstract":"Accurate detection and tracking of surrounding objects is essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications.","sentences":["Accurate detection and tracking of surrounding objects is essential to enable self-driving vehicles.","While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high performance, the appeal of camera-only solutions lies in their cost-effectiveness.","Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise.","As a recent development, the combination of RADARs and cameras is emerging as a promising solution.","This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT).","Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor.","Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities.","CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications."],"url":"http://arxiv.org/abs/2403.15313v1","category":"cs.CV"}
{"created":"2024-03-22 16:04:26","title":"A Wasserstein perspective of Vanilla GANs","abstract":"The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative result for the approximation of a Lipschitz function by a feedforward ReLU network with bounded H\\\"older norm, we conclude a rate of convergence for Vanilla GANs as well as Wasserstein GANs as estimators of the unknown probability distribution.","sentences":["The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research.","The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties.","Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space.","To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance.","By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs.","In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance.","The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks.","By providing a quantitative result for the approximation of a Lipschitz function by a feedforward ReLU network with bounded H\\\"older norm, we conclude a rate of convergence for Vanilla GANs as well as Wasserstein GANs as estimators of the unknown probability distribution."],"url":"http://arxiv.org/abs/2403.15312v1","category":"math.ST"}
{"created":"2024-03-22 15:59:24","title":"Controlled Training Data Generation with Diffusion Models","abstract":"In this work, we present a method to control a text-to-image generative model to produce training data specifically \"useful\" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.","sentences":["In this work, we present a method to control a text-to-image generative model to produce training data specifically \"useful\" for supervised learning.","Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms.","The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss.","While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient.","Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution.","We call the method combining these two mechanisms Guided Adversarial Prompts.","We perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches."],"url":"http://arxiv.org/abs/2403.15309v1","category":"cs.CV"}
{"created":"2024-03-22 15:58:34","title":"HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers","abstract":"Horticultural tasks such as pruning and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates.","sentences":["Horticultural tasks such as pruning and selective harvesting are labor intensive and horticultural staff are hard to find.","Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs.","In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras.","Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants.","Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting.","We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments.","Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates."],"url":"http://arxiv.org/abs/2403.15306v1","category":"cs.RO"}
{"created":"2024-03-22 15:54:30","title":"KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing","abstract":"Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounting for changes in sequence length when expanding KCs, leading to different models being tested with varying sequence lengths but still compared against the same benchmark.   To address these problems, we introduce a general masking framework that mitigates the first problem and enhances the performance of such KT models while preserving the original model architecture without significant alterations. Additionally, we introduce KTbench, an open-source benchmark library designed to ensure the reproducibility of this work while mitigating the second problem.","sentences":["Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems.","Learning items are tagged with skill labels called knowledge concepts (KCs).","Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs.","This often results in a longer sequence length.","This approach addresses the issue of sparse item-student interactions and minimises model parameters.","However, two problems have been identified with such models.   ","The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance.","This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item.","The second problem is that the available benchmark implementations ignore accounting for changes in sequence length when expanding KCs, leading to different models being tested with varying sequence lengths but still compared against the same benchmark.   ","To address these problems, we introduce a general masking framework that mitigates the first problem and enhances the performance of such KT models while preserving the original model architecture without significant alterations.","Additionally, we introduce KTbench, an open-source benchmark library designed to ensure the reproducibility of this work while mitigating the second problem."],"url":"http://arxiv.org/abs/2403.15304v1","category":"cs.CY"}
{"created":"2024-03-22 15:54:08","title":"Network Calculus Characterization of Congestion Control for Time-Varying Traffic","abstract":"Models for the dynamics of congestion control generally involve systems of coupled differential equations. Universally, these models assume that traffic sources saturate the maximum transmissions allowed by the congestion control method. This is not suitable for studying congestion control of intermittent but bursty traffic sources. In this paper, we present a characterization of congestion control for arbitrary time-varying traffic that applies to rate-based as well as window-based congestion control. We leverage the capability of network calculus to precisely describe the input-output relationship at network elements for arbitrary source traffic. We show that our characterization can closely track the dynamics of even complex congestion control algorithms.","sentences":["Models for the dynamics of congestion control generally involve systems of coupled differential equations.","Universally, these models assume that traffic sources saturate the maximum transmissions allowed by the congestion control method.","This is not suitable for studying congestion control of intermittent but bursty traffic sources.","In this paper, we present a characterization of congestion control for arbitrary time-varying traffic that applies to rate-based as well as window-based congestion control.","We leverage the capability of network calculus to precisely describe the input-output relationship at network elements for arbitrary source traffic.","We show that our characterization can closely track the dynamics of even complex congestion control algorithms."],"url":"http://arxiv.org/abs/2403.15303v1","category":"cs.NI"}
{"created":"2024-03-22 15:51:39","title":"Planning with a Learned Policy Basis to Optimally Solve Complex Tasks","abstract":"Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.","sentences":["Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems.","However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem.","We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem.","In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning.","In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments."],"url":"http://arxiv.org/abs/2403.15301v1","category":"cs.LG"}
{"created":"2024-03-22 15:44:59","title":"Sphere Neural-Networks for Rational Reasoning","abstract":"The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model that can determine the validity of long-chained syllogistic reasoning in one epoch by constructing sphere configurations as Euler diagrams, with the worst computational complexity of O(N^2). SphNN can evolve into various types of reasoning, such as spatio-temporal reasoning, logical reasoning with negation and disjunction, event reasoning, neuro-symbolic reasoning, and humour understanding (the highest level of cognition). All these suggest a new kind of Herbert A. Simon's scissors with two neural blades. SphNNs will tremendously enhance interdisciplinary collaborations to develop the two neural blades and realise deterministic neural reasoning and human-bounded rationality and elevate LLMs to reliable psychological AI. This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination.","sentences":["The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance.","However, it remains unclear whether LLMs reason.","It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition.","Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres.","We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality.","Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target.","SphNN is the first neural model that can determine the validity of long-chained syllogistic reasoning in one epoch by constructing sphere configurations as Euler diagrams, with the worst computational complexity of O(N^2).","SphNN can evolve into various types of reasoning, such as spatio-temporal reasoning, logical reasoning with negation and disjunction, event reasoning, neuro-symbolic reasoning, and humour understanding (the highest level of cognition).","All these suggest a new kind of Herbert A. Simon's scissors with two neural blades.","SphNNs will tremendously enhance interdisciplinary collaborations to develop the two neural blades and realise deterministic neural reasoning and human-bounded rationality and elevate LLMs to reliable psychological AI.","This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination."],"url":"http://arxiv.org/abs/2403.15297v1","category":"cs.AI"}
{"created":"2024-03-22 15:40:26","title":"Nonlinear Reachable Set Computation and Model Predictive Control for Safe Hypersonic Re-entry of Atmospheric Vehicles","abstract":"This paper investigates the application of reachability analysis to the re-entry problem faced by vehicles entering Earth's atmosphere. The study delves into the time evolution of reachable sets for the system, particularly when subject to nonlinear implicit controls, given the potential damage from the intense heat generated during hypersonic re-entry. Our proposed methodology leverages zonotopes and constrained zonotopes to ensure compliance with safety specifications. Furthermore, we utilize Model Predictive Control for detailed trajectory planning. To substantiate our methodology, we provide detailed simulations that not only tackle nonlinear re-entry scenarios but also illustrate trajectory planning using MPC.","sentences":["This paper investigates the application of reachability analysis to the re-entry problem faced by vehicles entering Earth's atmosphere.","The study delves into the time evolution of reachable sets for the system, particularly when subject to nonlinear implicit controls, given the potential damage from the intense heat generated during hypersonic re-entry.","Our proposed methodology leverages zonotopes and constrained zonotopes to ensure compliance with safety specifications.","Furthermore, we utilize Model Predictive Control for detailed trajectory planning.","To substantiate our methodology, we provide detailed simulations that not only tackle nonlinear re-entry scenarios but also illustrate trajectory planning using MPC."],"url":"http://arxiv.org/abs/2403.15294v1","category":"math.OC"}
{"created":"2024-03-22 15:40:11","title":"Human behaviour through a LENS: How Linguistic content triggers Emotions and Norms and determines Strategy choices","abstract":"Over the last two decades, a growing body of experimental research has provided evidence that linguistic frames influence human behaviour in economic games, beyond the economic consequences of the available actions. This article proposes a novel framework that transcends the traditional confines of outcome-based preference models. According to the LENS model, the Linguistic description of the decision problem triggers Emotional responses and suggests potential Norms of behaviour, which then interact to shape an individual's Strategic choice. The article reviews experimental evidence that supports each path of the LENS model. Furthermore, it identifies and discusses several critical research questions that arise from this model, pointing towards avenues for future inquiry.","sentences":["Over the last two decades, a growing body of experimental research has provided evidence that linguistic frames influence human behaviour in economic games, beyond the economic consequences of the available actions.","This article proposes a novel framework that transcends the traditional confines of outcome-based preference models.","According to the LENS model, the Linguistic description of the decision problem triggers Emotional responses and suggests potential Norms of behaviour, which then interact to shape an individual's Strategic choice.","The article reviews experimental evidence that supports each path of the LENS model.","Furthermore, it identifies and discusses several critical research questions that arise from this model, pointing towards avenues for future inquiry."],"url":"http://arxiv.org/abs/2403.15293v1","category":"cs.CL"}
{"created":"2024-03-22 15:40:09","title":"A data-driven approach to PDE-constrained optimization in inverse problems","abstract":"Inverse problems are ubiquitous in science and engineering. Many of these are naturally formulated as a PDE-constrained optimization problem. These non-linear, large-scale, constrained optimization problems know many challenges, of which the inherent non-linearity of the problem is an important one. As an alternative to this physics-driven approach, data-driven methods have been proposed. These methods come with their own set of challenges, and it appears that, ideally, one would devise hybrid methods that combine the best of both worlds. In this paper, we propose one way of combining PDE-constrained optimization with recently proposed data-driven reduced-order models. Starting from an infinite-dimensional formulation of the inverse problem with discrete data, we propose a general framework for the analysis and discretisation of such problems. The proposed approach is based on a relaxed formulation of the PDE-constrained optimization problem, which reduces to a weighted non-linear least-squares problem. The weight matrix turns out to be the Gram matrix of solutions of the PDE, and it can be estimated directly from the measurements. We provide a number of representative case studies and numerical examples.","sentences":["Inverse problems are ubiquitous in science and engineering.","Many of these are naturally formulated as a PDE-constrained optimization problem.","These non-linear, large-scale, constrained optimization problems know many challenges, of which the inherent non-linearity of the problem is an important one.","As an alternative to this physics-driven approach, data-driven methods have been proposed.","These methods come with their own set of challenges, and it appears that, ideally, one would devise hybrid methods that combine the best of both worlds.","In this paper, we propose one way of combining PDE-constrained optimization with recently proposed data-driven reduced-order models.","Starting from an infinite-dimensional formulation of the inverse problem with discrete data, we propose a general framework for the analysis and discretisation of such problems.","The proposed approach is based on a relaxed formulation of the PDE-constrained optimization problem, which reduces to a weighted non-linear least-squares problem.","The weight matrix turns out to be the Gram matrix of solutions of the PDE, and it can be estimated directly from the measurements.","We provide a number of representative case studies and numerical examples."],"url":"http://arxiv.org/abs/2403.15292v1","category":"math.OC"}
{"created":"2024-03-22 15:40:05","title":"Wastewater-based Epidemiology for COVID-19 Surveillance: A Survey","abstract":"The pandemic of COVID-19 has imposed tremendous pressure on public health systems and social economic ecosystems over the past years. To alleviate its social impact, it is important to proactively track the prevalence of COVID-19 within communities. The traditional way to estimate the disease prevalence is to estimate from reported clinical test data or surveys. However, the coverage of clinical tests is often limited and the tests can be labor-intensive, requires reliable and timely results, and consistent diagnostic and reporting criteria. Recent studies revealed that patients who are diagnosed with COVID-19 often undergo fecal shedding of SARS-CoV-2 virus into wastewater, which makes wastewater-based epidemiology (WBE) for COVID-19 surveillance a promising approach to complement traditional clinical testing. In this paper, we survey the existing literature regarding WBE for COVID-19 surveillance and summarize the current advances in the area. Specifically, we have covered the key aspects of wastewater sampling, sample testing, and presented a comprehensive and organized summary of wastewater data analytical methods. Finally, we provide the open challenges on current wastewater-based COVID-19 surveillance studies, aiming to encourage new ideas to advance the development of effective wastewater-based surveillance systems for general infectious diseases.","sentences":["The pandemic of COVID-19 has imposed tremendous pressure on public health systems and social economic ecosystems over the past years.","To alleviate its social impact, it is important to proactively track the prevalence of COVID-19 within communities.","The traditional way to estimate the disease prevalence is to estimate from reported clinical test data or surveys.","However, the coverage of clinical tests is often limited and the tests can be labor-intensive, requires reliable and timely results, and consistent diagnostic and reporting criteria.","Recent studies revealed that patients who are diagnosed with COVID-19 often undergo fecal shedding of SARS-CoV-2 virus into wastewater, which makes wastewater-based epidemiology (WBE) for COVID-19 surveillance a promising approach to complement traditional clinical testing.","In this paper, we survey the existing literature regarding WBE for COVID-19 surveillance and summarize the current advances in the area.","Specifically, we have covered the key aspects of wastewater sampling, sample testing, and presented a comprehensive and organized summary of wastewater data analytical methods.","Finally, we provide the open challenges on current wastewater-based COVID-19 surveillance studies, aiming to encourage new ideas to advance the development of effective wastewater-based surveillance systems for general infectious diseases."],"url":"http://arxiv.org/abs/2403.15291v1","category":"stat.AP"}
{"created":"2024-03-22 15:39:56","title":"Contact interactions, self-adjoint extensions, and low-energy scattering","abstract":"Low-energy scattering is well described by the effective-range expansion. In quantum mechanics, a tower of contact interactions can generate terms in this expansion after renormalization. Scattering parameters are also encoded in the self-adjoint extension of the Hamiltonian. We briefly review this well-known result for two particles with s-wave interactions using impenetrable self-adjoint extensions, including the case of harmonically trapped two-particle states. By contrast, the one-dimensional scattering problem is surprisingly intricate. We show that the families of self-adjoint extensions correspond to a coupled system of symmetric and antisymmetric outgoing waves, which is diagonalized by an SU(2) transformation that accounts for mixing and a relative phase. This is corroborated by an effective theory computation that includes all four energy-independent contact interactions. The equivalence of various one-dimensional contact interactions is discussed and scrutinized from the perspective of renormalization. As an application, the spectrum of a general point interaction with a harmonic trap is solved in one dimension.","sentences":["Low-energy scattering is well described by the effective-range expansion.","In quantum mechanics, a tower of contact interactions can generate terms in this expansion after renormalization.","Scattering parameters are also encoded in the self-adjoint extension of the Hamiltonian.","We briefly review this well-known result for two particles with s-wave interactions using impenetrable self-adjoint extensions, including the case of harmonically trapped two-particle states.","By contrast, the one-dimensional scattering problem is surprisingly intricate.","We show that the families of self-adjoint extensions correspond to a coupled system of symmetric and antisymmetric outgoing waves, which is diagonalized by an SU(2) transformation that accounts for mixing and a relative phase.","This is corroborated by an effective theory computation that includes all four energy-independent contact interactions.","The equivalence of various one-dimensional contact interactions is discussed and scrutinized from the perspective of renormalization.","As an application, the spectrum of a general point interaction with a harmonic trap is solved in one dimension."],"url":"http://arxiv.org/abs/2403.15290v1","category":"quant-ph"}
{"created":"2024-03-22 15:31:37","title":"Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse","abstract":"Driven by the great advances in metaverse and edge computing technologies, vehicular edge metaverses are expected to disrupt the current paradigm of intelligent transportation systems. As highly computerized avatars of Vehicular Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can provide valuable metaverse services to improve driving safety and on-board satisfaction for their VMUs throughout journeys. To maintain uninterrupted metaverse experiences, VTs must be migrated among edge servers following the movements of vehicles. This can raise concerns about privacy breaches during the dynamic communications among vehicular edge metaverses. To address these concerns and safeguard location privacy, pseudonyms as temporary identifiers can be leveraged by both VMUs and VTs to realize anonymous communications in the physical space and virtual spaces. However, existing pseudonym management methods fall short in meeting the extensive pseudonym demands in vehicular edge metaverses, thus dramatically diminishing the performance of privacy preservation. To this end, we present a cross-metaverse empowered dual pseudonym management framework. We utilize cross-chain technology to enhance management efficiency and data security for pseudonyms. Furthermore, we propose a metric to assess the privacy level and employ a Multi-Agent Deep Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym generating strategy. Numerical results demonstrate that our proposed schemes are high-efficiency and cost-effective, showcasing their promising applications in vehicular edge metaverses.","sentences":["Driven by the great advances in metaverse and edge computing technologies, vehicular edge metaverses are expected to disrupt the current paradigm of intelligent transportation systems.","As highly computerized avatars of Vehicular Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can provide valuable metaverse services to improve driving safety and on-board satisfaction for their VMUs throughout journeys.","To maintain uninterrupted metaverse experiences, VTs must be migrated among edge servers following the movements of vehicles.","This can raise concerns about privacy breaches during the dynamic communications among vehicular edge metaverses.","To address these concerns and safeguard location privacy, pseudonyms as temporary identifiers can be leveraged by both VMUs and VTs to realize anonymous communications in the physical space and virtual spaces.","However, existing pseudonym management methods fall short in meeting the extensive pseudonym demands in vehicular edge metaverses, thus dramatically diminishing the performance of privacy preservation.","To this end, we present a cross-metaverse empowered dual pseudonym management framework.","We utilize cross-chain technology to enhance management efficiency and data security for pseudonyms.","Furthermore, we propose a metric to assess the privacy level and employ a Multi-Agent Deep Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym generating strategy.","Numerical results demonstrate that our proposed schemes are high-efficiency and cost-effective, showcasing their promising applications in vehicular edge metaverses."],"url":"http://arxiv.org/abs/2403.15285v1","category":"cs.NI"}
{"created":"2024-03-22 15:26:05","title":"A story of viral co-infection, co-transmission and co-feeding in ticks: how to compute an invasion reproduction number","abstract":"With a single circulating vector-borne virus, the basic reproduction number incorporates contributions from tick-to-tick (co-feeding), tick-to-host and host-to-tick transmission routes. With two different circulating vector-borne viral strains, resident and invasive, and under the assumption that co-feeding is the only transmission route in a tick population, the invasion reproduction number depends on whether the model system of ordinary differential equations possesses the property of neutrality. We show that a simple model, with two populations of ticks infected with one strain, resident or invasive, and one population of co-infected ticks, does not have Alizon's neutrality property. We present model alternatives that are capable of representing the invasion potential of a novel strain by including populations of ticks dually infected with the same strain. The invasion reproduction number is analysed with the next-generation method and via numerical simulations.","sentences":["With a single circulating vector-borne virus, the basic reproduction number incorporates contributions from tick-to-tick (co-feeding), tick-to-host and host-to-tick transmission routes.","With two different circulating vector-borne viral strains, resident and invasive, and under the assumption that co-feeding is the only transmission route in a tick population, the invasion reproduction number depends on whether the model system of ordinary differential equations possesses the property of neutrality.","We show that a simple model, with two populations of ticks infected with one strain, resident or invasive, and one population of co-infected ticks, does not have Alizon's neutrality property.","We present model alternatives that are capable of representing the invasion potential of a novel strain by including populations of ticks dually infected with the same strain.","The invasion reproduction number is analysed with the next-generation method and via numerical simulations."],"url":"http://arxiv.org/abs/2403.15282v1","category":"q-bio.PE"}
{"created":"2024-03-22 15:23:19","title":"Measuring Gender and Racial Biases in Large Language Models","abstract":"In traditional decision making processes, social biases of human decision makers can lead to unequal economic outcomes for underrepresented social groups, such as women, racial or ethnic minorities. Recently, the increasing popularity of Large language model based artificial intelligence suggests a potential transition from human to AI based decision making. How would this impact the distributional outcomes across social groups? Here we investigate the gender and racial biases of OpenAIs GPT, a widely used LLM, in a high stakes decision making setting, specifically assessing entry level job candidates from diverse social groups. Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications. These biases may result in a 1 or 2 percentage point difference in hiring probabilities for otherwise similar candidates at a certain threshold and are consistent across various job positions and subsamples. Meanwhile, we also find stronger pro female and weaker anti black male patterns in democratic states. Our results demonstrate that this LLM based AI system has the potential to mitigate the gender bias, but it may not necessarily cure the racial bias. Further research is needed to comprehend the root causes of these outcomes and develop strategies to minimize the remaining biases in AI systems. As AI based decision making tools are increasingly employed across diverse domains, our findings underscore the necessity of understanding and addressing the potential unequal outcomes to ensure equitable outcomes across social groups.","sentences":["In traditional decision making processes, social biases of human decision makers can lead to unequal economic outcomes for underrepresented social groups, such as women, racial or ethnic minorities.","Recently, the increasing popularity of Large language model based artificial intelligence suggests a potential transition from human to AI based decision making.","How would this impact the distributional outcomes across social groups?","Here we investigate the gender and racial biases of OpenAIs GPT, a widely used LLM, in a high stakes decision making setting, specifically assessing entry level job candidates from diverse social groups.","Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications.","These biases may result in a 1 or 2 percentage point difference in hiring probabilities for otherwise similar candidates at a certain threshold and are consistent across various job positions and subsamples.","Meanwhile, we also find stronger pro female and weaker anti black male patterns in democratic states.","Our results demonstrate that this LLM based AI system has the potential to mitigate the gender bias, but it may not necessarily cure the racial bias.","Further research is needed to comprehend the root causes of these outcomes and develop strategies to minimize the remaining biases in AI systems.","As AI based decision making tools are increasingly employed across diverse domains, our findings underscore the necessity of understanding and addressing the potential unequal outcomes to ensure equitable outcomes across social groups."],"url":"http://arxiv.org/abs/2403.15281v1","category":"econ.GN"}
{"created":"2024-03-22 15:21:07","title":"Specifying Genericity through Inclusiveness and Abstractness Continuous Scales","abstract":"This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases' (NPs) genericity in natural language. The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks. Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory. Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research. To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework's effectiveness in capturing nuanced aspects of genericity. Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on the semantics of genericity, and NLP practitioners, contributing to the development of commonsense knowledge repositories valuable in enhancing various NLP applications.","sentences":["This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases' (NPs) genericity in natural language.","The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks.","Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory.","Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research.","To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework's effectiveness in capturing nuanced aspects of genericity.","Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on the semantics of genericity, and NLP practitioners, contributing to the development of commonsense knowledge repositories valuable in enhancing various NLP applications."],"url":"http://arxiv.org/abs/2403.15278v1","category":"cs.CL"}
{"created":"2024-03-22 15:16:23","title":"Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review","abstract":"The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development.","sentences":["The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines.","We surveyed the applications of ChatGPT in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education.","Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development."],"url":"http://arxiv.org/abs/2403.15274v1","category":"q-bio.OT"}
{"created":"2024-03-22 15:16:10","title":"Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs","abstract":"Event temporal relation (TempRel) is a primary subject of the event relation extraction task. However, the inherent ambiguity of TempRel increases the difficulty of the task. With the rise of prompt engineering, it is important to design effective prompt templates and verbalizers to extract relevant knowledge. The traditional manually designed templates struggle to extract precise temporal knowledge. This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from large language models (LLMs) to enhance prompt templates and verbalizers. Our method capitalizes on the diverse capabilities of various LLMs to generate a wide array of ideas for template and verbalizer design. Our proposed method fully exploits the potential of LLMs for generation tasks and contributes more knowledge to our design. Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our method in improving the performance of event temporal relation extraction tasks.","sentences":["Event temporal relation (TempRel) is a primary subject of the event relation extraction task.","However, the inherent ambiguity of TempRel increases the difficulty of the task.","With the rise of prompt engineering, it is important to design effective prompt templates and verbalizers to extract relevant knowledge.","The traditional manually designed templates struggle to extract precise temporal knowledge.","This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from large language models (LLMs) to enhance prompt templates and verbalizers.","Our method capitalizes on the diverse capabilities of various LLMs to generate a wide array of ideas for template and verbalizer design.","Our proposed method fully exploits the potential of LLMs for generation tasks and contributes more knowledge to our design.","Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our method in improving the performance of event temporal relation extraction tasks."],"url":"http://arxiv.org/abs/2403.15273v1","category":"cs.CL"}
{"created":"2024-03-22 15:15:44","title":"WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization","abstract":"Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for $\\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our weakly-supervised relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available.","sentences":["Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor.","While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views.","In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions.","This is realized with two stages.","In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information.","To ensure a stable learning process, we incorporate temporal information as input.","Furthermore, instead of optimizing SE(3), we opt for $\\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint.","In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose.","This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information.","We validate our approaches on two publicly available datasets, one outdoor and one indoor.","Our experimental results demonstrate that our weakly-supervised relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods.","We will make our code publicly available."],"url":"http://arxiv.org/abs/2403.15272v1","category":"cs.CV"}
{"created":"2024-03-22 15:15:28","title":"From Hardware Fingerprint to Access Token: Enhancing the Authentication on IoT Devices","abstract":"The proliferation of consumer IoT products in our daily lives has raised the need for secure device authentication and access control. Unfortunately, these resource-constrained devices typically use token-based authentication, which is vulnerable to token compromise attacks that allow attackers to impersonate the devices and perform malicious operations by stealing the access token. Using hardware fingerprints to secure their authentication is a promising way to mitigate these threats. However, once attackers have stolen some hardware fingerprints (e.g., via MitM attacks), they can bypass the hardware authentication by training a machine learning model to mimic fingerprints or reusing these fingerprints to craft forge requests.   In this paper, we present MCU-Token, a secure hardware fingerprinting framework for MCU-based IoT devices even if the cryptographic mechanisms (e.g., private keys) are compromised. MCU-Token can be easily integrated with various IoT devices by simply adding a short hardware fingerprint-based token to the existing payload. To prevent the reuse of this token, we propose a message mapping approach that binds the token to a specific request via generating the hardware fingerprints based on the request payload. To defeat the machine learning attacks, we mix the valid fingerprints with poisoning data so that attackers cannot train a usable model with the leaked tokens. MCU-Token can defend against armored adversary who may replay, craft, and offload the requests via MitM or use both hardware (e.g., use identical devices) and software (e.g., machine learning attacks) strategies to mimic the fingerprints. The system evaluation shows that MCU-Token can achieve high accuracy (over 97%) with a low overhead across various IoT devices and application scenarios.","sentences":["The proliferation of consumer IoT products in our daily lives has raised the need for secure device authentication and access control.","Unfortunately, these resource-constrained devices typically use token-based authentication, which is vulnerable to token compromise attacks that allow attackers to impersonate the devices and perform malicious operations by stealing the access token.","Using hardware fingerprints to secure their authentication is a promising way to mitigate these threats.","However, once attackers have stolen some hardware fingerprints (e.g., via MitM attacks), they can bypass the hardware authentication by training a machine learning model to mimic fingerprints or reusing these fingerprints to craft forge requests.   ","In this paper, we present MCU-Token, a secure hardware fingerprinting framework for MCU-based IoT devices even if the cryptographic mechanisms (e.g., private keys) are compromised.","MCU-Token can be easily integrated with various IoT devices by simply adding a short hardware fingerprint-based token to the existing payload.","To prevent the reuse of this token, we propose a message mapping approach that binds the token to a specific request via generating the hardware fingerprints based on the request payload.","To defeat the machine learning attacks, we mix the valid fingerprints with poisoning data so that attackers cannot train a usable model with the leaked tokens.","MCU-Token can defend against armored adversary who may replay, craft, and offload the requests via MitM or use both hardware (e.g., use identical devices) and software (e.g., machine learning attacks) strategies to mimic the fingerprints.","The system evaluation shows that MCU-Token can achieve high accuracy (over 97%) with a low overhead across various IoT devices and application scenarios."],"url":"http://arxiv.org/abs/2403.15271v1","category":"cs.CR"}
{"created":"2024-03-22 15:10:58","title":"Bipartite Sachdev-Ye Models with Read-Saleur Symmetries","abstract":"We introduce an SU(M)-symmetric disordered bipartite spin model with unusual characteristics. Although superficially similar to the Sachdev-Ye model, it has several markedly different properties for M>2. In particular, it has a large non-trivial nullspace whose dimension grows exponentially with system size. The states in this nullspace are frustration-free, and are ground states when the interactions are ferromagnetic. The exponential growth of the nullspace leads to Hilbert-space fragmentation and a violation of the eigenstate thermalization hypothesis. We demonstrate that the commutant algebra responsible for this fragmentation is a non-trivial subalgebra of the Read-Saleur commutant algebra of certain nearest-neighbour models such as the spin-1 biquadratic spin chain. We also discuss the low-energy behaviour of correlations for the disordered version of this model in the limit of a large number of spins and large M, using techniques similar to those applied to the SY model. We conclude by generalizing the Shiraishi-Mori embedding formalism to non-local models, and apply it to turn some of our nullspace states into quantum many-body scars.","sentences":["We introduce an SU(M)-symmetric disordered bipartite spin model with unusual characteristics.","Although superficially similar to the Sachdev-Ye model, it has several markedly different properties for M>2.","In particular, it has a large non-trivial nullspace whose dimension grows exponentially with system size.","The states in this nullspace are frustration-free, and are ground states when the interactions are ferromagnetic.","The exponential growth of the nullspace leads to Hilbert-space fragmentation and a violation of the eigenstate thermalization hypothesis.","We demonstrate that the commutant algebra responsible for this fragmentation is a non-trivial subalgebra of the Read-Saleur commutant algebra of certain nearest-neighbour models such as the spin-1 biquadratic spin chain.","We also discuss the low-energy behaviour of correlations for the disordered version of this model in the limit of a large number of spins and large M, using techniques similar to those applied to the SY model.","We conclude by generalizing the Shiraishi-Mori embedding formalism to non-local models, and apply it to turn some of our nullspace states into quantum many-body scars."],"url":"http://arxiv.org/abs/2403.15270v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-22 15:06:45","title":"Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models","abstract":"Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a short dummy document with long context compress and implicit imagination with HyperNetwork for generating adapter weights. Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in both in-distribution performance and out-of-distribution generalizations. Our code will be available at https://github.com/Xnhyacinth/IAG.","sentences":["Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs).","However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption.","Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated.","Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources.","Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a short dummy document with long context compress and implicit imagination with HyperNetwork for generating adapter weights.","Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in both in-distribution performance and out-of-distribution generalizations.","Our code will be available at https://github.com/Xnhyacinth/IAG."],"url":"http://arxiv.org/abs/2403.15268v1","category":"cs.CL"}
{"created":"2024-03-22 15:06:31","title":"Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies","abstract":"Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science. In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs. In particular, deep reinforcement learning (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications. Most DRL methods rely on deep neural network (DNN) control policies. However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability. In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs. Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradient and actor-critic DRL algorithms without changing their policy-optimization procedure. We test our approach on the challenging tasks of controlling parametric Kuramoto-Sivashinsky and convection-diffusion-reaction PDEs. We show that our method (1) outperforms baseline DNN-based DRL policies, (2) allows for the derivation of interpretable equations of the learned optimal control laws, and (3) generalizes to unseen parameters of the PDE without retraining the policies.","sentences":["Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science.","In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs.","In particular, deep reinforcement learning (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications.","Most DRL methods rely on deep neural network (DNN) control policies.","However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability.","In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs.","Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradient and actor-critic DRL algorithms without changing their policy-optimization procedure.","We test our approach on the challenging tasks of controlling parametric Kuramoto-Sivashinsky and convection-diffusion-reaction PDEs.","We show that our method (1) outperforms baseline DNN-based DRL policies, (2) allows for the derivation of interpretable equations of the learned optimal control laws, and (3) generalizes to unseen parameters of the PDE without retraining the policies."],"url":"http://arxiv.org/abs/2403.15267v1","category":"cs.LG"}
{"created":"2024-03-22 15:06:06","title":"Graph neural network coarse-grain force field for the molecular crystal RDX","abstract":"Condense phase molecular systems organize in wide range of distinct molecular configurations, including amorphous melt and glass as well as crystals often exhibiting polymorphism, that originate from their intricate intra- and intermolecular forces. While accurate coarse-grain (CG) models for these materials are critical to understand phenomena beyond the reach of all-atom simulations, current models cannot capture the diversity of molecular structures. We introduce a generally applicable approach to develop CG force fields for molecular crystals combining graph neural networks (GNN) and data from an all-atom simulations and apply it to the high-energy density material RDX. We address the challenge of expanding the training data with relevant configurations via an iterative procedure that performs CG molecular dynamics of processes of interest and reconstructs the atomistic configurations using a pre-trained neural network decoder. The multi-site CG model uses a GNN architecture constructed to satisfy translational invariance and rotational covariance for forces. The resulting model captures both crystalline and amorphous states for a wide range of temperatures and densities.","sentences":["Condense phase molecular systems organize in wide range of distinct molecular configurations, including amorphous melt and glass as well as crystals often exhibiting polymorphism, that originate from their intricate intra- and intermolecular forces.","While accurate coarse-grain (CG) models for these materials are critical to understand phenomena beyond the reach of all-atom simulations, current models cannot capture the diversity of molecular structures.","We introduce a generally applicable approach to develop CG force fields for molecular crystals combining graph neural networks (GNN) and data from an all-atom simulations and apply it to the high-energy density material RDX.","We address the challenge of expanding the training data with relevant configurations via an iterative procedure that performs CG molecular dynamics of processes of interest and reconstructs the atomistic configurations using a pre-trained neural network decoder.","The multi-site CG model uses a GNN architecture constructed to satisfy translational invariance and rotational covariance for forces.","The resulting model captures both crystalline and amorphous states for a wide range of temperatures and densities."],"url":"http://arxiv.org/abs/2403.15266v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 15:00:42","title":"AI Exposure and Strategic Positioning on an Online Work Platform","abstract":"AI technologies have the potential to affect labor market outcomes by both increasing worker productivity and reducing the demand for certain skills or tasks. Such changes may have important implications for the ways in which workers seek jobs and position themselves. In this project, we examine how exposure to generative AI technologies affects the strategic behavior of freelancer workers on an online work platform following the launch of ChatGPT in December 2022. Relative to their less exposed counterparts, we show that freelancers that are more exposed to language modeling technologies apply for more job posts on the platform following the launch of ChatGPT and increase the concentration of these applications across specializations. We document heterogeneity in this effect across freelancer characteristics and consider how such behaviors shape whether and to what extent the technological shock affects freelancer performance on the platform.","sentences":["AI technologies have the potential to affect labor market outcomes by both increasing worker productivity and reducing the demand for certain skills or tasks.","Such changes may have important implications for the ways in which workers seek jobs and position themselves.","In this project, we examine how exposure to generative AI technologies affects the strategic behavior of freelancer workers on an online work platform following the launch of ChatGPT in December 2022.","Relative to their less exposed counterparts, we show that freelancers that are more exposed to language modeling technologies apply for more job posts on the platform following the launch of ChatGPT and increase the concentration of these applications across specializations.","We document heterogeneity in this effect across freelancer characteristics and consider how such behaviors shape whether and to what extent the technological shock affects freelancer performance on the platform."],"url":"http://arxiv.org/abs/2403.15262v1","category":"econ.GN"}
{"created":"2024-03-22 15:00:36","title":"Crossing lemmas for $k$-systems of arcs","abstract":"We show a generalization of the crossing lemma for multi-graphs drawn on orientable surfaces in which pairs of edges are assumed to be drawn by non-homotopic simple arcs which pairwise cross at most $k$ times.","sentences":["We show a generalization of the crossing lemma for multi-graphs drawn on orientable surfaces in which pairs of edges are assumed to be drawn by non-homotopic simple arcs which pairwise cross at most $k$ times."],"url":"http://arxiv.org/abs/2403.15261v1","category":"math.CO"}
{"created":"2024-03-22 14:57:27","title":"Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks","abstract":"Understanding information cascades in networks is a fundamental issue in numerous applications. Current researches often sample cascade information into several independent paths or subgraphs to learn a simple cascade representation. However, these approaches fail to exploit the hierarchical semantic associations between different modalities, limiting their predictive performance. In this work, we propose a novel Hierarchical Information Enhancement Network (HIENet) for cascade prediction. Our approach integrates fundamental cascade sequence, user social graphs, and sub-cascade graph into a unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades information into a series of sequences. It then gathers path information between users to extract the social relationships of propagators. Additionally, we employ a time-stamped graph convolutional network to aggregate sub-cascade graph information effectively. Ultimately, we introduce a Multi-modal Cascade Transformer to powerfully fuse these clues, providing a comprehensive understanding of cascading process. Extensive experiments have demonstrated the effectiveness of the proposed method.","sentences":["Understanding information cascades in networks is a fundamental issue in numerous applications.","Current researches often sample cascade information into several independent paths or subgraphs to learn a simple cascade representation.","However, these approaches fail to exploit the hierarchical semantic associations between different modalities, limiting their predictive performance.","In this work, we propose a novel Hierarchical Information Enhancement Network (HIENet) for cascade prediction.","Our approach integrates fundamental cascade sequence, user social graphs, and sub-cascade graph into a unified framework.","Specifically, HIENet utilizes DeepWalk to sample cascades information into a series of sequences.","It then gathers path information between users to extract the social relationships of propagators.","Additionally, we employ a time-stamped graph convolutional network to aggregate sub-cascade graph information effectively.","Ultimately, we introduce a Multi-modal Cascade Transformer to powerfully fuse these clues, providing a comprehensive understanding of cascading process.","Extensive experiments have demonstrated the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.15257v1","category":"cs.SI"}
{"created":"2024-03-22 14:55:58","title":"Non-traditional Cartan subalgebras in twisted groupoid C*-algebras","abstract":"Well-known work of Renault shows that if $\\mathcal{E}$ is a twist over a second countable, effective, \\'etale groupoid $G$, then there is a naturally associated Cartan subalgebra of the reduced twisted groupoid C*-algebra $C^*_{r}(G;\\mathcal{E})$, and that every Cartan subalgebra of a separable C*-algebra arises in this way. However twists over non-effective groupoids can also possess Cartan subalgebras. For twists over \\'etale groupoids built from continuous 2-cocycles, this was shown in work by the first author with Gillaspy, Norton, Reznikoff, and Wright. In this paper, we extend those results to general twists over \\'etale groupoids. In particular, we prove that certain clopen subgroupoids of locally compact, Hausdorff, \\'etale groupoids give rise to Cartan subalgebras in the twisted groupoid C*-algebra.","sentences":["Well-known work of Renault shows that if $\\mathcal{E}$ is a twist over a second countable, effective, \\'etale groupoid $G$, then there is a naturally associated Cartan subalgebra of the reduced twisted groupoid C*-algebra $C^*_{r}(G;\\mathcal{E})$, and that every Cartan subalgebra of a separable C*-algebra arises in this way.","However twists over non-effective groupoids can also possess Cartan subalgebras.","For twists over \\'etale groupoids built from continuous 2-cocycles, this was shown in work by the first author with Gillaspy, Norton, Reznikoff, and Wright.","In this paper, we extend those results to general twists over \\'etale groupoids.","In particular, we prove that certain clopen subgroupoids of locally compact, Hausdorff, \\'etale groupoids give rise to Cartan subalgebras in the twisted groupoid C*-algebra."],"url":"http://arxiv.org/abs/2403.15255v1","category":"math.OA"}
{"created":"2024-03-22 14:53:00","title":"Comparison of the disk precession models with the photometric behavior of TT Ari in 2021-2023","abstract":"We present a comparative analysis of photometric observations of the cataclysmic variable TT Ari in its bright state, obtained by the TESS orbital observatory in 2021 and 2023 and by ground-based amateur telescopes in 2022. The light curves from 2021 and 2022 are dominated by modulations with a period slightly shorter than the orbital one (negative superhumps), 0.13293 and 0.13273$\\,$d respectively. In 2023, much stronger modulations appeared on a much longer time scale of a few days with an amplitude of up to 0.5 mag, compared to 0.2 mag in 2021. The negative superhump variability with the period of 0.13376$\\,$d was also found in the first half of the 2023 observations, but then negative superhumps suddenly almost disappeared. Less significant additional modulations with a period exceeding the orbital one (positive superhumps) were detected in 2021 and 2022. Their periods were 0.15106 and 0.1523$\\,$d, correspondingly. We also found a previously unnoticed periodic signal corresponding to the orbital period of 0.13755$\\,$d in the TESS observations in 2021. Theoretical models of tidal precession of an elliptical disk predict a decrease in the precession period with increasing disk radius, which is consistent with the observed photometric behavior of the system. It enables us to estimate the mass ratio of the components in TT$\\,$Ari to be $q\\approx 0.235$. The tilted disk precession model predicts a period of nodal precession whose value is in general agreement with observations.","sentences":["We present a comparative analysis of photometric observations of the cataclysmic variable TT Ari in its bright state, obtained by the TESS orbital observatory in 2021 and 2023 and by ground-based amateur telescopes in 2022.","The light curves from 2021 and 2022 are dominated by modulations with a period slightly shorter than the orbital one (negative superhumps), 0.13293 and 0.13273$\\,$d respectively.","In 2023, much stronger modulations appeared on a much longer time scale of a few days with an amplitude of up to 0.5 mag, compared to 0.2 mag in 2021.","The negative superhump variability with the period of 0.13376$\\,$d was also found in the first half of the 2023 observations, but then negative superhumps suddenly almost disappeared.","Less significant additional modulations with a period exceeding the orbital one (positive superhumps) were detected in 2021 and 2022.","Their periods were 0.15106 and 0.1523$\\,$d, correspondingly.","We also found a previously unnoticed periodic signal corresponding to the orbital period of 0.13755$\\,$d in the TESS observations in 2021.","Theoretical models of tidal precession of an elliptical disk predict a decrease in the precession period with increasing disk radius, which is consistent with the observed photometric behavior of the system.","It enables us to estimate the mass ratio of the components in TT$\\,$Ari to be $q\\approx 0.235$.","The tilted disk precession model predicts a period of nodal precession whose value is in general agreement with observations."],"url":"http://arxiv.org/abs/2403.15254v1","category":"astro-ph.SR"}
{"created":"2024-03-22 14:52:17","title":"Stability of abstract coupled systems","abstract":"We study stability of abstract differential equations coupled by means of a general algebraic condition. Our approach is based on techniques from operator theory and systems theory, and it allows us to study coupled systems by exploiting properties of the components, which are typically much simpler to analyse. As our main results we establish resolvent estimates and decay rates for abstract boundary-coupled systems. We illustrate the power of the general results by using them to obtain rates of energy decay in coupled systems of one-dimensional wave and heat equations, and in a wave equation with an acoustic boundary condition.","sentences":["We study stability of abstract differential equations coupled by means of a general algebraic condition.","Our approach is based on techniques from operator theory and systems theory, and it allows us to study coupled systems by exploiting properties of the components, which are typically much simpler to analyse.","As our main results we establish resolvent estimates and decay rates for abstract boundary-coupled systems.","We illustrate the power of the general results by using them to obtain rates of energy decay in coupled systems of one-dimensional wave and heat equations, and in a wave equation with an acoustic boundary condition."],"url":"http://arxiv.org/abs/2403.15253v1","category":"math.FA"}
{"created":"2024-03-22 14:49:49","title":"Safe Learning of PDDL Domains with Conditional Effects -- Extended Version","abstract":"Powerful domain-independent planners have been developed to solve various types of planning problems. These planners often require a model of the acting agent's actions, given in some planning domain description language. Manually designing such an action model is a notoriously challenging task. An alternative is to automatically learn action models from observation. Such an action model is called safe if every plan created with it is consistent with the real, unknown action model. Algorithms for learning such safe action models exist, yet they cannot handle domains with conditional or universal effects, which are common constructs in many planning problems. We prove that learning non-trivial safe action models with conditional effects may require an exponential number of samples. Then, we identify reasonable assumptions under which such learning is tractable and propose SAM Learning of Conditional Effects (Conditional-SAM), the first algorithm capable of doing so. We analyze Conditional-SAM theoretically and evaluate it experimentally. Our results show that the action models learned by Conditional-SAM can be used to solve perfectly most of the test set problems in most of the experimented domains.","sentences":["Powerful domain-independent planners have been developed to solve various types of planning problems.","These planners often require a model of the acting agent's actions, given in some planning domain description language.","Manually designing such an action model is a notoriously challenging task.","An alternative is to automatically learn action models from observation.","Such an action model is called safe if every plan created with it is consistent with the real, unknown action model.","Algorithms for learning such safe action models exist, yet they cannot handle domains with conditional or universal effects, which are common constructs in many planning problems.","We prove that learning non-trivial safe action models with conditional effects may require an exponential number of samples.","Then, we identify reasonable assumptions under which such learning is tractable and propose SAM Learning of Conditional Effects (Conditional-SAM), the first algorithm capable of doing so.","We analyze Conditional-SAM theoretically and evaluate it experimentally.","Our results show that the action models learned by Conditional-SAM can be used to solve perfectly most of the test set problems in most of the experimented domains."],"url":"http://arxiv.org/abs/2403.15251v1","category":"cs.AI"}
{"created":"2024-03-22 14:47:35","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach","abstract":"Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering a robust and transparent approach to deciphering LLM performance data. Contrary to prevailing findings, our results challenge assumptions about emergent abilities and the influence of given training types and architectures in LLMs. These findings furnish new perspectives on the characteristics, intrinsic nature, and developmental trajectories of LLMs. By providing straightforward and reliable methods to scrutinize and reassess LLM performance data, this study contributes a nuanced perspective on LLM efficiency and potentials.","sentences":["Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount.","Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs.","However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points.","Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens.","Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods.","With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology.","This includes the application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering a robust and transparent approach to deciphering LLM performance data.","Contrary to prevailing findings, our results challenge assumptions about emergent abilities and the influence of given training types and architectures in LLMs.","These findings furnish new perspectives on the characteristics, intrinsic nature, and developmental trajectories of LLMs.","By providing straightforward and reliable methods to scrutinize and reassess LLM performance data, this study contributes a nuanced perspective on LLM efficiency and potentials."],"url":"http://arxiv.org/abs/2403.15250v1","category":"cs.CL"}
{"created":"2024-03-22 14:47:18","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion Models","abstract":"The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.","sentences":["The evolution of diffusion models has greatly impacted video generation and understanding.","Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc.","Despite these advances, challenges persist in accurately distilling motion information from video frames.","While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions.","To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms.","SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts.","Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks."],"url":"http://arxiv.org/abs/2403.15249v1","category":"cs.CV"}
{"created":"2024-03-22 14:46:51","title":"Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks","abstract":"Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry. Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets. This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive. The lack of efficient labeling approaches inspired us to consider self-supervised learning as a paradigm shift, learning meaningful feature representations from raw agricultural image data. In this work, we explore how self-supervised representation learning unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets. We propose a lightweight framework utilizing SimCLR, a contrastive learning approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agriculture field images. Our experimental analysis and results indicate that the model learns robust features applicable to a broad range of downstream agriculture tasks discussed in the paper. Additionally, the reduced reliance on annotated data makes our approach more cost-effective and accessible, paving the way for broader adoption of computer vision in agriculture.","sentences":["Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry.","Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets.","This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive.","The lack of efficient labeling approaches inspired us to consider self-supervised learning as a paradigm shift, learning meaningful feature representations from raw agricultural image data.","In this work, we explore how self-supervised representation learning unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets.","We propose a lightweight framework utilizing SimCLR, a contrastive learning approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agriculture field images.","Our experimental analysis and results indicate that the model learns robust features applicable to a broad range of downstream agriculture tasks discussed in the paper.","Additionally, the reduced reliance on annotated data makes our approach more cost-effective and accessible, paving the way for broader adoption of computer vision in agriculture."],"url":"http://arxiv.org/abs/2403.15248v1","category":"cs.CV"}
{"created":"2024-03-22 14:41:55","title":"Reasoning-Enhanced Object-Centric Learning for Videos","abstract":"Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatiotemporal attention computations and fusion. Our experiment results on various datasets show that STATM can significantly enhance object-centric learning capabilities of slot-based video models.","sentences":["Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world.","Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module.","In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics.","Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes.","The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatiotemporal attention computations and fusion.","Our experiment results on various datasets show that STATM can significantly enhance object-centric learning capabilities of slot-based video models."],"url":"http://arxiv.org/abs/2403.15245v1","category":"cs.CV"}
{"created":"2024-03-22 14:40:29","title":"A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness","abstract":"Classical convergence analyses for optimization algorithms rely on the widely-adopted uniform smoothness assumption. However, recent experimental studies have demonstrated that many machine learning problems exhibit non-uniform smoothness, meaning the smoothness factor is a function of the model parameter instead of a universal constant. In particular, it has been observed that the smoothness grows with respect to the gradient norm along the training trajectory. Motivated by this phenomenon, the recently introduced $(L_0, L_1)$-smoothness is a more general notion, compared to traditional $L$-smoothness, that captures such positive relationship between smoothness and gradient norm. Under this type of non-uniform smoothness, existing literature has designed stochastic first-order algorithms by utilizing gradient clipping techniques to obtain the optimal $\\mathcal{O}(\\epsilon^{-3})$ sample complexity for finding an $\\epsilon$-approximate first-order stationary solution. Nevertheless, the studies of quasi-Newton methods are still lacking. Considering higher accuracy and more robustness for quasi-Newton methods, in this paper we propose a fast stochastic quasi-Newton method when there exists non-uniformity in smoothness. Leveraging gradient clipping and variance reduction, our algorithm can achieve the best-known $\\mathcal{O}(\\epsilon^{-3})$ sample complexity and enjoys convergence speedup with simple hyperparameter tuning. Our numerical experiments show that our proposed algorithm outperforms the state-of-the-art approaches.","sentences":["Classical convergence analyses for optimization algorithms rely on the widely-adopted uniform smoothness assumption.","However, recent experimental studies have demonstrated that many machine learning problems exhibit non-uniform smoothness, meaning the smoothness factor is a function of the model parameter instead of a universal constant.","In particular, it has been observed that the smoothness grows with respect to the gradient norm along the training trajectory.","Motivated by this phenomenon, the recently introduced $(L_0, L_1)$-smoothness is a more general notion, compared to traditional $L$-smoothness, that captures such positive relationship between smoothness and gradient norm.","Under this type of non-uniform smoothness, existing literature has designed stochastic first-order algorithms by utilizing gradient clipping techniques to obtain the optimal $\\mathcal{O}(\\epsilon^{-3})$ sample complexity for finding an $\\epsilon$-approximate first-order stationary solution.","Nevertheless, the studies of quasi-Newton methods are still lacking.","Considering higher accuracy and more robustness for quasi-Newton methods, in this paper we propose a fast stochastic quasi-Newton method when there exists non-uniformity in smoothness.","Leveraging gradient clipping and variance reduction, our algorithm can achieve the best-known $\\mathcal{O}(\\epsilon^{-3})$ sample complexity and enjoys convergence speedup with simple hyperparameter tuning.","Our numerical experiments show that our proposed algorithm outperforms the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.15244v1","category":"cs.LG"}
{"created":"2024-03-22 14:36:39","title":"Robust Utility Optimization via a GAN Approach","abstract":"Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a generative adversarial network (GAN) approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones. Lastly, we uncover that our generative approach for learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings.","sentences":["Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome.","In this work, we propose a generative adversarial network (GAN) approach to (approximately) solve robust utility optimization problems in general and realistic settings.","In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game.","This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used.","A large empirical study shows the versatile usability of our method.","Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies.","Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones.","Lastly, we uncover that our generative approach for learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings."],"url":"http://arxiv.org/abs/2403.15243v1","category":"q-fin.CP"}
{"created":"2024-03-22 14:32:27","title":"Guided Decoding for Robot Motion Generation and Adaption","abstract":"We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   We train a transformer architecture on a large dataset of simulated trajectories. This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it can adapt trajectories in navigating complex tasks, including obstacle avoidance, via points, and meeting velocity and acceleration constraints, across platforms.","sentences":["We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc.","A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process.","This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   ","We train a transformer architecture on a large dataset of simulated trajectories.","This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints.","Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation.","We show that our model can generate motion from initial and target points, but also that it can adapt trajectories in navigating complex tasks, including obstacle avoidance, via points, and meeting velocity and acceleration constraints, across platforms."],"url":"http://arxiv.org/abs/2403.15239v1","category":"cs.RO"}
{"created":"2024-03-22 14:29:03","title":"Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks","abstract":"Identifying key nodes in social networks plays a crucial role in timely blocking false information. Existing key node identification methods usually consider node influence only from the propagation structure perspective and have insufficient generalization ability to unknown scenarios. In this paper, we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for identifying key nodes in social networks, which mines key nodes from multiple perspectives and utilizes memory networks to store historical information. Specifically, MMEN first constructs two propagation networks from the perspectives of user attributes and propagation structure and updates node feature representations using graph attention networks. Meanwhile, the memory network is employed to store information of similar subgraphs, enhancing the model's generalization performance in unknown scenarios. Finally, MMEN applies adaptive weights to combine the node influence of the two propagation networks to select the ultimate key nodes. Extensive experiments demonstrate that our method significantly outperforms previous methods.","sentences":["Identifying key nodes in social networks plays a crucial role in timely blocking false information.","Existing key node identification methods usually consider node influence only from the propagation structure perspective and have insufficient generalization ability to unknown scenarios.","In this paper, we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for identifying key nodes in social networks, which mines key nodes from multiple perspectives and utilizes memory networks to store historical information.","Specifically, MMEN first constructs two propagation networks from the perspectives of user attributes and propagation structure and updates node feature representations using graph attention networks.","Meanwhile, the memory network is employed to store information of similar subgraphs, enhancing the model's generalization performance in unknown scenarios.","Finally, MMEN applies adaptive weights to combine the node influence of the two propagation networks to select the ultimate key nodes.","Extensive experiments demonstrate that our method significantly outperforms previous methods."],"url":"http://arxiv.org/abs/2403.15235v1","category":"cs.SI"}
{"created":"2024-03-22 14:27:58","title":"Shadow Generation for Composite Image Using Diffusion model","abstract":"In the realm of image composition, generating realistic shadow for the inserted foreground remains a formidable challenge. Previous works have developed image-to-image translation models which are trained on paired training data. However, they are struggling to generate shadows with accurate shapes and intensities, hindered by data scarcity and inherent task complexity. In this paper, we resort to foundation model with rich prior knowledge of natural shadow images. Specifically, we first adapt ControlNet to our task and then propose intensity modulation modules to improve the shadow intensity. Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel data acquisition pipeline. Experimental results on both DESOBA and DESOBAv2 datasets as well as real composite images demonstrate the superior capability of our model for shadow generation task. The dataset, code, and model are released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.","sentences":["In the realm of image composition, generating realistic shadow for the inserted foreground remains a formidable challenge.","Previous works have developed image-to-image translation models which are trained on paired training data.","However, they are struggling to generate shadows with accurate shapes and intensities, hindered by data scarcity and inherent task complexity.","In this paper, we resort to foundation model with rich prior knowledge of natural shadow images.","Specifically, we first adapt ControlNet to our task and then propose intensity modulation modules to improve the shadow intensity.","Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel data acquisition pipeline.","Experimental results on both DESOBA and DESOBAv2 datasets as well as real composite images demonstrate the superior capability of our model for shadow generation task.","The dataset, code, and model are released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2."],"url":"http://arxiv.org/abs/2403.15234v1","category":"cs.CV"}
{"created":"2024-03-22 14:24:02","title":"Pushforward of structure sheaf and virtual global generation","abstract":"Let $f:X\\rightarrow Y$ be a generically smooth morphism between irreducible smooth projective curves over an algebraically closed field of arbitrary characteristic. We prove that the vector bundle $((f_*{\\mathcal O}_X)/{\\mathcal O}_Y)^*$ is virtually globally generated. Moreover, $((f_*{\\mathcal O}_X)/{\\mathcal O}_Y)^*$ is ample if and only if $f$ is genuinely ramified.","sentences":["Let $f:","X\\rightarrow Y$ be a generically smooth morphism between irreducible smooth projective curves over an algebraically closed field of arbitrary characteristic.","We prove that the vector bundle $((f_*{\\mathcal O}_X)/{\\mathcal O}_Y)^*$ is virtually globally generated.","Moreover, $((f_*{\\mathcal O}_X)/{\\mathcal O}_Y)^*$ is ample if and only if $f$ is genuinely ramified."],"url":"http://arxiv.org/abs/2403.15231v1","category":"math.AG"}
{"created":"2024-03-22 14:22:30","title":"Spin Hall Effect: Symmetry Breaking, Twisting, and Giant Disorder Renormalization","abstract":"Atomically-thin materials based on transition metal dichalcogenides and graphene offer a promising avenue for unlocking the mechanisms underlying the spin Hall effect (SHE) in heterointerfaces. Here, we develop a microscopic theory of the SHE for twisted van der Waals heterostructures that fully incorporates twisting and disorder effects, and illustrate the critical role of symmetry breaking in the generation of spin-Hall currents. We find that an accurate treatment of vertex corrections leads to a qualitatively and quantitatively different SHE than that obtained from popular approaches like the ``$i\\,\\eta$'' and ladder approximations. A pronounced oscillatory behavior of skew-scattering processes with twist angle, $\\theta$, is predicted, reflecting a non-trivial interplay of Rashba and valley-Zeeman effects and yields a vanishing SHE for $\\theta = 30^\\circ$ and, for graphene-WSe$_2$, an optimal SHE for $\\theta \\approx 17^\\circ$. Our findings reveal disorder and broken symmetries as important knobs to optimize interfacial SHEs.","sentences":["Atomically-thin materials based on transition metal dichalcogenides and graphene offer a promising avenue for unlocking the mechanisms underlying the spin Hall effect (SHE) in heterointerfaces.","Here, we develop a microscopic theory of the SHE for twisted van der Waals heterostructures that fully incorporates twisting and disorder effects, and illustrate the critical role of symmetry breaking in the generation of spin-Hall currents.","We find that an accurate treatment of vertex corrections leads to a qualitatively and quantitatively different SHE than that obtained from popular approaches like the ``$i\\,\\eta$'' and ladder approximations.","A pronounced oscillatory behavior of skew-scattering processes with twist angle, $\\theta$, is predicted, reflecting a non-trivial interplay of Rashba and valley-Zeeman effects and yields a vanishing SHE for $\\theta = 30^\\circ$ and, for graphene-WSe$_2$, an optimal SHE for $\\theta \\approx 17^\\circ$. Our findings reveal disorder and broken symmetries as important knobs to optimize interfacial SHEs."],"url":"http://arxiv.org/abs/2403.15229v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 14:20:54","title":"LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example","abstract":"Recent advances in 3D face stylization have made significant strides in few to zero-shot settings. However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations. To this end, we propose a method that can produce a highly stylized 3D face model with desired topology. Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar. The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses. Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space. The resulting stylized face model can be animated by commonly used 3DMM blend shapes. A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology. We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars.","sentences":["Recent advances in 3D face stylization have made significant strides in few to zero-shot settings.","However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations.","To this end, we propose a method that can produce a highly stylized 3D face model with desired topology.","Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar.","The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses.","Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space.","The resulting stylized face model can be animated by commonly used 3DMM blend shapes.","A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology.","We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars."],"url":"http://arxiv.org/abs/2403.15227v1","category":"cs.CV"}
{"created":"2024-03-22 14:19:48","title":"The Dissolution of Planetesimals in Electrostatic Fields","abstract":"Planetesimals or smaller bodies in protoplanetary disks are often considered to form as pebble piles in current planet formation models. They are supposed to be large but loose, weakly bound clusters of more robust dust aggregates. This makes them easy prey for destructive processes. In microgravity experiments, we apply strong electric fields on clusters of slightly conductive dust aggregates. We find that this generates enough tensile stress on the fragile clusters to sequentially rip off the aggregates from the cluster. These experiments imply that electric fields in protoplanetary disks can dissolve pebble pile planetesimals. This process might induce a bias for the local planetesimal reservoir in regions with strong fields. Planetesimals prevail with certain kinds of compositions where they are either good isolators or compacted bodies. The less lucky ones generate pebble clouds which might be observable as signposts of electrostatic activity in protoplanetary disks.","sentences":["Planetesimals or smaller bodies in protoplanetary disks are often considered to form as pebble piles in current planet formation models.","They are supposed to be large but loose, weakly bound clusters of more robust dust aggregates.","This makes them easy prey for destructive processes.","In microgravity experiments, we apply strong electric fields on clusters of slightly conductive dust aggregates.","We find that this generates enough tensile stress on the fragile clusters to sequentially rip off the aggregates from the cluster.","These experiments imply that electric fields in protoplanetary disks can dissolve pebble pile planetesimals.","This process might induce a bias for the local planetesimal reservoir in regions with strong fields.","Planetesimals prevail with certain kinds of compositions where they are either good isolators or compacted bodies.","The less lucky ones generate pebble clouds which might be observable as signposts of electrostatic activity in protoplanetary disks."],"url":"http://arxiv.org/abs/2403.15225v1","category":"astro-ph.EP"}
{"created":"2024-03-22 14:08:36","title":"Robust Microgrid Dispatch with Real-Time Energy Sharing and Endogenous Uncertainty","abstract":"With the rising adoption of distributed energy resources (DERs), microgrid dispatch is facing new challenges: DER owners are independent stakeholders seeking to maximize their individual profits rather than being controlled centrally; and the dispatch of renewable generators may affect the microgrid's exposure to uncertainty. To address these challenges, this paper proposes a two-stage robust microgrid dispatch model with real-time energy sharing and endogenous uncertainty. In the day-ahead stage, the connection/disconnection of renewable generators is optimized, which influences the size and dimension of the uncertainty set. As a result, the uncertainty set is endogenously given. In addition, non-anticipative operational bounds for energy storage (ES) are derived to enable the online operation of ES in real-time. In the real-time stage, DER owners (consumers and prosumers) share energy with each other via a proposed energy sharing mechanism, which forms a generalized Nash game. To solve the robust microgrid dispatch model, we develop an equivalent optimization model to compute the real-time energy sharing equilibrium. Based on this, a projection-based column-and-constraint generation (C&CG) method is proposed to handle the endogenous uncertainty. Numerical experiments show the effectiveness and advantages of the proposed model and method.","sentences":["With the rising adoption of distributed energy resources (DERs), microgrid dispatch is facing new challenges: DER owners are independent stakeholders seeking to maximize their individual profits rather than being controlled centrally; and the dispatch of renewable generators may affect the microgrid's exposure to uncertainty.","To address these challenges, this paper proposes a two-stage robust microgrid dispatch model with real-time energy sharing and endogenous uncertainty.","In the day-ahead stage, the connection/disconnection of renewable generators is optimized, which influences the size and dimension of the uncertainty set.","As a result, the uncertainty set is endogenously given.","In addition, non-anticipative operational bounds for energy storage (ES) are derived to enable the online operation of ES in real-time.","In the real-time stage, DER owners (consumers and prosumers) share energy with each other via a proposed energy sharing mechanism, which forms a generalized Nash game.","To solve the robust microgrid dispatch model, we develop an equivalent optimization model to compute the real-time energy sharing equilibrium.","Based on this, a projection-based column-and-constraint generation (C&CG) method is proposed to handle the endogenous uncertainty.","Numerical experiments show the effectiveness and advantages of the proposed model and method."],"url":"http://arxiv.org/abs/2403.15219v1","category":"math.OC"}
{"created":"2024-03-22 14:07:07","title":"Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations","abstract":"Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in \"narrowly\" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing \"sparse\" annotations from non-experts to generate \"dense\" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth annotations, nnU-Net models trained on SAM-generated annotations perform significantly worse than nnU-Net models trained on ground-truth annotations ($p<0.001$, all).","sentences":["Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in \"narrowly\" focused deep learning (DL) models with limited translational utility.","Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process.","However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models.","In this work, we explore the potential of SAM for crowd-sourcing \"sparse\" annotations from non-experts to generate \"dense\" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model.","Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth annotations, nnU-Net models trained on SAM-generated annotations perform significantly worse than nnU-Net models trained on ground-truth annotations ($p<0.001$, all)."],"url":"http://arxiv.org/abs/2403.15218v1","category":"cs.CV"}
{"created":"2024-03-22 14:03:37","title":"(Un)making AI Magic: a Design Taxonomy","abstract":"This paper examines the role that enchantment plays in the design of AI things by constructing a taxonomy of design approaches that increase or decrease the perception of magic and enchantment. We start from the design discourse surrounding recent developments in AI technologies, highlighting specific interaction qualities such as algorithmic uncertainties and errors and articulating relations to the rhetoric of magic and supernatural thinking. Through analyzing and reflecting upon 52 students' design projects from two editions of a Master course in design and AI, we identify seven design principles and unpack the effects of each in terms of enchantment and disenchantment. We conclude by articulating ways in which this taxonomy can be approached and appropriated by design/HCI practitioners, especially to support exploration and reflexivity.","sentences":["This paper examines the role that enchantment plays in the design of AI things by constructing a taxonomy of design approaches that increase or decrease the perception of magic and enchantment.","We start from the design discourse surrounding recent developments in AI technologies, highlighting specific interaction qualities such as algorithmic uncertainties and errors and articulating relations to the rhetoric of magic and supernatural thinking.","Through analyzing and reflecting upon 52 students' design projects from two editions of a Master course in design and AI, we identify seven design principles and unpack the effects of each in terms of enchantment and disenchantment.","We conclude by articulating ways in which this taxonomy can be approached and appropriated by design/HCI practitioners, especially to support exploration and reflexivity."],"url":"http://arxiv.org/abs/2403.15216v1","category":"cs.HC"}
{"created":"2024-03-22 13:58:42","title":"InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection","abstract":"Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media. This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored content online. We investigate the use of LLMs for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets. For this, we implement content-level and network-level metrics to assess whether synthetic captions are realistic. The second objective (utility) is to create synthetic data that is useful for sponsored content detection. For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram. Our investigations show that the objectives of fidelity and utility may conflict and that prompt engineering is a useful but insufficient strategy. Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity, and realistic user interaction patterns.","sentences":["Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media.","This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored content online.","We investigate the use of LLMs for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets.","For this, we implement content-level and network-level metrics to assess whether synthetic captions are realistic.","The second objective (utility) is to create synthetic data that is useful for sponsored content detection.","For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram.","Our investigations show that the objectives of fidelity and utility may conflict and that prompt engineering is a useful but insufficient strategy.","Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity, and realistic user interaction patterns."],"url":"http://arxiv.org/abs/2403.15214v1","category":"cs.CY"}
{"created":"2024-03-22 13:52:53","title":"Early Period of Training Impacts Out-of-Distribution Generalization","abstract":"Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to out-of-distribution (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual unfreezing -- has a minuscule impact on ID results, but greatly affects the generalization to OOD data; 2) the absolute values of sharpness and trace of Fisher Information at the initial period of training are not indicative for OOD generalization, but the relative values could be; 3) the trace of Fisher Information and sharpness may be used as indicators for the removal of interventions during early period of training for better OOD generalization.","sentences":["Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks.","However, neural networks are often sensitive to out-of-distribution (OOD) data, making them less reliable in downstream applications.","Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies.","In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training.","We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation.","Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual unfreezing -- has a minuscule impact on ID results, but greatly affects the generalization to OOD data; 2) the absolute values of sharpness and trace of Fisher Information at the initial period of training are not indicative for OOD generalization, but the relative values could be; 3) the trace of Fisher Information and sharpness may be used as indicators for the removal of interventions during early period of training for better OOD generalization."],"url":"http://arxiv.org/abs/2403.15210v1","category":"cs.LG"}
{"created":"2024-03-22 13:50:27","title":"MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection","abstract":"Multispectral pedestrian detection is attractive for around-the-clock applications due to the complementary information between RGB and thermal modalities. However, current models often fail to detect pedestrians in obvious cases, especially due to the modality bias learned from statistically biased datasets. From these problems, we anticipate that maybe understanding the complementary information itself is difficult to achieve from vision-only models. Accordingly, we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework, which incorporates Large Language Models (LLMs) to understand the complementary information at the semantic level and further enhance the fusion process. Specifically, we generate text descriptions of the pedestrian in each RGB and thermal modality and design a Multispectral Chain-of-Thought (MSCoT) prompting, which models a step-by-step process to facilitate cross-modal reasoning at the semantic level and perform accurate detection. Moreover, we design a Language-driven Multi-modal Fusion (LMF) strategy that enables fusing vision-driven and language-driven detections. Extensive experiments validate that MSCoTDet improves multispectral pedestrian detection.","sentences":["Multispectral pedestrian detection is attractive for around-the-clock applications due to the complementary information between RGB and thermal modalities.","However, current models often fail to detect pedestrians in obvious cases, especially due to the modality bias learned from statistically biased datasets.","From these problems, we anticipate that maybe understanding the complementary information itself is difficult to achieve from vision-only models.","Accordingly, we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework, which incorporates Large Language Models (LLMs) to understand the complementary information at the semantic level and further enhance the fusion process.","Specifically, we generate text descriptions of the pedestrian in each RGB and thermal modality and design a Multispectral Chain-of-Thought (MSCoT) prompting, which models a step-by-step process to facilitate cross-modal reasoning at the semantic level and perform accurate detection.","Moreover, we design a Language-driven Multi-modal Fusion (LMF) strategy that enables fusing vision-driven and language-driven detections.","Extensive experiments validate that MSCoTDet improves multispectral pedestrian detection."],"url":"http://arxiv.org/abs/2403.15209v1","category":"cs.CV"}
{"created":"2024-03-22 13:50:22","title":"VPAS: Publicly Verifiable and Privacy-Preserving Aggregate Statistics on Distributed Datasets","abstract":"Aggregate statistics play an important role in extracting meaningful insights from distributed data while preserving privacy. A growing number of application domains, such as healthcare, utilize these statistics in advancing research and improving patient care.   In this work, we explore the challenge of input validation and public verifiability within privacy-preserving aggregation protocols. We address the scenario in which a party receives data from multiple sources and must verify the validity of the input and correctness of the computations over this data to third parties, such as auditors, while ensuring input data privacy. To achieve this, we propose the \"VPAS\" protocol, which satisfies these requirements. Our protocol utilizes homomorphic encryption for data privacy, and employs Zero-Knowledge Proofs (ZKP) and a blockchain system for input validation and public verifiability. We constructed VPAS by extending existing verifiable encryption schemes into secure protocols that enable N clients to encrypt, aggregate, and subsequently release the final result to a collector in a verifiable manner.   We implemented and experimentally evaluated VPAS with regard to encryption costs, proof generation, and verification. The findings indicate that the overhead associated with verifiability in our protocol is 10x lower than that incurred by simply using conventional zkSNARKs. This enhanced efficiency makes it feasible to apply input validation with public verifiability across a wider range of applications or use cases that can tolerate moderate computational overhead associated with proof generation.","sentences":["Aggregate statistics play an important role in extracting meaningful insights from distributed data while preserving privacy.","A growing number of application domains, such as healthcare, utilize these statistics in advancing research and improving patient care.   ","In this work, we explore the challenge of input validation and public verifiability within privacy-preserving aggregation protocols.","We address the scenario in which a party receives data from multiple sources and must verify the validity of the input and correctness of the computations over this data to third parties, such as auditors, while ensuring input data privacy.","To achieve this, we propose the \"VPAS\" protocol, which satisfies these requirements.","Our protocol utilizes homomorphic encryption for data privacy, and employs Zero-Knowledge Proofs (ZKP) and a blockchain system for input validation and public verifiability.","We constructed VPAS by extending existing verifiable encryption schemes into secure protocols that enable N clients to encrypt, aggregate, and subsequently release the final result to a collector in a verifiable manner.   ","We implemented and experimentally evaluated VPAS with regard to encryption costs, proof generation, and verification.","The findings indicate that the overhead associated with verifiability in our protocol is 10x lower than that incurred by simply using conventional zkSNARKs.","This enhanced efficiency makes it feasible to apply input validation with public verifiability across a wider range of applications or use cases that can tolerate moderate computational overhead associated with proof generation."],"url":"http://arxiv.org/abs/2403.15208v1","category":"cs.CR"}
{"created":"2024-03-22 13:48:57","title":"Scattering matrix approach to dynamical Sauter-Schwinger process: Spin- and helicity-resolved momentum distributions","abstract":"Dynamical Sauter-Schwinger mechanism of electron-positron pair creation by a time-dependent electric field pulses is considered using the $S$-matrix approach and reduction formulas. They lead to the development of framework based on the solutions of the Dirac equation with the Feynman- or anti-Feynman boundary conditions. Their asymptotic properties are linked to the spin-resolved probability amplitudes of created pairs. The same concerns the helicity-resolved amplitudes. Most importantly, the aforementioned spin- or helicity-resolved amplitudes, when summed over spin or helicity configurations, reproduce the momentum distributions of created particles calculated with other methods that are typically used in this context. This does validate the current approach. It also allows us to investigate the vortex structures in momentum distributions of produced particles, as the method provides an access to the phase of the probability amplitude. As we also illustrate numerically, the method is applicable to arbitrary time-dependent electric fields with, in general, elliptical polarization. This proves its great flexibility.","sentences":["Dynamical Sauter-Schwinger mechanism of electron-positron pair creation by a time-dependent electric field pulses is considered using the $S$-matrix approach and reduction formulas.","They lead to the development of framework based on the solutions of the Dirac equation with the Feynman- or anti-Feynman boundary conditions.","Their asymptotic properties are linked to the spin-resolved probability amplitudes of created pairs.","The same concerns the helicity-resolved amplitudes.","Most importantly, the aforementioned spin- or helicity-resolved amplitudes, when summed over spin or helicity configurations, reproduce the momentum distributions of created particles calculated with other methods that are typically used in this context.","This does validate the current approach.","It also allows us to investigate the vortex structures in momentum distributions of produced particles, as the method provides an access to the phase of the probability amplitude.","As we also illustrate numerically, the method is applicable to arbitrary time-dependent electric fields with, in general, elliptical polarization.","This proves its great flexibility."],"url":"http://arxiv.org/abs/2403.15206v1","category":"quant-ph"}
{"created":"2024-03-22 13:46:56","title":"Delicate curvature bounces in the no-boundary wave function and in the late universe","abstract":"Theoretical considerations motivate us to consider vacuum energy to be able to decay and to assume that the spatial geometry of the universe is closed. Combining both aspects leads to the possibility that the universe, or certain regions thereof, can collapse and subsequently undergo a curvature bounce. This may have occurred in the very early universe, in a pre-inflationary phase. We discuss the construction of the corresponding no-boundary instantons and show that they indeed reproduce a bouncing history of the universe, interestingly with a small and potentially observable departure from classicality during the contracting phase. Such an early bouncing history receives a large weighting and provides competition for a more standard inflationary branch of the wave function. Curvature bounces may also occur in the future. We discuss the conditions under which they may take place, allowing for density fluctuations in the matter distribution in the universe. Overall, we find that curvature bounces require a delicate combination of matter content and initial conditions to occur, though with significant consequences if these conditions are met.","sentences":["Theoretical considerations motivate us to consider vacuum energy to be able to decay and to assume that the spatial geometry of the universe is closed.","Combining both aspects leads to the possibility that the universe, or certain regions thereof, can collapse and subsequently undergo a curvature bounce.","This may have occurred in the very early universe, in a pre-inflationary phase.","We discuss the construction of the corresponding no-boundary instantons and show that they indeed reproduce a bouncing history of the universe, interestingly with a small and potentially observable departure from classicality during the contracting phase.","Such an early bouncing history receives a large weighting and provides competition for a more standard inflationary branch of the wave function.","Curvature bounces may also occur in the future.","We discuss the conditions under which they may take place, allowing for density fluctuations in the matter distribution in the universe.","Overall, we find that curvature bounces require a delicate combination of matter content and initial conditions to occur, though with significant consequences if these conditions are met."],"url":"http://arxiv.org/abs/2403.15205v1","category":"gr-qc"}
{"created":"2024-03-22 13:46:51","title":"DITTO: Demonstration Imitation by Trajectory Transformation","abstract":"Teaching robots new skills quickly and conveniently is crucial for the broader adoption of robotic systems. In this work, we address the problem of one-shot imitation from a single human demonstration, given by an RGB-D video recording through a two-stage process. In the first stage which is offline, we extract the trajectory of the demonstration. This entails segmenting manipulated objects and determining their relative motion in relation to secondary objects such as containers. Subsequently, in the live online trajectory generation stage, we first \\mbox{re-detect} all objects, then we warp the demonstration trajectory to the current scene, and finally, we trace the trajectory with the robot. To complete these steps, our method makes leverages several ancillary models, including those for segmentation, relative object pose estimation, and grasp prediction. We systematically evaluate different combinations of correspondence and re-detection methods to validate our design decision across a diverse range of tasks. Specifically, we collect demonstrations of ten different tasks including pick-and-place tasks as well as articulated object manipulation. Finally, we perform extensive evaluations on a real robot system to demonstrate the effectiveness and utility of our approach in real-world scenarios. We make the code publicly available at http://ditto.cs.uni-freiburg.de.","sentences":["Teaching robots new skills quickly and conveniently is crucial for the broader adoption of robotic systems.","In this work, we address the problem of one-shot imitation from a single human demonstration, given by an RGB-D video recording through a two-stage process.","In the first stage which is offline, we extract the trajectory of the demonstration.","This entails segmenting manipulated objects and determining their relative motion in relation to secondary objects such as containers.","Subsequently, in the live online trajectory generation stage, we first \\mbox{re-detect} all objects, then we warp the demonstration trajectory to the current scene, and finally, we trace the trajectory with the robot.","To complete these steps, our method makes leverages several ancillary models, including those for segmentation, relative object pose estimation, and grasp prediction.","We systematically evaluate different combinations of correspondence and re-detection methods to validate our design decision across a diverse range of tasks.","Specifically, we collect demonstrations of ten different tasks including pick-and-place tasks as well as articulated object manipulation.","Finally, we perform extensive evaluations on a real robot system to demonstrate the effectiveness and utility of our approach in real-world scenarios.","We make the code publicly available at http://ditto.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2403.15203v1","category":"cs.RO"}
{"created":"2024-03-22 13:46:39","title":"Structure Formation in Various Dynamical Dark Energy Scenarios","abstract":"This research investigates the impact of the nature of Dark Energy (DE) on structure formation, focusing on the matter power spectrum and the Integrated Sachs-Wolfe effect (ISW). By analyzing the matter power spectrum at redshifts $z = 0$ and $z = 5$, as well as the ISW effect on the scale of $\\ell = 10-100$, the study provides valuable insights into the influence of DE equations of state (EoS) on structure formation. The findings reveal that dynamical DE models exhibit a stronger matter power spectrum compared to constant DE models, with the JBP model demonstrating the highest amplitude and the CPL model the weakest. Additionally, the study delves into the ISW effect, highlighting the time evolution of the ISW source term $\\mathcal{F}(a)$ and its derivative $d\\mathcal{F}(a)/da$, and demonstrating that models with constant DE EoS exhibit a stronger amplitude of $\\mathcal{F}(a)$ overall, while dynamical models such as CPL exhibit the highest amplitude among the dynamical models, whereas JBP has the lowest. The study also explores the ISW auto-correlation power spectrum and the ISW cross-correlation power spectrum, revealing that dynamical DE models dominate over those with constant DE EoS across various surveys. Moreover, it emphasizes the potential of studying the non-linear matter power spectrum and incorporating datasets from the small scales to further elucidate the dynamical nature of dark energy. This comprehensive analysis underscores the significance of both the matter power spectrum and the ISW signal in discerning the nature of dark energy, paving the way for future research to explore the matter power spectrum at higher redshifts and in the non-linear regime, providing deeper insights into the dynamical nature of dark energy.","sentences":["This research investigates the impact of the nature of Dark Energy (DE) on structure formation, focusing on the matter power spectrum and the Integrated Sachs-Wolfe effect (ISW).","By analyzing the matter power spectrum at redshifts $z = 0$ and $z = 5$, as well as the ISW effect on the scale of $\\ell = 10-100$, the study provides valuable insights into the influence of DE equations of state (EoS) on structure formation.","The findings reveal that dynamical DE models exhibit a stronger matter power spectrum compared to constant DE models, with the JBP model demonstrating the highest amplitude and the CPL model the weakest.","Additionally, the study delves into the ISW effect, highlighting the time evolution of the ISW source term $\\mathcal{F}(a)$ and its derivative $d\\mathcal{F}(a)/da$, and demonstrating that models with constant DE EoS exhibit a stronger amplitude of $\\mathcal{F}(a)$ overall, while dynamical models such as CPL exhibit the highest amplitude among the dynamical models, whereas JBP has the lowest.","The study also explores the ISW auto-correlation power spectrum and the ISW cross-correlation power spectrum, revealing that dynamical DE models dominate over those with constant DE EoS across various surveys.","Moreover, it emphasizes the potential of studying the non-linear matter power spectrum and incorporating datasets from the small scales to further elucidate the dynamical nature of dark energy.","This comprehensive analysis underscores the significance of both the matter power spectrum and the ISW signal in discerning the nature of dark energy, paving the way for future research to explore the matter power spectrum at higher redshifts and in the non-linear regime, providing deeper insights into the dynamical nature of dark energy."],"url":"http://arxiv.org/abs/2403.15202v1","category":"astro-ph.CO"}
{"created":"2024-03-22 13:44:17","title":"Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes","abstract":"A conjecture in algorithmic model theory predicts that the model-checking problem for first-order logic is fixed-parameter tractable on a hereditary graph class if and only if the class is monadically dependent. Originating in model theory, this notion is defined in terms of logic, and encompasses nowhere dense classes, monadically stable classes, and classes of bounded twin-width. Working towards this conjecture, we provide the first two combinatorial characterizations of monadically dependent graph classes. This yields the following dichotomy.   On the structure side, we characterize monadic dependence by a Ramsey-theoretic property called flip-breakability. This notion generalizes the notions of uniform quasi-wideness, flip-flatness, and bounded grid rank, which characterize nowhere denseness, monadic stability, and bounded twin-width, respectively, and played a key role in their respective model checking algorithms. Natural restrictions of flip-breakability additionally characterize bounded treewidth and cliquewidth and bounded treedepth and shrubdepth.   On the non-structure side, we characterize monadic dependence by explicitly listing few families of forbidden induced subgraphs. This result is analogous to the characterization of nowhere denseness via forbidden subdivided cliques, and allows us to resolve one half of the motivating conjecture: First-order model checking is AW[$*$]-hard on every hereditary graph class that is monadically independent. The result moreover implies that hereditary graph classes which are small, have almost bounded twin-width, or have almost bounded flip-width, are monadically dependent.   Lastly, we lift our result to also obtain a combinatorial dichotomy in the more general setting of monadically dependent classes of binary structures.","sentences":["A conjecture in algorithmic model theory predicts that the model-checking problem for first-order logic is fixed-parameter tractable on a hereditary graph class if and only if the class is monadically dependent.","Originating in model theory, this notion is defined in terms of logic, and encompasses nowhere dense classes, monadically stable classes, and classes of bounded twin-width.","Working towards this conjecture, we provide the first two combinatorial characterizations of monadically dependent graph classes.","This yields the following dichotomy.   ","On the structure side, we characterize monadic dependence by a Ramsey-theoretic property called flip-breakability.","This notion generalizes the notions of uniform quasi-wideness, flip-flatness, and bounded grid rank, which characterize nowhere denseness, monadic stability, and bounded twin-width, respectively, and played a key role in their respective model checking algorithms.","Natural restrictions of flip-breakability additionally characterize bounded treewidth and cliquewidth and bounded treedepth and shrubdepth.   ","On the non-structure side, we characterize monadic dependence by explicitly listing few families of forbidden induced subgraphs.","This result is analogous to the characterization of nowhere denseness via forbidden subdivided cliques, and allows us to resolve one half of the motivating conjecture: First-order model checking is AW[$*$]-hard on every hereditary graph class that is monadically independent.","The result moreover implies that hereditary graph classes which are small, have almost bounded twin-width, or have almost bounded flip-width, are monadically dependent.   ","Lastly, we lift our result to also obtain a combinatorial dichotomy in the more general setting of monadically dependent classes of binary structures."],"url":"http://arxiv.org/abs/2403.15201v1","category":"math.CO"}
{"created":"2024-03-22 13:37:38","title":"Teamwork and Spillover Effects in Performance Evaluations","abstract":"This article shows how coworker performance affects individual performance evaluation in a teamwork setting at the workplace. We use high-quality data on football matches to measure an important component of individual performance, shooting performance, isolated from collaborative effects. Employing causal machine learning methods, we address the assortative matching of workers and estimate both average and heterogeneous effects. There is substantial evidence for spillover effects in performance evaluations. Coworker shooting performance, meaningfully impacts both, manager decisions and third-party expert evaluations of individual performance. Our results underscore the significant role coworkers play in shaping career advancements and highlight a complementary channel, to productivity gains and learning effects, how coworkers impact career advancement. We characterize the groups of workers that are most and least affected by spillover effects and show that spillover effects are reference point dependent. While positive deviations from a reference point create positive spillover effects, negative deviations are not harmful for coworkers.","sentences":["This article shows how coworker performance affects individual performance evaluation in a teamwork setting at the workplace.","We use high-quality data on football matches to measure an important component of individual performance, shooting performance, isolated from collaborative effects.","Employing causal machine learning methods, we address the assortative matching of workers and estimate both average and heterogeneous effects.","There is substantial evidence for spillover effects in performance evaluations.","Coworker shooting performance, meaningfully impacts both, manager decisions and third-party expert evaluations of individual performance.","Our results underscore the significant role coworkers play in shaping career advancements and highlight a complementary channel, to productivity gains and learning effects, how coworkers impact career advancement.","We characterize the groups of workers that are most and least affected by spillover effects and show that spillover effects are reference point dependent.","While positive deviations from a reference point create positive spillover effects, negative deviations are not harmful for coworkers."],"url":"http://arxiv.org/abs/2403.15200v1","category":"econ.GN"}
{"created":"2024-03-22 13:34:41","title":"On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation","abstract":"We study a family of distance functions on rankings that allow for asymmetric treatments of alternatives and consider the distinct relevance of the top and bottom positions for ordered lists. We provide a full axiomatic characterization of our distance. In doing so, we retrieve new characterizations of existing axioms and show how to effectively weaken them for our purposes. This analysis highlights the generality of our distance as it embeds many (semi)metrics previously proposed in the literature. Subsequently, we show that, notwithstanding its level of generality, our distance is still readily applicable. We apply it to preference aggregation, studying the features of the associated median voting rule. It is shown how the derived preference function satisfies many desirable features in the context of voting rules, ranging from fairness to majority and Pareto-related properties. We show how to compute consensus rankings exactly, and provide generalized Diaconis-Graham inequalities that can be leveraged to obtain approximation algorithms. Finally, we propose some truncation ideas for our distances inspired by Lu and Boutilier (2010). These can be leveraged to devise a Polynomial-Time-Approximation Scheme for the corresponding rank aggregation problem.","sentences":["We study a family of distance functions on rankings that allow for asymmetric treatments of alternatives and consider the distinct relevance of the top and bottom positions for ordered lists.","We provide a full axiomatic characterization of our distance.","In doing so, we retrieve new characterizations of existing axioms and show how to effectively weaken them for our purposes.","This analysis highlights the generality of our distance as it embeds many (semi)metrics previously proposed in the literature.","Subsequently, we show that, notwithstanding its level of generality, our distance is still readily applicable.","We apply it to preference aggregation, studying the features of the associated median voting rule.","It is shown how the derived preference function satisfies many desirable features in the context of voting rules, ranging from fairness to majority and Pareto-related properties.","We show how to compute consensus rankings exactly, and provide generalized Diaconis-Graham inequalities that can be leveraged to obtain approximation algorithms.","Finally, we propose some truncation ideas for our distances inspired by Lu and Boutilier (2010).","These can be leveraged to devise a Polynomial-Time-Approximation Scheme for the corresponding rank aggregation problem."],"url":"http://arxiv.org/abs/2403.15198v1","category":"cs.GT"}
{"created":"2024-03-22 13:31:24","title":"FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication","abstract":"Serverless computing offers attractive scalability, elasticity and cost-effectiveness. However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional 'server-ful' platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory. In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging. We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing. We introduce novel fully serverless communication schemes for ML inference workloads, leveraging both cloud-based publish-subscribe/queueing and object storage offerings. We demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC with comparable performance to object storage, while offering significantly reduced cost at high parallelism levels. We conduct in-depth experiments on benchmark DNNs of various sizes. The results show that when compared to server-based alternatives, FSD-Inference is significantly more cost-effective and scalable, and can even achieve competitive performance against optimized HPC solutions. Experiments also confirm that our serverless solution can handle large distributed workloads and leverage high degrees of FaaS parallelism.","sentences":["Serverless computing offers attractive scalability, elasticity and cost-effectiveness.","However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads.","Traditional 'server-ful' platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory.","In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging.","We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference.","We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing.","We introduce novel fully serverless communication schemes for ML inference workloads, leveraging both cloud-based publish-subscribe/queueing and object storage offerings.","We demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC with comparable performance to object storage, while offering significantly reduced cost at high parallelism levels.","We conduct in-depth experiments on benchmark DNNs of various sizes.","The results show that when compared to server-based alternatives, FSD-Inference is significantly more cost-effective and scalable, and can even achieve competitive performance against optimized HPC solutions.","Experiments also confirm that our serverless solution can handle large distributed workloads and leverage high degrees of FaaS parallelism."],"url":"http://arxiv.org/abs/2403.15195v1","category":"cs.DC"}
{"created":"2024-03-22 13:27:57","title":"Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion","abstract":"The landscape of deep learning research is moving towards innovative strategies to harness the true potential of data. Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, data quality (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when plugging-in our DAS over different light-weight video backbones.","sentences":["The landscape of deep learning research is moving towards innovative strategies to harness the true potential of data.","Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources.","However, independently of the model size, data quality (i.e. amount and variability) is still a major factor that affects model generalization.","In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of image classification and semantic segmentation.","We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos.","Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day.","Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field.","More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations.","As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when plugging-in our DAS over different light-weight video backbones."],"url":"http://arxiv.org/abs/2403.15194v1","category":"cs.CV"}
{"created":"2024-03-22 13:26:55","title":"Dipole-dipole interacting two-level emitters in a moderately intense laser field","abstract":"We investigate the resonance fluorescence features of a small ensemble of closely packed and moderately laser pumped two-level emitters at resonance. The mean distance between any two-level radiators is smaller than the corresponding emission wavelength, such that the dipole-dipole interactions are not negligible. We have found that under the secular approximation, the collective resonance fluorescence spectrum consists of 2N+1 spectral lines, where N is the number of emitters from the sample. The 2N lateral spectral-bands, symmetrically located around the generalized Rabi frequency with respect to the central line at the laser frequency, are distinguishable if the dipole-dipole coupling strength is larger than the collective spontaneous decay rate. This way, one can extract the radiators number within the ensemble via measuring of the spontaneously scattered collective resonance fluorescence spectrum. Contrary, if the dipole-dipole coupling is of the order of or smaller than the cooperative spontaneous decay rate, but still non-negligible, the spectrum turns into a Mollow-like fluorescence spectrum, where the two lateral spectral lines broadens, proportional to the dipole-dipole coupling strength, respectively.","sentences":["We investigate the resonance fluorescence features of a small ensemble of closely packed and moderately laser pumped two-level emitters at resonance.","The mean distance between any two-level radiators is smaller than the corresponding emission wavelength, such that the dipole-dipole interactions are not negligible.","We have found that under the secular approximation, the collective resonance fluorescence spectrum consists of 2N+1 spectral lines, where N is the number of emitters from the sample.","The 2N lateral spectral-bands, symmetrically located around the generalized Rabi frequency with respect to the central line at the laser frequency, are distinguishable if the dipole-dipole coupling strength is larger than the collective spontaneous decay rate.","This way, one can extract the radiators number within the ensemble via measuring of the spontaneously scattered collective resonance fluorescence spectrum.","Contrary, if the dipole-dipole coupling is of the order of or smaller than the cooperative spontaneous decay rate, but still non-negligible, the spectrum turns into a Mollow-like fluorescence spectrum, where the two lateral spectral lines broadens, proportional to the dipole-dipole coupling strength, respectively."],"url":"http://arxiv.org/abs/2403.15193v1","category":"quant-ph"}
{"created":"2024-03-22 13:24:50","title":"SFOD: Spiking Fusion Object Detector","abstract":"Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for object detection in specialized contexts. Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing object detection algorithms. Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties. However, their performance in object detection using event cameras is limited in current implementations. In this paper, we propose the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to SNN-based object detection. Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally, through integrating our analysis and experiments conducted during the pretraining of the backbone network on the NCAR dataset, we delve deeply into the impact of spiking decoding strategies and loss functions on model performance. Thereby, we establish state-of-the-art classification results based on SNNs, achieving 93.7\\% accuracy on the NCAR dataset. Experimental results on the GEN1 detection dataset demonstrate that the SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing SNN-based approaches. Our research not only underscores the potential of SNNs in object detection with event cameras but also propels the advancement of SNNs. Code is available at https://github.com/yimeng-fan/SFOD.","sentences":["Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for object detection in specialized contexts.","Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing object detection algorithms.","Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties.","However, their performance in object detection using event cameras is limited in current implementations.","In this paper, we propose the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to SNN-based object detection.","Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras.","Additionally, through integrating our analysis and experiments conducted during the pretraining of the backbone network on the NCAR dataset, we delve deeply into the impact of spiking decoding strategies and loss functions on model performance.","Thereby, we establish state-of-the-art classification results based on SNNs, achieving 93.7\\% accuracy on the NCAR dataset.","Experimental results on the GEN1 detection dataset demonstrate that the SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing SNN-based approaches.","Our research not only underscores the potential of SNNs in object detection with event cameras but also propels the advancement of SNNs.","Code is available at https://github.com/yimeng-fan/SFOD."],"url":"http://arxiv.org/abs/2403.15192v1","category":"cs.CV"}
{"created":"2024-03-22 13:18:45","title":"Forecasting the load of Parcel Pickup Points using a Markov Jump Process","abstract":"The growth of e-commerce has resulted in a surge in parcel deliveries, increasing transportation costs and pollution issues. Alternatives to home delivery have emerged, such as the delivery to so-called parcel pick-up points (PUPs), which eliminates delivery failure due to customers not being at home. Nevertheless, parcels reaching overloaded PUPs may need to be redirected to alternative PUPs, sometimes far from the chosen ones, which may generate customer dissatisfaction. Consequently, predicting the PUP load is critical for a PUP management company to infer the availability of PUPs for future orders and better balance parcel flows between PUPs.   This paper proposes a new approach to forecasting the PUP load evolution using a Markov jump process that models the parcel life cycle. The latest known status of each parcel is considered to estimate its contribution to the future load of its target PUP. This approach can account for the variability of activity, the various parcel preparation delays by sellers, and the diversity of parcel carriers that may result in different delivery delays. Here, results are provided for predicting the load associated with parcels ordered from online retailers by customers (Business-to-Customer, B2C). The proposed approach is generic and can also be applied to other parcel flows to PUPs, such as second-hand products (Customer-to-Customer, C2C) sent via a PUP network.","sentences":["The growth of e-commerce has resulted in a surge in parcel deliveries, increasing transportation costs and pollution issues.","Alternatives to home delivery have emerged, such as the delivery to so-called parcel pick-up points (PUPs), which eliminates delivery failure due to customers not being at home.","Nevertheless, parcels reaching overloaded PUPs may need to be redirected to alternative PUPs, sometimes far from the chosen ones, which may generate customer dissatisfaction.","Consequently, predicting the PUP load is critical for a PUP management company to infer the availability of PUPs for future orders and better balance parcel flows between PUPs.   ","This paper proposes a new approach to forecasting the PUP load evolution using a Markov jump process that models the parcel life cycle.","The latest known status of each parcel is considered to estimate its contribution to the future load of its target PUP.","This approach can account for the variability of activity, the various parcel preparation delays by sellers, and the diversity of parcel carriers that may result in different delivery delays.","Here, results are provided for predicting the load associated with parcels ordered from online retailers by customers (Business-to-Customer, B2C).","The proposed approach is generic and can also be applied to other parcel flows to PUPs, such as second-hand products (Customer-to-Customer, C2C) sent via a PUP network."],"url":"http://arxiv.org/abs/2403.15189v1","category":"eess.SY"}
{"created":"2024-03-22 13:16:47","title":"Pursuit-Evasion on a Sphere and When It Can Be Considered Flat","abstract":"In classical works on a planar differential pursuit-evasion game with a faster pursuer, the intercept point resulting from the equilibrium strategies lies on the Apollonius circle. This property was exploited for the construction of the equilibrium strategies for two faster pursuers against one evader. Extensions for planar multiple-pursuer single-evader scenarios have been considered. We study a pursuit-evasion game on a sphere and the relation of the equilibrium intercept point to the Apollonius domain on the sphere. The domain is a generalization of the planar Apollonius circle set. We find a condition resulting in the intercept point belonging to the Apollonius domain, which is the characteristic of the planar game solution. Finally, we use this characteristic to discuss pursuit and evasion strategies in the context of two pursuers and a single slower evader on the sphere and illustrate it using numerical simulations.","sentences":["In classical works on a planar differential pursuit-evasion game with a faster pursuer, the intercept point resulting from the equilibrium strategies lies on the Apollonius circle.","This property was exploited for the construction of the equilibrium strategies for two faster pursuers against one evader.","Extensions for planar multiple-pursuer single-evader scenarios have been considered.","We study a pursuit-evasion game on a sphere and the relation of the equilibrium intercept point to the Apollonius domain on the sphere.","The domain is a generalization of the planar Apollonius circle set.","We find a condition resulting in the intercept point belonging to the Apollonius domain, which is the characteristic of the planar game solution.","Finally, we use this characteristic to discuss pursuit and evasion strategies in the context of two pursuers and a single slower evader on the sphere and illustrate it using numerical simulations."],"url":"http://arxiv.org/abs/2403.15188v1","category":"math.OC"}
{"created":"2024-03-22 13:13:13","title":"Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study","abstract":"Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages, but that code completion on functional languages is feasible. Consequently, this shows the need for more high-quality Haskell datasets. A manual evaluation on HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions. Finally, we release HumanEval-Haskell, along with the fine-tuned models and all code required to reproduce our experiments on GitHub (https://github.com/AISE-TUDelft/HaskellCCEval).","sentences":["Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages.","However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages.","Consequently, these models often perform poorly on functional languages such as Haskell.","To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell.","We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace.","Additionally, we manually evaluate the models using our novel translated HumanEval dataset.","Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages, but that code completion on functional languages is feasible.","Consequently, this shows the need for more high-quality Haskell datasets.","A manual evaluation on HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions.","Finally, we release HumanEval-Haskell, along with the fine-tuned models and all code required to reproduce our experiments on GitHub (https://github.com/AISE-TUDelft/HaskellCCEval)."],"url":"http://arxiv.org/abs/2403.15185v1","category":"cs.CL"}
{"created":"2024-03-22 13:12:30","title":"CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition","abstract":"The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D object detection. However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems. Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task. As such, current well-designed camera-radar fusion methods for 3D object detection can hardly take effect in place recognition because they mainly focus on dynamic foreground objects. In this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition. To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points. With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature. As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset. The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%).","sentences":["The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D object detection.","However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems.","Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task.","As such, current well-designed camera-radar fusion methods for 3D object detection can hardly take effect in place recognition because they mainly focus on dynamic foreground objects.","In this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition.","To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points.","With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature.","As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset.","The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%)."],"url":"http://arxiv.org/abs/2403.15183v1","category":"cs.RO"}
{"created":"2024-03-22 13:11:26","title":"PDE-CNNs: Axiomatic Derivations and Applications","abstract":"PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in comparison to CNNs. We also investigate what effect the use of different semifields has on the performance of the models.","sentences":["PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs.","PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability.","In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout.","We call this variant of the framework a PDE-CNN.","We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN.","Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals.","Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in comparison to CNNs.","We also investigate what effect the use of different semifields has on the performance of the models."],"url":"http://arxiv.org/abs/2403.15182v1","category":"cs.LG"}
{"created":"2024-03-22 13:09:10","title":"Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement","abstract":"Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning. While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune. In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no computational overhead. We evaluate our approach on the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The models trained with our method achieve comparable performance and generalization to those trained with expert data. Additionally, we apply our method to the Job Shop Scheduling Problem using a transformer-based architecture and outperform existing state-of-the-art methods by a wide margin.","sentences":["Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning.","While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune.","In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning.","To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement.","This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no computational overhead.","We evaluate our approach on the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem.","The models trained with our method achieve comparable performance and generalization to those trained with expert data.","Additionally, we apply our method to the Job Shop Scheduling Problem using a transformer-based architecture and outperform existing state-of-the-art methods by a wide margin."],"url":"http://arxiv.org/abs/2403.15180v1","category":"cs.LG"}
{"created":"2024-03-22 13:08:27","title":"Rate-fidelity trade-off in cavity-based remote entanglement generation","abstract":"The qubit scalability imposes a paramount challenge in the field of quantum computing. Photonic interconnects between distinct quantum computing modules provide a solution to deal with this issue. The fundamental part of this approach is entanglement distribution via travelling photons emitted by matter qubits. However, randomness of the spontaneous emission in the matter qubits limits both the entanglement fidelity and the generation rate. In this paper, by numerical and analytical methods, we investigate the relationship between the entanglement affected by the spontaneous emission and the waveform of the pump pulse used in the photon generation. We confirm and analyze a rate-fidelity trade-off in the entanglement swapping with Gaussian pump pulses and show that a simple extension to non-Gaussian pump pulses improves the trade-off in a certain parameter region. Furthermore we extend our analysis to entanglement distribution in the general multipartite setting and show that the analysis of the bipartite entanglement can be straightforwardly applied in this case as well.","sentences":["The qubit scalability imposes a paramount challenge in the field of quantum computing.","Photonic interconnects between distinct quantum computing modules provide a solution to deal with this issue.","The fundamental part of this approach is entanglement distribution via travelling photons emitted by matter qubits.","However, randomness of the spontaneous emission in the matter qubits limits both the entanglement fidelity and the generation rate.","In this paper, by numerical and analytical methods, we investigate the relationship between the entanglement affected by the spontaneous emission and the waveform of the pump pulse used in the photon generation.","We confirm and analyze a rate-fidelity trade-off in the entanglement swapping with Gaussian pump pulses and show that a simple extension to non-Gaussian pump pulses improves the trade-off in a certain parameter region.","Furthermore we extend our analysis to entanglement distribution in the general multipartite setting and show that the analysis of the bipartite entanglement can be straightforwardly applied in this case as well."],"url":"http://arxiv.org/abs/2403.15179v1","category":"quant-ph"}
{"created":"2024-03-22 13:03:54","title":"Calculations of magnetic field produced by spin-vortex-induced loop currents in Bi$_2$Sr$_2$CaCu$_2$O$_{8+\u03b4}$ thin films using the particle-number conserving Bogoliubov-de Gennes formalism","abstract":"A theory for cuprate superconductivity predicts the existence of nano-sized loop currents called, `` spin-vortex-induced loop currents (SVILCs)''. We calculate magnetic fields produced by them for a model of Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (Bi-2212) thin films composed of one surface and two bulk CuO$_2$ bilayers. In this model, bulk CuO$_2$ layers host stable spin-vortices around small polarons formed from doped holes; they give rise to a $U(1)$ gauge field described by the Berry connection from many-body wave functions, and generates the SVILCs. The effect of the gauge field is taken into account by the particle-number conserving Bogoliubov-de Gennes (PNC-BdG) formalism. The magnitude of the calculated magnetic field produced by the SVILCs in the vicinity of the surface ($10a \\approx 4$ nm, where $a$ is the lattice constant of the CuO$_2$ plane) is in the order of mT; thus, may be detectable by currently available detection methods. The detection of the SVILCs by the magnetic field measurement may bring about the elucidation of the cuprate superconductivity, and may also lead to their quantum device applications, including qubits.","sentences":["A theory for cuprate superconductivity predicts the existence of nano-sized loop currents called, `` spin-vortex-induced loop currents (SVILCs)''.","We calculate magnetic fields produced by them for a model of Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (Bi-2212) thin films composed of one surface and two bulk CuO$_2$ bilayers.","In this model, bulk CuO$_2$ layers host stable spin-vortices around small polarons formed from doped holes; they give rise to a $U(1)$ gauge field described by the Berry connection from many-body wave functions, and generates the SVILCs.","The effect of the gauge field is taken into account by the particle-number conserving Bogoliubov-de Gennes (PNC-BdG) formalism.","The magnitude of the calculated magnetic field produced by the SVILCs in the vicinity of the surface ($10a \\approx 4$ nm, where $a$ is the lattice constant of the CuO$_2$ plane) is in the order of mT; thus, may be detectable by currently available detection methods.","The detection of the SVILCs by the magnetic field measurement may bring about the elucidation of the cuprate superconductivity, and may also lead to their quantum device applications, including qubits."],"url":"http://arxiv.org/abs/2403.15177v1","category":"cond-mat.supr-con"}
{"created":"2024-03-22 13:01:10","title":"Brain-grounding of semantic vectors improves neural decoding of visual stimuli","abstract":"Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector representation of stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a representation learning framework, termed brain-grounding of semantic vectors, which fine-tunes pretrained feature vectors to better align with the neural representation of visual stimuli in the human brain. We trained this model this model with functional magnetic resonance imaging (fMRI) of 150 different visual stimuli categories, and then performed zero-shot brain decoding and identification analyses on 1) fMRI and 2) magnetoencephalography (MEG). Interestingly, we observed that by using the brain-grounded vectors, the brain decoding and identification accuracy on brain data from different neuroimaging modalities increases. These findings underscore the potential of incorporating a richer array of brain-derived features to enhance performance of brain decoding algorithms.","sentences":["Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces.","Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector representation of stimuli.","These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping.","To address this issue, we propose a representation learning framework, termed brain-grounding of semantic vectors, which fine-tunes pretrained feature vectors to better align with the neural representation of visual stimuli in the human brain.","We trained this model this model with functional magnetic resonance imaging (fMRI) of 150 different visual stimuli categories, and then performed zero-shot brain decoding and identification analyses on 1) fMRI and 2) magnetoencephalography (MEG).","Interestingly, we observed that by using the brain-grounded vectors, the brain decoding and identification accuracy on brain data from different neuroimaging modalities increases.","These findings underscore the potential of incorporating a richer array of brain-derived features to enhance performance of brain decoding algorithms."],"url":"http://arxiv.org/abs/2403.15176v1","category":"q-bio.NC"}
{"created":"2024-03-22 12:55:51","title":"Hot Casimir Wormholes","abstract":"In this paper, we have for the first time considered the consequences of thermal fluctuations to the Casimir effect on a traversable wormhole. This was done by using finite temperature generalization of the Casimir effect as a source of a hot traversable wormhole. Thus, we have considered a more physical scenario, where the effects of thermal fluctuations are also considered as a source of a traversable wormhole. To obtain a dependence on such a thermal Casimir effect, consider the plates positioned at a distance either parametrically fixed or radially varying. In both cases, the temperature effects are investigated. We demonstrate that thermal fluctuations modify the throat of the wormhole. Such results have been obtained in both regimes, i.e. high temperature and low temperature. We explicitly investigate the effect of such finite temperature effects on the size of a wormhole.","sentences":["In this paper, we have for the first time considered the consequences of thermal fluctuations to the Casimir effect on a traversable wormhole.","This was done by using finite temperature generalization of the Casimir effect as a source of a hot traversable wormhole.","Thus, we have considered a more physical scenario, where the effects of thermal fluctuations are also considered as a source of a traversable wormhole.","To obtain a dependence on such a thermal Casimir effect, consider the plates positioned at a distance either parametrically fixed or radially varying.","In both cases, the temperature effects are investigated.","We demonstrate that thermal fluctuations modify the throat of the wormhole.","Such results have been obtained in both regimes, i.e. high temperature and low temperature.","We explicitly investigate the effect of such finite temperature effects on the size of a wormhole."],"url":"http://arxiv.org/abs/2403.15174v1","category":"gr-qc"}
{"created":"2024-03-22 12:46:58","title":"Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders","abstract":"Self-supervised learning (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD and PTSD. Furthermore, we modify the hyper-parameters of the SSL encoder predicting fixed targets to generate global representations that capture varying temporal contexts. Both these innovations are noted to yield improved detection performances for considered mental disorders and exhibit task-agnostic traits. In the context of the SSL model predicting masked frames, the generated global representations are also noted to exhibit task-agnostic traits.","sentences":["Self-supervised learning (SSL) has been investigated to generate task-agnostic representations across various domains.","However, such investigation has not been conducted for detecting multiple mental disorders.","The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders.","Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders.","Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions.","This study employs SSL models trained by predicting multiple fixed targets or masked frames.","We propose a list of fixed targets to make the generated representation more efficient for detecting MDD and PTSD.","Furthermore, we modify the hyper-parameters of the SSL encoder predicting fixed targets to generate global representations that capture varying temporal contexts.","Both these innovations are noted to yield improved detection performances for considered mental disorders and exhibit task-agnostic traits.","In the context of the SSL model predicting masked frames, the generated global representations are also noted to exhibit task-agnostic traits."],"url":"http://arxiv.org/abs/2403.15170v1","category":"cs.LG"}
{"created":"2024-03-22 12:37:14","title":"Transition Graph Properties of Target Class Classification","abstract":"Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class. The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes. The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class. The transition process can be described in the form of a directed graph, and the success of the final classification is mainly due to the properties of this graph. In our previous research we showed that the desirable structure of the transition graph is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class. It is clear that the transition graph of an arbitrary algorithm (policy) may not have this property. In this paper we study the structure of realistic transition graphs, which makes it possible to find classification inconsistencies, helping to transfer it into the desired form. The medical interpretation of dynamic treatment regime considered in the article further clarifies the investigated framework.","sentences":["Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class.","The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes.","The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class.","The transition process can be described in the form of a directed graph, and the success of the final classification is mainly due to the properties of this graph.","In our previous research we showed that the desirable structure of the transition graph is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class.","It is clear that the transition graph of an arbitrary algorithm (policy) may not have this property.","In this paper we study the structure of realistic transition graphs, which makes it possible to find classification inconsistencies, helping to transfer it into the desired form.","The medical interpretation of dynamic treatment regime considered in the article further clarifies the investigated framework."],"url":"http://arxiv.org/abs/2403.15167v1","category":"cs.LG"}
{"created":"2024-03-22 12:36:28","title":"Channel Orthogonalization with Reconfigurable Surfaces: General Models, Theoretical Limits, and Effective Configuration","abstract":"We envision a future in which multi-antenna technology effectively exploits the spatial domain as a set of non-interfering orthogonal resources, allowing for flexible resource allocation and efficient modulation/demodulation. Reconfigurable intelligent surface (RIS) has emerged as a promising technology which allows shaping the propagation environment for improved performance. This paper studies the ability of three extended types of reconfigurable surface (RS), including the recently proposed beyond diagonal RIS (BD-RIS), to achieve perfectly orthogonal channels in a general multi-user multiple-input multiple-output (MU-MIMO) scenario. We propose practical implementations for the three types of RS consisting of passive components, and obtain the corresponding restrictions on their reconfigurability. We then use these restrictions to derive closed-form conditions for achieving arbitrary (orthogonal) channels. We also study the problem of optimal orthogonal channel selection for achieving high channel gain without active amplification at the RS, and we propose some methods with satisfying performance. Finally, we provide efficient channel estimation and RS configuration techniques such that all the computation, including the channel selection, may be performed at the base station (BS). The numerical results showcase the potential and practicality of RS channel orthogonalization, thus taking a step towards orthogonal spatial domain multiplexing (OSDM).","sentences":["We envision a future in which multi-antenna technology effectively exploits the spatial domain as a set of non-interfering orthogonal resources, allowing for flexible resource allocation and efficient modulation/demodulation.","Reconfigurable intelligent surface (RIS) has emerged as a promising technology which allows shaping the propagation environment for improved performance.","This paper studies the ability of three extended types of reconfigurable surface (RS), including the recently proposed beyond diagonal RIS (BD-RIS), to achieve perfectly orthogonal channels in a general multi-user multiple-input multiple-output (MU-MIMO) scenario.","We propose practical implementations for the three types of RS consisting of passive components, and obtain the corresponding restrictions on their reconfigurability.","We then use these restrictions to derive closed-form conditions for achieving arbitrary (orthogonal) channels.","We also study the problem of optimal orthogonal channel selection for achieving high channel gain without active amplification at the RS, and we propose some methods with satisfying performance.","Finally, we provide efficient channel estimation and RS configuration techniques such that all the computation, including the channel selection, may be performed at the base station (BS).","The numerical results showcase the potential and practicality of RS channel orthogonalization, thus taking a step towards orthogonal spatial domain multiplexing (OSDM)."],"url":"http://arxiv.org/abs/2403.15165v1","category":"cs.IT"}
{"created":"2024-03-22 12:20:23","title":"FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos","abstract":"Digitising the 3D world into a clean, CAD model-based representation has important applications for augmented reality and robotics. Current state-of-the-art methods are computationally intensive as they individually encode each detected object and optimise CAD alignments in a second stage. In this work, we propose FastCAD, a real-time method that simultaneously retrieves and aligns CAD models for all objects in a given scene. In contrast to previous works, we directly predict alignment parameters and shape embeddings. We achieve high-quality shape retrievals by learning CAD embeddings in a contrastive learning framework and distilling those into FastCAD. Our single-stage method accelerates the inference time by a factor of 50 compared to other methods operating on RGB-D scans while outperforming them on the challenging Scan2CAD alignment benchmark. Further, our approach collaborates seamlessly with online 3D reconstruction techniques. This enables the real-time generation of precise CAD model-based reconstructions from videos at 10 FPS. Doing so, we significantly improve the Scan2CAD alignment accuracy in the video setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to 29.6%.","sentences":["Digitising the 3D world into a clean, CAD model-based representation has important applications for augmented reality and robotics.","Current state-of-the-art methods are computationally intensive as they individually encode each detected object and optimise CAD alignments in a second stage.","In this work, we propose FastCAD, a real-time method that simultaneously retrieves and aligns CAD models for all objects in a given scene.","In contrast to previous works, we directly predict alignment parameters and shape embeddings.","We achieve high-quality shape retrievals by learning CAD embeddings in a contrastive learning framework and distilling those into FastCAD.","Our single-stage method accelerates the inference time by a factor of 50 compared to other methods operating on RGB-D scans while outperforming them on the challenging Scan2CAD alignment benchmark.","Further, our approach collaborates seamlessly with online 3D reconstruction techniques.","This enables the real-time generation of precise CAD model-based reconstructions from videos at 10 FPS.","Doing so, we significantly improve the Scan2CAD alignment accuracy in the video setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to 29.6%."],"url":"http://arxiv.org/abs/2403.15161v1","category":"cs.CV"}
{"created":"2024-03-22 12:13:44","title":"Magnetic Bianchi-I cosmology in the Horndeski theory","abstract":"We study the evolution of Bianchi-I space-times within the framework of the Horndeski theory with $G_5=\\text{const}/X$. The space-times are filled a global unidirectional electromagnetic field interacting with a scalar field. We consider the minimal interaction and the non-minimal interaction by the law $f^2(\\phi)F_{\\mu\\nu}F^{\\mu\\nu}$. The Horndeski theory allows anisotropy to grow over time, so the question arises of regulating the anisotropic level in this theory. Using the reconstruction method, we build models in which the anisotropic level tends to a small value as the Universe expands. One of the results is a model with a anisotropic bounce.","sentences":["We study the evolution of Bianchi-I space-times within the framework of the Horndeski theory with $G_5=\\text{const}/X$. The space-times are filled a global unidirectional electromagnetic field interacting with a scalar field.","We consider the minimal interaction and the non-minimal interaction by the law $f^2(\\phi)F_{\\mu\\nu}F^{\\mu\\nu}$. The Horndeski theory allows anisotropy to grow over time, so the question arises of regulating the anisotropic level in this theory.","Using the reconstruction method, we build models in which the anisotropic level tends to a small value as the Universe expands.","One of the results is a model with a anisotropic bounce."],"url":"http://arxiv.org/abs/2403.15158v1","category":"gr-qc"}
{"created":"2024-03-22 12:13:16","title":"AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models","abstract":"Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. Effectively and efficiently extracting valuable insights from such data poses a challenging task. This paper introduces Allhands , an innovative analytic framework designed for large-scale feedback analysis through a natural language interface, leveraging large language models (LLMs). Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and topic modeling on the feedback to convert them into a structurally augmented format, incorporating LLMs to enhance accuracy, robustness, generalization, and user-friendliness. Subsequently, an LLM agent is employed to interpret users' diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive multi-modal responses, including text, code, tables, and images.   We evaluate Allhands across three diverse feedback datasets. The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and topic modeling, eventually providing users with an ``ask me anything'' experience with comprehensive, correct and human-readable response. To the best of our knowledge, Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.","sentences":["Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development.","Effectively and efficiently extracting valuable insights from such data poses a challenging task.","This paper introduces Allhands , an innovative analytic framework designed for large-scale feedback analysis through a natural language interface, leveraging large language models (LLMs).","Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and topic modeling on the feedback to convert them into a structurally augmented format, incorporating LLMs to enhance accuracy, robustness, generalization, and user-friendliness.","Subsequently, an LLM agent is employed to interpret users' diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive multi-modal responses, including text, code, tables, and images.   ","We evaluate Allhands across three diverse feedback datasets.","The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and topic modeling, eventually providing users with an ``ask me anything'' experience with comprehensive, correct and human-readable response.","To the best of our knowledge, Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface."],"url":"http://arxiv.org/abs/2403.15157v1","category":"cs.SE"}
{"created":"2024-03-22 12:08:53","title":"Electrostatic dipole polarizability and plasmon resonances of multilayer nanoshells","abstract":"We propose a generalized formula for calculating the dipole polarizability of spherical multilayer nanoshells (MNSs) within the long-wavelength approximation (LWA). Given a MNS with a finite number of concentric layers, radii, and dielectric properties, embedded in a dielectric medium, in the presence of a uniform electric field, we show that its frequency-dependent and complex dipole polarizability can be expressed in terms of the dipole polarizability of the preceding MNS. This approach is different from previous more involved methods where the LWA polarizability of a MNS is usually derived from scattering coefficients. Using both finite-element method- and Mie theory-based simulations, we show that our proposed formula reproduces the usual LWA results, when it is used to predict absorption spectra, by comparing the results to simulated spectra obtained from MNSs with $n$ number of layers up to $n = 6$ layers. An iterative algorithm for calculating the dipole polarizability of a MNS based on the generalized formula is presented. A Fr\\\"{o}hlich function whose zeroes correspond to the dipolar localized surface plasmon resonances (LSPRs) supported by the MNS is proposed. We identify a pairing behaviour by some LSPRs in the Fr\\\"{o}hlich function that might also be useful for mode characterization.","sentences":["We propose a generalized formula for calculating the dipole polarizability of spherical multilayer nanoshells (MNSs) within the long-wavelength approximation (LWA).","Given a MNS with a finite number of concentric layers, radii, and dielectric properties, embedded in a dielectric medium, in the presence of a uniform electric field, we show that its frequency-dependent and complex dipole polarizability can be expressed in terms of the dipole polarizability of the preceding MNS.","This approach is different from previous more involved methods where the LWA polarizability of a MNS is usually derived from scattering coefficients.","Using both finite-element method- and Mie theory-based simulations, we show that our proposed formula reproduces the usual LWA results, when it is used to predict absorption spectra, by comparing the results to simulated spectra obtained from MNSs with $n$ number of layers","up to $n = 6$ layers.","An iterative algorithm for calculating the dipole polarizability of a MNS based on the generalized formula is presented.","A Fr\\\"{o}hlich function whose zeroes correspond to the dipolar localized surface plasmon resonances (LSPRs) supported by the MNS is proposed.","We identify a pairing behaviour by some LSPRs in the Fr\\\"{o}hlich function that might also be useful for mode characterization."],"url":"http://arxiv.org/abs/2403.15153v1","category":"physics.optics"}
{"created":"2024-03-22 12:08:16","title":"A Multimodal Approach for Cross-Domain Image Retrieval","abstract":"Image generators are gaining vast amount of popularity and have rapidly changed how digital content is created. With the latest AI technology, millions of high quality images are being generated by the public, which are constantly motivating the research community to push the limits of generative models to create more complex and realistic images. This paper focuses on Cross-Domain Image Retrieval (CDIR) which can be used as an additional tool to inspect collections of generated images by determining the level of similarity between images in a dataset. An ideal retrieval system would be able to generalize to unseen complex images from multiple domains (e.g., photos, drawings and paintings). To address this goal, we propose a novel caption-matching approach that leverages multimodal language-vision architectures pre-trained on large datasets. The method is tested on DomainNet and Office-Home datasets and consistently achieves state-of-the-art performance over the latest approaches in the literature for cross-domain image retrieval. In order to verify the effectiveness with AI-generated images, the method was also put to test with a database composed by samples collected from Midjourney, which is a widely used generative platform for content creation.","sentences":["Image generators are gaining vast amount of popularity and have rapidly changed how digital content is created.","With the latest AI technology, millions of high quality images are being generated by the public, which are constantly motivating the research community to push the limits of generative models to create more complex and realistic images.","This paper focuses on Cross-Domain Image Retrieval (CDIR) which can be used as an additional tool to inspect collections of generated images by determining the level of similarity between images in a dataset.","An ideal retrieval system would be able to generalize to unseen complex images from multiple domains (e.g., photos, drawings and paintings).","To address this goal, we propose a novel caption-matching approach that leverages multimodal language-vision architectures pre-trained on large datasets.","The method is tested on DomainNet and Office-Home datasets and consistently achieves state-of-the-art performance over the latest approaches in the literature for cross-domain image retrieval.","In order to verify the effectiveness with AI-generated images, the method was also put to test with a database composed by samples collected from Midjourney, which is a widely used generative platform for content creation."],"url":"http://arxiv.org/abs/2403.15152v1","category":"cs.CV"}
{"created":"2024-03-22 12:07:03","title":"RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit","abstract":"In 1997, the very first tour guide robot RHINO was deployed in a museum in Germany. With the ability to navigate autonomously through the environment, the robot gave tours to over 2,000 visitors. Today, RHINO itself has become an exhibit and is no longer operational. In this paper, we present RHINO-VR, an interactive museum exhibit using virtual reality (VR) that allows museum visitors to experience the historical robot RHINO in operation in a virtual museum. RHINO-VR, unlike static exhibits, enables users to familiarize themselves with basic mobile robotics concepts without the fear of damaging the exhibit. In the virtual environment, the user is able to interact with RHINO in VR by pointing to a location to which the robot should navigate and observing the corresponding actions of the robot. To include other visitors who cannot use the VR, we provide an external observation view to make RHINO visible to them. We evaluated our system by measuring the frame rate of the VR simulation, comparing the generated virtual 3D models with the originals, and conducting a user study. The user-study showed that RHINO-VR improved the visitors' understanding of the robot's functionality and that they would recommend experiencing the VR exhibit to others.","sentences":["In 1997, the very first tour guide robot RHINO was deployed in a museum in Germany.","With the ability to navigate autonomously through the environment, the robot gave tours to over 2,000 visitors.","Today, RHINO itself has become an exhibit and is no longer operational.","In this paper, we present RHINO-VR, an interactive museum exhibit using virtual reality (VR) that allows museum visitors to experience the historical robot RHINO in operation in a virtual museum.","RHINO-VR, unlike static exhibits, enables users to familiarize themselves with basic mobile robotics concepts without the fear of damaging the exhibit.","In the virtual environment, the user is able to interact with RHINO in VR by pointing to a location to which the robot should navigate and observing the corresponding actions of the robot.","To include other visitors who cannot use the VR, we provide an external observation view to make RHINO visible to them.","We evaluated our system by measuring the frame rate of the VR simulation, comparing the generated virtual 3D models with the originals, and conducting a user study.","The user-study showed that RHINO-VR improved the visitors' understanding of the robot's functionality and that they would recommend experiencing the VR exhibit to others."],"url":"http://arxiv.org/abs/2403.15151v1","category":"cs.RO"}
{"created":"2024-03-22 12:05:18","title":"On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions","abstract":"Code completion is a key feature of Integrated Development Environments (IDEs), aimed at predicting the next tokens a developer is likely to write, helping them write code faster and with less effort. Modern code completion approaches are often powered by deep learning (DL) models. However, the swift evolution of programming languages poses a critical challenge to the performance of DL-based code completion models: Can these models generalize across different language versions? This paper delves into such a question. In particular, we assess the capabilities of a state-of-the-art model, CodeT5, to generalize across nine different Java versions, ranging from Java 2 to Java 17, while being exclusively trained on Java 8 code. Our evaluation spans three completion scenarios, namely, predicting tokens, constructs (e.g., the condition of an if statement) and entire code blocks. The results of our study reveal a noticeable disparity among language versions, with the worst performance being obtained in Java 2 and 17 - the most far apart versions compared to Java 8. We investigate possible causes for the performance degradation and show that the adoption of a limited version-specific fine-tuning can partially alleviate the problem. Our work raises awareness on the importance of continuous model refinement, and it can inform the design of alternatives to make code completion models more robust to language evolution.","sentences":["Code completion is a key feature of Integrated Development Environments (IDEs), aimed at predicting the next tokens a developer is likely to write, helping them write code faster and with less effort.","Modern code completion approaches are often powered by deep learning (DL) models.","However, the swift evolution of programming languages poses a critical challenge to the performance of DL-based code completion models: Can these models generalize across different language versions?","This paper delves into such a question.","In particular, we assess the capabilities of a state-of-the-art model, CodeT5, to generalize across nine different Java versions, ranging from Java 2 to Java 17, while being exclusively trained on Java 8 code.","Our evaluation spans three completion scenarios, namely, predicting tokens, constructs (e.g., the condition of an if statement) and entire code blocks.","The results of our study reveal a noticeable disparity among language versions, with the worst performance being obtained in Java 2 and 17 - the most far apart versions compared to Java 8.","We investigate possible causes for the performance degradation and show that the adoption of a limited version-specific fine-tuning can partially alleviate the problem.","Our work raises awareness on the importance of continuous model refinement, and it can inform the design of alternatives to make code completion models more robust to language evolution."],"url":"http://arxiv.org/abs/2403.15149v1","category":"cs.SE"}
{"created":"2024-03-22 11:59:38","title":"Splitting methods for unbounded operators","abstract":"This paper considers computational methods that split a vector field into three components in the case when both the vector field and the split components might be unbounded. We first employ classical Taylor expansion which, after some algebra, results in an expression for a second-order splitting which, strictly speaking, makes sense only for bounded operators. Next, using an alternative approach, we derive an error expression and an error bound in the same setting which are however valid in the presence of unbounded operators.   While the paper itself is concerned with second-order splittings using three components, the method of proof in the presence of unboundedness remains valid (although significantly more complicated) in a more general scenario, which will be the subject of a forthcoming paper.","sentences":["This paper considers computational methods that split a vector field into three components in the case when both the vector field and the split components might be unbounded.","We first employ classical Taylor expansion which, after some algebra, results in an expression for a second-order splitting which, strictly speaking, makes sense only for bounded operators.","Next, using an alternative approach, we derive an error expression and an error bound in the same setting which are however valid in the presence of unbounded operators.   ","While the paper itself is concerned with second-order splittings using three components, the method of proof in the presence of unboundedness remains valid (although significantly more complicated) in a more general scenario, which will be the subject of a forthcoming paper."],"url":"http://arxiv.org/abs/2403.15147v1","category":"math.NA"}
{"created":"2024-03-22 11:56:41","title":"Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems","abstract":"A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted simultaneous wireless information and power transfer (SWIPT) system is proposed. More particularly, an STAR-RIS is deployed to assist in the information/power transfer from a multi-antenna access point (AP) to multiple single-antenna information users (IUs) and energy users (EUs), where two practical STAR-RIS operating protocols, namely energy splitting (ES) and time switching (TS), are employed. Under the imperfect channel state information (CSI) condition, a multi-objective optimization problem (MOOP) framework, that simultaneously maximizes the minimum data rate and minimum harvested power, is employed to investigate the fundamental rate-energy trade-off between IUs and EUs. To obtain the optimal robust resource allocation strategy, the MOOP is first transformed into a single-objective optimization problem (SOOP) via the {\\epsilon}-constraint method, which is then reformulated by approximating semi-infinite inequality constraints with the S-procedure. For ES, an alternating optimization (AO)-based algorithm is proposed to jointly design AP active beamforming and STAR-RIS passive beamforming, where a penalty method is leveraged in STAR-RIS beamforming design. Furthermore, the developed algorithm is extended to optimize the time allocation policy and beamforming vectors in a two-layer iterative manner for TS. Numerical results reveal that: 1) deploying STAR-RISs achieves a significant performance gain over conventional RISs, especially in terms of harvested power for EUs; 2) the ES protocol obtains a better user fairness performance when focusing only on IUs or EUs, while the TS protocol yields a better balance between IUs and EUs; 3) the imperfect CSI affects IUs more significantly than EUs, whereas TS can confer a more robust design to attenuate these effects.","sentences":["A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted simultaneous wireless information and power transfer (SWIPT) system is proposed.","More particularly, an STAR-RIS is deployed to assist in the information/power transfer from a multi-antenna access point (AP) to multiple single-antenna information users (IUs) and energy users (EUs), where two practical STAR-RIS operating protocols, namely energy splitting (ES) and time switching (TS), are employed.","Under the imperfect channel state information (CSI) condition, a multi-objective optimization problem (MOOP) framework, that simultaneously maximizes the minimum data rate and minimum harvested power, is employed to investigate the fundamental rate-energy trade-off between IUs and EUs.","To obtain the optimal robust resource allocation strategy, the MOOP is first transformed into a single-objective optimization problem (SOOP) via the {\\epsilon}-constraint method, which is then reformulated by approximating semi-infinite inequality constraints with the S-procedure.","For ES, an alternating optimization (AO)-based algorithm is proposed to jointly design AP active beamforming and STAR-RIS passive beamforming, where a penalty method is leveraged in STAR-RIS beamforming design.","Furthermore, the developed algorithm is extended to optimize the time allocation policy and beamforming vectors in a two-layer iterative manner for TS.","Numerical results reveal that: 1) deploying STAR-RISs achieves a significant performance gain over conventional RISs, especially in terms of harvested power for EUs; 2) the ES protocol obtains a better user fairness performance when focusing only on IUs or EUs, while the TS protocol yields a better balance between IUs and EUs; 3) the imperfect CSI affects IUs more significantly than EUs, whereas TS can confer a more robust design to attenuate these effects."],"url":"http://arxiv.org/abs/2403.15145v1","category":"cs.IT"}
{"created":"2024-03-22 11:54:46","title":"Dynamic Interface Printing","abstract":"Additive manufacturing is an expanding multidisciplinary field encompassing applications including medical devices, aerospace components, microfabrication strategies, and artificial organs. Among additive manufacturing approaches, light-based printing technologies, including two-photon polymerization, projection micro stereolithography, and volumetric printing, have garnered significant attention due to their speed, resolution and/or potential applications for biofabrication. In this study, we introduce dynamic interface printing (DIP), a new 3D printing approach that leverages an acoustically modulated, constrained air-liquid boundary to rapidly generate cm-scale three-dimensional structures within tens of seconds. Distinct from volumetric approaches, this process eliminates the need for intricate feedback systems, specialized chemistry, or complex optics while maintaining rapid printing speeds. We demonstrate the versatility of this technique across a broad array of materials and intricate geometries, including those that would be impossible to print via conventional layer-by-layer methods. In doing so, we demonstrate the rapid fabrication of complex structures in-situ, overprinting, structural parallelisation, and biofabrication utility. Moreover, we showcase that the formation of surface waves at this boundary enables enhanced mass transport, material flexibility, and permits three-dimensional particle patterning. We therefore anticipate that this approach will be invaluable for applications where high resolution, scalable throughput, and biocompatible printing is required.","sentences":["Additive manufacturing is an expanding multidisciplinary field encompassing applications including medical devices, aerospace components, microfabrication strategies, and artificial organs.","Among additive manufacturing approaches, light-based printing technologies, including two-photon polymerization, projection micro stereolithography, and volumetric printing, have garnered significant attention due to their speed, resolution and/or potential applications for biofabrication.","In this study, we introduce dynamic interface printing (DIP), a new 3D printing approach that leverages an acoustically modulated, constrained air-liquid boundary to rapidly generate cm-scale three-dimensional structures within tens of seconds.","Distinct from volumetric approaches, this process eliminates the need for intricate feedback systems, specialized chemistry, or complex optics while maintaining rapid printing speeds.","We demonstrate the versatility of this technique across a broad array of materials and intricate geometries, including those that would be impossible to print via conventional layer-by-layer methods.","In doing so, we demonstrate the rapid fabrication of complex structures in-situ, overprinting, structural parallelisation, and biofabrication utility.","Moreover, we showcase that the formation of surface waves at this boundary enables enhanced mass transport, material flexibility, and permits three-dimensional particle patterning.","We therefore anticipate that this approach will be invaluable for applications where high resolution, scalable throughput, and biocompatible printing is required."],"url":"http://arxiv.org/abs/2403.15144v1","category":"physics.app-ph"}
{"created":"2024-03-22 11:53:03","title":"Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project","abstract":"Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis. However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation. By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation. Additionally, by incorporating Active Learning (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle. It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annotation tool that supports the classification and segmentation of medical images. The user-friendly interface allows for easy alteration of the AL and DL model settings through a configuration file, requiring no prior programming experience. While MedDeepCyleAL can be applied to any kind of image data, we have specifically applied it to ophthalmology data in this project.","sentences":["Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis.","However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious.","Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation.","By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation.","Additionally, by incorporating Active Learning (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data.","We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle.","It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annotation tool that supports the classification and segmentation of medical images.","The user-friendly interface allows for easy alteration of the AL and DL model settings through a configuration file, requiring no prior programming experience.","While MedDeepCyleAL can be applied to any kind of image data, we have specifically applied it to ophthalmology data in this project."],"url":"http://arxiv.org/abs/2403.15143v1","category":"cs.CV"}
{"created":"2024-03-22 11:48:09","title":"Deep Generative Model based Rate-Distortion for Image Downscaling Assessment","abstract":"In this paper, we propose Image Downscaling Assessment by Rate-Distortion (IDA-RD), a novel measure to quantitatively evaluate image downscaling algorithms. In contrast to image-based methods that measure the quality of downscaled images, ours is process-based that draws ideas from rate-distortion theory to measure the distortion incurred during downscaling. Our main idea is that downscaling and super-resolution (SR) can be viewed as the encoding and decoding processes in the rate-distortion model, respectively, and that a downscaling algorithm that preserves more details in the resulting low-resolution (LR) images should lead to less distorted high-resolution (HR) images in SR. In other words, the distortion should increase as the downscaling algorithm deteriorates. However, it is non-trivial to measure this distortion as it requires the SR algorithm to be blind and stochastic. Our key insight is that such requirements can be met by recent SR algorithms based on deep generative models that can find all matching HR images for a given LR image on their learned image manifolds. Extensive experimental results show the effectiveness of our IDA-RD measure.","sentences":["In this paper, we propose Image Downscaling Assessment by Rate-Distortion (IDA-RD), a novel measure to quantitatively evaluate image downscaling algorithms.","In contrast to image-based methods that measure the quality of downscaled images, ours is process-based that draws ideas from rate-distortion theory to measure the distortion incurred during downscaling.","Our main idea is that downscaling and super-resolution (SR) can be viewed as the encoding and decoding processes in the rate-distortion model, respectively, and that a downscaling algorithm that preserves more details in the resulting low-resolution (LR) images should lead to less distorted high-resolution (HR) images in SR.","In other words, the distortion should increase as the downscaling algorithm deteriorates.","However, it is non-trivial to measure this distortion as it requires the SR algorithm to be blind and stochastic.","Our key insight is that such requirements can be met by recent SR algorithms based on deep generative models that can find all matching HR images for a given LR image on their learned image manifolds.","Extensive experimental results show the effectiveness of our IDA-RD measure."],"url":"http://arxiv.org/abs/2403.15139v1","category":"cs.CV"}
{"created":"2024-03-22 11:42:47","title":"CACA Agent: Capability Collaboration based AI Agent","abstract":"As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario extension of CACA Agent.","sentences":["As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge.","Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality.","In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing.","CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents.","Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario extension of CACA Agent."],"url":"http://arxiv.org/abs/2403.15137v1","category":"cs.AI"}
{"created":"2024-03-22 11:39:06","title":"A Surface Hydrothermal Source of Nitriles and Isonitriles","abstract":"Giant impacts can generate transient hydrogen-rich atmospheres, reducing atmospheric carbon. The reduced carbon will form hazes that rain out onto the surface and can become incorporated into the crust. Once heated, a large fraction of the carbon would be converted into graphite. The result is that local regions of the Hadean crust were plausibly saturated with graphite. We explore the consequences of such a crust for a prebiotic surface hydrothermal vent scenario. We model a surface vent fed by nitrogen-rich volcanic gas from high-temperature magmas passing through graphite-saturated crust. We consider this occurring at pressures of 1-1000 bar and temperatures of 1500-1700 degC. The equilibrium with graphite purifies the left-over gas, resulting in substantial quantities of nitriles (0.1% HCN and 1 ppm HC3N) and isonitriles (0.01% HNC) relevant for prebiotic chemistry. We use these results to predict gas-phase concentrations of methyl isonitrile of ~ 1 ppm. Methyl isocyanide can participate in the non-enzymatic activation and ligation of the monomeric building blocks of life, and surface, or shallow, hydrothermal environments provide its only known equilibrium geochemical source.","sentences":["Giant impacts can generate transient hydrogen-rich atmospheres, reducing atmospheric carbon.","The reduced carbon will form hazes that rain out onto the surface and can become incorporated into the crust.","Once heated, a large fraction of the carbon would be converted into graphite.","The result is that local regions of the Hadean crust were plausibly saturated with graphite.","We explore the consequences of such a crust for a prebiotic surface hydrothermal vent scenario.","We model a surface vent fed by nitrogen-rich volcanic gas from high-temperature magmas passing through graphite-saturated crust.","We consider this occurring at pressures of 1-1000 bar and temperatures of 1500-1700 degC. The equilibrium with graphite purifies the left-over gas, resulting in substantial quantities of nitriles (0.1% HCN and 1 ppm HC3N) and isonitriles (0.01% HNC) relevant for prebiotic chemistry.","We use these results to predict gas-phase concentrations of methyl isonitrile of ~ 1 ppm.","Methyl isocyanide can participate in the non-enzymatic activation and ligation of the monomeric building blocks of life, and surface, or shallow, hydrothermal environments provide its only known equilibrium geochemical source."],"url":"http://arxiv.org/abs/2403.15135v1","category":"astro-ph.EP"}
{"created":"2024-03-22 11:38:21","title":"First investigation of void statistics in numerical relativity simulations","abstract":"We apply and extend standard tools for void statistics to cosmological simulations that solve Einstein's equations with numerical relativity (NR). We obtain a simulated void catalogue without Newtonian approximations, using a new watershed void finder which operates on fluid-based NR simulations produced with the Einstein Toolkit. We compare and contrast measures of void size and void fraction, and compare radial stacked density profiles to empirically-derived Hamaus-Sutter-Wandelt (HSW) density profiles and profiles based on distance to void boundaries. We recover statistics roughly consistent with Newtonian N-body simulations where such a comparison is meaningful. We study variation of dynamical spatial curvature and local expansion explicitly demonstrating the spatial fluctuations of these quantities in void regions. We find that voids in our simulations expand ~10-30% faster than the global average and the kinetic curvature density parameter in the centre of voids reaches ~60-80%. Within the limits of resolution of the simulations, the results are consistent with the parameters of the Timescape model of cosmological backreaction.","sentences":["We apply and extend standard tools for void statistics to cosmological simulations that solve Einstein's equations with numerical relativity (NR).","We obtain a simulated void catalogue without Newtonian approximations, using a new watershed void finder which operates on fluid-based NR simulations produced with the Einstein Toolkit.","We compare and contrast measures of void size and void fraction, and compare radial stacked density profiles to empirically-derived Hamaus-Sutter-Wandelt (HSW) density profiles and profiles based on distance to void boundaries.","We recover statistics roughly consistent with Newtonian N-body simulations where such a comparison is meaningful.","We study variation of dynamical spatial curvature and local expansion explicitly demonstrating the spatial fluctuations of these quantities in void regions.","We find that voids in our simulations expand ~10-30% faster than the global average and the kinetic curvature density parameter in the centre of voids reaches ~60-80%.","Within the limits of resolution of the simulations, the results are consistent with the parameters of the Timescape model of cosmological backreaction."],"url":"http://arxiv.org/abs/2403.15134v1","category":"astro-ph.CO"}
{"created":"2024-03-22 11:34:19","title":"Robust realization of electrically tunable spin-orbit locked vortex pairs in polariton condensates at room temperature","abstract":"Coupling of orbital and spin degrees of freedom gives rise to intriguing physical phenomena in bosonic condensates, such as formation of stripe phases and domains with vortex arrays. However, the robust locking of spin and orbital degrees of freedom of nonlinear topological objects such as vortices in bosonic condensates at room temperature remains challenging. In the present work, we realize a non-equilibrium room-temperature condensate in a liquid crystal (LC) planar photonic microcavity with the perovskite CsPbBr3 as optically active material. We use the interplay of TE-TM mode splitting and Rashba-Dresselhaus spin-orbit coupling (RDSOC) to realize electrically tunable polariton vortex pairs with locked spin and orbital angular momentum. Our results are robust against sample imperfections and pave the way to investigate coupling and locking of vortex orbital and spin degrees of freedom in a quantum fluid of light at room temperature, offering potential for generation of complex states of light for optical information processing with optoelectronic chips.","sentences":["Coupling of orbital and spin degrees of freedom gives rise to intriguing physical phenomena in bosonic condensates, such as formation of stripe phases and domains with vortex arrays.","However, the robust locking of spin and orbital degrees of freedom of nonlinear topological objects such as vortices in bosonic condensates at room temperature remains challenging.","In the present work, we realize a non-equilibrium room-temperature condensate in a liquid crystal (LC) planar photonic microcavity with the perovskite CsPbBr3 as optically active material.","We use the interplay of TE-TM mode splitting and Rashba-Dresselhaus spin-orbit coupling (RDSOC) to realize electrically tunable polariton vortex pairs with locked spin and orbital angular momentum.","Our results are robust against sample imperfections and pave the way to investigate coupling and locking of vortex orbital and spin degrees of freedom in a quantum fluid of light at room temperature, offering potential for generation of complex states of light for optical information processing with optoelectronic chips."],"url":"http://arxiv.org/abs/2403.15133v1","category":"physics.optics"}
{"created":"2024-03-22 11:33:04","title":"Transfer CLIP for Generalizable Image Denoising","abstract":"Image denoising is a fundamental task in computer vision. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in-distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.","sentences":["Image denoising is a fundamental task in computer vision.","While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in-distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge.","The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation.","Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored.","This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising.","Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising.","The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder.","Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method."],"url":"http://arxiv.org/abs/2403.15132v1","category":"cs.CV"}
{"created":"2024-03-22 11:31:22","title":"Coexisting Passive RIS and Active Relay Assisted NOMA Systems","abstract":"A novel coexisting passive reconfigurable intelligent surface (RIS) and active decode-and-forward (DF) relay assisted non-orthogonal multiple access (NOMA) transmission framework is proposed. In particular, two communication protocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA). Based on the proposed two protocols, both the sum rate maximization and max-min rate fairness problems are formulated for jointly optimizing the power allocation at the access point and relay as well as the passive beamforming design at the RIS. To tackle the non-convex problems, an alternating optimization (AO) based algorithm is first developed, where the transmit power and the RIS phase-shift are alternatingly optimized by leveraging the two-dimensional search and rank-relaxed difference-of-convex (DC) programming, respectively. Then, a two-layer penalty based joint optimization (JO) algorithm is developed to jointly optimize the resource allocation coefficients within each iteration. Finally, numerical results demonstrate that: i) the proposed coexisting RIS and relay assisted transmission framework is capable of achieving a significant user performance improvement than conventional schemes without RIS or relay; ii) compared with the AO algorithm, the JO algorithm requires less execution time at the cost of a slight performance loss; and iii) the H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate fairness and enhancing user sum rate, respectively.","sentences":["A novel coexisting passive reconfigurable intelligent surface (RIS) and active decode-and-forward (DF) relay assisted non-orthogonal multiple access (NOMA) transmission framework is proposed.","In particular, two communication protocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA).","Based on the proposed two protocols, both the sum rate maximization and max-min rate fairness problems are formulated for jointly optimizing the power allocation at the access point and relay as well as the passive beamforming design at the RIS.","To tackle the non-convex problems, an alternating optimization (AO) based algorithm is first developed, where the transmit power and the RIS phase-shift are alternatingly optimized by leveraging the two-dimensional search and rank-relaxed difference-of-convex (DC) programming, respectively.","Then, a two-layer penalty based joint optimization (JO) algorithm is developed to jointly optimize the resource allocation coefficients within each iteration.","Finally, numerical results demonstrate that: i) the proposed coexisting RIS and relay assisted transmission framework is capable of achieving a significant user performance improvement than conventional schemes without RIS or relay; ii) compared with the AO algorithm, the JO algorithm requires less execution time at the cost of a slight performance loss; and iii) the H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate fairness and enhancing user sum rate, respectively."],"url":"http://arxiv.org/abs/2403.15130v1","category":"cs.IT"}
{"created":"2024-03-22 11:30:57","title":"Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering","abstract":"The present study introduces an advanced multi-physics and multi-scale modeling approach to investigate in silico colon motility. We introduce a generalized electromechanical framework, integrating cellular electrophysiology and smooth muscle contractility, thus advancing a first-of-its-kind computational model of laser tissue soldering after incision resection. The proposed theoretical framework comprises three main elements: a microstructural material model describing intestine wall geometry and composition of reinforcing fibers, with four fiber families, two active-conductive and two passive; an electrophysiological model describing the propagation of slow waves, based on a fully-coupled nonlinear phenomenological approach; and a thermodynamical consistent mechanical model describing the hyperelastic energetic contributions ruling tissue equilibrium under diverse loading conditions. The active strain approach was adopted to describe tissue electromechanics by exploiting the multiplicative decomposition of the deformation gradient for each active fiber family and solving the governing equations via a staggered finite element scheme. The computational framework was fine-tuned according to state-of-the-art experimental evidence, and extensive numerical analyses allowed us to compare manometric traces computed via numerical simulations with those obtained clinically in human patients. The model proved capable of reproducing both qualitatively and quantitatively high or low-amplitude propagation contractions. Colon motility after laser tissue soldering demonstrates that material properties and couplings of the deposited tissue are critical to reproducing a physiological muscular contraction, thus restoring a proper peristaltic activity.","sentences":["The present study introduces an advanced multi-physics and multi-scale modeling approach to investigate in silico colon motility.","We introduce a generalized electromechanical framework, integrating cellular electrophysiology and smooth muscle contractility, thus advancing a first-of-its-kind computational model of laser tissue soldering after incision resection.","The proposed theoretical framework comprises three main elements: a microstructural material model describing intestine wall geometry and composition of reinforcing fibers, with four fiber families, two active-conductive and two passive; an electrophysiological model describing the propagation of slow waves, based on a fully-coupled nonlinear phenomenological approach; and a thermodynamical consistent mechanical model describing the hyperelastic energetic contributions ruling tissue equilibrium under diverse loading conditions.","The active strain approach was adopted to describe tissue electromechanics by exploiting the multiplicative decomposition of the deformation gradient for each active fiber family and solving the governing equations via a staggered finite element scheme.","The computational framework was fine-tuned according to state-of-the-art experimental evidence, and extensive numerical analyses allowed us to compare manometric traces computed via numerical simulations with those obtained clinically in human patients.","The model proved capable of reproducing both qualitatively and quantitatively high or low-amplitude propagation contractions.","Colon motility after laser tissue soldering demonstrates that material properties and couplings of the deposited tissue are critical to reproducing a physiological muscular contraction, thus restoring a proper peristaltic activity."],"url":"http://arxiv.org/abs/2403.15129v1","category":"physics.med-ph"}
{"created":"2024-03-22 11:28:36","title":"Gravitational Waves from Collapse of Pressureless Matter in the Early Universe","abstract":"If an early matter phase of the Universe existed after inflation with the proper power spectrum, enhanced density perturbations can decouple from the Hubble flow, turn around and collapse. In contrast to what happens in a radiation dominated Universe where pressure nullifies deviations from sphericity in these perturbations, in a matter dominated Universe, the lack of pressure although on the one hand facilitates the gravitational collapse, it allows small deviations from sphericity to grow substantially as the collapse takes place. The subsequent collapse is complicated: initially as non-spherical deviations grow, the collapsing cloud takes the form of a ``Zel'dovich pancake\". After that, the more chaotic and nonlinear stage of violent relaxation begins where shells of the cloud cross and the matter is redistributed within a factor of a few of the free fall timescale, reaching a spherical virialized state. During the whole process, strong gravitational waves are emitted due to the anisotropy of the collapse and the small time interval that the effect takes place. The emission of gravitational waves during the stage of the violent relaxation cannot be easily estimated with an analytical model. We perform an $N$-body simulation to capture the behaviour of matter during this stage in order to estimate the precise spectrum of gravitational waves produced in this scenario.","sentences":["If an early matter phase of the Universe existed after inflation with the proper power spectrum, enhanced density perturbations can decouple from the Hubble flow, turn around and collapse.","In contrast to what happens in a radiation dominated Universe where pressure nullifies deviations from sphericity in these perturbations, in a matter dominated Universe, the lack of pressure although on the one hand facilitates the gravitational collapse, it allows small deviations from sphericity to grow substantially as the collapse takes place.","The subsequent collapse is complicated: initially as non-spherical deviations grow, the collapsing cloud takes the form of a ``Zel'dovich pancake\".","After that, the more chaotic and nonlinear stage of violent relaxation begins where shells of the cloud cross and the matter is redistributed within a factor of a few of the free fall timescale, reaching a spherical virialized state.","During the whole process, strong gravitational waves are emitted due to the anisotropy of the collapse and the small time interval that the effect takes place.","The emission of gravitational waves during the stage of the violent relaxation cannot be easily estimated with an analytical model.","We perform an $N$-body simulation to capture the behaviour of matter during this stage in order to estimate the precise spectrum of gravitational waves produced in this scenario."],"url":"http://arxiv.org/abs/2403.15126v1","category":"astro-ph.CO"}
{"created":"2024-03-22 11:24:31","title":"SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation","abstract":"Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with profound societal impact. Identifying risk markers early is crucial for understanding disease progression and enabling preventive measures. The Danish High Risk and Resilience Study (VIA) focuses on understanding early disease processes, particularly in children with familial high risk (FHR). Understanding structural brain changes associated with these diseases during early stages is essential for effective interventions. The central sulcus (CS) is a prominent brain landmark related to brain regions involved in motor and sensory processing. Analyzing CS morphology can provide valuable insights into neurodevelopmental abnormalities in the FHR group. However, segmenting the central sulcus (CS) presents challenges due to its variability, especially in adolescents. This study introduces two novel approaches to improve CS segmentation: synthetic data generation to model CS variability and self-supervised pre-training with multi-task learning to adapt models to new cohorts. These methods aim to enhance segmentation performance across diverse populations, eliminating the need for extensive preprocessing.","sentences":["Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with profound societal impact.","Identifying risk markers early is crucial for understanding disease progression and enabling preventive measures.","The Danish High Risk and Resilience Study (VIA) focuses on understanding early disease processes, particularly in children with familial high risk (FHR).","Understanding structural brain changes associated with these diseases during early stages is essential for effective interventions.","The central sulcus (CS) is a prominent brain landmark related to brain regions involved in motor and sensory processing.","Analyzing CS morphology can provide valuable insights into neurodevelopmental abnormalities in the FHR group.","However, segmenting the central sulcus (CS) presents challenges due to its variability, especially in adolescents.","This study introduces two novel approaches to improve CS segmentation: synthetic data generation to model CS variability and self-supervised pre-training with multi-task learning to adapt models to new cohorts.","These methods aim to enhance segmentation performance across diverse populations, eliminating the need for extensive preprocessing."],"url":"http://arxiv.org/abs/2403.15121v1","category":"cs.CV"}
{"created":"2024-03-22 11:24:10","title":"STAR-RIS Assisted Downlink Active and Uplink Backscatter Communications with NOMA","abstract":"A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted downlink (DL) active and uplink (UL) backscatter communication (BackCom) framework is proposed. More particularly, a full-duplex (FD) base station (BS) communicates with the DL users via the STAR-RIS's transmission link, while exciting and receiving the information from the UL BackCom devices with the aid of the STAR-RIS's reflection link. Non-orthogonal multiple access (NOMA) is exploited in both DL and UL communications for improving the spectrum efficiency. The system weighted sum rate maximization problem is formulated for jointly optimizing the FD BS active receive and transmit beamforming, the STAR- RIS passive beamforming, and the DL NOMA decoding orders, subject to the DL user's individual rate constraint. To tackle this challenging non-convex problem, we propose an alternating optimization (AO) based algorithm for the joint active and passive beamforming design with a given DL NOMA decoding order. To address the potential high computational complexity required for exhaustive searching all the NOMA decoding orders, an efficient NOMA user ordering scheme is further developed. Finally, numerical results demonstrate that: i) compared with the baseline schemes employing conventional RISs or space division multiple access, the proposed scheme achieves higher performance gains; and ii) higher UL rate gain is obtained at a cost of DL performance degradation, as a remedy, a more flexible performance tradeoff can be achieved by introducing the STAR-RIS.","sentences":["A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted downlink (DL) active and uplink (UL) backscatter communication (BackCom) framework is proposed.","More particularly, a full-duplex (FD) base station (BS) communicates with the DL users via the STAR-RIS's transmission link, while exciting and receiving the information from the UL BackCom devices with the aid of the STAR-RIS's reflection link.","Non-orthogonal multiple access (NOMA) is exploited in both DL and UL communications for improving the spectrum efficiency.","The system weighted sum rate maximization problem is formulated for jointly optimizing the FD BS active receive and transmit beamforming, the STAR- RIS passive beamforming, and the DL NOMA decoding orders, subject to the DL user's individual rate constraint.","To tackle this challenging non-convex problem, we propose an alternating optimization (AO) based algorithm for the joint active and passive beamforming design with a given DL NOMA decoding order.","To address the potential high computational complexity required for exhaustive searching all the NOMA decoding orders, an efficient NOMA user ordering scheme is further developed.","Finally, numerical results demonstrate that: i) compared with the baseline schemes employing conventional RISs or space division multiple access, the proposed scheme achieves higher performance gains; and ii) higher UL rate gain is obtained at a cost of DL performance degradation, as a remedy, a more flexible performance tradeoff can be achieved by introducing the STAR-RIS."],"url":"http://arxiv.org/abs/2403.15120v1","category":"cs.IT"}
{"created":"2024-03-22 11:21:51","title":"An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification","abstract":"Person re-identification (ReID) has made great strides thanks to the data-driven deep learning techniques. However, the existing benchmark datasets lack diversity, and models trained on these data cannot generalize well to dynamic wild scenarios. To meet the goal of improving the explicit generalization of ReID models, we develop a new Open-World, Diverse, Cross-Spatial-Temporal dataset named OWD with several distinct features. 1) Diverse collection scenes: multiple independent open-world and highly dynamic collecting scenes, including streets, intersections, shopping malls, etc. 2) Diverse lighting variations: long time spans from daytime to nighttime with abundant illumination changes. 3) Diverse person status: multiple camera networks in all seasons with normal/adverse weather conditions and diverse pedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4) Protected privacy: invisible faces for privacy critical applications. To improve the implicit generalization of ReID, we further propose a Latent Domain Expansion (LDE) method to develop the potential of source data, which decouples discriminative identity-relevant and trustworthy domain-relevant features and implicitly enforces domain-randomized identity feature space expansion with richer domain diversity to facilitate domain invariant representations. Our comprehensive evaluations with most benchmark datasets in the community are crucial for progress, although this work is far from the grand goal toward open-world and dynamic wild applications.","sentences":["Person re-identification (ReID) has made great strides thanks to the data-driven deep learning techniques.","However, the existing benchmark datasets lack diversity, and models trained on these data cannot generalize well to dynamic wild scenarios.","To meet the goal of improving the explicit generalization of ReID models, we develop a new Open-World, Diverse, Cross-Spatial-Temporal dataset named OWD with several distinct features.","1) Diverse collection scenes: multiple independent open-world and highly dynamic collecting scenes, including streets, intersections, shopping malls, etc. 2) Diverse lighting variations: long time spans from daytime to nighttime with abundant illumination changes.","3) Diverse person status: multiple camera networks in all seasons with normal/adverse weather conditions and diverse pedestrian appearances (e.g., clothes, personal belongings, poses, etc.).","4) Protected privacy: invisible faces for privacy critical applications.","To improve the implicit generalization of ReID, we further propose a Latent Domain Expansion (LDE) method to develop the potential of source data, which decouples discriminative identity-relevant and trustworthy domain-relevant features and implicitly enforces domain-randomized identity feature space expansion with richer domain diversity to facilitate domain invariant representations.","Our comprehensive evaluations with most benchmark datasets in the community are crucial for progress, although this work is far from the grand goal toward open-world and dynamic wild applications."],"url":"http://arxiv.org/abs/2403.15119v1","category":"cs.CV"}
{"created":"2024-03-22 11:20:30","title":"Collision Avoidance Safety Filter for an Autonomous E-Scooter using Ultrasonic Sensors","abstract":"In this paper, we propose a collision avoidance safety filter for autonomous electric scooters to enable safe operation of such vehicles in pedestrian areas. In particular, we employ multiple low-cost ultrasonic sensors to detect a wide range of possible obstacles in front of the e-scooter. Based on possibly faulty distance measurements, we design a filter to mitigate measurement noise and missing values as well as a gain-scheduled controller to limit the velocity commanded to the e-scooter when required due to imminent collisions. The proposed controller structure is able to prevent collisions with unknown obstacles by deploying a reduced safe velocity ensuring a sufficiently large safety distance. The collision avoidance approach is designed such that it may be easily deployed in similar applications of general micromobility vehicles. The effectiveness of our proposed safety filter is demonstrated in real-world experiments.","sentences":["In this paper, we propose a collision avoidance safety filter for autonomous electric scooters to enable safe operation of such vehicles in pedestrian areas.","In particular, we employ multiple low-cost ultrasonic sensors to detect a wide range of possible obstacles in front of the e-scooter.","Based on possibly faulty distance measurements, we design a filter to mitigate measurement noise and missing values as well as a gain-scheduled controller to limit the velocity commanded to the e-scooter when required due to imminent collisions.","The proposed controller structure is able to prevent collisions with unknown obstacles by deploying a reduced safe velocity ensuring a sufficiently large safety distance.","The collision avoidance approach is designed such that it may be easily deployed in similar applications of general micromobility vehicles.","The effectiveness of our proposed safety filter is demonstrated in real-world experiments."],"url":"http://arxiv.org/abs/2403.15116v1","category":"eess.SY"}
{"created":"2024-03-22 11:16:43","title":"Language Models in Dialogue: Conversational Maxims for Human-AI Interactions","abstract":"Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how to assess conversational quality between humans and LLM-driven conversational agents, informing both their evaluation and improved design.","sentences":["Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings.","We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles.","By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation.","We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions.","We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions.","The proposed maxims offer prescriptive guidance on how to assess conversational quality between humans and LLM-driven conversational agents, informing both their evaluation and improved design."],"url":"http://arxiv.org/abs/2403.15115v1","category":"cs.CL"}
{"created":"2024-03-22 11:16:11","title":"Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers","abstract":"Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years. Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem. Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements. The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts. Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages. Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the application of Q4RPD, an experimentation composed of six different instances has been conducted, aiming to serve as illustrative examples.","sentences":["Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years.","Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem.","Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements.","The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts.","Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages.","Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave.","To demonstrate the application of Q4RPD, an experimentation composed of six different instances has been conducted, aiming to serve as illustrative examples."],"url":"http://arxiv.org/abs/2403.15114v1","category":"cs.ET"}
{"created":"2024-03-22 11:08:48","title":"Text clustering with LLM embeddings","abstract":"Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a complex balance between the need for nuanced text representation and computational feasibility in text clustering applications. This study extends traditional text clustering frameworks by incorporating embeddings from LLMs, thereby paving the way for improved methodologies and opening new avenues for future research in various types of textual analysis.","sentences":["Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data.","In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered.","A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment.","Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance.","In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models.","These results highlight a complex balance between the need for nuanced text representation and computational feasibility in text clustering applications.","This study extends traditional text clustering frameworks by incorporating embeddings from LLMs, thereby paving the way for improved methodologies and opening new avenues for future research in various types of textual analysis."],"url":"http://arxiv.org/abs/2403.15112v1","category":"cs.CL"}
{"created":"2024-03-22 11:05:11","title":"Probing gluon GTMDs of the proton in deep inelastic diffractive dijet production at HERA","abstract":"We calculate several differential distributions for diffractive dijets production in $e p \\to e' {\\rm jet \\, jet} p$ in the pQCD dipole approach using off diagonal unintegrated gluon distributions (GTMDs). Different models from the literature are used. We concentrate on the contribution from exclusive $q \\bar q$ dijets.   Results of our calculations are compared to H1 and ZEUS data, including specific experimental cuts in our calculations. In general, except of one GTMD, our results are below the HERA data. The considered mechanism is expected to gives a sizeable contribution to the ZEUS data, while it is negligible in the kinematics of the H1 measurement.   This is in contrast to recent results from the literature where the normalization was adjusted to some selected distributions of H1 collaboration and no agreement with other observables was checked.   The ZEUS data provide stricter limitations on the GTMDs than the H1 data. We conclude, based on comparison to different observables, that the calculated cross sections are only a small fraction of the measured ones which contain probably also processes with pomeron remnant. Alternatively the experimental data could be explained by inclusion of $q \\bar q g$ component.   We present also azimuthal correlations between the sum and the difference of dijet transverse momenta. The cuts on transverse momenta of jets generate corresponding azimuthal correlations which can be misidentified as due to elliptic gluon distributions.","sentences":["We calculate several differential distributions for diffractive dijets production in $e p \\to e' {\\rm jet \\, jet} p$ in the pQCD dipole approach using off diagonal unintegrated gluon distributions (GTMDs).","Different models from the literature are used.","We concentrate on the contribution from exclusive $q \\bar q$ dijets.   ","Results of our calculations are compared to H1 and ZEUS data, including specific experimental cuts in our calculations.","In general, except of one GTMD, our results are below the HERA data.","The considered mechanism is expected to gives a sizeable contribution to the ZEUS data, while it is negligible in the kinematics of the H1 measurement.   ","This is in contrast to recent results from the literature where the normalization was adjusted to some selected distributions of H1 collaboration and no agreement with other observables was checked.   ","The ZEUS data provide stricter limitations on the GTMDs than the H1 data.","We conclude, based on comparison to different observables, that the calculated cross sections are only a small fraction of the measured ones which contain probably also processes with pomeron remnant.","Alternatively the experimental data could be explained by inclusion of $q \\bar q g$ component.   ","We present also azimuthal correlations between the sum and the difference of dijet transverse momenta.","The cuts on transverse momenta of jets generate corresponding azimuthal correlations which can be misidentified as due to elliptic gluon distributions."],"url":"http://arxiv.org/abs/2403.15110v1","category":"hep-ph"}
{"created":"2024-03-22 10:42:25","title":"Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data","abstract":"Segmentation of fetal brain tissue from magnetic resonance imaging (MRI) plays a crucial role in the study of in utero neurodevelopment. However, automated tools face substantial domain shift challenges as they must be robust to highly heterogeneous clinical data, often limited in numbers and lacking annotations. Indeed, high variability of the fetal brain morphology, MRI acquisition parameters, and superresolution reconstruction (SR) algorithms adversely affect the model's performance when evaluated out-of-domain. In this work, we introduce FetalSynthSeg, a domain randomization method to segment fetal brain MRI, inspired by SynthSeg. Our results show that models trained solely on synthetic data outperform models trained on real data in out-ofdomain settings, validated on a 120-subject cross-domain dataset. Furthermore, we extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and reconstructed with novel SR models, showcasing robustness across different magnetic field strengths and SR algorithms. Leveraging a generative synthetic approach, we tackle the domain shift problem in fetal brain MRI and offer compelling prospects for applications in fields with limited and highly heterogeneous data.","sentences":["Segmentation of fetal brain tissue from magnetic resonance imaging (MRI) plays a crucial role in the study of in utero neurodevelopment.","However, automated tools face substantial domain shift challenges as they must be robust to highly heterogeneous clinical data, often limited in numbers and lacking annotations.","Indeed, high variability of the fetal brain morphology, MRI acquisition parameters, and superresolution reconstruction (SR) algorithms adversely affect the model's performance when evaluated out-of-domain.","In this work, we introduce FetalSynthSeg, a domain randomization method to segment fetal brain MRI, inspired by SynthSeg.","Our results show that models trained solely on synthetic data outperform models trained on real data in out-ofdomain settings, validated on a 120-subject cross-domain dataset.","Furthermore, we extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and reconstructed with novel SR models, showcasing robustness across different magnetic field strengths and SR algorithms.","Leveraging a generative synthetic approach, we tackle the domain shift problem in fetal brain MRI and offer compelling prospects for applications in fields with limited and highly heterogeneous data."],"url":"http://arxiv.org/abs/2403.15103v1","category":"eess.IV"}
{"created":"2024-03-22 10:39:58","title":"Paddy: Evolutionary Optimization Algorithm for Chemical Systems and Spaces","abstract":"Optimization of chemical systems and processes have been enhanced and enabled by the guidance of algorithms and analytical approaches. While many methods will systematically investigate how underlying variables govern a given outcome, there is often a substantial number of experiments needed to accurately model these relations. As chemical systems increase in complexity, inexhaustive processes must propose experiments that efficiently optimize the underlying objective, while ideally avoiding convergence on unsatisfactory local minima. We have developed the Paddy software package around the Paddy Field Algorithm, a biologically inspired evolutionary optimization algorithm that propagates parameters without direct inference of the underlying objective function. Benchmarked against the Tree of Parzen Estimator, a Bayesian algorithm implemented in the Hyperopt software Library, Paddy displays efficient optimization with lower runtime, and avoidance of early convergence. Herein we report these findings for the cases of: global optimization of a two-dimensional bimodal distribution, interpolation of an irregular sinusoidal function, hyperparameter optimization of an artificial neural network tasked with classification of solvent for reaction components, and targeted molecule generation via optimization of input vectors for a decoder network. We anticipate that the facile nature of Paddy will serve to aid in automated experimentation, where minimization of investigative trials and or diversity of suitable solutions is of high priority.","sentences":["Optimization of chemical systems and processes have been enhanced and enabled by the guidance of algorithms and analytical approaches.","While many methods will systematically investigate how underlying variables govern a given outcome, there is often a substantial number of experiments needed to accurately model these relations.","As chemical systems increase in complexity, inexhaustive processes must propose experiments that efficiently optimize the underlying objective, while ideally avoiding convergence on unsatisfactory local minima.","We have developed the Paddy software package around the Paddy Field Algorithm, a biologically inspired evolutionary optimization algorithm that propagates parameters without direct inference of the underlying objective function.","Benchmarked against the Tree of Parzen Estimator, a Bayesian algorithm implemented in the Hyperopt software Library, Paddy displays efficient optimization with lower runtime, and avoidance of early convergence.","Herein we report these findings for the cases of: global optimization of a two-dimensional bimodal distribution, interpolation of an irregular sinusoidal function, hyperparameter optimization of an artificial neural network tasked with classification of solvent for reaction components, and targeted molecule generation via optimization of input vectors for a decoder network.","We anticipate that the facile nature of Paddy will serve to aid in automated experimentation, where minimization of investigative trials and or diversity of suitable solutions is of high priority."],"url":"http://arxiv.org/abs/2403.15101v1","category":"math.OC"}
{"created":"2024-03-22 10:39:22","title":"Subequivariant Reinforcement Learning Framework for Coordinated Motion Control","abstract":"Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.","sentences":["Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases.","However, many existing methods struggle to account for the intricate dependencies between joints.","We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning.","This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control.","Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach.","Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency."],"url":"http://arxiv.org/abs/2403.15100v1","category":"cs.RO"}
{"created":"2024-03-22 10:36:50","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","abstract":"Vehicle trajectory prediction has increasingly relied on data-driven solutions, but their ability to scale to different data domains and the impact of larger dataset sizes on their generalization remain under-explored. While these questions can be studied by employing multiple datasets, it is challenging due to several discrepancies, \\textit{e.g.,} in data formats, map resolution, and semantic annotation types. To address these challenges, we introduce UniTraj, a comprehensive framework that unifies various datasets, models, and evaluation criteria, presenting new opportunities for the vehicle trajectory prediction field. In particular, using UniTraj, we conduct extensive experiments and find that model performance significantly drops when transferred to other datasets. However, enlarging data size and diversity can substantially improve performance, leading to a new state-of-the-art result for the nuScenes dataset. We provide insights into dataset characteristics to explain these findings. The code can be found here: \\hyperlink{https://github.com/vita-epfl/UniTraj}{https://github.com/vita-epfl/UniTraj}.","sentences":["Vehicle trajectory prediction has increasingly relied on data-driven solutions, but their ability to scale to different data domains and the impact of larger dataset sizes on their generalization remain under-explored.","While these questions can be studied by employing multiple datasets, it is challenging due to several discrepancies, \\textit{e.g.,} in data formats, map resolution, and semantic annotation types.","To address these challenges, we introduce UniTraj, a comprehensive framework that unifies various datasets, models, and evaluation criteria, presenting new opportunities for the vehicle trajectory prediction field.","In particular, using UniTraj, we conduct extensive experiments and find that model performance significantly drops when transferred to other datasets.","However, enlarging data size and diversity can substantially improve performance, leading to a new state-of-the-art result for the nuScenes dataset.","We provide insights into dataset characteristics to explain these findings.","The code can be found here: \\hyperlink{https://github.com/vita-epfl/UniTraj}{https://github.com/vita-epfl/UniTraj}."],"url":"http://arxiv.org/abs/2403.15098v1","category":"cs.CV"}
{"created":"2024-03-22 10:32:43","title":"Argument-Aware Approach To Event Linking","abstract":"Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize out-of-KB training examples from in-KB instances through controlled manipulation of event arguments. Our experiment across two test datasets showed significant enhancements in both in-KB and out-of-KB scenarios, with a notable 22% improvement in out-of-KB evaluations.","sentences":["Event linking connects event mentions in text with relevant nodes in a knowledge base (KB).","Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events.","Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments.","Moreover, the information-rich nature of events leads to the scarcity of event KBs.","This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention.","In this work, we tackle these challenges by introducing an argument-aware approach.","First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions.","Subsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize out-of-KB training examples from in-KB instances through controlled manipulation of event arguments.","Our experiment across two test datasets showed significant enhancements in both in-KB and out-of-KB scenarios, with a notable 22% improvement in out-of-KB evaluations."],"url":"http://arxiv.org/abs/2403.15097v1","category":"cs.CL"}
{"created":"2024-03-22 10:25:03","title":"Quantum group deformations and quantum $ R $-(co)matrices vs. Quantum Duality Principle","abstract":"In this paper we describe the effect on quantum groups - namely, both QUEA's and QFSHA's - of deformations by twist and by 2-cocycles, showing how such deformations affect the semiclassical limit.   As a second, more important task, we discuss how these deformation procedures can be \"stretched\" to a new extent, via a formal variation of the original recipes, using quasi-twists and quasi-2-cocycles. These recipes seemingly should make no sense at all, yet we prove that they actually work, thus providing well-defined, more general deformation procedures. Later on, we explain the underlying reason that motivates such a result: this comes from the \"Quantum Duality Principle\", through which every \"quasi-twist/2-cocycle\" for a given quantum group can be seen as a standard twist/2-cocycle for another quantum group, associated to the original one via the appropriate Drinfeld functor.   As a third task, we consider standard constructions involving $ R $-(co)matrices in the general theory of Hopf algebras. First we adapt them to quantum groups, then we show that they extend to the case of quasi-$ R $-(co)matrices, and finally we discuss how these constructions interact with the Quantum Duality Principle. As a byproduct, this yields new special symmetries (isomorphisms) for the underlying pair of dual Poisson (formal) groups that one gets by specialization.","sentences":["In this paper we describe the effect on quantum groups - namely, both QUEA's and QFSHA's - of deformations by twist and by 2-cocycles, showing how such deformations affect the semiclassical limit.   ","As a second, more important task, we discuss how these deformation procedures can be \"stretched\" to a new extent, via a formal variation of the original recipes, using quasi-twists and quasi-2-cocycles.","These recipes seemingly should make no sense at all, yet we prove that they actually work, thus providing well-defined, more general deformation procedures.","Later on, we explain the underlying reason that motivates such a result: this comes from the \"Quantum Duality Principle\", through which every \"quasi-twist/2-cocycle\" for a given quantum group can be seen as a standard twist/2-cocycle for another quantum group, associated to the original one via the appropriate Drinfeld functor.   ","As a third task, we consider standard constructions involving $ R $-(co)matrices in the general theory of Hopf algebras.","First we adapt them to quantum groups, then we show that they extend to the case of quasi-$ R $-(co)matrices, and finally we discuss how these constructions interact with the Quantum Duality Principle.","As a byproduct, this yields new special symmetries (isomorphisms) for the underlying pair of dual Poisson (formal) groups that one gets by specialization."],"url":"http://arxiv.org/abs/2403.15096v1","category":"math.QA"}
{"created":"2024-03-22 10:23:48","title":"End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography","abstract":"This paper presents an innovative end-to-end workflow for mineral exploration, integrating ambient noise tomography (ANT) and artificial intelligence (AI) to enhance the discovery and delineation of mineral resources essential for the global transition to a low carbon economy. We focus on copper as a critical element, required in significant quantities for renewable energy solutions. We show the benefits of utilising ANT, characterised by its speed, scalability, depth penetration, resolution, and low environmental impact, alongside artificial intelligence (AI) techniques to refine a continent-scale prospectivity model at the deposit scale by fine-tuning our model on local high-resolution data. We show the promise of the method by first presenting a new data-driven AI prospectivity model for copper within Australia, which serves as our foundation model for further fine-tuning. We then focus on the Hillside IOCG deposit on the prospective Yorke Peninsula. We show that with relatively few local training samples (orebody intercepts), we can fine tune the foundation model to provide a good estimate of the Hillside orebody outline. Our methodology demonstrates how AI can augment geophysical data interpretation, providing a novel approach to mineral exploration with improved decision-making capabilities for targeting mineralization, thereby addressing the urgent need for increased mineral resource discovery.","sentences":["This paper presents an innovative end-to-end workflow for mineral exploration, integrating ambient noise tomography (ANT) and artificial intelligence (AI) to enhance the discovery and delineation of mineral resources essential for the global transition to a low carbon economy.","We focus on copper as a critical element, required in significant quantities for renewable energy solutions.","We show the benefits of utilising ANT, characterised by its speed, scalability, depth penetration, resolution, and low environmental impact, alongside artificial intelligence (AI) techniques to refine a continent-scale prospectivity model at the deposit scale by fine-tuning our model on local high-resolution data.","We show the promise of the method by first presenting a new data-driven AI prospectivity model for copper within Australia, which serves as our foundation model for further fine-tuning.","We then focus on the Hillside IOCG deposit on the prospective Yorke Peninsula.","We show that with relatively few local training samples (orebody intercepts), we can fine tune the foundation model to provide a good estimate of the Hillside orebody outline.","Our methodology demonstrates how AI can augment geophysical data interpretation, providing a novel approach to mineral exploration with improved decision-making capabilities for targeting mineralization, thereby addressing the urgent need for increased mineral resource discovery."],"url":"http://arxiv.org/abs/2403.15095v1","category":"physics.geo-ph"}
{"created":"2024-03-22 10:23:14","title":"Measurements of the production cross-section for a $Z$ boson in association with $b$- or $c$-jets in proton-proton collisions at $\\sqrt{s} = 13$ TeV with the ATLAS detector","abstract":"This paper presents a measurement of the production cross-section of a $Z$ boson in association with $b$- or $c$-jets, in proton-proton collisions at $\\sqrt{s} = 13$ TeV with the ATLAS experiment at the Large Hadron Collider using data corresponding to an integrated luminosity of 140 fb$^{-1}$. Inclusive and differential cross-sections are measured for events containing a $Z$ boson decaying into electrons or muons and produced in association with at least one $b$-jet, at least one $c$-jet, or at least two $b$-jets with transverse momentum $p_\\textrm{T} > 20$ GeV and rapidity $|y| < 2.5$. Predictions from several Monte Carlo generators based on next-to-leading-order matrix elements interfaced with a parton-shower simulation, with different choices of flavour schemes for initial-state partons, are compared with the measured cross-sections. The results are also compared with novel predictions, based on infrared and collinear safe jet flavour dressing algorithms. Selected $Z + \\ge 1 c$-jet observables, optimized for sensitivity to intrinsic-charm, are compared with benchmark models with different intrinsic-charm fractions.","sentences":["This paper presents a measurement of the production cross-section of a $Z$ boson in association with $b$- or $c$-jets, in proton-proton collisions at $\\sqrt{s} = 13$ TeV with the ATLAS experiment at the Large Hadron Collider using data corresponding to an integrated luminosity of 140 fb$^{-1}$. Inclusive and differential cross-sections are measured for events containing a $Z$ boson decaying into electrons or muons and produced in association with at least one $b$-jet, at least one $c$-jet, or at least two $b$-jets with transverse momentum $p_\\textrm{T} > 20$ GeV and rapidity $|y| <","2.5$. Predictions from several Monte Carlo generators based on next-to-leading-order matrix elements interfaced with a parton-shower simulation, with different choices of flavour schemes for initial-state partons, are compared with the measured cross-sections.","The results are also compared with novel predictions, based on infrared and collinear safe jet flavour dressing algorithms.","Selected $Z + \\ge 1 c$-jet observables, optimized for sensitivity to intrinsic-charm, are compared with benchmark models with different intrinsic-charm fractions."],"url":"http://arxiv.org/abs/2403.15093v1","category":"hep-ex"}
{"created":"2024-03-22 10:20:09","title":"Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning","abstract":"Even though Deep Reinforcement Learning (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment. One of the challenges is the lack of a simulation environment that will represent the actual plant as accurately as possible to train DRL policies. Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over long time horizons. One possible reason for the models' incorrect simulation behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the simulation. The compounding error occurs because the model utilizes its predictions as inputs at each time step. The error between the actual data and the prediction accumulates as the simulation continues. We implemented two methods to improve the trained models for wastewater treatment data, which resulted in more accurate simulators: 1- Using the model's prediction data as input in the training step as a tool of correction, and 2- Change in the loss function to consider the long-term predicted shape (dynamics). The experimental results showed that implementing these methods can improve the behavior of simulators in terms of Dynamic Time Warping throughout a year up to 98% compared to the base model. These improvements demonstrate significant promise in creating simulators for biological processes that do not need pre-existing knowledge of the process but instead depend exclusively on time series data obtained from the system.","sentences":["Even though Deep Reinforcement Learning (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment.","One of the challenges is the lack of a simulation environment that will represent the actual plant as accurately as possible to train DRL policies.","Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over long time horizons.","One possible reason for the models' incorrect simulation behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the simulation.","The compounding error occurs because the model utilizes its predictions as inputs at each time step.","The error between the actual data and the prediction accumulates as the simulation continues.","We implemented two methods to improve the trained models for wastewater treatment data, which resulted in more accurate simulators:","1- Using the model's prediction data as input in the training step as a tool of correction, and 2- Change in the loss function to consider the long-term predicted shape (dynamics).","The experimental results showed that implementing these methods can improve the behavior of simulators in terms of Dynamic Time Warping throughout a year up to 98% compared to the base model.","These improvements demonstrate significant promise in creating simulators for biological processes that do not need pre-existing knowledge of the process but instead depend exclusively on time series data obtained from the system."],"url":"http://arxiv.org/abs/2403.15091v1","category":"cs.LG"}
{"created":"2024-03-22 10:19:50","title":"Symmetry, Superposition and Fragmentation in Classical Spin Liquids: A General Framework and Applications to Square Kagome Magnets","abstract":"Classical magnets offer glimpses of quantum-like features like spin liquids and fractionalization, promising an analogous construction of superposition and projective symmetry in classical field theory. While models based on system-specific spin-ice or soft-spin rules exist, a formal theory for general classical magnets remains elusive. Here, we introduce a mutatis mutandis symmetry group construction built from a vector field in a plaquette of classical spins, demonstrating how classical spins superpose in irreducible representations (irreps) of the symmetry group. The corresponding probability amplitudes serve as order parameters and local spins as fragmented excitations. The formalism offers a many-body vector field representation of diverse ground states, including spin liquids and fragmented phases described as degenerate ensembles of irreps. We apply the theory specifically to a frustrated square Kagome lattice, where spin-ice or soft spin rules are inapt, to describe spin liquids and fragmented phases, all validated through irreps ensembles and unbiased Monte Carlo simulation. Our work sheds light on previously unknown aspects of spin-liquid phases and fragmentation and broadens their applications to other branches of field theory.","sentences":["Classical magnets offer glimpses of quantum-like features like spin liquids and fractionalization, promising an analogous construction of superposition and projective symmetry in classical field theory.","While models based on system-specific spin-ice or soft-spin rules exist, a formal theory for general classical magnets remains elusive.","Here, we introduce a mutatis mutandis symmetry group construction built from a vector field in a plaquette of classical spins, demonstrating how classical spins superpose in irreducible representations (irreps) of the symmetry group.","The corresponding probability amplitudes serve as order parameters and local spins as fragmented excitations.","The formalism offers a many-body vector field representation of diverse ground states, including spin liquids and fragmented phases described as degenerate ensembles of irreps.","We apply the theory specifically to a frustrated square Kagome lattice, where spin-ice or soft spin rules are inapt, to describe spin liquids and fragmented phases, all validated through irreps ensembles and unbiased Monte Carlo simulation.","Our work sheds light on previously unknown aspects of spin-liquid phases and fragmentation and broadens their applications to other branches of field theory."],"url":"http://arxiv.org/abs/2403.15090v1","category":"cond-mat.str-el"}
{"created":"2024-03-22 10:15:53","title":"IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence","abstract":"Training a computer vision system to segment a novel class typically requires collecting and painstakingly annotating lots of images with objects from that class. Few-shot segmentation techniques reduce the required number of images to learn to segment a new class, but careful annotations of object boundaries are still required. On the other hand, interactive segmentation techniques only focus on incrementally improving the segmentation of one object at a time (typically, using clicks given by an expert) in a class-agnostic manner. We combine the two concepts to drastically reduce the effort required to train segmentation models for novel classes. Instead of trivially feeding interactive segmentation masks as ground truth to a few-shot segmentation model, we propose IFSENet, which can accept sparse supervision on a single or few support images in the form of clicks to generate masks on support (training, at least clicked upon once) as well as query (test, never clicked upon) images. To trade-off effort for accuracy flexibly, the number of images and clicks can be incrementally added to the support set to further improve the segmentation of support as well as query images. The proposed model approaches the accuracy of previous state-of-the-art few-shot segmentation models with considerably lower annotation effort (clicks instead of maps), when tested on Pascal and SBD datasets on query images. It also works well as an interactive segmentation method on support images.","sentences":["Training a computer vision system to segment a novel class typically requires collecting and painstakingly annotating lots of images with objects from that class.","Few-shot segmentation techniques reduce the required number of images to learn to segment a new class, but careful annotations of object boundaries are still required.","On the other hand, interactive segmentation techniques only focus on incrementally improving the segmentation of one object at a time (typically, using clicks given by an expert) in a class-agnostic manner.","We combine the two concepts to drastically reduce the effort required to train segmentation models for novel classes.","Instead of trivially feeding interactive segmentation masks as ground truth to a few-shot segmentation model, we propose IFSENet, which can accept sparse supervision on a single or few support images in the form of clicks to generate masks on support (training, at least clicked upon once) as well as query (test, never clicked upon) images.","To trade-off effort for accuracy flexibly, the number of images and clicks can be incrementally added to the support set to further improve the segmentation of support as well as query images.","The proposed model approaches the accuracy of previous state-of-the-art few-shot segmentation models with considerably lower annotation effort (clicks instead of maps), when tested on Pascal and SBD datasets on query images.","It also works well as an interactive segmentation method on support images."],"url":"http://arxiv.org/abs/2403.15089v1","category":"cs.CV"}
{"created":"2024-03-22 10:11:16","title":"On alternating sum formulas for the Arakawa-Kaneko multiple zeta values","abstract":"In this paper, we construct generating functions of alternating sums for the Arakawa-Kaneko zeta values. From the expressions, we show alternating sum formulas for them. Based on these results, we apply the same method to other zeta values.","sentences":["In this paper, we construct generating functions of alternating sums for the Arakawa-Kaneko zeta values.","From the expressions, we show alternating sum formulas for them.","Based on these results, we apply the same method to other zeta values."],"url":"http://arxiv.org/abs/2403.15086v1","category":"math.NT"}
{"created":"2024-03-22 10:06:31","title":"Cell Variational Information Bottleneck Network","abstract":"In this work, we propose Cell Variational Information Bottleneck Network (cellVIB), a convolutional neural network using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method. Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty. As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB. Under each VIB cell, the feedforward process learns an independent mean term and an standard deviation term, and predicts the Gaussian distribution based on them. The feedback process is based on reparameterization trick for effective training. This work performs an extensive analysis on MNIST dataset to verify the effectiveness of each VIB cells, and provides an insightful analysis on how the VIB cells affect mutual information. Experiments conducted on CIFAR-10 also prove that our cellVIB is robust against noisy labels during training and against corrupted images during testing. Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model. Finally, in a more complex representation learning task, face recognition, our network structure has also achieved very competitive results.","sentences":["In this work, we propose Cell Variational Information Bottleneck Network (cellVIB), a convolutional neural network using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method.","Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty.","As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB.","Under each VIB cell, the feedforward process learns an independent mean term and an standard deviation term, and predicts the Gaussian distribution based on them.","The feedback process is based on reparameterization trick for effective training.","This work performs an extensive analysis on MNIST dataset to verify the effectiveness of each VIB cells, and provides an insightful analysis on how the VIB cells affect mutual information.","Experiments conducted on CIFAR-10 also prove that our cellVIB is robust against noisy labels during training and against corrupted images during testing.","Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model.","Finally, in a more complex representation learning task, face recognition, our network structure has also achieved very competitive results."],"url":"http://arxiv.org/abs/2403.15082v1","category":"cs.CV"}
{"created":"2024-03-22 10:05:21","title":"Automated Feature Selection for Inverse Reinforcement Learning","abstract":"Inverse reinforcement learning (IRL) is an imitation learning approach to learning reward functions from expert demonstrations. Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of reinforcement learning. In IRL, the reward is usually represented as a linear combination of features. In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general. To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions. Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations. We demonstrate the approach's effectiveness by recovering reward functions that capture expert policies across non-linear control tasks of increasing complexity. Code, data, and videos are available at https://sites.google.com/view/feature4irl.","sentences":["Inverse reinforcement learning (IRL) is an imitation learning approach to learning reward functions from expert demonstrations.","Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of reinforcement learning.","In IRL, the reward is usually represented as a linear combination of features.","In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general.","To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions.","Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations.","We demonstrate the approach's effectiveness by recovering reward functions that capture expert policies across non-linear control tasks of increasing complexity.","Code, data, and videos are available at https://sites.google.com/view/feature4irl."],"url":"http://arxiv.org/abs/2403.15079v1","category":"cs.LG"}
{"created":"2024-03-22 10:02:13","title":"GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks","abstract":"Graph Neural Networks (GNN) have emerged as a popular and standard approach for learning from graph-structured data. The literature on GNN highlights the potential of this evolving research area and its widespread adoption in real-life applications. However, most of the approaches are either new in concept or derived from specific techniques. Therefore, the potential of more than one approach in hybrid form has not been studied extensively, which can be well utilized for sequenced data or static data together. We derive a hybrid approach based on two established techniques as generalized aggregation networks and topology adaptive graph convolution networks that solve our purpose to apply on both types of sequenced and static nature of data, effectively. The proposed method applies to both node and graph classification. Our empirical analysis reveals that the results are at par with literature results and better for handwritten strokes as sequenced data, where graph structures have not been explored.","sentences":["Graph Neural Networks (GNN) have emerged as a popular and standard approach for learning from graph-structured data.","The literature on GNN highlights the potential of this evolving research area and its widespread adoption in real-life applications.","However, most of the approaches are either new in concept or derived from specific techniques.","Therefore, the potential of more than one approach in hybrid form has not been studied extensively, which can be well utilized for sequenced data or static data together.","We derive a hybrid approach based on two established techniques as generalized aggregation networks and topology adaptive graph convolution networks that solve our purpose to apply on both types of sequenced and static nature of data, effectively.","The proposed method applies to both node and graph classification.","Our empirical analysis reveals that the results are at par with literature results and better for handwritten strokes as sequenced data, where graph structures have not been explored."],"url":"http://arxiv.org/abs/2403.15077v1","category":"cs.LG"}
{"created":"2024-03-22 10:00:52","title":"Comprehensive Lipidomic Automation Workflow using Large Language Models","abstract":"Lipidomics generates large data that makes manual annotation and interpretation challenging. Lipid chemical and structural diversity with structural isomers further complicates annotation. Although, several commercial and open-source software for targeted lipid identification exists, it lacks automated method generation workflows and integration with statistical and bioinformatics tools. We have developed the Comprehensive Lipidomic Automated Workflow (CLAW) platform with integrated workflow for parsing, detailed statistical analysis and lipid annotations based on custom multiple reaction monitoring (MRM) precursor and product ion pair transitions. CLAW contains several modules including identification of carbon-carbon double bond position(s) in unsaturated lipids when combined with ozone electrospray ionization (OzESI)-MRM methodology. To demonstrate the utility of the automated workflow in CLAW, large-scale lipidomics data was collected with traditional and OzESI-MRM profiling on biological and non-biological samples. Specifically, a total of 1497 transitions organized into 10 MRM-based mass spectrometry methods were used to profile lipid droplets isolated from different brain regions of 18-24 month-old Alzheimer's disease mice and age-matched wild-type controls. Additionally, triacyclglycerols (TGs) profiles with carbon-carbon double bond specificity were generated from canola oil samples using OzESI-MRM profiling. We also developed an integrated language user interface with large language models using artificially intelligent (AI) agents that permits users to interact with the CLAW platform using a chatbot terminal to perform statistical and bioinformatic analyses. We envision CLAW pipeline to be used in high-throughput lipid structural identification tasks aiding users to generate automated lipidomics workflows ranging from data acquisition to AI agent-based bioinformatic analysis.","sentences":["Lipidomics generates large data that makes manual annotation and interpretation challenging.","Lipid chemical and structural diversity with structural isomers further complicates annotation.","Although, several commercial and open-source software for targeted lipid identification exists, it lacks automated method generation workflows and integration with statistical and bioinformatics tools.","We have developed the Comprehensive Lipidomic Automated Workflow (CLAW) platform with integrated workflow for parsing, detailed statistical analysis and lipid annotations based on custom multiple reaction monitoring (MRM) precursor and product ion pair transitions.","CLAW contains several modules including identification of carbon-carbon double bond position(s) in unsaturated lipids when combined with ozone electrospray ionization (OzESI)-MRM methodology.","To demonstrate the utility of the automated workflow in CLAW, large-scale lipidomics data was collected with traditional and OzESI-MRM profiling on biological and non-biological samples.","Specifically, a total of 1497 transitions organized into 10 MRM-based mass spectrometry methods were used to profile lipid droplets isolated from different brain regions of 18-24 month-old Alzheimer's disease mice and age-matched wild-type controls.","Additionally, triacyclglycerols (TGs) profiles with carbon-carbon double bond specificity were generated from canola oil samples using OzESI-MRM profiling.","We also developed an integrated language user interface with large language models using artificially intelligent (AI) agents that permits users to interact with the CLAW platform using a chatbot terminal to perform statistical and bioinformatic analyses.","We envision CLAW pipeline to be used in high-throughput lipid structural identification tasks aiding users to generate automated lipidomics workflows ranging from data acquisition to AI agent-based bioinformatic analysis."],"url":"http://arxiv.org/abs/2403.15076v1","category":"q-bio.QM"}
{"created":"2024-03-22 09:58:33","title":"Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation","abstract":"Recent methods utilize graph contrastive Learning within graph-structured user-item interaction data for collaborative filtering and have demonstrated their efficacy in recommendation tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of graphs on bilateral nodes to be different after multi-hop graph interaction calculation, which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for recommendation tasks called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item graph reasoning better with bilateral slicing contrastive training. Especially, taking into account the aggregation ability of hypergraph-based graph convolutional network (GCN) in digging implicit similarities is more suitable for user nodes, embeddings generated from three different modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into two subviews by the user- and item-side respectively, and selectively combined into subview pairs bilaterally based on the characteristics of inter-node relation structure. Furthermore, to align the distribution of user and item embeddings after aggregation, a dispersing loss is leveraged to adjust the mutual distance between all embeddings for maintaining learning ability. Comprehensive experiments on two public datasets have proved the superiority of BusGCL in comparison to various recommendation methods. Other models can simply utilize our bilateral slicing contrastive learning to enhance recommending performance without incurring extra expenses.","sentences":["Recent methods utilize graph contrastive Learning within graph-structured user-item interaction data for collaborative filtering and have demonstrated their efficacy in recommendation tasks.","However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of graphs on bilateral nodes to be different after multi-hop graph interaction calculation, which limits existing models to achieve ideal results.","To solve this issue, we propose a novel framework for recommendation tasks called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item graph reasoning better with bilateral slicing contrastive training.","Especially, taking into account the aggregation ability of hypergraph-based graph convolutional network (GCN) in digging implicit similarities is more suitable for user nodes, embeddings generated from three different modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into two subviews by the user- and item-side respectively, and selectively combined into subview pairs bilaterally based on the characteristics of inter-node relation structure.","Furthermore, to align the distribution of user and item embeddings after aggregation, a dispersing loss is leveraged to adjust the mutual distance between all embeddings for maintaining learning ability.","Comprehensive experiments on two public datasets have proved the superiority of BusGCL in comparison to various recommendation methods.","Other models can simply utilize our bilateral slicing contrastive learning to enhance recommending performance without incurring extra expenses."],"url":"http://arxiv.org/abs/2403.15075v1","category":"cs.IR"}
{"created":"2024-03-22 09:54:18","title":"A Taxmans guide to taxation of crypto assets","abstract":"The Financial system has witnessed rapid technological changes. The rise of Bitcoin and other crypto assets based on Distributed Ledger Technology mark a fundamental change in the way people transact and transmit value over a decentralized network, spread across geographies. This has created regulatory and tax policy blind spots, as governments and tax administrations take time to understand and provide policy responses to this innovative, revolutionary, and fast-paced technology. Due to the breakneck speed of innovation in blockchain technology and advent of Decentralized Finance, Decentralized Autonomous Organizations and the Metaverse, it is unlikely that the policy interventions and guidance by regulatory authorities or tax administrations would be ahead or in sync with the pace of innovation. This paper tries to explain the principles on which crypto assets function, their underlying technology and relates them to the tax issues and taxable events which arise within this ecosystem. It also provides instances of tax and regulatory policy responses already in effect in various jurisdictions, including the recent changes in reporting standards by the FATF and the OECD. This paper tries to explain the rationale behind existing laws and policies and the challenges in their implementation. It also attempts to present a ballpark estimate of tax potential of this asset class and suggests creation of global public digital infrastructure that can address issues related to pseudonymity and extra-territoriality. The paper analyses both direct and indirect taxation issues related to crypto assets and discusses more recent aspects like proof-of-stake and maximal extractable value in greater detail.","sentences":["The Financial system has witnessed rapid technological changes.","The rise of Bitcoin and other crypto assets based on Distributed Ledger Technology mark a fundamental change in the way people transact and transmit value over a decentralized network, spread across geographies.","This has created regulatory and tax policy blind spots, as governments and tax administrations take time to understand and provide policy responses to this innovative, revolutionary, and fast-paced technology.","Due to the breakneck speed of innovation in blockchain technology and advent of Decentralized Finance, Decentralized Autonomous Organizations and the Metaverse, it is unlikely that the policy interventions and guidance by regulatory authorities or tax administrations would be ahead or in sync with the pace of innovation.","This paper tries to explain the principles on which crypto assets function, their underlying technology and relates them to the tax issues and taxable events which arise within this ecosystem.","It also provides instances of tax and regulatory policy responses already in effect in various jurisdictions, including the recent changes in reporting standards by the FATF and the OECD.","This paper tries to explain the rationale behind existing laws and policies and the challenges in their implementation.","It also attempts to present a ballpark estimate of tax potential of this asset class and suggests creation of global public digital infrastructure that can address issues related to pseudonymity and extra-territoriality.","The paper analyses both direct and indirect taxation issues related to crypto assets and discusses more recent aspects like proof-of-stake and maximal extractable value in greater detail."],"url":"http://arxiv.org/abs/2403.15074v1","category":"q-fin.GN"}
{"created":"2024-03-22 09:50:55","title":"Gauge invariant variational formulations of electromagnetic gyrokinetic theory","abstract":"The use of gyrokinetics, wherein phase-space coordinate transformations result in a phase-space dimensionality reduction as well as removal of fast time scales, has enabled the simulation of microturbulence in fusion devices. The state-of-the-art gyrokinetic models used in practice are parallel-only models wherein the perpendicular part of the vector potential is neglected. Such models are inherently not gauge invariant. We generalize the work of [Burby, Brizard. Physics Letters A, 383(18):2172-2175] by deriving a sufficient condition on the gyrocentre coordinate transformation which ensures gauge invariance. This leads to a parametrized family of gyrokinetic models for which we motivate a specific choice of parameters that results in a physically meaningful gauge invariant model. Due to gauge invariance this model can be expressed directly in terms of the electromagnetic fields, rather than the potentials, and the gyrokinetic model thereby results in the macroscopic Maxwell's equations. For the linearized model, it is demonstrated that the shear and compressional Alfv\\'en waves are present with the correct frequencies. The fast compressional Alfv\\'en wave can be removed by making use of a Darwin-like approximation. This approximation breaks gauge invariance, but we find that the field equations are still compatible without the need of a Lagrange multiplier.","sentences":["The use of gyrokinetics, wherein phase-space coordinate transformations result in a phase-space dimensionality reduction as well as removal of fast time scales, has enabled the simulation of microturbulence in fusion devices.","The state-of-the-art gyrokinetic models used in practice are parallel-only models wherein the perpendicular part of the vector potential is neglected.","Such models are inherently not gauge invariant.","We generalize the work of [Burby, Brizard.","Physics Letters A, 383(18):2172-2175] by deriving a sufficient condition on the gyrocentre coordinate transformation which ensures gauge invariance.","This leads to a parametrized family of gyrokinetic models for which we motivate a specific choice of parameters that results in a physically meaningful gauge invariant model.","Due to gauge invariance this model can be expressed directly in terms of the electromagnetic fields, rather than the potentials, and the gyrokinetic model thereby results in the macroscopic Maxwell's equations.","For the linearized model, it is demonstrated that the shear and compressional Alfv\\'en waves are present with the correct frequencies.","The fast compressional Alfv\\'en wave can be removed by making use of a Darwin-like approximation.","This approximation breaks gauge invariance, but we find that the field equations are still compatible without the need of a Lagrange multiplier."],"url":"http://arxiv.org/abs/2403.15071v1","category":"physics.plasm-ph"}
{"created":"2024-03-22 09:50:08","title":"Uncovering Bound States in the Continuum in InSb nanowire networks","abstract":"Bound states in the continuum (BICs) are exotic, localized states even though their energy lies in the continuum spectra. Since its discovery in 1929, the quest to unveil these exotic states in charge transport experiments remains an active pursuit in condensed matter physics. Here, we study charge transport in InSb nanowire networks in the ballistic regime and subject to a perpendicular magnetic field as ideal candidates to observe and control the appearance of BICs. We find that BICs reveal themselves as distinctive resonances or antiresonances in the conductance by varying the applied magnetic field and the Fermi energy. We systematically consider different lead connections in hashtag-like nanowire networks, finding the optimal configuration that enhances the features associated with the emergence of BICs. Finally, the investigation focuses on the effect of the Rashba spin-orbit interaction of InSb on the occurrence of BICs in nanowire networks. While the interaction generally plays a detrimental role in the signatures of the BICs in the conductance of the nanowire networks, it opens the possibility to operate these nanostructures as spin filters for spintronics. We believe that this work could pave the way for the unambiguous observation of BICs in charge transport experiments and for the development of advanced spintronic devices.","sentences":["Bound states in the continuum (BICs) are exotic, localized states even though their energy lies in the continuum spectra.","Since its discovery in 1929, the quest to unveil these exotic states in charge transport experiments remains an active pursuit in condensed matter physics.","Here, we study charge transport in InSb nanowire networks in the ballistic regime and subject to a perpendicular magnetic field as ideal candidates to observe and control the appearance of BICs.","We find that BICs reveal themselves as distinctive resonances or antiresonances in the conductance by varying the applied magnetic field and the Fermi energy.","We systematically consider different lead connections in hashtag-like nanowire networks, finding the optimal configuration that enhances the features associated with the emergence of BICs.","Finally, the investigation focuses on the effect of the Rashba spin-orbit interaction of InSb on the occurrence of BICs in nanowire networks.","While the interaction generally plays a detrimental role in the signatures of the BICs in the conductance of the nanowire networks, it opens the possibility to operate these nanostructures as spin filters for spintronics.","We believe that this work could pave the way for the unambiguous observation of BICs in charge transport experiments and for the development of advanced spintronic devices."],"url":"http://arxiv.org/abs/2403.15070v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 09:46:58","title":"Unitary-invariant witnesses of quantum imaginarity","abstract":"Quantum theory is traditionally formulated using complex numbers. This imaginarity of quantum theory has been quantified as a resource with applications in discrimination tasks, pseudorandomness generation, and quantum metrology. Here we propose witnesses for imaginarity that are basis-independent, relying on measurements of unitary-invariant properties of sets of states. For 3 pure states, we completely characterize the invariant values attainable by quantum theory, and give a partial characterization for 4 pure states. We show that simple pairwise overlap measurements suffice to witness imaginarity of sets of 4 states, but not for sets of 3. Our witnesses are experimentally friendly, opening up a new path for measuring and using imaginarity as a resource.","sentences":["Quantum theory is traditionally formulated using complex numbers.","This imaginarity of quantum theory has been quantified as a resource with applications in discrimination tasks, pseudorandomness generation, and quantum metrology.","Here we propose witnesses for imaginarity that are basis-independent, relying on measurements of unitary-invariant properties of sets of states.","For 3 pure states, we completely characterize the invariant values attainable by quantum theory, and give a partial characterization for 4 pure states.","We show that simple pairwise overlap measurements suffice to witness imaginarity of sets of 4 states, but not for sets of 3.","Our witnesses are experimentally friendly, opening up a new path for measuring and using imaginarity as a resource."],"url":"http://arxiv.org/abs/2403.15066v1","category":"quant-ph"}
{"created":"2024-03-22 09:46:30","title":"Testing for Fault Diversity in Reinforcement Learning","abstract":"Reinforcement Learning is the premier technique to approach sequential decision problems, including complex tasks such as driving cars and landing spacecraft. Among the software validation and verification practices, testing for functional fault detection is a convenient way to build trustworthiness in the learned decision model. While recent works seek to maximise the number of detected faults, none consider fault characterisation during the search for more diversity. We argue that policy testing should not find as many failures as possible (e.g., inputs that trigger similar car crashes) but rather aim at revealing as informative and diverse faults as possible in the model. In this paper, we explore the use of quality diversity optimisation to solve the problem of fault diversity in policy testing. Quality diversity (QD) optimisation is a type of evolutionary algorithm to solve hard combinatorial optimisation problems where high-quality diverse solutions are sought. We define and address the underlying challenges of adapting QD optimisation to the test of action policies. Furthermore, we compare classical QD optimisers to state-of-the-art frameworks dedicated to policy testing, both in terms of search efficiency and fault diversity. We show that QD optimisation, while being conceptually simple and generally applicable, finds effectively more diverse faults in the decision model, and conclude that QD-based policy testing is a promising approach.","sentences":["Reinforcement Learning is the premier technique to approach sequential decision problems, including complex tasks such as driving cars and landing spacecraft.","Among the software validation and verification practices, testing for functional fault detection is a convenient way to build trustworthiness in the learned decision model.","While recent works seek to maximise the number of detected faults, none consider fault characterisation during the search for more diversity.","We argue that policy testing should not find as many failures as possible (e.g., inputs that trigger similar car crashes) but rather aim at revealing as informative and diverse faults as possible in the model.","In this paper, we explore the use of quality diversity optimisation to solve the problem of fault diversity in policy testing.","Quality diversity (QD) optimisation is a type of evolutionary algorithm to solve hard combinatorial optimisation problems where high-quality diverse solutions are sought.","We define and address the underlying challenges of adapting QD optimisation to the test of action policies.","Furthermore, we compare classical QD optimisers to state-of-the-art frameworks dedicated to policy testing, both in terms of search efficiency and fault diversity.","We show that QD optimisation, while being conceptually simple and generally applicable, finds effectively more diverse faults in the decision model, and conclude that QD-based policy testing is a promising approach."],"url":"http://arxiv.org/abs/2403.15065v1","category":"cs.SE"}
{"created":"2024-03-22 09:46:11","title":"Recent Trends in 3D Reconstruction of General Non-Rigid Scenes","abstract":"Reconstructing models of the real world, including 3D geometry, appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem. This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is concluded with a discussion of the remaining limitations and open challenges.","sentences":["Reconstructing models of the real world, including 3D geometry, appearance, and motion of real scenes, is essential for computer graphics and computer vision.","It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications.","It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes.","Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world.","Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem.","This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions.","The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling.","More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications.","The STAR is concluded with a discussion of the remaining limitations and open challenges."],"url":"http://arxiv.org/abs/2403.15064v1","category":"cs.CV"}
{"created":"2024-03-22 09:40:52","title":"Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans","abstract":"Segment anything model (SAM) demonstrates strong generalization ability on natural image segmentation. However, its direct adaption in medical image segmentation tasks shows significant performance drops with inferior accuracy and unstable results. It may also requires an excessive number of prompt points to obtain a reasonable accuracy. For segmenting 3D radiological CT or MRI scans, a 2D SAM model has to separately handle hundreds of 2D slices. Although quite a few studies explore adapting SAM into medical image volumes, the efficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods only capable of segmenting specific organs/tumors. In this work, we propose a comprehensive and scalable 3D SAM model for whole-body CT segmentation, named CT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation model using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively, ensuring the model's accurate responses to higher-dimensional spatial prompts is crucial, and 3D patch-wise training is required due to GPU memory constraints. For this purpose, we propose two key technical developments: 1) a progressively and spatially aligned prompt encoding method to effectively encode click prompts in local 3D space; and 2) a cross-patch prompt learning scheme to capture more 3D spatial context, which is beneficial for reducing the editing workloads when interactively prompting on large organs. CT-SAM3D is trained and validated using a curated dataset of 1204 CT scans containing 107 whole-body anatomies, reporting significantly better quantitative performance against all previous SAM-derived models by a large margin with much fewer click prompts. Our model can handle segmenting unseen organ as well. Code, data, and our 3D interactive segmentation tool with quasi-real-time responses will be made publicly available.","sentences":["Segment anything model (SAM) demonstrates strong generalization ability on natural image segmentation.","However, its direct adaption in medical image segmentation tasks shows significant performance drops with inferior accuracy and unstable results.","It may also requires an excessive number of prompt points to obtain a reasonable accuracy.","For segmenting 3D radiological CT or MRI scans, a 2D SAM model has to separately handle hundreds of 2D slices.","Although quite a few studies explore adapting SAM into medical image volumes, the efficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods only capable of segmenting specific organs/tumors.","In this work, we propose a comprehensive and scalable 3D SAM model for whole-body CT segmentation, named CT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation model using a (nearly) fully labeled CT dataset.","To train CT-SAM3D effectively, ensuring the model's accurate responses to higher-dimensional spatial prompts is crucial, and 3D patch-wise training is required due to GPU memory constraints.","For this purpose, we propose two key technical developments: 1) a progressively and spatially aligned prompt encoding method to effectively encode click prompts in local 3D space; and 2) a cross-patch prompt learning scheme to capture more 3D spatial context, which is beneficial for reducing the editing workloads when interactively prompting on large organs.","CT-SAM3D is trained and validated using a curated dataset of 1204 CT scans containing 107 whole-body anatomies, reporting significantly better quantitative performance against all previous SAM-derived models by a large margin with much fewer click prompts.","Our model can handle segmenting unseen organ as well.","Code, data, and our 3D interactive segmentation tool with quasi-real-time responses will be made publicly available."],"url":"http://arxiv.org/abs/2403.15063v1","category":"cs.CV"}
{"created":"2024-03-22 09:36:29","title":"Interference patterns of propagating spin wave in spin Hall oscillator arrays","abstract":"In this study, we discuss the observation of spin wave interference generated by magnetic oscillators. We employ micromagnetic simulations for two coherent spin Hall nanowire oscillators positioned nearby, horizontally or vertically. The two nanowires produce circular waves with short wavelengths on the order of 100 nm, which interfere with each other. In the horizontal configuration, the spin waves exhibit constructive and destructive fringes, indicating amplification or cancellation of the amplitudes, respectively. The synchronization of spin waves in the current geometry of the two nanowires is facilitated by the combination of dipolar field and propagating spin waves. Additionally, the vertical alignment results in standing spin waves characterized by multiple antinodes and nodes. These observations are interpreted using a wave model that incorporates the superposition principle for each case.","sentences":["In this study, we discuss the observation of spin wave interference generated by magnetic oscillators.","We employ micromagnetic simulations for two coherent spin Hall nanowire oscillators positioned nearby, horizontally or vertically.","The two nanowires produce circular waves with short wavelengths on the order of 100 nm, which interfere with each other.","In the horizontal configuration, the spin waves exhibit constructive and destructive fringes, indicating amplification or cancellation of the amplitudes, respectively.","The synchronization of spin waves in the current geometry of the two nanowires is facilitated by the combination of dipolar field and propagating spin waves.","Additionally, the vertical alignment results in standing spin waves characterized by multiple antinodes and nodes.","These observations are interpreted using a wave model that incorporates the superposition principle for each case."],"url":"http://arxiv.org/abs/2403.15060v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 09:32:31","title":"MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration","abstract":"Recent advances in tuning-free personalized image generation based on diffusion models are impressive. However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddings, both of which are efficiently integrated into the diffusion model through the well-designed multimodal cross-attention mechanism. Additionally, MM-Diff introduces cross-attention map constraints during the training phase, ensuring flexible multi-subject image sampling during inference without any predefined inputs (e.g., layout). Extensive experiments demonstrate the superior performance of MM-Diff over other leading methods.","sentences":["Recent advances in tuning-free personalized image generation based on diffusion models are impressive.","However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency.","Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism.","In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds.","Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings.","CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddings, both of which are efficiently integrated into the diffusion model through the well-designed multimodal cross-attention mechanism.","Additionally, MM-Diff introduces cross-attention map constraints during the training phase, ensuring flexible multi-subject image sampling during inference without any predefined inputs (e.g., layout).","Extensive experiments demonstrate the superior performance of MM-Diff over other leading methods."],"url":"http://arxiv.org/abs/2403.15059v1","category":"cs.CV"}
{"created":"2024-03-22 09:30:27","title":"Stable multivariate Narayana polynomials and labeled plane trees","abstract":"In this paper, we introduce stable multivariate generalizations of Narayana polynomials of type A and type B. Our polynomials are multivariate generating polynomials over labeled plane trees and can be generated by a grammatical labeling based on a context-free grammar. Our proof of real stability uses a characterization of stable-preserving linear operators due to Borcea and Br\\\"and\\'en.","sentences":["In this paper, we introduce stable multivariate generalizations of Narayana polynomials of type A and type B.","Our polynomials are multivariate generating polynomials over labeled plane trees and can be generated by a grammatical labeling based on a context-free grammar.","Our proof of real stability uses a characterization of stable-preserving linear operators due to Borcea and Br\\\"and\\'en."],"url":"http://arxiv.org/abs/2403.15058v1","category":"math.CO"}
{"created":"2024-03-22 09:27:34","title":"Optimal control of gradient flows via the Weighted Energy-Dissipation method","abstract":"We consider a general optimal control problem in the setting of gradient flows. Two approximations of the problem are presented, both relying on the variational reformulation of gradient-flow dynamics via the Weighted-Energy-Dissipation variational approach. This consists in the minimization of global-in-time functionals over trajectories, combined with a limit passage. We show that the original nonpenalized problem and the two successive approximations admits solutions. Moreover, resorting to a $\\Gamma$-convergence analysis we show that penalised optimal controls converge to nonpenalized one as the approximation is removed.","sentences":["We consider a general optimal control problem in the setting of gradient flows.","Two approximations of the problem are presented, both relying on the variational reformulation of gradient-flow dynamics via the Weighted-Energy-Dissipation variational approach.","This consists in the minimization of global-in-time functionals over trajectories, combined with a limit passage.","We show that the original nonpenalized problem and the two successive approximations admits solutions.","Moreover, resorting to a $\\Gamma$-convergence analysis we show that penalised optimal controls converge to nonpenalized one as the approximation is removed."],"url":"http://arxiv.org/abs/2403.15055v1","category":"math.OC"}
{"created":"2024-03-22 09:26:52","title":"Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping","abstract":"Robotic grasping is a primitive skill for complex tasks and is fundamental to intelligence. For general 6-Dof grasping, most previous methods directly extract scene-level semantic or geometric information, while few of them consider the suitability for various downstream applications, such as target-oriented grasping. Addressing this issue, we rethink 6-Dof grasp detection from a grasp-centric view and propose a versatile grasp framework capable of handling both scene-level and target-oriented grasping. Our framework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp Model. Specifically, the Flexible Guidance Module is compatible with both global (e.g., grasp heatmap) and local (e.g., visual grounding) guidance, enabling the generation of high-quality grasps across various tasks. The Local Grasp Model focuses on object-agnostic regional points and predicts grasps locally and intently. Experiment results reveal that our framework achieves over 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset. Furthermore, real-world robotic tests in three distinct settings yield a 95% success rate.","sentences":["Robotic grasping is a primitive skill for complex tasks and is fundamental to intelligence.","For general 6-Dof grasping, most previous methods directly extract scene-level semantic or geometric information, while few of them consider the suitability for various downstream applications, such as target-oriented grasping.","Addressing this issue, we rethink 6-Dof grasp detection from a grasp-centric view and propose a versatile grasp framework capable of handling both scene-level and target-oriented grasping.","Our framework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp Model.","Specifically, the Flexible Guidance Module is compatible with both global (e.g., grasp heatmap) and local (e.g., visual grounding) guidance, enabling the generation of high-quality grasps across various tasks.","The Local Grasp Model focuses on object-agnostic regional points and predicts grasps locally and intently.","Experiment results reveal that our framework achieves over 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset.","Furthermore, real-world robotic tests in three distinct settings yield a 95% success rate."],"url":"http://arxiv.org/abs/2403.15054v1","category":"cs.RO"}
{"created":"2024-03-22 09:26:11","title":"On certain Fibonacci representations","abstract":"One of the most popular and studied recursive series is the Fibonacci sequence. It is challenging to see how Fibonacci numbers can be used to generate other recursive sequences. In our article, we describe some families of integer recurrence sequences as rational polynomial linear combinations of Fibonacci numbers.","sentences":["One of the most popular and studied recursive series is the Fibonacci sequence.","It is challenging to see how Fibonacci numbers can be used to generate other recursive sequences.","In our article, we describe some families of integer recurrence sequences as rational polynomial linear combinations of Fibonacci numbers."],"url":"http://arxiv.org/abs/2403.15053v1","category":"math.NT"}
{"created":"2024-03-22 09:15:36","title":"Continual Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based methods for CVLN, Perplexity Replay (PerpR) and Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging episodes based on action perplexity, while ESR replays previously predicted action logits to preserve learned behaviors. We demonstrate the effectiveness of the proposed methods on CVLN through extensive experiments.","sentences":["Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe.","Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge.","This makes it difficult to train VLN agents that operate in the ever-changing real world.","To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process.","For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents.","Furthermore, we propose two novel rehearsal-based methods for CVLN, Perplexity Replay (PerpR) and Episodic Self-Replay (ESR).","PerpR prioritizes replaying challenging episodes based on action perplexity, while ESR replays previously predicted action logits to preserve learned behaviors.","We demonstrate the effectiveness of the proposed methods on CVLN through extensive experiments."],"url":"http://arxiv.org/abs/2403.15049v1","category":"cs.CV"}
{"created":"2024-03-22 09:13:09","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","abstract":"Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.","sentences":["Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields.","However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters.","We propose a novel visual hallucination detection system for cartoon character images generated by TTI models.","Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information.","By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions.","Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images.","This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains."],"url":"http://arxiv.org/abs/2403.15048v1","category":"cs.CV"}
{"created":"2024-03-22 09:10:12","title":"Atom Number Fluctuations in Bose Gases -- Statistical analysis of parameter estimation","abstract":"The investigation of the fluctuations in interacting quantum systems at finite temperatures showcases the ongoing challenges in understanding complex quantum systems. Recently, atom number fluctuations in weakly interacting Bose-Einstein condensates were observed, motivating an investigation of the thermal component of partially condensed Bose gases. Here, we present a combined analysis of both components, revealing the presence of fluctuations in the thermal component. This analysis includes a comprehensive statistical evaluation of uncertainties in the preparation and parameter estimation of partially condensed Bose gases. Using Monte Carlo simulations of optical density profiles, we estimate the noise contributions to the atom number and temperature estimation of the condensed and thermal cloud, which is generally applicable in the field of ultracold atoms. Furthermore, we investigate the specific noise contributions in the analysis of atom number fluctuations and show that preparation noise in the total atom number leads to an important technical noise contribution. Subtracting all known noise contributions from the variance of the atom number in the BEC and thermal component allows us to improve the estimate of the fundamental peak fluctuations.","sentences":["The investigation of the fluctuations in interacting quantum systems at finite temperatures showcases the ongoing challenges in understanding complex quantum systems.","Recently, atom number fluctuations in weakly interacting Bose-Einstein condensates were observed, motivating an investigation of the thermal component of partially condensed Bose gases.","Here, we present a combined analysis of both components, revealing the presence of fluctuations in the thermal component.","This analysis includes a comprehensive statistical evaluation of uncertainties in the preparation and parameter estimation of partially condensed Bose gases.","Using Monte Carlo simulations of optical density profiles, we estimate the noise contributions to the atom number and temperature estimation of the condensed and thermal cloud, which is generally applicable in the field of ultracold atoms.","Furthermore, we investigate the specific noise contributions in the analysis of atom number fluctuations and show that preparation noise in the total atom number leads to an important technical noise contribution.","Subtracting all known noise contributions from the variance of the atom number in the BEC and thermal component allows us to improve the estimate of the fundamental peak fluctuations."],"url":"http://arxiv.org/abs/2403.15047v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-22 09:02:12","title":"DP-Dueling: Learning from Preference Feedback without Compromising User Privacy","abstract":"We consider the well-studied dueling bandit problem, where a learner aims to identify near-optimal actions using pairwise comparisons, under the constraint of differential privacy. We consider a general class of utility-based preference matrices for large (potentially unbounded) decision spaces and give the first differentially private dueling bandit algorithm for active learning with user preferences. Our proposed algorithms are computationally efficient with near-optimal performance, both in terms of the private and non-private regret bound. More precisely, we show that when the decision space is of finite size $K$, our proposed algorithm yields order optimal $O\\Big(\\sum_{i = 2}^K\\log\\frac{KT}{\\Delta_i} + \\frac{K}{\\epsilon}\\Big)$ regret bound for pure $\\epsilon$-DP, where $\\Delta_i$ denotes the suboptimality gap of the $i$-th arm. We also present a matching lower bound analysis which proves the optimality of our algorithms. Finally, we extend our results to any general decision space in $d$-dimensions with potentially infinite arms and design an $\\epsilon$-DP algorithm with regret $\\tilde{O} \\left( \\frac{d^6}{\\kappa \\epsilon } + \\frac{ d\\sqrt{T }}{\\kappa} \\right)$, providing privacy for free when $T \\gg d$.","sentences":["We consider the well-studied dueling bandit problem, where a learner aims to identify near-optimal actions using pairwise comparisons, under the constraint of differential privacy.","We consider a general class of utility-based preference matrices for large (potentially unbounded) decision spaces and give the first differentially private dueling bandit algorithm for active learning with user preferences.","Our proposed algorithms are computationally efficient with near-optimal performance, both in terms of the private and non-private regret bound.","More precisely, we show that when the decision space is of finite size $K$, our proposed algorithm yields order optimal $O\\Big(\\sum_{i = 2}^K\\log\\frac{KT}{\\Delta_i} + \\frac{K}{\\epsilon}\\Big)$ regret bound for pure $\\epsilon$-DP, where $\\Delta_i$ denotes the suboptimality gap of the $i$-th arm.","We also present a matching lower bound analysis which proves the optimality of our algorithms.","Finally, we extend our results to any general decision space in $d$-dimensions with potentially infinite arms and design an $\\epsilon$-DP algorithm with regret $\\tilde{O} \\left( \\frac{d^6}{\\kappa \\epsilon } + \\frac{ d\\sqrt{T }}{\\kappa} \\right)$, providing privacy for free when $T \\gg d$."],"url":"http://arxiv.org/abs/2403.15045v1","category":"cs.LG"}
{"created":"2024-03-22 09:00:24","title":"Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild","abstract":"Multimodal fusion is a significant method for most multimodal tasks. With the recent surge in the number of large pre-trained models, combining both multimodal fusion methods and pre-trained model features can achieve outstanding performance in many multimodal tasks. In this paper, we present our approach, which leverages both advantages for addressing the task of Expression (Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the Aff-Wild2 database using pre-trained models, then extract the final hidden layers of the models as features. Following preprocessing and interpolation or convolution to align the extracted features, different models are employed for modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.","sentences":["Multimodal fusion is a significant method for most multimodal tasks.","With the recent surge in the number of large pre-trained models, combining both multimodal fusion methods and pre-trained model features can achieve outstanding performance in many multimodal tasks.","In this paper, we present our approach, which leverages both advantages for addressing the task of Expression (Expr) Recognition and Valence-Arousal (VA) Estimation.","We evaluate the Aff-Wild2 database using pre-trained models, then extract the final hidden layers of the models as features.","Following preprocessing and interpolation or convolution to align the extracted features, different models are employed for modal fusion.","Our code is available at GitHub - FulgenceWen/ABAW6th."],"url":"http://arxiv.org/abs/2403.15044v1","category":"cs.CV"}
{"created":"2024-03-22 08:57:07","title":"LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement","abstract":"Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a LLaMA2-7B student model.","sentences":["Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks.","While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging.","To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task.","LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data.","This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM.","Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines.","LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks.","We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a LLaMA2-7B student model."],"url":"http://arxiv.org/abs/2403.15042v1","category":"cs.CL"}
{"created":"2024-03-22 08:45:30","title":"ESG Classification by Implicit Rule Learning via GPT-4","abstract":"Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators. Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily. However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential. This paper investigates whether state-of-the-art language models like GPT-4 can be guided to align with unknown ESG evaluation criteria through strategies such as prompting, chain-of-thought reasoning, and dynamic in-context learning. We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data. We also explore how adjusting prompts impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights. We observe longer general pre-training to correlate with enhanced performance in financial downstream tasks. Our findings showcase the potential of language models to navigate complex, subjective evaluation guidelines despite lacking explicit training examples, revealing opportunities for training-free solutions for financial downstream tasks.","sentences":["Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators.","Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily.","However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential.","This paper investigates whether state-of-the-art language models like GPT-4 can be guided to align with unknown ESG evaluation criteria through strategies such as prompting, chain-of-thought reasoning, and dynamic in-context learning.","We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data.","We also explore how adjusting prompts impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights.","We observe longer general pre-training to correlate with enhanced performance in financial downstream tasks.","Our findings showcase the potential of language models to navigate complex, subjective evaluation guidelines despite lacking explicit training examples, revealing opportunities for training-free solutions for financial downstream tasks."],"url":"http://arxiv.org/abs/2403.15040v1","category":"cs.CL"}
{"created":"2024-03-22 08:45:02","title":"Deep learning scheme for forward utilities using ergodic BSDEs *","abstract":"In this paper, we present a probabilistic numerical method for a class of forward utilities in a stochastic factor model. For this purpose, we use the representation of dynamic consistent utilities with mean of ergodic Backward Stochastic Differential Equations (eBSDEs) introduced by Liang and Zariphopoulou in [27]. We establish a connection between the solution of the ergodic BSDE and the solution of an associated BSDE with random terminal time $\\tau$ , defined as the hitting time of the positive recurrent stochastic factor V . The viewpoint based on BSDEs with random horizon yields a new characterization of the ergodic cost $\\lambda$ which is a part of the solution of the eBSDEs. In particular, for a certain class of eBSDEs with quadratic generator, the Cole-Hopf transform leads to a semi-explicit representation of the solution as well as a new expression of the ergodic cost $\\lambda$. The latter can be estimated with Monte Carlo methods. We also propose two new deep learning numerical schemes for eBSDEs, where the ergodic cost $\\lambda$ is optimized according to a loss function at the random horizon $\\tau$ or taking into account the whole trajectory. Finally, we present numerical results for different examples of eBSDEs and forward utilities along with the associated investment strategies.","sentences":["In this paper, we present a probabilistic numerical method for a class of forward utilities in a stochastic factor model.","For this purpose, we use the representation of dynamic consistent utilities with mean of ergodic Backward Stochastic Differential Equations (eBSDEs) introduced by Liang and Zariphopoulou in [27].","We establish a connection between the solution of the ergodic BSDE and the solution of an associated BSDE with random terminal time $\\tau$ , defined as the hitting time of the positive recurrent stochastic factor V .","The viewpoint based on BSDEs with random horizon yields a new characterization of the ergodic cost $\\lambda$ which is a part of the solution of the eBSDEs.","In particular, for a certain class of eBSDEs with quadratic generator, the Cole-Hopf transform leads to a semi-explicit representation of the solution as well as a new expression of the ergodic cost $\\lambda$.","The latter can be estimated with Monte Carlo methods.","We also propose two new deep learning numerical schemes for eBSDEs, where the ergodic cost $\\lambda$ is optimized according to a loss function at the random horizon $\\tau$ or taking into account the whole trajectory.","Finally, we present numerical results for different examples of eBSDEs and forward utilities along with the associated investment strategies."],"url":"http://arxiv.org/abs/2403.15039v1","category":"math.PR"}
{"created":"2024-03-22 08:42:27","title":"Implementation of Firm-Dispatchable Generation in South Africa","abstract":"South Africa is currently facing a critical situation in its power generation landscape, which is plagued by frequent power outages and the need to move from fossil fuels to renewable energy sources. This period emphasizes the importance of having firm-dispatchable power to balance out the intermittent nature of wind and solar energy sources. The paper proposes to repurpose old coal-fired power plants to generate firm-dispatchable energy in line with the principles of a Just Transition. Eskom's coal plants are approaching the end of their economic life, and their declining energy availability factor is becoming a challenge in meeting the country's energy needs. The study suggests that a comprehensive strategy that integrates wind, solar, and firm-dispatchable power can be cost-effective and reliable compared to the traditional coal-based approach or the nuclear alternative. The study emphasizes the necessity of a 25-year plan that would invest in flexible and modular dispatchable generation. It also highlights the strategic location of this generating capacity, including repurposing decommissioned coal plant sites. The proposed model integrates private investment, adheres to established best practices, and emphasizes adaptability to changing demand dynamics. The study provides a roadmap for enabling firm-dispatchable capacity for South Africa's energy transition, emphasizing economic prudence, environmental sustainability, and alignment with the principles of the Just Transition program.","sentences":["South Africa is currently facing a critical situation in its power generation landscape, which is plagued by frequent power outages and the need to move from fossil fuels to renewable energy sources.","This period emphasizes the importance of having firm-dispatchable power to balance out the intermittent nature of wind and solar energy sources.","The paper proposes to repurpose old coal-fired power plants to generate firm-dispatchable energy in line with the principles of a Just Transition.","Eskom's coal plants are approaching the end of their economic life, and their declining energy availability factor is becoming a challenge in meeting the country's energy needs.","The study suggests that a comprehensive strategy that integrates wind, solar, and firm-dispatchable power can be cost-effective and reliable compared to the traditional coal-based approach or the nuclear alternative.","The study emphasizes the necessity of a 25-year plan that would invest in flexible and modular dispatchable generation.","It also highlights the strategic location of this generating capacity, including repurposing decommissioned coal plant sites.","The proposed model integrates private investment, adheres to established best practices, and emphasizes adaptability to changing demand dynamics.","The study provides a roadmap for enabling firm-dispatchable capacity for South Africa's energy transition, emphasizing economic prudence, environmental sustainability, and alignment with the principles of the Just Transition program."],"url":"http://arxiv.org/abs/2403.15037v1","category":"eess.SY"}
{"created":"2024-03-22 08:32:30","title":"Toward Tiny and High-quality Facial Makeup with Data Amplify Learning","abstract":"Contemporary makeup approaches primarily hinge on unpaired learning paradigms, yet they grapple with the challenges of inaccurate supervision (e.g., face misalignment) and sophisticated facial prompts (including face parsing, and landmark detection). These challenges prohibit low-cost deployment of facial makeup models, especially on mobile devices. To solve above problems, we propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\" alongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies in employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images for the model training, thereby enabling accurate pixel-to-pixel supervision with merely a handful of annotations. Two pivotal innovations in DDA facilitate the above training approach: (1) A Residual Diffusion Model (RDM) is designed to generate high-fidelity detail and circumvent the detail vanishing problem in the vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is proposed to achieve precise makeup control and combination while retaining face identity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to achieve a state-of-the-art performance without intricate face prompts. Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on the iPhone 13. Extensive experiments show that DAL can produce highly competitive makeup models using only 5 image pairs.","sentences":["Contemporary makeup approaches primarily hinge on unpaired learning paradigms, yet they grapple with the challenges of inaccurate supervision (e.g., face misalignment) and sophisticated facial prompts (including face parsing, and landmark detection).","These challenges prohibit low-cost deployment of facial makeup models, especially on mobile devices.","To solve above problems, we propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\" alongside a compact makeup model named \"TinyBeauty.\"","The core idea of DAL lies in employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images for the model training, thereby enabling accurate pixel-to-pixel supervision with merely a handful of annotations.","Two pivotal innovations in DDA facilitate the above training approach: (1) A Residual Diffusion Model (RDM) is designed to generate high-fidelity detail and circumvent the detail vanishing problem in the vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is proposed to achieve precise makeup control and combination while retaining face identity.","Coupled with DAL, TinyBeauty necessitates merely 80K parameters to achieve a state-of-the-art performance without intricate face prompts.","Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on the iPhone 13.","Extensive experiments show that DAL can produce highly competitive makeup models using only 5 image pairs."],"url":"http://arxiv.org/abs/2403.15033v1","category":"cs.CV"}
{"created":"2024-03-22 08:24:12","title":"Tie-Breaking Rule Based on Partial Proof of Work in a Blockchain","abstract":"Numerous methods have been proposed for suppressing intentional forks by attackers in blockchain systems. Among these, last-generated rules, which select the latest chain among chains in a tie, are effective methods that do not require significant changes to the blockchain protocol. However, existing methods either require a trusted third party or rely on timestamps that attackers can manipulate which makes applying a last-generated rule to existing systems such as Bitcoin challenging.   To address these issues, we propose a last-generated rule that can be easily applied to existing proof of work blockchain systems. Our method uses partial proof of work, which does not function as a block, as a time standard with finer granularity. Only weak synchronization, which is already met by existing systems, is required for effective functioning.   We evaluated the proposed method through a detailed analysis that is lacking in existing works. In networks that adopt our method, the proportion of the attacker hashrate necessary for selfish mining was approximately 0.31479 or higher, regardless of the block propagation capability of the attacker. Furthermore, we demonstrated through extended selfish mining that the impact of Match against pre-generated block, which is a concern in all last-generated rules, can be mitigated with appropriate parameter settings.","sentences":["Numerous methods have been proposed for suppressing intentional forks by attackers in blockchain systems.","Among these, last-generated rules, which select the latest chain among chains in a tie, are effective methods that do not require significant changes to the blockchain protocol.","However, existing methods either require a trusted third party or rely on timestamps that attackers can manipulate which makes applying a last-generated rule to existing systems such as Bitcoin challenging.   ","To address these issues, we propose a last-generated rule that can be easily applied to existing proof of work blockchain systems.","Our method uses partial proof of work, which does not function as a block, as a time standard with finer granularity.","Only weak synchronization, which is already met by existing systems, is required for effective functioning.   ","We evaluated the proposed method through a detailed analysis that is lacking in existing works.","In networks that adopt our method, the proportion of the attacker hashrate necessary for selfish mining was approximately 0.31479 or higher, regardless of the block propagation capability of the attacker.","Furthermore, we demonstrated through extended selfish mining that the impact of Match against pre-generated block, which is a concern in all last-generated rules, can be mitigated with appropriate parameter settings."],"url":"http://arxiv.org/abs/2403.15030v1","category":"cs.CR"}
{"created":"2024-03-22 16:55:01","title":"Optimal Exploration Strategy for Regret Minimization in Unconstrained Scalar Optimization Problems","abstract":"We study the problem of determining the optimal exploration strategy in an unconstrained scalar optimization problem depending on an unknown parameter to be learned from online collected noisy data. An optimal trade-off between exploration and exploitation is crucial for effective optimization under uncertainties, and to achieve this we consider a cumulative regret minimization approach over a finite horizon, with each time instant in the horizon characterized by a stochastic exploration signal, whose variance has to be designed. In this setting, under an idealized assumption on an appropriately defined information function associated with the excitation, we are able to show that the optimal exploration strategy is either to use no exploration at all (called lazy exploration) or adding an exploration excitation only at the first time instant of the horizon (called immediate exploration). A quadratic numerical example is used to illustrate the results.","sentences":["We study the problem of determining the optimal exploration strategy in an unconstrained scalar optimization problem depending on an unknown parameter to be learned from online collected noisy data.","An optimal trade-off between exploration and exploitation is crucial for effective optimization under uncertainties, and to achieve this we consider a cumulative regret minimization approach over a finite horizon, with each time instant in the horizon characterized by a stochastic exploration signal, whose variance has to be designed.","In this setting, under an idealized assumption on an appropriately defined information function associated with the excitation, we are able to show that the optimal exploration strategy is either to use no exploration at all (called lazy exploration) or adding an exploration excitation only at the first time instant of the horizon (called immediate exploration).","A quadratic numerical example is used to illustrate the results."],"url":"http://arxiv.org/abs/2403.15344v1","category":"math.OC"}
{"created":"2024-03-22 15:41:41","title":"Measurements of electroweak $W^{\\pm}Z$ boson pair production in association with two jets in $pp$ collisions at $\\sqrt{s} = 13$ TeV with the ATLAS detector","abstract":"Measurements of integrated and differential cross-sections for electroweak $W^{\\pm}Z$ production in association with two jets ($W^{\\pm}Zjj$) in proton-proton collisions are presented. The data collected by the ATLAS detector at the Large Hadron Collider from $2015$ to $2018$ at a centre-of-mass energy of $\\sqrt{s} = 13$ TeV are used, corresponding to an integrated luminosity of $140$ fb$^{-1}$. The $W^{\\pm}Zjj$ candidate events are reconstructed using leptonic decay modes of the gauge bosons. Events containing three identified leptons, either electrons or muons, and two jets are selected. Processes involving pure electroweak $W^{\\pm}Zjj$ production at Born level are separated from $W^{\\pm}Zjj$ production involving a strong coupling. The measured integrated fiducial cross-section of electroweak $W^{\\pm}Zjj$ production per lepton flavour is $\\sigma_{WZjj\\mathrm{-EW} \\rightarrow \\ell^{'} \\nu \\ell \\ell jj} = 0.368 \\; \\pm 0.037 \\,(\\mathrm{stat.}) \\; \\pm 0.059 \\,(\\mathrm{syst.}) \\; \\pm 0.003 \\,(\\mathrm{lumi.}) \\; \\mathrm{fb}$. Respective cross-sections of electroweak and strong $W^{\\pm}Zjj$ production are measured separately for events with exactly two jets or with more than two jets, and in three bins of the invariant mass of the two jets. The inclusive $W^{\\pm}Zjj$ production cross-section, without separating electroweak and strong production, is also measured to be $\\sigma_{WZjj \\rightarrow \\ell^{'} \\nu \\ell \\ell jj} = 1.462 \\; \\pm 0.063 \\,(\\mathrm{stat.}) \\; \\pm 0.118 \\,(\\mathrm{syst.}) \\; \\pm 0.012 \\,(\\mathrm{lumi.}) \\; \\mathrm{fb}$, per lepton flavour. The inclusive $W^{\\pm}Zjj$ production cross-section is measured differentially for several kinematic observables. Finally, the measurements are used to constrain anomalous quartic gauge couplings by extracting $95$ % confidence level intervals on dimension-$8$ operators.","sentences":["Measurements of integrated and differential cross-sections for electroweak $W^{\\pm}Z$ production in association with two jets ($W^{\\pm}Zjj$) in proton-proton collisions are presented.","The data collected by the ATLAS detector at the Large Hadron Collider from $2015$ to $2018$ at a centre-of-mass energy of $\\sqrt{s} = 13$ TeV are used, corresponding to an integrated luminosity of $140$ fb$^{-1}$. The $W^{\\pm}Zjj$ candidate events are reconstructed using leptonic decay modes of the gauge bosons.","Events containing three identified leptons, either electrons or muons, and two jets are selected.","Processes involving pure electroweak $W^{\\pm}Zjj$ production at Born level are separated from $W^{\\pm}Zjj$ production involving a strong coupling.","The measured integrated fiducial cross-section of electroweak $W^{\\pm}Zjj$ production per lepton flavour is $\\sigma_{WZjj\\mathrm{-EW} \\rightarrow \\ell^{'} \\nu \\ell \\ell jj} = 0.368 \\; \\pm 0.037 \\,(\\mathrm{stat.})","\\; \\pm 0.059 \\,(\\mathrm{syst.})","\\; \\pm 0.003 \\,(\\mathrm{lumi.})","\\; \\mathrm{fb}$.","Respective cross-sections of electroweak and strong $W^{\\pm}Zjj$ production are measured separately for events with exactly two jets or with more than two jets, and in three bins of the invariant mass of the two jets.","The inclusive $W^{\\pm}Zjj$ production cross-section, without separating electroweak and strong production, is also measured to be $\\sigma_{WZjj \\rightarrow \\ell^{'} \\nu \\ell \\ell jj} = 1.462 \\; \\pm 0.063 \\,(\\mathrm{stat.})","\\; \\pm 0.118 \\,(\\mathrm{syst.})","\\; \\pm 0.012 \\,(\\mathrm{lumi.})","\\; \\mathrm{fb}$, per lepton flavour.","The inclusive $W^{\\pm}Zjj$ production cross-section is measured differentially for several kinematic observables.","Finally, the measurements are used to constrain anomalous quartic gauge couplings by extracting $95$ % confidence level intervals on dimension-$8$ operators."],"url":"http://arxiv.org/abs/2403.15296v1","category":"hep-ex"}
{"created":"2024-03-22 15:35:45","title":"Reconnaissance ultracool spectra in the Euclid Deep Fields","abstract":"Context. Euclid will carry out a deep survey benefiting the discovery and characterisation of ultracool dwarfs (UCDs), especially in the Euclid Deep Fields (EDFs), which the telescope will scan repeatedly throughout its mission. The photometric and spectroscopic standards in the EDFs are important benchmarks, crucial for the classification and characterisation of new UCD discoveries and for the calibration of the mission itself. Aims. We aim to provide a list of photometric UCD candidates and collect near-infrared reconnaissance spectra for M, L, and T-type UCDs in the EDFs as future Euclid UCD references. Methods. In EDF North, we cross-matched public optical and infrared surveys with certain photometric criteria to select UCDs. In EDF Fornax and EDF South, we used photometrically classified samples from the literature. We also include UCDs identified by Gaia DR2. We selected 7 UCD targets with different spectral types from the lists and obtained low-resolution 0.9-2.5 {\\mu}m spectra of them using GTC/EMIR and the VLT/X-shooter. We also selected a young, bright L dwarf near EDF Fornax to test the coherence of these two facilities. We included an extra T dwarf in EDF North with its published J-band spectrum. Results. We retrieved a list of 92 (49, 231) M, 33 (29, 115) L, and 1 (0, 2) T dwarf candidates in EDF North, Fornax, and South, respectively. They are provided to guide future UCD discoveries and characterisations by Euclid. In total, we collected near-infrared spectra for 9 UCDs, including 2 M types, 3 L types, and 4 T types in or close to the 3 EDFs. The Euclidised spectra show consistency in their spectral classification, which demonstrates that slitless Euclid spectroscopy will recover the spectral types with high fidelity for UCDs, both in the EDFs and in the wide survey. We also demonstrate that Euclid will be able to distinguish different age groups of UCDs.","sentences":["Context.","Euclid will carry out a deep survey benefiting the discovery and characterisation of ultracool dwarfs (UCDs), especially in the Euclid Deep Fields (EDFs), which the telescope will scan repeatedly throughout its mission.","The photometric and spectroscopic standards in the EDFs are important benchmarks, crucial for the classification and characterisation of new UCD discoveries and for the calibration of the mission itself.","Aims.","We aim to provide a list of photometric UCD candidates and collect near-infrared reconnaissance spectra for M, L, and T-type UCDs in the EDFs as future Euclid UCD references.","Methods.","In EDF North, we cross-matched public optical and infrared surveys with certain photometric criteria to select UCDs.","In EDF Fornax and EDF South, we used photometrically classified samples from the literature.","We also include UCDs identified by Gaia DR2.","We selected 7 UCD targets with different spectral types from the lists and obtained low-resolution 0.9-2.5 {\\mu}m spectra of them using GTC/EMIR and the VLT/X-shooter.","We also selected a young, bright L dwarf near EDF Fornax to test the coherence of these two facilities.","We included an extra T dwarf in EDF North with its published J-band spectrum.","Results.","We retrieved a list of 92 (49, 231) M, 33 (29, 115) L, and 1 (0, 2) T dwarf candidates in EDF North, Fornax, and South, respectively.","They are provided to guide future UCD discoveries and characterisations by Euclid.","In total, we collected near-infrared spectra for 9 UCDs, including 2 M types, 3 L types, and 4 T types in or close to the 3 EDFs.","The Euclidised spectra show consistency in their spectral classification, which demonstrates that slitless Euclid spectroscopy will recover the spectral types with high fidelity for UCDs, both in the EDFs and in the wide survey.","We also demonstrate that Euclid will be able to distinguish different age groups of UCDs."],"url":"http://arxiv.org/abs/2403.15288v1","category":"astro-ph.SR"}
{"created":"2024-03-22 15:22:06","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions","abstract":"This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code. Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper. This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts. Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline. By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users. This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers. Our evaluation shows that Fundus yields significantly higher quality extractions (complete and artifact-free news articles) than prior work. The framework is available on GitHub under https://github.com/flairNLP/fundus and can be simply installed using pip.","sentences":["This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code.","Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper.","This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts.","Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline.","By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users.","This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers.","Our evaluation shows that Fundus yields significantly higher quality extractions (complete and artifact-free news articles) than prior work.","The framework is available on GitHub under https://github.com/flairNLP/fundus and can be simply installed using pip."],"url":"http://arxiv.org/abs/2403.15279v1","category":"cs.CL"}
{"created":"2024-03-22 14:42:29","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions","abstract":"Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and alters the annotator instructions, re-annotating relevant documents. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information. However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements (over 13%) after fine-tuning on our training set.","sentences":["Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks.","However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions.","For the handful of recent models that do take instructions, it's unclear how they use them.","We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions.","FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions.","Our evaluation benchmark starts with three deeply judged TREC collections and alters the annotator instructions, re-annotating relevant documents.","Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework.","Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information.","However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements (over 13%) after fine-tuning on our training set."],"url":"http://arxiv.org/abs/2403.15246v1","category":"cs.IR"}
{"created":"2024-03-22 14:31:45","title":"Driven-dissipative phase separation in free-space atomic ensembles","abstract":"The driven Dicke model, wherein an ensemble of atoms is driven by an external field and undergoes collective spontaneous emission due to coupling to a leaky cavity mode, is a paradigmatic example of a system exhibiting a driven-dissipative phase transition as a function of driving strength. Recently, a similar phenomenon was experimentally observed, not in a cavity setting, but rather in a free-space atomic ensemble. The reason why similar behavior should emerge in free space is not obvious, as the system interacts with a continuum of optical modes, which encodes light propagation effects. Here, we present and solve a simple model to explain the behavior of the free-space system, based on the one-dimensional Maxwell-Bloch equations. On one hand, we show that a free-space ensemble at a low optical depth can exhibit similar behavior as the cavity system, as spatial propagation effects are negligible. On the other hand, in the thermodynamic limit of large atom number, we show that certain observables such as the transmittance or the atomic excited population exhibit non-analytic behavior as a function of the driving intensity, reminiscent of a phase transition. However, a closer analysis reveals that the atomic properties are highly inhomogeneous in space, and based on this we argue that the free-space system does not undergo a phase transition but rather a ``phase separation\", roughly speaking, between saturated and unsaturated regions.","sentences":["The driven Dicke model, wherein an ensemble of atoms is driven by an external field and undergoes collective spontaneous emission due to coupling to a leaky cavity mode, is a paradigmatic example of a system exhibiting a driven-dissipative phase transition as a function of driving strength.","Recently, a similar phenomenon was experimentally observed, not in a cavity setting, but rather in a free-space atomic ensemble.","The reason why similar behavior should emerge in free space is not obvious, as the system interacts with a continuum of optical modes, which encodes light propagation effects.","Here, we present and solve a simple model to explain the behavior of the free-space system, based on the one-dimensional Maxwell-Bloch equations.","On one hand, we show that a free-space ensemble at a low optical depth can exhibit similar behavior as the cavity system, as spatial propagation effects are negligible.","On the other hand, in the thermodynamic limit of large atom number, we show that certain observables such as the transmittance or the atomic excited population exhibit non-analytic behavior as a function of the driving intensity, reminiscent of a phase transition.","However, a closer analysis reveals that the atomic properties are highly inhomogeneous in space, and based on this we argue that the free-space system does not undergo a phase transition but rather a ``phase separation\", roughly speaking, between saturated and unsaturated regions."],"url":"http://arxiv.org/abs/2403.15237v1","category":"quant-ph"}
{"created":"2024-03-22 13:35:37","title":"Entropy and the City: Origins, trajectories and explorations of the concept in urban science","abstract":"Entropy is arguably one of the most powerful concepts to understand the world, from the behavior of molecules to the expansion of the universe, from how life emerges to how hybrid complex systems like cities come into being and continue existing. Yet, despite its widespread application, it is also one of the most misunderstood concepts across the sciences. This chapter seeks to demystify entropy and its main interpretations, along with some of its explorations in the context of cities. It first establishes the foundations of the concept by describing its trajectory since its inception in thermodynamics and statistical mechanics in the 19th century, its different incarnations from Boltzmanns pioneering formulation and Shannons information theory to its absorption in biology and the social sciences, until it reaches a nascent urban science in the late 1960s. The chapter then identifies some of the main domains in which entropy has been explored to understand cities as complex systems, from entropy-maximizing models of spatial interaction and applications as a measure of urban form, diversity, and complexity to a tool for understanding conditions of self-organization and urban sustainability.","sentences":["Entropy is arguably one of the most powerful concepts to understand the world, from the behavior of molecules to the expansion of the universe, from how life emerges to how hybrid complex systems like cities come into being and continue existing.","Yet, despite its widespread application, it is also one of the most misunderstood concepts across the sciences.","This chapter seeks to demystify entropy and its main interpretations, along with some of its explorations in the context of cities.","It first establishes the foundations of the concept by describing its trajectory since its inception in thermodynamics and statistical mechanics in the 19th century, its different incarnations from Boltzmanns pioneering formulation and Shannons information theory to its absorption in biology and the social sciences, until it reaches a nascent urban science in the late 1960s.","The chapter then identifies some of the main domains in which entropy has been explored to understand cities as complex systems, from entropy-maximizing models of spatial interaction and applications as a measure of urban form, diversity, and complexity to a tool for understanding conditions of self-organization and urban sustainability."],"url":"http://arxiv.org/abs/2403.15199v1","category":"physics.soc-ph"}
{"created":"2024-03-22 12:22:09","title":"Nonlinear shifts and dislocations in financial market structure and composition","abstract":"This paper develops new mathematical techniques to identify temporal shifts among a collection of US equities partitioned into a new and more detailed set of market sectors. Although conceptually related, our three analyses reveal distinct insights about financial markets, with meaningful implications for investment managers. First, we explore a variety of methods to identify nonlinear shifts in market sector structure and describe the mathematical connection between the measure used and the captured phenomena. Second, we study network structure with respect to our new market sectors and identify meaningfully connected sector-to-sector mappings. Finally, we conduct a series of sampling experiments over different sample spaces and contrast the distribution of Sharpe ratios produced by long-only, long-short and short-only investment portfolios. In addition, we examine the sector composition of the top-performing portfolios for each of these portfolio styles. In practice, the methods proposed in this paper could be used to identify regime shifts, optimally structured portfolios, and better communities of equities.","sentences":["This paper develops new mathematical techniques to identify temporal shifts among a collection of US equities partitioned into a new and more detailed set of market sectors.","Although conceptually related, our three analyses reveal distinct insights about financial markets, with meaningful implications for investment managers.","First, we explore a variety of methods to identify nonlinear shifts in market sector structure and describe the mathematical connection between the measure used and the captured phenomena.","Second, we study network structure with respect to our new market sectors and identify meaningfully connected sector-to-sector mappings.","Finally, we conduct a series of sampling experiments over different sample spaces and contrast the distribution of Sharpe ratios produced by long-only, long-short and short-only investment portfolios.","In addition, we examine the sector composition of the top-performing portfolios for each of these portfolio styles.","In practice, the methods proposed in this paper could be used to identify regime shifts, optimally structured portfolios, and better communities of equities."],"url":"http://arxiv.org/abs/2403.15163v1","category":"q-fin.ST"}
{"created":"2024-03-22 11:25:38","title":"Quantification using Permutation-Invariant Networks based on Histograms","abstract":"Quantification, also known as class prevalence estimation, is the supervised learning task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric supervised approach that eliminates the need for classification as an intermediary step, directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems. Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural architectures devised for set processing, as well as the state-of-the-art quantification methods. Furthermore, HistNetQ offers two significant advantages over traditional quantification methods: i) it does not require the labels of the training examples but only the prevalence values of a collection of training bags, making it applicable to new scenarios; and ii) it is able to optimize any custom quantification-oriented loss function.","sentences":["Quantification, also known as class prevalence estimation, is the supervised learning task in which a model is trained to predict the prevalence of each class in a given bag of examples.","This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric supervised approach that eliminates the need for classification as an intermediary step, directly addressing the quantification problem.","Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification.","In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems.","Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural architectures devised for set processing, as well as the state-of-the-art quantification methods.","Furthermore, HistNetQ offers two significant advantages over traditional quantification methods: i) it does not require the labels of the training examples but only the prevalence values of a collection of training bags, making it applicable to new scenarios; and ii) it is able to optimize any custom quantification-oriented loss function."],"url":"http://arxiv.org/abs/2403.15123v1","category":"cs.LG"}
{"created":"2024-03-22 10:51:31","title":"PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic Manipulation","abstract":"Humans seemingly incorporate potential touch signals in their perception. Our goal is to equip robots with a similar capability, which we term \\ourmodel. \\ourmodel aims to predict the expected touch signal based on a visual patch representing the touched area. We frame this problem as the task of learning a low-dimensional visual-tactile embedding, wherein we encode a depth patch from which we decode the tactile signal. To accomplish this task, we employ ReSkin, an inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we collect and train PseudoTouch on a dataset comprising aligned tactile and visual data pairs obtained through random touching of eight basic geometric shapes. We demonstrate the efficacy of PseudoTouch through its application to two downstream tasks: object recognition and grasp stability prediction. In the object recognition task, we evaluate the learned embedding's performance on a set of five basic geometric shapes and five household objects. Using PseudoTouch, we achieve an object recognition accuracy 84% after just ten touches, surpassing a proprioception baseline. For the grasp stability task, we use ACRONYM labels to train and evaluate a grasp success predictor using PseudoTouch's predictions derived from virtual depth information. Our approach yields an impressive 32% absolute improvement in accuracy compared to the baseline relying on partial point cloud data. We make the data, code, and trained models publicly available at http://pseudotouch.cs.uni-freiburg.de.","sentences":["Humans seemingly incorporate potential touch signals in their perception.","Our goal is to equip robots with a similar capability, which we term \\ourmodel.","\\ourmodel aims to predict the expected touch signal based on a visual patch representing the touched area.","We frame this problem as the task of learning a low-dimensional visual-tactile embedding, wherein we encode a depth patch from which we decode the tactile signal.","To accomplish this task, we employ ReSkin, an inexpensive and replaceable magnetic-based tactile sensor.","Using ReSkin, we collect and train PseudoTouch on a dataset comprising aligned tactile and visual data pairs obtained through random touching of eight basic geometric shapes.","We demonstrate the efficacy of PseudoTouch through its application to two downstream tasks: object recognition and grasp stability prediction.","In the object recognition task, we evaluate the learned embedding's performance on a set of five basic geometric shapes and five household objects.","Using PseudoTouch, we achieve an object recognition accuracy 84% after just ten touches, surpassing a proprioception baseline.","For the grasp stability task, we use ACRONYM labels to train and evaluate a grasp success predictor using PseudoTouch's predictions derived from virtual depth information.","Our approach yields an impressive 32% absolute improvement in accuracy compared to the baseline relying on partial point cloud data.","We make the data, code, and trained models publicly available at http://pseudotouch.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2403.15107v1","category":"cs.RO"}
{"created":"2024-03-22 10:48:12","title":"LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns","abstract":"In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. With the development of large language models (LLMs), we now have the opportunity to capture the nuanced exchanges of information within social networks. Hence, in this work, we first introduce an Influencer Dynamics Simulator (IDS), helping promoters identify and select the right influencers to market their products, based on LLM simulation. Concretely, we first propose an influencer-influencee engagement-based pre-selection module to screen potential influencer candidates. Subsequently, a simulation is constructed for these candidates and their influencees. Each user is represented as an LLM-based agent, drawing from their interaction history to deduce their profile and interests. The influencee agents will predict their behavior in response to influencer advertising. Finally, we develop a ranking metric designed to pinpoint influencers who are most likely to drive product purchases based on feedback from their influencees. To evaluate our framework, we collect a real-world advertising network dataset, including social relations, post and comment content, and user behaviors. Our dataset covers six products in diverse categories with their promoter influencers. Experiments show that IDS accurately identifies influencers from hundreds of candidates and provides valuable insights through detailed comments and specific attitudes.","sentences":["In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees.","Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis.","Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics.","With the development of large language models (LLMs), we now have the opportunity to capture the nuanced exchanges of information within social networks.","Hence, in this work, we first introduce an Influencer Dynamics Simulator (IDS), helping promoters identify and select the right influencers to market their products, based on LLM simulation.","Concretely, we first propose an influencer-influencee engagement-based pre-selection module to screen potential influencer candidates.","Subsequently, a simulation is constructed for these candidates and their influencees.","Each user is represented as an LLM-based agent, drawing from their interaction history to deduce their profile and interests.","The influencee agents will predict their behavior in response to influencer advertising.","Finally, we develop a ranking metric designed to pinpoint influencers who are most likely to drive product purchases based on feedback from their influencees.","To evaluate our framework, we collect a real-world advertising network dataset, including social relations, post and comment content, and user behaviors.","Our dataset covers six products in diverse categories with their promoter influencers.","Experiments show that IDS accurately identifies influencers from hundreds of candidates and provides valuable insights through detailed comments and specific attitudes."],"url":"http://arxiv.org/abs/2403.15105v1","category":"cs.SI"}
{"created":"2024-03-22 10:41:25","title":"Learning from Visual Demonstrations through Differentiable Nonlinear MPC for Personalized Autonomous Driving","abstract":"Human-like autonomous driving controllers have the potential to enhance passenger perception of autonomous vehicles. This paper proposes DriViDOC: a model for Driving from Vision through Differentiable Optimal Control, and its application to learn personalized autonomous driving controllers from human demonstrations. DriViDOC combines the automatic inference of relevant features from camera frames with the properties of nonlinear model predictive control (NMPC), such as constraint satisfaction. Our approach leverages the differentiability of parametric NMPC, allowing for end-to-end learning of the driving model from images to control. The model is trained on an offline dataset comprising various driving styles collected on a motion-base driving simulator. During online testing, the model demonstrates successful imitation of different driving styles, and the interpreted NMPC parameters provide insights into the achievement of specific driving behaviors. Our experimental results show that DriViDOC outperforms other methods involving NMPC and neural networks, exhibiting an average improvement of 20% in imitation scores.","sentences":["Human-like autonomous driving controllers have the potential to enhance passenger perception of autonomous vehicles.","This paper proposes DriViDOC: a model for Driving from Vision through Differentiable Optimal Control, and its application to learn personalized autonomous driving controllers from human demonstrations.","DriViDOC combines the automatic inference of relevant features from camera frames with the properties of nonlinear model predictive control (NMPC), such as constraint satisfaction.","Our approach leverages the differentiability of parametric NMPC, allowing for end-to-end learning of the driving model from images to control.","The model is trained on an offline dataset comprising various driving styles collected on a motion-base driving simulator.","During online testing, the model demonstrates successful imitation of different driving styles, and the interpreted NMPC parameters provide insights into the achievement of specific driving behaviors.","Our experimental results show that DriViDOC outperforms other methods involving NMPC and neural networks, exhibiting an average improvement of 20% in imitation scores."],"url":"http://arxiv.org/abs/2403.15102v1","category":"cs.RO"}
{"created":"2024-03-22 09:38:16","title":"Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos","abstract":"High Dynamic Range (HDR) videos are able to represent wider ranges of contrasts and colors than Standard Dynamic Range (SDR) videos, giving more vivid experiences. Due to this, HDR videos are expected to grow into the dominant video modality of the future. However, HDR videos are incompatible with existing SDR displays, which form the majority of affordable consumer displays on the market. Because of this, HDR videos must be processed by tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited video consumers. Here, we analyze the impact of tone-mapping operators on the visual quality of streaming HDR videos. To this end, we built the first large-scale subjectively annotated open-source database of compressed tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40 unique HDR source contents. The videos in the database were labeled with more than 750,000 subjective quality annotations, collected from more than 1,600 unique human observers. We demonstrate the usefulness of the new subjective database by benchmarking objective models of visual quality on it. We envision that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant progress on HDR video tone mapping and quality assessment in the future. To this end, we make the database freely available to the community at https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html","sentences":["High Dynamic Range (HDR) videos are able to represent wider ranges of contrasts and colors than Standard Dynamic Range (SDR) videos, giving more vivid experiences.","Due to this, HDR videos are expected to grow into the dominant video modality of the future.","However, HDR videos are incompatible with existing SDR displays, which form the majority of affordable consumer displays on the market.","Because of this, HDR videos must be processed by tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited video consumers.","Here, we analyze the impact of tone-mapping operators on the visual quality of streaming HDR videos.","To this end, we built the first large-scale subjectively annotated open-source database of compressed tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40 unique HDR source contents.","The videos in the database were labeled with more than 750,000 subjective quality annotations, collected from more than 1,600 unique human observers.","We demonstrate the usefulness of the new subjective database by benchmarking objective models of visual quality on it.","We envision that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant progress on HDR video tone mapping and quality assessment in the future.","To this end, we make the database freely available to the community at https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html"],"url":"http://arxiv.org/abs/2403.15061v1","category":"eess.IV"}
{"created":"2024-03-22 08:35:14","title":"Phase-resolved XMM-Newton observations of the massive post-RLOF system HD 149404","abstract":"We investigated the X-ray emission of HD 149404, a 9.81-day period O-star binary in a post-Roche lobe overflow evolutionary stage. X-ray emission of O-star binaries consists of the intrinsic emission of the individual O stars and a putative additional component arising from the wind-wind interaction. Phase-locked variations in the X-ray spectra can be used to probe the properties of the stellar winds of such systems. XMM-Newton observations of HD 149404 collected at two conjunction phases and a quadrature phase were analysed. X-ray spectra were extracted and flux variations as a function of orbital phase were inferred. The flux ratios were analysed with models considering various origins for the X-ray emission. The highest and lowest X-ray fluxes are observed at conjunction phases respectively with the primary and secondary star in front. The flux variations are nearly grey with only marginal energy dependence. None of the models accounting for photoelectric absorption by homogeneous stellar winds perfectly reproduces the observed variations. Whilst the overall X-ray luminosity is consistent with a pure intrinsic emission, the best formal agreement with the observed variations is obtained with a model assuming pure wind-wind collision X-ray emission. The lack of significant energy-dependence of the opacity most likely hints at the presence of optically thick clumps in the winds of HD 149404.","sentences":["We investigated the X-ray emission of HD 149404, a 9.81-day period O-star binary in a post-Roche lobe overflow evolutionary stage.","X-ray emission of O-star binaries consists of the intrinsic emission of the individual O stars and a putative additional component arising from the wind-wind interaction.","Phase-locked variations in the X-ray spectra can be used to probe the properties of the stellar winds of such systems.","XMM-Newton observations of HD 149404 collected at two conjunction phases and a quadrature phase were analysed.","X-ray spectra were extracted and flux variations as a function of orbital phase were inferred.","The flux ratios were analysed with models considering various origins for the X-ray emission.","The highest and lowest X-ray fluxes are observed at conjunction phases respectively with the primary and secondary star in front.","The flux variations are nearly grey with only marginal energy dependence.","None of the models accounting for photoelectric absorption by homogeneous stellar winds perfectly reproduces the observed variations.","Whilst the overall X-ray luminosity is consistent with a pure intrinsic emission, the best formal agreement with the observed variations is obtained with a model assuming pure wind-wind collision X-ray emission.","The lack of significant energy-dependence of the opacity most likely hints at the presence of optically thick clumps in the winds of HD 149404."],"url":"http://arxiv.org/abs/2403.15034v1","category":"astro-ph.SR"}
{"created":"2024-03-22 17:23:37","title":"Learning Topological Representations for Deep Image Understanding","abstract":"In many scenarios, especially biomedical applications, the correct delineation of complex fine-scaled structures such as neurons, tissues, and vessels is critical for downstream analysis. Despite the strong predictive power of deep learning methods, they do not provide a satisfactory representation of these structures, thus creating significant barriers in scalable annotation and downstream analysis. In this dissertation, we tackle such challenges by proposing novel representations of these topological structures in a deep learning framework. We leverage the mathematical tools from topological data analysis, i.e., persistent homology and discrete Morse theory, to develop principled methods for better segmentation and uncertainty estimation, which will become powerful tools for scalable annotation.","sentences":["In many scenarios, especially biomedical applications, the correct delineation of complex fine-scaled structures such as neurons, tissues, and vessels is critical for downstream analysis.","Despite the strong predictive power of deep learning methods, they do not provide a satisfactory representation of these structures, thus creating significant barriers in scalable annotation and downstream analysis.","In this dissertation, we tackle such challenges by proposing novel representations of these topological structures in a deep learning framework.","We leverage the mathematical tools from topological data analysis, i.e., persistent homology and discrete Morse theory, to develop principled methods for better segmentation and uncertainty estimation, which will become powerful tools for scalable annotation."],"url":"http://arxiv.org/abs/2403.15361v1","category":"cs.CV"}
{"created":"2024-03-22 17:22:56","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series","abstract":"Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art transformers. Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and transfer learning benchmarks such as Stanford Car and Flower as well as task learning benchmarks as well as seven time series benchmark datasets. The project page is available on this website ~\\url{https://github.com/badripatro/Simba}.","sentences":["Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains.","However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length.","State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths.","Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets.","We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling.","Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art transformers.","Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and transfer learning benchmarks such as Stanford Car and Flower as well as task learning benchmarks as well as seven time series benchmark datasets.","The project page is available on this website ~\\url{https://github.com/badripatro/Simba}."],"url":"http://arxiv.org/abs/2403.15360v1","category":"cs.CV"}
{"created":"2024-03-22 17:00:26","title":"Admissibility of C*-Covers for Operator Algebra Dynamical Systems","abstract":"We characterize when a C*-cover admits a C*-dynamical extension of dynamics on an operator algebra in terms of the boundary ideal structure for the operator algebra in its maximal representation and show that the C*-covers that admit such an extension form a complete lattice. We study dynamical systems arising from groups acting via inner automorphisms in a C*-cover and produce an example of a C*-cover that admits no extension of dynamics on a finite-dimensional non-self-adjoint operator algebra. We construct a partial action on a class of C*-covers that recovers the crossed product of an operator algebra as a subalgebra of the partial crossed product, even when the C*-cover admits no dynamical extension.","sentences":["We characterize when a C*-cover admits a C*-dynamical extension of dynamics on an operator algebra in terms of the boundary ideal structure for the operator algebra in its maximal representation and show that the C*-covers that admit such an extension form a complete lattice.","We study dynamical systems arising from groups acting via inner automorphisms in a C*-cover and produce an example of a C*-cover that admits no extension of dynamics on a finite-dimensional non-self-adjoint operator algebra.","We construct a partial action on a class of C*-covers that recovers the crossed product of an operator algebra as a subalgebra of the partial crossed product, even when the C*-cover admits no dynamical extension."],"url":"http://arxiv.org/abs/2403.15349v1","category":"math.OA"}
{"created":"2024-03-22 17:00:18","title":"Crossover from relativistic to non-relativistic net magnetization for MnTe altermagnet candidate","abstract":"We experimentally study magnetization reversal curves for MnTe single crystals, which is the altermagnetic candidate. Above 85~K temperature, we confirm the antiferromagnetic behavior of magnetization $M$, which is known for $\\alpha$--MnTe. Below 85~K, we observe anomalous low-field magnetization behavior, which is accompanied by the sophisticated $M(\\alpha)$ angle dependence with beating pattern as the interplay between $M(\\alpha)$ maxima and minima: in low fields, $M(\\alpha)$ shows ferromagnetic-like 180$^\\circ$ periodicity, while at high magnetic fields, the periodicity is changed to the 90$^\\circ$ one. This angle dependence is the most striking result of our experiment, while it can not be expected for standard magnetic systems. In contrast, in altermagnets, symmetry allows ferromagnetic behavior only due to the spin-orbit coupling. Thus, we claim that our experiment shows the effect of weak spin-orbit coupling in MnTe, with crossover from relativistic to non-relativistic net magnetization, and, therefore, we experimentally confirm altermagnetism in MnTe.","sentences":["We experimentally study magnetization reversal curves for MnTe single crystals, which is the altermagnetic candidate.","Above 85~K temperature, we confirm the antiferromagnetic behavior of magnetization $M$, which is known for $\\alpha$--MnTe.","Below 85~K, we observe anomalous low-field magnetization behavior, which is accompanied by the sophisticated $M(\\alpha)$ angle dependence with beating pattern as the interplay between $M(\\alpha)$ maxima and minima: in low fields, $M(\\alpha)$ shows ferromagnetic-like 180$^\\circ$ periodicity, while at high magnetic fields, the periodicity is changed to the 90$^\\circ$ one.","This angle dependence is the most striking result of our experiment, while it can not be expected for standard magnetic systems.","In contrast, in altermagnets, symmetry allows ferromagnetic behavior only due to the spin-orbit coupling.","Thus, we claim that our experiment shows the effect of weak spin-orbit coupling in MnTe, with crossover from relativistic to non-relativistic net magnetization, and, therefore, we experimentally confirm altermagnetism in MnTe."],"url":"http://arxiv.org/abs/2403.15348v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 16:53:12","title":"Topological analysis and experimental control of transformations of domain walls in magnetic cylindrical nanowires","abstract":"Topology is a powerful tool for categorizing magnetization textures, highlighting specific features in both 2D systems, such as thin films or curved surfaces, and in 3D bulk systems. In the emerging field of 3D nanomagnetism within confined geometries, the contributions from both volume and surface must be considered, requiring appropriate topological analysis to obtain a complete view of the system. Here, we consider domain walls in cylindrical magnetic nanowires to illustrate the use of topological invariants. We begin with micromagnetic simulations of domain wall transformation under the stimulus of an \\OErsted field, tracking bulk and surface topological signatures, and analyzing the interplay between multiple micromagnetic objects. For instance, the extensive analysis allowed us to highlight mechanisms of domain wall type conversion from topologically non-trivial to trivial states, a phenomenon disregarded in previous studies. Additionally, we provide experimental evidence of the transient states predicted to occur during the dynamical process.","sentences":["Topology is a powerful tool for categorizing magnetization textures, highlighting specific features in both 2D systems, such as thin films or curved surfaces, and in 3D bulk systems.","In the emerging field of 3D nanomagnetism within confined geometries, the contributions from both volume and surface must be considered, requiring appropriate topological analysis to obtain a complete view of the system.","Here, we consider domain walls in cylindrical magnetic nanowires to illustrate the use of topological invariants.","We begin with micromagnetic simulations of domain wall transformation under the stimulus of an \\OErsted field, tracking bulk and surface topological signatures, and analyzing the interplay between multiple micromagnetic objects.","For instance, the extensive analysis allowed us to highlight mechanisms of domain wall type conversion from topologically non-trivial to trivial states, a phenomenon disregarded in previous studies.","Additionally, we provide experimental evidence of the transient states predicted to occur during the dynamical process."],"url":"http://arxiv.org/abs/2403.15343v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 16:50:07","title":"Photonic cross noise spectroscopy of Majorana bound states","abstract":"We propose a route to detect Majorana bound states (MBSs) by coupling a topological superconductor to quantum dots (QDs) in a $pnp$ junction. Here, two MBSs are coherently coupled to electrons on two QDs, which recombine with holes to photons. We focus on the spectroscopy of cross-correlated shot noise and the polarization of the emitted photons. Our detection scheme allows us to probe the necessary condition for the emergence of MBSs, specifically, the existence of non-local triplet superconducting correlations and also the fundamental property that two MBSs comprise a single complex fermion. We compare our results to the ones obtained from non-topological quasi-MBSs (qMBSs) and establish a correspondence between the number of peaks in the cross-correlation with the number of MBSs in the system. Here, we can identify a tunneling regime that facilitates differentiation between topological MBSs and trivial qMBSs. Additionally, we test the robustness of the detection scheme by the addition of uncorrelated particles.","sentences":["We propose a route to detect Majorana bound states (MBSs) by coupling a topological superconductor to quantum dots (QDs) in a $pnp$ junction.","Here, two MBSs are coherently coupled to electrons on two QDs, which recombine with holes to photons.","We focus on the spectroscopy of cross-correlated shot noise and the polarization of the emitted photons.","Our detection scheme allows us to probe the necessary condition for the emergence of MBSs, specifically, the existence of non-local triplet superconducting correlations and also the fundamental property that two MBSs comprise a single complex fermion.","We compare our results to the ones obtained from non-topological quasi-MBSs (qMBSs) and establish a correspondence between the number of peaks in the cross-correlation with the number of MBSs in the system.","Here, we can identify a tunneling regime that facilitates differentiation between topological MBSs and trivial qMBSs.","Additionally, we test the robustness of the detection scheme by the addition of uncorrelated particles."],"url":"http://arxiv.org/abs/2403.15340v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 16:39:13","title":"Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications","abstract":"This paper presents a formation control approach for contactless gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended for monitoring the safety of human workers, especially those working at heights. In the proposed dynamic formation scheme, one UAV acts as the leader of the formation and is equipped with sensors for human worker detection and gesture recognition. The follower UAVs maintain a predetermined formation relative to the worker's position, thereby providing additional perspectives of the monitored scene. Hand gestures allow the human worker to specify movements and action commands for the UAV team and initiate other mission-related commands without the need for an additional communication channel or specific markers. Together with a novel unified human detection and tracking algorithm, human pose estimation approach and gesture detection pipeline, the proposed approach forms a first instance of an HSI system incorporating all these modules onboard real-world UAVs. Simulations and field experiments with three UAVs and a human worker in a mock-up scenario showcase the effectiveness and responsiveness of the proposed approach.","sentences":["This paper presents a formation control approach for contactless gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor Unmanned Aerial Vehicles (UAVs) and a human worker.","The approach is intended for monitoring the safety of human workers, especially those working at heights.","In the proposed dynamic formation scheme, one UAV acts as the leader of the formation and is equipped with sensors for human worker detection and gesture recognition.","The follower UAVs maintain a predetermined formation relative to the worker's position, thereby providing additional perspectives of the monitored scene.","Hand gestures allow the human worker to specify movements and action commands for the UAV team and initiate other mission-related commands without the need for an additional communication channel or specific markers.","Together with a novel unified human detection and tracking algorithm, human pose estimation approach and gesture detection pipeline, the proposed approach forms a first instance of an HSI system incorporating all these modules onboard real-world UAVs.","Simulations and field experiments with three UAVs and a human worker in a mock-up scenario showcase the effectiveness and responsiveness of the proposed approach."],"url":"http://arxiv.org/abs/2403.15333v1","category":"cs.RO"}
{"created":"2024-03-22 16:35:24","title":"Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models","abstract":"Data-driven control uses a past signal trajectory to characterise the input-output behaviour of a system. Willems' lemma provides a data-based prediction model allowing a control designer to bypass the step of identifying a state-space or transfer function model. This paper provides a more parsimonious formulation of Willems' lemma that separates the model into initial condition matching and predictive control design parts. This avoids the need for regularisers in the predictive control problem that are found in other data-driven predictive control methods. It also gives a closed form expression for the optimal (minimum variance) unbiased predictor of the future output trajectory and applies it for predictive control. Simulation comparisons illustrate very good control performance.","sentences":["Data-driven control uses a past signal trajectory to characterise the input-output behaviour of a system.","Willems' lemma provides a data-based prediction model allowing a control designer to bypass the step of identifying a state-space or transfer function model.","This paper provides a more parsimonious formulation of Willems' lemma that separates the model into initial condition matching and predictive control design parts.","This avoids the need for regularisers in the predictive control problem that are found in other data-driven predictive control methods.","It also gives a closed form expression for the optimal (minimum variance) unbiased predictor of the future output trajectory and applies it for predictive control.","Simulation comparisons illustrate very good control performance."],"url":"http://arxiv.org/abs/2403.15329v1","category":"eess.SY"}
{"created":"2024-03-22 16:34:33","title":"Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search","abstract":"In this paper we present a comprehensive design and benchmarking study of Content Addressable Memory (CAM) at the 7nm technology node in the context of similarity search applications. We design CAM cells based on SRAM, spin-orbit torque, and ferroelectric field effect transistor devices and from their layouts extract cell parasitics using state of the art EDA tools. These parasitics are used to develop SPICE netlists to model search operations. We use a CAM-based dataset search and a sequential recommendation system to highlight the application-level performance degradation due to interconnect parasitics. We propose and evaluate two solutions to mitigate interconnect effects.","sentences":["In this paper we present a comprehensive design and benchmarking study of Content Addressable Memory (CAM) at the 7nm technology node in the context of similarity search applications.","We design CAM cells based on SRAM, spin-orbit torque, and ferroelectric field effect transistor devices and from their layouts extract cell parasitics using state of the art EDA tools.","These parasitics are used to develop SPICE netlists to model search operations.","We use a CAM-based dataset search and a sequential recommendation system to highlight the application-level performance degradation due to interconnect parasitics.","We propose and evaluate two solutions to mitigate interconnect effects."],"url":"http://arxiv.org/abs/2403.15328v1","category":"cs.ET"}
{"created":"2024-03-22 16:34:13","title":"On two-sample testing for data with arbitrarily missing values","abstract":"We develop a new rank-based approach for univariate two-sample testing in the presence of missing data which makes no assumptions about the missingness mechanism. This approach is a theoretical extension of the Wilcoxon-Mann-Whitney test that controls the Type I error by providing exact bounds for the test statistic after accounting for the number of missing values. Greater statistical power is shown when the method is extended to account for a bounded domain. Furthermore, exact bounds are provided on the proportions of data that can be missing in the two samples while yielding a significant result. Simulations demonstrate that our method has good power, typically for cases of $10\\%$ to $20\\%$ missing data, while standard imputation approaches fail to control the Type I error. We illustrate our method on complex clinical trial data in which patients' withdrawal from the trial lead to missing values.","sentences":["We develop a new rank-based approach for univariate two-sample testing in the presence of missing data which makes no assumptions about the missingness mechanism.","This approach is a theoretical extension of the Wilcoxon-Mann-Whitney test that controls the Type I error by providing exact bounds for the test statistic after accounting for the number of missing values.","Greater statistical power is shown when the method is extended to account for a bounded domain.","Furthermore, exact bounds are provided on the proportions of data that can be missing in the two samples while yielding a significant result.","Simulations demonstrate that our method has good power, typically for cases of $10\\%$ to $20\\%$ missing data, while standard imputation approaches fail to control the Type I error.","We illustrate our method on complex clinical trial data in which patients' withdrawal from the trial lead to missing values."],"url":"http://arxiv.org/abs/2403.15327v1","category":"stat.ME"}
{"created":"2024-03-22 16:25:34","title":"ProvDeploy: Provenance-oriented Containerization of High Performance Computing Scientific Workflows","abstract":"Many existing scientific workflows require High Performance Computing environments to produce results in a timely manner. These workflows have several software library components and use different environments, making the deployment and execution of the software stack not trivial. This complexity increases if the user needs to add provenance data capture services to the workflow. This manuscript introduces ProvDeploy to assist the user in configuring containers for scientific workflows with integrated provenance data capture. ProvDeploy was evaluated with a Scientific Machine Learning workflow, exploring containerization strategies focused on provenance in two distinct HPC environments","sentences":["Many existing scientific workflows require High Performance Computing environments to produce results in a timely manner.","These workflows have several software library components and use different environments, making the deployment and execution of the software stack not trivial.","This complexity increases if the user needs to add provenance data capture services to the workflow.","This manuscript introduces ProvDeploy to assist the user in configuring containers for scientific workflows with integrated provenance data capture.","ProvDeploy was evaluated with a Scientific Machine Learning workflow, exploring containerization strategies focused on provenance in two distinct HPC environments"],"url":"http://arxiv.org/abs/2403.15324v1","category":"cs.DC"}
{"created":"2024-03-22 16:08:39","title":"Quantum Fluctuations Suppress the Critical Fields in BaCo$_2$(AsO$_4$)$_2$","abstract":"Early efforts to realize exotic quantum ground states in frustrated magnets focused on frustration arising from the lattice geometry alone. Attention has shifted to bond-dependent anisotropic interactions, as well as further-neighbor interactions, on non-geometrically-frustrated lattices due to their greater versatility. The honeycomb magnet BaCo$_2$(AsO$_4$)$_2$ recently emerged as a candidate host for both bond-dependent (e.g. Kitaev) and third-neighbor ($J_3$) interactions, and has become a model experimental system due to its relatively low levels of disorder. Understanding the relative importance of different exchange interactions holds the key to achieving novel ground states, such as quantum spin liquids. Here, we use the magnetotropic susceptibility to map out the intermediate and high-field phase diagram of BaCo$_2$(AsO$_4$)$_2$ as a function of the out-of-plane magnetic field direction at $T = 1.6$ K. We show that the experimental data are qualitatively consistent with classical Monte Carlo results of the XXZ-$J_1$-$J_3$ model with small Kitaev and off-diagonal exchange couplings included. However, the calculated critical fields are systematically larger than the experimental values. Infinite-DMRG computations on the quantum model reveal that quantum corrections from a nearby ferromagnetic state are likely responsible for the suppressed critical fields. Together, our experiment and theory analyses demonstrate that, while quantum fluctuations play an important role in determining the phase diagram, most of the physics of BaCo$_2$(AsO$_4$)$_2$ can be understood in terms of the classical dynamics of long-range ordered states, leaving little room for the possibility of a quantum spin liquid.","sentences":["Early efforts to realize exotic quantum ground states in frustrated magnets focused on frustration arising from the lattice geometry alone.","Attention has shifted to bond-dependent anisotropic interactions, as well as further-neighbor interactions, on non-geometrically-frustrated lattices due to their greater versatility.","The honeycomb magnet BaCo$_2$(AsO$_4$)$_2$ recently emerged as a candidate host for both bond-dependent (e.g. Kitaev) and third-neighbor ($J_3$) interactions, and has become a model experimental system due to its relatively low levels of disorder.","Understanding the relative importance of different exchange interactions holds the key to achieving novel ground states, such as quantum spin liquids.","Here, we use the magnetotropic susceptibility to map out the intermediate and high-field phase diagram of BaCo$_2$(AsO$_4$)$_2$ as a function of the out-of-plane magnetic field direction at $T = 1.6$ K. We show that the experimental data are qualitatively consistent with classical Monte Carlo results of the XXZ-$J_1$-$J_3$ model with small Kitaev and off-diagonal exchange couplings included.","However, the calculated critical fields are systematically larger than the experimental values.","Infinite-DMRG computations on the quantum model reveal that quantum corrections from a nearby ferromagnetic state are likely responsible for the suppressed critical fields.","Together, our experiment and theory analyses demonstrate that, while quantum fluctuations play an important role in determining the phase diagram, most of the physics of BaCo$_2$(AsO$_4$)$_2$ can be understood in terms of the classical dynamics of long-range ordered states, leaving little room for the possibility of a quantum spin liquid."],"url":"http://arxiv.org/abs/2403.15315v1","category":"cond-mat.str-el"}
{"created":"2024-03-22 15:55:50","title":"X-ray emission spectrum for axion-photon conversion in magnetospheres of strongly magnetized neutron stars","abstract":"Detecting axionic dark matter (DM) could be possible in an X-ray spectrum from strongly magnetized neutron stars (NSs). We examine the possibility of axion-photon conversion in the magnetospheres of strongly magnetized NSs. In the current work, we investigate how the modified Tolman Oppenheimer Volkoff (TOV) system of equations (in the presence of a magnetic field) affects the energy spectrum of axions and axions-converted photon flux. We have considered the distance-dependent magnetic field in the modified TOV equations. We employ three different equations of states (EoSs) to solve these equations. We obtain the axions emission rate by including the Cooper-pair-breaking formation process (PBF) and Bremsstrahlung process in the core of NSs using the NSCool code. We primarily focus on three NSs: PSR B0531+21, PSR J0538+2817, and one Magnificient seven (M7) star RXJ 1856.5-3754. We further investigate the impact of the magnetic field on the actual observables, such as axion`s energy spectrum and axion-photon flux. We also compare our calculated axion-photon flux from all available archival data from PN+MOS+Chandra. Our predicted axion-photon flux values as a function of axion`s energy closely follow the experimentally archival data, which allows us to put bounds on the axion`s mass for the three different EoS.13","sentences":["Detecting axionic dark matter (DM) could be possible in an X-ray spectrum from strongly magnetized neutron stars (NSs).","We examine the possibility of axion-photon conversion in the magnetospheres of strongly magnetized NSs.","In the current work, we investigate how the modified Tolman Oppenheimer Volkoff (TOV) system of equations (in the presence of a magnetic field) affects the energy spectrum of axions and axions-converted photon flux.","We have considered the distance-dependent magnetic field in the modified TOV equations.","We employ three different equations of states (EoSs) to solve these equations.","We obtain the axions emission rate by including the Cooper-pair-breaking formation process (PBF) and Bremsstrahlung process in the core of NSs using the NSCool code.","We primarily focus on three NSs: PSR B0531+21, PSR J0538+2817, and one Magnificient seven (M7) star RXJ 1856.5-3754.","We further investigate the impact of the magnetic field on the actual observables, such as axion`s energy spectrum and axion-photon flux.","We also compare our calculated axion-photon flux from all available archival data from PN+MOS+Chandra.","Our predicted axion-photon flux values as a function of axion`s energy closely follow the experimentally archival data, which allows us to put bounds on the axion`s mass for the three different EoS.13"],"url":"http://arxiv.org/abs/2403.15305v1","category":"astro-ph.HE"}
{"created":"2024-03-22 15:45:09","title":"On the matching complexes of categorical product of path graphs","abstract":"The matching complex $\\mathsf{M}(G)$ of a graph $G$ is a simplicial complex whose simplices are matchings in $G$. These complexes appears in various places and found applications in many areas of mathematics including; discrete geometry, representation theory, combinatorics, etc. In this article, we consider the matching complexes of categorical product $P_n \\times P_m$ of path graphs $P_n$ and $P_m$. For $m = 1$, $P_n \\times P_m$ is a discrete graph and therefore its matching complex is the void complex. For $m = 2$, $\\mathsf{M}(P_n \\times P_m)$ has been proved to be homotopy equivalent to a wedge of spheres by Kozlov. We show that for $n \\geq 2$ and $3 \\leq m \\leq 5$, the matching complex of $P_n \\times P_m$ is homotopy equivalent to a wedge of spheres. For $m =3$, we give a closed form formula for the number and dimension of spheres appearing in the wedge. Further, for $m \\in \\{4, 5\\}$, we give minimum and maximum dimension of spheres appearing in the wedge in the homotopy type of $\\mathsf{M}(P_n \\times P_m)$.","sentences":["The matching complex $\\mathsf{M}(G)$ of a graph $G$ is a simplicial complex whose simplices are matchings in $G$. These complexes appears in various places and found applications in many areas of mathematics including; discrete geometry, representation theory, combinatorics, etc.","In this article, we consider the matching complexes of categorical product $P_n \\times P_m$ of path graphs $P_n$ and $P_m$. For $m = 1$, $P_n \\times P_m$ is a discrete graph and therefore its matching complex is the void complex.","For $m = 2$, $\\mathsf{M}(P_n \\times P_m)$ has been proved to be homotopy equivalent to a wedge of spheres by Kozlov.","We show that for $n \\geq 2$ and $3 \\leq m \\leq 5$, the matching complex of $P_n \\times P_m$ is homotopy equivalent to a wedge of spheres.","For $m =3$, we give a closed form formula for the number and dimension of spheres appearing in the wedge.","Further, for $m \\in \\{4, 5\\}$, we give minimum and maximum dimension of spheres appearing in the wedge in the homotopy type of $\\mathsf{M}(P_n \\times P_m)$."],"url":"http://arxiv.org/abs/2403.15298v1","category":"math.CO"}
{"created":"2024-03-22 15:40:59","title":"Complete quantum control of orbital qubits by phase-controlled stimulated Raman transitions","abstract":"Complete quantum control of a stationary quantum bit embedded in a quantum emitter is crucial for photonic quantum information technologies. Recently, the orbital degree of freedom in optically active semiconductor quantum dots emerged as a promising candidate. However, the crucial ability to perform arbitrary rotation on orbital qubits remains elusive. Here, we demonstrate complete control of hole orbital states in a quantum dot. This is enabled by successfully inducing stimulated Raman transitions within $\\Lambda$ systems connected via radiative Auger transitions. This new capability allows manipulations of polar and azimuth angles of the Bloch vector, as evidenced by Rabi oscillations and Ramsey interference, respectively. Simultaneous control of both parameters is achieved by concurrently varying the amplitude and phase of picosecond Raman pulses, enabling arbitrary unitary rotation of the Bloch vector. Our results establish the orbital states in solid-state quantum emitters as a potentially viable resource for applications in quantum information processing and quantum communication.","sentences":["Complete quantum control of a stationary quantum bit embedded in a quantum emitter is crucial for photonic quantum information technologies.","Recently, the orbital degree of freedom in optically active semiconductor quantum dots emerged as a promising candidate.","However, the crucial ability to perform arbitrary rotation on orbital qubits remains elusive.","Here, we demonstrate complete control of hole orbital states in a quantum dot.","This is enabled by successfully inducing stimulated Raman transitions within $\\Lambda$ systems connected via radiative Auger transitions.","This new capability allows manipulations of polar and azimuth angles of the Bloch vector, as evidenced by Rabi oscillations and Ramsey interference, respectively.","Simultaneous control of both parameters is achieved by concurrently varying the amplitude and phase of picosecond Raman pulses, enabling arbitrary unitary rotation of the Bloch vector.","Our results establish the orbital states in solid-state quantum emitters as a potentially viable resource for applications in quantum information processing and quantum communication."],"url":"http://arxiv.org/abs/2403.15295v1","category":"quant-ph"}
{"created":"2024-03-22 15:37:33","title":"Event-Triggered State Estimation Through Confidence Level","abstract":"This paper considers the state estimation problem for discrete-time linear systems under event-triggered scheme. In order to improve performance, a novel event-triggered scheme based on confidence level is proposed using the chi-square distribution and mild regularity assumption. In terms of the novel event-triggered scheme, a minimum mean squared error (MMSE) state estimator is proposed using some results presented in this paper. Two algorithms for communication rate estimation of the proposed MMSE state estimator are developed where the first algorithm is based on information with one-step delay, and the second algorithm is based on information with two-step delay. The performance and effectiveness of the proposed MMSE state estimator and the two communication rate estimation algorithms are illustrated using a target tracking scenario.","sentences":["This paper considers the state estimation problem for discrete-time linear systems under event-triggered scheme.","In order to improve performance, a novel event-triggered scheme based on confidence level is proposed using the chi-square distribution and mild regularity assumption.","In terms of the novel event-triggered scheme, a minimum mean squared error (MMSE) state estimator is proposed using some results presented in this paper.","Two algorithms for communication rate estimation of the proposed MMSE state estimator are developed where the first algorithm is based on information with one-step delay, and the second algorithm is based on information with two-step delay.","The performance and effectiveness of the proposed MMSE state estimator and the two communication rate estimation algorithms are illustrated using a target tracking scenario."],"url":"http://arxiv.org/abs/2403.15289v1","category":"eess.SY"}
{"created":"2024-03-22 15:32:00","title":"Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms","abstract":"We consider the aeroelastic simulation of flexible mechanical structures submerged in subsonic fluid flows at low Mach numbers. The nonlinear kinematics of flexible bodies are described in the total Lagrangian formulation and discretized by finite elements. The aerodynamic loads are computed using the unsteady vortex-lattice method wherein a free wake is tracked over time. Each implicit time step in the dynamic simulation then requires solving a nonlinear equation system in the structural variables with additional aerodynamic load terms. Our focus here is on the efficient numerical solution of this system by accelerating the Newton algorithm. The particular structure of the aeroelastic nonlinear system suggests the structural derivative as an approximation to the full derivative in the linear Newton system. We investigate and compare two promising algorithms based in this approximation, a quasi-Newton type algorithm and a novel inexact Newton algorithm. Numerical experiments are performed on a flexible plate and on a wind turbine. Our computational results show that the approximation can indeed accelerate the Newton algorithm substantially. Surprisingly, the theoretically preferable inexact Newton algorithm is much slower than the quasi-Newton algorithm, which motivates further research to speed up derivative evaluations.","sentences":["We consider the aeroelastic simulation of flexible mechanical structures submerged in subsonic fluid flows at low Mach numbers.","The nonlinear kinematics of flexible bodies are described in the total Lagrangian formulation and discretized by finite elements.","The aerodynamic loads are computed using the unsteady vortex-lattice method wherein a free wake is tracked over time.","Each implicit time step in the dynamic simulation then requires solving a nonlinear equation system in the structural variables with additional aerodynamic load terms.","Our focus here is on the efficient numerical solution of this system by accelerating the Newton algorithm.","The particular structure of the aeroelastic nonlinear system suggests the structural derivative as an approximation to the full derivative in the linear Newton system.","We investigate and compare two promising algorithms based in this approximation, a quasi-Newton type algorithm and a novel inexact Newton algorithm.","Numerical experiments are performed on a flexible plate and on a wind turbine.","Our computational results show that the approximation can indeed accelerate the Newton algorithm substantially.","Surprisingly, the theoretically preferable inexact Newton algorithm is much slower than the quasi-Newton algorithm, which motivates further research to speed up derivative evaluations."],"url":"http://arxiv.org/abs/2403.15286v1","category":"math.NA"}
{"created":"2024-03-22 15:19:11","title":"Inverse Design of Crystals and Quasicrystals in a Non-Additive Binary Mixture of Hard Disks","abstract":"The development of new materials typically involves a process of trial and error, guided by insights from past experimental and theoretical findings. The inverse design approach for soft-matter systems has the potential to optimize specific physical parameters such as particle interactions, particle shape, or composition and packing fraction. This optimization aims to facilitate the spontaneous formation of specific target structures through self-assembly. In this study, we expand upon a recently introduced inverse design protocol for monodisperse systems to identify the required conditions and interactions for assembling crystal and quasicrystal phases within a binary mixture of two distinct species. This method utilizes an evolutionary algorithm to identify the optimal state point and interaction parameters, enabling the self-assembly of the desired structure. Additionally, we employ a convolutional neural network (CNN) that classifies different phases based on their diffraction patterns, serving as a fitness function for the desired structure. Using our protocol, we successfully inverse design two-dimensional crystalline structures, including a hexagonal lattice, and a dodecagonal quasicrystal, within a non-additive binary mixture of hard disks. Finally, we introduce a symmetry-based order parameter that leverages the encoded symmetry within the diffraction pattern. This order parameter circumvents the need for training a CNN, and is used as a fitness function to inverse design an octagonal quasicrystal.","sentences":["The development of new materials typically involves a process of trial and error, guided by insights from past experimental and theoretical findings.","The inverse design approach for soft-matter systems has the potential to optimize specific physical parameters such as particle interactions, particle shape, or composition and packing fraction.","This optimization aims to facilitate the spontaneous formation of specific target structures through self-assembly.","In this study, we expand upon a recently introduced inverse design protocol for monodisperse systems to identify the required conditions and interactions for assembling crystal and quasicrystal phases within a binary mixture of two distinct species.","This method utilizes an evolutionary algorithm to identify the optimal state point and interaction parameters, enabling the self-assembly of the desired structure.","Additionally, we employ a convolutional neural network (CNN) that classifies different phases based on their diffraction patterns, serving as a fitness function for the desired structure.","Using our protocol, we successfully inverse design two-dimensional crystalline structures, including a hexagonal lattice, and a dodecagonal quasicrystal, within a non-additive binary mixture of hard disks.","Finally, we introduce a symmetry-based order parameter that leverages the encoded symmetry within the diffraction pattern.","This order parameter circumvents the need for training a CNN, and is used as a fitness function to inverse design an octagonal quasicrystal."],"url":"http://arxiv.org/abs/2403.15277v1","category":"cond-mat.soft"}
{"created":"2024-03-22 15:02:31","title":"Control contraction metrics on Lie groups","abstract":"In this paper, we extend the control contraction metrics (CCM) approach, which was originally proposed for the universal tracking control of nonlinear systems, to those that evolves on Lie groups. Our idea is to view the manifold as a constrained set that is embedded in Euclidean space, and then propose the sufficient conditions for the existence of a CCM and the associated controller design. Notably, we demonstrate that the search for CCM on Lie groups can be reformulated as convex conditions. The results extend the applicability of the CCM approach and provide a framework for analyzing the behavior of control systems with Lie group structures.","sentences":["In this paper, we extend the control contraction metrics (CCM) approach, which was originally proposed for the universal tracking control of nonlinear systems, to those that evolves on Lie groups.","Our idea is to view the manifold as a constrained set that is embedded in Euclidean space, and then propose the sufficient conditions for the existence of a CCM and the associated controller design.","Notably, we demonstrate that the search for CCM on Lie groups can be reformulated as convex conditions.","The results extend the applicability of the CCM approach and provide a framework for analyzing the behavior of control systems with Lie group structures."],"url":"http://arxiv.org/abs/2403.15264v1","category":"eess.SY"}
{"created":"2024-03-22 15:02:24","title":"Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models","abstract":"Federated learning (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associated with FL schemes for deterministic models is either impossible or results in sub-optimal performance. In this work, we use independent and identically distributed (IID) and non-IID partitions of the CIFAR-10 dataset and a fully variational ResNet-20 architecture to analyze six different aggregation strategies for Bayesian DL models. Additionally, we analyze the traditional federated averaging approach applied to an approximate Bayesian Monte Carlo dropout model as a lightweight alternative to more complex variational inference methods in FL. We show that aggregation strategy is a key hyperparameter in the design of a Bayesian FL system with downstream effects on accuracy, calibration, uncertainty quantification, training stability, and client compute requirements.","sentences":["Federated learning (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets.","Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications.","Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy.","Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associated with FL schemes for deterministic models is either impossible or results in sub-optimal performance.","In this work, we use independent and identically distributed (IID) and non-IID partitions of the CIFAR-10 dataset and a fully variational ResNet-20 architecture to analyze six different aggregation strategies for Bayesian DL models.","Additionally, we analyze the traditional federated averaging approach applied to an approximate Bayesian Monte Carlo dropout model as a lightweight alternative to more complex variational inference methods in FL.","We show that aggregation strategy is a key hyperparameter in the design of a Bayesian FL system with downstream effects on accuracy, calibration, uncertainty quantification, training stability, and client compute requirements."],"url":"http://arxiv.org/abs/2403.15263v1","category":"cs.LG"}
{"created":"2024-03-22 14:34:39","title":"Single-layer of Bi$_{1-x}$Sb$_x$ grown on Ag(111)","abstract":"In this work, we report the growth of a single mixed Bi$_{1-x}$Sb$_x$ layer, with diverse stoichiometries, on a Ag(111) substrate. The atomic geometry has been thoroughly investigated by low energy electron diffraction, scanning tunneling microscopy, and X-ray photoelectron spectroscopy experiments, as well as calculations based on density functional theory (DFT). We first determined that both pure systems (Bi/Ag(111) and Sb/Ag(111)) show similar behaviors: they form surface alloys with ($\\sqrt{3}\\times\\sqrt{3}$)R30$^\\circ$ periodicity for coverages lower than 1/3 ML, and undergo a dealloying transition for higher coverages up to 2/3 ML. We then established a simple preparation procedure to obtain a mixed Bi-Sb overlayer on Ag(111): it is essential to start with a surface completely covered by either of the two pure surface alloys and then deposit the other element on it. The energetics derived from DFT calculations provide insight into the systems preference towards the formation of this phase, and also predict a pathway to the formation of Bi-rich non-alloyed phases. The obtained mixed Bi-Sb phase has a lateral atomic arrangement very similar to the one in the non-alloyed phase observed for Sb on Ag(111), with Sb and Bi atoms distributed disorderly, and presents a significant vertical corrugation, promising considerable Rashba effects.","sentences":["In this work, we report the growth of a single mixed Bi$_{1-x}$Sb$_x$ layer, with diverse stoichiometries, on a Ag(111) substrate.","The atomic geometry has been thoroughly investigated by low energy electron diffraction, scanning tunneling microscopy, and X-ray photoelectron spectroscopy experiments, as well as calculations based on density functional theory (DFT).","We first determined that both pure systems (Bi/Ag(111) and Sb/Ag(111)) show similar behaviors: they form surface alloys with ($\\sqrt{3}\\times\\sqrt{3}$)R30$^\\circ$ periodicity for coverages lower than 1/3 ML, and undergo a dealloying transition for higher coverages up to 2/3 ML.","We then established a simple preparation procedure to obtain a mixed Bi-Sb overlayer on Ag(111): it is essential to start with a surface completely covered by either of the two pure surface alloys and then deposit the other element on it.","The energetics derived from DFT calculations provide insight into the systems preference towards the formation of this phase, and also predict a pathway to the formation of Bi-rich non-alloyed phases.","The obtained mixed Bi-Sb phase has a lateral atomic arrangement very similar to the one in the non-alloyed phase observed for Sb on Ag(111), with Sb and Bi atoms distributed disorderly, and presents a significant vertical corrugation, promising considerable Rashba effects."],"url":"http://arxiv.org/abs/2403.15242v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-22 14:32:37","title":"Information Rates of Successive Interference Cancellation for Optical Fiber","abstract":"Successive interference cancellation (SIC) is used to approach the achievable information rates (AIRs) of joint detection and decoding for long-haul optical fiber links. The AIRs of memoryless ring constellations are compared to those of circularly symmetric complex Gaussian modulation for surrogate channel models with correlated phase noise. Simulations are performed for 1000 km of standard single-mode fiber with ideal Raman amplification. In this setup, 32 rings and 16 SIC-stages with Gaussian message-passing receivers achieve the AIR peaks of previous work. The computational complexity scales in proportion to the number of SIC-stages, where one stage has the complexity of separate detection and decoding.","sentences":["Successive interference cancellation (SIC) is used to approach the achievable information rates (AIRs) of joint detection and decoding for long-haul optical fiber links.","The AIRs of memoryless ring constellations are compared to those of circularly symmetric complex Gaussian modulation for surrogate channel models with correlated phase noise.","Simulations are performed for 1000 km of standard single-mode fiber with ideal Raman amplification.","In this setup, 32 rings and 16 SIC-stages with Gaussian message-passing receivers achieve the AIR peaks of previous work.","The computational complexity scales in proportion to the number of SIC-stages, where one stage has the complexity of separate detection and decoding."],"url":"http://arxiv.org/abs/2403.15240v1","category":"cs.IT"}
{"created":"2024-03-22 14:29:50","title":"ACCESS: Assurance Case Centric Engineering of Safety-critical Systems","abstract":"Assurance cases are used to communicate and assess confidence in critical system properties such as safety and security. Historically, assurance cases have been manually created documents, which are evaluated by system stakeholders through lengthy and complicated processes. In recent years, model-based system assurance approaches have gained popularity to improve the efficiency and quality of system assurance activities. This becomes increasingly important, as systems becomes more complex, it is a challenge to manage their development life-cycles, including coordination of development, verification and validation activities, and change impact analysis in inter-connected system assurance artifacts. Moreover, there is a need for assurance cases that support evolution during the operational life of the system, to enable continuous assurance in the face of an uncertain environment, as Robotics and Autonomous Systems (RAS) are adopted into society. In this paper, we contribute ACCESS - Assurance Case Centric Engineering of Safety-critical Systems, an engineering methodology, together with its tool support, for the development of safety critical systems around evolving model-based assurance cases. We show how model-based system assurance cases can trace to heterogeneous engineering artifacts (e.g. system architectural models, system safety analysis, system behaviour models, etc.), and how formal methods can be integrated during the development process. We demonstrate how assurance cases can be automatically evaluated both at development and runtime. We apply our approach to a case study based on an Autonomous Underwater Vehicle (AUV).","sentences":["Assurance cases are used to communicate and assess confidence in critical system properties such as safety and security.","Historically, assurance cases have been manually created documents, which are evaluated by system stakeholders through lengthy and complicated processes.","In recent years, model-based system assurance approaches have gained popularity to improve the efficiency and quality of system assurance activities.","This becomes increasingly important, as systems becomes more complex, it is a challenge to manage their development life-cycles, including coordination of development, verification and validation activities, and change impact analysis in inter-connected system assurance artifacts.","Moreover, there is a need for assurance cases that support evolution during the operational life of the system, to enable continuous assurance in the face of an uncertain environment, as Robotics and Autonomous Systems (RAS) are adopted into society.","In this paper, we contribute ACCESS - Assurance Case Centric Engineering of Safety-critical Systems, an engineering methodology, together with its tool support, for the development of safety critical systems around evolving model-based assurance cases.","We show how model-based system assurance cases can trace to heterogeneous engineering artifacts (e.g. system architectural models, system safety analysis, system behaviour models, etc.), and how formal methods can be integrated during the development process.","We demonstrate how assurance cases can be automatically evaluated both at development and runtime.","We apply our approach to a case study based on an Autonomous Underwater Vehicle (AUV)."],"url":"http://arxiv.org/abs/2403.15236v1","category":"cs.SE"}
{"created":"2024-03-22 14:21:14","title":"On moment relaxations for linear state feedback controller synthesis with non-convex quadratic costs and constraints","abstract":"We present a simple and effective way to account for non-convex costs and constraints~in~state feedback synthesis, and an interpretation for the variables in which state feedback synthesis is typically convex. We achieve this by deriving the controller design using moment matrices of state and input. It turns out that this approach allows the consideration of non-convex constraints by relaxing them as expectation constraints, and that the variables in which state feedback synthesis is typically convexified can be identified with blocks of these moment matrices.","sentences":["We present a simple and effective way to account for non-convex costs and constraints~in~state feedback synthesis, and an interpretation for the variables in which state feedback synthesis is typically convex.","We achieve this by deriving the controller design using moment matrices of state and input.","It turns out that this approach allows the consideration of non-convex constraints by relaxing them as expectation constraints, and that the variables in which state feedback synthesis is typically convexified can be identified with blocks of these moment matrices."],"url":"http://arxiv.org/abs/2403.15228v1","category":"math.OC"}
{"created":"2024-03-22 14:13:51","title":"Linear magnetoelectricity in the Zintl phase pnictides (Ba, Ca, Sr)$\\mathrm{Mn}_2\\mathrm{(P, As, Sb)}_2$ from first principles calculations","abstract":"We report a comprehensive set of density functional theory calculations on the family of layered antiferromagnetic manganese pnictides (Ba, Ca, Sr)$\\mathrm{Mn}_2\\mathrm{(P, As, Sb)}_2$. We characterize all components to the linear magnetoelectric (ME) tensor $\\alpha$ which are parsed into their contributions from spin and orbital moments for both lattice-mediated and their clamped-ion electronic analogs. Our main results show that the orbital magnetization components cannot be neglected in these systems. The ME response is dominated by electronic effects with total $\\alpha$ values exceeding those of the prototypical $\\mathrm{Cr}_2\\mathrm{O}_3$ (i.e. $\\alpha \\simeq$ 6.79 ps/m in $\\mathrm{BaMn}_2\\mathrm{As}_2$). We also identify a strong correlation with the computed ME susceptibility on pnictogen substitution in the trigonal subfamily albeit with weaker amplitudes ($\\alpha \\simeq$ 0.2-1.7 ps/m). Additionally, we provide the dependence of these predictions on the Hubbard +U correction, at the level of the local density approximation, which show large variations on the calculated ME coefficients in the tetragonal compounds highlighting the role of strong correlation in these compounds.","sentences":["We report a comprehensive set of density functional theory calculations on the family of layered antiferromagnetic manganese pnictides (Ba, Ca, Sr)$\\mathrm{Mn}_2\\mathrm{(P, As, Sb)}_2$. We characterize all components to the linear magnetoelectric (ME) tensor $\\alpha$ which are parsed into their contributions from spin and orbital moments for both lattice-mediated and their clamped-ion electronic analogs.","Our main results show that the orbital magnetization components cannot be neglected in these systems.","The ME response is dominated by electronic effects with total $\\alpha$ values exceeding those of the prototypical $\\mathrm{Cr}_2\\mathrm{O}_3$ (i.e. $\\alpha \\simeq$ 6.79 ps/m in $\\mathrm{BaMn}_2\\mathrm{As}_2$).","We also identify a strong correlation with the computed ME susceptibility on pnictogen substitution in the trigonal subfamily albeit with weaker amplitudes ($\\alpha \\simeq$ 0.2-1.7 ps/m).","Additionally, we provide the dependence of these predictions on the Hubbard +U correction, at the level of the local density approximation, which show large variations on the calculated ME coefficients in the tetragonal compounds highlighting the role of strong correlation in these compounds."],"url":"http://arxiv.org/abs/2403.15222v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-22 14:11:36","title":"Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory","abstract":"The mutual information (MI) of Poisson-type channels has been linked to a filtering problem since the 70s, but its evaluation for specific continuous-time, discrete-state systems remains a demanding task. As an advantage, Markov renewal processes (MrP) retain their renewal property under state space filtering. This offers a way to solve the filtering problem analytically for small systems. We consider a class of communication systems $X \\to Y$ that can be derived from a MrP by a custom filtering procedure. For the subclasses, where (i) $Y$ is a renewal process or (ii) $(X,Y)$ belongs to a class of MrPs, we provide an evolution equation for finite transmission duration $T>0$ and limit theorems for $T \\to \\infty$ that facilitate simulation-free evaluation of the MI $\\mathbb{I}(X_{[0,T]}; Y_{[0,T]})$ and its associated mutual information rate (MIR). In other cases, simulation cost is significantly reduced. We show that systems with an additional $X$-modulating level $C$, which statically chooses between different processes $X_{[0,T]}(c)$, can naturally be included in our framework thereby giving an expression for $\\mathbb{I}(C; Y_{[0,T]})$. The theoretical framework is showcased in an application to bacterial gene expression, where filtering is analytically tractable.","sentences":["The mutual information (MI) of Poisson-type channels has been linked to a filtering problem since the 70s, but its evaluation for specific continuous-time, discrete-state systems remains a demanding task.","As an advantage, Markov renewal processes (MrP) retain their renewal property under state space filtering.","This offers a way to solve the filtering problem analytically for small systems.","We consider a class of communication systems $X \\to Y$ that can be derived from a MrP by a custom filtering procedure.","For the subclasses, where (i) $Y$ is a renewal process or (ii) $(X,Y)$ belongs to a class of MrPs, we provide an evolution equation for finite transmission duration $T>0$ and limit theorems for $T \\to \\infty$ that facilitate simulation-free evaluation of the MI $\\mathbb{I}(X_{[0,T]}; Y_{[0,T]})$ and its associated mutual information rate (MIR).","In other cases, simulation cost is significantly reduced.","We show that systems with an additional $X$-modulating level $C$, which statically chooses between different processes $X_{[0,T]}(c)$, can naturally be included in our framework thereby giving an expression for $\\mathbb{I}(C; Y_{[0,T]})$. The theoretical framework is showcased in an application to bacterial gene expression, where filtering is analytically tractable."],"url":"http://arxiv.org/abs/2403.15221v1","category":"cs.IT"}
{"created":"2024-03-22 14:06:49","title":"Multiphysics Numerical Method for Modeling Josephson Traveling-Wave Parametric Amplifiers","abstract":"Josephson traveling-wave parametric amplifiers (JTWPAs) are wideband, ultralow-noise amplifiers used to enable the readout of superconducting qubits. While individual JTWPAs have achieved high performance, behavior between devices is inconsistent due to wide manufacturing tolerances. Amplifier designs could be modified to improve resilience towards variations in amplifier components; however, existing device models often rely on analytical techniques that typically fail to incorporate component variations. To begin addressing this issue, a 1D numerical method for modeling JTWPAs is introduced in this work. The method treats the Josephson junctions and transmission lines in an amplifier as coupled subsystems and can easily incorporate arbitrary parameter variations. We discretize the transmission line subsystem with a finite element time domain method and the Josephson junction subsystem with a finite difference method, with leapfrog time marching used to evolve the system in time. We validate our method by comparing the computed gain to an analytical model for a traditional JTWPA architecture and one with resonant phase matching. We then use our method to demonstrate the impact of variations in Josephson junctions and phase-matching resonators on amplification. In future work, the method will be adjusted to incorporate additional amplifier architectures and extended to a 3D full-wave approach.","sentences":["Josephson traveling-wave parametric amplifiers (JTWPAs) are wideband, ultralow-noise amplifiers used to enable the readout of superconducting qubits.","While individual JTWPAs have achieved high performance, behavior between devices is inconsistent due to wide manufacturing tolerances.","Amplifier designs could be modified to improve resilience towards variations in amplifier components; however, existing device models often rely on analytical techniques that typically fail to incorporate component variations.","To begin addressing this issue, a 1D numerical method for modeling JTWPAs is introduced in this work.","The method treats the Josephson junctions and transmission lines in an amplifier as coupled subsystems and can easily incorporate arbitrary parameter variations.","We discretize the transmission line subsystem with a finite element time domain method and the Josephson junction subsystem with a finite difference method, with leapfrog time marching used to evolve the system in time.","We validate our method by comparing the computed gain to an analytical model for a traditional JTWPA architecture and one with resonant phase matching.","We then use our method to demonstrate the impact of variations in Josephson junctions and phase-matching resonators on amplification.","In future work, the method will be adjusted to incorporate additional amplifier architectures and extended to a 3D full-wave approach."],"url":"http://arxiv.org/abs/2403.15217v1","category":"physics.comp-ph"}
{"created":"2024-03-22 13:57:00","title":"Systematic study of flow vector decorrelation in $\\mathbf{\\sqrt{\\textit{s}_{_{\\bf NN}}}=5.02}$ TeV Pb-Pb collisions","abstract":"Measurements of the $p_{\\rm T}$-dependent flow vector fluctuations in Pb-Pb collisions at $\\sqrt{s_{_{\\rm NN}}} = 5.02~\\mathrm{TeV}$ using azimuthal correlations with the ALICE experiment at the LHC are presented. A four-particle correlation approach [1] is used to quantify the effects of flow angle and magnitude fluctuations separately. This paper extends previous studies to additional centrality intervals and provides measurements of the $p_{\\rm T}$-dependent flow vector fluctuations at $\\sqrt{s_{_{\\rm NN}}} = 5.02~\\mathrm{TeV}$ with two-particle correlations. Significant $p_{\\rm T}$-dependent fluctuations of the $\\vec{V}_{2}$ flow vector in Pb-Pb collisions are found across different centrality ranges, with the largest fluctuations of up to $\\sim$15% being present in the 5% most central collisions. In parallel, no evidence of significant $p_{\\rm T}$-dependent fluctuations of $\\vec{V}_{3}$ or $\\vec{V}_{4}$ is found. Additionally, evidence of flow angle and magnitude fluctuations is observed with more than $5\\sigma$ significance in central collisions. These observations in Pb-Pb collisions indicate where the classical picture of hydrodynamic modeling with a common symmetry plane breaks down. This has implications for hard probes at high $p_{\\rm T}$, which might be biased by $p_{\\rm T}$-dependent flow angle fluctuations of at least 23% in central collisions. Given the presented results, existing theoretical models should be re-examined to improve our understanding of initial conditions, quark--gluon plasma (QGP) properties, and the dynamic evolution of the created system.","sentences":["Measurements of the $p_{\\rm T}$-dependent flow vector fluctuations in Pb-Pb collisions at $\\sqrt{s_{_{\\rm NN}}} = 5.02~\\mathrm{TeV}$ using azimuthal correlations with the ALICE experiment at the LHC are presented.","A four-particle correlation approach [1] is used to quantify the effects of flow angle and magnitude fluctuations separately.","This paper extends previous studies to additional centrality intervals and provides measurements of the $p_{\\rm T}$-dependent flow vector fluctuations at $\\sqrt{s_{_{\\rm NN}}} = 5.02~\\mathrm{TeV}$ with two-particle correlations.","Significant $p_{\\rm T}$-dependent fluctuations of the $\\vec{V}_{2}$ flow vector in Pb-Pb collisions are found across different centrality ranges, with the largest fluctuations of up to $\\sim$15% being present in the 5% most central collisions.","In parallel, no evidence of significant $p_{\\rm T}$-dependent fluctuations of $\\vec{V}_{3}$ or $\\vec{V}_{4}$ is found.","Additionally, evidence of flow angle and magnitude fluctuations is observed with more than $5\\sigma$ significance in central collisions.","These observations in Pb-Pb collisions indicate where the classical picture of hydrodynamic modeling with a common symmetry plane breaks down.","This has implications for hard probes at high $p_{\\rm T}$, which might be biased by $p_{\\rm T}$-dependent flow angle fluctuations of at least 23% in central collisions.","Given the presented results, existing theoretical models should be re-examined to improve our understanding of initial conditions, quark--gluon plasma (QGP) properties, and the dynamic evolution of the created system."],"url":"http://arxiv.org/abs/2403.15213v1","category":"nucl-ex"}
{"created":"2024-03-22 13:53:22","title":"On The Relationship Between The Logarithmic Lower Order of Coefficients and The Growth of Solutions of Complex Linear Differential Equations in $\\overline{\\mathbb{C}}\\setminus\\{z_{0}\\}$","abstract":"In this article, we study the growth of solutions of the homogeneous complex linear differential equation \\begin{equation*}   f^{(k)}+A_{k-1}(z)f^{(k-1)}+\\cdots+A_{1}(z)f^{\\prime}+ A_{0}(z)f=0, \\end{equation*}% where the coefficients $A_{j}(z)$ $(j=0,1,\\ldots ,k-1)$ are analytic or meromorphic functions in $\\overline{\\mathbb{C}}\\setminus\\{z_{0}\\}$. Under the sufficient condition that there exists one dominant coefficient by its logarithmic lower order or by its logarithmic lower type. We extend some precedent results due to Liu, Long and Zeng and others.","sentences":["In this article, we study the growth of solutions of the homogeneous complex linear differential equation \\begin{equation*}   f^{(k)}+A_{k-1}(z)f^{(k-1)}+\\cdots+A_{1}(z)f^{\\prime}+ A_{0}(z)f=0, \\end{equation*}% where the coefficients $A_{j}(z)$ $(j=0,1,\\ldots ,k-1)$ are analytic or meromorphic functions in $\\overline{\\mathbb{C}}\\setminus\\{z_{0}\\}$. Under the sufficient condition that there exists one dominant coefficient by its logarithmic lower order or by its logarithmic lower type.","We extend some precedent results due to Liu, Long and Zeng and others."],"url":"http://arxiv.org/abs/2403.15211v1","category":"math.CV"}
{"created":"2024-03-22 13:49:53","title":"Robust optimization for adversarial learning with finite sample complexity guarantees","abstract":"Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations. In the case where uncertainty stems from the presence of adversarial attacks this need is becoming more prominent. In this paper we focus on linear and nonlinear classification problems and propose a novel adversarial training method for robust classifiers, inspired by Support Vector Machine (SVM) margins. We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios. Notably, our bounds match natural classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models. Numerical experiments on the benchmark MNIST and CIFAR10 datasets show our approach's comparable performance to state-of-the-art methods, without needing adversarial examples during training. Our work offers a comprehensive framework for enhancing binary linear and non-linear classifier robustness, embedding robustness in learning under the presence of adversaries.","sentences":["Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations.","In the case where uncertainty stems from the presence of adversarial attacks this need is becoming more prominent.","In this paper we focus on linear and nonlinear classification problems and propose a novel adversarial training method for robust classifiers, inspired by Support Vector Machine (SVM) margins.","We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios.","Notably, our bounds match natural classifiers' complexity.","Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models.","Numerical experiments on the benchmark MNIST and CIFAR10 datasets show our approach's comparable performance to state-of-the-art methods, without needing adversarial examples during training.","Our work offers a comprehensive framework for enhancing binary linear and non-linear classifier robustness, embedding robustness in learning under the presence of adversaries."],"url":"http://arxiv.org/abs/2403.15207v1","category":"cs.LG"}
{"created":"2024-03-22 13:46:51","title":"Coherent Phonon Control of Ultrafast Magnetization Dynamics in Fe$_\\text{3}$GeTe$_\\text{2}$ from Time-Dependent Ab Initio Theory","abstract":"Exploring ultrafast magnetization control in two-dimensional (2D) magnets through optically driven coherent phonons has been well-established. Yet, the microscopic interplay between spin dynamics and lattice degrees of freedom remains less explored. Employing real-time time-dependent density functional theory (rt-TDDFT) coupled with Ehrenfest dynamics, we systematically investigate laser-induced spin-nuclei dynamics with coherent phonon excitation in the 2D ferromagnet Fe3GeTe2. We found that selectively pre-exciting three typical coherent phonon modes results in up to a 53% additional spin moment loss in an out-of-plane A2 1g mode within ~50 fs. Coherent phonon control of spin dynamics is closely linked to laser pulse parameters. The underlying microscopic mechanism of this phenomenon is primarily governed by coherent phonon-induced asymmetric spin-resolved charge transfer following the disappearance of the laser pulse, thereby enabling effective control of the spin moment loss. Our findings offer a novel insight into the coupling of coherent phonons with spin systems in 2D limits on femtosecond timescales.","sentences":["Exploring ultrafast magnetization control in two-dimensional (2D) magnets through optically driven coherent phonons has been well-established.","Yet, the microscopic interplay between spin dynamics and lattice degrees of freedom remains less explored.","Employing real-time time-dependent density functional theory (rt-TDDFT) coupled with Ehrenfest dynamics, we systematically investigate laser-induced spin-nuclei dynamics with coherent phonon excitation in the 2D ferromagnet Fe3GeTe2.","We found that selectively pre-exciting three typical coherent phonon modes results in up to a 53% additional spin moment loss in an out-of-plane A2 1g mode within ~50 fs.","Coherent phonon control of spin dynamics is closely linked to laser pulse parameters.","The underlying microscopic mechanism of this phenomenon is primarily governed by coherent phonon-induced asymmetric spin-resolved charge transfer following the disappearance of the laser pulse, thereby enabling effective control of the spin moment loss.","Our findings offer a novel insight into the coupling of coherent phonons with spin systems in 2D limits on femtosecond timescales."],"url":"http://arxiv.org/abs/2403.15204v1","category":"physics.comp-ph"}
{"created":"2024-03-22 13:16:03","title":"Spectrum of $S$- and $P$-wave $cc\\bar{q}\\bar{q}'$ $(\\bar{q},\\bar{q}' = \\bar{u}, \\bar{d}, \\bar{s})$ systems in a chiral SU(3) quark model","abstract":"Inspired by the resonance $T_{cc}^+(3875)$ recently observed by the LHCb Collaboration, we systematically explore the $S$- and $P$-wave $cc\\bar{q}\\bar{q}'$ $(\\bar{q},\\bar{q}' = \\bar{u}, \\bar{d}, \\bar{s})$ systems in a chiral SU(3) quark model. The Hamiltonian contains the kinetic energy, the one-gluon-exchange (OGE) potential, the confinement potential, and the one-boson-exchange (OBE) potential stemming from the coupling of quark and chiral fields. The Schr\\\"odinger equation is solved by use of the variational method with the spacial trial wave functions chosen as Gaussian functions. It is found that the lowest state has a mass $3879$ MeV, isospin and spin-parity $IJ^P=01^+$, and quark constituent $cc\\bar{u}\\bar{d}$, in agreement with the experimentally observed $T_{cc}^+(3875)$. This state is approximately at the calculated $DD^\\ast$ threshold, and has a root-mean-square radius about $0.48$ fm. These demonstrates that the $T_{cc}^+(3875)$ can be accommodated as a stable and compact tetraquark sate in the chiral SU(3) quark model. All the other $S$- and $P$-wave $cc\\bar{q}\\bar{q}'$ $(\\bar{q},\\bar{q}' = \\bar{u}, \\bar{d}, \\bar{s})$ states lie about one hundred to few hundreds MeV higher than the corresponding meson-meson thresholds, and thus are not suggested to be candidates of stable and compact tetraquark states due to their fall-apart decays to two mesons.","sentences":["Inspired by the resonance $T_{cc}^+(3875)$ recently observed by the LHCb Collaboration, we systematically explore the $S$- and $P$-wave $cc\\bar{q}\\bar{q}'$ $(\\bar{q},\\bar{q}' = \\bar{u}, \\bar{d}, \\bar{s})$ systems in a chiral SU(3) quark model.","The Hamiltonian contains the kinetic energy, the one-gluon-exchange (OGE) potential, the confinement potential, and the one-boson-exchange (OBE) potential stemming from the coupling of quark and chiral fields.","The Schr\\\"odinger equation is solved by use of the variational method with the spacial trial wave functions chosen as Gaussian functions.","It is found that the lowest state has a mass $3879$ MeV, isospin and spin-parity $IJ^P=01^+$, and quark constituent $cc\\bar{u}\\bar{d}$, in agreement with the experimentally observed $T_{cc}^+(3875)$. This state is approximately at the calculated $DD^\\ast$ threshold, and has a root-mean-square radius about $0.48$ fm.","These demonstrates that the $T_{cc}^+(3875)$ can be accommodated as a stable and compact tetraquark sate in the chiral SU(3) quark model.","All the other $S$- and $P$-wave $cc\\bar{q}\\bar{q}'$ $(\\bar{q},\\bar{q}' = \\bar{u}, \\bar{d}, \\bar{s})$ states lie about one hundred to few hundreds MeV higher than the corresponding meson-meson thresholds, and thus are not suggested to be candidates of stable and compact tetraquark states due to their fall-apart decays to two mesons."],"url":"http://arxiv.org/abs/2403.15187v1","category":"hep-ph"}
{"created":"2024-03-22 13:12:31","title":"Calabi-Yau threefolds with boundary","abstract":"We develop the deformation theory of Calabi-Yau threefolds, by which we mean 3-dimensional complex manifolds with a nowhere-vanishing holomorphic 3-form, on manifolds with boundary. The boundary data is a closed, real 3-form on the 5-dimensional boundary. In the case of strongly pseudoconvex boundary, we obtain an analogue of Hitchin's local Torelli Theorem for compact manifolds, modulo a finite dimensional obstruction space, which we show is zero in many cases of interest.","sentences":["We develop the deformation theory of Calabi-Yau threefolds, by which we mean 3-dimensional complex manifolds with a nowhere-vanishing holomorphic 3-form, on manifolds with boundary.","The boundary data is a closed, real 3-form on the 5-dimensional boundary.","In the case of strongly pseudoconvex boundary, we obtain an analogue of Hitchin's local Torelli Theorem for compact manifolds, modulo a finite dimensional obstruction space, which we show is zero in many cases of interest."],"url":"http://arxiv.org/abs/2403.15184v1","category":"math.DG"}
{"created":"2024-03-22 12:54:33","title":"LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels","abstract":"Autonomous systems need to process large-scale, sparse, and irregular point clouds with limited compute resources. Consequently, it is essential to develop LiDAR perception methods that are both efficient and effective. Although naively enlarging 3D kernel size can enhance performance, it will also lead to a cubically-increasing overhead. Therefore, it is crucial to develop streamlined 3D large kernel designs that eliminate redundant weights and work effectively with larger kernels. In this paper, we propose an efficient and effective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages dynamic pruning to amplify the 3D kernel size. Our method comprises two core components: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight Selection (CWS). SDS dynamically prunes and regrows volumetric weights from the beginning to learn a large sparse 3D kernel. It not only boosts performance but also significantly reduces model size and computational cost. Moreover, CWS selects the most important channels for 3D convolution during training and subsequently prunes the redundant channels to accelerate inference for 3D vision tasks. We demonstrate the effectiveness of LSK3DNet on three benchmark datasets and five tracks compared with classical models and large kernel designs. Notably, LSK3DNet achieves the state-of-the-art performance on SemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with roughly 40% model size reduction and 60% computing operations reduction compared to the naive large 3D kernel model.","sentences":["Autonomous systems need to process large-scale, sparse, and irregular point clouds with limited compute resources.","Consequently, it is essential to develop LiDAR perception methods that are both efficient and effective.","Although naively enlarging 3D kernel size can enhance performance, it will also lead to a cubically-increasing overhead.","Therefore, it is crucial to develop streamlined 3D large kernel designs that eliminate redundant weights and work effectively with larger kernels.","In this paper, we propose an efficient and effective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages dynamic pruning to amplify the 3D kernel size.","Our method comprises two core components: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight Selection (CWS).","SDS dynamically prunes and regrows volumetric weights from the beginning to learn a large sparse 3D kernel.","It not only boosts performance but also significantly reduces model size and computational cost.","Moreover, CWS selects the most important channels for 3D convolution during training and subsequently prunes the redundant channels to accelerate inference for 3D vision tasks.","We demonstrate the effectiveness of LSK3DNet on three benchmark datasets and five tracks compared with classical models and large kernel designs.","Notably, LSK3DNet achieves the state-of-the-art performance on SemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with roughly 40% model size reduction and 60% computing operations reduction compared to the naive large 3D kernel model."],"url":"http://arxiv.org/abs/2403.15173v1","category":"cs.CV"}
{"created":"2024-03-22 12:48:00","title":"AV-Occupant Perceived Risk Model for Cut-In Scenarios with Empirical Evaluation","abstract":"Advancements in autonomous vehicle (AV) technologies necessitate precise estimation of perceived risk to enhance user comfort, acceptance and trust. This paper introduces a novel AV-Occupant Risk (AVOR) model designed for perceived risk estimation during AV cut-in scenarios. An empirical study is conducted with 18 participants with realistic cut-in scenarios. Two factors were investigated: scenario risk and scene population. 76% of subjective risk responses indicate an increase in perceived risk at cut-in initiation. The existing perceived risk model did not capture this critical phenomenon. Our AVOR model demonstrated a significant improvement in estimating perceived risk during the early stages of cut-ins, especially for the high-risk scenario, enhancing modelling accuracy by up to 54%. The concept of the AVOR model can quantify perceived risk in other diverse driving contexts characterized by dynamic uncertainties, enhancing the reliability and human-centred focus of AV systems.","sentences":["Advancements in autonomous vehicle (AV) technologies necessitate precise estimation of perceived risk to enhance user comfort, acceptance and trust.","This paper introduces a novel AV-Occupant Risk (AVOR) model designed for perceived risk estimation during AV cut-in scenarios.","An empirical study is conducted with 18 participants with realistic cut-in scenarios.","Two factors were investigated: scenario risk and scene population.","76% of subjective risk responses indicate an increase in perceived risk at cut-in initiation.","The existing perceived risk model did not capture this critical phenomenon.","Our AVOR model demonstrated a significant improvement in estimating perceived risk during the early stages of cut-ins, especially for the high-risk scenario, enhancing modelling accuracy by up to 54%.","The concept of the AVOR model can quantify perceived risk in other diverse driving contexts characterized by dynamic uncertainties, enhancing the reliability and human-centred focus of AV systems."],"url":"http://arxiv.org/abs/2403.15171v1","category":"cs.RO"}
{"created":"2024-03-22 12:31:08","title":"Symmetries & Correlations in Continous Time Crystals","abstract":"We demonstrate the inadequacy of mean-field theory by exploring the effects of initial state correlations on the dynamics of continuous time crystals, necessitating higher-order cumulant expansions. We exemplify this using cat states for which the mean field fails to predict a phase transition but the second order cumulant expansion theory captures it. Motivated by the symmetries of the system, we choose a truncation of cumulant theory at the second-order and demonstrate that it is sufficient to accurately capture the dynamical features overlooked by the mean-field.","sentences":["We demonstrate the inadequacy of mean-field theory by exploring the effects of initial state correlations on the dynamics of continuous time crystals, necessitating higher-order cumulant expansions.","We exemplify this using cat states for which the mean field fails to predict a phase transition but the second order cumulant expansion theory captures it.","Motivated by the symmetries of the system, we choose a truncation of cumulant theory at the second-order and demonstrate that it is sufficient to accurately capture the dynamical features overlooked by the mean-field."],"url":"http://arxiv.org/abs/2403.15164v1","category":"quant-ph"}
{"created":"2024-03-22 12:20:26","title":"New completeness theorems on the boundary in Elasticity","abstract":"The completeness on the boundary (in the sense of Picone) of certain systems related to the III and IV BVPs for the elasticity system is proved. The completeness is obtained in both $L^p$ ($1\\leq 1<\\infty$) and uniform norms.","sentences":["The completeness on the boundary (in the sense of Picone) of certain systems related to the III and IV BVPs for the elasticity system is proved.","The completeness is obtained in both $L^p$ ($1\\leq 1<\\infty$) and uniform norms."],"url":"http://arxiv.org/abs/2403.15162v1","category":"math.AP"}
{"created":"2024-03-22 12:16:48","title":"Logarithm laws for BCZ map","abstract":"We present the logarithm laws for partial sum of itinerary function over non-periodic BCZ orbit using the even and odd diophantine exponent defined by Athreya-Margulis. We give the detailed description of BCZ map and introduce its excursion.","sentences":["We present the logarithm laws for partial sum of itinerary function over non-periodic BCZ orbit using the even and odd diophantine exponent defined by Athreya-Margulis.","We give the detailed description of BCZ map and introduce its excursion."],"url":"http://arxiv.org/abs/2403.15160v1","category":"math.DS"}
{"created":"2024-03-22 12:11:06","title":"Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective","abstract":"Environmental perception in Automated Valet Parking (AVP) has been a challenging task due to severe occlusions in parking garages. Although Collaborative Perception (CP) can be applied to broaden the field of view of connected vehicles, the limited bandwidth of vehicular communications restricts its application. In this work, we propose a BEV feature-based CP network architecture for infrastructure-assisted AVP systems. The model takes the roadside camera and LiDAR as optional inputs and adaptively fuses them with onboard sensors in a unified BEV representation. Autoencoder and downsampling are applied for channel-wise and spatial-wise dimension reduction, while sparsification and quantization further compress the feature map with little loss in data precision. Combining these techniques, the size of a BEV feature map is effectively compressed to fit in the feasible data rate of the NR-V2X network. With the synthetic AVP dataset, we observe that CP can effectively increase perception performance, especially for pedestrians. Moreover, the advantage of infrastructure-assisted CP is demonstrated in two typical safety-critical scenarios in the AVP setting, increasing the maximum safe cruising speed by up to 3m/s in both scenarios.","sentences":["Environmental perception in Automated Valet Parking (AVP) has been a challenging task due to severe occlusions in parking garages.","Although Collaborative Perception (CP) can be applied to broaden the field of view of connected vehicles, the limited bandwidth of vehicular communications restricts its application.","In this work, we propose a BEV feature-based CP network architecture for infrastructure-assisted AVP systems.","The model takes the roadside camera and LiDAR as optional inputs and adaptively fuses them with onboard sensors in a unified BEV representation.","Autoencoder and downsampling are applied for channel-wise and spatial-wise dimension reduction, while sparsification and quantization further compress the feature map with little loss in data precision.","Combining these techniques, the size of a BEV feature map is effectively compressed to fit in the feasible data rate of the NR-V2X network.","With the synthetic AVP dataset, we observe that CP can effectively increase perception performance, especially for pedestrians.","Moreover, the advantage of infrastructure-assisted CP is demonstrated in two typical safety-critical scenarios in the AVP setting, increasing the maximum safe cruising speed by up to 3m/s in both scenarios."],"url":"http://arxiv.org/abs/2403.15156v1","category":"cs.RO"}
{"created":"2024-03-22 12:09:14","title":"Spread complexity and dynamical transition in two-mode Bose-Einstein condensations","abstract":"We study the spread complexity in two-mode Bose-Einstein condensations and unveil that the long-time average of the spread complexity $\\overline{C}_{K}$ can probe the dynamical transition from self-trapping to Josephson oscillation. When the parameter $\\omega$ increases over a critical value $\\omega_c$, we reveal that the spread complexity exhibits a sharp transition from lower to higher value, with the corresponding phase space trajectory changing from self-trapping to Josephson oscillation. Moreover, we scrutinize the eigen-spectrum and uncover the relation between the dynamical transition and the excited state quantum phase transition, which is characterized by the emergence of singularity in the density of states at critical energy $E_{c}$. In the thermodynamical limit, the cross point of $E_{c}(\\omega)$ and the initial energy $E_{0}(\\omega)$ determines the dynamical transition point $\\omega_c$. Finally, we show that the different dynamical behavior for the initial state at a fixed point can be distinguished by the long-time average of the spread complexity, when the fixed point changes from unstable to stable.","sentences":["We study the spread complexity in two-mode Bose-Einstein condensations and unveil that the long-time average of the spread complexity $\\overline{C}_{K}$ can probe the dynamical transition from self-trapping to Josephson oscillation.","When the parameter $\\omega$ increases over a critical value $\\omega_c$, we reveal that the spread complexity exhibits a sharp transition from lower to higher value, with the corresponding phase space trajectory changing from self-trapping to Josephson oscillation.","Moreover, we scrutinize the eigen-spectrum and uncover the relation between the dynamical transition and the excited state quantum phase transition, which is characterized by the emergence of singularity in the density of states at critical energy $E_{c}$. In the thermodynamical limit, the cross point of $E_{c}(\\omega)$ and the initial energy $E_{0}(\\omega)$ determines the dynamical transition point $\\omega_c$. Finally, we show that the different dynamical behavior for the initial state at a fixed point can be distinguished by the long-time average of the spread complexity, when the fixed point changes from unstable to stable."],"url":"http://arxiv.org/abs/2403.15154v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-22 12:06:40","title":"An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning","abstract":"In recent years, Deep Learning has gained popularity for its ability to solve complex classification tasks, increasingly delivering better results thanks to the development of more accurate models, the availability of huge volumes of data and the improved computational capabilities of modern computers. However, these improvements in performance also bring efficiency problems, related to the storage of datasets and models, and to the waste of energy and time involved in both the training and inference processes. In this context, data reduction can help reduce energy consumption when training a deep learning model. In this paper, we present up to eight different methods to reduce the size of a tabular training dataset, and we develop a Python package to apply them. We also introduce a representativeness metric based on topology to measure how similar are the reduced datasets and the full training dataset. Additionally, we develop a methodology to apply these data reduction methods to image datasets for object detection tasks. Finally, we experimentally compare how these data reduction methods affect the representativeness of the reduced dataset, the energy consumption and the predictive performance of the model.","sentences":["In recent years, Deep Learning has gained popularity for its ability to solve complex classification tasks, increasingly delivering better results thanks to the development of more accurate models, the availability of huge volumes of data and the improved computational capabilities of modern computers.","However, these improvements in performance also bring efficiency problems, related to the storage of datasets and models, and to the waste of energy and time involved in both the training and inference processes.","In this context, data reduction can help reduce energy consumption when training a deep learning model.","In this paper, we present up to eight different methods to reduce the size of a tabular training dataset, and we develop a Python package to apply them.","We also introduce a representativeness metric based on topology to measure how similar are the reduced datasets and the full training dataset.","Additionally, we develop a methodology to apply these data reduction methods to image datasets for object detection tasks.","Finally, we experimentally compare how these data reduction methods affect the representativeness of the reduced dataset, the energy consumption and the predictive performance of the model."],"url":"http://arxiv.org/abs/2403.15150v1","category":"cs.LG"}
{"created":"2024-03-22 11:52:31","title":"ALPINE: a climbing robot for operations in mountain environments","abstract":"Mountain slopes are perfect examples of harsh environments in which humans are required to perform difficult and dangerous operations such as removing unstable boulders, dangerous vegetation or deploying safety nets. A good replacement for human intervention can be offered by climbing robots. The different solutions existing in the literature are not up to the task for the difficulty of the requirements (navigation, heavy payloads, flexibility in the execution of the tasks). In this paper, we propose a robotic platform that can fill this gap. Our solution is based on a robot that hangs on ropes, and uses a retractable leg to jump away from the mountain walls. Our package of mechanical solutions, along with the algorithms developed for motion planning and control, delivers swift navigation on irregular and steep slopes, the possibility to overcome or travel around significant natural barriers, and the ability to carry heavy payloads and execute complex tasks. In the paper, we give a full account of our main design and algorithmic choices and show the feasibility of the solution through a large number of physically simulated scenarios.","sentences":["Mountain slopes are perfect examples of harsh environments in which humans are required to perform difficult and dangerous operations such as removing unstable boulders, dangerous vegetation or deploying safety nets.","A good replacement for human intervention can be offered by climbing robots.","The different solutions existing in the literature are not up to the task for the difficulty of the requirements (navigation, heavy payloads, flexibility in the execution of the tasks).","In this paper, we propose a robotic platform that can fill this gap.","Our solution is based on a robot that hangs on ropes, and uses a retractable leg to jump away from the mountain walls.","Our package of mechanical solutions, along with the algorithms developed for motion planning and control, delivers swift navigation on irregular and steep slopes, the possibility to overcome or travel around significant natural barriers, and the ability to carry heavy payloads and execute complex tasks.","In the paper, we give a full account of our main design and algorithmic choices and show the feasibility of the solution through a large number of physically simulated scenarios."],"url":"http://arxiv.org/abs/2403.15142v1","category":"cs.RO"}
{"created":"2024-03-22 11:51:47","title":"Hybrid integrator-gain system based integral resonant controllers for negative imaginary systems","abstract":"We introduce a hybrid control system called a hybrid integrator-gain system (HIGS) based integral resonant controller (IRC) to stabilize negative imaginary (NI) systems. A HIGS-based IRC has a similar structure to an IRC, with the integrator replaced by a HIGS. We show that a HIGS-based IRC is an NI system. Also, for a SISO NI system with a minimal realization, we show there exists a HIGS-based IRC such that their closed-loop interconnection is asymptotically stable. Also, we propose a proportional-integral-double-integral resonant controller and a HIGS-based proportional-integral-double-integral resonant controller and show that both of them can be applied to asymptotically stabilize an NI system. An example is provided to illustrate the proposed results.","sentences":["We introduce a hybrid control system called a hybrid integrator-gain system (HIGS) based integral resonant controller (IRC) to stabilize negative imaginary (NI) systems.","A HIGS-based IRC has a similar structure to an IRC, with the integrator replaced by a HIGS.","We show that a HIGS-based IRC is an NI system.","Also, for a SISO NI system with a minimal realization, we show there exists a HIGS-based IRC such that their closed-loop interconnection is asymptotically stable.","Also, we propose a proportional-integral-double-integral resonant controller and a HIGS-based proportional-integral-double-integral resonant controller and show that both of them can be applied to asymptotically stabilize an NI system.","An example is provided to illustrate the proposed results."],"url":"http://arxiv.org/abs/2403.15140v1","category":"eess.SY"}
{"created":"2024-03-22 11:40:26","title":"Mixed finite element methods for linear Cosserat equations","abstract":"We consider the equilibrium equations for a linearized Cosserat material. We identify their structure in terms of a differential complex, which is isomorphic to six copies of the de Rham complex through an algebraic isomorphism. Moreover, we show how the Cosserat materials can be analyzed by inheriting results from linearized elasticity. Both perspectives give rise to mixed finite element methods, which we refer to as strongly and weaky coupled, respectively. We prove convergence of both classes of methods, with particular attention to improved convergence rate estimates, and stability in the limit of vanishing Cosserat material parameters. The theoretical results are fully reflected in the actual performance of the methods, as shown by the numerical verifications.","sentences":["We consider the equilibrium equations for a linearized Cosserat material.","We identify their structure in terms of a differential complex, which is isomorphic to six copies of the de Rham complex through an algebraic isomorphism.","Moreover, we show how the Cosserat materials can be analyzed by inheriting results from linearized elasticity.","Both perspectives give rise to mixed finite element methods, which we refer to as strongly and weaky coupled, respectively.","We prove convergence of both classes of methods, with particular attention to improved convergence rate estimates, and stability in the limit of vanishing Cosserat material parameters.","The theoretical results are fully reflected in the actual performance of the methods, as shown by the numerical verifications."],"url":"http://arxiv.org/abs/2403.15136v1","category":"math.NA"}
{"created":"2024-03-22 11:31:55","title":"Uplink soft handover for LEO constellations: how strong the inter-satellite link should be","abstract":"We consider a constellation of low-earth-orbit (LEO) satellites connected to a handheld device on the ground. Due to the very large orbital speed, an effective handover strategy becomes of paramount importance. In particular, we study the benefits of soft handover in the uplink from the physical-layer point of view. We give a realistic model for both the ground-to-satellite and the inter-satellite links, following the 3GPP channel model for the former. We suppose that, during handover from a serving satellite to a target satellite, one of the two satellites forwards the received signal from the ground user to the other, thus acting as a relay. We quantify through simulations the loss of hard handover, compared to soft handover. For the latter, we test both amplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and verify that, at least in the simulated conditions, DF does not repay, in terms of block error rate (BLER), the increase of complexity with respect to AF. Also, we study the effect of the LEO constellation size on the network BLER. Finally, we show that, with soft handover, the impact of misalignment on the inter-satellite link is severe, especially at optical frequencies.","sentences":["We consider a constellation of low-earth-orbit (LEO) satellites connected to a handheld device on the ground.","Due to the very large orbital speed, an effective handover strategy becomes of paramount importance.","In particular, we study the benefits of soft handover in the uplink from the physical-layer point of view.","We give a realistic model for both the ground-to-satellite and the inter-satellite links, following the 3GPP channel model for the former.","We suppose that, during handover from a serving satellite to a target satellite, one of the two satellites forwards the received signal from the ground user to the other, thus acting as a relay.","We quantify through simulations the loss of hard handover, compared to soft handover.","For the latter, we test both amplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and verify that, at least in the simulated conditions, DF does not repay, in terms of block error rate (BLER), the increase of complexity with respect to AF.","Also, we study the effect of the LEO constellation size on the network BLER.","Finally, we show that, with soft handover, the impact of misalignment on the inter-satellite link is severe, especially at optical frequencies."],"url":"http://arxiv.org/abs/2403.15131v1","category":"cs.IT"}
{"created":"2024-03-22 11:30:38","title":"An Agent-Centric Perspective on Norm Enforcement and Sanctions","abstract":"In increasingly autonomous and highly distributed multi-agent systems, centralized coordination becomes impractical and raises the need for governance and enforcement mechanisms from an agent-centric perspective. In our conceptual view, sanctioning norm enforcement is part of this agent-centric approach and they aim at promoting norm compliance while preserving agents' autonomy. The few works dealing with sanctioning norm enforcement and sanctions from the agent-centric perspective present limitations regarding the representation of sanctions and the comprehensiveness of their norm enforcement process. To address these drawbacks, we propose the NPL(s), an extension of the NPL normative programming language enriched with the representation of norms and sanctions as first-class abstractions. We also propose a BDI normative agent architecture embedding an engine for processing the NPL(s) language and a set of capabilities for approaching more comprehensively the sanctioning norm enforcement process. We apply our contributions in a case study for improving the robustness of agents' decision-making in a production automation system.","sentences":["In increasingly autonomous and highly distributed multi-agent systems, centralized coordination becomes impractical and raises the need for governance and enforcement mechanisms from an agent-centric perspective.","In our conceptual view, sanctioning norm enforcement is part of this agent-centric approach and they aim at promoting norm compliance while preserving agents' autonomy.","The few works dealing with sanctioning norm enforcement and sanctions from the agent-centric perspective present limitations regarding the representation of sanctions and the comprehensiveness of their norm enforcement process.","To address these drawbacks, we propose the NPL(s), an extension of the NPL normative programming language enriched with the representation of norms and sanctions as first-class abstractions.","We also propose a BDI normative agent architecture embedding an engine for processing the NPL(s) language and a set of capabilities for approaching more comprehensively the sanctioning norm enforcement process.","We apply our contributions in a case study for improving the robustness of agents' decision-making in a production automation system."],"url":"http://arxiv.org/abs/2403.15128v1","category":"cs.MA"}
{"created":"2024-03-22 11:24:31","title":"A hybrid approach to semi-automated Rust verification","abstract":"While recent years have been witness to a large body of work on efficient and automated verification of safe Rust code, enabled by the rich guarantees of the Rust type system, much less progress has been made on reasoning about unsafe code due to its unique complexities. We propose a hybrid approach to end-to-end Rust verification in which powerful automated verification of safe Rust is combined with targeted semi-automated verification of unsafe~Rust. To this end, we present Gillian-Rust, a proof-of-concept semi-automated verification tool that is able to reason about type safety and functional correctness of unsafe~code. Built on top of the Gillian parametric compositional verification platform, Gillian-Rust automates a rich separation logic for real-world Rust, embedding the lifetime logic of RustBelt and the parametric propheciees of RustHornBelt. Using the unique extensibility of Gillian, our novel encoding of these features is fine-tuned to maximise automation and exposes a user-friendly API, allowing for low-effort verification of unsafe code. We link Gillian-Rust with Creusot, a state-of-the-art verifier for safe Rust, by providing a systematic encoding of unsafe code specifications that Creusot may use but not verify, demonstrating the feasibility of our hybrid~approach.","sentences":["While recent years have been witness to a large body of work on efficient and automated verification of safe Rust code, enabled by the rich guarantees of the Rust type system, much less progress has been made on reasoning about unsafe code due to its unique complexities.","We propose a hybrid approach to end-to-end Rust verification in which powerful automated verification of safe Rust is combined with targeted semi-automated verification of unsafe~Rust.","To this end, we present Gillian-Rust, a proof-of-concept semi-automated verification tool that is able to reason about type safety and functional correctness of unsafe~code.","Built on top of the Gillian parametric compositional verification platform, Gillian-Rust automates a rich separation logic for real-world Rust, embedding the lifetime logic of RustBelt and the parametric propheciees of RustHornBelt.","Using the unique extensibility of Gillian, our novel encoding of these features is fine-tuned to maximise automation and exposes a user-friendly API, allowing for low-effort verification of unsafe code.","We link Gillian-Rust with Creusot, a state-of-the-art verifier for safe Rust, by providing a systematic encoding of unsafe code specifications that Creusot may use but not verify, demonstrating the feasibility of our hybrid~approach."],"url":"http://arxiv.org/abs/2403.15122v1","category":"cs.PL"}
{"created":"2024-03-22 11:08:56","title":"Set-membership target search and tracking within an unknown cluttered area using cooperating UAVs equipped with vision systems","abstract":"This paper addresses the problem of target search and tracking using a fleet of cooperating UAVs evolving in some unknown region of interest containing an a priori unknown number of moving ground targets. Each drone is equipped with an embedded Computer Vision System (CVS), providing an image with labeled pixels and a depth map of the observed part of its environment. Moreover, a box containing the corresponding pixels in the image frame is available when a UAV identifies a target. Hypotheses regarding information provided by the pixel classification, depth map construction, and target identification algorithms are proposed to allow its exploitation by set-membership approaches. A set-membership target location estimator is developed using the information provided by the CVS. Each UAV evaluates sets guaranteed to contain the location of the identified targets and a set possibly containing the locations of targets still to be identified. Then, each UAV uses these sets to search and track targets cooperatively.","sentences":["This paper addresses the problem of target search and tracking using a fleet of cooperating UAVs evolving in some unknown region of interest containing an a priori unknown number of moving ground targets.","Each drone is equipped with an embedded Computer Vision System (CVS), providing an image with labeled pixels and a depth map of the observed part of its environment.","Moreover, a box containing the corresponding pixels in the image frame is available when a UAV identifies a target.","Hypotheses regarding information provided by the pixel classification, depth map construction, and target identification algorithms are proposed to allow its exploitation by set-membership approaches.","A set-membership target location estimator is developed using the information provided by the CVS.","Each UAV evaluates sets guaranteed to contain the location of the identified targets and a set possibly containing the locations of targets still to be identified.","Then, each UAV uses these sets to search and track targets cooperatively."],"url":"http://arxiv.org/abs/2403.15113v1","category":"eess.SY"}
{"created":"2024-03-22 11:07:35","title":"Fast TTC Computation","abstract":"This paper proposes a fast Markov Matrix-based methodology for computing Top Trading Cycles (TTC) that delivers O(1) computational speed, that is speed independent of the number of agents and objects in the system. The proposed methodology is well suited for complex large-dimensional problems like housing choice. The methodology retains all the properties of TTC, namely, Pareto-efficiency, individual rationality and strategy-proofness.","sentences":["This paper proposes a fast Markov Matrix-based methodology for computing Top Trading Cycles (TTC) that delivers O(1) computational speed, that is speed independent of the number of agents and objects in the system.","The proposed methodology is well suited for complex large-dimensional problems like housing choice.","The methodology retains all the properties of TTC, namely, Pareto-efficiency, individual rationality and strategy-proofness."],"url":"http://arxiv.org/abs/2403.15111v1","category":"econ.EM"}
{"created":"2024-03-22 10:38:46","title":"Optimal Contract Design for End-of-Life Care Payments","abstract":"A large fraction of total healthcare expenditure occurs due to end-of-life (EOL) care, which means it is important to study the problem of more carefully incentivizing necessary versus unnecessary EOL care because this has the potential to reduce overall healthcare spending. This paper introduces a principal-agent model that integrates a mixed payment system of fee-for-service and pay-for-performance in order to analyze whether it is possible to better align healthcare provider incentives with patient outcomes and cost-efficiency in EOL care. The primary contributions are to derive optimal contracts for EOL care payments using a principal-agent framework under three separate models for the healthcare provider, where each model considers a different level of risk tolerance for the provider. We derive these optimal contracts by converting the underlying principal-agent models from a bilevel optimization problem into a single-level optimization problem that can be analytically solved. Our results are demonstrated using a simulation where an optimal contract is used to price intracranial pressure monitoring for traumatic brain injuries.","sentences":["A large fraction of total healthcare expenditure occurs due to end-of-life (EOL) care, which means it is important to study the problem of more carefully incentivizing necessary versus unnecessary EOL care because this has the potential to reduce overall healthcare spending.","This paper introduces a principal-agent model that integrates a mixed payment system of fee-for-service and pay-for-performance in order to analyze whether it is possible to better align healthcare provider incentives with patient outcomes and cost-efficiency in EOL care.","The primary contributions are to derive optimal contracts for EOL care payments using a principal-agent framework under three separate models for the healthcare provider, where each model considers a different level of risk tolerance for the provider.","We derive these optimal contracts by converting the underlying principal-agent models from a bilevel optimization problem into a single-level optimization problem that can be analytically solved.","Our results are demonstrated using a simulation where an optimal contract is used to price intracranial pressure monitoring for traumatic brain injuries."],"url":"http://arxiv.org/abs/2403.15099v1","category":"math.OC"}
{"created":"2024-03-22 10:21:28","title":"Frequency-dependent covariance reveals critical spatio-temporal patterns of synchronized activity in the human brain","abstract":"Recent analyses combining advanced theoretical techniques and high-quality data from thousands of simultaneously recorded neurons provide strong support for the hypothesis that neural dynamics operate near the edge of instability across regions in the brain. However, these analyses, as well as related studies, often fail to capture the intricate temporal structure of brain activity as they primarily rely on time-integrated measurements across neurons. In this study, we present a novel framework designed to explore signatures of criticality across diverse frequency bands and construct a much more comprehensive description of brain activity. Additionally, we introduce a method for projecting brain activity onto a basis of spatio-temporal patterns, facilitating time-dependent dimensionality reduction. Applying this framework to a magnetoencephalography dataset, we observe significant differences in both criticality signatures and spatio-temporal activity patterns between healthy subjects and individuals with Parkinson's disease.","sentences":["Recent analyses combining advanced theoretical techniques and high-quality data from thousands of simultaneously recorded neurons provide strong support for the hypothesis that neural dynamics operate near the edge of instability across regions in the brain.","However, these analyses, as well as related studies, often fail to capture the intricate temporal structure of brain activity as they primarily rely on time-integrated measurements across neurons.","In this study, we present a novel framework designed to explore signatures of criticality across diverse frequency bands and construct a much more comprehensive description of brain activity.","Additionally, we introduce a method for projecting brain activity onto a basis of spatio-temporal patterns, facilitating time-dependent dimensionality reduction.","Applying this framework to a magnetoencephalography dataset, we observe significant differences in both criticality signatures and spatio-temporal activity patterns between healthy subjects and individuals with Parkinson's disease."],"url":"http://arxiv.org/abs/2403.15092v1","category":"q-bio.NC"}
{"created":"2024-03-22 10:05:37","title":"Evaluating the Influence of Multi-Factor Authentication and Recovery Settings on the Security and Accessibility of User Accounts","abstract":"Nowadays, most online services offer different authentication methods that users can set up for multi-factor authentication but also as a recovery method. This configuration must be done thoroughly to prevent an adversary's access while ensuring the legitimate user does not lose access to their account. This is particularly important for fundamental everyday services, where either failure would have severe consequences. Nevertheless, little research has been done on the authentication of actual users regarding security and the risk of being locked out of their accounts.   To foster research in this direction, this paper presents a study on the account settings of Google and Apple users. Considering the multi-factor authentication configuration and recovery options, we analyzed the account security and lock-out risks. Our results provide insights into the usage of multi-factor authentication in practice, show significant security differences between Google and Apple accounts, and reveal that many users would miss access to their accounts when losing a single authentication device.","sentences":["Nowadays, most online services offer different authentication methods that users can set up for multi-factor authentication but also as a recovery method.","This configuration must be done thoroughly to prevent an adversary's access while ensuring the legitimate user does not lose access to their account.","This is particularly important for fundamental everyday services, where either failure would have severe consequences.","Nevertheless, little research has been done on the authentication of actual users regarding security and the risk of being locked out of their accounts.   ","To foster research in this direction, this paper presents a study on the account settings of Google and Apple users.","Considering the multi-factor authentication configuration and recovery options, we analyzed the account security and lock-out risks.","Our results provide insights into the usage of multi-factor authentication in practice, show significant security differences between Google and Apple accounts, and reveal that many users would miss access to their accounts when losing a single authentication device."],"url":"http://arxiv.org/abs/2403.15080v1","category":"cs.CR"}
{"created":"2024-03-22 09:54:04","title":"On the Inclusion of Charge and Spin States in Cartesian Tensor Neural Network Potentials","abstract":"In this letter, we present an extension to TensorNet, a state-of-the-art equivariant Cartesian tensor neural network potential, allowing it to handle charged molecules and spin states without architectural changes or increased costs. By incorporating these attributes, we address input degeneracy issues, enhancing the model's predictive accuracy across diverse chemical systems. This advancement significantly broadens TensorNet's applicability, maintaining its efficiency and accuracy.","sentences":["In this letter, we present an extension to TensorNet, a state-of-the-art equivariant Cartesian tensor neural network potential, allowing it to handle charged molecules and spin states without architectural changes or increased costs.","By incorporating these attributes, we address input degeneracy issues, enhancing the model's predictive accuracy across diverse chemical systems.","This advancement significantly broadens TensorNet's applicability, maintaining its efficiency and accuracy."],"url":"http://arxiv.org/abs/2403.15073v1","category":"cs.LG"}
{"created":"2024-03-22 09:52:18","title":"Direct and Indirect Hydrogen Storage: Dynamics and Interactions in the Transition to a Renewable Energy Based System for Europe","abstract":"To move towards a low-carbon society by 2050, understanding the intricate dynamics of energy systems is critical. Our study examines these interactions through the lens of hydrogen storage, dividing it into 'direct' and 'indirect' hydrogen storage. Direct hydrogen storage involves electrolysis-produced hydrogen being stored before use, while indirect storage first transforms hydrogen into gas via the Sabatier process for later energy distribution. Firstly, we utilize the PyPSA-Eur-Sec-30-path model to capture the interactions within the energy system. The model is an hour-level, one node per country system that encompasses a range of energy transformation technologies, outlining a pathway for Europe to reduce carbon emissions by 95 percent by 2050 compared to 1990, with updates every 5 years. Subsequently, we employ both quantitative and qualitative approaches to thoroughly analyze these complex relationships. Our research indicates that during the European green transition, cross-country flow of electricity will play an important role in Europe's rapid decarbonization stage before the large-scale introduction of energy storage. Under the paper cost assumptions, fuel cells are not considered a viable option. This research further identifies the significant impact of natural resource variability on the local energy mix, highlighting indirect hydrogen storage as a common solution due to the better economic performance and actively fluctuation pattern. Specifically, indirect hydrogen storage will contribute at least 60 percent of hydrogen storage benefits, reaching 100 percent in Italy. Moreover, its fluctuation pattern will change with the local energy structure, which is a distinct difference with the unchanged pattern of direct hydrogen storage and battery storage.","sentences":["To move towards a low-carbon society by 2050, understanding the intricate dynamics of energy systems is critical.","Our study examines these interactions through the lens of hydrogen storage, dividing it into 'direct' and 'indirect' hydrogen storage.","Direct hydrogen storage involves electrolysis-produced hydrogen being stored before use, while indirect storage first transforms hydrogen into gas via the Sabatier process for later energy distribution.","Firstly, we utilize the PyPSA-Eur-Sec-30-path model to capture the interactions within the energy system.","The model is an hour-level, one node per country system that encompasses a range of energy transformation technologies, outlining a pathway for Europe to reduce carbon emissions by 95 percent by 2050 compared to 1990, with updates every 5 years.","Subsequently, we employ both quantitative and qualitative approaches to thoroughly analyze these complex relationships.","Our research indicates that during the European green transition, cross-country flow of electricity will play an important role in Europe's rapid decarbonization stage before the large-scale introduction of energy storage.","Under the paper cost assumptions, fuel cells are not considered a viable option.","This research further identifies the significant impact of natural resource variability on the local energy mix, highlighting indirect hydrogen storage as a common solution due to the better economic performance and actively fluctuation pattern.","Specifically, indirect hydrogen storage will contribute at least 60 percent of hydrogen storage benefits, reaching 100 percent in Italy.","Moreover, its fluctuation pattern will change with the local energy structure, which is a distinct difference with the unchanged pattern of direct hydrogen storage and battery storage."],"url":"http://arxiv.org/abs/2403.15072v1","category":"eess.SY"}
{"created":"2024-03-22 09:48:53","title":"Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems","abstract":"The advent of Transformers has revolutionized computer vision, offering a powerful alternative to convolutional neural networks (CNNs), especially with the local attention mechanism that excels at capturing local structures within the input and achieve state-of-the-art performance. Processing in-memory (PIM) architecture offers extensive parallelism, low data movement costs, and scalable memory bandwidth, making it a promising solution to accelerate Transformer with memory-intensive operations. However, the crucial challenge lies in efficiently deploying the entire model onto a resource-limited PIM system while parallelizing each transformer block with potentially many computational branches based on local attention mechanisms. We present Allspark, which focuses on workload orchestration for visual Transformers on PIM systems, aiming at minimizing inference latency. Firstly, to fully utilize the massive parallelism of PIM, Allspark empolys a finer-grained partitioning scheme for computational branches, and format a systematic layout and interleaved dataflow with maximized data locality and reduced data movement. Secondly, Allspark formulates the scheduling of the complete model on a resource-limited distributed PIM system as an integer linear programming (ILP) problem. Thirdly, as local-global data interactions exhibit complex yet regular dependencies, Allspark provides a greedy-based mapping method to allocate computational branches onto the PIM system and minimize NoC communication costs. Extensive experiments on 3D-stacked DRAM-based PIM systems show that Allspark brings 1.2x-24.0x inference speedup for various visual Transformers over baselines, and that Allspark-enriched PIM system yields average speedups of 2.3x and energy savings of 20x-55x over Nvidia V100 GPU.","sentences":["The advent of Transformers has revolutionized computer vision, offering a powerful alternative to convolutional neural networks (CNNs), especially with the local attention mechanism that excels at capturing local structures within the input and achieve state-of-the-art performance.","Processing in-memory (PIM) architecture offers extensive parallelism, low data movement costs, and scalable memory bandwidth, making it a promising solution to accelerate Transformer with memory-intensive operations.","However, the crucial challenge lies in efficiently deploying the entire model onto a resource-limited PIM system while parallelizing each transformer block with potentially many computational branches based on local attention mechanisms.","We present Allspark, which focuses on workload orchestration for visual Transformers on PIM systems, aiming at minimizing inference latency.","Firstly, to fully utilize the massive parallelism of PIM, Allspark empolys a finer-grained partitioning scheme for computational branches, and format a systematic layout and interleaved dataflow with maximized data locality and reduced data movement.","Secondly, Allspark formulates the scheduling of the complete model on a resource-limited distributed PIM system as an integer linear programming (ILP) problem.","Thirdly, as local-global data interactions exhibit complex yet regular dependencies, Allspark provides a greedy-based mapping method to allocate computational branches onto the PIM system and minimize NoC communication costs.","Extensive experiments on 3D-stacked DRAM-based PIM systems show that Allspark brings 1.2x-24.0x inference speedup for various visual Transformers over baselines, and that Allspark-enriched PIM system yields average speedups of 2.3x and energy savings of 20x-55x over Nvidia V100 GPU."],"url":"http://arxiv.org/abs/2403.15069v1","category":"cs.AR"}
{"created":"2024-03-22 09:48:40","title":"A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness","abstract":"Autonomous ground vehicle (UGV) navigation has the potential to revolutionize the transportation system by increasing accessibility to disabled people, ensure safety and convenience of use. However, UGV requires extensive and efficient testing and evaluation to ensure its acceptance for public use. This testing are mostly done in a simulator which result to sim2real transfer gap. In this paper, we propose a digital twin perception awareness approach for the control of robot navigation without prior creation of the virtual environment (VT) environment state. To achieve this, we develop a twin delayed deep deterministic policy gradient (TD3) algorithm that ensures collision avoidance and goal-based path planning. We demonstrate the performance of our approach on different environment dynamics. We show that our approach is capable of efficiently avoiding collision with obstacles and navigating to its desired destination, while at the same time safely avoids obstacles using the information received from the LIDAR sensor mounted on the robot. Our approach bridges the gap between sim-to-real transfer and contributes to the adoption of UGVs in real world. We validate our approach in simulation and a real-world application in an office space.","sentences":["Autonomous ground vehicle (UGV) navigation has the potential to revolutionize the transportation system by increasing accessibility to disabled people, ensure safety and convenience of use.","However, UGV requires extensive and efficient testing and evaluation to ensure its acceptance for public use.","This testing are mostly done in a simulator which result to sim2real transfer gap.","In this paper, we propose a digital twin perception awareness approach for the control of robot navigation without prior creation of the virtual environment (VT) environment state.","To achieve this, we develop a twin delayed deep deterministic policy gradient (TD3) algorithm that ensures collision avoidance and goal-based path planning.","We demonstrate the performance of our approach on different environment dynamics.","We show that our approach is capable of efficiently avoiding collision with obstacles and navigating to its desired destination, while at the same time safely avoids obstacles using the information received from the LIDAR sensor mounted on the robot.","Our approach bridges the gap between sim-to-real transfer and contributes to the adoption of UGVs in real world.","We validate our approach in simulation and a real-world application in an office space."],"url":"http://arxiv.org/abs/2403.15067v1","category":"cs.RO"}
{"created":"2024-03-22 09:28:14","title":"Perturbations in PDE-constrained optimal control decay exponentially in space","abstract":"For linear-quadratic optimal control problems (OCPs) governed by elliptic and parabolic partial differential equations (PDEs), we investigate the impact of perturbations on optimal solutions. Local perturbations may occur, e.g., due to discretization of the optimality system or disturbed problem data. Whereas these perturbations may exhibit global effects in the uncontrolled case, we prove that the ramifications are exponentially damped in space under stabilizability- and detectability-like conditions. To this end, we prove a bound on the optimality condition's solution operator that is uniform in the domain size. Then, this uniformity is used in a scaling argument to show the exponential decay of perturbations in space. We numerically validate and illustrate our results by solving OCPs involving Helmholtz, Poisson, and advection-diffusion-reaction equations.","sentences":["For linear-quadratic optimal control problems (OCPs) governed by elliptic and parabolic partial differential equations (PDEs), we investigate the impact of perturbations on optimal solutions.","Local perturbations may occur, e.g., due to discretization of the optimality system or disturbed problem data.","Whereas these perturbations may exhibit global effects in the uncontrolled case, we prove that the ramifications are exponentially damped in space under stabilizability- and detectability-like conditions.","To this end, we prove a bound on the optimality condition's solution operator that is uniform in the domain size.","Then, this uniformity is used in a scaling argument to show the exponential decay of perturbations in space.","We numerically validate and illustrate our results by solving OCPs involving Helmholtz, Poisson, and advection-diffusion-reaction equations."],"url":"http://arxiv.org/abs/2403.15056v1","category":"math.OC"}
{"created":"2024-03-22 09:20:58","title":"Mesoscopic Lattice Boltzmann modeling of dense gas flows in curvilinear geometries","abstract":"We derive the Enskog equation utilizing orthonormal vielbein fields, enabling the utilization of arbitrary coordinate systems to characterize spatial geometry. Additionally, we employ an adapted coordinate system in the momentum space, connected to the physical space through vielbeins. Within this framework, the momentum component perpendicular to a curved boundary can be treated as an independent one, facilitating the application of half-range Gauss-Hermite quadratures. We develop an appropriate finite-difference Lattice Boltzmann model and validate it against a DSMC-like particle-based method for solving the Enskog equation in curvilinear geometries. Our test scenarios include cylindrical Couette flow, cylindrical Fourier flow between coaxial cylinders, and spherical Fourier flow between concentric spheres. Excellent agreement between the two approaches is observed throughout the parameter range and curvature-specific effects are well captured.","sentences":["We derive the Enskog equation utilizing orthonormal vielbein fields, enabling the utilization of arbitrary coordinate systems to characterize spatial geometry.","Additionally, we employ an adapted coordinate system in the momentum space, connected to the physical space through vielbeins.","Within this framework, the momentum component perpendicular to a curved boundary can be treated as an independent one, facilitating the application of half-range Gauss-Hermite quadratures.","We develop an appropriate finite-difference Lattice Boltzmann model and validate it against a DSMC-like particle-based method for solving the Enskog equation in curvilinear geometries.","Our test scenarios include cylindrical Couette flow, cylindrical Fourier flow between coaxial cylinders, and spherical Fourier flow between concentric spheres.","Excellent agreement between the two approaches is observed throughout the parameter range and curvature-specific effects are well captured."],"url":"http://arxiv.org/abs/2403.15051v1","category":"physics.flu-dyn"}
{"created":"2024-03-22 09:20:44","title":"Quantum Valley Hall effect without Berry curvature","abstract":"The quantum valley Hall effect (QVHE) is characterized by the valley Chern number (VCN) in a way that one-dimensional (1D) chiral metallic states are guaranteed to appear at the domain walls (DW) between two domains with opposite VCN for a given valley. Although in the case of QVHE, the total BC of the system is zero, the BC distributed locally around each valley makes the VCN well-defined as long as inter-valley scattering is negligible. Here, we propose a new type of valley-dependent topological phenomenon that occurs when the BC is strictly zero at each momentum. Such zero Berry curvature (ZBC) QVHE is characterized by the valley Euler number (VEN) which is computed by integrating the Euler curvature around a given valley in two-dimensional (2D) systems with space-time inversion symmetry. 1D helical metallic states can be topologically protected at the DW between two domains with the opposite VENs when the DW configuration preserves either the mirror symmetry with respect to the DW or the combination of the DW space-time inversion , and chiral symmetries. We establish the fundamental origin of ZBC-QVHE. Also, by combining tight-binding model study and first-principles calculations, we propose stacked hexagonal bilayer lattices including h-BX (X=As, P) and large-angle twisted bilayer graphenes as candidate systems with robust helical DW states protected by VEN.","sentences":["The quantum valley Hall effect (QVHE) is characterized by the valley Chern number (VCN) in a way that one-dimensional (1D) chiral metallic states are guaranteed to appear at the domain walls (DW) between two domains with opposite VCN for a given valley.","Although in the case of QVHE, the total BC of the system is zero, the BC distributed locally around each valley makes the VCN well-defined as long as inter-valley scattering is negligible.","Here, we propose a new type of valley-dependent topological phenomenon that occurs when the BC is strictly zero at each momentum.","Such zero Berry curvature (ZBC) QVHE is characterized by the valley Euler number (VEN) which is computed by integrating the Euler curvature around a given valley in two-dimensional (2D) systems with space-time inversion symmetry.","1D helical metallic states can be topologically protected at the DW between two domains with the opposite VENs when the DW configuration preserves either the mirror symmetry with respect to the DW or the combination of the DW space-time inversion , and chiral symmetries.","We establish the fundamental origin of ZBC-QVHE.","Also, by combining tight-binding model study and first-principles calculations, we propose stacked hexagonal bilayer lattices including h-BX (X=As, P) and large-angle twisted bilayer graphenes as candidate systems with robust helical DW states protected by VEN."],"url":"http://arxiv.org/abs/2403.15050v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 08:21:35","title":"On the Solution Uniqueness of Data-Driven Modeling of Flexible Loads","abstract":"This letter first explores the solution uniqueness of the data-driven modeling of price-responsive flexible loads (PFL). The PFL on the demand side is critical in modern power systems. An accurate PFL model is fundamental for system operations. Yet, whether the PFL model can be uniquely and correctly identified from operational data remains unclear. To address this, we analyze the structural and practical identifiability of the PFL model, deriving the condition for the solution uniqueness. Based on this, we point out the implications for selecting physical models of PFL to enhance the identification results. Numerical results validate this work.","sentences":["This letter first explores the solution uniqueness of the data-driven modeling of price-responsive flexible loads (PFL).","The PFL on the demand side is critical in modern power systems.","An accurate PFL model is fundamental for system operations.","Yet, whether the PFL model can be uniquely and correctly identified from operational data remains unclear.","To address this, we analyze the structural and practical identifiability of the PFL model, deriving the condition for the solution uniqueness.","Based on this, we point out the implications for selecting physical models of PFL to enhance the identification results.","Numerical results validate this work."],"url":"http://arxiv.org/abs/2403.15029v1","category":"eess.SY"}
{"created":"2024-03-22 17:59:35","title":"Unifying area and unit-level small area estimation through calibration","abstract":"When estimating area means, direct estimators based on area-specific data, are usually consistent under the sampling design without model assumptions. However, they are inefficient if the area sample size is small. In small area estimation, model assumptions linking the areas are used to \"borrow strength\" from other areas. The basic area-level model provides design-consistent estimators but error variances are assumed to be known. In practice, they are estimated with the (scarce) area-specific data. These estimators are inefficient, and their error is not accounted for in the associated mean squared error estimators. Unit-level models do not require to know the error variances but do not account for the survey design. Here we describe a unified estimator of an area mean that may be obtained both from an area-level model or a unit-level model and based on consistent estimators of the model error variances as the number of areas increases. We propose bootstrap mean squared error estimators that account for the uncertainty due to the estimation of the error variances. We show a better performance of the new small area estimators and our bootstrap estimators of the mean squared error. We apply the results to education data from Colombia.","sentences":["When estimating area means, direct estimators based on area-specific data, are usually consistent under the sampling design without model assumptions.","However, they are inefficient if the area sample size is small.","In small area estimation, model assumptions linking the areas are used to \"borrow strength\" from other areas.","The basic area-level model provides design-consistent estimators but error variances are assumed to be known.","In practice, they are estimated with the (scarce) area-specific data.","These estimators are inefficient, and their error is not accounted for in the associated mean squared error estimators.","Unit-level models do not require to know the error variances but do not account for the survey design.","Here we describe a unified estimator of an area mean that may be obtained both from an area-level model or a unit-level model and based on consistent estimators of the model error variances as the number of areas increases.","We propose bootstrap mean squared error estimators that account for the uncertainty due to the estimation of the error variances.","We show a better performance of the new small area estimators and our bootstrap estimators of the mean squared error.","We apply the results to education data from Colombia."],"url":"http://arxiv.org/abs/2403.15384v1","category":"stat.ME"}
{"created":"2024-03-22 17:48:13","title":"OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments","abstract":"We develop a hierarchical LLM-task-motion planning and replanning framework to efficiently ground an abstracted human command into tangible Autonomous Underwater Vehicle (AUV) control through enhanced representations of the world. We also incorporate a holistic replanner to provide real-world feedback with all planners for robust AUV operation. While there has been extensive research in bridging the gap between LLMs and robotic missions, they are unable to guarantee success of AUV applications in the vast and unknown ocean environment. To tackle specific challenges in marine robotics, we design a hierarchical planner to compose executable motion plans, which achieves planning efficiency and solution quality by decomposing long-horizon missions into sub-tasks. At the same time, real-time data stream is obtained by a replanner to address environmental uncertainties during plan execution. Experiments validate that our proposed framework delivers successful AUV performance of long-duration missions through natural language piloting.","sentences":["We develop a hierarchical LLM-task-motion planning and replanning framework to efficiently ground an abstracted human command into tangible Autonomous Underwater Vehicle (AUV) control through enhanced representations of the world.","We also incorporate a holistic replanner to provide real-world feedback with all planners for robust AUV operation.","While there has been extensive research in bridging the gap between LLMs and robotic missions, they are unable to guarantee success of AUV applications in the vast and unknown ocean environment.","To tackle specific challenges in marine robotics, we design a hierarchical planner to compose executable motion plans, which achieves planning efficiency and solution quality by decomposing long-horizon missions into sub-tasks.","At the same time, real-time data stream is obtained by a replanner to address environmental uncertainties during plan execution.","Experiments validate that our proposed framework delivers successful AUV performance of long-duration missions through natural language piloting."],"url":"http://arxiv.org/abs/2403.15369v1","category":"cs.RO"}
{"created":"2024-03-22 16:59:01","title":"Exciton-activated effective phonon magnetic moment in monolayer MoS2","abstract":"Optical excitation of chiral phonons plays a vital role in studying the phonon-driven magnetic phenomena in solids. Transition metal dichalcogenides host chiral phonons at high symmetry points of the Brillouin zone, providing an ideal platform to explore the interplay between chiral phonons and valley degree of freedom. Here, we investigate the helicity-resolved magneto-Raman response of monolayer MoS2 and identify a doubly degenerate Brillouin-zone-center chiral phonon mode at ~270 cm-1. Our wavelength- and temperature-dependent measurements show that this chiral phonon is activated through the resonant excitation of A exciton. Under an out-of-plane magnetic field, the chiral phonon exhibits giant Zeeman splitting, which corresponds to an effective magnetic moment of 2.5mu_B. Moreover, we carry out theoretical calculations based on the Morphic effects in nonmagnetic crystals, which reproduce the linear Zeeman splitting and Raman cross-section of the chiral phonon. Our study provides important insights into lifting the chiral phonon degeneracy in an achiral covalent material, paving a new route to excite and control chiral phonons.","sentences":["Optical excitation of chiral phonons plays a vital role in studying the phonon-driven magnetic phenomena in solids.","Transition metal dichalcogenides host chiral phonons at high symmetry points of the Brillouin zone, providing an ideal platform to explore the interplay between chiral phonons and valley degree of freedom.","Here, we investigate the helicity-resolved magneto-Raman response of monolayer MoS2 and identify a doubly degenerate Brillouin-zone-center chiral phonon mode at ~270 cm-1.","Our wavelength- and temperature-dependent measurements show that this chiral phonon is activated through the resonant excitation of A exciton.","Under an out-of-plane magnetic field, the chiral phonon exhibits giant Zeeman splitting, which corresponds to an effective magnetic moment of 2.5mu_B.","Moreover, we carry out theoretical calculations based on the Morphic effects in nonmagnetic crystals, which reproduce the linear Zeeman splitting and Raman cross-section of the chiral phonon.","Our study provides important insights into lifting the chiral phonon degeneracy in an achiral covalent material, paving a new route to excite and control chiral phonons."],"url":"http://arxiv.org/abs/2403.15347v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-22 16:17:51","title":"Visual Highlighting for Situated Brushing and Linking","abstract":"Brushing and linking is widely used for visual analytics in desktop environments. However, using this approach to link many data items between situated (e.g., a virtual screen with data) and embedded views (e.g., highlighted objects in the physical environment) is largely unexplored. To this end, we study the effectiveness of visual highlighting techniques in helping users identify and link physical referents to brushed data marks in a situated scatterplot. In an exploratory virtual reality user study (N=20), we evaluated four highlighting techniques under different physical layouts and tasks. We discuss the effectiveness of these techniques, as well as implications for the design of brushing and linking operations in situated analytics.","sentences":["Brushing and linking is widely used for visual analytics in desktop environments.","However, using this approach to link many data items between situated (e.g., a virtual screen with data) and embedded views (e.g., highlighted objects in the physical environment) is largely unexplored.","To this end, we study the effectiveness of visual highlighting techniques in helping users identify and link physical referents to brushed data marks in a situated scatterplot.","In an exploratory virtual reality user study (N=20), we evaluated four highlighting techniques under different physical layouts and tasks.","We discuss the effectiveness of these techniques, as well as implications for the design of brushing and linking operations in situated analytics."],"url":"http://arxiv.org/abs/2403.15321v1","category":"cs.HC"}
{"created":"2024-03-22 16:12:49","title":"Discounted Subjective Expected Utility in Continuous Time","abstract":"By embedding uncertainty into time, we obtain a conjoint axiomatic characterization of both Exponential Discounting and Subjective Expected Utility that accommodates arbitrary state and outcome spaces. In doing so, we provide a novel and simple time-interpretation of subjective probability. The subjective probability of an event is calibrated using time discounting.","sentences":["By embedding uncertainty into time, we obtain a conjoint axiomatic characterization of both Exponential Discounting and Subjective Expected Utility that accommodates arbitrary state and outcome spaces.","In doing so, we provide a novel and simple time-interpretation of subjective probability.","The subjective probability of an event is calibrated using time discounting."],"url":"http://arxiv.org/abs/2403.15319v1","category":"econ.TH"}
{"created":"2024-03-22 15:59:39","title":"Theory of quasiparticle-induced errors in driven-dissipative Schr\u00f6dinger cat qubits","abstract":"Understanding the mechanisms of qubit decoherence is a crucial prerequisite for improving the qubit performance. In this work we discuss the effects of residual Bogolyubov quasiparticles in Schr\\\"odinger cat qubits, either of the dissipative or Kerr type. The major difference from previous studies of quasiparticles in superconducting qubits is that the Schr\\\"odinger cat qubits are operated under non-equilibrium conditions. Indeed, an external microwave drive is needed to stabilize \"cat states\", which are superpositions of coherent degenerate eigenstates of an effective stationary Lindbladian in the rotating frame. We present a microscopic derivation of the master equation for cat qubits and express the effect of the quasiparticles as dissipators acting on the density matrix of the cat qubit. This enables us to determine the conditions under which the quasiparticles give a substantial contribution to the qubit errors.","sentences":["Understanding the mechanisms of qubit decoherence is a crucial prerequisite for improving the qubit performance.","In this work we discuss the effects of residual Bogolyubov quasiparticles in Schr\\\"odinger cat qubits, either of the dissipative or Kerr type.","The major difference from previous studies of quasiparticles in superconducting qubits is that the Schr\\\"odinger cat qubits are operated under non-equilibrium conditions.","Indeed, an external microwave drive is needed to stabilize \"cat states\", which are superpositions of coherent degenerate eigenstates of an effective stationary Lindbladian in the rotating frame.","We present a microscopic derivation of the master equation for cat qubits and express the effect of the quasiparticles as dissipators acting on the density matrix of the cat qubit.","This enables us to determine the conditions under which the quasiparticles give a substantial contribution to the qubit errors."],"url":"http://arxiv.org/abs/2403.15310v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 15:52:00","title":"Optimal Survival Analyses With Prevalent and Incident Patients","abstract":"Period-prevalent cohorts are often used for their cost-saving potential in epidemiological studies of survival outcomes. Under this design, prevalent patients allow for evaluations of long-term survival outcomes without the need for long follow-up, whereas incident patients allow for evaluations of short-term survival outcomes without the issue of left-truncation. In most period-prevalent survival analyses from the existing literature, patients have been recruited to achieve an overall sample size, with little attention given to the relative frequencies of prevalent and incident patients and their statistical implications. Furthermore, there are no existing methods available to rigorously quantify the impact of these relative frequencies on estimation and inference and incorporate this information into study design strategies. To address these gaps, we develop an approach to identify the optimal mix of prevalent and incident patients that maximizes precision over the entire estimated survival curve, subject to a flexible weighting scheme. In addition, we prove that inference based on the weighted log-rank test or Cox proportional hazards model is most powerful with an entirely prevalent or incident cohort, and we derive theoretical formulas to determine the optimal choice. Simulations confirm the validity of the proposed optimization criteria and show that substantial efficiency gains can be achieved by recruiting the optimal mix of prevalent and incident patients. The proposed methods are applied to assess waitlist outcomes among kidney transplant candidates.","sentences":["Period-prevalent cohorts are often used for their cost-saving potential in epidemiological studies of survival outcomes.","Under this design, prevalent patients allow for evaluations of long-term survival outcomes without the need for long follow-up, whereas incident patients allow for evaluations of short-term survival outcomes without the issue of left-truncation.","In most period-prevalent survival analyses from the existing literature, patients have been recruited to achieve an overall sample size, with little attention given to the relative frequencies of prevalent and incident patients and their statistical implications.","Furthermore, there are no existing methods available to rigorously quantify the impact of these relative frequencies on estimation and inference and incorporate this information into study design strategies.","To address these gaps, we develop an approach to identify the optimal mix of prevalent and incident patients that maximizes precision over the entire estimated survival curve, subject to a flexible weighting scheme.","In addition, we prove that inference based on the weighted log-rank test or Cox proportional hazards model is most powerful with an entirely prevalent or incident cohort, and we derive theoretical formulas to determine the optimal choice.","Simulations confirm the validity of the proposed optimization criteria and show that substantial efficiency gains can be achieved by recruiting the optimal mix of prevalent and incident patients.","The proposed methods are applied to assess waitlist outcomes among kidney transplant candidates."],"url":"http://arxiv.org/abs/2403.15302v1","category":"stat.ME"}
{"created":"2024-03-22 15:31:07","title":"A data-informed mathematical model of microglial cell dynamics during ischemic stroke in the middle cerebral artery","abstract":"Neuroinflammation immediately follows the onset of ischemic stroke in the middle cerebral artery. During this process, microglial cells are activated in and recruited to the penumbra. Microglial cells can be activated into two different phenotypes: M1, which can worsen brain injury; or M2, which can aid in long-term recovery. In this study, we contribute a summary of experimental data on microglial cell counts in the penumbra following ischemic stroke induced by middle cerebral artery occlusion (MCAO) in mice and compile available data sets into a single set suitable for time series analysis. Further, we formulate a mathematical model of microglial cells in the penumbra during ischemic stroke due to MCAO. Through use of global sensitivity analysis and Markov Chain Monte Carlo (MCMC)-based parameter estimation, we analyze the effects of the model parameters on the number of M1 and M2 cells in the penumbra and fit identifiable parameters to the compiled experimental data set. We utilize results from MCMC parameter estimation to ascertain uncertainty bounds and forward predictions for the number of M1 and M2 microglial cells over time. Results demonstrate the significance of parameters related to M1 and M2 activation on the number of M1 and M2 microglial cells. Simulations further suggest that potential outliers in the observed data may be omitted and forecast predictions suggest a lingering inflammatory response.","sentences":["Neuroinflammation immediately follows the onset of ischemic stroke in the middle cerebral artery.","During this process, microglial cells are activated in and recruited to the penumbra.","Microglial cells can be activated into two different phenotypes: M1, which can worsen brain injury; or M2, which can aid in long-term recovery.","In this study, we contribute a summary of experimental data on microglial cell counts in the penumbra following ischemic stroke induced by middle cerebral artery occlusion (MCAO) in mice and compile available data sets into a single set suitable for time series analysis.","Further, we formulate a mathematical model of microglial cells in the penumbra during ischemic stroke due to MCAO.","Through use of global sensitivity analysis and Markov Chain Monte Carlo (MCMC)-based parameter estimation, we analyze the effects of the model parameters on the number of M1 and M2 cells in the penumbra and fit identifiable parameters to the compiled experimental data set.","We utilize results from MCMC parameter estimation to ascertain uncertainty bounds and forward predictions for the number of M1 and M2 microglial cells over time.","Results demonstrate the significance of parameters related to M1 and M2 activation on the number of M1 and M2 microglial cells.","Simulations further suggest that potential outliers in the observed data may be omitted and forecast predictions suggest a lingering inflammatory response."],"url":"http://arxiv.org/abs/2403.15284v1","category":"q-bio.CB"}
{"created":"2024-03-22 15:03:51","title":"Practical considerations for high-fidelity wavefront shaping experiments","abstract":"Wavefront shaping is a technique for directing light through turbid media. The theoretical aspects of wavefront shaping are well understood, and under near-ideal experimental conditions, accurate predictions for the expected signal enhancement can be given. In practice, however, there are many experimental factors that negatively affect the outcome of the experiment. Here, we present a comprehensive overview of these experimental factors, including the effect of sample scattering properties, noise, and response of the spatial light modulator. We present simple means to identify experimental imperfections and to minimize their negative effect on the outcome of the experiment. This paper is accompanied by Python code for automatically quantifying experimental problems using the OpenWFS framework for running and simulating wavefront shaping experiments.","sentences":["Wavefront shaping is a technique for directing light through turbid media.","The theoretical aspects of wavefront shaping are well understood, and under near-ideal experimental conditions, accurate predictions for the expected signal enhancement can be given.","In practice, however, there are many experimental factors that negatively affect the outcome of the experiment.","Here, we present a comprehensive overview of these experimental factors, including the effect of sample scattering properties, noise, and response of the spatial light modulator.","We present simple means to identify experimental imperfections and to minimize their negative effect on the outcome of the experiment.","This paper is accompanied by Python code for automatically quantifying experimental problems using the OpenWFS framework for running and simulating wavefront shaping experiments."],"url":"http://arxiv.org/abs/2403.15265v1","category":"physics.optics"}
{"created":"2024-03-22 15:00:29","title":"Hyperbolic Metric Learning for Visual Outlier Detection","abstract":"Out-Of-Distribution (OOD) detection is critical to deploy deep learning models in safety-critical applications. However, the inherent hierarchical concept structure of visual data, which is instrumental to OOD detection, is often poorly captured by conventional methods based on Euclidean geometry. This work proposes a metric framework that leverages the strengths of Hyperbolic geometry for OOD detection. Inspired by previous works that refine the decision boundary for OOD data with synthetic outliers, we extend this method to Hyperbolic space. Interestingly, we find that synthetic outliers do not benefit OOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we explore the relationship between OOD detection performance and Hyperbolic embedding dimension, addressing practical concerns in resource-constrained environments. Extensive experiments show that our framework improves the FPR95 for OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and CIFAR-100 respectively compared to Euclidean methods.","sentences":["Out-Of-Distribution (OOD) detection is critical to deploy deep learning models in safety-critical applications.","However, the inherent hierarchical concept structure of visual data, which is instrumental to OOD detection, is often poorly captured by conventional methods based on Euclidean geometry.","This work proposes a metric framework that leverages the strengths of Hyperbolic geometry for OOD detection.","Inspired by previous works that refine the decision boundary for OOD data with synthetic outliers, we extend this method to Hyperbolic space.","Interestingly, we find that synthetic outliers do not benefit OOD detection in Hyperbolic space as they do in Euclidean space.","Furthermore we explore the relationship between OOD detection performance and Hyperbolic embedding dimension, addressing practical concerns in resource-constrained environments.","Extensive experiments show that our framework improves the FPR95 for OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and CIFAR-100 respectively compared to Euclidean methods."],"url":"http://arxiv.org/abs/2403.15260v1","category":"cs.CV"}
{"created":"2024-03-22 14:58:48","title":"Tests for almost stochastic dominance","abstract":"We introduce a 2-dimensional stochastic dominance (2DSD) index to characterize both strict and almost stochastic dominance. Based on this index, we derive an estimator for the minimum violation ratio (MVR), also known as the critical parameter, of the almost stochastic ordering condition between two variables. We determine the asymptotic properties of the empirical 2DSD index and MVR for the most frequently used stochastic orders. We also provide conditions under which the bootstrap estimators of these quantities are strongly consistent. As an application, we develop consistent bootstrap testing procedures for almost stochastic dominance. The performance of the tests is checked via simulations and the analysis of real data.","sentences":["We introduce a 2-dimensional stochastic dominance (2DSD) index to characterize both strict and almost stochastic dominance.","Based on this index, we derive an estimator for the minimum violation ratio (MVR), also known as the critical parameter, of the almost stochastic ordering condition between two variables.","We determine the asymptotic properties of the empirical 2DSD index and MVR for the most frequently used stochastic orders.","We also provide conditions under which the bootstrap estimators of these quantities are strongly consistent.","As an application, we develop consistent bootstrap testing procedures for almost stochastic dominance.","The performance of the tests is checked via simulations and the analysis of real data."],"url":"http://arxiv.org/abs/2403.15258v1","category":"econ.EM"}
{"created":"2024-03-22 14:32:02","title":"WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology","abstract":"Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI). Weakly supervised learning of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading). In this context, there is a need for improved spatial interpretability of predictions from such models. We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation. It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label. We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology. WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications.","sentences":["Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI).","Weakly supervised learning of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading).","In this context, there is a need for improved spatial interpretability of predictions from such models.","We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation.","It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label.","We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology.","WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications."],"url":"http://arxiv.org/abs/2403.15238v1","category":"eess.IV"}
{"created":"2024-03-22 14:27:45","title":"Attacking with Something That Does Not Exist: Low-Rate Flood with 'Proof of Non-Existence' Can Exhaust DNS Resolver CPU","abstract":"NSEC3 is a proof of non-existence in DNSSEC, which provides an authenticated assertion that a queried resource does not exist in the target domain. NSEC3 consists of alphabetically sorted hashed names before and after the queried hostname. To make dictionary attacks harder, the hash function can be applied in multiple iterations, which however also increases the load on the DNS resolver during the computation of the SHA-1 hashes in NSEC3 records. Concerns about the load created by the computation of NSEC3 records on the DNS resolvers were already considered in the NSEC3 specifications RFC5155 and RFC9276. In February 2024, the potential of NSEC3 to exhaust DNS resolvers' resources was assigned a CVE-2023-50868, confirming that extra iterations of NSEC3 created substantial load. However, there is no published evaluation of the attack and the impact of the attack on the resolvers was not clarified.   In this work we perform the first evaluation of the NSEC3-encloser attack against DNS resolver implementations and find that the NSEC3-encloser attack can still create a 72x increase in CPU instruction count, despite the victim resolver following RFC5155 recommendations in limiting hash iteration counts. The impact of the attack varies across the different DNS resolvers, but we show that with a sufficient volume of DNS packets the attack can increase CPU load and cause packet loss. We find that at a rate of 150 malicious NSEC3 records per second, depending on the DNS implementation, the loss rate of benign DNS requests varies between 2.7% and 30%. We provide a detailed description and implementation the NSEC3-encloser attack along with evaluation against five popular DNS resolver implementations. We also develop the first analysis how each NSEC3 parameter impacts the load inflicted on the victim resolver during NSEC3-encloser attack.","sentences":["NSEC3 is a proof of non-existence in DNSSEC, which provides an authenticated assertion that a queried resource does not exist in the target domain.","NSEC3 consists of alphabetically sorted hashed names before and after the queried hostname.","To make dictionary attacks harder, the hash function can be applied in multiple iterations, which however also increases the load on the DNS resolver during the computation of the SHA-1 hashes in NSEC3 records.","Concerns about the load created by the computation of NSEC3 records on the DNS resolvers were already considered in the NSEC3 specifications RFC5155 and RFC9276.","In February 2024, the potential of NSEC3 to exhaust DNS resolvers' resources was assigned a CVE-2023-50868, confirming that extra iterations of NSEC3 created substantial load.","However, there is no published evaluation of the attack and the impact of the attack on the resolvers was not clarified.   ","In this work we perform the first evaluation of the NSEC3-encloser attack against DNS resolver implementations and find that the NSEC3-encloser attack can still create a 72x increase in CPU instruction count, despite the victim resolver following RFC5155 recommendations in limiting hash iteration counts.","The impact of the attack varies across the different DNS resolvers, but we show that with a sufficient volume of DNS packets the attack can increase CPU load and cause packet loss.","We find that at a rate of 150 malicious NSEC3 records per second, depending on the DNS implementation, the loss rate of benign DNS requests varies between 2.7% and 30%.","We provide a detailed description and implementation the NSEC3-encloser attack along with evaluation against five popular DNS resolver implementations.","We also develop the first analysis how each NSEC3 parameter impacts the load inflicted on the victim resolver during NSEC3-encloser attack."],"url":"http://arxiv.org/abs/2403.15233v1","category":"cs.CR"}
{"created":"2024-03-22 14:03:30","title":"Exploring the Crochemore and Ziv-Lempel factorizations of some automatic sequences with the software Walnut","abstract":"We explore the Ziv-Lempel and Crochemore factorizations of some classical automatic sequences making an extensive use of the theorem prover Walnut.","sentences":["We explore the Ziv-Lempel and Crochemore factorizations of some classical automatic sequences making an extensive use of the theorem prover Walnut."],"url":"http://arxiv.org/abs/2403.15215v1","category":"cs.DM"}
{"created":"2024-03-22 13:32:08","title":"Cryptic Bytes: WebAssembly Obfuscation for Evading Cryptojacking Detection","abstract":"WebAssembly has gained significant traction as a high-performance, secure, and portable compilation target for the Web and beyond. However, its growing adoption has also introduced new security challenges. One such threat is cryptojacking, where websites mine cryptocurrencies on visitors' devices without their knowledge or consent, often through the use of WebAssembly. While detection methods have been proposed, research on circumventing them remains limited. In this paper, we present the most comprehensive evaluation of code obfuscation techniques for WebAssembly to date, assessing their effectiveness, detectability, and overhead across multiple abstraction levels. We obfuscate a diverse set of applications, including utilities, games, and crypto miners, using state-of-the-art obfuscation tools like Tigress and wasm-mutate, as well as our novel tool, emcc-obf. Our findings suggest that obfuscation can effectively produce dissimilar WebAssembly binaries, with Tigress proving most effective, followed by emcc-obf and wasm-mutate. The impact on the resulting native code is also significant, although the V8 engine's TurboFan optimizer can reduce native code size by 30\\% on average. Notably, we find that obfuscation can successfully evade state-of-the-art cryptojacking detectors. Although obfuscation can introduce substantial performance overheads, we demonstrate how obfuscation can be used for evading detection with minimal overhead in real-world scenarios by strategically applying transformations. These insights are valuable for researchers, providing a foundation for developing more robust detection methods. Additionally, we make our dataset of over 20,000 obfuscated WebAssembly binaries and the emcc-obf tool publicly available to stimulate further research.","sentences":["WebAssembly has gained significant traction as a high-performance, secure, and portable compilation target for the Web and beyond.","However, its growing adoption has also introduced new security challenges.","One such threat is cryptojacking, where websites mine cryptocurrencies on visitors' devices without their knowledge or consent, often through the use of WebAssembly.","While detection methods have been proposed, research on circumventing them remains limited.","In this paper, we present the most comprehensive evaluation of code obfuscation techniques for WebAssembly to date, assessing their effectiveness, detectability, and overhead across multiple abstraction levels.","We obfuscate a diverse set of applications, including utilities, games, and crypto miners, using state-of-the-art obfuscation tools like Tigress and wasm-mutate, as well as our novel tool, emcc-obf.","Our findings suggest that obfuscation can effectively produce dissimilar WebAssembly binaries, with Tigress proving most effective, followed by emcc-obf and wasm-mutate.","The impact on the resulting native code is also significant, although the V8 engine's TurboFan optimizer can reduce native code size by 30\\% on average.","Notably, we find that obfuscation can successfully evade state-of-the-art cryptojacking detectors.","Although obfuscation can introduce substantial performance overheads, we demonstrate how obfuscation can be used for evading detection with minimal overhead in real-world scenarios by strategically applying transformations.","These insights are valuable for researchers, providing a foundation for developing more robust detection methods.","Additionally, we make our dataset of over 20,000 obfuscated WebAssembly binaries and the emcc-obf tool publicly available to stimulate further research."],"url":"http://arxiv.org/abs/2403.15197v1","category":"cs.CR"}
{"created":"2024-03-22 13:21:09","title":"ECHO: Efficient Off-Chain Payments and Cross-Chain Swaps for Cryptocurrencies","abstract":"In this paper, we present ECHO, a TEE-based layer-2 solution that tackles two crucial challenges in the realm of cryptocurrencies: off-chain payments and cross-chain swaps. It offers three notable features: - Channel-free off-chain payments: it allows a payer to make direct payments to anyone without requiring any on-chain relationship or intermediary channels. - Real-time yet decentralized cross-chain swaps: it is the first known solution that enables real-time cross-chain swaps without relying on a central server. This novel feature is made possible through a ground-breaking fair exchange protocol. - TEE crash-tolerance: it offers two solutions to handle TEE crashes, one of which involves an innovative application of time-lock puzzles in this context. We evaluate ECHO on a network consists of 1000 nodes and the evaluation results show that ECHO can achieve 7000 TPS","sentences":["In this paper, we present ECHO, a TEE-based layer-2 solution that tackles two crucial challenges in the realm of cryptocurrencies: off-chain payments and cross-chain swaps.","It offers three notable features: - Channel-free off-chain payments: it allows a payer to make direct payments to anyone without requiring any on-chain relationship or intermediary channels.","- Real-time yet decentralized cross-chain swaps: it is the first known solution that enables real-time cross-chain swaps without relying on a central server.","This novel feature is made possible through a ground-breaking fair exchange protocol.","- TEE crash-tolerance: it offers two solutions to handle TEE crashes, one of which involves an innovative application of time-lock puzzles in this context.","We evaluate ECHO on a network consists of 1000 nodes and the evaluation results show that ECHO can achieve 7000 TPS"],"url":"http://arxiv.org/abs/2403.15191v1","category":"cs.CR"}
{"created":"2024-03-22 13:14:55","title":"Measuring two temperatures using a single thermometer","abstract":"We consider the question: Is it possible to measure two temperatures simultaneously using a single thermometer? Under common circumstances, where the thermometer can interact with only one bath at a time and the interaction leads to complete thermalization, this is clearly impossible because the final state of the thermometer would be independent of the temperature of the first bath. In this work, we show that this task can indeed be accomplished with the assistance of quantum control. In particular, we consider a composite particle with multiple quantum degrees of freedom (DoF) as a temperature sensor, where one of the DoF -- termed as internal DoF -- is susceptible to the local temperature, thereby functioning as a thermometer, whereas another DoF -- termed external DoF -- is quantum-controlled. We leverage the entanglement between the aforementioned DoF in a composite particle for two-temperature thermometry by preparing the external DoF in a quantum superposition, exposing the internal DoF to two local temperatures. We show that such a particle used in a Mach-Zehnder type interferometer, or a quantum switch -- which allows quantum control over the order of application of quantum channels -- can be used to estimate two temperatures simultaneously, thus affirming our main proposition. For each of these setups, we obtain the variance in the estimated temperatures through the multi-parameter Cram\\'er-Rao bound, and compare their performances based on the range of total variance of the two temperatures estimated. On benchmarking all the setups based on the total variance of the estimated temperatures, we find that a quantum switch with a qudit probe outperforms other setups. On restricting our probe to be a qubit, we find that quantum switch performs equally well as a Mach-Zehnder type interferometer.","sentences":["We consider the question: Is it possible to measure two temperatures simultaneously using a single thermometer?","Under common circumstances, where the thermometer can interact with only one bath at a time and the interaction leads to complete thermalization, this is clearly impossible because the final state of the thermometer would be independent of the temperature of the first bath.","In this work, we show that this task can indeed be accomplished with the assistance of quantum control.","In particular, we consider a composite particle with multiple quantum degrees of freedom (DoF) as a temperature sensor, where one of the DoF -- termed as internal DoF -- is susceptible to the local temperature, thereby functioning as a thermometer, whereas another DoF -- termed external DoF -- is quantum-controlled.","We leverage the entanglement between the aforementioned DoF in a composite particle for two-temperature thermometry by preparing the external DoF in a quantum superposition, exposing the internal DoF to two local temperatures.","We show that such a particle used in a Mach-Zehnder type interferometer, or a quantum switch -- which allows quantum control over the order of application of quantum channels -- can be used to estimate two temperatures simultaneously, thus affirming our main proposition.","For each of these setups, we obtain the variance in the estimated temperatures through the multi-parameter Cram\\'er-Rao bound, and compare their performances based on the range of total variance of the two temperatures estimated.","On benchmarking all the setups based on the total variance of the estimated temperatures, we find that a quantum switch with a qudit probe outperforms other setups.","On restricting our probe to be a qubit, we find that quantum switch performs equally well as a Mach-Zehnder type interferometer."],"url":"http://arxiv.org/abs/2403.15186v1","category":"quant-ph"}
{"created":"2024-03-22 13:05:32","title":"Sedimentation of a Spheroidal Particle in an Elastoviscoplastic Fluid","abstract":"The sedimentation dynamics of a prolate spheroidal particle in an unbounded elastoviscoplastic (EVP) fluid is studied by direct finite element simulations under inertialess flow conditions. The Saramito-Giesekus constitutive equation is employed to model the suspending liquid. The Arbitrary Lagrangian-Eulerian formulation is used to handle the particle motion. The sedimentation, lift, and angular velocities of spheroids with aspect ratio between 1 and 8 are computed as the initial orientation, Bingham, and Weissenberg numbers are varied. Similarly to the purely viscoelastic case, a spheroid in an EVP fluid rotates up to align its major axis with the applied force. As the Bingham number increases, the settling rate monotonically reduces while the angular velocity first increases and then decreases. The initial orientation has a relevant effect on the particle stoppage because of the different drag experienced by the spheroid as its orientation is varied. The yielded and unyielded regions around the spheroid reveal that, for particle oriented transversely to the force, the yielded envelope shrinks near the tips due to the fast spatial decay of the stresses, and unyielded regions appear along the surface of the particle, similar to the solid caps observed at the front and back of a sphere. Fluid plasticity enhances the negative wake phenomenon that is observed at Weissenberg numbers significantly lower than the purely viscoelastic case. Results of the drag correction coefficient for particles aligned with longest axis along the force are presented.","sentences":["The sedimentation dynamics of a prolate spheroidal particle in an unbounded elastoviscoplastic (EVP) fluid is studied by direct finite element simulations under inertialess flow conditions.","The Saramito-Giesekus constitutive equation is employed to model the suspending liquid.","The Arbitrary Lagrangian-Eulerian formulation is used to handle the particle motion.","The sedimentation, lift, and angular velocities of spheroids with aspect ratio between 1 and 8 are computed as the initial orientation, Bingham, and Weissenberg numbers are varied.","Similarly to the purely viscoelastic case, a spheroid in an EVP fluid rotates up to align its major axis with the applied force.","As the Bingham number increases, the settling rate monotonically reduces while the angular velocity first increases and then decreases.","The initial orientation has a relevant effect on the particle stoppage because of the different drag experienced by the spheroid as its orientation is varied.","The yielded and unyielded regions around the spheroid reveal that, for particle oriented transversely to the force, the yielded envelope shrinks near the tips due to the fast spatial decay of the stresses, and unyielded regions appear along the surface of the particle, similar to the solid caps observed at the front and back of a sphere.","Fluid plasticity enhances the negative wake phenomenon that is observed at Weissenberg numbers significantly lower than the purely viscoelastic case.","Results of the drag correction coefficient for particles aligned with longest axis along the force are presented."],"url":"http://arxiv.org/abs/2403.15178v1","category":"physics.flu-dyn"}
{"created":"2024-03-22 12:59:03","title":"Double Cross-fit Doubly Robust Estimators: Beyond Series Regression","abstract":"Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\\\"{o}lder smoothness, is available then more accurate \"double cross-fit doubly robust\" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\\\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are semiparametric efficient under minimal conditions and achieve fast convergence rates in the non-$\\sqrt{n}$ regime. When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\\sqrt{n}$ regime. Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves semiparametric efficiency while the usual \"single cross-fit\" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator.","sentences":["Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees.","However, when additional structure, such as H\\\"{o}lder smoothness, is available then more accurate \"double cross-fit doubly robust\" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples.","We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions.","We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators.","Then, assuming the nuisance functions are H\\\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are semiparametric efficient under minimal conditions and achieve fast convergence rates in the non-$\\sqrt{n}$ regime.","When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression.","Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\\sqrt{n}$ regime.","Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves semiparametric efficiency while the usual \"single cross-fit\" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator."],"url":"http://arxiv.org/abs/2403.15175v1","category":"math.ST"}
{"created":"2024-03-22 12:36:59","title":"Translators of the Mean Curvature Flow in Hyperbolic Einstein's Static Universe","abstract":"In this study, we deal with non-degenerate translators of the mean curvature flow in the well-known hyperbolic Einstein's static universe. We classify translators foliated by horospheres and rotationally invariant ones, both space-like and time-like. For space-like translators, we show a uniqueness theorem as well as a result to extend an isometry of the boundary of the domain to the whole translator, under simple conditions. As an application, we obtain a characterization of the the bowl when the boundary is a ball, and of certain translators foliated by horospheres whose boundary is a rectangle.","sentences":["In this study, we deal with non-degenerate translators of the mean curvature flow in the well-known hyperbolic Einstein's static universe.","We classify translators foliated by horospheres and rotationally invariant ones, both space-like and time-like.","For space-like translators, we show a uniqueness theorem as well as a result to extend an isometry of the boundary of the domain to the whole translator, under simple conditions.","As an application, we obtain a characterization of the the bowl when the boundary is a ball, and of certain translators foliated by horospheres whose boundary is a rectangle."],"url":"http://arxiv.org/abs/2403.15166v1","category":"math.DG"}
{"created":"2024-03-22 11:57:51","title":"On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond","abstract":"This paper aims to clearly distinguish between Stochastic Gradient Descent with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in stochastic setting, Adam's convergence rate upper bound matches the lower bounds of stochastic first-order optimizers, considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introducing a novel stopping-time based technique, we further prove that if we consider the minimum gradient norm during iterations, the corresponding convergence rate can match the lower bounds across all problem hyperparameters. The technique can also help proving that Adam with a specific hyperparameter scheduler is parameter-agnostic, which hence can be of independent interest.","sentences":["This paper aims to clearly distinguish between Stochastic Gradient Descent with Momentum (SGDM) and Adam in terms of their convergence rates.","We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness.","Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in stochastic setting, Adam's convergence rate upper bound matches the lower bounds of stochastic first-order optimizers, considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate.","These insights distinctly differentiate Adam and SGDM regarding their convergence rates.","Additionally, by introducing a novel stopping-time based technique, we further prove that if we consider the minimum gradient norm during iterations, the corresponding convergence rate can match the lower bounds across all problem hyperparameters.","The technique can also help proving that Adam with a specific hyperparameter scheduler is parameter-agnostic, which hence can be of independent interest."],"url":"http://arxiv.org/abs/2403.15146v1","category":"cs.LG"}
{"created":"2024-03-22 11:30:10","title":"Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection","abstract":"Current semi-supervised object detection (SSOD) algorithms typically assume class balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets (MS-COCO, etc). This assumption can be easily violated since real world datasets can be extremely class imbalanced in nature, thus making the performance of semi-supervised object detectors far from satisfactory. Besides, the research for this problem in SSOD is severely under-explored. To bridge this research gap, we comprehensively study the class imbalance problem for SSOD under more challenging scenarios, thus forming the first experimental setting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet effective gradient-based sampling framework that tackles the class imbalance problem from the perspective of two types of confirmation biases. To tackle confirmation bias towards majority classes, the gradient-based reweighting and gradient-based thresholding modules leverage the gradients from each class to fully balance the influence of the majority and minority classes. To tackle the confirmation bias from incorrect pseudo labels of minority classes, the class-rebalancing sampling module resamples unlabeled data following the guidance of the gradient-based reweighting module. Experiments on three proposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that our method outperforms current class imbalanced object detectors by clear margins, serving as a baseline for future research in CI-SSOD. Code will be available at https://github.com/nightkeepers/CI-SSOD.","sentences":["Current semi-supervised object detection (SSOD) algorithms typically assume class balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets (MS-COCO, etc).","This assumption can be easily violated since real world datasets can be extremely class imbalanced in nature, thus making the performance of semi-supervised object detectors far from satisfactory.","Besides, the research for this problem in SSOD is severely under-explored.","To bridge this research gap, we comprehensively study the class imbalance problem for SSOD under more challenging scenarios, thus forming the first experimental setting for class imbalanced SSOD (CI-SSOD).","Moreover, we propose a simple yet effective gradient-based sampling framework that tackles the class imbalance problem from the perspective of two types of confirmation biases.","To tackle confirmation bias towards majority classes, the gradient-based reweighting and gradient-based thresholding modules leverage the gradients from each class to fully balance the influence of the majority and minority classes.","To tackle the confirmation bias from incorrect pseudo labels of minority classes, the class-rebalancing sampling module resamples unlabeled data following the guidance of the gradient-based reweighting module.","Experiments on three proposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that our method outperforms current class imbalanced object detectors by clear margins, serving as a baseline for future research in CI-SSOD.","Code will be available at https://github.com/nightkeepers/CI-SSOD."],"url":"http://arxiv.org/abs/2403.15127v1","category":"cs.CV"}
{"created":"2024-03-22 10:51:55","title":"Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks","abstract":"This paper addresses a new active learning strategy for regression problems. The presented Wasserstein active regression model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein active regression often achieves more precise estimations and tends to improve accuracy faster than other models.","sentences":["This paper addresses a new active learning strategy for regression problems.","The presented Wasserstein active regression model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset.","The Wasserstein distance is computed using GroupSort Neural Networks.","The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth.","This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy.","Finally, this method is compared with other classical and recent solutions.","The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure.","Moreover, the Wasserstein active regression often achieves more precise estimations and tends to improve accuracy faster than other models."],"url":"http://arxiv.org/abs/2403.15108v1","category":"cs.LG"}
{"created":"2024-03-22 10:08:51","title":"Measurement of the W-boson mass and width with the ATLAS detector using proton-proton collisions at $\\sqrt{s}$ = 7 TeV","abstract":"Proton-proton data recorded by the ATLAS detector in 2011, at a centre-of-mass energy of 7 TeV, have been used for an improved determination of the W-boson mass and a first measurement of the W-boson width at the LHC. Recent fits to the proton parton distribution functions are incorporated in the measurement procedure and an improved statistical method is used to increase the measurement precision. The measurement of the W-boson mass yields a value of $m_W = 80366.5 \\pm 9.8 (stat.) \\pm 12.5 (syst.)$ MeV = $80366.5 \\pm 15.9$ MeV, and the width is measured as $\\Gamma_W = 2202 \\pm 32 (stat.) \\pm 34 (syst.)$ MeV = $2202 \\pm 47$ MeV. The first uncertainty components are statistical and the second correspond to the experimental and physics-modelling systematic uncertainties. Both results are consistent with the expectation from fits to electroweak precision data. The present measurement of $m_W$ is compatible with and supersedes the previous measurement performed using the same data.","sentences":["Proton-proton data recorded by the ATLAS detector in 2011, at a centre-of-mass energy of 7 TeV, have been used for an improved determination of the W-boson mass and a first measurement of the W-boson width at the LHC.","Recent fits to the proton parton distribution functions are incorporated in the measurement procedure and an improved statistical method is used to increase the measurement precision.","The measurement of the W-boson mass yields a value of $m_W = 80366.5 \\pm 9.8 (stat.)","\\pm 12.5 (syst.)$ MeV = $80366.5 \\pm 15.9$ MeV, and the width is measured as $\\Gamma_W = 2202 \\pm 32 (stat.)","\\pm 34 (syst.)$ MeV = $2202 \\pm 47$ MeV.","The first uncertainty components are statistical and the second correspond to the experimental and physics-modelling systematic uncertainties.","Both results are consistent with the expectation from fits to electroweak precision data.","The present measurement of $m_W$ is compatible with and supersedes the previous measurement performed using the same data."],"url":"http://arxiv.org/abs/2403.15085v1","category":"hep-ex"}
{"created":"2024-03-22 17:11:47","title":"Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities","abstract":"The development of foundation models has revolutionized our ability to interpret the Earth's surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA's innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data.","sentences":["The development of foundation models has revolutionized our ability to interpret the Earth's surface using satellite observational data.","Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics.","This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources.","Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively.","This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining.","DOFA's innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data."],"url":"http://arxiv.org/abs/2403.15356v1","category":"cs.CV"}
{"created":"2024-03-22 16:55:46","title":"Enhanced Imaging of Electronic Hot Spots Using Quantum Squeezed Light","abstract":"Detecting electronic hot spots is important for understanding the heat dissipation and thermal management of electronic and semiconductor devices. Optical thermoreflective imaging is being used to perform precise temporal and spatial imaging of heat on wires and semiconductor materials. We apply quantum squeezed light to perform thermoreflective imaging on micro-wires, surpassing the shot-noise limit of classical approaches. We obtain a far-field temperature sensing accuracy of 42 mK after 50 ms of averaging and show that a $256\\times256$ pixel image can be constructed with such sensitivity in 10 minutes. We can further obtain single-shot temperature sensing of 1.6 K after only 10 $\\mathrm{\\mu s}$ of averaging enabling dynamical study of heat dissipation. Not only do the quantum images provide accurate spatio-temporal information about heat distribution, but the measure of quantum correlation provides additional information, inaccessible by classical techniques, that can lead to a better understanding of the dynamics. We apply the technique to both Al and Nb microwires and discuss the applications of the technique in studying electron dynamics at low temperatures.","sentences":["Detecting electronic hot spots is important for understanding the heat dissipation and thermal management of electronic and semiconductor devices.","Optical thermoreflective imaging is being used to perform precise temporal and spatial imaging of heat on wires and semiconductor materials.","We apply quantum squeezed light to perform thermoreflective imaging on micro-wires, surpassing the shot-noise limit of classical approaches.","We obtain a far-field temperature sensing accuracy of 42 mK after 50 ms of averaging and show that a $256\\times256$ pixel image can be constructed with such sensitivity in 10 minutes.","We can further obtain single-shot temperature sensing of 1.6 K after only 10 $\\mathrm{\\mu s}$ of averaging enabling dynamical study of heat dissipation.","Not only do the quantum images provide accurate spatio-temporal information about heat distribution, but the measure of quantum correlation provides additional information, inaccessible by classical techniques, that can lead to a better understanding of the dynamics.","We apply the technique to both Al and Nb microwires and discuss the applications of the technique in studying electron dynamics at low temperatures."],"url":"http://arxiv.org/abs/2403.15345v1","category":"quant-ph"}
{"created":"2024-03-22 16:47:00","title":"Topoi with enough points","abstract":"We extend Deligne's original argument showing that locally coherent topoi have enough points, clarified using collage diagrams. We show that our refinement of Deligne's technique can be adapted to recover every existing result of this kind, including the most recent results about $\\kappa$-coherent $\\kappa$-topoi. Our presentation allows us to relax the cardinality assumptions typically imposed on the sites involved. We show that a larger class of locally finitely presentable toposes have enough points and that a closed subtopos of a topos with enough points has enough points.","sentences":["We extend Deligne's original argument showing that locally coherent topoi have enough points, clarified using collage diagrams.","We show that our refinement of Deligne's technique can be adapted to recover every existing result of this kind, including the most recent results about $\\kappa$-coherent $\\kappa$-topoi.","Our presentation allows us to relax the cardinality assumptions typically imposed on the sites involved.","We show that a larger class of locally finitely presentable toposes have enough points and that a closed subtopos of a topos with enough points has enough points."],"url":"http://arxiv.org/abs/2403.15338v1","category":"math.CT"}
{"created":"2024-03-22 16:06:43","title":"Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel Segmentation","abstract":"Personalized 3D vascular models can aid in a range of diagnostic, prognostic, and treatment-planning tasks relevant to cardiovascular disease management. Deep learning provides a means to automatically obtain such models. Ideally, a user should have control over the exact region of interest (ROI) to be included in a vascular model, and the model should be watertight and highly accurate. To this end, we propose a combination of a global controller leveraging voxel mask segmentations to provide boundary conditions for vessels of interest to a local, iterative vessel segmentation model. We introduce the preservation of scale- and rotational symmetries in the local segmentation model, leading to generalisation to vessels of unseen sizes and orientations. Combined with the global controller, this enables flexible 3D vascular model building, without additional retraining. We demonstrate the potential of our method on a dataset containing abdominal aortic aneurysms (AAAs). Our method performs on par with a state-of-the-art segmentation model in the segmentation of AAAs, iliac arteries and renal arteries, while providing a watertight, smooth surface segmentation. Moreover, we demonstrate that by adapting the global controller, we can easily extend vessel sections in the 3D model.","sentences":["Personalized 3D vascular models can aid in a range of diagnostic, prognostic, and treatment-planning tasks relevant to cardiovascular disease management.","Deep learning provides a means to automatically obtain such models.","Ideally, a user should have control over the exact region of interest (ROI) to be included in a vascular model, and the model should be watertight and highly accurate.","To this end, we propose a combination of a global controller leveraging voxel mask segmentations to provide boundary conditions for vessels of interest to a local, iterative vessel segmentation model.","We introduce the preservation of scale- and rotational symmetries in the local segmentation model, leading to generalisation to vessels of unseen sizes and orientations.","Combined with the global controller, this enables flexible 3D vascular model building, without additional retraining.","We demonstrate the potential of our method on a dataset containing abdominal aortic aneurysms (AAAs).","Our method performs on par with a state-of-the-art segmentation model in the segmentation of AAAs, iliac arteries and renal arteries, while providing a watertight, smooth surface segmentation.","Moreover, we demonstrate that by adapting the global controller, we can easily extend vessel sections in the 3D model."],"url":"http://arxiv.org/abs/2403.15314v1","category":"eess.IV"}
{"created":"2024-03-22 14:20:34","title":"Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models","abstract":"In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN","sentences":["In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS).","Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks.","Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference.","Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency.","To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks.","The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed.","For instance, LaVIN-EAS can obtain 89.98\\% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN"],"url":"http://arxiv.org/abs/2403.15226v1","category":"cs.MM"}
{"created":"2024-03-22 13:11:21","title":"A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering","abstract":"To alleviate the performance and energy overheads of contemporary applications with large data footprints, we propose the Two Level Perceptron (TLP) predictor, a neural mechanism that effectively combines predicting whether an access will be off-chip with adaptive prefetch filtering at the first-level data cache (L1D). TLP is composed of two connected microarchitectural perceptron predictors, named First Level Predictor (FLP) and Second Level Predictor (SLP). FLP performs accurate off-chip prediction by using several program features based on virtual addresses and a novel selective delay component. The novelty of SLP relies on leveraging off-chip prediction to drive L1D prefetch filtering by using physical addresses and the FLP prediction as features. TLP constitutes the first hardware proposal targeting both off-chip prediction and prefetch filtering using a multi-level perceptron hardware approach. TLP only requires 7KB of storage. To demonstrate the benefits of TLP we compare its performance with state-of-the-art approaches using off-chip prediction and prefetch filtering on a wide range of single-core and multi-core workloads. Our experiments show that TLP reduces the average DRAM transactions by 30.7% and 17.7%, as compared to a baseline using state-of-the-art cache prefetchers but no off-chip prediction mechanism, across the single-core and multi-core workloads, respectively, while recent work significantly increases DRAM transactions. As a result, TLP achieves geometric mean performance speedups of 6.2% and 11.8% across single-core and multi-core workloads, respectively. In addition, our evaluation demonstrates that TLP is effective independently of the L1D prefetching logic.","sentences":["To alleviate the performance and energy overheads of contemporary applications with large data footprints, we propose the Two Level Perceptron (TLP) predictor, a neural mechanism that effectively combines predicting whether an access will be off-chip with adaptive prefetch filtering at the first-level data cache (L1D).","TLP is composed of two connected microarchitectural perceptron predictors, named First Level Predictor (FLP) and Second Level Predictor (SLP).","FLP performs accurate off-chip prediction by using several program features based on virtual addresses and a novel selective delay component.","The novelty of SLP relies on leveraging off-chip prediction to drive L1D prefetch filtering by using physical addresses and the FLP prediction as features.","TLP constitutes the first hardware proposal targeting both off-chip prediction and prefetch filtering using a multi-level perceptron hardware approach.","TLP only requires 7KB of storage.","To demonstrate the benefits of TLP we compare its performance with state-of-the-art approaches using off-chip prediction and prefetch filtering on a wide range of single-core and multi-core workloads.","Our experiments show that TLP reduces the average DRAM transactions by 30.7% and 17.7%, as compared to a baseline using state-of-the-art cache prefetchers but no off-chip prediction mechanism, across the single-core and multi-core workloads, respectively, while recent work significantly increases DRAM transactions.","As a result, TLP achieves geometric mean performance speedups of 6.2% and 11.8% across single-core and multi-core workloads, respectively.","In addition, our evaluation demonstrates that TLP is effective independently of the L1D prefetching logic."],"url":"http://arxiv.org/abs/2403.15181v1","category":"cs.AR"}
{"created":"2024-03-22 17:20:32","title":"Scaling limit of fluctuations for high contrast stochastic homogenization of the Helmholtz equation: second order moments","abstract":"This work is concerned with the high contrast stochastic homogenization of the Helmholtz equation. Our goal is to characterize the second order moments of the scaling limit of the fluctuations of the wavefield. We show that these moments are those of a random wavefield solution to a homogenized Helmholtz equation with a white noise source term and obtain expressions for its variance. Two factors contribute to the white noise: fluctuations in the inverse permittivity of the high contrast inhomogeneities, and fluctuations in their size. This problem is motivated by wave propagation in sea ice, which is a random compositive of ice and pockets of air and brine. The analysis hinges on three ingredients: a covariance formula due to Chatterjee for functions of independent random variables; small-volume expansions to quantify the fluctuations due to one inclusion; and the standard two-scale expansions for stochastic homogenization.","sentences":["This work is concerned with the high contrast stochastic homogenization of the Helmholtz equation.","Our goal is to characterize the second order moments of the scaling limit of the fluctuations of the wavefield.","We show that these moments are those of a random wavefield solution to a homogenized Helmholtz equation with a white noise source term and obtain expressions for its variance.","Two factors contribute to the white noise: fluctuations in the inverse permittivity of the high contrast inhomogeneities, and fluctuations in their size.","This problem is motivated by wave propagation in sea ice, which is a random compositive of ice and pockets of air and brine.","The analysis hinges on three ingredients: a covariance formula due to Chatterjee for functions of independent random variables; small-volume expansions to quantify the fluctuations due to one inclusion; and the standard two-scale expansions for stochastic homogenization."],"url":"http://arxiv.org/abs/2403.15359v1","category":"math.AP"}
{"created":"2024-03-22 17:02:48","title":"Low-Regularity Solutions of the Nonlinear Schr\u00f6dinger Equation on the Spatial Quarter-Plane","abstract":"The Hadamard well-posedness of the nonlinear Schr\\\"odinger equation with power nonlinearity formulated on the spatial quarter-plane is established in a low-regularity setting with Sobolev initial data and Dirichlet boundary data in appropriate Bourgain-type spaces. As both of the spatial variables are restricted to the half-line, a different approach is needed than the one previously used for the well-posedness of other initial-boundary value problems. In particular, now the solution of the forced linear initial-boundary problem is estimated \\textit{directly}, both in Sobolev spaces and in Strichartz-type spaces, i.e. without a linear decomposition that would require estimates for the associated homogeneous and nonhomogeneous initial value problems. In the process of deriving the linear estimates, the function spaces for the boundary data are identified as the intersections of certain modified Bourgain-type spaces that involve spatial half-line Fourier transforms instead of the usual whole-line Fourier transform found in the definition of the standard Bourgain space associated with the one-dimensional initial value problem. The fact that the quarter-plane has a corner at the origin poses an additional challenge, as it requires one to expand the validity of certain Sobolev extension results to the case of a domain with a non-smooth (Lipschitz) and non-compact boundary.","sentences":["The Hadamard well-posedness of the nonlinear Schr\\\"odinger equation with power nonlinearity formulated on the spatial quarter-plane is established in a low-regularity setting with Sobolev initial data and Dirichlet boundary data in appropriate Bourgain-type spaces.","As both of the spatial variables are restricted to the half-line, a different approach is needed than the one previously used for the well-posedness of other initial-boundary value problems.","In particular, now the solution of the forced linear initial-boundary problem is estimated \\textit{directly}, both in Sobolev spaces and in Strichartz-type spaces, i.e. without a linear decomposition that would require estimates for the associated homogeneous and nonhomogeneous initial value problems.","In the process of deriving the linear estimates, the function spaces for the boundary data are identified as the intersections of certain modified Bourgain-type spaces that involve spatial half-line Fourier transforms instead of the usual whole-line Fourier transform found in the definition of the standard Bourgain space associated with the one-dimensional initial value problem.","The fact that the quarter-plane has a corner at the origin poses an additional challenge, as it requires one to expand the validity of certain Sobolev extension results to the case of a domain with a non-smooth (Lipschitz) and non-compact boundary."],"url":"http://arxiv.org/abs/2403.15350v1","category":"math.AP"}
{"created":"2024-03-22 16:40:03","title":"Benchmarking of machine learning interatomic potentials for reactive hydrogen dynamics at metal surfaces","abstract":"Simulations of chemical reaction probabilities in gas surface dynamics require the calculation of ensemble averages over many tens of thousands of reaction events to predict dynamical observables that can be compared to experiments. At the same time, the energy landscapes need to be accurately mapped, as small errors in barriers can lead to large deviations in reaction probabilities. This brings a particularly interesting challenge for machine learning interatomic potentials, which are becoming well-established tools to accelerate molecular dynamics simulations. We compare state-of-the-art machine learning interatomic potentials with a particular focus on their inference performance on CPUs and suitability for high throughput simulation of reactive chemistry at surfaces. The considered models include polarizable atom interaction neural networks (PaiNN), recursively embedded atom neural networks (REANN), the MACE equivariant graph neural network, and atomic cluster expansion potentials (ACE). The models are applied to a dataset on reactive molecular hydrogen scattering on low-index surface facets of copper. All models are assessed for their accuracy, time-to-solution, and ability to simulate reactive sticking probabilities as a function of the rovibrational initial state and kinetic incidence energy of the molecule. REANN and MACE models provide the best balance between accuracy and time-to-solution and can be considered the current state-of-the-art in gas-surface dynamics. PaiNN models require many features for the best accuracy, which causes significant losses in computational efficiency. ACE models provide the fastest time-to-solution, however, models trained on the existing dataset were not able to achieve sufficiently accurate predictions in all cases.","sentences":["Simulations of chemical reaction probabilities in gas surface dynamics require the calculation of ensemble averages over many tens of thousands of reaction events to predict dynamical observables that can be compared to experiments.","At the same time, the energy landscapes need to be accurately mapped, as small errors in barriers can lead to large deviations in reaction probabilities.","This brings a particularly interesting challenge for machine learning interatomic potentials, which are becoming well-established tools to accelerate molecular dynamics simulations.","We compare state-of-the-art machine learning interatomic potentials with a particular focus on their inference performance on CPUs and suitability for high throughput simulation of reactive chemistry at surfaces.","The considered models include polarizable atom interaction neural networks (PaiNN), recursively embedded atom neural networks (REANN), the MACE equivariant graph neural network, and atomic cluster expansion potentials (ACE).","The models are applied to a dataset on reactive molecular hydrogen scattering on low-index surface facets of copper.","All models are assessed for their accuracy, time-to-solution, and ability to simulate reactive sticking probabilities as a function of the rovibrational initial state and kinetic incidence energy of the molecule.","REANN and MACE models provide the best balance between accuracy and time-to-solution and can be considered the current state-of-the-art in gas-surface dynamics.","PaiNN models require many features for the best accuracy, which causes significant losses in computational efficiency.","ACE models provide the fastest time-to-solution, however, models trained on the existing dataset were not able to achieve sufficiently accurate predictions in all cases."],"url":"http://arxiv.org/abs/2403.15334v1","category":"physics.chem-ph"}
{"created":"2024-03-22 15:50:13","title":"Quantitative propagation of smallness and spectral estimates for the Schr\u00f6dinger operator","abstract":"In this paper, we investigate quantitative propagation of smallness properties for the Schr\\\"odinger operator on a bounded domain in $\\mathbb R^d$. We extend Logunov, Malinnikova's results concerning propagation of smallness for $A$-harmonic functions to solutions of divergence elliptic equations perturbed by a bounded zero order term. We also prove similar results for gradient of solutions to some particular equations. This latter result enables us to follow the recent strategy of Burq, Moyano for the obtaining of spectral estimates on rough sets for the Schr\\\"odinger operator. Applications to observability estimates and to the null-controllability of associated parabolic equations posed on compact manifolds or the whole euclidean space are then considered.","sentences":["In this paper, we investigate quantitative propagation of smallness properties for the Schr\\\"odinger operator on a bounded domain in $\\mathbb R^d$.","We extend Logunov, Malinnikova's results concerning propagation of smallness for $A$-harmonic functions to solutions of divergence elliptic equations perturbed by a bounded zero order term.","We also prove similar results for gradient of solutions to some particular equations.","This latter result enables us to follow the recent strategy of Burq, Moyano for the obtaining of spectral estimates on rough sets for the Schr\\\"odinger operator.","Applications to observability estimates and to the null-controllability of associated parabolic equations posed on compact manifolds or the whole euclidean space are then considered."],"url":"http://arxiv.org/abs/2403.15299v1","category":"math.AP"}
{"created":"2024-03-22 14:26:04","title":"$L^p$-boundedness properties for some harmonic analysis operators defined by resolvents for a Laplacian with drift in Euclidean spaces","abstract":"We consider the Laplacian with drift in $\\mathbb R^n$ defined by $\\Delta_\\nu = \\sum_{i=1}^n(\\frac{\\partial^2}{\\partial x_i^2} + 2 \\nu_i\\frac{\\partial }{\\partial{x_i}})$ where $\\nu=(\\nu_1,\\ldots,\\nu_n)\\in \\mathbb R^n\\setminus\\{0\\}$. The operator $\\Delta_\\nu$ is selfadjoint with respect to the measure $d\\mu_\\nu(x)=e^{2\\langle\\nu,x\\rangle}dx$. This measure is not doubling but it is locally doubling in $\\mathbb R^n$. We define, for every $M>0$ and $k \\in \\mathbb N$, the operators $$ W^k_{\\nu,M,*}(f) = \\sup_{t>0}\\left|A^k_{\\nu,M,t}(f)\\right|,\\hspace{5mm}g_{\\nu,M}^k(f) = \\left(\\int_0^\\infty\\left|A^k_{\\nu,M,t}(f)\\right|^2\\frac{dt}{t}\\right)^{\\frac{1}{2}},\\,k\\geq 1, $$ the $\\rho$-variation operator $$ V_\\rho\\left( \\{A^k_{\\nu,M,t}\\}_{t>0}\\right)(f)= \\sup_{0<t_1<\\cdots<t_\\ell,\\,\\ell \\in \\mathbb N}\\left(\\sum^{\\ell-1}_{j=1}\\left|A^k_{\\nu,M,t_j}(f)- A^k_{\\nu,M,t_{j+1}}(f)\\right|^\\rho\\right)^{\\frac{1}{\\rho}},\\;\\; \\rho>2, $$ and, if $\\{t_j\\}_{j\\in \\mathbb N}$ is a decreasing sequence in $(0,\\infty)$, the oscillation operator $$ O(\\{A_{\\nu,M,t}^k\\}_{t>0},\\{t_j\\}_{j\\in \\mathbb N})(f)=\\Big(\\sum_{j\\in \\mathbb N}\\;\\;\\sup_{t_{j+1}\\leq \\varepsilon <\\varepsilon '\\leq t_j}|A^k_{\\nu,M,\\varepsilon}(f)-A^k_{\\nu,M,\\varepsilon '}(f)|^2 \\Big)^{1/2}. $$ where $A^k_{\\nu,M,t}=t^k\\partial^k_t(I-t\\Delta_\\nu)^{-M}$, $t>0$. We denote by $T_{\\nu,M}^k$ any of the above operators. We analyze the boundedness of $T^k_{\\nu,M}$ on $L^p(\\mathbb R^n,\\mu_\\nu)$ into itself, for every $1<p<\\infty$, and from $L^1(\\mathbb R^n,\\mu_\\nu)$ into $L^{1,\\infty}(\\mathbb R^n,\\mu_\\nu)$. In addition, we obtain boundedness properties for the operator $G_{\\nu,M}^{k,\\ell}$, $1\\leq \\ell <2M$, defined by $$ G_{\\nu,M}^{k,\\ell}(f)=\\left(\\int_0^\\infty\\left|t^{\\ell /2+k}\\partial _t^kD^{(\\ell)}(I-t\\Delta _\\nu)^{-M}(f) \\right|^2\\frac{dt}{t}\\right)^{\\frac{1}{2}}, $$ for certain differentiation operator $D^{(\\ell)}$.","sentences":["We consider the Laplacian with drift in $\\mathbb R^n$ defined by $\\Delta_\\nu = \\sum_{i=1}^n(\\frac{\\partial^2}{\\partial x_i^2} +","2 \\nu_i\\frac{\\partial }{\\partial{x_i}})$ where $\\nu=(\\nu_1,\\ldots,\\nu_n)\\in \\mathbb R^n\\setminus\\{0\\}$.","The operator $\\Delta_\\nu$ is selfadjoint with respect to the measure $d\\mu_\\nu(x)=e^{2\\langle\\nu,x\\rangle}dx$.","This measure is not doubling but it is locally doubling in $\\mathbb R^n$. We define, for every $M>0$ and $k \\in \\mathbb N$, the operators $$ W^k_{\\nu,M,*}(f) = \\sup_{t>0}\\left|A^k_{\\nu,M,t}(f)\\right|,\\hspace{5mm}g_{\\nu,M}^k(f) = \\left(\\int_0^\\infty\\left|A^k_{\\nu,M,t}(f)\\right|^2\\frac{dt}{t}\\right)^{\\frac{1}{2}},\\,k\\geq 1, $$ the $\\rho$-variation operator $$ V_\\rho\\left( \\{A^k_{\\nu,M,t}\\}_{t>0}\\right)(f)= \\sup_{0<t_1<\\cdots<t_\\ell,\\,\\ell \\in \\mathbb N}\\left(\\sum^{\\ell-1}_{j=1}\\left|A^k_{\\nu,M,t_j}(f)- A^k_{\\nu,M,t_{j+1}}(f)\\right|^\\rho\\right)^{\\frac{1}{\\rho}},\\;\\; \\rho>2, $$ and, if $\\{t_j\\}_{j\\in \\mathbb N}$ is a decreasing sequence in $(0,\\infty)$, the oscillation operator $$ O(\\{A_{\\nu,M,t}^k\\}_{t>0},\\{t_j\\}_{j\\in \\mathbb N})(f)=\\Big(\\sum_{j\\in \\mathbb N}\\;\\;\\sup_{t_{j+1}\\leq \\varepsilon <\\varepsilon '\\leq t_j}|A^k_{\\nu,M,\\varepsilon}(f)-A^k_{\\nu,M,\\varepsilon '}(f)|^2 \\Big)^{1/2}.","$$ where $A^k_{\\nu,M,t}=t^k\\partial^k_t(I-t\\Delta_\\nu)^{-M}$, $t>0$. We denote by $T_{\\nu,M}^k$ any of the above operators.","We analyze the boundedness of $T^k_{\\nu,M}$ on $L^p(\\mathbb R^n,\\mu_\\nu)$ into itself, for every $1<p<\\infty$, and from $L^1(\\mathbb R^n,\\mu_\\nu)$ into $L^{1,\\infty}(\\mathbb R^n,\\mu_\\nu)$.","In addition, we obtain boundedness properties for the operator $G_{\\nu,M}^{k,\\ell}$, $1\\leq \\ell <2M$, defined by $$ G_{\\nu,M}^{k,\\ell}(f)=\\left(\\int_0^\\infty\\left|t^{\\ell /2+k}\\partial _t^kD^{(\\ell)}(I-t\\Delta _","\\nu)^{-M}(f) \\right|^2\\frac{dt}{t}\\right)^{\\frac{1}{2}}, $$ for certain differentiation operator $D^{(\\ell)}$."],"url":"http://arxiv.org/abs/2403.15232v1","category":"math.CA"}
{"created":"2024-03-22 14:18:52","title":"Differentially Private Ad Conversion Measurement","abstract":"In this work, we study ad conversion measurement, a central functionality in digital advertising, where an advertiser seeks to estimate advertiser website (or mobile app) conversions attributed to ad impressions that users have interacted with on various publisher websites (or mobile apps). Using differential privacy (DP), a notion that has gained in popularity due to its strong mathematical guarantees, we develop a formal framework for private ad conversion measurement. In particular, we define the notion of an operationally valid configuration of the attribution rule, DP adjacency relation, contribution bounding scope and enforcement point. We then provide, for the set of configurations that most commonly arises in practice, a complete characterization, which uncovers a delicate interplay between attribution and privacy.","sentences":["In this work, we study ad conversion measurement, a central functionality in digital advertising, where an advertiser seeks to estimate advertiser website (or mobile app) conversions attributed to ad impressions that users have interacted with on various publisher websites (or mobile apps).","Using differential privacy (DP), a notion that has gained in popularity due to its strong mathematical guarantees, we develop a formal framework for private ad conversion measurement.","In particular, we define the notion of an operationally valid configuration of the attribution rule, DP adjacency relation, contribution bounding scope and enforcement point.","We then provide, for the set of configurations that most commonly arises in practice, a complete characterization, which uncovers a delicate interplay between attribution and privacy."],"url":"http://arxiv.org/abs/2403.15224v1","category":"cs.CR"}
{"created":"2024-03-22 13:55:52","title":"GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition","abstract":"Skeleton-based action recognition (SAR) in videos is an important but challenging task in computer vision. The recent state-of-the-art models for SAR are primarily based on graph convolutional neural networks (GCNs), which are powerful in extracting the spatial information of skeleton data. However, it is yet clear that such GCN-based models can effectively capture the temporal dynamics of human action sequences. To this end, we propose the DevLSTM module, which exploits the path development -- a principled and parsimonious representation for sequential data by leveraging the Lie group structure. The path development, originated from Rough path theory, can effectively capture the order of events in high-dimensional stream data with massive dimension reduction and consequently enhance the LSTM module substantially. Our proposed G-DevLSTM module can be conveniently plugged into the temporal graph, complementing existing advanced GCN-based models. Our empirical studies on the NTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid model significantly outperforms the current best-performing methods in SAR tasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.","sentences":["Skeleton-based action recognition (SAR) in videos is an important but challenging task in computer vision.","The recent state-of-the-art models for SAR are primarily based on graph convolutional neural networks (GCNs), which are powerful in extracting the spatial information of skeleton data.","However, it is yet clear that such GCN-based models can effectively capture the temporal dynamics of human action sequences.","To this end, we propose the DevLSTM module, which exploits the path development -- a principled and parsimonious representation for sequential data by leveraging the Lie group structure.","The path development, originated from Rough path theory, can effectively capture the order of events in high-dimensional stream data with massive dimension reduction and consequently enhance the LSTM module substantially.","Our proposed G-DevLSTM module can be conveniently plugged into the temporal graph, complementing existing advanced GCN-based models.","Our empirical studies on the NTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid model significantly outperforms the current best-performing methods in SAR tasks.","The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM."],"url":"http://arxiv.org/abs/2403.15212v1","category":"cs.CV"}
{"created":"2024-03-22 11:28:05","title":"A non-linear characterization of stochastic completeness of graphs","abstract":"We study non-linear Schr\\\"odinger operators on graphs. We construct minimal nonnegative solutions to corresponding semi-linear elliptic equations and use them to introduce the notion of stochastic completeness at infinity in a non-linear setting. We provide characterizations for this property in terms of a semi-linear Liouville theorem. It is employed to establish a non-linear characterization for stochastic completeness, which is a graph version of a recent result on Riemannian manifolds.","sentences":["We study non-linear Schr\\\"odinger operators on graphs.","We construct minimal nonnegative solutions to corresponding semi-linear elliptic equations and use them to introduce the notion of stochastic completeness at infinity in a non-linear setting.","We provide characterizations for this property in terms of a semi-linear Liouville theorem.","It is employed to establish a non-linear characterization for stochastic completeness, which is a graph version of a recent result on Riemannian manifolds."],"url":"http://arxiv.org/abs/2403.15125v1","category":"math.AP"}
{"created":"2024-03-22 11:27:43","title":"EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting","abstract":"Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com","sentences":["Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots.","However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries.","In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing.","Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries.","The project page is at https://EndoGSLAM.loping151.com"],"url":"http://arxiv.org/abs/2403.15124v1","category":"cs.CV"}
{"created":"2024-03-22 10:06:42","title":"SIMAP: A simplicial-map layer for neural networks","abstract":"In this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing the interpretability of the output. The SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an explainable neural network based on support sets and simplicial maps (functions used in topology to transform shapes while preserving their structural connectivity). The novelty of the methodology proposed in this paper is two-fold: Firstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. Secondly, unlike SMNNs, the support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.","sentences":["In this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing the interpretability of the output.","The SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an explainable neural network based on support sets and simplicial maps (functions used in topology to transform shapes while preserving their structural connectivity).","The novelty of the methodology proposed in this paper is two-fold: Firstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers.","Secondly, unlike SMNNs, the support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm."],"url":"http://arxiv.org/abs/2403.15083v1","category":"cs.LG"}
{"created":"2024-03-22 09:48:50","title":"Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks","abstract":"Graph convolutional networks (GCNs) have emerged as a powerful alternative to multiple instance learning with convolutional neural networks in digital pathology, offering superior handling of structural information across various spatial ranges - a crucial aspect of learning from gigapixel H&E-stained whole slide images (WSI). However, graph message-passing algorithms often suffer from oversmoothing when aggregating a large neighborhood. Hence, effective modeling of multi-range interactions relies on the careful construction of the graph. Our proposed multi-scale GCN (MS-GCN) tackles this issue by leveraging information across multiple magnification levels in WSIs. MS-GCN enables the simultaneous modeling of long-range structural dependencies at lower magnifications and high-resolution cellular details at higher magnifications, akin to analysis pipelines usually conducted by pathologists. The architecture's unique configuration allows for the concurrent modeling of structural patterns at lower magnifications and detailed cellular features at higher ones, while also quantifying the contribution of each magnification level to the prediction. Through testing on different datasets, MS-GCN demonstrates superior performance over existing single-magnification GCN methods. The enhancement in performance and interpretability afforded by our method holds promise for advancing computational pathology models, especially in tasks requiring extensive spatial context.","sentences":["Graph convolutional networks (GCNs) have emerged as a powerful alternative to multiple instance learning with convolutional neural networks in digital pathology, offering superior handling of structural information across various spatial ranges - a crucial aspect of learning from gigapixel H&E-stained whole slide images (WSI).","However, graph message-passing algorithms often suffer from oversmoothing when aggregating a large neighborhood.","Hence, effective modeling of multi-range interactions relies on the careful construction of the graph.","Our proposed multi-scale GCN (MS-GCN) tackles this issue by leveraging information across multiple magnification levels in WSIs.","MS-GCN enables the simultaneous modeling of long-range structural dependencies at lower magnifications and high-resolution cellular details at higher magnifications, akin to analysis pipelines usually conducted by pathologists.","The architecture's unique configuration allows for the concurrent modeling of structural patterns at lower magnifications and detailed cellular features at higher ones, while also quantifying the contribution of each magnification level to the prediction.","Through testing on different datasets, MS-GCN demonstrates superior performance over existing single-magnification GCN methods.","The enhancement in performance and interpretability afforded by our method holds promise for advancing computational pathology models, especially in tasks requiring extensive spatial context."],"url":"http://arxiv.org/abs/2403.15068v1","category":"eess.IV"}
{"created":"2024-03-22 09:40:27","title":"Construction of a Japanese Financial Benchmark for Large Language Models","abstract":"With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for benchmarks to evaluate the performance of current LLMs in each domain. Therefore, in this study, we constructed a benchmark comprising multiple tasks specific to the Japanese and financial domains and performed benchmark measurements on some models. Consequently, we confirmed that GPT-4 is currently outstanding, and that the constructed benchmarks function effectively. According to our analysis, our benchmark can differentiate benchmark scores among models in all performance ranges by combining tasks with different difficulties.","sentences":["With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity.","There is also a growing need for benchmarks to evaluate the performance of current LLMs in each domain.","Therefore, in this study, we constructed a benchmark comprising multiple tasks specific to the Japanese and financial domains and performed benchmark measurements on some models.","Consequently, we confirmed that GPT-4 is currently outstanding, and that the constructed benchmarks function effectively.","According to our analysis, our benchmark can differentiate benchmark scores among models in all performance ranges by combining tasks with different difficulties."],"url":"http://arxiv.org/abs/2403.15062v1","category":"q-fin.CP"}
{"created":"2024-03-22 08:59:16","title":"Stability of conforming space-time isogeometric methods for the wave equation","abstract":"We consider a family of conforming space-time finite element discretizations for the wave equation based on splines of maximal regularity in time. Traditional techniques may require a CFL condition to guarantee stability. Recent works by O. Steinbach and M. Zank (2018), and S. Fraschini, G. Loli, A. Moiola, and G. Sangalli (2023), have introduced unconditionally stable schemes by adding non-consistent penalty terms to the underlying bilinear form. Stability and error analysis have been carried out for lowest order discrete spaces. While higher order methods have shown promising properties through numerical testing, their rigorous analysis was still missing. In this paper, we address this stability analysis by studying the properties of the condition number of a family of matrices associated with the time discretization. For each spline order, we derive explicit estimates of both the CFL condition required in the unstabilized case and the penalty term that minimises the consistency error in the stabilized case. Numerical tests confirm the sharpness of our results.","sentences":["We consider a family of conforming space-time finite element discretizations for the wave equation based on splines of maximal regularity in time.","Traditional techniques may require a CFL condition to guarantee stability.","Recent works by O. Steinbach and M. Zank (2018), and S. Fraschini, G. Loli, A. Moiola, and G. Sangalli (2023), have introduced unconditionally stable schemes by adding non-consistent penalty terms to the underlying bilinear form.","Stability and error analysis have been carried out for lowest order discrete spaces.","While higher order methods have shown promising properties through numerical testing, their rigorous analysis was still missing.","In this paper, we address this stability analysis by studying the properties of the condition number of a family of matrices associated with the time discretization.","For each spline order, we derive explicit estimates of both the CFL condition required in the unstabilized case and the penalty term that minimises the consistency error in the stabilized case.","Numerical tests confirm the sharpness of our results."],"url":"http://arxiv.org/abs/2403.15043v1","category":"math.NA"}
{"created":"2024-03-22 16:17:55","title":"CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction","abstract":"The process of cyber mapping gives insights in relationships among financial entities and service providers. Centered around the outsourcing practices of companies within fund prospectuses in Germany, we introduce a dataset specifically designed for named entity recognition and relation extraction tasks. The labeling process on 948 sentences was carried out by three experts which yields to 5,969 annotations for four entity types (Outsourcing, Company, Location and Software) and 4,102 relation annotations (Outsourcing-Company, Company-Location). State-of-the-art deep learning models were trained to recognize entities and extract relations showing first promising results. An anonymized version of the dataset, along with guidelines and the code used for model training, are publicly available at https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.","sentences":["The process of cyber mapping gives insights in relationships among financial entities and service providers.","Centered around the outsourcing practices of companies within fund prospectuses in Germany, we introduce a dataset specifically designed for named entity recognition and relation extraction tasks.","The labeling process on 948 sentences was carried out by three experts which yields to 5,969 annotations for four entity types (Outsourcing, Company, Location and Software) and 4,102 relation annotations (Outsourcing-Company, Company-Location).","State-of-the-art deep learning models were trained to recognize entities and extract relations showing first promising results.","An anonymized version of the dataset, along with guidelines and the code used for model training, are publicly available at https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip."],"url":"http://arxiv.org/abs/2403.15322v1","category":"cs.CL"}
{"created":"2024-03-22 15:59:21","title":"Quantum-inspired classification via efficient simulation of Helstrom measurement","abstract":"The Helstrom measurement (HM) is known to be the optimal strategy for distinguishing non-orthogonal quantum states with minimum error. Previously, a binary classifier based on classical simulation of the HM has been proposed. It was observed that using multiple copies of the sample data reduced the classification error. Nevertheless, the exponential growth in simulation runtime hindered a comprehensive investigation of the relationship between the number of copies and classification performance. We present an efficient simulation method for an arbitrary number of copies by utilizing the relationship between HM and state fidelity. Our method reveals that the classification performance does not improve monotonically with the number of data copies. Instead, it needs to be treated as a hyperparameter subject to optimization, achievable only through the method proposed in this work. We present a Quantum-Inspired Machine Learning binary classifier with excellent performance, providing such empirical evidence by benchmarking on eight datasets and comparing it with 13 hyperparameter optimized standard classifiers.","sentences":["The Helstrom measurement (HM) is known to be the optimal strategy for distinguishing non-orthogonal quantum states with minimum error.","Previously, a binary classifier based on classical simulation of the HM has been proposed.","It was observed that using multiple copies of the sample data reduced the classification error.","Nevertheless, the exponential growth in simulation runtime hindered a comprehensive investigation of the relationship between the number of copies and classification performance.","We present an efficient simulation method for an arbitrary number of copies by utilizing the relationship between HM and state fidelity.","Our method reveals that the classification performance does not improve monotonically with the number of data copies.","Instead, it needs to be treated as a hyperparameter subject to optimization, achievable only through the method proposed in this work.","We present a Quantum-Inspired Machine Learning binary classifier with excellent performance, providing such empirical evidence by benchmarking on eight datasets and comparing it with 13 hyperparameter optimized standard classifiers."],"url":"http://arxiv.org/abs/2403.15308v1","category":"quant-ph"}
{"created":"2024-03-22 14:23:21","title":"An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets","abstract":"Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.   Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.","sentences":["Does the training of large language models potentially infringe upon code licenses?","Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses?","In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models.","Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future.","To accomplish this, we compiled a list of 53 large language models trained on file-level code.","We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.   ","Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses.","We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset.","Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license.","Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management."],"url":"http://arxiv.org/abs/2403.15230v1","category":"cs.SE"}
{"created":"2024-03-22 12:42:33","title":"Towards Deep Learning Enabled Cybersecurity Risk Assessment for Microservice Architectures","abstract":"The widespread adoption of microservice architectures has given rise to a new set of software security challenges. These challenges stem from the unique features inherent in microservices. It is important to systematically assess and address software security challenges such as software security risk assessment. However, existing approaches prove inefficient in accurately evaluating the security risks associated with microservice architectures. To address this issue, we propose CyberWise Predictor, a framework designed for predicting and assessing security risks associated with microservice architectures. Our framework employs deep learning-based natural language processing models to analyze vulnerability descriptions for predicting vulnerability metrics to assess security risks. Our experimental evaluation shows the effectiveness of CyberWise Predictor, achieving an average accuracy of 92% in automatically predicting vulnerability metrics for new vulnerabilities. Our framework and findings serve as a guide for software developers to identify and mitigate security risks in microservice architectures.","sentences":["The widespread adoption of microservice architectures has given rise to a new set of software security challenges.","These challenges stem from the unique features inherent in microservices.","It is important to systematically assess and address software security challenges such as software security risk assessment.","However, existing approaches prove inefficient in accurately evaluating the security risks associated with microservice architectures.","To address this issue, we propose CyberWise Predictor, a framework designed for predicting and assessing security risks associated with microservice architectures.","Our framework employs deep learning-based natural language processing models to analyze vulnerability descriptions for predicting vulnerability metrics to assess security risks.","Our experimental evaluation shows the effectiveness of CyberWise Predictor, achieving an average accuracy of 92% in automatically predicting vulnerability metrics for new vulnerabilities.","Our framework and findings serve as a guide for software developers to identify and mitigate security risks in microservice architectures."],"url":"http://arxiv.org/abs/2403.15169v1","category":"cs.SE"}
{"created":"2024-03-22 11:52:19","title":"Oxygenation of CO and NO on Amorphous Solid Water","abstract":"\\noindent   \\textit{Context. }The dynamics for molecule formation, relaxation, diffusion, and desorption on amorphous solid water is studied in a quantitative fashion.   \\noindent   \\textit{Aims. }We aim at characterizing, at a quantitative level, the formation probability, stabilization, energy relaxation and diffusion dynamics of CO$_2$ and NO$_2$ on cold amorphous solid water following atom+diatom recombination reactions.   \\noindent   \\textit{Methods. }Accurate machine-learned energy functions combined with fluctuating charge models were used to investigate the diffusion, interactions, and recombination dynamics of atomic oxygen with CO and NO on amorphous solid water (ASW). Energy relaxation to the ASW and into water-internal-degrees of freedom were determined from analysis of the vibrational density of states. The surface diffusion and desorption energetics was investigated from extended and nonequilibrium MD simulations.   \\noindent   \\textit{Results. }The reaction probability on the nanosecond time scale is determined in a quantitative fashion and demonstrates that surface diffusion of the reactants leads to recombination for initial separations up to 20 \\AA\\/. After recombination both, CO$_2$ and NO$_2$, stabilize by energy transfer to water internal and surface phonon modes on the picosecond time scale. The average diffusion barriers and desorption energies agree with those reported from experiments. After recombination, the triatomic products diffuse easily which contrasts with the equilibrium situation in which both, CO$_2$ and NO$_2$, are stationary on the multi-nanosecond time scale.","sentences":["\\noindent   \\textit{Context.","}The dynamics for molecule formation, relaxation, diffusion, and desorption on amorphous solid water is studied in a quantitative fashion.   ","\\noindent   \\textit{Aims.","}We aim at characterizing, at a quantitative level, the formation probability, stabilization, energy relaxation and diffusion dynamics of CO$_2$ and NO$_2$ on cold amorphous solid water following atom+diatom recombination reactions.   ","\\noindent   \\textit{Methods.","}Accurate machine-learned energy functions combined with fluctuating charge models were used to investigate the diffusion, interactions, and recombination dynamics of atomic oxygen with CO and","NO on amorphous solid water (ASW).","Energy relaxation to the ASW and into water-internal-degrees of freedom were determined from analysis of the vibrational density of states.","The surface diffusion and desorption energetics was investigated from extended and nonequilibrium MD simulations.   ","\\noindent   \\textit{Results.","}The reaction probability on the nanosecond time scale is determined in a quantitative fashion and demonstrates that surface diffusion of the reactants leads to recombination for initial separations up to 20 \\AA\\/. After recombination both, CO$_2$ and NO$_2$, stabilize by energy transfer to water internal and surface phonon modes on the picosecond time scale.","The average diffusion barriers and desorption energies agree with those reported from experiments.","After recombination, the triatomic products diffuse easily which contrasts with the equilibrium situation in which both, CO$_2$ and NO$_2$, are stationary on the multi-nanosecond time scale."],"url":"http://arxiv.org/abs/2403.15141v1","category":"physics.chem-ph"}
{"created":"2024-03-22 11:21:45","title":"Galaxy merger challenge: A comparison study between machine learning-based detection methods","abstract":"Various galaxy merger detection methods have been applied to diverse datasets. However, it is difficult to understand how they compare. We aim to benchmark the relative performance of machine learning (ML) merger detection methods. We explore six leading ML methods using three main datasets. The first one (the training data) consists of mock observations from the IllustrisTNG simulations and allows us to quantify the performance metrics of the detection methods. The second one consists of mock observations from the Horizon-AGN simulations, introduced to evaluate the performance of classifiers trained on different, but comparable data. The third one consists of real observations from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) survey. For the binary classification task (mergers vs. non-mergers), all methods perform reasonably well in the domain of the training data. At $0.1<z<0.3$, precision and recall range between $\\sim$70\\% and 80\\%, both of which decrease with increasing $z$ as expected (by $\\sim$5\\% for precision and $\\sim$10\\% for recall at $0.76<z<1.0$). When transferred to a different domain, the precision of all classifiers is only slightly reduced, but the recall is significantly worse (by $\\sim$20-40\\% depending on the method). Zoobot offers the best overall performance in terms of precision and F1 score. When applied to real HSC observations, all methods agree well with visual labels of clear mergers but can differ by more than an order of magnitude in predicting the overall fraction of major mergers. For the multi-class classification task to distinguish pre-, post- and non-mergers, none of the methods offer a good performance, which could be partly due to limitations in resolution and depth of the data. With the advent of better quality data (e.g. JWST and Euclid), it is important to improve our ability to detect mergers and distinguish between merger stages.","sentences":["Various galaxy merger detection methods have been applied to diverse datasets.","However, it is difficult to understand how they compare.","We aim to benchmark the relative performance of machine learning (ML) merger detection methods.","We explore six leading ML methods using three main datasets.","The first one (the training data) consists of mock observations from the IllustrisTNG simulations and allows us to quantify the performance metrics of the detection methods.","The second one consists of mock observations from the Horizon-AGN simulations, introduced to evaluate the performance of classifiers trained on different, but comparable data.","The third one consists of real observations from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) survey.","For the binary classification task (mergers vs. non-mergers), all methods perform reasonably well in the domain of the training data.","At $0.1<z<0.3$, precision and recall range between $\\sim$70\\% and 80\\%, both of which decrease with increasing $z$ as expected (by $\\sim$5\\% for precision and $\\sim$10\\% for recall at $0.76<z<1.0$).","When transferred to a different domain, the precision of all classifiers is only slightly reduced, but the recall is significantly worse (by $\\sim$20-40\\% depending on the method).","Zoobot offers the best overall performance in terms of precision and F1 score.","When applied to real HSC observations, all methods agree well with visual labels of clear mergers but can differ by more than an order of magnitude in predicting the overall fraction of major mergers.","For the multi-class classification task to distinguish pre-, post- and non-mergers, none of the methods offer a good performance, which could be partly due to limitations in resolution and depth of the data.","With the advent of better quality data (e.g. JWST and Euclid), it is important to improve our ability to detect mergers and distinguish between merger stages."],"url":"http://arxiv.org/abs/2403.15118v1","category":"astro-ph.GA"}
{"created":"2024-03-22 08:42:41","title":"Estimation of multiple mean vectors in high dimension","abstract":"We endeavour to estimate numerous multi-dimensional means of various probability distributions on a common space based on independent samples. Our approach involves forming estimators through convex combinations of empirical means derived from these samples. We introduce two strategies to find appropriate data-dependent convex combination weights: a first one employing a testing procedure to identify neighbouring means with low variance, which results in a closed-form plug-in formula for the weights, and a second one determining weights via minimization of an upper confidence bound on the quadratic risk.Through theoretical analysis, we evaluate the improvement in quadratic risk offered by our methods compared to the empirical means. Our analysis focuses on a dimensional asymptotics perspective, showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases.We demonstrate the efficacy of our methods in estimating multiple kernel mean embeddings through experiments on both simulated and real-world datasets.","sentences":["We endeavour to estimate numerous multi-dimensional means of various probability distributions on a common space based on independent samples.","Our approach involves forming estimators through convex combinations of empirical means derived from these samples.","We introduce two strategies to find appropriate data-dependent convex combination weights: a first one employing a testing procedure to identify neighbouring means with low variance, which results in a closed-form plug-in formula for the weights, and a second one determining weights via minimization of an upper confidence bound on the quadratic risk.","Through theoretical analysis, we evaluate the improvement in quadratic risk offered by our methods compared to the empirical means.","Our analysis focuses on a dimensional asymptotics perspective, showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases.","We demonstrate the efficacy of our methods in estimating multiple kernel mean embeddings through experiments on both simulated and real-world datasets."],"url":"http://arxiv.org/abs/2403.15038v1","category":"stat.ML"}
{"created":"2024-03-22 08:27:25","title":"An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images","abstract":"Open-pit mine change detection (CD) in high-resolution (HR) remote sensing images plays a crucial role in mineral development and environmental protection. Significant progress has been made in this field in recent years, largely due to the advancement of deep learning techniques. However, existing deep-learning-based CD methods encounter challenges in effectively integrating neighborhood and scale information, resulting in suboptimal performance. Therefore, by exploring the influence patterns of neighborhood and scale information, this paper proposes an Integrated Neighborhood and Scale Information Network (INSINet) for open-pit mine CD in HR remote sensing images. Specifically, INSINet introduces 8-neighborhood-image information to acquire a larger receptive field, improving the recognition of center image boundary regions. Drawing on techniques of skip connection, deep supervision, and attention mechanism, the multi-path deep supervised attention (MDSA) module is designed to enhance multi-scale information fusion and change feature extraction. Experimental analysis reveals that incorporating neighborhood and scale information enhances the F1 score of INSINet by 6.40%, with improvements of 3.08% and 3.32% respectively. INSINet outperforms existing methods with an Overall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of 83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing images.","sentences":["Open-pit mine change detection (CD) in high-resolution (HR) remote sensing images plays a crucial role in mineral development and environmental protection.","Significant progress has been made in this field in recent years, largely due to the advancement of deep learning techniques.","However, existing deep-learning-based CD methods encounter challenges in effectively integrating neighborhood and scale information, resulting in suboptimal performance.","Therefore, by exploring the influence patterns of neighborhood and scale information, this paper proposes an Integrated Neighborhood and Scale Information Network (INSINet) for open-pit mine CD in HR remote sensing images.","Specifically, INSINet introduces 8-neighborhood-image information to acquire a larger receptive field, improving the recognition of center image boundary regions.","Drawing on techniques of skip connection, deep supervision, and attention mechanism, the multi-path deep supervised attention (MDSA) module is designed to enhance multi-scale information fusion and change feature extraction.","Experimental analysis reveals that incorporating neighborhood and scale information enhances the F1 score of INSINet by 6.40%, with improvements of 3.08% and 3.32% respectively.","INSINet outperforms existing methods with an Overall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of 83.22%.","INSINet shows significance for open-pit mine CD in HR remote sensing images."],"url":"http://arxiv.org/abs/2403.15032v1","category":"cs.CV"}
{"created":"2024-03-22 08:26:31","title":"Image Classification with Rotation-Invariant Variational Quantum Circuits","abstract":"Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices.","sentences":["Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices.","One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters.","Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning.","In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry.","The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance.","Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices."],"url":"http://arxiv.org/abs/2403.15031v1","category":"quant-ph"}
{"created":"2024-03-22 17:58:33","title":"Time-efficient, high-resolution 3T whole-brain relaxometry using Cartesian 3D MR-STAT with CSF suppression","abstract":"Purpose: Current 3D Magnetic Resonance Spin TomogrAphy in Time-domain (MR-STAT) protocols use transient-state, gradient-spoiled gradient-echo sequences that are prone to cerebrospinal fluid (CSF) pulsation artifacts when applied to the brain. This study aims at developing a 3D MR-STAT protocol for whole-brain relaxometry that overcomes the challenges posed by CSF-induced ghosting artifacts. Method: We optimized the flip-angle train within the Cartesian 3D MR-STAT framework to achieve two objectives: (1) minimization of the noise level in the reconstructed quantitative maps, and (2) reduction of the CSF-to-white-matter signal ratio to suppress CSF signal and the associated pulsation artifacts. The optimized new sequence was tested on a gel/water-phantom to evaluate the accuracy of the quantitative maps, and on healthy volunteers to explore the effectiveness of the CSF artifact suppression and robustness of the new protocol. Results: A new optimized sequence with both high parameter encoding capability and low CSF intensity was proposed and initially validated in the gel/water-phantom experiment. From in-vivo experiments with five volunteers, the proposed CSF-suppressed sequence shows no CSF ghosting artifacts and overall greatly improved image quality for all quantitative maps compared to the baseline sequence. Statistical analysis indicated low inter-subject and inter-scan variability for quantitative parameters in gray matter and white matter (1.6%-2.4% for T1 and 2.0%-4.6% for T2), demonstrating the robustness of the new sequence. Conclusion: We presented a new 3D MR-STAT sequence with CSF suppression that effectively eliminates CSF pulsation artifacts. The new sequence ensures consistently high-quality, 1mm^3 whole-brain relaxometry within a rapid 5.5-minute scan time.","sentences":["Purpose: Current 3D Magnetic Resonance Spin TomogrAphy in Time-domain (MR-STAT) protocols use transient-state, gradient-spoiled gradient-echo sequences that are prone to cerebrospinal fluid (CSF) pulsation artifacts when applied to the brain.","This study aims at developing a 3D MR-STAT protocol for whole-brain relaxometry that overcomes the challenges posed by CSF-induced ghosting artifacts.","Method: We optimized the flip-angle train within the Cartesian 3D MR-STAT framework to achieve two objectives: (1) minimization of the noise level in the reconstructed quantitative maps, and (2) reduction of the CSF-to-white-matter signal ratio to suppress CSF signal and the associated pulsation artifacts.","The optimized new sequence was tested on a gel/water-phantom to evaluate the accuracy of the quantitative maps, and on healthy volunteers to explore the effectiveness of the CSF artifact suppression and robustness of the new protocol.","Results:","A new optimized sequence with both high parameter encoding capability and low CSF intensity was proposed and initially validated in the gel/water-phantom experiment.","From in-vivo experiments with five volunteers, the proposed CSF-suppressed sequence shows no CSF ghosting artifacts and overall greatly improved image quality for all quantitative maps compared to the baseline sequence.","Statistical analysis indicated low inter-subject and inter-scan variability for quantitative parameters in gray matter and white matter (1.6%-2.4% for T1 and 2.0%-4.6% for T2), demonstrating the robustness of the new sequence.","Conclusion: We presented a new 3D MR-STAT sequence with CSF suppression that effectively eliminates CSF pulsation artifacts.","The new sequence ensures consistently high-quality, 1mm^3 whole-brain relaxometry within a rapid 5.5-minute scan time."],"url":"http://arxiv.org/abs/2403.15379v1","category":"physics.med-ph"}
{"created":"2024-03-22 15:58:39","title":"Strategic Network Creation for Enabling Greedy Routing","abstract":"In this paper, we present the first game-theoretic network creation model that incorporates greedy routing, i.e., the agents in our model are embedded in some metric space and strive for creating a network where all-pairs greedy routing is enabled. In contrast to graph-theoretic shortest paths, our agents route their traffic along greedy paths, which are sequences of nodes where the distance in the metric space to the respective target node gets strictly smaller by each hop. Besides enabling greedy routing, the agents also optimize their connection quality within the created network by constructing greedy paths with low stretch. This ensures that greedy routing is always possible in equilibrium networks, while realistically modeling the agents' incentives for local structural changes to the network. With this we augment the elegant network creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with the feature of greedy routing.   For our model, we analyze the existence of (approximate)-equilibria and the computational hardness in different underlying metric spaces. E.g., we characterize the set of equilibria in 1-2-metrics and tree metrics, we show that in both metrics Nash equilibria always exist, and we prove that the well-known $\\Theta$-graph construction yields constant-approximate Nash equilibria in Euclidean space. The latter justifies distributed network construction via $\\Theta$-graphs from a new point-of-view, since it shows that this powerful technique not only guarantees networks having a low stretch but also networks that are almost stable.","sentences":["In this paper, we present the first game-theoretic network creation model that incorporates greedy routing, i.e., the agents in our model are embedded in some metric space and strive for creating a network where all-pairs greedy routing is enabled.","In contrast to graph-theoretic shortest paths, our agents route their traffic along greedy paths, which are sequences of nodes where the distance in the metric space to the respective target node gets strictly smaller by each hop.","Besides enabling greedy routing, the agents also optimize their connection quality within the created network by constructing greedy paths with low stretch.","This ensures that greedy routing is always possible in equilibrium networks, while realistically modeling the agents' incentives for local structural changes to the network.","With this we augment the elegant network creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with the feature of greedy routing.   ","For our model, we analyze the existence of (approximate)-equilibria and the computational hardness in different underlying metric spaces.","E.g., we characterize the set of equilibria in 1-2-metrics and tree metrics, we show that in both metrics Nash equilibria always exist, and we prove that the well-known $\\Theta$-graph construction yields constant-approximate Nash equilibria in Euclidean space.","The latter justifies distributed network construction via $\\Theta$-graphs from a new point-of-view, since it shows that this powerful technique not only guarantees networks having a low stretch but also networks that are almost stable."],"url":"http://arxiv.org/abs/2403.15307v1","category":"cs.GT"}
{"created":"2024-03-22 15:00:14","title":"Compressibility and Stochastic Stability of Monotone Markov Chain","abstract":"For a stochastically monotone Markov chain taking values in a Polish space, we present a number of conditions for existence and for uniqueness of its stationary regime, as well as for closeness of its transient trajectories. In particular, we generalise a basic result by Bhattacharya and Majumdar (2007) where a certain form of mixing, or swap condition was assumed uniformly over the state space. We do not rely on continuity properties of transition probabilities.","sentences":["For a stochastically monotone Markov chain taking values in a Polish space, we present a number of conditions for existence and for uniqueness of its stationary regime, as well as for closeness of its transient trajectories.","In particular, we generalise a basic result by Bhattacharya and Majumdar (2007) where a certain form of mixing, or swap condition was assumed uniformly over the state space.","We do not rely on continuity properties of transition probabilities."],"url":"http://arxiv.org/abs/2403.15259v1","category":"math.PR"}
{"created":"2024-03-22 12:15:22","title":"Near-optimal performance of stochastic economic MPC","abstract":"This paper presents first results for near optimality in expectation of the closed-loop solutions for stochastic economic MPC. The approach relies on a recently developed turnpike property for stochastic optimal control problems at an optimal stationary process, combined with techniques for analyzing time-varying economic MPC schemes. We obtain near optimality in finite time as well as overtaking and average near optimality on infinite time horizons.","sentences":["This paper presents first results for near optimality in expectation of the closed-loop solutions for stochastic economic MPC.","The approach relies on a recently developed turnpike property for stochastic optimal control problems at an optimal stationary process, combined with techniques for analyzing time-varying economic MPC schemes.","We obtain near optimality in finite time as well as overtaking and average near optimality on infinite time horizons."],"url":"http://arxiv.org/abs/2403.15159v1","category":"math.OC"}
{"created":"2024-03-22 12:10:23","title":"Broad Instantaneous Bandwidth Microwave Spectrum Analyzer with a Microfabricated Atomic Vapor Cell","abstract":"We report on broad instantaneous bandwidth microwave spectrum analysis with hot $^{87}\\mathrm{Rb}$ atoms in a microfabricated vapor cell in a large magnetic field gradient. The sensor is a MEMS atomic vapor cell filled with isotopically pure $^{87}\\mathrm{Rb}$ and $\\mathrm{N}_2$ buffer gas to localize the motion of the atoms. The microwave signals of interest are coupled through a coplanar waveguide to the cell, inducing spin flip transitions between optically pumped ground states of the atoms. A static magnetic field with large gradient maps the $\\textit{frequency spectrum}$ of the input microwave signals to a position-dependent $\\textit{spin-flip pattern}$ on absorption images of the cell recorded with a laser beam onto a camera. In our proof-of-principle experiment, we demonstrate a microwave spectrum analyzer that has $\\approx$ 1 GHz instantaneous bandwidth centered at 13.165 GHz, 3 MHz frequency resolution, 2 kHz refresh rate, and a -23 dBm single-tone microwave power detection limit in 1 s measurement time. A theoretical model is constructed to simulate the image signals by considering the processes of optical pumping, microwave interaction, diffusion of $^{87}\\mathrm{Rb}$ atoms, and laser absorption. We expect to reach more than 25 GHz instantaneous bandwidth in an optimized setup, limited by the applied magnetic field gradient. Our demonstration offers a practical alternative to conventional microwave spectrum analyzers based on electronic heterodyne detection.","sentences":["We report on broad instantaneous bandwidth microwave spectrum analysis with hot $^{87}\\mathrm{Rb}$ atoms in a microfabricated vapor cell in a large magnetic field gradient.","The sensor is a MEMS atomic vapor cell filled with isotopically pure $^{87}\\mathrm{Rb}$ and $\\mathrm{N}_2$ buffer gas to localize the motion of the atoms.","The microwave signals of interest are coupled through a coplanar waveguide to the cell, inducing spin flip transitions between optically pumped ground states of the atoms.","A static magnetic field with large gradient maps the $\\textit{frequency spectrum}$ of the input microwave signals to a position-dependent $\\textit{spin-flip pattern}$ on absorption images of the cell recorded with a laser beam onto a camera.","In our proof-of-principle experiment, we demonstrate a microwave spectrum analyzer that has $\\approx$ 1 GHz instantaneous bandwidth centered at 13.165 GHz, 3 MHz frequency resolution, 2 kHz refresh rate, and a -23 dBm single-tone microwave power detection limit in 1 s measurement time.","A theoretical model is constructed to simulate the image signals by considering the processes of optical pumping, microwave interaction, diffusion of $^{87}\\mathrm{Rb}$ atoms, and laser absorption.","We expect to reach more than 25 GHz instantaneous bandwidth in an optimized setup, limited by the applied magnetic field gradient.","Our demonstration offers a practical alternative to conventional microwave spectrum analyzers based on electronic heterodyne detection."],"url":"http://arxiv.org/abs/2403.15155v1","category":"physics.atom-ph"}
{"created":"2024-03-22 10:02:54","title":"Real-time Threat Detection Strategies for Resource-constrained Devices","abstract":"As more devices connect to the internet, it becomes crucial to address their limitations and basic security needs. While much research focuses on utilizing ML and DL to tackle security challenges, there is often a tendency to overlook the practicality and feasibility of implementing these methods in real-time settings. This oversight stems from the constrained processing power and memory of certain devices (IoT devices), as well as concerns about the generalizability of these approaches. Focusing on the detection of DNS-tunneling attacks in a router as a case study, we present an end-to-end process designed to effectively address these challenges. The process spans from developing a lightweight DNS-tunneling detection model to integrating it into a resource-constrained device for real-time detection. Through our experiments, we demonstrate that utilizing stateless features for training the ML model, along with features chosen to be independent of the network configuration, leads to highly accurate results. The deployment of this carefully crafted model, optimized for embedded devices across diverse environments, resulted in high DNS-tunneling attack detection with minimal latency. With this work, we aim to encourage solutions that strike a balance between theoretical advancements and the practical applicability of ML approaches in the ever-evolving landscape of device security.","sentences":["As more devices connect to the internet, it becomes crucial to address their limitations and basic security needs.","While much research focuses on utilizing ML and DL to tackle security challenges, there is often a tendency to overlook the practicality and feasibility of implementing these methods in real-time settings.","This oversight stems from the constrained processing power and memory of certain devices (IoT devices), as well as concerns about the generalizability of these approaches.","Focusing on the detection of DNS-tunneling attacks in a router as a case study, we present an end-to-end process designed to effectively address these challenges.","The process spans from developing a lightweight DNS-tunneling detection model to integrating it into a resource-constrained device for real-time detection.","Through our experiments, we demonstrate that utilizing stateless features for training the ML model, along with features chosen to be independent of the network configuration, leads to highly accurate results.","The deployment of this carefully crafted model, optimized for embedded devices across diverse environments, resulted in high DNS-tunneling attack detection with minimal latency.","With this work, we aim to encourage solutions that strike a balance between theoretical advancements and the practical applicability of ML approaches in the ever-evolving landscape of device security."],"url":"http://arxiv.org/abs/2403.15078v1","category":"cs.CR"}
{"created":"2024-03-22 17:59:41","title":"Magnetic, charge, and bond order in the two-dimensional Su-Schrieffer-Heeger-Holstein model","abstract":"Most nonperturbative numerical studies of electron-phonon interactions focus on model Hamiltonians where the electrons interact with a phonon branch via a single type of microscopic mechanism. Two commonly explored couplings in this context are the Holstein and Su-Schrieffer-Heeger (SSH) interactions, which describe phonons modulating the on-site energy and intersite electron hopping, respectively. Many materials, however, have multiple phonon branches that can each interact with electronic degrees of freedom in different ways. We present here a determinant quantum Monte Carlo study of the half-filled two-dimensional (bond) SSH-Holstein Hamiltonian, where electrons couple to different phonon branches via either the Holstein or SSH mechanism. We map the model's phase diagram and determine the nature of the transitions between charge-density wave, bond order wave, and antiferromagnetic order.","sentences":["Most nonperturbative numerical studies of electron-phonon interactions focus on model Hamiltonians where the electrons interact with a phonon branch via a single type of microscopic mechanism.","Two commonly explored couplings in this context are the Holstein and Su-Schrieffer-Heeger (SSH) interactions, which describe phonons modulating the on-site energy and intersite electron hopping, respectively.","Many materials, however, have multiple phonon branches that can each interact with electronic degrees of freedom in different ways.","We present here a determinant quantum Monte Carlo study of the half-filled two-dimensional (bond) SSH-Holstein Hamiltonian, where electrons couple to different phonon branches via either the Holstein or SSH mechanism.","We map the model's phase diagram and determine the nature of the transitions between charge-density wave, bond order wave, and antiferromagnetic order."],"url":"http://arxiv.org/abs/2403.15386v1","category":"cond-mat.str-el"}
{"created":"2024-03-22 17:09:30","title":"An alternative approach to baryon masses in the $1/N_c$ expansion of QCD","abstract":"The baryon mass operator is studied within a combined expansion in $1/N_c$ and perturbative $SU(3)$ flavor symmetry breaking, where $N_c$ denotes the number of quark charges. Flavor projection operators are used to classify the baryon operators involved in the expansion, which fall into the flavor representations $1$, $8$, $10+\\overline{10}$, $27$, $35+\\overline{35}$ and $64$. This approach allows one to incorporate up to third-order flavor symmetry breaking in the baryon mass operator in a rigorous and systematic way. Previous work on the subject is considered to validate the approach. A fit to data is performed to evaluate the free parameters in the theory and to produce some numerical values of baryon masses. Results are consistent and reaffirm the striking success of the $1/N_c$ expansion.","sentences":["The baryon mass operator is studied within a combined expansion in $1/N_c$ and perturbative $SU(3)$ flavor symmetry breaking, where $N_c$ denotes the number of quark charges.","Flavor projection operators are used to classify the baryon operators involved in the expansion, which fall into the flavor representations $1$, $8$, $10+\\overline{10}$, $27$, $35+\\overline{35}$ and $64$. This approach allows one to incorporate up to third-order flavor symmetry breaking in the baryon mass operator in a rigorous and systematic way.","Previous work on the subject is considered to validate the approach.","A fit to data is performed to evaluate the free parameters in the theory and to produce some numerical values of baryon masses.","Results are consistent and reaffirm the striking success of the $1/N_c$ expansion."],"url":"http://arxiv.org/abs/2403.15354v1","category":"hep-ph"}
{"created":"2024-03-22 16:58:35","title":"Manipulating carbon related spin defects in boron nitride by changing the MOCVD growth temperature","abstract":"A common solution for precise magnetic field sensing is to employ spin-active defects in semiconductors, with the NV center in diamond as prominent example. However, the three-dimensional nature of diamond limits the obtainable proximity of the defect to the sample. Two-dimensional boron nitride, which can host spin-active defects, can be used to overcome those limitations. In this work, we study spin properties of sp2-bonded boron nitride layers grown using Metal Organic Chemical Vapor Deposition at temperatures ranging from 700 {\\deg}C to 1200 {\\deg}C. With Electron Spin Resonance (ESR) we show that our layers exhibit spin properties, which we ascribe to carbon related defects. Supported by photoluminescence and Fourier-transform infrared spectroscopy, we distinguish three different regimes: (i) growth at low temperatures with no ESR signal, (ii) growth at intermediate temperatures with a strong ESR signal and a large number of spin defects, (iii) growth at high temperatures with a weaker ESR signal and a lower number of spin defects. The observed effects can be further enhanced by an additional annealing step. Our studies demonstrate wafer-scale boron nitride that intrinsically hosts spin defects without any ion or neutron irradiation, which may be employed in spin memories or magnetic field detectors.","sentences":["A common solution for precise magnetic field sensing is to employ spin-active defects in semiconductors, with the NV center in diamond as prominent example.","However, the three-dimensional nature of diamond limits the obtainable proximity of the defect to the sample.","Two-dimensional boron nitride, which can host spin-active defects, can be used to overcome those limitations.","In this work, we study spin properties of sp2-bonded boron nitride layers grown using Metal Organic Chemical Vapor Deposition at temperatures ranging from 700 {\\deg}C to 1200 {\\deg}C. With Electron Spin Resonance (ESR) we show that our layers exhibit spin properties, which we ascribe to carbon related defects.","Supported by photoluminescence and Fourier-transform infrared spectroscopy, we distinguish three different regimes: (i) growth at low temperatures with no ESR signal, (ii) growth at intermediate temperatures with a strong ESR signal and a large number of spin defects, (iii) growth at high temperatures with a weaker ESR signal and a lower number of spin defects.","The observed effects can be further enhanced by an additional annealing step.","Our studies demonstrate wafer-scale boron nitride that intrinsically hosts spin defects without any ion or neutron irradiation, which may be employed in spin memories or magnetic field detectors."],"url":"http://arxiv.org/abs/2403.15346v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-22 16:37:19","title":"Search for light long-lived particles in $pp$ collisions at $\\sqrt{s}=13$ TeV using displaced vertices in the ATLAS inner detector","abstract":"A search for long-lived particles (LLPs) using 140 fb$^{-1}$ of $pp$ collision data with $\\sqrt{s} = 13$ TeV recorded by the ATLAS experiment at the LHC is presented. The search targets LLPs with masses between $5$ and $55$ GeV that decay hadronically in the ATLAS inner detector. Benchmark models with LLP pair production from exotic decays of the Higgs boson and models featuring long-lived axion-like particles (ALPs) are considered. No significant excess above the expected background is observed. Upper limits are placed on the branching ratio of the Higgs boson to pairs of LLPs, the cross-section for ALPs produced in association with a vector boson, and, for the first time, on the branching ratio of the top quark to an ALP and a $u/c$ quark.","sentences":["A search for long-lived particles (LLPs) using 140 fb$^{-1}$ of $pp$ collision data with $\\sqrt{s} = 13$ TeV recorded by the ATLAS experiment at the LHC is presented.","The search targets LLPs with masses between $5$ and $55$ GeV that decay hadronically in the ATLAS inner detector.","Benchmark models with LLP pair production from exotic decays of the Higgs boson and models featuring long-lived axion-like particles (ALPs) are considered.","No significant excess above the expected background is observed.","Upper limits are placed on the branching ratio of the Higgs boson to pairs of LLPs, the cross-section for ALPs produced in association with a vector boson, and, for the first time, on the branching ratio of the top quark to an ALP and a $u/c$ quark."],"url":"http://arxiv.org/abs/2403.15332v1","category":"hep-ex"}
{"created":"2024-03-22 16:36:37","title":"Local fraction in Static Causal Orders","abstract":"In this Letter, we introduce a notion of local fraction for experiments taking place against arbitrary static causal backgrounds -- greatly generalising previous results on no-signalling scenarios -- and we explicitly formulate a linear program to compute this quantity. We derive a free characterisation of causal functions which allows us to efficiently construct the matrices required to perform concrete calculations. We demonstrate our techniques by analysing the local fraction of a novel example involving two Bell tests in interleaved causal order.","sentences":["In this Letter, we introduce a notion of local fraction for experiments taking place against arbitrary static causal backgrounds -- greatly generalising previous results on no-signalling scenarios -- and we explicitly formulate a linear program to compute this quantity.","We derive a free characterisation of causal functions which allows us to efficiently construct the matrices required to perform concrete calculations.","We demonstrate our techniques by analysing the local fraction of a novel example involving two Bell tests in interleaved causal order."],"url":"http://arxiv.org/abs/2403.15331v1","category":"quant-ph"}
{"created":"2024-03-22 16:31:49","title":"Marginally deformed AdS$_5$/CFT$_4$ Backgrounds in Type IIB","abstract":"Multi-parameter families of $\\mathcal{N}=0$ Type IIA and Type IIB AdS$_5$ solutions are presented, promoting to $\\mathcal{N}=1$ in some special cases. The G-Structure description of each $\\mathcal{N}=1$ solution is given, requiring an Abelian T-Duality of the G-Structure conditions and Pure Spinors. Investigations at the boundaries are performed for a two-parameter family of Type IIA and a three-parameter family of type IIB solutions, finding the presence of orbifold singularities in some backgrounds. All parameters drop out of the Holographic Central Charge calculation, pointing to marginal deformations in the dual CFT description.","sentences":["Multi-parameter families of $\\mathcal{N}=0$ Type IIA and Type IIB AdS$_5$ solutions are presented, promoting to $\\mathcal{N}=1$ in some special cases.","The G-Structure description of each $\\mathcal{N}=1$ solution is given, requiring an Abelian T-Duality of the G-Structure conditions and Pure Spinors.","Investigations at the boundaries are performed for a two-parameter family of Type IIA and a three-parameter family of type IIB solutions, finding the presence of orbifold singularities in some backgrounds.","All parameters drop out of the Holographic Central Charge calculation, pointing to marginal deformations in the dual CFT description."],"url":"http://arxiv.org/abs/2403.15326v1","category":"hep-th"}
{"created":"2024-03-22 15:17:31","title":"Bell-CHSH inequality and unitary operators","abstract":"Unitary operators are employed to investigate the violation of the Bell-CHSH inequality. The ensuing modifications affecting both classical and quantum bounds are elucidated. The relevance of a particular class of unitary operators whose expectation values are real is pointed out. For these operators, the classical and quantum bounds remain unaltered, being given, respectively, by $2$ and $2\\sqrt{2}$. As an example, the Weyl unitary operators for a real scalar field in relativistic Quantum Field Theory are discussed.","sentences":["Unitary operators are employed to investigate the violation of the Bell-CHSH inequality.","The ensuing modifications affecting both classical and quantum bounds are elucidated.","The relevance of a particular class of unitary operators whose expectation values are real is pointed out.","For these operators, the classical and quantum bounds remain unaltered, being given, respectively, by $2$ and $2\\sqrt{2}$. As an example, the Weyl unitary operators for a real scalar field in relativistic Quantum Field Theory are discussed."],"url":"http://arxiv.org/abs/2403.15276v1","category":"quant-ph"}
{"created":"2024-03-22 15:08:11","title":"Gravitational Wave Sourced by Decay of Massive Particle from Primordial Black Hole evaporation","abstract":"In this article, we investigate the stochastic gravitational waves (GWs) spectrum, resulting from the emission of gravitons through bremsstrahlung, in the decay of particles produced by Hawking radiation. Although particle decays inevitably entail the emission of graviton due to bremsstrahlung, the associated decay width is notably suppressed due to the Planck scale suppression in the coupling of matter fields to gravitons. Consequently, the relic abundance of such GWs constituted of these gravitons undergoes a corresponding reduction. However, we demonstrate that super-heavy particles, reaching masses as high as Planck scale, can emerge naturally in the Hawking radiation of evaporating primordial black holes (PBHs) and can compensate for this suppression. In addition, we also discuss the stochastic gravitational waves constituted out of the gravitons directly radiated from such evaporating PBHs. When the super-heavy particle decays promptly after its production, then the corresponding GW spectrum remains subdominant to the one arising from direct PBH evaporation. However, if this particle is long-lived and decays after PBH evaporation, then the resulting GWs produced in these two processes have two distinct spectra with their peaks at extremely high frequencies, providing avenues for proposed ultra-high frequency gravitational wave detectors. We also show that such gravitational waves contribute significantly to substantial dark radiation, well within the anticipated sensitivity thresholds of future experiments like CMB-S4 and EUCLID.","sentences":["In this article, we investigate the stochastic gravitational waves (GWs) spectrum, resulting from the emission of gravitons through bremsstrahlung, in the decay of particles produced by Hawking radiation.","Although particle decays inevitably entail the emission of graviton due to bremsstrahlung, the associated decay width is notably suppressed due to the Planck scale suppression in the coupling of matter fields to gravitons.","Consequently, the relic abundance of such GWs constituted of these gravitons undergoes a corresponding reduction.","However, we demonstrate that super-heavy particles, reaching masses as high as Planck scale, can emerge naturally in the Hawking radiation of evaporating primordial black holes (PBHs) and can compensate for this suppression.","In addition, we also discuss the stochastic gravitational waves constituted out of the gravitons directly radiated from such evaporating PBHs.","When the super-heavy particle decays promptly after its production, then the corresponding GW spectrum remains subdominant to the one arising from direct PBH evaporation.","However, if this particle is long-lived and decays after PBH evaporation, then the resulting GWs produced in these two processes have two distinct spectra with their peaks at extremely high frequencies, providing avenues for proposed ultra-high frequency gravitational wave detectors.","We also show that such gravitational waves contribute significantly to substantial dark radiation, well within the anticipated sensitivity thresholds of future experiments like CMB-S4 and EUCLID."],"url":"http://arxiv.org/abs/2403.15269v1","category":"hep-ph"}
{"created":"2024-03-22 14:51:25","title":"The cause of the difference in the propagation distances between compact and transient jets in black-hole X-ray binaries","abstract":"Accreting black-hole binaries change their properties during evolution, passing through two main luminous states, dominated by either hard or soft X-rays. In the hard state, steady compact jets emitting multiwavelength radiation are present. Those jets are usually observed in radio, and when resolved, their extent is $\\lesssim\\!10^{15}$ cm. Then, during hard-to-soft transitions, powerful ejecta in the form of blobs appear. They are observed up to distances of $\\sim\\!10^{18}$ cm, which are $\\gtrsim$1000 times larger than the extent of hard-state jets. On the other hand, estimates of the accretion rates during most luminous hard states and the hard-to-soft transitions are very similar, implying that maximum achievable powers of both types of jets are similar and cannot cause the huge difference in their propagation. Instead, we explain the difference in the propagation length by postulating that the ejecta consist of electron-ion plasmas, whereas the hard-state jets consist mostly of electron-positron pairs. The inertia of the ejecta are then much higher than those of compact jets, and the former are not readily stopped by ambient media. A related result is that the accretion flow during the hard state is of Standard and Normal Evolution (SANE), while it is a Magnetically Arrested Disk (MAD) during transient ejections. The pairs in hard-state jets can be produced by collisions of photons of the hard spectrum emitted by hot accretion flows within the jet base. On the other hand, the X-ray spectra during the state transitions are relatively soft and the same process produces much fewer pairs.","sentences":["Accreting black-hole binaries change their properties during evolution, passing through two main luminous states, dominated by either hard or soft X-rays.","In the hard state, steady compact jets emitting multiwavelength radiation are present.","Those jets are usually observed in radio, and when resolved, their extent is $\\lesssim\\!10^{15}$ cm.","Then, during hard-to-soft transitions, powerful ejecta in the form of blobs appear.","They are observed up to distances of $\\sim\\!10^{18}$ cm, which are $\\gtrsim$1000 times larger than the extent of hard-state jets.","On the other hand, estimates of the accretion rates during most luminous hard states and the hard-to-soft transitions are very similar, implying that maximum achievable powers of both types of jets are similar and cannot cause the huge difference in their propagation.","Instead, we explain the difference in the propagation length by postulating that the ejecta consist of electron-ion plasmas, whereas the hard-state jets consist mostly of electron-positron pairs.","The inertia of the ejecta are then much higher than those of compact jets, and the former are not readily stopped by ambient media.","A related result is that the accretion flow during the hard state is of Standard and Normal Evolution (SANE), while it is a Magnetically Arrested Disk (MAD) during transient ejections.","The pairs in hard-state jets can be produced by collisions of photons of the hard spectrum emitted by hot accretion flows within the jet base.","On the other hand, the X-ray spectra during the state transitions are relatively soft and the same process produces much fewer pairs."],"url":"http://arxiv.org/abs/2403.15252v1","category":"astro-ph.HE"}
{"created":"2024-03-22 14:43:47","title":"The NNLO gluon beam function for jet-veto resummation","abstract":"We compute the gluon beam function for jet-veto resummation to next-to-next-to-leading order (NNLO) in the strong-coupling expansion. Our calculation is based on an automated framework that was previously used for the computation of the respective quark beam function, and which we significantly extended for the present calculation. In particular, the perturbative matching kernels are directly calculated in momentum space, without the need to perform an additional Mellin transform. We present results for both gluon and quark-initiated processes, which we cross-checked with an independent semi-analytical method that exploits the similarity of the beam functions to the more familiar case of transverse-momentum resummation. Our computation is relevant for jet-veto resummations at NNLL$'$ accuracy.","sentences":["We compute the gluon beam function for jet-veto resummation to next-to-next-to-leading order (NNLO) in the strong-coupling expansion.","Our calculation is based on an automated framework that was previously used for the computation of the respective quark beam function, and which we significantly extended for the present calculation.","In particular, the perturbative matching kernels are directly calculated in momentum space, without the need to perform an additional Mellin transform.","We present results for both gluon and quark-initiated processes, which we cross-checked with an independent semi-analytical method that exploits the similarity of the beam functions to the more familiar case of transverse-momentum resummation.","Our computation is relevant for jet-veto resummations at NNLL$'$ accuracy."],"url":"http://arxiv.org/abs/2403.15247v1","category":"hep-ph"}
{"created":"2024-03-22 12:50:48","title":"Magnetically arrested disks in FR I radio galaxies","abstract":"A sample of 17 FR I radio galaxies constructed from the 3CR catalog, which is characterized by edge-darkened radio structures, is studied. The optical core luminosities derived from Hubble Space Telescope observation are used to estimate the Eddington ratios which are found to be below $10^{-3.4}$ for this sample. This is supported by the Baldwin-Phillips-Terlevich optical diagnostic diagrams derived with the spectroscopic observation of Telescopio Nazionale Galileo, suggesting that these sources are of low ionization nuclear Emission-line Regions (LINERs). It implies that the accretion in these FR I sources can be modeled as advection-dominated accretion flows (ADAFs). Given the low accretion rate, the predicted jet power with a fast-spinning black hole (BH) $a=0.95$ in the Blandford-Znajek mechanics is lower than the estimated one for almost all the sources in our sample. Such powerful jets indicate the presence of magnetically arrested disks (MAD) in the inner region of the ADAF, in the sense that the magnetic fields in the inner accretion zone are strong. Moreover, we show that, even in the MAD scenario, the BH spins in the sample are most likely moderate and/or fast with $a\\gtrsim0.5$.","sentences":["A sample of 17 FR I radio galaxies constructed from the 3CR catalog, which is characterized by edge-darkened radio structures, is studied.","The optical core luminosities derived from Hubble Space Telescope observation are used to estimate the Eddington ratios which are found to be below $10^{-3.4}$ for this sample.","This is supported by the Baldwin-Phillips-Terlevich optical diagnostic diagrams derived with the spectroscopic observation of Telescopio Nazionale Galileo, suggesting that these sources are of low ionization nuclear Emission-line Regions (LINERs).","It implies that the accretion in these FR I sources can be modeled as advection-dominated accretion flows (ADAFs).","Given the low accretion rate, the predicted jet power with a fast-spinning black hole (BH) $a=0.95$ in the Blandford-Znajek mechanics is lower than the estimated one for almost all the sources in our sample.","Such powerful jets indicate the presence of magnetically arrested disks (MAD) in the inner region of the ADAF, in the sense that the magnetic fields in the inner accretion zone are strong.","Moreover, we show that, even in the MAD scenario, the BH spins in the sample are most likely moderate and/or fast with $a\\gtrsim0.5$."],"url":"http://arxiv.org/abs/2403.15172v1","category":"astro-ph.CO"}
{"created":"2024-03-22 12:41:15","title":"UV- and X-ray-activated broadband NIR garnet-type Ca3Ga2Sn3O12:Fe3+ phosphors with efficient persistent luminescence","abstract":"Near-infrared phosphor-converted light-emitting diodes (NIR pc-LEDs) are compact light sources of great interest for NIR spectroscopy applications. Beyond typical Cr3+-activated NIR-emitting phosphors, there exists a strong demand for Cr3+-free alternatives with high efficiency and broadband emission to rich the landscape of NIR luminescent materials and extend their range of application fields. Here, we report a series of Fe3+-activated Ca3Ga2Sn3O12 garnet-type phosphors exhibiting broadband NIR emission in the 650-1000 nm range attributed to 4T1(G)-->6A1(S) transition, with a maximum at 754 nm and a FWHM of 89 nm upon UV excitation. The spectroscopic results were analyzed according to the Tanabe-Sugano theory from which the crystal field parameter Dq and Racah parameters B and C were obtained for the octahedrally coordinated Fe3+ ion. Notably, the NIR persistent luminescence lasting over 1 h was detected following UV or X-ray irradiation. The possible mechanism involving electron traps was proposed to explain the observed persistent luminescence. Furthermore, a NIR pc-LED was fabricated by coating synthesized phosphor on a UV chip, and its performance was evaluated to assess its potential suitability as a NIR light source. Our discovery of novel type of nontoxic Fe3+-activated broadband NIR luminescence phosphors with efficient NIR persistent luminescence paves the way for discovering Cr3+-free multifunctional NIR luminescence materials, thereby expanding their application possibilities.","sentences":["Near-infrared phosphor-converted light-emitting diodes (NIR pc-LEDs) are compact light sources of great interest for NIR spectroscopy applications.","Beyond typical Cr3+-activated NIR-emitting phosphors, there exists a strong demand for Cr3+-free alternatives with high efficiency and broadband emission to rich the landscape of NIR luminescent materials and extend their range of application fields.","Here, we report a series of Fe3+-activated Ca3Ga2Sn3O12 garnet-type phosphors exhibiting broadband NIR emission in the 650-1000 nm range attributed to 4T1(G)-->6A1(S) transition, with a maximum at 754 nm and a FWHM of 89 nm upon UV excitation.","The spectroscopic results were analyzed according to the Tanabe-Sugano theory from which the crystal field parameter Dq and Racah parameters B and C were obtained for the octahedrally coordinated Fe3+ ion.","Notably, the NIR persistent luminescence lasting over 1 h was detected following UV or X-ray irradiation.","The possible mechanism involving electron traps was proposed to explain the observed persistent luminescence.","Furthermore, a NIR pc-LED was fabricated by coating synthesized phosphor on a UV chip, and its performance was evaluated to assess its potential suitability as a NIR light source.","Our discovery of novel type of nontoxic Fe3+-activated broadband NIR luminescence phosphors with efficient NIR persistent luminescence paves the way for discovering Cr3+-free multifunctional NIR luminescence materials, thereby expanding their application possibilities."],"url":"http://arxiv.org/abs/2403.15168v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-22 10:11:48","title":"Does the Redshift Distribution of Swift Long GRBs Trace the Star-Formation Rate?","abstract":"Gamma-ray bursts (GRBs) are extremely powerful explosions that have been traditionally classified into two categories: long bursts (LGRBs) with an observed duration T90 > 2 s, and short bursts (SGRBs) with an observed duration T90 < 2 s, where T90 is the time interval during which 90% of the fluence is detected. LGRBs are believed to emanate from the core-collapse of massive stars, while SGRBs are believed to result from the merging of two compact objects, like two neutron stars. Because LGRBs are produced by the violent death of massive stars, we expect that their redshift distribution should trace the star-formation rate (SFR). The purpose of our study is to investigate the extent to which the redshift distribution of LGRBs follows and reflects the SFR. We use a sample of 370 LGRBs taken from the Swift catalog, and we investigate different models for the LGRB redshift distribution. We also carry out Monte Carlo simulations to check the consistency of our results. Our results indicate that the SFR can describe the LGRB redshift distribution well for high redshift bursts, but it needs an evolution term to fit the distribution well at low redshift.","sentences":["Gamma-ray bursts (GRBs) are extremely powerful explosions that have been traditionally classified into two categories: long bursts (LGRBs) with an observed duration T90 > 2 s, and short bursts (SGRBs) with an observed duration T90 < 2 s, where T90 is the time interval during which 90% of the fluence is detected.","LGRBs are believed to emanate from the core-collapse of massive stars, while SGRBs are believed to result from the merging of two compact objects, like two neutron stars.","Because LGRBs are produced by the violent death of massive stars, we expect that their redshift distribution should trace the star-formation rate (SFR).","The purpose of our study is to investigate the extent to which the redshift distribution of LGRBs follows and reflects the SFR.","We use a sample of 370 LGRBs taken from the Swift catalog, and we investigate different models for the LGRB redshift distribution.","We also carry out Monte Carlo simulations to check the consistency of our results.","Our results indicate that the SFR can describe the LGRB redshift distribution well for high redshift bursts, but it needs an evolution term to fit the distribution well at low redshift."],"url":"http://arxiv.org/abs/2403.15087v1","category":"astro-ph.HE"}
{"created":"2024-03-22 10:08:44","title":"Including a Luminous Central Remnant in Radiative Transfer Simulations for Type Iax Supernovae","abstract":"Type Iax supernovae (SNe Iax) are proposed to arise from deflagrations of Chandrasekhar mass white dwarfs. Previous deflagration simulations have achieved good agreement with the light curves and spectra of intermediate-luminosity and bright SNe Iax. However, the model light curves decline too quickly after peak, particularly in red optical and NIR bands. Deflagration models with a variety of ignition configurations do not fully unbind the white dwarf, leaving a remnant polluted with $^{56}\\mathrm{Ni}$. Emission from such a remnant may contribute to the luminosity of SNe Iax. Here we investigate the impact of adding a central energy source, assuming instantaneous powering by $^{56}\\mathrm{Ni}$ decay in the remnant, in radiative transfer calculations of deflagration models. Including the remnant contribution improves agreement with the light curves of SNe Iax, particularly due to the slower post-maximum decline of the models. Spectroscopic agreement is also improved, with intermediate-luminosity and faint models showing greatest improvement. We adopt the full remnant $^{56}\\mathrm{Ni}$ mass predicted for bright models, but good agreement with intermediate-luminosity and faint SNe Iax is only possible for remnant $^{56}\\mathrm{Ni}$ masses significantly lower than those predicted. This may indicate that some of the $^{56}\\mathrm{Ni}$ decay energy in the remnant does not contribute to the radiative luminosity but instead drives mass ejection, or that escape of energy from the remnant is significantly delayed. Future work should investigate the structure of remnants predicted by deflagration models and the potential roles of winds and delayed energy escape, as well as extend radiative transfer simulations to late times.","sentences":["Type Iax supernovae (SNe Iax) are proposed to arise from deflagrations of Chandrasekhar mass white dwarfs.","Previous deflagration simulations have achieved good agreement with the light curves and spectra of intermediate-luminosity and bright SNe Iax.","However, the model light curves decline too quickly after peak, particularly in red optical and NIR bands.","Deflagration models with a variety of ignition configurations do not fully unbind the white dwarf, leaving a remnant polluted with $^{56}\\mathrm{Ni}$. Emission from such a remnant may contribute to the luminosity of SNe Iax.","Here we investigate the impact of adding a central energy source, assuming instantaneous powering by $^{56}\\mathrm{Ni}$ decay in the remnant, in radiative transfer calculations of deflagration models.","Including the remnant contribution improves agreement with the light curves of SNe Iax, particularly due to the slower post-maximum decline of the models.","Spectroscopic agreement is also improved, with intermediate-luminosity and faint models showing greatest improvement.","We adopt the full remnant $^{56}\\mathrm{Ni}$ mass predicted for bright models, but good agreement with intermediate-luminosity and faint SNe Iax is only possible for remnant $^{56}\\mathrm{Ni}$ masses significantly lower than those predicted.","This may indicate that some of the $^{56}\\mathrm{Ni}$ decay energy in the remnant does not contribute to the radiative luminosity but instead drives mass ejection, or that escape of energy from the remnant is significantly delayed.","Future work should investigate the structure of remnants predicted by deflagration models and the potential roles of winds and delayed energy escape, as well as extend radiative transfer simulations to late times."],"url":"http://arxiv.org/abs/2403.15084v1","category":"astro-ph.HE"}
{"created":"2024-03-22 09:06:34","title":"Benchmark Lines and Planes for Higgs-to-Higgs Decays in the NMSSM","abstract":"A number of benchmark scenarios for NMSSM Higgs boson searches via Higgs-to-Higgs decays at the LHC have been proposed by the NMSSM Subgroup of the LHC HWG3. Some of them are already in use by the ATLAS and CMS collaborations for the interpretation of their results from Run 2. In this document we summarize the theory setup, the underlying procedures and reproduce the benchmark scenarios in table form.","sentences":["A number of benchmark scenarios for NMSSM Higgs boson searches via Higgs-to-Higgs decays at the LHC have been proposed by the NMSSM Subgroup of the LHC HWG3.","Some of them are already in use by the ATLAS and CMS collaborations for the interpretation of their results from Run 2.","In this document we summarize the theory setup, the underlying procedures and reproduce the benchmark scenarios in table form."],"url":"http://arxiv.org/abs/2403.15046v1","category":"hep-ph"}
{"created":"2024-03-22 08:41:33","title":"Super-Planckian radiative heat transfer between coplanar two-dimensional metals","abstract":"We use the nonequilibrium Green's function formalism to investigate the radiative heat transfer (RHT) between coplanar two-dimensional (2D) metals via a tight-binding square lattice model and the Drude model. Our results reveal that the RHT between coplanar 2D metals is significantly larger than black-body radiation in both the near and far fields, leading to a global super-Planckian RHT. As the separation distance increases, the heat flux density exhibits a rapid decrease in the near field, followed by a slower decrease and eventual $1/d$ dependence in the far field, while maintaining a much higher magnitude than black-body radiation. Evanescent waves dominate the heat transfer in the near field, while propagating waves dominate the far field. Surprisingly, the propagating heat flux remains almost constant over a wide range of distances, resulting in a super-Planckian behavior in the far field. The dispersion relation of the spectrum function reveals distinct contributions from propagating and evanescent waves, with possible origins from surface plasmon resonance. These findings provide insights into the unique characteristics of RHT between coplanar 2D metals and highlight the potential for achieving enhanced heat transfer beyond the black-body limit, with implications for thermal management and energy conversion applications.","sentences":["We use the nonequilibrium Green's function formalism to investigate the radiative heat transfer (RHT) between coplanar two-dimensional (2D) metals via a tight-binding square lattice model and the Drude model.","Our results reveal that the RHT between coplanar 2D metals is significantly larger than black-body radiation in both the near and far fields, leading to a global super-Planckian RHT.","As the separation distance increases, the heat flux density exhibits a rapid decrease in the near field, followed by a slower decrease and eventual $1/d$ dependence in the far field, while maintaining a much higher magnitude than black-body radiation.","Evanescent waves dominate the heat transfer in the near field, while propagating waves dominate the far field.","Surprisingly, the propagating heat flux remains almost constant over a wide range of distances, resulting in a super-Planckian behavior in the far field.","The dispersion relation of the spectrum function reveals distinct contributions from propagating and evanescent waves, with possible origins from surface plasmon resonance.","These findings provide insights into the unique characteristics of RHT between coplanar 2D metals and highlight the potential for achieving enhanced heat transfer beyond the black-body limit, with implications for thermal management and energy conversion applications."],"url":"http://arxiv.org/abs/2403.15036v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-22 08:36:53","title":"Millimeter and submillimeter spectroscopy of the deuterated molecular ion SD+","abstract":"Seven rotational and fine-structure transitions of the deuterated molecular ion SD+ in the X 3S- ground electronic state have been measured in the 271-863 GHz region in the laboratory. This ion has been produced by DC-glow discharge using a mixture of D2S and argon in a free space cell in a temperature range of -140 to -160C. The rotational, centrifugal distortion, spin-spin interaction, and hyperfine constants have been determined; the standard deviation of the residuals in the fitting is 109 kHz. The set of obtained spectroscopic parameters provides a list of accurate sub-millimeter rest frequencies of SD+ for astronomical detection. We have investigated lines of SD+ toward the quasar PKS 1830-211 using the ALMA archive, as the z = 0.89 molecular absorber exists in front of this quasar. A data set covering the 297 GHz region includes the N_J = 2_3-1_2 transition at 561 GHz due to redshift, providing an upper limit of the column density Ntot = 3 x 10^12 cm-2 for SD+.","sentences":["Seven rotational and fine-structure transitions of the deuterated molecular ion SD+ in the X 3S- ground electronic state have been measured in the 271-863 GHz region in the laboratory.","This ion has been produced by DC-glow discharge using a mixture of D2S and argon in a free space cell in a temperature range of -140 to -160C.","The rotational, centrifugal distortion, spin-spin interaction, and hyperfine constants have been determined; the standard deviation of the residuals in the fitting is 109 kHz.","The set of obtained spectroscopic parameters provides a list of accurate sub-millimeter rest frequencies of SD+ for astronomical detection.","We have investigated lines of SD+ toward the quasar PKS 1830-211 using the ALMA archive, as the z = 0.89 molecular absorber exists in front of this quasar.","A data set covering the 297 GHz region includes the N_J = 2_3-1_2 transition at 561 GHz due to redshift, providing an upper limit of the column density Ntot = 3 x 10^12 cm-2 for SD+."],"url":"http://arxiv.org/abs/2403.15035v1","category":"astro-ph.GA"}
