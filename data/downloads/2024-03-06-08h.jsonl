{"created":"2024-03-05 18:59:47","title":"LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits","abstract":"This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\\beta \\in (0, \\infty]$. We then show that the algorithm's regret satisfies $O\\left(\\left\\{\\log(T)\\right\\}^{\\frac{1+\\beta}{2+\\beta}}T^{\\frac{1}{2+\\beta}}\\right)$. Here, $\\beta= \\infty$ corresponds to the case in the existing studies where a lower bound exists in the suboptimality gap, and our regret satisfies $O(\\log(T))$ in that case. Our proposed algorithm is based on the Follow-The-Regularized-Leader with the Tsallis entropy and referred to as the $\\alpha$-Linear-Contextual (LC)-Tsallis-INF.","sentences":["This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts.","In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\\sqrt{T})$ in an adversarial regime.","However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed.","For this issue, this study proposes an algorithm whose regret satisfies $O(\\log(T))$ in the setting when the suboptimality gap is lower-bounded.","Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap.","That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\\beta \\in (0, \\infty]$. We then show that the algorithm's regret satisfies $O\\left(\\left\\{\\log(T)\\right\\}^{\\frac{1+\\beta}{2+\\beta}}T^{\\frac{1}{2+\\beta}}\\right)$. Here, $\\beta= \\infty$ corresponds to the case in the existing studies where a lower bound exists in the suboptimality gap, and our regret satisfies $O(\\log(T))$ in that case.","Our proposed algorithm is based on the Follow-The-Regularized-Leader with the Tsallis entropy and referred to as the $\\alpha$-Linear-Contextual (LC)-Tsallis-INF."],"url":"http://arxiv.org/abs/2403.03219v1","category":"cs.LG"}
{"created":"2024-03-05 18:59:35","title":"The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning","abstract":"The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations. CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai","sentences":["The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons.","To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs.","However, current evaluations are private, preventing further research into mitigating risk.","Furthermore, they focus on only a few, highly specific pathways for malicious use.","To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security.","WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release.","WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge.","To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations.","CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs.","We release our benchmark and code publicly at https://wmdp.ai"],"url":"http://arxiv.org/abs/2403.03218v1","category":"cs.LG"}
{"created":"2024-03-05 18:58:55","title":"Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion","abstract":"3D patient body modeling is critical to the success of automated patient positioning for smart medical scanning and operating rooms. Existing CNN-based end-to-end patient modeling solutions typically require a) customized network designs demanding large amount of relevant training data, covering extensive realistic clinical scenarios (e.g., patient covered by sheets), which leads to suboptimal generalizability in practical deployment, b) expensive 3D human model annotations, i.e., requiring huge amount of manual effort, resulting in systems that scale poorly. To address these issues, we propose a generic modularized 3D patient modeling method consists of (a) a multi-modal keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy occlusions); and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment. We demonstrate the efficacy of the proposed method by extensive patient positioning experiments on both public and clinical data. Our evaluation results achieve superior patient positioning performance across various imaging modalities in real clinical scenarios.","sentences":["3D patient body modeling is critical to the success of automated patient positioning for smart medical scanning and operating rooms.","Existing CNN-based end-to-end patient modeling solutions typically require a) customized network designs demanding large amount of relevant training data, covering extensive realistic clinical scenarios (e.g., patient covered by sheets), which leads to suboptimal generalizability in practical deployment, b) expensive 3D human model annotations, i.e., requiring huge amount of manual effort, resulting in systems that scale poorly.","To address these issues, we propose a generic modularized 3D patient modeling method consists of (a) a multi-modal keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy occlusions); and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment.","We demonstrate the efficacy of the proposed method by extensive patient positioning experiments on both public and clinical data.","Our evaluation results achieve superior patient positioning performance across various imaging modalities in real clinical scenarios."],"url":"http://arxiv.org/abs/2403.03217v1","category":"cs.CV"}
{"created":"2024-03-05 18:58:39","title":"A Safety-Critical Framework for UGVs in Complex Environments: A Data-Driven Discrepancy-Aware Approach","abstract":"This work presents a novel data-driven multi-layered planning and control framework for the safe navigation of a class of unmanned ground vehicles (UGVs) in the presence of unknown stationary obstacles and additive modeling uncertainties. The foundation of this framework is a novel robust model predictive planner, designed to generate optimal collision-free trajectories given an occupancy grid map, and a paired ancillary controller, augmented to provide robustness against model uncertainties extracted from learning data.   To tackle modeling discrepancies, we identify both matched (input discrepancies) and unmatched model residuals between the true and the nominal reduced-order models using closed-loop tracking errors as training data. Utilizing conformal prediction, we extract probabilistic upper bounds for the unknown model residuals, which serve to construct a robustifying ancillary controller. Further, we also determine maximum tracking discrepancies, also known as the robust control invariance tube, under the augmented policy, formulating them as collision buffers. Employing a LiDAR-based occupancy map to characterize the environment, we construct a discrepancy-aware cost map that incorporates these collision buffers. This map is then integrated into a sampling-based model predictive path planner that generates optimal and safe trajectories that can be robustly tracked by the augmented ancillary controller in the presence of model mismatches.   The effectiveness of the framework is experimentally validated for autonomous high-speed trajectory tracking in a cluttered environment with four different vehicle-terrain configurations. We also showcase the framework's versatility by reformulating it as a driver-assist program, providing collision avoidance corrections based on user joystick commands.","sentences":["This work presents a novel data-driven multi-layered planning and control framework for the safe navigation of a class of unmanned ground vehicles (UGVs) in the presence of unknown stationary obstacles and additive modeling uncertainties.","The foundation of this framework is a novel robust model predictive planner, designed to generate optimal collision-free trajectories given an occupancy grid map, and a paired ancillary controller, augmented to provide robustness against model uncertainties extracted from learning data.   ","To tackle modeling discrepancies, we identify both matched (input discrepancies) and unmatched model residuals between the true and the nominal reduced-order models using closed-loop tracking errors as training data.","Utilizing conformal prediction, we extract probabilistic upper bounds for the unknown model residuals, which serve to construct a robustifying ancillary controller.","Further, we also determine maximum tracking discrepancies, also known as the robust control invariance tube, under the augmented policy, formulating them as collision buffers.","Employing a LiDAR-based occupancy map to characterize the environment, we construct a discrepancy-aware cost map that incorporates these collision buffers.","This map is then integrated into a sampling-based model predictive path planner that generates optimal and safe trajectories that can be robustly tracked by the augmented ancillary controller in the presence of model mismatches.   ","The effectiveness of the framework is experimentally validated for autonomous high-speed trajectory tracking in a cluttered environment with four different vehicle-terrain configurations.","We also showcase the framework's versatility by reformulating it as a driver-assist program, providing collision avoidance corrections based on user joystick commands."],"url":"http://arxiv.org/abs/2403.03215v1","category":"cs.RO"}
{"created":"2024-03-05 18:58:09","title":"On the computation of lattice sums without translational invariance","abstract":"This work introduces a framework for the efficient computation of oscillatory multidimensional lattice sums in geometries with boundaries, a problem intersecting pure and applied mathematics with immediate applications in condensed matter physics and topological quantum physics. The challenge in evaluating the arising sums results from the combination of singular long-range interactions with the loss of translational invariance caused by the boundaries, rendering standard tools ineffective. Our work shows that these lattice sums can be generated from a generalization of the Riemann zeta function to multidimensional non-periodic lattice sums. We put forth a new representation of this zeta function together with a numerical algorithm that ensures super-exponential convergence across an extensive range of geometries. Notably, our method's runtime is influenced only by the complexity of the considered geometries and not by the sheer number of particles, providing the foundation for efficient simulations of macroscopic condensed matter systems. We showcase the practical utility of our method by computing interaction energies in a three-dimensional crystal structure with $3\\times 10^{23}$ particles. Our method's accuracy is thoroughly assessed through a detailed error analysis that both uses analytical results and numerical experiments. A reference implementation is provided online along with the article","sentences":["This work introduces a framework for the efficient computation of oscillatory multidimensional lattice sums in geometries with boundaries, a problem intersecting pure and applied mathematics with immediate applications in condensed matter physics and topological quantum physics.","The challenge in evaluating the arising sums results from the combination of singular long-range interactions with the loss of translational invariance caused by the boundaries, rendering standard tools ineffective.","Our work shows that these lattice sums can be generated from a generalization of the Riemann zeta function to multidimensional non-periodic lattice sums.","We put forth a new representation of this zeta function together with a numerical algorithm that ensures super-exponential convergence across an extensive range of geometries.","Notably, our method's runtime is influenced only by the complexity of the considered geometries and not by the sheer number of particles, providing the foundation for efficient simulations of macroscopic condensed matter systems.","We showcase the practical utility of our method by computing interaction energies in a three-dimensional crystal structure with $3\\times 10^{23}$ particles.","Our method's accuracy is thoroughly assessed through a detailed error analysis that both uses analytical results and numerical experiments.","A reference implementation is provided online along with the article"],"url":"http://arxiv.org/abs/2403.03213v1","category":"math.NA"}
{"created":"2024-03-05 18:46:00","title":"Thomas K. Gaisser, a Pioneer of Particle Astrophysics","abstract":"We describe the pioneering contributions of Thomas K. Gaisser to the birth and development of particle astrophysics, a new field of research at the intersection of cosmic ray physics, astronomy, astrophysics, and particle physics that has emerged in the last few decades. We will especially focus on his studies of natural beams of neutrinos: those generated by the interactions of cosmic rays in the Earth's atmosphere and those emitted by astrophysical sources. Tom actively participated in the discovery of these cosmic neutrinos as well. His contributions also extend to gamma-ray astronomy, the study of the cosmic ray spectra and composition, and the modeling of cosmic ray interactions in the atmosphere and in astrophysical environments. Tom invariably focused his research on the theoretical and phenomenological problems of greatest interest at the time, producing frameworks that transparently interpreted often complex data. These studies have been very influential and have shaped the development of the field.","sentences":["We describe the pioneering contributions of Thomas K. Gaisser to the birth and development of particle astrophysics, a new field of research at the intersection of cosmic ray physics, astronomy, astrophysics, and particle physics that has emerged in the last few decades.","We will especially focus on his studies of natural beams of neutrinos: those generated by the interactions of cosmic rays in the Earth's atmosphere and those emitted by astrophysical sources.","Tom actively participated in the discovery of these cosmic neutrinos as well.","His contributions also extend to gamma-ray astronomy, the study of the cosmic ray spectra and composition, and the modeling of cosmic ray interactions in the atmosphere and in astrophysical environments.","Tom invariably focused his research on the theoretical and phenomenological problems of greatest interest at the time, producing frameworks that transparently interpreted often complex data.","These studies have been very influential and have shaped the development of the field."],"url":"http://arxiv.org/abs/2403.03207v1","category":"hep-ex"}
{"created":"2024-03-05 18:45:39","title":"Scaling Rectified Flow Transformers for High-Resolution Image Synthesis","abstract":"Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.","sentences":["Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos.","Rectified flow is a recent generative model formulation that connects data and noise in a straight line.","Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice.","In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales.","Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis.","Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings.","We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations.","Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."],"url":"http://arxiv.org/abs/2403.03206v1","category":"cs.CV"}
{"created":"2024-03-05 18:41:48","title":"Non-Gaussian two mode squeezed thermal states in continuous variable quantum teleportation","abstract":"While photon catalyzed two mode squeezed vacuum state has been considered in context of quantum teleportation, similar studies have not been yet conducted for photon catalyzed two-mode squeezed thermal (TMST) state. This can be attributed to challenges involved in the evaluation of teleportation fidelity for photon catalyzed TMST state. In this article, we consider a practical scheme for the implementation of non-Gaussian operation, viz., photon subtraction, photon addition, and photon catalysis, on TMST state. The generated states are employed as resources in continuous-variable quantum teleportation. The results show that the three non-Gaussian operations can enhance the teleportation fidelity. Considering the success probability of the non-Gaussian operations, we identify single-photon catalysis and single photon subtraction to be optimal for teleporting input coherent states, at low and intermediate squeezing levels.","sentences":["While photon catalyzed two mode squeezed vacuum state has been considered in context of quantum teleportation, similar studies have not been yet conducted for photon catalyzed two-mode squeezed thermal (TMST) state.","This can be attributed to challenges involved in the evaluation of teleportation fidelity for photon catalyzed TMST state.","In this article, we consider a practical scheme for the implementation of non-Gaussian operation, viz., photon subtraction, photon addition, and photon catalysis, on TMST state.","The generated states are employed as resources in continuous-variable quantum teleportation.","The results show that the three non-Gaussian operations can enhance the teleportation fidelity.","Considering the success probability of the non-Gaussian operations, we identify single-photon catalysis and single photon subtraction to be optimal for teleporting input coherent states, at low and intermediate squeezing levels."],"url":"http://arxiv.org/abs/2403.03204v1","category":"quant-ph"}
{"created":"2024-03-05 18:41:37","title":"CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments","abstract":"The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC ascertains the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC.","sentences":["The integration of learning and reasoning is high on the research agenda in AI.","Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene.","Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones).","Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific.","We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints.","In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene.","For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed.","Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC ascertains the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial.","Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC."],"url":"http://arxiv.org/abs/2403.03203v1","category":"cs.AI"}
{"created":"2024-03-05 18:37:37","title":"Operator Learning Renormalization Group","abstract":"In this paper, we present a general framework for quantum many-body simulations called the operator learning renormalization group (OLRG). Inspired by machine learning perspectives, OLRG is a generalization of Wilson's numerical renormalization group and White's density matrix renormalization group, which recursively builds a simulatable system to approximate a target system of the same number of sites via operator maps. OLRG uses a loss function to minimize the error of a target property directly by learning the operator map in lieu of a state ansatz. This loss function is designed by a scaling consistency condition that also provides a provable bound for real-time evolution. We implement two versions of the operator maps for classical and quantum simulations. The former, which we call the Operator Matrix Map, can be implemented via neural networks on classical computers. The latter, which we call the Hamiltonian Expression Map, generates device pulse sequences to leverage the capabilities of quantum computing hardware. We illustrate the performance of both maps for calculating time-dependent quantities in the quantum Ising model Hamiltonian.","sentences":["In this paper, we present a general framework for quantum many-body simulations called the operator learning renormalization group (OLRG).","Inspired by machine learning perspectives, OLRG is a generalization of Wilson's numerical renormalization group and White's density matrix renormalization group, which recursively builds a simulatable system to approximate a target system of the same number of sites via operator maps.","OLRG uses a loss function to minimize the error of a target property directly by learning the operator map in lieu of a state ansatz.","This loss function is designed by a scaling consistency condition that also provides a provable bound for real-time evolution.","We implement two versions of the operator maps for classical and quantum simulations.","The former, which we call the Operator Matrix Map, can be implemented via neural networks on classical computers.","The latter, which we call the Hamiltonian Expression Map, generates device pulse sequences to leverage the capabilities of quantum computing hardware.","We illustrate the performance of both maps for calculating time-dependent quantities in the quantum Ising model Hamiltonian."],"url":"http://arxiv.org/abs/2403.03199v1","category":"quant-ph"}
{"created":"2024-03-05 18:31:28","title":"MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets","abstract":"Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative \\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.","sentences":["Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs.","Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints.","In this work, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative \\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images.","Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text.","Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues.","We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation.","Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small."],"url":"http://arxiv.org/abs/2403.03194v1","category":"cs.CL"}
{"created":"2024-03-05 18:30:29","title":"VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints","abstract":"The task of SQL query equivalence checking is important in various real-world applications (including query rewriting and automated grading) that involve complex queries with integrity constraints; yet, state-of-the-art techniques are very limited in their capability of reasoning about complex features (e.g., those that involve sorting, case statement, rich integrity constraints, etc.) in real-life queries. To the best of our knowledge, we propose the first SMT-based approach and its implementation, VeriEQL, capable of proving and disproving bounded equivalence of complex SQL queries. VeriEQL is based on a new logical encoding that models query semantics over symbolic tuples using the theory of integers with uninterpreted functions. It is simple yet highly practical -- our comprehensive evaluation on over 20,000 benchmarks shows that VeriEQL outperforms all state-of-the-art techniques by more than one order of magnitude in terms of the number of benchmarks that can be proved or disproved. VeriEQL can also generate counterexamples that facilitate many downstream tasks (such as finding serious bugs in systems like MySQL and Apache Calcite).","sentences":["The task of SQL query equivalence checking is important in various real-world applications (including query rewriting and automated grading) that involve complex queries with integrity constraints; yet, state-of-the-art techniques are very limited in their capability of reasoning about complex features (e.g., those that involve sorting, case statement, rich integrity constraints, etc.) in real-life queries.","To the best of our knowledge, we propose the first SMT-based approach and its implementation, VeriEQL, capable of proving and disproving bounded equivalence of complex SQL queries.","VeriEQL is based on a new logical encoding that models query semantics over symbolic tuples using the theory of integers with uninterpreted functions.","It is simple yet highly practical -- our comprehensive evaluation on over 20,000 benchmarks shows that VeriEQL outperforms all state-of-the-art techniques by more than one order of magnitude in terms of the number of benchmarks that can be proved or disproved.","VeriEQL can also generate counterexamples that facilitate many downstream tasks (such as finding serious bugs in systems like MySQL and Apache Calcite)."],"url":"http://arxiv.org/abs/2403.03193v1","category":"cs.PL"}
{"created":"2024-03-05 18:29:26","title":"Generic models for genus 2 curves with real multiplication","abstract":"Explicit models of families of genus 2 curves with multiplication by $\\sqrt D$ are known for $D= 2, 3, 5$. We obtain generic models for genus 2 curves over $\\mathbb Q$ with real multiplication in 12 new cases, including all fundamental discriminants $D < 40$. A key step in our proof is to develop an algorithm for minimisation of conic bundles fibred over $\\mathbb{P}^2$. We apply this algorithm to simplify the equations for the Mestre conic associated to the generic point on the Hilbert modular surface of fundamental discriminant $D < 100$ computed by Elkies--Kumar.","sentences":["Explicit models of families of genus 2 curves with multiplication by $\\sqrt D$ are known for $D= 2, 3, 5$. We obtain generic models for genus 2 curves over $\\mathbb Q$ with real multiplication in 12 new cases, including all fundamental discriminants $D < 40$.","A key step in our proof is to develop an algorithm for minimisation of conic bundles fibred over $\\mathbb{P}^2$. We apply this algorithm to simplify the equations for the Mestre conic associated to the generic point on the Hilbert modular surface of fundamental discriminant $D < 100$ computed by Elkies--Kumar."],"url":"http://arxiv.org/abs/2403.03191v1","category":"math.NT"}
{"created":"2024-03-05 18:29:17","title":"Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process","abstract":"Abstract reasoning problems pose significant challenges to artificial intelligence algorithms, demanding cognitive capabilities beyond those required for perception tasks. This study introduces the Triple-CFN approach to tackle the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly reorganizing the concept space of conflicting instances. Additionally, the Triple-CFN paradigm proves effective for the RPM problem with necessary modifications, yielding competitive results. To further enhance performance on the RPM issue, we develop the Meta Triple-CFN network, which explicitly structures the problem space while maintaining interpretability on progressive patterns. The success of Meta Triple-CFN is attributed to its paradigm of modeling the conceptual space, equivalent to normalizing reasoning information. Based on this ideology, we introduce the Re-space layer, enhancing the performance of both Meta Triple-CFN and Triple-CFN. This paper aims to contribute to advancements in machine intelligence by exploring innovative network designs for addressing abstract reasoning problems, paving the way for further breakthroughs in this domain.","sentences":["Abstract reasoning problems pose significant challenges to artificial intelligence algorithms, demanding cognitive capabilities beyond those required for perception tasks.","This study introduces the Triple-CFN approach to tackle the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly reorganizing the concept space of conflicting instances.","Additionally, the Triple-CFN paradigm proves effective for the RPM problem with necessary modifications, yielding competitive results.","To further enhance performance on the RPM issue, we develop the Meta Triple-CFN network, which explicitly structures the problem space while maintaining interpretability on progressive patterns.","The success of Meta Triple-CFN is attributed to its paradigm of modeling the conceptual space, equivalent to normalizing reasoning information.","Based on this ideology, we introduce the Re-space layer, enhancing the performance of both Meta Triple-CFN and Triple-CFN.","This paper aims to contribute to advancements in machine intelligence by exploring innovative network designs for addressing abstract reasoning problems, paving the way for further breakthroughs in this domain."],"url":"http://arxiv.org/abs/2403.03190v1","category":"cs.CV"}
{"created":"2024-03-05 18:24:52","title":"Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement","abstract":"Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses. However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making. Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies. And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks. To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model. This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge. The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to provide immediate flood alerts and respond to various flood-related inquiries. Our developed prototype integrates real-time flood warnings with flood maps and social vulnerability data. It also effectively translates complex flood zone information into actionable risk management advice. To assess its performance, we evaluated the prototype using six criteria within three main categories: relevance, error resilience, and understanding of context. Our research marks a significant step towards a more accessible and user-friendly approach in flood risk management. This study highlights the potential of advanced AI tools like GPT-4 in democratizing information and enhancing public engagement in critical social and environmental issues.","sentences":["Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses.","However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making.","Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies.","And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks.","To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model.","This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge.","The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to provide immediate flood alerts and respond to various flood-related inquiries.","Our developed prototype integrates real-time flood warnings with flood maps and social vulnerability data.","It also effectively translates complex flood zone information into actionable risk management advice.","To assess its performance, we evaluated the prototype using six criteria within three main categories: relevance, error resilience, and understanding of context.","Our research marks a significant step towards a more accessible and user-friendly approach in flood risk management.","This study highlights the potential of advanced AI tools like GPT-4 in democratizing information and enhancing public engagement in critical social and environmental issues."],"url":"http://arxiv.org/abs/2403.03188v1","category":"cs.AI"}
{"created":"2024-03-05 18:22:33","title":"Reliable, Adaptable, and Attributable Language Models with Retrieval","abstract":"Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.","sentences":["Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability.","However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability.","In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs.","By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable.","Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling.","To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs.","This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference."],"url":"http://arxiv.org/abs/2403.03187v1","category":"cs.CL"}
{"created":"2024-03-05 18:22:29","title":"Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study","abstract":"Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios. However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks. To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target. Our agent can follow the main storyline and finish real missions in this complex AAA game, with minimal reliance on prior knowledge and application-specific resources. The project website is at https://baai-agents.github.io/Cradle/.","sentences":["Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios.","However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources.","In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction.","To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks.","To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target.","Our agent can follow the main storyline and finish real missions in this complex AAA game, with minimal reliance on prior knowledge and application-specific resources.","The project website is at https://baai-agents.github.io/Cradle/."],"url":"http://arxiv.org/abs/2403.03186v1","category":"cs.AI"}
{"created":"2024-03-05 18:22:15","title":"Preventing Reward Hacking with Occupancy Measure Regularization","abstract":"Reward hacking occurs when an agent performs very well with respect to a \"proxy\" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a \"safe\" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM). Thus, we propose regularizing based on the OM divergence between policies instead of AD divergence to prevent reward hacking. We theoretically establish that OM regularization can more effectively avoid large drops in true reward. Then, we empirically demonstrate in a variety of realistic environments that OM divergence is superior to AD divergence for preventing reward hacking by regularizing towards a safe policy. Furthermore, we show that occupancy measure divergence can also regularize learned policies away from reward hacking behavior. Our code and data are available at https://github.com/cassidylaidlaw/orpo","sentences":["Reward hacking occurs when an agent performs very well with respect to a \"proxy\" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward.","Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively.","Prior work has particularly focused on enforcing the learned policy to behave similarly to a \"safe\" policy by penalizing the KL divergence between their action distributions (AD).","However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity.","Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM).","Thus, we propose regularizing based on the OM divergence between policies instead of AD divergence to prevent reward hacking.","We theoretically establish that OM regularization can more effectively avoid large drops in true reward.","Then, we empirically demonstrate in a variety of realistic environments that OM divergence is superior to AD divergence for preventing reward hacking by regularizing towards a safe policy.","Furthermore, we show that occupancy measure divergence can also regularize learned policies away from reward hacking behavior.","Our code and data are available at https://github.com/cassidylaidlaw/orpo"],"url":"http://arxiv.org/abs/2403.03185v1","category":"cs.LG"}
{"created":"2024-03-05 18:20:59","title":"Realistic photon-number resolution in Gaussian Boson Sampling","abstract":"Gaussian Boson Sampling (GBS) is the model of non-universal quantum computation that has already demonstrated quantum supremacy in experiments. This model entails sampling photocounting events from a multimode Gaussian state at the outputs of a linear interferometer. In this scheme, collision events -- those with more than one photon for each mode -- are infrequent. However, they are still used for validation purposes. Therefore, the limitation of realistic detectors to perfectly resolve adjacent photon numbers becomes pivotal. We have derived the photocounting probability distribution in the GBS schemes, which is applicable for use with general detectors and photocounting techniques. This probability distribution is expressed in terms of functionals of the field-quadrature covariance matrix -- e.g., Hafnian and Torontonian in the well-known special cases of photon-number resolving and on-off detectors, respectively. Based on our results, we have considered a GBS validation technique involving detectors with realistic photon-number resolution.","sentences":["Gaussian Boson Sampling (GBS) is the model of non-universal quantum computation that has already demonstrated quantum supremacy in experiments.","This model entails sampling photocounting events from a multimode Gaussian state at the outputs of a linear interferometer.","In this scheme, collision events -- those with more than one photon for each mode -- are infrequent.","However, they are still used for validation purposes.","Therefore, the limitation of realistic detectors to perfectly resolve adjacent photon numbers becomes pivotal.","We have derived the photocounting probability distribution in the GBS schemes, which is applicable for use with general detectors and photocounting techniques.","This probability distribution is expressed in terms of functionals of the field-quadrature covariance matrix -- e.g., Hafnian and Torontonian in the well-known special cases of photon-number resolving and on-off detectors, respectively.","Based on our results, we have considered a GBS validation technique involving detectors with realistic photon-number resolution."],"url":"http://arxiv.org/abs/2403.03184v1","category":"quant-ph"}
{"created":"2024-03-05 18:20:10","title":"How Well Can Transformers Emulate In-context Newton's Method?","abstract":"Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\\epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.","sentences":["Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms.","Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression.","In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression.","We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\\epsilon$ error with only a logarithmic to the error more layers.","As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers.","These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent."],"url":"http://arxiv.org/abs/2403.03183v1","category":"cs.LG"}
{"created":"2024-03-05 18:19:29","title":"Behavior Generation with Latent Actions","abstract":"Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies. Videos and code can be found https://sjlee.cc/vq-bet","sentences":["Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making.","Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction.","A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes.","However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions.","In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations.","VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module.","Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies.","Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies.","Videos and code can be found https://sjlee.cc/vq-bet"],"url":"http://arxiv.org/abs/2403.03181v1","category":"cs.LG"}
{"created":"2024-03-05 18:16:49","title":"Quantum 2D Liouville Path-Integral Is a Sum over Geometries in AdS$_3$ Einstein Gravity","abstract":"There is a renowned solution of the modular bootstrap that defines the UV complete quantum Liouville theory. We triangulate the path-integral of this Liouville CFT on any 2D surface $\\mathcal{M}$, by proposing a shrinkable boundary condition for this special CFT that allows small holes to close, analogous to the proposal in rational CFTs [1-3]. This is essentially a tensor network that admits an interpretation of a state-sum of a 3D topological theory constructed with quantum 6j symbols of $\\mathcal{U}_q(SL(2,\\mathbb{R}))$ with non-trivial boundary conditions, and it reduces to a sum over 3D geometries weighted by the Einstein-Hilbert action to leading order in large $c$. The boundary conditions of quantum Liouville theory specifies a very special sum over bulk geometries to faithfully reproduce the CFT path-integral. The triangulation coincides with producing a network of geodesics in the AdS bulk, which can be changed making use of the pentagon identity and orthogonality condition satisfied by the 6j symbols, and arranged into a precise holographic tensor network.","sentences":["There is a renowned solution of the modular bootstrap that defines the UV complete quantum Liouville theory.","We triangulate the path-integral of this Liouville CFT on any 2D surface $\\mathcal{M}$, by proposing a shrinkable boundary condition for this special CFT that allows small holes to close, analogous to the proposal in rational CFTs [1-3].","This is essentially a tensor network that admits an interpretation of a state-sum of a 3D topological theory constructed with quantum 6j symbols of $\\mathcal{U}_q(SL(2,\\mathbb{R}))$ with non-trivial boundary conditions, and it reduces to a sum over 3D geometries weighted by the Einstein-Hilbert action to leading order in large $c$. The boundary conditions of quantum Liouville theory specifies a very special sum over bulk geometries to faithfully reproduce the CFT path-integral.","The triangulation coincides with producing a network of geodesics in the AdS bulk, which can be changed making use of the pentagon identity and orthogonality condition satisfied by the 6j symbols, and arranged into a precise holographic tensor network."],"url":"http://arxiv.org/abs/2403.03179v1","category":"hep-th"}
{"created":"2024-03-05 18:13:18","title":"Unifying and Certifying Top-Quality Planning","abstract":"The growing utilization of planning tools in practical scenarios has sparked an interest in generating multiple high-quality plans. Consequently, a range of computational problems under the general umbrella of top-quality planning were introduced over a short time period, each with its own definition. In this work, we show that the existing definitions can be unified into one, based on a dominance relation. The different computational problems, therefore, simply correspond to different dominance relations. Given the unified definition, we can now certify the top-quality of the solutions, leveraging existing certification of unsolvability and optimality. We show that task transformations found in the existing literature can be employed for the efficient certification of various top-quality planning problems and propose a novel transformation to efficiently certify loopless top-quality planning.","sentences":["The growing utilization of planning tools in practical scenarios has sparked an interest in generating multiple high-quality plans.","Consequently, a range of computational problems under the general umbrella of top-quality planning were introduced over a short time period, each with its own definition.","In this work, we show that the existing definitions can be unified into one, based on a dominance relation.","The different computational problems, therefore, simply correspond to different dominance relations.","Given the unified definition, we can now certify the top-quality of the solutions, leveraging existing certification of unsolvability and optimality.","We show that task transformations found in the existing literature can be employed for the efficient certification of various top-quality planning problems and propose a novel transformation to efficiently certify loopless top-quality planning."],"url":"http://arxiv.org/abs/2403.03176v1","category":"cs.AI"}
{"created":"2024-03-05 18:08:45","title":"MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting","abstract":"Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.","sentences":["Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals.","While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question.","In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions.","At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world.","By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources.","To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve.","Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation.","We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement."],"url":"http://arxiv.org/abs/2403.03174v1","category":"cs.RO"}
{"created":"2024-03-05 18:07:34","title":"Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination","abstract":"Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states. Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance.","sentences":["Reaching consensus is key to multi-agent coordination.","To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward.","However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem.","In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents.","The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal.","The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states.","We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods.","We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states.","Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance."],"url":"http://arxiv.org/abs/2403.03172v1","category":"cs.AI"}
{"created":"2024-03-05 18:04:59","title":"SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection","abstract":"Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.","sentences":["Misinformation is a prevalent societal issue due to its potential high risks.","Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences.","Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation.","While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences.","In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation.","SNIFFER employs two-stage instruction tuning on InstructBLIP.","The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers.","Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification.","Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy.","SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations."],"url":"http://arxiv.org/abs/2403.03170v1","category":"cs.MM"}
{"created":"2024-03-05 18:03:51","title":"Learning Explicitly Conditioned Sparsifying Transforms","abstract":"Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.","sentences":["Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains.","Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers.","Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model.","Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms.","We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art."],"url":"http://arxiv.org/abs/2403.03168v1","category":"math.NA"}
{"created":"2024-03-05 18:01:59","title":"PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset","abstract":"Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with https://github.com/GGLAB-KU/paradise.","sentences":["Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans.","However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities.","These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis.","To tackle this, we present PARADISE, an abductive reasoning task using Q\\&A format on practical procedural text sourced from wikiHow.","It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal.","Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios.","Despite advancements, all models fall short of human performance.","Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks.","The PARADISE dataset and associated resources are publicly available for further research exploration with https://github.com/GGLAB-KU/paradise."],"url":"http://arxiv.org/abs/2403.03167v1","category":"cs.CL"}
{"created":"2024-03-05 17:58:26","title":"Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks","abstract":"To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated. A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers. However, FL networks are expected to involve thousands of heterogeneous distributed devices. As a result, communication efficiency remains a key bottleneck. To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation. Therefore, based on the improvement of edge server resource utilization, this paper can effectively make up for the limitation of cache capacity. In order to mitigate the impact of soft clicks on the quality of user experience (QoE), the authors model the user QoE as a comprehensive system cost. To solve the formulaic problem, the authors propose a decentralized caching algorithm with federated deep reinforcement learning (DRL) and federated learning (FL), where multiple agents learn and make decisions independently","sentences":["To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated.","A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers.","However, FL networks are expected to involve thousands of heterogeneous distributed devices.","As a result, communication efficiency remains a key bottleneck.","To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation.","Therefore, based on the improvement of edge server resource utilization, this paper can effectively make up for the limitation of cache capacity.","In order to mitigate the impact of soft clicks on the quality of user experience (QoE), the authors model the user QoE as a comprehensive system cost.","To solve the formulaic problem, the authors propose a decentralized caching algorithm with federated deep reinforcement learning (DRL) and federated learning (FL), where multiple agents learn and make decisions independently"],"url":"http://arxiv.org/abs/2403.03165v1","category":"cs.AI"}
{"created":"2024-03-05 17:56:27","title":"Design2Code: How Far Are We From Automating Front-End Engineering?","abstract":"Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that successfully matches the performance of Gemini Pro Vision. Both human evaluation and automatic metrics show that GPT-4V performs the best on this task compared to other models. Moreover, annotators think GPT-4V generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages are considered better than the original reference webpages. Our fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning.","sentences":["Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation.","This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations.","In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking.","Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input.","We also complement automatic metrics with comprehensive human evaluations.","We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision.","We further finetune an open-source Design2Code-18B model that successfully matches the performance of Gemini Pro Vision.","Both human evaluation and automatic metrics show that GPT-4V performs the best on this task compared to other models.","Moreover, annotators think GPT-4V generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages are considered better than the original reference webpages.","Our fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning."],"url":"http://arxiv.org/abs/2403.03163v1","category":"cs.CL"}
{"created":"2024-03-05 17:54:22","title":"PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning","abstract":"Palms play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding palm distribution and abundance. However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patches in two distinct sizes. These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters. Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap. This heatmap effectively visualizes the distribution of palms, showcasing the scalability and adaptability of our approach in various forest densities. Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.","sentences":["Palms play an outsized role in tropical forests and are important resources for humans and wildlife.","A central question in tropical ecosystems is understanding palm distribution and abundance.","However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes.","Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest.","This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests.","Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patches in two distinct sizes.","These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters.","Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap.","This heatmap effectively visualizes the distribution of palms, showcasing the scalability and adaptability of our approach in various forest densities.","Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing."],"url":"http://arxiv.org/abs/2403.03161v1","category":"cs.CV"}
{"created":"2024-03-05 17:54:03","title":"Soliton Frequency Combs in Elastomer Membrane-Cavity Optomechanics","abstract":"Solitons, arising from nonlinear wave-matter interactions, stand out for their intrinsic stability during wave propagation and exceptional spectral characteristics1. Their applications span diverse physical systems, including telecommunications, atomic clocks, and precise measurements. In recent years, significant strides have been made in developing cavity-optomechanics based approaches to generate optical frequency combs (FCs). In this study, we present an innovative approach, never explored before, that leverages elastomer membrane (EM)-cavity optomechanics to achieve the generation of soliton FCs, a highly sought-after phenomenon in the realm of nonlinear wave-matter interactions. Our method represents a significant breakthrough due to its streamlined simplicity, relying on a single continuous-wave (CW) laser pump and an externally applied acoustic wave exciting an EM-cavity, which gives rise to phonons, quantized vibrational energy states intrinsic to the elastomer's crystalline lattice structure. The mechanical resonator and electromagnetic cavity resonance are parametrically coupled within the microwave frequency range, collectively orchestrate the process of soliton FCs formation with remarkable efficiency. Numerical simulations and experimental observations demonstrate the emergence of multiple stable localized opto-mechanical wave packets, characterized by a narrow pulses time-domain response. Crucially, by setting the acoustic wave frequency to match the natural frequency of the EM resonator, the solitons' teeth are precisely spaced, and the EM's motion is significantly amplified, giving rise to a Kerr medium. The successful realization of optomechanical stable solitons represents a monumental advancement with transformative potential across various fields, including quantum computing and spectroscopy.","sentences":["Solitons, arising from nonlinear wave-matter interactions, stand out for their intrinsic stability during wave propagation and exceptional spectral characteristics1.","Their applications span diverse physical systems, including telecommunications, atomic clocks, and precise measurements.","In recent years, significant strides have been made in developing cavity-optomechanics based approaches to generate optical frequency combs (FCs).","In this study, we present an innovative approach, never explored before, that leverages elastomer membrane (EM)-cavity optomechanics to achieve the generation of soliton FCs, a highly sought-after phenomenon in the realm of nonlinear wave-matter interactions.","Our method represents a significant breakthrough due to its streamlined simplicity, relying on a single continuous-wave (CW) laser pump and an externally applied acoustic wave exciting an EM-cavity, which gives rise to phonons, quantized vibrational energy states intrinsic to the elastomer's crystalline lattice structure.","The mechanical resonator and electromagnetic cavity resonance are parametrically coupled within the microwave frequency range, collectively orchestrate the process of soliton FCs formation with remarkable efficiency.","Numerical simulations and experimental observations demonstrate the emergence of multiple stable localized opto-mechanical wave packets, characterized by a narrow pulses time-domain response.","Crucially, by setting the acoustic wave frequency to match the natural frequency of the EM resonator, the solitons' teeth are precisely spaced, and the EM's motion is significantly amplified, giving rise to a Kerr medium.","The successful realization of optomechanical stable solitons represents a monumental advancement with transformative potential across various fields, including quantum computing and spectroscopy."],"url":"http://arxiv.org/abs/2403.03160v1","category":"physics.optics"}
{"created":"2024-03-05 17:52:20","title":"Novel approach of exploring ASEP-like models through the Yang Baxter Equation","abstract":"We explore the algebraic structure of a particular ansatz of Yang Baxter Equation which is inspired from the Bethe Ansatz treatment of the ASEP spin-model. Various classes of Hamiltonian density arriving from two types of R-Matrices are found which also appear as solutions of constant YBE. We identify the idempotent and nilpotent categories of such constant R-Matrices and perform a rank-1 numerical search for the lowest dimension. A summary of finalised results reveals general non-hermitian spin-1/2 chain models.","sentences":["We explore the algebraic structure of a particular ansatz of Yang Baxter Equation which is inspired from the Bethe Ansatz treatment of the ASEP spin-model.","Various classes of Hamiltonian density arriving from two types of R-Matrices are found which also appear as solutions of constant YBE.","We identify the idempotent and nilpotent categories of such constant R-Matrices and perform a rank-1 numerical search for the lowest dimension.","A summary of finalised results reveals general non-hermitian spin-1/2 chain models."],"url":"http://arxiv.org/abs/2403.03159v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 17:50:24","title":"The Amplitude Equation for the Space-Fractional Swift-Hohenberg Equation","abstract":"Non-local reaction-diffusion partial differential equations (PDEs) involving the fractional Laplacian have arisen in a wide variety of applications. One common tool to analyse the dynamics of classical local PDEs near instability is to derive local amplitude/modulation approximations, which provide local normal forms classifying a wide variety of pattern-formation phenomena. In this work, we study amplitude equations for the space-fractional Swift-Hohenberg equation. The Swift-Hohenberg equation is a basic model problem motivated by pattern formation in fluid dynamics and has served as one of the main PDEs to develop general techniques to derive amplitude equations. We prove that there exists near the first bifurcation point an approximation by a (real) Ginzburg-Landau equation. Interestingly, this Ginzburg-Landau equation is a local PDE, which provides a rigorous justification of the physical conjecture that suitably localized unstable modes can out-compete superdiffusion and re-localize a PDE near instability. Our main technical contributions are to provide a suitable function space setting for the approximation problem, and to then bound the residual between the original PDE and its amplitude equation.","sentences":["Non-local reaction-diffusion partial differential equations (PDEs) involving the fractional Laplacian have arisen in a wide variety of applications.","One common tool to analyse the dynamics of classical local PDEs near instability is to derive local amplitude/modulation approximations, which provide local normal forms classifying a wide variety of pattern-formation phenomena.","In this work, we study amplitude equations for the space-fractional Swift-Hohenberg equation.","The Swift-Hohenberg equation is a basic model problem motivated by pattern formation in fluid dynamics and has served as one of the main PDEs to develop general techniques to derive amplitude equations.","We prove that there exists near the first bifurcation point an approximation by a (real) Ginzburg-Landau equation.","Interestingly, this Ginzburg-Landau equation is a local PDE, which provides a rigorous justification of the physical conjecture that suitably localized unstable modes can out-compete superdiffusion and re-localize a PDE near instability.","Our main technical contributions are to provide a suitable function space setting for the approximation problem, and to then bound the residual between the original PDE and its amplitude equation."],"url":"http://arxiv.org/abs/2403.03158v1","category":"math.AP"}
{"created":"2024-03-05 17:49:09","title":"Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks","abstract":"This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels. A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented. Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties. Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior. The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the derived closed-form solution. The extensive simulation results show that the proposed cluster-based FL framework can outperform FL baselines in terms of both test accuracy and convergence rate. Moreover, jointly optimizing sub-channel and power allocation in NOMA-enhanced networks can lead to a significant improvement.","sentences":["This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels.","A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented.","Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties.","Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior.","The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the derived closed-form solution.","The extensive simulation results show that the proposed cluster-based FL framework can outperform FL baselines in terms of both test accuracy and convergence rate.","Moreover, jointly optimizing sub-channel and power allocation in NOMA-enhanced networks can lead to a significant improvement."],"url":"http://arxiv.org/abs/2403.03157v1","category":"cs.NI"}
{"created":"2024-03-05 17:47:22","title":"Quantum Many-Body Physics Calculations with Large Language Models","abstract":"Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases. Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps. Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory. We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases. The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale.","sentences":["Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning.","We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics.","We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations.","To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information.","We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.","Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps.","Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory.","We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases.","The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale."],"url":"http://arxiv.org/abs/2403.03154v1","category":"physics.comp-ph"}
{"created":"2024-03-05 17:42:39","title":"Deep-Learned Compression for Radio-Frequency Signal Classification","abstract":"Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples. This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data. The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making. Moving large amounts of data to AI agents may result in significant bandwidth and latency costs. We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes. We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal. Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient use of the bandwidth and storage for non-real-time analytics, and for a decreased delay in real-time applications. While exploring the effectiveness of the HQARF signal reconstructions in modulation classification tasks, we highlight the DLC optimization space and some open problems related to the training of the VQ embedded in HQARF.","sentences":["Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples.","This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data.","The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making.","Moving large amounts of data to AI agents may result in significant bandwidth and latency costs.","We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes.","We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal.","Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient use of the bandwidth and storage for non-real-time analytics, and for a decreased delay in real-time applications.","While exploring the effectiveness of the HQARF signal reconstructions in modulation classification tasks, we highlight the DLC optimization space and some open problems related to the training of the VQ embedded in HQARF."],"url":"http://arxiv.org/abs/2403.03150v1","category":"cs.LG"}
{"created":"2024-03-05 17:41:22","title":"Obligations and permissions, algebraically","abstract":"We further develop the algebraic approach to input/output logic initiated in \\cite{wollic22}, where subordination algebras and a family of their generalizations were proposed as a semantic environment of various input/output logics.In particular, we consider precontact algebras as a suitable algebraic environment for negative permission, and we characterize properties of several types of permission (negative, static, dynamic), as well as their interactions with normative systems, by means of suitable modal languages encoding outputs.","sentences":["We further develop the algebraic approach to input/output logic initiated in \\cite{wollic22}, where subordination algebras and a family of their generalizations were proposed as a semantic environment of various input/output logics.","In particular, we consider precontact algebras as a suitable algebraic environment for negative permission, and we characterize properties of several types of permission (negative, static, dynamic), as well as their interactions with normative systems, by means of suitable modal languages encoding outputs."],"url":"http://arxiv.org/abs/2403.03148v1","category":"math.LO"}
{"created":"2024-03-05 17:38:19","title":"The resonant history of gravitational atoms in black hole binaries","abstract":"Rotating black holes can produce superradiant clouds of ultralight bosons. When the black hole is part of a binary system, its cloud can undergo resonances and ionization. These processes leave a distinct signature on the gravitational waveform that depends on the cloud's properties. To determine the state of the cloud by the time the system enters the band of future millihertz detectors, we study the chronological sequence of resonances encountered during the inspiral. For the first time, we consistently take into account the nonlinearities induced by the orbital backreaction and we allow the orbit to have generic eccentricity and inclination. We find that the resonance phenomenology exhibits striking new features. Resonances can \"start\" or \"break\" above critical thresholds of the parameters, which we compute analytically, and induce dramatic changes in eccentricity and inclination. Applying these results to realistic systems, we find two possible outcomes. If the binary and the cloud are sufficiently close to counter-rotating, the cloud survives in its original state until the system enters in band; otherwise, the cloud is destroyed during a resonance at large separations, but leaves an imprint on the eccentricity and inclination. In both scenarios, we characterize the observational signatures, with particular focus on future gravitational wave detectors.","sentences":["Rotating black holes can produce superradiant clouds of ultralight bosons.","When the black hole is part of a binary system, its cloud can undergo resonances and ionization.","These processes leave a distinct signature on the gravitational waveform that depends on the cloud's properties.","To determine the state of the cloud by the time the system enters the band of future millihertz detectors, we study the chronological sequence of resonances encountered during the inspiral.","For the first time, we consistently take into account the nonlinearities induced by the orbital backreaction and we allow the orbit to have generic eccentricity and inclination.","We find that the resonance phenomenology exhibits striking new features.","Resonances can \"start\" or \"break\" above critical thresholds of the parameters, which we compute analytically, and induce dramatic changes in eccentricity and inclination.","Applying these results to realistic systems, we find two possible outcomes.","If the binary and the cloud are sufficiently close to counter-rotating, the cloud survives in its original state until the system enters in band; otherwise, the cloud is destroyed during a resonance at large separations, but leaves an imprint on the eccentricity and inclination.","In both scenarios, we characterize the observational signatures, with particular focus on future gravitational wave detectors."],"url":"http://arxiv.org/abs/2403.03147v1","category":"gr-qc"}
{"created":"2024-03-05 17:37:55","title":"Unexpected but recurrent phenomena for Quot and Hilbert schemes of points","abstract":"We investigate some aspects of the geometry of two classical generalisations of the Hilbert schemes of points. Precisely, we show that parity conjecture for $\\text{Quot}_r^d\\mathbb{A}^3$ already fails for $d=8$ and $r=2$ and that lots of the elementary components of the nested Hilbert schemes of points on smooth quasi-projective varieties of dimension at least 4 are generically non-reduced. Finally, we give an infinite family of elementary components of the classical Hilbert schemes of points.","sentences":["We investigate some aspects of the geometry of two classical generalisations of the Hilbert schemes of points.","Precisely, we show that parity conjecture for $\\text{Quot}_r^d\\mathbb{A}^3$ already fails for $d=8$ and $r=2$ and that lots of the elementary components of the nested Hilbert schemes of points on smooth quasi-projective varieties of dimension at least 4 are generically non-reduced.","Finally, we give an infinite family of elementary components of the classical Hilbert schemes of points."],"url":"http://arxiv.org/abs/2403.03146v1","category":"math.AG"}
{"created":"2024-03-05 17:35:46","title":"Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization","abstract":"Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%, 9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods respectively, given only 3% positional-annotations. We also extend our framework to some existing AVSL methods and consistently boost their performance.","sentences":["Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips.","Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence.","Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives.","Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data.","In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue.","Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps.","The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%, 9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods respectively, given only 3% positional-annotations.","We also extend our framework to some existing AVSL methods and consistently boost their performance."],"url":"http://arxiv.org/abs/2403.03145v1","category":"cs.CV"}
{"created":"2024-03-05 17:24:16","title":"Positivity of Ulrich bundles in the ample and free case","abstract":"We study the positivity of an Ulrich vector bundle defined with respect to a globally generated ample line bundle. First we prove a generalization of a Lopez theorem on the first Chern class and the bigness of an Ulrich bundle. Then, under some additional assumptions on the polarization, we give a description of its augmented base locus, which consequently leads to a characterization of the V-bigness and of the ampleness of an Ulrich bundle in this setting.","sentences":["We study the positivity of an Ulrich vector bundle defined with respect to a globally generated ample line bundle.","First we prove a generalization of a Lopez theorem on the first Chern class and the bigness of an Ulrich bundle.","Then, under some additional assumptions on the polarization, we give a description of its augmented base locus, which consequently leads to a characterization of the V-bigness and of the ampleness of an Ulrich bundle in this setting."],"url":"http://arxiv.org/abs/2403.03139v1","category":"math.AG"}
{"created":"2024-03-05 17:21:31","title":"Simplicity in Complexity","abstract":"The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \\textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images. This suggests that the complexity of images can be surprisingly simple.","sentences":["The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation.","Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \\textit{complex}.","There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise.","On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem.","Here we propose to model complexity using segment-based representations of images.","We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively.","We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images.","This suggests that the complexity of images can be surprisingly simple."],"url":"http://arxiv.org/abs/2403.03134v1","category":"cs.CV"}
{"created":"2024-03-05 17:15:28","title":"CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following","abstract":"With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context. 2) While specialized smaller models fine-tuned on the synthetic dataset show promise, they still lag behind their larger counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues.","sentences":["With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend.","In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative.","In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically.","Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue.","Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively.","Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context.","2) While specialized smaller models fine-tuned on the synthetic dataset show promise, they still lag behind their larger counterparts.","3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues."],"url":"http://arxiv.org/abs/2403.03129v1","category":"cs.CL"}
{"created":"2024-03-05 17:12:45","title":"3D-$N_{\\rm H}$-tool","abstract":"Absorption of light is one of the main selection effects limiting our ability to detect celestial sources, and ultimately, appearance of the sky across most of the electromagnetic spectrum. Recent advances in quantity and quality of available observational data and analysis methods led to major improvements in resolution, depth and fidelity of 3D dust distribution and extinction maps. The Galactic plane remains, however, essentially ``terra incognita'' beyond distances of a few kilo-parsecs due to the strong absorption in optical and near-infrared bands. Here I attempt to address this issue and present a 3D-$N_{\\rm H}$-tool to estimate line of sight reddening and X-ray absorption column combining state of the art optical extinction and dust emission maps, and the results of dispersion measure modeling based on radio pulsar observations. The resulting maps are calibrated using independent absorption tracers and are accessible to general community via a convenient web-interface and full data cube.","sentences":["Absorption of light is one of the main selection effects limiting our ability to detect celestial sources, and ultimately, appearance of the sky across most of the electromagnetic spectrum.","Recent advances in quantity and quality of available observational data and analysis methods led to major improvements in resolution, depth and fidelity of 3D dust distribution and extinction maps.","The Galactic plane remains, however, essentially ``terra incognita'' beyond distances of a few kilo-parsecs due to the strong absorption in optical and near-infrared bands.","Here I attempt to address this issue and present a 3D-$N_{\\rm H}$-tool to estimate line of sight reddening and X-ray absorption column combining state of the art optical extinction and dust emission maps, and the results of dispersion measure modeling based on radio pulsar observations.","The resulting maps are calibrated using independent absorption tracers and are accessible to general community via a convenient web-interface and full data cube."],"url":"http://arxiv.org/abs/2403.03127v1","category":"astro-ph.HE"}
{"created":"2024-03-05 17:07:49","title":"Particle-hole asymmetric phases in doped twisted bilayer graphene","abstract":"Despite much theoretical work, developing a comprehensive ab initio model for twisted bilayer graphene (TBG) has proven challenging due to the inherent trade-off between accurately describing the band structure and incorporating the interactions within the Hamiltonian, particularly given the topological obstruction -- so-called fragile topology -- to the description of the model in terms of localized symmetric Wannier functions within the flat band manifold. Here, we circumvent this obstruction by using an extended 8-orbital model, for which localized Wannier orbitals have been formulated by Carr et al. [1]. We constructed an extended multi-orbital Hubbard model, and performed Hartree-Fock (HF) calculations to explore its phase diagram across commensurate fillings from -3 to 3. We found several nearly-degenerate insulating states at charge neutrality, all of which exhibit orbital orders. Crucially, TBG near magic angle is known to be particle-hole asymmetric, which is naturally captured by the single-particle band structure of our model and is reflected in the distinction between the symmetry broken states obtained at electron and hole dopings away from the charge neutral point. At filling -1 and +2, quantum anomalous hall states are obtained, while for the rest of the integer fillings away from charge neutrality, we found the system to realize metallic states with various orbital, valley and spin orderings. We also observed that most of the Hartree--Fock ground states exhibit a generalized valley Hund's-like rule, resulting in valley polarization. Importantly, we show that the incorporation of the intra-valley and inter-valley exchange interactions is crucial to properly stabilize the ordered symmetry-broken states. In agreement with experiments, we find significant particle-hole asymmetry, which underscores the importance of using particle-hole asymmetric models.","sentences":["Despite much theoretical work, developing a comprehensive ab initio model for twisted bilayer graphene (TBG) has proven challenging due to the inherent trade-off between accurately describing the band structure and incorporating the interactions within the Hamiltonian, particularly given the topological obstruction -- so-called fragile topology -- to the description of the model in terms of localized symmetric Wannier functions within the flat band manifold.","Here, we circumvent this obstruction by using an extended 8-orbital model, for which localized Wannier orbitals have been formulated by Carr et al.","[1].","We constructed an extended multi-orbital Hubbard model, and performed Hartree-Fock (HF) calculations to explore its phase diagram across commensurate fillings from -3 to 3.","We found several nearly-degenerate insulating states at charge neutrality, all of which exhibit orbital orders.","Crucially, TBG near magic angle is known to be particle-hole asymmetric, which is naturally captured by the single-particle band structure of our model and is reflected in the distinction between the symmetry broken states obtained at electron and hole dopings away from the charge neutral point.","At filling -1 and +2, quantum anomalous hall states are obtained, while for the rest of the integer fillings away from charge neutrality, we found the system to realize metallic states with various orbital, valley and spin orderings.","We also observed that most of the Hartree--Fock ground states exhibit a generalized valley Hund's-like rule, resulting in valley polarization.","Importantly, we show that the incorporation of the intra-valley and inter-valley exchange interactions is crucial to properly stabilize the ordered symmetry-broken states.","In agreement with experiments, we find significant particle-hole asymmetry, which underscores the importance of using particle-hole asymmetric models."],"url":"http://arxiv.org/abs/2403.03123v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 17:07:29","title":"NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors","abstract":"Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.","sentences":["Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge.","To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space.","To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm.","We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times.","NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model.","We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance.","Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation."],"url":"http://arxiv.org/abs/2403.03122v1","category":"cs.CV"}
{"created":"2024-03-05 17:04:05","title":"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution","abstract":"Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.","sentences":["Large language models (LLMs) reflect societal norms and biases, especially about gender.","While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis.","However, emotion and gender are closely linked in societal discourse.","E.g., women are often thought of as more empathetic, while men's anger is more socially accepted.","To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source).","We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes.","We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'.","We then analyze the emotions generated by the models in relation to the gender-event pairs.","We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes.","These findings are in line with established research in psychology and gender studies.","Our study sheds light on the complex societal interplay between language, gender, and emotion.","The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications."],"url":"http://arxiv.org/abs/2403.03121v1","category":"cs.CL"}
{"created":"2024-03-05 17:01:17","title":"Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation","abstract":"Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions. Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise. Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset. This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling. With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions. Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the geometry of the current frame. The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach. We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach.","sentences":["Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions.","Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise.","Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset.","This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling.","With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions.","Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the geometry of the current frame.","The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach.","We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach."],"url":"http://arxiv.org/abs/2403.03120v1","category":"cs.CV"}
{"created":"2024-03-05 16:59:53","title":"Higher form symmetries and orbifolds of two-dimensional Yang-Mills theory","abstract":"We undertake a detailed study of the gaugings of two-dimensional Yang-Mills theory by its intrinsic charge conjugation 0-form and centre 1-form global symmetries, elucidating their higher algebraic and geometric structures, as well as the meaning of dual lower form symmetries. Our derivations of orbifold gauge theories make use of a combination of standard continuum path integral methods, networks of topological defects, and techniques from higher gauge theory. We provide a unified description of higher and lower form gauge fields for a $p$-form symmetry in the geometric setting of $p$-gerbes, and derive reverse orbifolds by the dual $(-1)$-form symmetries. We identify those orbifolds in which charge conjugation symmetry is spontaneously broken, and relate the breaking to mixed anomalies involving $(-1)$-form symmetries. We extend these considerations to gaugings by the non-invertible 1-form symmetries of two-dimensional Yang-Mills theory by introducing a notion of generalized $\\theta$-angle.","sentences":["We undertake a detailed study of the gaugings of two-dimensional Yang-Mills theory by its intrinsic charge conjugation 0-form and centre 1-form global symmetries, elucidating their higher algebraic and geometric structures, as well as the meaning of dual lower form symmetries.","Our derivations of orbifold gauge theories make use of a combination of standard continuum path integral methods, networks of topological defects, and techniques from higher gauge theory.","We provide a unified description of higher and lower form gauge fields for a $p$-form symmetry in the geometric setting of $p$-gerbes, and derive reverse orbifolds by the dual $(-1)$-form symmetries.","We identify those orbifolds in which charge conjugation symmetry is spontaneously broken, and relate the breaking to mixed anomalies involving $(-1)$-form symmetries.","We extend these considerations to gaugings by the non-invertible 1-form symmetries of two-dimensional Yang-Mills theory by introducing a notion of generalized $\\theta$-angle."],"url":"http://arxiv.org/abs/2403.03119v1","category":"hep-th"}
{"created":"2024-03-05 16:56:09","title":"Equilibria in Two-Stage Facility Location with Atomic Clients","abstract":"We consider competitive facility location as a two-stage multi-agent system with two types of clients. For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities. Then, the clients strategically select which of the opened facilities in their neighborhood to patronize. Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility.   All recently studied versions of this model assume that clients can split their weight strategically. We consider clients with unsplittable weights, but allow mixed strategies. So clients may randomize over which facility to patronize. Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible.   As our main result, we show that pure subgame perfect equilibria always exist if all client weights are identical. For this, we use a novel potential function argument, employing a hierarchical classification of the clients and sophisticated rounding in each step. In contrast, for non-identical clients, we show that deciding the existence of even approximately stable states is computationally intractable. On the positive side, we give a tight bound of 2 on the price of anarchy which implies high social welfare of equilibria, if they exist.","sentences":["We consider competitive facility location as a two-stage multi-agent system with two types of clients.","For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities.","Then, the clients strategically select which of the opened facilities in their neighborhood to patronize.","Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility.   ","All recently studied versions of this model assume that clients can split their weight strategically.","We consider clients with unsplittable weights, but allow mixed strategies.","So clients may randomize over which facility to patronize.","Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible.   ","As our main result, we show that pure subgame perfect equilibria always exist if all client weights are identical.","For this, we use a novel potential function argument, employing a hierarchical classification of the clients and sophisticated rounding in each step.","In contrast, for non-identical clients, we show that deciding the existence of even approximately stable states is computationally intractable.","On the positive side, we give a tight bound of 2 on the price of anarchy which implies high social welfare of equilibria, if they exist."],"url":"http://arxiv.org/abs/2403.03114v1","category":"cs.GT"}
{"created":"2024-03-05 16:53:24","title":"Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and Novel Outliers Detection","abstract":"Perception is a key element for enabling intelligent autonomous navigation. Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks. Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms. In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms. Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data. We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occur between different objects of the same semantic class. To this end, we propose a novel algorithm that explicitly identifies and discards potential outliers in the matching process. In our experiments, we study the effect of improving the matching process on the robustness of LiDAR odometry against high speed motion. Our experimental evaluations on KITTI dataset demonstrate that utilizing semantic information and rejecting outliers significantly enhance the robustness of LiDAR odometry and mapping when there are large gaps between scan acquisition poses, which is typical for fast moving platforms.","sentences":["Perception is a key element for enabling intelligent autonomous navigation.","Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks.","Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms.","In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms.","Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data.","We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occur between different objects of the same semantic class.","To this end, we propose a novel algorithm that explicitly identifies and discards potential outliers in the matching process.","In our experiments, we study the effect of improving the matching process on the robustness of LiDAR odometry against high speed motion.","Our experimental evaluations on KITTI dataset demonstrate that utilizing semantic information and rejecting outliers significantly enhance the robustness of LiDAR odometry and mapping when there are large gaps between scan acquisition poses, which is typical for fast moving platforms."],"url":"http://arxiv.org/abs/2403.03111v1","category":"cs.CV"}
{"created":"2024-03-05 16:52:03","title":"Grid-constrained online scheduling of flexible electric vehicle charging","abstract":"We study Electric Vehicle (EV) charging from a scheduling perspective, aiming to minimize delays while respecting the grid constraints. A network of parking lots is considered, each with a given number of charging stations for electric vehicles. Some of the parking lots have a roof with solar panels. The demand that can be served at each parking lot is limited by the capacity of the cables connecting them to the grid. We assume that EVs arrive at the parking lots according to a known distribution. Upon arrival, we learn the desired departure time, the amount of electrical energy it needs to charge its battery, and the range of rates that it can be charged at. Vehicle arrival patterns, connection times, and charging volume are based on data collected in the city of Utrecht. The departure time of an EV is delayed if it has not finished charging in time for its desired departure. We aim to minimize the total delay. We present a novel approach, based on an online variant of well-known schedule generation schemes. We extend these schemes and include them in a destroy-and-repair heuristic. This resulted in several scheduling strategies. We show their effectiveness using a discrete event simulation. With this, we show that applying scheduling approaches increases the amount of EVs that can be charged at a site and reduces the average delay. Furthermore, we argue the importance of considering aspects of the grid layout in electricity networks and show the benefits of using flexible charging rates.","sentences":["We study Electric Vehicle (EV) charging from a scheduling perspective, aiming to minimize delays while respecting the grid constraints.","A network of parking lots is considered, each with a given number of charging stations for electric vehicles.","Some of the parking lots have a roof with solar panels.","The demand that can be served at each parking lot is limited by the capacity of the cables connecting them to the grid.","We assume that EVs arrive at the parking lots according to a known distribution.","Upon arrival, we learn the desired departure time, the amount of electrical energy it needs to charge its battery, and the range of rates that it can be charged at.","Vehicle arrival patterns, connection times, and charging volume are based on data collected in the city of Utrecht.","The departure time of an EV is delayed if it has not finished charging in time for its desired departure.","We aim to minimize the total delay.","We present a novel approach, based on an online variant of well-known schedule generation schemes.","We extend these schemes and include them in a destroy-and-repair heuristic.","This resulted in several scheduling strategies.","We show their effectiveness using a discrete event simulation.","With this, we show that applying scheduling approaches increases the amount of EVs that can be charged at a site and reduces the average delay.","Furthermore, we argue the importance of considering aspects of the grid layout in electricity networks and show the benefits of using flexible charging rates."],"url":"http://arxiv.org/abs/2403.03109v1","category":"math.OC"}
{"created":"2024-03-05 16:51:02","title":"On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future","abstract":"Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban logistics and transportation infrastructure, in the wake of disruptive events. This paper presents a survey on the usage of MOD services for resilience improvement (MOD-R) and finds a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD-R services for improving supply chain resilience, empirical impact evaluation, and supporting technologies. MOD-R services have been utilized for anomaly detection, essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair. The review reveals integrating electrification, automation, and advanced communication technologies offers significant synergistic benefits. The review also suggests the importance of harnessing the collective capabilities of humans and intelligent machines to effectively implement versatile, multi-functional MOD-R services during crises.","sentences":["Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban logistics and transportation infrastructure, in the wake of disruptive events.","This paper presents a survey on the usage of MOD services for resilience improvement (MOD-R) and finds a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD-R services for improving supply chain resilience, empirical impact evaluation, and supporting technologies.","MOD-R services have been utilized for anomaly detection, essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair.","The review reveals integrating electrification, automation, and advanced communication technologies offers significant synergistic benefits.","The review also suggests the importance of harnessing the collective capabilities of humans and intelligent machines to effectively implement versatile, multi-functional MOD-R services during crises."],"url":"http://arxiv.org/abs/2403.03107v1","category":"cs.CY"}
{"created":"2024-03-05 16:48:23","title":"Understanding gravitationally induced decoherence parameters in neutrino oscillations using a microscopic quantum mechanical model","abstract":"In this work, a microscopic quantum mechanical model for gravitationally induced decoherence introduced by Blencowe and Xu is investigated in the context of neutrino oscillations. The focus is on the comparison with existing phenomenological models and the physical interpretation of the decoherence parameters in such models. The results show that for neutrino oscillations in vacuum gravitationally induced decoherence can be matched with phenomenological models with decoherence parameters of the form $\\Gamma_{ij}\\sim \\Delta m^4_{ij}E^{-2}$. When matter effects are included, the decoherence parameters show a dependence on matter effects, which vary in the different layers of the Earth, that can be explained with the form of the coupling between neutrinos and the gravitational wave environment inspired by linearised gravity. Consequently, in the case of neutrino oscillations in matter, the microscopic model does not agree with many existing phenomenological models that assume constant decoherence parameters in matter, and their existing bounds cannot be used to further constrain the model considered here. The probabilities for neutrino oscillations with constant and varying decoherence parameters are compared and it is shown that the deviations can be up to 10%. On a theoretical level, these different models can be characterised by a different choice of Lindblad operators, with the model with decoherence parameters that do not include matter effects being less suitable from the point of view of linearised gravity.","sentences":["In this work, a microscopic quantum mechanical model for gravitationally induced decoherence introduced by Blencowe and Xu is investigated in the context of neutrino oscillations.","The focus is on the comparison with existing phenomenological models and the physical interpretation of the decoherence parameters in such models.","The results show that for neutrino oscillations in vacuum gravitationally induced decoherence can be matched with phenomenological models with decoherence parameters of the form","$\\Gamma_{ij}\\sim \\Delta m^4_{ij}E^{-2}$. When matter effects are included, the decoherence parameters show a dependence on matter effects, which vary in the different layers of the Earth, that can be explained with the form of the coupling between neutrinos and the gravitational wave environment inspired by linearised gravity.","Consequently, in the case of neutrino oscillations in matter, the microscopic model does not agree with many existing phenomenological models that assume constant decoherence parameters in matter, and their existing bounds cannot be used to further constrain the model considered here.","The probabilities for neutrino oscillations with constant and varying decoherence parameters are compared and it is shown that the deviations can be up to 10%.","On a theoretical level, these different models can be characterised by a different choice of Lindblad operators, with the model with decoherence parameters that do not include matter effects being less suitable from the point of view of linearised gravity."],"url":"http://arxiv.org/abs/2403.03106v1","category":"gr-qc"}
{"created":"2024-03-05 16:45:41","title":"Low-rank approximated Kalman-Bucy filters using Oja's principal component flow for linear time-invariant systems","abstract":"The Kalman-Bucy filter is widely used in various applications. However, the filter becomes computationally complex under large-scale systems. To address this problem, a low-rank approximated Kalman-Bucy filter consisting of Oja's principal component flow and a low-dimensional Riccati differential equation was proposed. However, the estimation error was established only for linear time-invariant systems with a symmetric system matrix. This study removes restrictions on the symmetricity of the system matrix and reveals the equilibrium points of the Oja flow and their stability for general real square matrices. In addition, the attraction domain for a set of stable equilibrium points is estimated. Based on these results, we demonstrate that the low-rank approximated Kalman-Bucy filter has a bounded estimation error covariance matrix when the system is controllable and observable.","sentences":["The Kalman-Bucy filter is widely used in various applications.","However, the filter becomes computationally complex under large-scale systems.","To address this problem, a low-rank approximated Kalman-Bucy filter consisting of Oja's principal component flow and a low-dimensional Riccati differential equation was proposed.","However, the estimation error was established only for linear time-invariant systems with a symmetric system matrix.","This study removes restrictions on the symmetricity of the system matrix and reveals the equilibrium points of the Oja flow and their stability for general real square matrices.","In addition, the attraction domain for a set of stable equilibrium points is estimated.","Based on these results, we demonstrate that the low-rank approximated Kalman-Bucy filter has a bounded estimation error covariance matrix when the system is controllable and observable."],"url":"http://arxiv.org/abs/2403.03104v1","category":"math.OC"}
{"created":"2024-03-05 16:43:03","title":"\"In Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning","abstract":"Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.","sentences":["Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas.","However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility.","We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles.","Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively.","Additionally, the results of human evaluations further validate the efficacy of our proposed method."],"url":"http://arxiv.org/abs/2403.03102v1","category":"cs.CL"}
{"created":"2024-03-05 16:39:12","title":"KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents","abstract":"Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.","sentences":["Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions.","This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination.","To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge.","Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents.","Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines.","Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.","Code is available in https://github.com/zjunlp/KnowAgent."],"url":"http://arxiv.org/abs/2403.03101v1","category":"cs.CL"}
{"created":"2024-03-05 16:35:25","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models","abstract":"While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.","sentences":["While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody.","Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually.","Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way.","Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt.","With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way.","Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility.","Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data."],"url":"http://arxiv.org/abs/2403.03100v1","category":"eess.AS"}
{"created":"2024-03-05 16:35:11","title":"Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure","abstract":"Big data, with NxP dimension where N is extremely large, has created new challenges for data analysis, particularly in the realm of creating meaningful clusters of data. Clustering techniques, such as K-means or hierarchical clustering are popular methods for performing exploratory analysis on large datasets. Unfortunately, these methods are not always possible to apply to big data due to memory or time constraints generated by calculations of order PxN(N-1). To circumvent this problem, typically, the clustering technique is applied to a random sample drawn from the dataset: however, a weakness is that the structure of the dataset, particularly at the edges, is not necessarily maintained. We propose a new solution through the concept of \"data nuggets\", which reduce a large dataset into a small collection of nuggets of data, each containing a center, weight, and scale parameter. The data nuggets are then input into algorithms that compute methods such as principal components analysis and clustering in a more computationally efficient manner. We show the consistency of the data nuggets-based covariance estimator and apply the methodology of data nuggets to perform exploratory analysis of a flow cytometry dataset containing over one million observations using PCA and K-means clustering for weighted observations. Supplementary materials for this article are available online.","sentences":["Big data, with NxP dimension where N is extremely large, has created new challenges for data analysis, particularly in the realm of creating meaningful clusters of data.","Clustering techniques, such as K-means or hierarchical clustering are popular methods for performing exploratory analysis on large datasets.","Unfortunately, these methods are not always possible to apply to big data due to memory or time constraints generated by calculations of order PxN(N-1).","To circumvent this problem, typically, the clustering technique is applied to a random sample drawn from the dataset: however, a weakness is that the structure of the dataset, particularly at the edges, is not necessarily maintained.","We propose a new solution through the concept of \"data nuggets\", which reduce a large dataset into a small collection of nuggets of data, each containing a center, weight, and scale parameter.","The data nuggets are then input into algorithms that compute methods such as principal components analysis and clustering in a more computationally efficient manner.","We show the consistency of the data nuggets-based covariance estimator and apply the methodology of data nuggets to perform exploratory analysis of a flow cytometry dataset containing over one million observations using PCA and K-means clustering for weighted observations.","Supplementary materials for this article are available online."],"url":"http://arxiv.org/abs/2403.03099v1","category":"stat.ME"}
{"created":"2024-03-05 16:30:58","title":"Stretch-independent magnetization in incompressible magnetorheological elastomers","abstract":"In this study, we perform a critical examination of the phenomenon where the magnetization is stretch-independent in incompressible hard-magnetic magnetorheological elastomers (h-MREs), as observed in several recent experimental and numerical investigations. We demonstrate that the fully dissipative model proposed by Mukherjee et al. (2021) may be reduced, under physically consistent assumptions, to that of Yan et al. (2023), but not that of Zhao et al. (2019). In cases where the h-MRE solid undergoes non-negligible stretching, the model of Zhao et al. (2019) provides predictions that are in disagreement with experimental observations, given that, by construction, that model produces a magnetization response that is not stretch-independent. By contrast, the other two models are able to describe this important feature present in h-MREs, as well as in incompressible magnetically soft s-MRES. Note that in cases where stretching is negligible, such as for inextensible slender structures under bending deformation, the Zhao et al. (2019) model provides accurate predictions despite its underlying assumptions. Additionally, our analysis reveals two key points about the magnetization vector in the context of the more general, fully dissipative model. First, the magnetization can be related to an internal variable in that theory. However, it cannot be formally used as an internal variable except in the special case of an ideal magnet, and, as such, it is subject to constitutive assumptions. Furthermore, we clarify that the magnetization vector alone is insufficient to describe entirely the magnetic response of an MRE solid; instead, the introduction of one of the original Maxwell fields is always necessary for a complete representation.","sentences":["In this study, we perform a critical examination of the phenomenon where the magnetization is stretch-independent in incompressible hard-magnetic magnetorheological elastomers (h-MREs), as observed in several recent experimental and numerical investigations.","We demonstrate that the fully dissipative model proposed by Mukherjee et al.","(2021) may be reduced, under physically consistent assumptions, to that of Yan et al. (2023), but not that of Zhao et al. (2019).","In cases where the h-MRE solid undergoes non-negligible stretching, the model of Zhao et al. (2019) provides predictions that are in disagreement with experimental observations, given that, by construction, that model produces a magnetization response that is not stretch-independent.","By contrast, the other two models are able to describe this important feature present in h-MREs, as well as in incompressible magnetically soft s-MRES.","Note that in cases where stretching is negligible, such as for inextensible slender structures under bending deformation, the Zhao et al. (2019) model provides accurate predictions despite its underlying assumptions.","Additionally, our analysis reveals two key points about the magnetization vector in the context of the more general, fully dissipative model.","First, the magnetization can be related to an internal variable in that theory.","However, it cannot be formally used as an internal variable except in the special case of an ideal magnet, and, as such, it is subject to constitutive assumptions.","Furthermore, we clarify that the magnetization vector alone is insufficient to describe entirely the magnetic response of an MRE solid; instead, the introduction of one of the original Maxwell fields is always necessary for a complete representation."],"url":"http://arxiv.org/abs/2403.03096v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:26:16","title":"Refined geometric characterizations of weak $p$-quasiconformal mappings","abstract":"In this paper we consider refined geometric characterizations of weak $p$-quasiconformal mappings $\\varphi:\\Omega\\to\\widetilde{\\Omega}$, where $\\Omega$ and $\\widetilde{\\Omega}$ are domains in $\\mathbb R^n$. We prove that mappings with the bounded on the set $\\Omega\\setminus S$, where a set $S$ has $\\sigma$-finite $(n-1)$-measure, geometric $p$-dilatation, are $W^1_{p,\\loc}$-- mappings and generate bounded composition operators on Sobolev spaces.","sentences":["In this paper we consider refined geometric characterizations of weak $p$-quasiconformal mappings $\\varphi:\\Omega\\to\\widetilde{\\Omega}$, where $\\Omega$ and $\\widetilde{\\Omega}$ are domains in $\\mathbb R^n$. We prove that mappings with the bounded on the set $\\Omega\\setminus S$, where a set $S$ has $\\sigma$-finite $(n-1)$-measure, geometric $p$-dilatation, are $W^1_{p,\\loc}$-- mappings and generate bounded composition operators on Sobolev spaces."],"url":"http://arxiv.org/abs/2403.03094v1","category":"math.AP"}
{"created":"2024-03-05 16:25:05","title":"Collective self-caging of active filaments in virtual confinement","abstract":"Motility coupled to responsive behavior is essential for many microorganisms to seek and establish appropriate habitats. One of the simplest possible responses, reversing the direction of motion, is believed to enable filamentous cyanobacteria to form stable aggregates or accumulate in suitable light conditions. Here, we demonstrate that filamentous morphology in combination with responding to light gradients by reversals has consequences far beyond simple accumulation: Entangled aggregates form at the boundaries of illuminated regions, harnessing the boundary to establish local order. We explore how the light pattern, in particular its boundary curvature, impacts aggregation. A minimal mechanistic model of active flexible filaments resembles the experimental findings, thereby revealing the emergent and generic character of these structures. This phenomenon may enable elongated microorganisms to generate adaptive colony architectures in limited habitats, or guide the assembly of biomimetic fibrous materials.","sentences":["Motility coupled to responsive behavior is essential for many microorganisms to seek and establish appropriate habitats.","One of the simplest possible responses, reversing the direction of motion, is believed to enable filamentous cyanobacteria to form stable aggregates or accumulate in suitable light conditions.","Here, we demonstrate that filamentous morphology in combination with responding to light gradients by reversals has consequences far beyond simple accumulation: Entangled aggregates form at the boundaries of illuminated regions, harnessing the boundary to establish local order.","We explore how the light pattern, in particular its boundary curvature, impacts aggregation.","A minimal mechanistic model of active flexible filaments resembles the experimental findings, thereby revealing the emergent and generic character of these structures.","This phenomenon may enable elongated microorganisms to generate adaptive colony architectures in limited habitats, or guide the assembly of biomimetic fibrous materials."],"url":"http://arxiv.org/abs/2403.03093v1","category":"physics.bio-ph"}
{"created":"2024-03-05 16:22:22","title":"Microelectronic readout of a diamond quantum sensor","abstract":"Quantum sensors based on the nitrogen-vacancy (NV) centre in diamond are rapidly advancing from scientific exploration towards the first generation of commercial applications. While significant progress has been made in developing suitable methods for the manipulation of the NV centre spin state, the detection of the defect luminescence has so far limited the performance of miniaturized sensor architectures. The recent development of photoelectric detection of the NV centre's spin state offers a path to circumvent these limitations, but has to-date required research-grade low current amplifiers to detect the picoampere-scale currents obtained from these systems. Here we report on the photoelectric detection of magnetic resonance (PDMR) with NV ensembles using a complementary metal-oxide semiconductor (CMOS) device. The integrated circuit delivers a digitized output of the diamond sensor with low noise and 50 femtoampere resolution. This integration provides the last missing component on the path to a compact, diamond-based quantum sensor. The device is suited for continuous wave (CW) as well as pulsed operation. We demonstrate its functionality with DC and AC magnetometry up to several megahertz, coherent spin rotation and multi-axial decoupling sequences for quantum sensing.","sentences":["Quantum sensors based on the nitrogen-vacancy (NV) centre in diamond are rapidly advancing from scientific exploration towards the first generation of commercial applications.","While significant progress has been made in developing suitable methods for the manipulation of the NV centre spin state, the detection of the defect luminescence has so far limited the performance of miniaturized sensor architectures.","The recent development of photoelectric detection of the NV centre's spin state offers a path to circumvent these limitations, but has to-date required research-grade low current amplifiers to detect the picoampere-scale currents obtained from these systems.","Here we report on the photoelectric detection of magnetic resonance (PDMR) with NV ensembles using a complementary metal-oxide semiconductor (CMOS) device.","The integrated circuit delivers a digitized output of the diamond sensor with low noise and 50 femtoampere resolution.","This integration provides the last missing component on the path to a compact, diamond-based quantum sensor.","The device is suited for continuous wave (CW) as well as pulsed operation.","We demonstrate its functionality with DC and AC magnetometry up to several megahertz, coherent spin rotation and multi-axial decoupling sequences for quantum sensing."],"url":"http://arxiv.org/abs/2403.03090v1","category":"quant-ph"}
{"created":"2024-03-05 16:21:53","title":"VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism","abstract":"The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies.","sentences":["The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy.","Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands.","The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions.","In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions.","Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research.","This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies."],"url":"http://arxiv.org/abs/2403.03089v1","category":"q-bio.QM"}
{"created":"2024-03-05 16:20:11","title":"Shear-enhanced Liquid Crystal Spinning of Conjugated Polymer Fibers","abstract":"Conjugated polymer fibers can be used to manufacture various soft fibrous optoelectronic devices, significantly advancing wearable devices and smart textiles. Recently, conjugated polymer-based fibrous electronic devices have been widely used in energy conversion, electrochemical sensing, and human-machine interaction. However, the insufficient mechanical properties of conjugated polymer fibers, the difficulty in solution processing semiconductors with rigid main chains, and the challenges in large-scale continuous production have limited their further development in the wearable field. We regulated the pi - pi stacking interactions in conjugated polymer molecules below their critical liquid crystal concentration by applying fluid shear stress. We implemented secondary orientation, leading to the continuous fabrication of anisotropic semiconductor fibers. This strategy enables conjugated polymers with rigid backbones to synergistically enhance the mechanical and semiconductor properties of fibers through liquid crystal spinning. Furthermore, conjugated polymer fibers, exhibiting excellent electrochemical performance and high mechanical strength (600 MPa) that essentially meet the requirements for industrialized preparation, maintain stability under extreme temperatures, radiation, and chemical reagents. Lastly, we have demonstrated logic circuits using semiconductor fiber organic electrochemical transistors, showcasing its application potential in the field of wearable fabric-style logic processing. These findings confirm the importance of the liquid crystalline state and solution control in optimizing the performance of conjugated polymer fibers, thus paving the way for developing a new generation of soft fiber semiconductor devices.","sentences":["Conjugated polymer fibers can be used to manufacture various soft fibrous optoelectronic devices, significantly advancing wearable devices and smart textiles.","Recently, conjugated polymer-based fibrous electronic devices have been widely used in energy conversion, electrochemical sensing, and human-machine interaction.","However, the insufficient mechanical properties of conjugated polymer fibers, the difficulty in solution processing semiconductors with rigid main chains, and the challenges in large-scale continuous production have limited their further development in the wearable field.","We regulated the pi - pi stacking interactions in conjugated polymer molecules below their critical liquid crystal concentration by applying fluid shear stress.","We implemented secondary orientation, leading to the continuous fabrication of anisotropic semiconductor fibers.","This strategy enables conjugated polymers with rigid backbones to synergistically enhance the mechanical and semiconductor properties of fibers through liquid crystal spinning.","Furthermore, conjugated polymer fibers, exhibiting excellent electrochemical performance and high mechanical strength (600 MPa) that essentially meet the requirements for industrialized preparation, maintain stability under extreme temperatures, radiation, and chemical reagents.","Lastly, we have demonstrated logic circuits using semiconductor fiber organic electrochemical transistors, showcasing its application potential in the field of wearable fabric-style logic processing.","These findings confirm the importance of the liquid crystalline state and solution control in optimizing the performance of conjugated polymer fibers, thus paving the way for developing a new generation of soft fiber semiconductor devices."],"url":"http://arxiv.org/abs/2403.03088v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:20:01","title":"Bounding speedup of quantum-enhanced Markov chain Monte Carlo","abstract":"Sampling tasks are a natural class of problems for quantum computers due to the probabilistic nature of the Born rule. Sampling from useful distributions on noisy quantum hardware remains a challenging problem. A recent paper [Layden, D. et al. Nature 619, 282-287 (2023)] proposed a quantum-enhanced Markov chain Monte Carlo algorithm where moves are generated by a quantum device and accepted or rejected by a classical algorithm. While this procedure is robust to noise and control imperfections, its potential for quantum advantage is unclear. Here we show that there is no speedup over classical sampling on a worst-case unstructured sampling problem. We present an upper bound to the Markov gap that rules out a speedup for any unital quantum proposal.","sentences":["Sampling tasks are a natural class of problems for quantum computers due to the probabilistic nature of the Born rule.","Sampling from useful distributions on noisy quantum hardware remains a challenging problem.","A recent paper [Layden, D. et al.","Nature 619, 282-287 (2023)] proposed a quantum-enhanced Markov chain Monte Carlo algorithm where moves are generated by a quantum device and accepted or rejected by a classical algorithm.","While this procedure is robust to noise and control imperfections, its potential for quantum advantage is unclear.","Here we show that there is no speedup over classical sampling on a worst-case unstructured sampling problem.","We present an upper bound to the Markov gap that rules out a speedup for any unital quantum proposal."],"url":"http://arxiv.org/abs/2403.03087v1","category":"quant-ph"}
{"created":"2024-03-05 16:09:55","title":"Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation","abstract":"Offline runtime verification involves the static analysis of executions of a system against a specification. For distributed systems, it is generally not possible to characterize executions in the form of global traces, given the absence of a global clock. To account for this, we model executions as collections of local traces called multi-traces, with one local trace per group of co-localized actors that share a common clock. Due to the difficulty of synchronizing the start and end of the recordings of local traces, events may be missing at their beginning or end. Considering such partially observed multi-traces is challenging for runtime verification. To that end, we propose an algorithm that verifies the conformity of such traces against formal specifications called Interactions (akin to Message Sequence Charts). It relies on parameterized simulation to reconstitute unobserved behaviors.","sentences":["Offline runtime verification involves the static analysis of executions of a system against a specification.","For distributed systems, it is generally not possible to characterize executions in the form of global traces, given the absence of a global clock.","To account for this, we model executions as collections of local traces called multi-traces, with one local trace per group of co-localized actors that share a common clock.","Due to the difficulty of synchronizing the start and end of the recordings of local traces, events may be missing at their beginning or end.","Considering such partially observed multi-traces is challenging for runtime verification.","To that end, we propose an algorithm that verifies the conformity of such traces against formal specifications called Interactions (akin to Message Sequence Charts).","It relies on parameterized simulation to reconstitute unobserved behaviors."],"url":"http://arxiv.org/abs/2403.03083v1","category":"cs.SE"}
{"created":"2024-03-05 16:08:59","title":"Recall-Oriented Continual Learning with Generative Adversarial Meta-Model","abstract":"The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios. Our code is available at: https://github.com/bigdata-inha/recall-oriented-cl-framework.","sentences":["The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks.","In this paper, we propose the recall-oriented continual learning framework to address this challenge.","Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary.","In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task.","Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios.","Our code is available at: https://github.com/bigdata-inha/recall-oriented-cl-framework."],"url":"http://arxiv.org/abs/2403.03082v1","category":"cs.LG"}
{"created":"2024-03-05 15:59:54","title":"On a Neural Implementation of Brenier's Polar Factorization","abstract":"In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of \\citeauthor{Brenier1991PolarFA}'s polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.","sentences":["In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$.","The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning.","The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network.","The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^*","\\circ F$, or learned as an auxiliary network.","Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator.","We illustrate possible applications of \\citeauthor{Brenier1991PolarFA}'s polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave."],"url":"http://arxiv.org/abs/2403.03071v1","category":"stat.ML"}
{"created":"2024-03-05 15:59:54","title":"Geometry-dependent matching pursuit: a transition phase for convergence on linear regression and LASSO","abstract":"Greedy first-order methods, such as coordinate descent with Gauss-Southwell rule or matching pursuit, have become popular in optimization due to their natural tendency to propose sparse solutions and their refined convergence guarantees. In this work, we propose a principled approach to generating (regularized) matching pursuit algorithms adapted to the geometry of the problem at hand, as well as their convergence guarantees. Building on these results, we derive approximate convergence guarantees and describe a transition phenomenon in the convergence of (regularized) matching pursuit from underparametrized to overparametrized models.","sentences":["Greedy first-order methods, such as coordinate descent with Gauss-Southwell rule or matching pursuit, have become popular in optimization due to their natural tendency to propose sparse solutions and their refined convergence guarantees.","In this work, we propose a principled approach to generating (regularized) matching pursuit algorithms adapted to the geometry of the problem at hand, as well as their convergence guarantees.","Building on these results, we derive approximate convergence guarantees and describe a transition phenomenon in the convergence of (regularized) matching pursuit from underparametrized to overparametrized models."],"url":"http://arxiv.org/abs/2403.03072v1","category":"math.OC"}
{"created":"2024-03-05 15:49:33","title":"When Industry meets Trustworthy AI: A Systematic Review of AI for Industry 5.0","abstract":"Industry is at the forefront of adopting new technologies, and the process followed by the adoption has a significant impact on the economy and society. In this work, we focus on analysing the current paradigm in which industry evolves, making it more sustainable and Trustworthy. In Industry 5.0, Artificial Intelligence (AI), among other technology enablers, is used to build services from a sustainable, human-centric and resilient perspective. It is crucial to understand those aspects that can bring AI to industry, respecting Trustworthy principles by collecting information to define how it is incorporated in the early stages, its impact, and the trends observed in the field. In addition, to understand the challenges and gaps in the transition from Industry 4.0 to Industry 5.0, a general perspective on the industry's readiness for new technologies is described. This provides practitioners with novel opportunities to be explored in pursuit of the adoption of Trustworthy AI in the sector.","sentences":["Industry is at the forefront of adopting new technologies, and the process followed by the adoption has a significant impact on the economy and society.","In this work, we focus on analysing the current paradigm in which industry evolves, making it more sustainable and Trustworthy.","In Industry 5.0, Artificial Intelligence (AI), among other technology enablers, is used to build services from a sustainable, human-centric and resilient perspective.","It is crucial to understand those aspects that can bring AI to industry, respecting Trustworthy principles by collecting information to define how it is incorporated in the early stages, its impact, and the trends observed in the field.","In addition, to understand the challenges and gaps in the transition from Industry 4.0 to Industry 5.0, a general perspective on the industry's readiness for new technologies is described.","This provides practitioners with novel opportunities to be explored in pursuit of the adoption of Trustworthy AI in the sector."],"url":"http://arxiv.org/abs/2403.03061v1","category":"cs.CY"}
{"created":"2024-03-05 15:48:54","title":"Exploring Time Delay Interferometry Ranging as a Practical Ranging Technique in the Bayesian Framework","abstract":"Knowledge on the inter-spacecraft optical paths (i.e. delays) is one of the key elements of time delay interferometry (TDI). Conventional method for inter-spacecraft ranging mainly relies on the pseudo-random noise (PRN) code signal modulated onto the lasers. To ensure the reliability and robustness of this ranging information, it would be highly beneficial to develop other methods which could serve as cross-validations or backups. This paper explores the practical implementation of an alternative data-driven approach - time delay interferometry ranging (TDIR) - as a ranging technique independent of the PRN signal. Distinguished from previous research, the inputs of our TDIR algorithm are interferometric measurements that have only undergone a 0.1 ms accuracy preliminary clock synchronization using the ground tracking data. This significantly relaxes the stringent requirement for clock synchronization imposed by traditional TDI procedure. In order to encode the information of time-varying light travel times and clock desynchronizations, we adopt a general polynomial parametrization to represent the delays. By framing TDIR as a Bayesian parameter estimation problem, we demonstrate our algorithm with simulated data based on the numerical orbit of Taiji. In the presence of laser frequency noise and secondary noises, the estimated median values of parameters exhibit a bias of only 5.28 ns from the ''true'' delays, capable of suppressing laser frequency noise to the desired level. Furthermore, we have also analysed the requirements of mitigating optical bench noise and clock noise on TDIR, and presented an illustrative example for the impact of laser locking.","sentences":["Knowledge on the inter-spacecraft optical paths (i.e. delays) is one of the key elements of time delay interferometry (TDI).","Conventional method for inter-spacecraft ranging mainly relies on the pseudo-random noise (PRN) code signal modulated onto the lasers.","To ensure the reliability and robustness of this ranging information, it would be highly beneficial to develop other methods which could serve as cross-validations or backups.","This paper explores the practical implementation of an alternative data-driven approach - time delay interferometry ranging (TDIR) - as a ranging technique independent of the PRN signal.","Distinguished from previous research, the inputs of our TDIR algorithm are interferometric measurements that have only undergone a 0.1 ms accuracy preliminary clock synchronization using the ground tracking data.","This significantly relaxes the stringent requirement for clock synchronization imposed by traditional TDI procedure.","In order to encode the information of time-varying light travel times and clock desynchronizations, we adopt a general polynomial parametrization to represent the delays.","By framing TDIR as a Bayesian parameter estimation problem, we demonstrate our algorithm with simulated data based on the numerical orbit of Taiji.","In the presence of laser frequency noise and secondary noises, the estimated median values of parameters exhibit a bias of only 5.28 ns from the ''true'' delays, capable of suppressing laser frequency noise to the desired level.","Furthermore, we have also analysed the requirements of mitigating optical bench noise and clock noise on TDIR, and presented an illustrative example for the impact of laser locking."],"url":"http://arxiv.org/abs/2403.03060v1","category":"astro-ph.IM"}
{"created":"2024-03-05 15:37:34","title":"Bounds for the independence and chromatic numbers of locally sparse graphs","abstract":"In this note we consider a more general version of local sparsity introduced recently by Anderson, Kuchukova, and the author. In particular, we say a graph $G = (V, E)$ is $(k, r)$-locally-sparse if for each vertex $v \\in V(G)$, the subgraph induced by its neighborhood contains at most $k$ cliques of size $r$. For $r \\geq 3$ and $\\epsilon \\in [0, 1]$, we show that a $(\\Delta^{\\epsilon r}, r)$-locally-sparse graph $G$ of maximum degree $\\Delta$ satisfies $\\alpha(G) = \\Omega\\left(\\dfrac{n}{\\gamma\\Delta}\\right)$ and $\\chi(G) = O\\left(\\gamma\\Delta\\right)$, where $\\gamma :=\\max\\left\\{\\epsilon,\\,\\dfrac{r\\log\\log \\Delta}{\\log \\Delta}\\right\\}$. As $K_{r+1}$-free graphs are $(k, r)$-locally-sparse for any $k$, we asymptotically recover classical results of Shearer and Johansson by setting $\\epsilon = 0$. We prove a stronger bound on the independence number in terms of the average degree, and establish a local version of the coloring result in the more general setting of correspondence coloring.","sentences":["In this note we consider a more general version of local sparsity introduced recently by Anderson, Kuchukova, and the author.","In particular, we say a graph $G = (V, E)$ is $(k, r)$-locally-sparse if for each vertex $v \\in V(G)$, the subgraph induced by its neighborhood contains at most $k$ cliques of size $r$. For $r \\geq 3$ and $\\epsilon \\in [0, 1]$, we show that a $(\\Delta^{\\epsilon r}, r)$-locally-sparse graph $G$ of maximum degree $\\Delta$ satisfies $\\alpha(G) = \\Omega\\left(\\dfrac{n}{\\gamma\\Delta}\\right)$ and $\\chi(G) = O\\left(\\gamma\\Delta\\right)$, where $\\gamma :=\\max\\left\\{\\epsilon,\\,\\dfrac{r\\log\\log \\Delta}{\\log \\Delta}\\right\\}$.","As $K_{r+1}$-free graphs are $(k, r)$-locally-sparse for any $k$, we asymptotically recover classical results of Shearer and Johansson by setting $\\epsilon = 0$.","We prove a stronger bound on the independence number in terms of the average degree, and establish a local version of the coloring result in the more general setting of correspondence coloring."],"url":"http://arxiv.org/abs/2403.03054v1","category":"math.CO"}
{"created":"2024-03-05 15:37:06","title":"Neural Codebook Design for Network Beam Management","abstract":"Obtaining accurate and timely channel state information (CSI) is a fundamental challenge for large antenna systems. Mobile systems like 5G use a beam management framework that joins the initial access, beamforming, CSI acquisition, and data transmission. The design of codebooks for these stages, however, is challenging due to their interrelationships, varying array sizes, and site-specific channel and user distributions. Furthermore, beam management is often focused on single-sector operations while ignoring the overarching network- and system-level optimization. In this paper, we proposed an end-to-end learned codebook design algorithm, network beamspace learning (NBL), that captures and optimizes codebooks to mitigate interference while maximizing the achievable performance with extremely large hybrid arrays. The proposed algorithm requires limited shared information yet designs codebooks that outperform traditional codebooks by over 10dB in beam alignment and achieve more than 25% improvements in network spectral efficiency.","sentences":["Obtaining accurate and timely channel state information (CSI) is a fundamental challenge for large antenna systems.","Mobile systems like 5G use a beam management framework that joins the initial access, beamforming, CSI acquisition, and data transmission.","The design of codebooks for these stages, however, is challenging due to their interrelationships, varying array sizes, and site-specific channel and user distributions.","Furthermore, beam management is often focused on single-sector operations while ignoring the overarching network- and system-level optimization.","In this paper, we proposed an end-to-end learned codebook design algorithm, network beamspace learning (NBL), that captures and optimizes codebooks to mitigate interference while maximizing the achievable performance with extremely large hybrid arrays.","The proposed algorithm requires limited shared information yet designs codebooks that outperform traditional codebooks by over 10dB in beam alignment and achieve more than 25% improvements in network spectral efficiency."],"url":"http://arxiv.org/abs/2403.03053v1","category":"eess.SP"}
{"created":"2024-03-05 15:36:11","title":"Simulation of Chemical Reactions on a Quantum Computer","abstract":"Studying chemical reactions, particularly in the gas phase, relies heavily on computing scattering matrix elements. These elements are essential for characterizing molecular reactions and accurately determining reaction probabilities. However, the intricate nature of quantum interactions poses challenges, necessitating the use of advanced mathematical models and computational approaches to tackle the inherent complexities. In this study, we develop and apply a quantum algorithm for the calculation of scattering matrix elements. In our approach, we employ the time-dependent method based on the M\\\"oller operator formulation where the S-matrix element between the respective reactant and product channels is determined through the time correlation function of the reactant and product M\\\"oller wavepackets. We successfully apply our quantum algorithm to calculate scattering matrix elements for 1D semi-infinite square well potential and on the co-linear hydrogen exchange reaction. As we navigate the complexities of quantum interactions, this quantum algorithm is general and emerges as a promising avenue, shedding light on new possibilities for simulating chemical reactions on quantum computers.","sentences":["Studying chemical reactions, particularly in the gas phase, relies heavily on computing scattering matrix elements.","These elements are essential for characterizing molecular reactions and accurately determining reaction probabilities.","However, the intricate nature of quantum interactions poses challenges, necessitating the use of advanced mathematical models and computational approaches to tackle the inherent complexities.","In this study, we develop and apply a quantum algorithm for the calculation of scattering matrix elements.","In our approach, we employ the time-dependent method based on the M\\\"oller operator formulation where the S-matrix element between the respective reactant and product channels is determined through the time correlation function of the reactant and product M\\\"oller wavepackets.","We successfully apply our quantum algorithm to calculate scattering matrix elements for 1D semi-infinite square well potential and on the co-linear hydrogen exchange reaction.","As we navigate the complexities of quantum interactions, this quantum algorithm is general and emerges as a promising avenue, shedding light on new possibilities for simulating chemical reactions on quantum computers."],"url":"http://arxiv.org/abs/2403.03052v1","category":"quant-ph"}
{"created":"2024-03-05 15:34:41","title":"Prediction of turbulent channel flow using Fourier neural operator-based machine-learning strategy","abstract":"Fast and accurate predictions of turbulent flows are of great importance in the science and engineering field. In this paper, we investigate the implicit U-Net enhanced Fourier neural operator (IUFNO) in the stable prediction of long-time dynamics of three-dimensional (3D) turbulent channel flows. The trained IUFNO models are tested in the large-eddy simulations (LES) at coarse grids for three friction Reynolds numbers: $Re_{\\tau}\\approx180$, $395$ and $590$. The adopted near-wall mesh grids are tangibly coarser than the general requirements for wall-resolved LES. The numerical experiments show that the IUFNO framework outperforms the traditional dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model in the predictions of a variety of flow statistics and structures, including the mean and fluctuating velocities, the probability density functions (PDFs) and joint PDF of velocity fluctuations, the Reynolds stress profile, the kinetic energy spectrum, and the Q-criterion (vortex structures). Meanwhile, the trained IUFNO models are computationally much faster than the traditional LES models. Thus, the IUFNO is a promising approach for the fast prediction of wall-bounded turbulent flow.","sentences":["Fast and accurate predictions of turbulent flows are of great importance in the science and engineering field.","In this paper, we investigate the implicit U-Net enhanced Fourier neural operator (IUFNO) in the stable prediction of long-time dynamics of three-dimensional (3D) turbulent channel flows.","The trained IUFNO models are tested in the large-eddy simulations (LES) at coarse grids for three friction Reynolds numbers: $Re_{\\tau}\\approx180$, $395$ and $590$. The adopted near-wall mesh grids are tangibly coarser than the general requirements for wall-resolved LES.","The numerical experiments show that the IUFNO framework outperforms the traditional dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model in the predictions of a variety of flow statistics and structures, including the mean and fluctuating velocities, the probability density functions (PDFs) and joint PDF of velocity fluctuations, the Reynolds stress profile, the kinetic energy spectrum, and the Q-criterion (vortex structures).","Meanwhile, the trained IUFNO models are computationally much faster than the traditional LES models.","Thus, the IUFNO is a promising approach for the fast prediction of wall-bounded turbulent flow."],"url":"http://arxiv.org/abs/2403.03051v1","category":"physics.flu-dyn"}
{"created":"2024-03-05 15:33:30","title":"Some solutions to the eigenstate equation of free scalar quantum field theory in the Schr\u00f6dinger representation","abstract":"Using extensions of the quadratic form in the potential term we construct Gaussian eigenstates of the free scalar quantum Hamiltonian operator acting in nontrivial functional space. Admissible positive closed extensions are generated at least by two external sources, the distance between them being limited by the extension parameter. The constructed functionals, in contrast to the ground state of the free theory, correspond to different boundary conditions and can be interpreted as eigenstates of some renormalized asymptotically free quantum Hamiltonian.","sentences":["Using extensions of the quadratic form in the potential term we construct Gaussian eigenstates of the free scalar quantum Hamiltonian operator acting in nontrivial functional space.","Admissible positive closed extensions are generated at least by two external sources, the distance between them being limited by the extension parameter.","The constructed functionals, in contrast to the ground state of the free theory, correspond to different boundary conditions and can be interpreted as eigenstates of some renormalized asymptotically free quantum Hamiltonian."],"url":"http://arxiv.org/abs/2403.03049v1","category":"math-ph"}
{"created":"2024-03-05 15:23:01","title":"Generalization of composite dynamics for the lattice Boltzmann method","abstract":"A generalized composite dynamics (GCD) framework is presented for the lattice Boltzmann method (LBM). It leverages synergies of various simple collision operators to model complex physical processes. The framework is validated by recovering semi-permeable fluid flow methods. It provides mathematical justification for the incorporation of forces, addressing a gap in the literature. The versatility of this framework is demonstrated by analyzing a flux boundary condition. The GCD framework opens innovative possibilities for simulating complex physical processes using LBM.","sentences":["A generalized composite dynamics (GCD) framework is presented for the lattice Boltzmann method (LBM).","It leverages synergies of various simple collision operators to model complex physical processes.","The framework is validated by recovering semi-permeable fluid flow methods.","It provides mathematical justification for the incorporation of forces, addressing a gap in the literature.","The versatility of this framework is demonstrated by analyzing a flux boundary condition.","The GCD framework opens innovative possibilities for simulating complex physical processes using LBM."],"url":"http://arxiv.org/abs/2403.03042v1","category":"physics.flu-dyn"}
{"created":"2024-03-05 15:19:53","title":"A hybrid optimization framework for the General Continuous Energy-Constrained Scheduling Problem","abstract":"We present a hybrid optimization framework for a class of problems, formalized as a generalization of the Continuous Energy-Con\\-strained Scheduling Problem (CECSP), introduced by Nattaf et al. (2014). This class is obtained from challenges concerning demand response in energy networks. Our framework extends a previously developed approach. A set of jobs has to be processed on a continuous, shared resource. Consequently, a schedule for a job does not only contain a start and completion time, but also a resource consumption profile, where we have to respect lower and upper bounds on resource consumption during processing. In this work, we develop a hybrid approach for the case where the objective is a step-wise increasing function of completion time, using local search, linear programming and O(n) lower bounds. We exploit that the costs are known in the local search and use bounds to assess feasibility more efficiently than by LP. We compare its performance to a mixed-integer linear program. After that, we extend this to a hybrid optimization framework for the General CECSP. This uses an event-based model, and applies a decomposition in two parts: 1) determining the order of events and 2) finding the event times, and hence the start and completion times of jobs, together with the resource consumption profiles. We argue the broad applicability of this framework.","sentences":["We present a hybrid optimization framework for a class of problems, formalized as a generalization of the Continuous Energy-Con\\-strained Scheduling Problem (CECSP), introduced by Nattaf et al. (2014).","This class is obtained from challenges concerning demand response in energy networks.","Our framework extends a previously developed approach.","A set of jobs has to be processed on a continuous, shared resource.","Consequently, a schedule for a job does not only contain a start and completion time, but also a resource consumption profile, where we have to respect lower and upper bounds on resource consumption during processing.","In this work, we develop a hybrid approach for the case where the objective is a step-wise increasing function of completion time, using local search, linear programming and O(n) lower bounds.","We exploit that the costs are known in the local search and use bounds to assess feasibility more efficiently than by LP.","We compare its performance to a mixed-integer linear program.","After that, we extend this to a hybrid optimization framework for the General CECSP.","This uses an event-based model, and applies a decomposition in two parts: 1) determining the order of events and 2) finding the event times, and hence the start and completion times of jobs, together with the resource consumption profiles.","We argue the broad applicability of this framework."],"url":"http://arxiv.org/abs/2403.03039v1","category":"math.OC"}
{"created":"2024-03-05 15:18:14","title":"Transition from topological to chaos in the nonlinear Su-Schrieffer-Heeger model","abstract":"Recent studies on topological insulators have expanded into the nonlinear regime, while the bulk-edge correspondence in strongly nonlinear systems has been unelucidated. Here, we reveal that nonlinear topological edge modes can exhibit a transition to spatial chaos by increasing nonlinearity, which can be a universal mechanism of the breakdown of the bulk-edge correspondence. Specifically, we unveil the underlying dynamical system describing the spatial distribution of zero modes and show the emergence of chaos. We also propose the correspondence between the absolute value of the topological invariant and the dimension of the stable manifold under sufficiently weak nonlinearity. Our results provide a general guiding principle to investigate the nonlinear bulk-edge correspondence that can potentially be extended to arbitrary dimensions.","sentences":["Recent studies on topological insulators have expanded into the nonlinear regime, while the bulk-edge correspondence in strongly nonlinear systems has been unelucidated.","Here, we reveal that nonlinear topological edge modes can exhibit a transition to spatial chaos by increasing nonlinearity, which can be a universal mechanism of the breakdown of the bulk-edge correspondence.","Specifically, we unveil the underlying dynamical system describing the spatial distribution of zero modes and show the emergence of chaos.","We also propose the correspondence between the absolute value of the topological invariant and the dimension of the stable manifold under sufficiently weak nonlinearity.","Our results provide a general guiding principle to investigate the nonlinear bulk-edge correspondence that can potentially be extended to arbitrary dimensions."],"url":"http://arxiv.org/abs/2403.03038v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 15:18:02","title":"A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives","abstract":"Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods.","sentences":["Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once.","We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills.","To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills.","We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed.","We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.03037v1","category":"cs.CV"}
{"created":"2024-03-05 15:14:32","title":"Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems","abstract":"We introduce Mars 2.0 for modeling, analysis, verification and code generation of Cyber-Physical Systems. Mars 2.0 integrates Mars 1.0 with several important extensions and improvements, allowing the design of cyber-physical systems using the combination of AADL and Simulink/Stateflow, which provide a unified graphical framework for modeling the functionality, physicality and architecture of the system to be developed. For a safety-critical system, formal analysis and verification of its combined AADL and Simulink/Stateflow model can be conducted via the following steps. First, the toolchain automatically translates AADL and Simulink/Stateflow models into Hybrid CSP (HCSP), an extension of CSP for formally modeling hybrid systems. Second, the HCSP processes can be simulated using the HCSP simulator, and to complement incomplete simulation, they can be verified using the Hybrid Hoare Logic prover in Isabelle/HOL, as well as the more automated HHLPy prover. Finally, implementations in SystemC or C can be automatically generated from the verified HCSP processes. The transformation from AADL and Simulink/Stateflow to HCSP, and the one from HCSP to SystemC or C, are both guaranteed to be correct with formal proofs. This approach allows model-driven design of safety-critical cyber-physical systems based on graphical and formal models and proven-correct translation procedures. We demonstrate the use of the toolchain on several benchmarks of varying complexity, including several industrial-sized examples.","sentences":["We introduce Mars 2.0 for modeling, analysis, verification and code generation of Cyber-Physical Systems.","Mars 2.0 integrates Mars 1.0 with several important extensions and improvements, allowing the design of cyber-physical systems using the combination of AADL and Simulink/Stateflow, which provide a unified graphical framework for modeling the functionality, physicality and architecture of the system to be developed.","For a safety-critical system, formal analysis and verification of its combined AADL and Simulink/Stateflow model can be conducted via the following steps.","First, the toolchain automatically translates AADL and Simulink/Stateflow models into Hybrid CSP (HCSP), an extension of CSP for formally modeling hybrid systems.","Second, the HCSP processes can be simulated using the HCSP simulator, and to complement incomplete simulation, they can be verified using the Hybrid Hoare Logic prover in Isabelle/HOL, as well as the more automated HHLPy prover.","Finally, implementations in SystemC or C can be automatically generated from the verified HCSP processes.","The transformation from AADL and Simulink/Stateflow to HCSP, and the one from HCSP to SystemC or C, are both guaranteed to be correct with formal proofs.","This approach allows model-driven design of safety-critical cyber-physical systems based on graphical and formal models and proven-correct translation procedures.","We demonstrate the use of the toolchain on several benchmarks of varying complexity, including several industrial-sized examples."],"url":"http://arxiv.org/abs/2403.03035v1","category":"cs.PL"}
{"created":"2024-03-05 15:11:05","title":"Logic Programming with Multiplicative Structures","abstract":"In the logic programming paradigm, a program is defined by a set of methods, each of which can be executed when specific conditions are met during the current state of an execution. The semantics of these programs can be elegantly represented using sequent calculi, in which each method is linked to an inference rule. In this context, proof search mirrors the program's execution. Previous works introduced a framework in which the process of constructing proof nets is employed to model executions, as opposed to the traditional approach of proof search in sequent calculus.   This paper further extends this investigation by focussing on the pure multiplicative fragment of this framework. We demonstrate, providing practical examples, the capability to define logic programming methods with context-sensitive behaviors solely through specific resource-preserving and context-free operations, corresponding to certain generalized multiplicative connectives explored in existing literature. We show how some of these methods, although still multiplicative, escape the purely multiplicative fragment of Linear Logic (MLL).","sentences":["In the logic programming paradigm, a program is defined by a set of methods, each of which can be executed when specific conditions are met during the current state of an execution.","The semantics of these programs can be elegantly represented using sequent calculi, in which each method is linked to an inference rule.","In this context, proof search mirrors the program's execution.","Previous works introduced a framework in which the process of constructing proof nets is employed to model executions, as opposed to the traditional approach of proof search in sequent calculus.   ","This paper further extends this investigation by focussing on the pure multiplicative fragment of this framework.","We demonstrate, providing practical examples, the capability to define logic programming methods with context-sensitive behaviors solely through specific resource-preserving and context-free operations, corresponding to certain generalized multiplicative connectives explored in existing literature.","We show how some of these methods, although still multiplicative, escape the purely multiplicative fragment of Linear Logic (MLL)."],"url":"http://arxiv.org/abs/2403.03032v1","category":"cs.LO"}
{"created":"2024-03-05 15:06:16","title":"Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs","abstract":"This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint. By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed. The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios. Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula. The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function. Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality. Simulations validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system.","sentences":["This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint.","By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed.","The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios.","Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula.","The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function.","Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality.","Simulations validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system."],"url":"http://arxiv.org/abs/2403.03030v1","category":"eess.SY"}
{"created":"2024-03-05 15:05:06","title":"Socratic Reasoning Improves Positive Text Rewriting","abstract":"Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \\textsc{SocraticReframe}. \\textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both automatic and human evaluations guided by criteria from psychotherapy research.","sentences":["Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions.","Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive.","However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step.","In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \\textsc{SocraticReframe}.","\\textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process.","We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both automatic and human evaluations guided by criteria from psychotherapy research."],"url":"http://arxiv.org/abs/2403.03029v1","category":"cs.CL"}
{"created":"2024-03-05 15:04:18","title":"Word Importance Explains How Prompts Affect Language Model Outputs","abstract":"The emergence of large language models (LLMs) has revolutionized numerous applications across industries. However, their \"black box\" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc. This procedure also enables measuring impact when attention weights are not available. To test the fidelity of this approach, we explore the effect of adding different suffixes to multiple different system prompts and comparing subsequent generations with different large language models. Results show that word importance scores are closely related to the expected suffix importances for multiple scoring functions.","sentences":["The emergence of large language models (LLMs) has revolutionized numerous applications across industries.","However, their \"black box\" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use.","This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs.","This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs.","Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc.","This procedure also enables measuring impact when attention weights are not available.","To test the fidelity of this approach, we explore the effect of adding different suffixes to multiple different system prompts and comparing subsequent generations with different large language models.","Results show that word importance scores are closely related to the expected suffix importances for multiple scoring functions."],"url":"http://arxiv.org/abs/2403.03028v1","category":"cs.AI"}
{"created":"2024-03-05 15:03:08","title":"Assessing the similarity of continuous gravitational-wave signals to narrow instrumental artifacts","abstract":"Continuous gravitational-wave signals (CWs) are long-lasting quasi-monochromatic gravitational-wave signals expected to be emitted by rapidly-rotating non-axisymmetric neutron stars. Depending on the rotational frequency and sky location of the source, certain CW signals may behave in a similar manner to narrow-band artifacts present in ground-based interferometric detectors. Part of the detector-characterisation tasks in the current generation of interferometric detectors (Advanced LIGO, Advanced Virgo, and KAGRA) aim at understanding the origin of these narrow artifacts, commonly known as ``spectral lines''. It is expected that similar tasks will continue after the arrival of next-generation detectors (e.g. Einstein Telescope and Cosmic Explorer). Typically, a fraction of the observed lines in a given detector can be associated to one or more instrumental causes; others, however, have an unknown origin. In this work, we assess the similarity of CW signals to spectral lines in order to understand whether a CW may be mistaken for a noise artifact. Albeit astrophysically unlikely, our results do not rule out the possibility of a CW signal being visible in the detector's power spectrum.","sentences":["Continuous gravitational-wave signals (CWs) are long-lasting quasi-monochromatic gravitational-wave signals expected to be emitted by rapidly-rotating non-axisymmetric neutron stars.","Depending on the rotational frequency and sky location of the source, certain CW signals may behave in a similar manner to narrow-band artifacts present in ground-based interferometric detectors.","Part of the detector-characterisation tasks in the current generation of interferometric detectors (Advanced LIGO, Advanced Virgo, and KAGRA) aim at understanding the origin of these narrow artifacts, commonly known as ``spectral lines''.","It is expected that similar tasks will continue after the arrival of next-generation detectors (e.g. Einstein Telescope and Cosmic Explorer).","Typically, a fraction of the observed lines in a given detector can be associated to one or more instrumental causes; others, however, have an unknown origin.","In this work, we assess the similarity of CW signals to spectral lines in order to understand whether a CW may be mistaken for a noise artifact.","Albeit astrophysically unlikely, our results do not rule out the possibility of a CW signal being visible in the detector's power spectrum."],"url":"http://arxiv.org/abs/2403.03027v1","category":"gr-qc"}
{"created":"2024-03-05 14:58:55","title":"Polytropic stellar structure in 5$\\mathcal{D}$ Einstein-Gauss-Bonnet gravity","abstract":"Polytropic stars are useful tools to learn about the stellar structure without the complexity of comprehensive stellar models. These models rely on a certain power-law correlation between the star's pressure and density. This paper proposes a polytropic star model to investigate some new features in the context of $5\\mathcal{D}$ Einstein-Gauss-Bonnet (EGB) gravity using the Finch-Skea {\\em ansatz} [{\\em M. R. Finch and J. E. Skea, Classical and Quantum Gravity 6, 467 (1989)}]. Analytical results are better described by graphical representations of the physical parameters for various values of the coupling parameter $\\alpha$. The solution for a specific compact object, EXO 1785 - 248, with radius $\\mathfrak{R} = 8.849_{-0.04}^{+0.04}$ km and mass $\\mathcal{M} = 1.3 \\pm 0.02~\\mathcal{M}_{\\odot}$, is shown here. We analyze the essential physical attributes of the star, which reveal the influence of the coupling parameter $\\alpha$ on the values of substance parameters. Ultimately, we have concluded that our current model is realistic because it satisfies all the physical criteria for an acceptable model.","sentences":["Polytropic stars are useful tools to learn about the stellar structure without the complexity of comprehensive stellar models.","These models rely on a certain power-law correlation between the star's pressure and density.","This paper proposes a polytropic star model to investigate some new features in the context of $5\\mathcal{D}$ Einstein-Gauss-Bonnet (EGB) gravity using the Finch-Skea {\\em ansatz} [{\\em M. R. Finch and J. E. Skea, Classical and Quantum Gravity 6, 467 (1989)}].","Analytical results are better described by graphical representations of the physical parameters for various values of the coupling parameter $\\alpha$.","The solution for a specific compact object, EXO 1785 - 248, with radius $\\mathfrak{R} = 8.849_{-0.04}^{+0.04}$ km and mass $\\mathcal{M} = 1.3 \\pm 0.02~\\mathcal{M}_{\\odot}$, is shown here.","We analyze the essential physical attributes of the star, which reveal the influence of the coupling parameter $\\alpha$ on the values of substance parameters.","Ultimately, we have concluded that our current model is realistic because it satisfies all the physical criteria for an acceptable model."],"url":"http://arxiv.org/abs/2403.03026v1","category":"gr-qc"}
{"created":"2024-03-05 14:57:13","title":"Accelerating the convergence of Newton's method for nonlinear elliptic PDEs using Fourier neural operators","abstract":"It is well known that Newton's method, especially when applied to large problems such as the discretization of nonlinear partial differential equations (PDEs), can have trouble converging if the initial guess is too far from the solution. This work focuses on accelerating this convergence, in the context of the discretization of nonlinear elliptic PDEs. We first provide a quick review of existing methods, and justify our choice of learning an initial guess with a Fourier neural operator (FNO). This choice was motivated by the mesh-independence of such operators, whose training and evaluation can be performed on grids with different resolutions. The FNO is trained using a loss minimization over generated data, loss functions based on the PDE discretization. Numerical results, in one and two dimensions, show that the proposed initial guess accelerates the convergence of Newton's method by a large margin compared to a naive initial guess, especially for highly nonlinear or anisotropic problems.","sentences":["It is well known that Newton's method, especially when applied to large problems such as the discretization of nonlinear partial differential equations (PDEs), can have trouble converging if the initial guess is too far from the solution.","This work focuses on accelerating this convergence, in the context of the discretization of nonlinear elliptic PDEs.","We first provide a quick review of existing methods, and justify our choice of learning an initial guess with a Fourier neural operator (FNO).","This choice was motivated by the mesh-independence of such operators, whose training and evaluation can be performed on grids with different resolutions.","The FNO is trained using a loss minimization over generated data, loss functions based on the PDE discretization.","Numerical results, in one and two dimensions, show that the proposed initial guess accelerates the convergence of Newton's method by a large margin compared to a naive initial guess, especially for highly nonlinear or anisotropic problems."],"url":"http://arxiv.org/abs/2403.03021v1","category":"math.NA"}
{"created":"2024-03-05 14:57:04","title":"SplAgger: Split Aggregation for Meta-Reinforcement Learning","abstract":"A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models with permutation invariant aggregation, which exploit the fact that, due to the Markov property, the task posterior does not depend on the order of data. We empirically confirm the advantage of permutation invariant sequence models without the use of task inference objectives. However, we also find, surprisingly, that there are multiple conditions under which permutation variance remains useful. Therefore, we propose SplAgger, which uses both permutation variant and invariant components to achieve the best of both worlds, outperforming all baselines on continuous control and memory environments.","sentences":["A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks.","Meta-RL aims to achieve this by directly learning such agents.","One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end.","In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task.","These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods.","However, recent evidence suggests that task inference objectives are unnecessary in practice.","Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not.","In this paper, we present strong evidence that task inference sequence models are still beneficial.","In particular, we investigate sequence models with permutation invariant aggregation, which exploit the fact that, due to the Markov property, the task posterior does not depend on the order of data.","We empirically confirm the advantage of permutation invariant sequence models without the use of task inference objectives.","However, we also find, surprisingly, that there are multiple conditions under which permutation variance remains useful.","Therefore, we propose SplAgger, which uses both permutation variant and invariant components to achieve the best of both worlds, outperforming all baselines on continuous control and memory environments."],"url":"http://arxiv.org/abs/2403.03020v1","category":"cs.LG"}
{"created":"2024-03-05 14:53:53","title":"OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following","abstract":"Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.","sentences":["Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions.","Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF.","Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance.","To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor.","Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance.","Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance.","Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance."],"url":"http://arxiv.org/abs/2403.03017v1","category":"cs.AI"}
{"created":"2024-03-05 14:52:09","title":"Wavelet Scattering Networks for Identifying Radio Galaxy Morphologies","abstract":"Classifying the morphologies of radio galaxies is important to understand their physical properties and evolutionary histories. A galaxy's morphology is often determined by visual inspection, but as survey size increases robust automated techniques will be needed. Deep neural networks are an attractive method for automated classification, but have many free parameters and therefore require extensive training data and are subject to overfitting and generalization issues. We explore hybrid classification methods using the scattering transform, the recursive wavelet decomposition of an input image. We analyse the performance of the scattering transform for the Fanaroff-Riley classification of radio galaxies with respect to CNNs and other machine learning algorithms. We test the robustness of the different classification methods with training data truncation and noise injection, and find that the scattering transform can offer competitive performance with the most accurate CNNs.","sentences":["Classifying the morphologies of radio galaxies is important to understand their physical properties and evolutionary histories.","A galaxy's morphology is often determined by visual inspection, but as survey size increases robust automated techniques will be needed.","Deep neural networks are an attractive method for automated classification, but have many free parameters and therefore require extensive training data and are subject to overfitting and generalization issues.","We explore hybrid classification methods using the scattering transform, the recursive wavelet decomposition of an input image.","We analyse the performance of the scattering transform for the Fanaroff-Riley classification of radio galaxies with respect to CNNs and other machine learning algorithms.","We test the robustness of the different classification methods with training data truncation and noise injection, and find that the scattering transform can offer competitive performance with the most accurate CNNs."],"url":"http://arxiv.org/abs/2403.03016v1","category":"astro-ph.IM"}
{"created":"2024-03-05 14:51:46","title":"Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split","abstract":"To support extremely high data rates, reconfigurable intelligent surface (RIS)-assisted terahertz (THz) communication is considered to be a promising technology for future sixth-generation networks. However, due to the typical employment of hybrid beamforming architecture in THz systems, as well as the passive nature of RIS which lacks the capability to process pilot signals, obtaining channel state information (CSI) is facing significant challenges. To accurately estimate the cascaded channel, we propose a novel low-complexity channel estimation scheme, which includes three steps. Specifically, we first estimate full CSI within a small subset of subcarriers (SCs). Then, we acquire angular information at base station and RIS based on the full CSI. Finally, we derive spatial directions and recover full-CSI for the remaining SCs. Theoretical analysis and simulation results demonstrate that the proposed scheme can achieve superior performance in terms of normalized mean-square-error and exhibit a lower computational complexity compared with the existing algorithms.","sentences":["To support extremely high data rates, reconfigurable intelligent surface (RIS)-assisted terahertz (THz) communication is considered to be a promising technology for future sixth-generation networks.","However, due to the typical employment of hybrid beamforming architecture in THz systems, as well as the passive nature of RIS which lacks the capability to process pilot signals, obtaining channel state information (CSI) is facing significant challenges.","To accurately estimate the cascaded channel, we propose a novel low-complexity channel estimation scheme, which includes three steps.","Specifically, we first estimate full CSI within a small subset of subcarriers (SCs).","Then, we acquire angular information at base station and RIS based on the full CSI.","Finally, we derive spatial directions and recover full-CSI for the remaining SCs.","Theoretical analysis and simulation results demonstrate that the proposed scheme can achieve superior performance in terms of normalized mean-square-error and exhibit a lower computational complexity compared with the existing algorithms."],"url":"http://arxiv.org/abs/2403.03015v1","category":"cs.IT"}
{"created":"2024-03-05 14:49:26","title":"Global N-body Simulation of Gap Edge Structures Created by Perturbations from a Small Satellite Embedded in Saturn's Rings","abstract":"Observations by the Voyager and Cassini spacecrafts have revealed various striking features of the gap structure in Saturn's ring, such as the density waves, sharp edge, and vertical wall structure. In order to explain these features in a single simulation, we perform a high-resolution (N~10^6-10^7) global full N-body simulation of gap formation by an embedded satellite considering gravitational interactions and inelastic collisions among all ring particles and the satellite, while these features have been mostly investigated separately with different theoretical approaches: the streamline models, 1D diffusion models, and local N-body simulation. As a first attempt of a series of papers, we here focus on the gap formation by separating satellite migration with fixing the satellite orbit in a Keplerian circular orbit. We reveal how the striking gap features - the density waves, sharp edge, and vertical wall structure - are simultaneously formed by an interplay of the satellite-ring and ring particle-particle interactions. In particular, we propose a new mechanism to quantitatively explain the creation of the vertical wall structure at the gap edge. Inelastic collisions between ring particles damp their eccentricity excited by the satellite's perturbations to enhance the surface density at the gap edge, making its sharp edges more pronounced. We find the eccentricity damping process inevitably raises the vertical wall structures the most effectively in the second epicycle waves. Particle-particle collisions generally convert their lateral epicyclic motion into vertical motion. Because the excited epicyclic motion is the greatest near the ring edge and the epicycle motions are aligned in the first waves, the conversion is the most efficient in the gap edge of the second waves and the wall height is scaled by the satellite Hill radius, which are consistent with the observations.","sentences":["Observations by the Voyager and Cassini spacecrafts have revealed various striking features of the gap structure in Saturn's ring, such as the density waves, sharp edge, and vertical wall structure.","In order to explain these features in a single simulation, we perform a high-resolution (N~10^6-10^7) global full N-body simulation of gap formation by an embedded satellite considering gravitational interactions and inelastic collisions among all ring particles and the satellite, while these features have been mostly investigated separately with different theoretical approaches: the streamline models, 1D diffusion models, and local N-body simulation.","As a first attempt of a series of papers, we here focus on the gap formation by separating satellite migration with fixing the satellite orbit in a Keplerian circular orbit.","We reveal how the striking gap features - the density waves, sharp edge, and vertical wall structure - are simultaneously formed by an interplay of the satellite-ring and ring particle-particle interactions.","In particular, we propose a new mechanism to quantitatively explain the creation of the vertical wall structure at the gap edge.","Inelastic collisions between ring particles damp their eccentricity excited by the satellite's perturbations to enhance the surface density at the gap edge, making its sharp edges more pronounced.","We find the eccentricity damping process inevitably raises the vertical wall structures the most effectively in the second epicycle waves.","Particle-particle collisions generally convert their lateral epicyclic motion into vertical motion.","Because the excited epicyclic motion is the greatest near the ring edge and the epicycle motions are aligned in the first waves, the conversion is the most efficient in the gap edge of the second waves and the wall height is scaled by the satellite Hill radius, which are consistent with the observations."],"url":"http://arxiv.org/abs/2403.03012v1","category":"astro-ph.EP"}
{"created":"2024-03-05 14:41:12","title":"Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations","abstract":"In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM. Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.","sentences":["In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content.","Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations.","However, their precision is still far away from acceptable in a sensitive field like education.","To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.","We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations.","With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM.","Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner.","We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners.","Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation."],"url":"http://arxiv.org/abs/2403.03008v1","category":"cs.AI"}
{"created":"2024-03-05 14:35:34","title":"Scalable Bayesian inference for the generalized linear mixed model","abstract":"The generalized linear mixed model (GLMM) is a popular statistical approach for handling correlated data, and is used extensively in applications areas where big data is common, including biomedical data settings. The focus of this paper is scalable statistical inference for the GLMM, where we define statistical inference as: (i) estimation of population parameters, and (ii) evaluation of scientific hypotheses in the presence of uncertainty. Artificial intelligence (AI) learning algorithms excel at scalable statistical estimation, but rarely include uncertainty quantification. In contrast, Bayesian inference provides full statistical inference, since uncertainty quantification results automatically from the posterior distribution. Unfortunately, Bayesian inference algorithms, including Markov Chain Monte Carlo (MCMC), become computationally intractable in big data settings. In this paper, we introduce a statistical inference algorithm at the intersection of AI and Bayesian inference, that leverages the scalability of modern AI algorithms with guaranteed uncertainty quantification that accompanies Bayesian inference. Our algorithm is an extension of stochastic gradient MCMC with novel contributions that address the treatment of correlated data (i.e., intractable marginal likelihood) and proper posterior variance estimation. Through theoretical and empirical results we establish our algorithm's statistical inference properties, and apply the method in a large electronic health records database.","sentences":["The generalized linear mixed model (GLMM) is a popular statistical approach for handling correlated data, and is used extensively in applications areas where big data is common, including biomedical data settings.","The focus of this paper is scalable statistical inference for the GLMM, where we define statistical inference as: (i) estimation of population parameters, and (ii) evaluation of scientific hypotheses in the presence of uncertainty.","Artificial intelligence (AI) learning algorithms excel at scalable statistical estimation, but rarely include uncertainty quantification.","In contrast, Bayesian inference provides full statistical inference, since uncertainty quantification results automatically from the posterior distribution.","Unfortunately, Bayesian inference algorithms, including Markov Chain Monte Carlo (MCMC), become computationally intractable in big data settings.","In this paper, we introduce a statistical inference algorithm at the intersection of AI and Bayesian inference, that leverages the scalability of modern AI algorithms with guaranteed uncertainty quantification that accompanies Bayesian inference.","Our algorithm is an extension of stochastic gradient MCMC with novel contributions that address the treatment of correlated data (i.e., intractable marginal likelihood) and proper posterior variance estimation.","Through theoretical and empirical results we establish our algorithm's statistical inference properties, and apply the method in a large electronic health records database."],"url":"http://arxiv.org/abs/2403.03007v1","category":"stat.CO"}
{"created":"2024-03-05 14:34:29","title":"Generation of gigahertz frequency surface acoustic waves in YIG/ZnO heterostructures","abstract":"We study surface acoustic waves (SAWs) in yttrium iron garnet (YIG)/zinc oxide (ZnO) heterostructures, comparing the results of a computationally lightweight analytical model with time-resolved micro-focused Brillouin light scattering data. Interdigital transducers (IDTs), with operational frequencies in the gigahertz regime, were fabricated on 50 and 100nm thin films of YIG prior to sputter deposition of 830nm and 890nm films of piezoelectric ZnO. We find good agreement between our analytical model and micro-focused Brillouin light scattering data of the IDT frequency response and SAW group velocity, with clear differentiation between the Rayleigh and Sezawa-like modes. This work paves the way for the study of SAW-spin wave (SW) interactions in low SW damping YIG, with the possibility of a method for future energy-efficient SW excitation.","sentences":["We study surface acoustic waves (SAWs) in yttrium iron garnet (YIG)/zinc oxide (ZnO) heterostructures, comparing the results of a computationally lightweight analytical model with time-resolved micro-focused Brillouin light scattering data.","Interdigital transducers (IDTs), with operational frequencies in the gigahertz regime, were fabricated on 50 and 100nm thin films of YIG prior to sputter deposition of 830nm and 890nm films of piezoelectric ZnO.","We find good agreement between our analytical model and micro-focused Brillouin light scattering data of the IDT frequency response and SAW group velocity, with clear differentiation between the Rayleigh and Sezawa-like modes.","This work paves the way for the study of SAW-spin wave (SW) interactions in low SW damping YIG, with the possibility of a method for future energy-efficient SW excitation."],"url":"http://arxiv.org/abs/2403.03006v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 14:32:26","title":"Ultralight vector dark matter search using data from the KAGRA O3GK run","abstract":"Among the various candidates for dark matter (DM), ultralight vector DM can be probed by laser interferometric gravitational wave detectors through the measurement of oscillating length changes in the arm cavities. In this context, KAGRA has a unique feature due to differing compositions of its mirrors, enhancing the signal of vector DM in the length change in the auxiliary channels. Here we present the result of a search for $U(1)_{B-L}$ gauge boson DM using the KAGRA data from auxiliary length channels during the first joint observation run together with GEO600. By applying our search pipeline, which takes into account the stochastic nature of ultralight DM, upper bounds on the coupling strength between the $U(1)_{B-L}$ gauge boson and ordinary matter are obtained for a range of DM masses. While our constraints are less stringent than those derived from previous experiments, this study demonstrates the applicability of our method to the lower-mass vector DM search, which is made difficult in this measurement by the short observation time compared to the auto-correlation time scale of DM.","sentences":["Among the various candidates for dark matter (DM), ultralight vector DM can be probed by laser interferometric gravitational wave detectors through the measurement of oscillating length changes in the arm cavities.","In this context, KAGRA has a unique feature due to differing compositions of its mirrors, enhancing the signal of vector DM in the length change in the auxiliary channels.","Here we present the result of a search for $U(1)_{B-L}$ gauge boson DM using the KAGRA data from auxiliary length channels during the first joint observation run together with GEO600.","By applying our search pipeline, which takes into account the stochastic nature of ultralight DM, upper bounds on the coupling strength between the $U(1)_{B-L}$ gauge boson and ordinary matter are obtained for a range of DM masses.","While our constraints are less stringent than those derived from previous experiments, this study demonstrates the applicability of our method to the lower-mass vector DM search, which is made difficult in this measurement by the short observation time compared to the auto-correlation time scale of DM."],"url":"http://arxiv.org/abs/2403.03004v1","category":"astro-ph.CO"}
{"created":"2024-03-05 14:28:40","title":"Mem-elements based Neuromorphic Hardware for Neural Network Application","abstract":"The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN). The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network. Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior. Transistor-level simulations in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator's viability with a power consumption of 0.337 mW. The design is further validated in neuromorphic circuits and CNN accelerators, achieving training and testing accuracies of 91.04% and 88.82%, respectively. Notably, the exclusive use of MOS transistors ensures the feasibility of monolithic IC fabrication. This research significantly contributes to the exploration of advanced hardware solutions for efficient and high-performance machine-learning applications.","sentences":["The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN).","The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network.","Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior.","Transistor-level simulations in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator's viability with a power consumption of 0.337 mW.","The design is further validated in neuromorphic circuits and CNN accelerators, achieving training and testing accuracies of 91.04% and 88.82%, respectively.","Notably, the exclusive use of MOS transistors ensures the feasibility of monolithic IC fabrication.","This research significantly contributes to the exploration of advanced hardware solutions for efficient and high-performance machine-learning applications."],"url":"http://arxiv.org/abs/2403.03002v1","category":"cs.NE"}
{"created":"2024-03-05 14:24:48","title":"Cover Edge-Based Novel Triangle Counting","abstract":"Listing and counting triangles in graphs is a key algorithmic kernel for network analyses, including community detection, clustering coefficients, k-trusses, and triangle centrality. In this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently. Leveraging the breadth-first search (BFS) method, we can quickly generate a compact cover-edge set. Novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented. The novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic graphs, such as those from the Graph500 Benchmark and the MIT/Amazon/IEEE Graph Challenge. We implement 22 sequential algorithms for performance evaluation and comparison. At the same time, we employ OpenMP to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance. Furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive graphs. In our estimate from massive-scale Graph500 graphs, our distributed parallel algorithm can reduce the communication on a scale~36 graph by 1156x and on a scale~42 graph by 2368x. Comprehensive experiments are conducted on the recently launched Intel Xeon 8480+ processor and shed light on how graph attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms.","sentences":["Listing and counting triangles in graphs is a key algorithmic kernel for network analyses, including community detection, clustering coefficients, k-trusses, and triangle centrality.","In this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently.","Leveraging the breadth-first search (BFS) method, we can quickly generate a compact cover-edge set.","Novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented.","The novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic graphs, such as those from the Graph500 Benchmark and the MIT/Amazon/IEEE Graph Challenge.","We implement 22 sequential algorithms for performance evaluation and comparison.","At the same time, we employ OpenMP to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance.","Furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive graphs.","In our estimate from massive-scale Graph500 graphs, our distributed parallel algorithm can reduce the communication on a scale~36 graph by 1156x and on a scale~42 graph by 2368x.","Comprehensive experiments are conducted on the recently launched Intel Xeon 8480+ processor and shed light on how graph attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms."],"url":"http://arxiv.org/abs/2403.02997v1","category":"cs.DS"}
{"created":"2024-03-05 14:21:57","title":"Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees","abstract":"Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify ML models against adversarial threats within the ML domain in network security. We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers. We conducted a case study using the Alexa and Phishing Site URL datasets and showed that LF attacks can be addressed using our proposed defense mechanism. Our experimental results prove that the LF attack achieved an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%.","sentences":["Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations.","Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks.","These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa.","This manipulation results in misclassification and leads to incorrect model behavior.","Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   ","The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees.","By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify ML models against adversarial threats within the ML domain in network security.","We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers.","We conducted a case study using the Alexa and Phishing Site URL datasets and showed that LF attacks can be addressed using our proposed defense mechanism.","Our experimental results prove that the LF attack achieved an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%."],"url":"http://arxiv.org/abs/2403.02995v1","category":"cs.CR"}
{"created":"2024-03-05 14:20:24","title":"SLICK: Strong Lensing Identification of Candidates Kindred in gravitational wave data","abstract":"By the end of the next decade, we hope to have detected strongly lensed gravitational waves by galaxies or clusters. Although there exist optimal methods for identifying lensed signal, it is shown that machine learning (ML) algorithms can give comparable performance but are orders of magnitude faster than non-ML methods. We present the SLICK pipeline which comprises a parallel network based on deep learning. We analyse the Q-transform maps (QT maps) and the Sine-Gaussian maps (SGP-maps) generated for the binary black hole signals injected in Gaussian as well as real noise. We compare our network performance with the previous work and find that the efficiency of our model is higher by a factor of 5 at a false positive rate of 0.001. Further, we show that including SGP maps with QT maps data results in a better performance than analysing QT maps alone. When combined with sky localisation constraints, we hope to get unprecedented accuracy in the predictions than previously possible. We also evaluate our model on the real events detected by the LIGO--Virgo collaboration and find that our network correctly classifies all of them, consistent with non-detection of lensing.","sentences":["By the end of the next decade, we hope to have detected strongly lensed gravitational waves by galaxies or clusters.","Although there exist optimal methods for identifying lensed signal, it is shown that machine learning (ML) algorithms can give comparable performance but are orders of magnitude faster than non-ML methods.","We present the SLICK pipeline which comprises a parallel network based on deep learning.","We analyse the Q-transform maps (QT maps) and the Sine-Gaussian maps (SGP-maps) generated for the binary black hole signals injected in Gaussian as well as real noise.","We compare our network performance with the previous work and find that the efficiency of our model is higher by a factor of 5 at a false positive rate of 0.001.","Further, we show that including SGP maps with QT maps data results in a better performance than analysing QT maps alone.","When combined with sky localisation constraints, we hope to get unprecedented accuracy in the predictions than previously possible.","We also evaluate our model on the real events detected by the LIGO--Virgo collaboration and find that our network correctly classifies all of them, consistent with non-detection of lensing."],"url":"http://arxiv.org/abs/2403.02994v1","category":"astro-ph.HE"}
{"created":"2024-03-05 14:18:15","title":"Localized Zeroth-Order Prompt Optimization","abstract":"The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments.","sentences":["The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs.","Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks.","This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization.","To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights.","Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I).","The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II).","Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization.","Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments."],"url":"http://arxiv.org/abs/2403.02993v1","category":"cs.AI"}
{"created":"2024-03-05 14:11:54","title":"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges","abstract":"In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a foundational guide for researchers and practitioners in this field.","sentences":["In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection.","This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond.","From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training.","Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation.","This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a foundational guide for researchers and practitioners in this field."],"url":"http://arxiv.org/abs/2403.02990v1","category":"cs.CL"}
{"created":"2024-03-05 14:04:13","title":"Evolution Transformer: In-Context Evolutionary Optimization","abstract":"Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization. An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization. In this work, we follow such a paradigm and introduce Evolution Transformer, a causal Transformer architecture, which can flexibly characterize a family of Evolution Strategies. Given a trajectory of evaluations and search distribution statistics, Evolution Transformer outputs a performance-improving update to the search distribution. The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions. We train the model weights using Evolutionary Algorithm Distillation, a technique for supervised optimization of sequence models using teacher algorithm trajectories. The resulting model exhibits strong in-context optimization performance and shows strong generalization capabilities to otherwise challenging neuroevolution tasks. We analyze the resulting properties of the Evolution Transformer and propose a technique to fully self-referentially train the Evolution Transformer, starting from a random initialization and bootstrapping its own learning progress. We provide an open source implementation under https://github.com/RobertTLange/evosax.","sentences":["Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization.","An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization.","In this work, we follow such a paradigm and introduce Evolution Transformer, a causal Transformer architecture, which can flexibly characterize a family of Evolution Strategies.","Given a trajectory of evaluations and search distribution statistics, Evolution Transformer outputs a performance-improving update to the search distribution.","The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions.","We train the model weights using Evolutionary Algorithm Distillation, a technique for supervised optimization of sequence models using teacher algorithm trajectories.","The resulting model exhibits strong in-context optimization performance and shows strong generalization capabilities to otherwise challenging neuroevolution tasks.","We analyze the resulting properties of the Evolution Transformer and propose a technique to fully self-referentially train the Evolution Transformer, starting from a random initialization and bootstrapping its own learning progress.","We provide an open source implementation under https://github.com/RobertTLange/evosax."],"url":"http://arxiv.org/abs/2403.02985v1","category":"cs.AI"}
{"created":"2024-03-05 14:03:15","title":"Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks","abstract":"Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We generated adversarial samples using the two attacks mentioned above, which were applied to a small percentage of datasets. Subsequently, we trained and tested the accuracy of the model on adversarial datasets. We recorded the results for both benign and manipulated datasets and observed significant differences between the accuracy of the models on different datasets. From the experimental results, it is evident that the LF attack failed, whereas the FP attack showed effective results, which proved its significance in fooling a server. With a 1% LF attack on the CIC, the accuracy was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily detectable, while with a 1% FP attack, the accuracy and ASR were both approximately 0.9600, hence, FP attacks are difficult to detect. We repeated the experiment with different poisoning percentages.","sentences":["Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data.","During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   ","In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect.","We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach.","In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data.","For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm.","The datasets used in this experiment were CIC and UNSW related to computer networks.","We generated adversarial samples using the two attacks mentioned above, which were applied to a small percentage of datasets.","Subsequently, we trained and tested the accuracy of the model on adversarial datasets.","We recorded the results for both benign and manipulated datasets and observed significant differences between the accuracy of the models on different datasets.","From the experimental results, it is evident that the LF attack failed, whereas the FP attack showed effective results, which proved its significance in fooling a server.","With a 1% LF attack on the CIC, the accuracy was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily detectable, while with a 1% FP attack, the accuracy and ASR were both approximately 0.9600, hence, FP attacks are difficult to detect.","We repeated the experiment with different poisoning percentages."],"url":"http://arxiv.org/abs/2403.02983v1","category":"cs.CR"}
{"created":"2024-03-05 13:59:20","title":"Fermionic Fixed-Point Structure of Asymptotically Safe QED with a Pauli Term","abstract":"We test the physical viability of a recent proposal for an asymptotically safe modification of quantum electrodynamics (QED), whose ultraviolet physics is dominated by a non-perturbative Pauli spin-field coupling. We focus in particular on its compatibility with the absence of dynamical generation of fermion mass in QED. Studying the renormalization group flow of chiral four-fermion operators and their fixed points, we discover a distinct class of behavior compared to the standard picture of fixed-point annihilation at large gauge couplings and the ensuing formation of chiral condensates. Instead, transcritical bifurcations, where the fixed points merely exchange infrared stability, are observed. Provided that non-chiral operators remain irrelevant, our theory accommodates a universality class of light fermions for $N_{\\text{f}} > 1$ irreducible Dirac flavors. On the contrary, in the special case of $N_{\\text{f}} = 1$ flavor, this comes only at the expense of introducing one additional relevant parameter.","sentences":["We test the physical viability of a recent proposal for an asymptotically safe modification of quantum electrodynamics (QED), whose ultraviolet physics is dominated by a non-perturbative Pauli spin-field coupling.","We focus in particular on its compatibility with the absence of dynamical generation of fermion mass in QED.","Studying the renormalization group flow of chiral four-fermion operators and their fixed points, we discover a distinct class of behavior compared to the standard picture of fixed-point annihilation at large gauge couplings and the ensuing formation of chiral condensates.","Instead, transcritical bifurcations, where the fixed points merely exchange infrared stability, are observed.","Provided that non-chiral operators remain irrelevant, our theory accommodates a universality class of light fermions for $N_{\\text{f}} > 1$ irreducible Dirac flavors.","On the contrary, in the special case of $N_{\\text{f}} = 1$ flavor, this comes only at the expense of introducing one additional relevant parameter."],"url":"http://arxiv.org/abs/2403.02980v1","category":"hep-th"}
{"created":"2024-03-05 13:58:12","title":"Fast Iterative Region Inflation for Computing Large 2-D/3-D Convex Regions of Obstacle-Free Space","abstract":"1) Restrictive Inflation is designed to ensure the managibility of the generated convex polytope. Based on its characteristic of few variables but rich constraints, an efficient and numerically stable solver is designed. 2) A novel method that formulates the MVIE problem into SOCP formulation is proposed, which avoids directly confronting the positive definite constraints and improves the computational efficiency. 3) Especially for 2-D MVIE, a linear-time exact algorithm is introduced for the first time, filling a gap that existed for several decades and further enabling ultra-fast computational performance. 4) Building upon the above methods, a reliable convex polytope generation algorithm FIRI is proposed. Extensive experiments verify its superior comprehensive performance in terms of quality, efficiency, and managibility. High-performance implementation of FIRI will be open-sourced for the reference of the community.","sentences":["1) Restrictive Inflation is designed to ensure the managibility of the generated convex polytope.","Based on its characteristic of few variables but rich constraints, an efficient and numerically stable solver is designed.","2) A novel method that formulates the MVIE problem into SOCP formulation is proposed, which avoids directly confronting the positive definite constraints and improves the computational efficiency.","3) Especially for 2-D MVIE, a linear-time exact algorithm is introduced for the first time, filling a gap that existed for several decades and further enabling ultra-fast computational performance.","4) Building upon the above methods, a reliable convex polytope generation algorithm FIRI is proposed.","Extensive experiments verify its superior comprehensive performance in terms of quality, efficiency, and managibility.","High-performance implementation of FIRI will be open-sourced for the reference of the community."],"url":"http://arxiv.org/abs/2403.02977v1","category":"cs.RO"}
{"created":"2024-03-05 13:55:16","title":"A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching","abstract":"Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \\textit{keywords} and \\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the model from the reliance on NER models. To this end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed \\underline{S}emantic \\underline{M}atching framework based on the pre-trained language models, abbreviated as \\textbf{MCP-SM}, to extract various concepts and infuse them into the classification tokens. We conduct comprehensive experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM. Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding performance further prove MCP-SM's applicability in low-resource languages.","sentences":["Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation.","Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \\textit{keywords} and \\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance.","Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain.","In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the model from the reliance on NER models.","To this end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed \\underline{S}emantic \\underline{M}atching framework based on the pre-trained language models, abbreviated as \\textbf{MCP-SM}, to extract various concepts and infuse them into the classification tokens.","We conduct comprehensive experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM.","Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding performance further prove MCP-SM's applicability in low-resource languages."],"url":"http://arxiv.org/abs/2403.02975v1","category":"cs.CL"}
{"created":"2024-03-05 13:45:46","title":"Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception","abstract":"Multimodal Large Language Model (MLLMs) leverages Large Language Models as a cognitive framework for diverse visual-language tasks. Recent efforts have been made to equip MLLMs with visual perceiving and grounding capabilities. However, there still remains a gap in providing fine-grained pixel-level perceptions and extending interactions beyond text-specific inputs. In this work, we propose {\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object perceptions and natural language descriptions from multi-modality references, such as texts, boxes, images, or audio. This innovation empowers users with greater flexibility to engage with the model beyond textual and regional prompts, without modality-specific designs. Through our proposed refocusing mechanism, the generated grounding output is guided to better focus on the referenced object, implicitly incorporating additional pixel-level supervision. This simple modification utilizes attention scores generated during the inference of LLM, eliminating the need for extra computations while exhibiting performance enhancements in both grounding masks and referring expressions. With only publicly available training data, our model achieves state-of-the-art results across multiple benchmarks, including diverse modality referring segmentation and region-level referring expression generation.","sentences":["Multimodal Large Language Model (MLLMs) leverages Large Language Models as a cognitive framework for diverse visual-language tasks.","Recent efforts have been made to equip MLLMs with visual perceiving and grounding capabilities.","However, there still remains a gap in providing fine-grained pixel-level perceptions and extending interactions beyond text-specific inputs.","In this work, we propose {\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object perceptions and natural language descriptions from multi-modality references, such as texts, boxes, images, or audio.","This innovation empowers users with greater flexibility to engage with the model beyond textual and regional prompts, without modality-specific designs.","Through our proposed refocusing mechanism, the generated grounding output is guided to better focus on the referenced object, implicitly incorporating additional pixel-level supervision.","This simple modification utilizes attention scores generated during the inference of LLM, eliminating the need for extra computations while exhibiting performance enhancements in both grounding masks and referring expressions.","With only publicly available training data, our model achieves state-of-the-art results across multiple benchmarks, including diverse modality referring segmentation and region-level referring expression generation."],"url":"http://arxiv.org/abs/2403.02969v1","category":"cs.CV"}
{"created":"2024-03-05 13:44:28","title":"Hamiltonian Property Testing","abstract":"Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\\tilde{\\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\\tilde{\\Omega}(2^n / \\varepsilon)$, and even coherent testers need $\\Omega(2^{n/2})$ many queries and $\\Omega(2^{n/2}/\\varepsilon)$ total evolution time. In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements. In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality. Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning. Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning.","sentences":["Locality is a fundamental feature of many physical time evolutions.","Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution.","However, no protocols to rigorously test whether an unknown Hamiltonian is local were known.","We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\\tilde{\\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\\tilde{\\Omega}(2^n / \\varepsilon)$, and even coherent testers need $\\Omega(2^{n/2})$ many queries and $\\Omega(2^{n/2}/\\varepsilon)$ total evolution time.","In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements.","In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality.","Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning.","Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning."],"url":"http://arxiv.org/abs/2403.02968v1","category":"quant-ph"}
{"created":"2024-03-05 13:43:58","title":"Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering","abstract":"Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.","sentences":["Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin.","Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues.","These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence.","To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs.","We optimize an open-source LLM as a fact summarizer through distillation and preference alignment.","Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary."],"url":"http://arxiv.org/abs/2403.02966v1","category":"cs.CL"}
{"created":"2024-03-05 13:43:58","title":"Non-Convex Stochastic Composite Optimization with Polyak Momentum","abstract":"The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.","sentences":["The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning.","However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used).","In this paper, we focus on the stochastic proximal gradient method with Polyak momentum.","We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size.","Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly.","Finally, we provide numerical experiments to validate our theoretical results."],"url":"http://arxiv.org/abs/2403.02967v1","category":"math.OC"}
{"created":"2024-03-05 13:41:25","title":"ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities","abstract":"This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.","sentences":["This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks.","We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation.","Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks.","Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy.","Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks.","Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics."],"url":"http://arxiv.org/abs/2403.02965v1","category":"cs.CV"}
{"created":"2024-03-05 13:34:49","title":"Opportunistic User Scheduling for Secure RIS-aided Wireless Communications","abstract":"In this paper, we provide expressions for the secrecy outage probability (SOP) for suboptimal and optimal opportunistic scheduling schemes in a reconfigurable intelligent surface (RIS) aided system with multiple eavesdroppers in approximate closed form. A suboptimal scheduling (SS) scheme is analyzed, which is used when the channel state information (CSI) of the eavesdropping links is unavailable, and the optimal scheduling (OS) scheme is also analyzed, which is used when the global CSI is available. For each scheme, we provide a simplified expression for the SOP in the high signal-to-noise ratio (SNR) regime to demonstrate its behavior as a function of the key system parameters. At high SNR, the SOP saturates to a constant level which decreases exponentially with the number of RIS elements in the SS scheme and with the product of the number of RIS elements and the number of users in the OS scheme. We compare the performance of the opportunistic user scheduling schemes with that of a non-orthogonal multiple access (NOMA) based scheduling scheme which chooses a pair of users in each time slot for scheduling and we show that the opportunistic schemes outperform the NOMA-based scheme. We also derive a closed-form expression for the SOP of a decode-and-forward (DF) relay-aided scheduling scheme in order to compare it with that of the RIS-aided system. It is found that the RIS-aided system outperforms the relay-aided systems when the number of RIS elements is sufficiently large. An increased number of RIS elements is required to outperform the relay-aided system at higher operating frequencies.","sentences":["In this paper, we provide expressions for the secrecy outage probability (SOP) for suboptimal and optimal opportunistic scheduling schemes in a reconfigurable intelligent surface (RIS) aided system with multiple eavesdroppers in approximate closed form.","A suboptimal scheduling (SS) scheme is analyzed, which is used when the channel state information (CSI) of the eavesdropping links is unavailable, and the optimal scheduling (OS) scheme is also analyzed, which is used when the global CSI is available.","For each scheme, we provide a simplified expression for the SOP in the high signal-to-noise ratio (SNR) regime to demonstrate its behavior as a function of the key system parameters.","At high SNR, the SOP saturates to a constant level which decreases exponentially with the number of RIS elements in the SS scheme and with the product of the number of RIS elements and the number of users in the OS scheme.","We compare the performance of the opportunistic user scheduling schemes with that of a non-orthogonal multiple access (NOMA) based scheduling scheme which chooses a pair of users in each time slot for scheduling and we show that the opportunistic schemes outperform the NOMA-based scheme.","We also derive a closed-form expression for the SOP of a decode-and-forward (DF) relay-aided scheduling scheme in order to compare it with that of the RIS-aided system.","It is found that the RIS-aided system outperforms the relay-aided systems when the number of RIS elements is sufficiently large.","An increased number of RIS elements is required to outperform the relay-aided system at higher operating frequencies."],"url":"http://arxiv.org/abs/2403.02963v1","category":"cs.IT"}
{"created":"2024-03-05 13:33:12","title":"WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction","abstract":"Tabular data, as a crucial form of data representation, exists in diverse formats on the Web. When confronted with complex and irregular tables, manual modification becomes a laborious task. This paper investigates the performance of Large Language Models (LLMs) in the context of table editing tasks. Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables. Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code. To address this, we introduce the WikiTableEdit dataset. Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances. Subsequently, we evaluate several representative large language models on the WikiTableEdit dataset to demonstrate the challenge of this task. The dataset will be released to the community to promote related researches.","sentences":["Tabular data, as a crucial form of data representation, exists in diverse formats on the Web.","When confronted with complex and irregular tables, manual modification becomes a laborious task.","This paper investigates the performance of Large Language Models (LLMs) in the context of table editing tasks.","Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables.","Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code.","To address this, we introduce the WikiTableEdit dataset.","Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances.","Subsequently, we evaluate several representative large language models on the WikiTableEdit dataset to demonstrate the challenge of this task.","The dataset will be released to the community to promote related researches."],"url":"http://arxiv.org/abs/2403.02962v1","category":"cs.AI"}
{"created":"2024-03-05 13:32:48","title":"Ab initio simulation of the universal properties of unitary Fermi gas in a harmonic trap","abstract":"Chang and Bertsch [Phys. Rev. A 76, 021603(R) (2007)] proposed a simple formula for the ground state energy of a unitary Fermi gas in a harmonic trap, based on their Green's function Monte Carlo simulations of up to 22 fermions, combined with general assumptions about the universal thermodynamic behavior of the unitary Fermi gas. In this work, we perform the ab initio simulations of the ground state energy of up to one hundred fermions using the fictitious identical particle method to overcome the Fermion sign problem, and we find that the formula proposed by Chang and Bertsch remains highly accurate. Since the number of fermions we simulate is much larger than that simulated by Chang and Bertsch when they proposed the formula, our work provides strong evidence for the universal validity of the formula. Our work demonstrates that fictitious identical particles provide a valuable tool for the ab initio simulations of ultracold Fermi gases.","sentences":["Chang and Bertsch [Phys.","Rev. A 76, 021603(R) (2007)] proposed a simple formula for the ground state energy of a unitary Fermi gas in a harmonic trap, based on their Green's function Monte Carlo simulations of up to 22 fermions, combined with general assumptions about the universal thermodynamic behavior of the unitary Fermi gas.","In this work, we perform the ab initio simulations of the ground state energy of up to one hundred fermions using the fictitious identical particle method to overcome the Fermion sign problem, and we find that the formula proposed by Chang and Bertsch remains highly accurate.","Since the number of fermions we simulate is much larger than that simulated by Chang and Bertsch when they proposed the formula, our work provides strong evidence for the universal validity of the formula.","Our work demonstrates that fictitious identical particles provide a valuable tool for the ab initio simulations of ultracold Fermi gases."],"url":"http://arxiv.org/abs/2403.02961v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-05 13:30:02","title":"SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents","abstract":"With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge. (3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.","sentences":["With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry.","However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration.","As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence.","In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents.","To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge.","(2) we propose a novel multi-agent framework, AgentsCourt.","Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge.","(3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively."],"url":"http://arxiv.org/abs/2403.02959v1","category":"cs.CL"}
{"created":"2024-03-05 13:25:44","title":"On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models","abstract":"Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.","sentences":["Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks.","Despite their practical utility, there is a notable gap in their theoretical understanding.","This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps.","The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training.","We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off.","The theoretical findings are validated by numerical results."],"url":"http://arxiv.org/abs/2403.02957v1","category":"cs.LG"}
{"created":"2024-03-05 13:25:34","title":"Review of Nanolayered Post-transition Metal Monochalcogenides: Synthesis, Properties, and Applications","abstract":"Nanolayered post-transition metal monochalcogenides (PTMMCs) stand out as promising advanced two-dimensional (2D) materials. Beyond inheriting the general advantages associated with traditional 2D materials, they exhibit unique properties, including a wide bandgap range covering the ultraviolet to the mid-infrared spectral ranges, thickness-dependent bandgap behaviors, good nonlinear optical performance, high thermoelectric coefficients, and ferroelectricity. Consequently, these materials hold significant potential in diverse applications such as photodetectors, field effect transistors, thermoelectrics, ferroelectrics, photovoltaics, and electrochemical devices, especially in the manufacturing of nanoscale devices. However, there is still a lack of systematic understanding of the PTMMC family. This study provides a broad overview of the crystal structures, bandgap structures, synthesis methods, physical properties, and state-of-the-art applications of PTMMC materials with a motif of X-M-M-X (M=Ga, In, Ge, Sn; X=S, Se, Te). An outlook for the development trends is emphasized at the end, underscoring the critical importance of this work to the future exploration of nanolayered PTMMCs.","sentences":["Nanolayered post-transition metal monochalcogenides (PTMMCs) stand out as promising advanced two-dimensional (2D) materials.","Beyond inheriting the general advantages associated with traditional 2D materials, they exhibit unique properties, including a wide bandgap range covering the ultraviolet to the mid-infrared spectral ranges, thickness-dependent bandgap behaviors, good nonlinear optical performance, high thermoelectric coefficients, and ferroelectricity.","Consequently, these materials hold significant potential in diverse applications such as photodetectors, field effect transistors, thermoelectrics, ferroelectrics, photovoltaics, and electrochemical devices, especially in the manufacturing of nanoscale devices.","However, there is still a lack of systematic understanding of the PTMMC family.","This study provides a broad overview of the crystal structures, bandgap structures, synthesis methods, physical properties, and state-of-the-art applications of PTMMC materials with a motif of X-M-M-X (M=Ga, In, Ge, Sn; X=S, Se, Te).","An outlook for the development trends is emphasized at the end, underscoring the critical importance of this work to the future exploration of nanolayered PTMMCs."],"url":"http://arxiv.org/abs/2403.02956v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-05 13:25:30","title":"XAI-Based Detection of Adversarial Attacks on Deepfake Detectors","abstract":"We introduce a novel methodology for identifying adversarial attacks on deepfake detectors using eXplainable Artificial Intelligence (XAI). In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems. However, these systems are frequently targeted by adversarial attacks that inhibit their performance. We address this gap, developing a defensible deepfake detector by leveraging the power of XAI. The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models. We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image. The feature embeddings extracted from this process are then used for training a simple yet effective classifier. Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible adversarial attacks, pinpointing potential vulnerabilities. Furthermore, this approach does not change the performance of the deepfake detector. The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms. We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors.","sentences":["We introduce a novel methodology for identifying adversarial attacks on deepfake detectors using eXplainable Artificial Intelligence (XAI).","In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems.","However, these systems are frequently targeted by adversarial attacks that inhibit their performance.","We address this gap, developing a defensible deepfake detector by leveraging the power of XAI.","The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models.","We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image.","The feature embeddings extracted from this process are then used for training a simple yet effective classifier.","Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible adversarial attacks, pinpointing potential vulnerabilities.","Furthermore, this approach does not change the performance of the deepfake detector.","The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms.","We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors."],"url":"http://arxiv.org/abs/2403.02955v1","category":"cs.CR"}
{"created":"2024-03-05 13:24:08","title":"Role of long jumps in L\u00e9vy noise induced multimodality","abstract":"L\\'evy noise is a paradigmatic noise used to describe out-of-equilibrium systems. Typically, properties of L\\'evy noise driven systems are very different from their Gaussian white noise driven counterparts. In particular, under action of L\\'evy noise, stationary states in single-well, super-harmonic, potentials are no longer unimodal. Typically, they are bimodal however for fine-tuned potentials the number of modes can be further increased. The multimodality arises as a consequence of the competition between long displacements induced by the non-equilibrium stochastic driving and action of the deterministic force. Here, we explore robustness of bimodality in the quartic potential under action of the L\\'evy noise. We explore various scenarios of bounding long jumps and assess their ability to weaken and disassembly multimodality. In general, we demonstrate that despite its robustness it is possible to destroy the bimodality by limiting the length of noise-induced jumps.","sentences":["L\\'evy noise is a paradigmatic noise used to describe out-of-equilibrium systems.","Typically, properties of L\\'evy noise driven systems are very different from their Gaussian white noise driven counterparts.","In particular, under action of L\\'evy noise, stationary states in single-well, super-harmonic, potentials are no longer unimodal.","Typically, they are bimodal however for fine-tuned potentials the number of modes can be further increased.","The multimodality arises as a consequence of the competition between long displacements induced by the non-equilibrium stochastic driving and action of the deterministic force.","Here, we explore robustness of bimodality in the quartic potential under action of the L\\'evy noise.","We explore various scenarios of bounding long jumps and assess their ability to weaken and disassembly multimodality.","In general, we demonstrate that despite its robustness it is possible to destroy the bimodality by limiting the length of noise-induced jumps."],"url":"http://arxiv.org/abs/2403.02952v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 13:23:48","title":"Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation","abstract":"Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.","sentences":["Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods.","Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks.","Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.","To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs.","Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.","Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task.","These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems."],"url":"http://arxiv.org/abs/2403.02951v1","category":"cs.CL"}
{"created":"2024-03-05 13:21:20","title":"A general approach to enhance the survivability of backdoor attacks by decision path coupling","abstract":"Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences. One of the mainstream backdoor defenses is model reconstruction-based. Such defenses adopt model unlearning or pruning to eliminate backdoors. However, little attention has been paid to survive from such defenses. To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses. We formalize Venom as a binary-task optimization problem. The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability. To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate. Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks.","sentences":["Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences.","One of the mainstream backdoor defenses is model reconstruction-based.","Such defenses adopt model unlearning or pruning to eliminate backdoors.","However, little attention has been paid to survive from such defenses.","To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses.","We formalize Venom as a binary-task optimization problem.","The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability.","To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate.","Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks."],"url":"http://arxiv.org/abs/2403.02950v1","category":"cs.AI"}
{"created":"2024-03-05 13:20:55","title":"Radial amplitude equations for fully localised planar patterns","abstract":"Isolated patches of spatially oscillating pattern have been found to emerge near a pattern-forming instability in a wide variety of experiments and mathematical models. However, there is currently no mathematical theory to explain this emergence or characterise the structure of these patches. We provide a method for formally deriving radial amplitude equations to planar patterns via non-autonomous multiple-scale analysis and convolutional sums of products of Bessel functions. Our novel approach introduces nonautonomous differential operators, which allow for the systematic manipulation of Bessel functions, as well as previously unseen identities involving infinite sums of Bessel functions. Solutions of the amplitude equations describe fully localised patterns with non-trivial angular dependence, where localisation occurs in a purely radial direction. Amplitude equations are derived for multiple examples of patterns with dihedral symmetry, including fully localised hexagons and quasipatterns with twelve-fold rotational symmetry. In particular, we show how to apply the asymptotic method to the Swift--Hohenberg equation and general reaction-diffusion systems.","sentences":["Isolated patches of spatially oscillating pattern have been found to emerge near a pattern-forming instability in a wide variety of experiments and mathematical models.","However, there is currently no mathematical theory to explain this emergence or characterise the structure of these patches.","We provide a method for formally deriving radial amplitude equations to planar patterns via non-autonomous multiple-scale analysis and convolutional sums of products of Bessel functions.","Our novel approach introduces nonautonomous differential operators, which allow for the systematic manipulation of Bessel functions, as well as previously unseen identities involving infinite sums of Bessel functions.","Solutions of the amplitude equations describe fully localised patterns with non-trivial angular dependence, where localisation occurs in a purely radial direction.","Amplitude equations are derived for multiple examples of patterns with dihedral symmetry, including fully localised hexagons and quasipatterns with twelve-fold rotational symmetry.","In particular, we show how to apply the asymptotic method to the Swift--Hohenberg equation and general reaction-diffusion systems."],"url":"http://arxiv.org/abs/2403.02949v1","category":"math.DS"}
{"created":"2024-03-05 13:17:54","title":"Phase Behavior and Dynamics of Active Brownian Particles in an Alignment Field","abstract":"Self-propelled particles that are subject to noise are a well-established generic model system for active matter. A homogeneous alignment field can be used to orient the direction of the self-propulsion velocity and to model systems like phoretic Janus particles with a magnetic dipole moment or magnetotactic bacteria in an external magnetic field. Computer simulations are used to predict the phase behavior and dynamics of self-propelled Brownian particles in a homogeneous alignment field in two dimensions. Phase boundaries of the gas-liquid coexistence region are calculated for various P\\'eclet numbers, particle densities, and alignment field strengths. Critical points and exponents are calculated and, in agreement with previous simulations, do not seem to belong to the universality class of the 2D Ising model. Finally, the dynamics of spinodal decomposition for quenching the system from the one-phase to the two-phase coexistence region by increasing P\\'eclet number is characterized. Our results may help to identify parameters for optimal transport of active matter in complex environments.","sentences":["Self-propelled particles that are subject to noise are a well-established generic model system for active matter.","A homogeneous alignment field can be used to orient the direction of the self-propulsion velocity and to model systems like phoretic Janus particles with a magnetic dipole moment or magnetotactic bacteria in an external magnetic field.","Computer simulations are used to predict the phase behavior and dynamics of self-propelled Brownian particles in a homogeneous alignment field in two dimensions.","Phase boundaries of the gas-liquid coexistence region are calculated for various P\\'eclet numbers, particle densities, and alignment field strengths.","Critical points and exponents are calculated and, in agreement with previous simulations, do not seem to belong to the universality class of the 2D Ising model.","Finally, the dynamics of spinodal decomposition for quenching the system from the one-phase to the two-phase coexistence region by increasing P\\'eclet number is characterized.","Our results may help to identify parameters for optimal transport of active matter in complex environments."],"url":"http://arxiv.org/abs/2403.02947v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 13:17:09","title":"SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators","abstract":"Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications. However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators. While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process. This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators.","sentences":["Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications.","However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators.","While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process.","This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators."],"url":"http://arxiv.org/abs/2403.02946v1","category":"cs.AI"}
{"created":"2024-03-05 13:15:01","title":"Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity","abstract":"Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions.","sentences":["Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images.","These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality.","To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity.","In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss.","By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level.","Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions.","In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions."],"url":"http://arxiv.org/abs/2403.02944v1","category":"cs.CV"}
{"created":"2024-03-05 13:13:01","title":"Tensor Decomposition-based Time Varying Channel Estimation for mmWave MIMO-OFDM Systems","abstract":"In this paper, we consider the time-varying channel estimation in millimeter wave (mmWave) multiple-input multiple-output MIMO systems with hybrid beamforming architectures. Different from the existing contributions that considered single-carrier mmWave systems with high mobility, the wideband orthogonal frequency division multiplexing (OFDM) system is considered in this work. To solve the channel estimation problem under channel double selectivity, we propose a pilot transmission scheme based on 5G OFDM, and the received signals are formed as a fourth-order tensor, which fits the low-rank CANDECOMP/PARAFAC (CP) model. By further exploring the Vandermonde structure of factor matrix, a tensor-subspace decomposition based channel estimation method is proposed to solve the CP decomposition, where the uniqueness condition is analyzed. Based on the decomposed factor matrices, the channel parameters, including angles of arrival/departure, delays, channel gains and Doppler shifts are estimated, and the Cram\\'{e}r-Rao bound (CRB) results are derived as performance metrics. Simulation results demonstrate the superior performance of the proposed method over other benchmarks. Furthermore, the channel estimation methods are tested based on the channel parameters generated by Wireless InSites, and simulation results show the effectiveness of the proposed method in practical scenarios.","sentences":["In this paper, we consider the time-varying channel estimation in millimeter wave (mmWave) multiple-input multiple-output MIMO systems with hybrid beamforming architectures.","Different from the existing contributions that considered single-carrier mmWave systems with high mobility, the wideband orthogonal frequency division multiplexing (OFDM) system is considered in this work.","To solve the channel estimation problem under channel double selectivity, we propose a pilot transmission scheme based on 5G OFDM, and the received signals are formed as a fourth-order tensor, which fits the low-rank CANDECOMP/PARAFAC (CP) model.","By further exploring the Vandermonde structure of factor matrix, a tensor-subspace decomposition based channel estimation method is proposed to solve the CP decomposition, where the uniqueness condition is analyzed.","Based on the decomposed factor matrices, the channel parameters, including angles of arrival/departure, delays, channel gains and Doppler shifts are estimated, and the Cram\\'{e}r-Rao bound (CRB) results are derived as performance metrics.","Simulation results demonstrate the superior performance of the proposed method over other benchmarks.","Furthermore, the channel estimation methods are tested based on the channel parameters generated by Wireless InSites, and simulation results show the effectiveness of the proposed method in practical scenarios."],"url":"http://arxiv.org/abs/2403.02942v1","category":"cs.IT"}
{"created":"2024-03-05 13:10:06","title":"PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers","abstract":"With the rapid growth of scholarly archives, researchers subscribe to \"paper alert\" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.","sentences":["With the rapid growth of scholarly archives, researchers subscribe to \"paper alert\" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers.","However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts.","To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers.","PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects.","Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers."],"url":"http://arxiv.org/abs/2403.02939v1","category":"cs.DL"}
{"created":"2024-03-05 13:08:52","title":"AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models","abstract":"Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear. This method can be used to produce fast but intelligible speech. In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.","sentences":["Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension.","To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed.","However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans.","In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility.","The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear.","This method can be used to produce fast but intelligible speech.","In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to."],"url":"http://arxiv.org/abs/2403.02938v1","category":"cs.CL"}
{"created":"2024-03-05 13:05:07","title":"Quantum Algorithms in a Superposition of Spacetimes","abstract":"Quantum computers are expected to revolutionize our ability to process information. The advancement from classical to quantum computing is a product of our advancement from classical to quantum physics -- the more our understanding of the universe grows, so does our ability to use it for computation. A natural question that arises is, what will physics allow in the future? Can more advanced theories of physics increase our computational power, beyond quantum computing?   An active field of research in physics studies theoretical phenomena outside the scope of explainable quantum mechanics, that form when attempting to combine Quantum Mechanics (QM) with General Relativity (GR) into a unified theory of Quantum Gravity (QG). QG is known to present the possibility of a quantum superposition of causal structure and event orderings. In the literature of quantum information theory, this translates to a superposition of unitary evolution orders.   In this work we show a first example of a natural computational model based on QG, that provides an exponential speedup over standard quantum computation (under standard hardness assumptions). We define a model and complexity measure for a quantum computer that has the ability to generate a superposition of unitary evolution orders, and show that such computer is able to solve in polynomial time two of the fundamental problems in computer science: The Graph Isomorphism Problem ($\\mathsf{GI}$) and the Gap Closest Vector Problem ($\\mathsf{GapCVP}$), with gap $O\\left( n^{2} \\right)$. These problems are believed by experts to be hard to solve for a regular quantum computer. Interestingly, our model does not seem overpowered, and we found no obvious way to solve entire complexity classes that are considered hard in computer science, like the classes $\\mathbf{NP}$ and $\\mathbf{SZK}$.","sentences":["Quantum computers are expected to revolutionize our ability to process information.","The advancement from classical to quantum computing is a product of our advancement from classical to quantum physics -- the more our understanding of the universe grows, so does our ability to use it for computation.","A natural question that arises is, what will physics allow in the future?","Can more advanced theories of physics increase our computational power, beyond quantum computing?   ","An active field of research in physics studies theoretical phenomena outside the scope of explainable quantum mechanics, that form when attempting to combine Quantum Mechanics (QM) with General Relativity (GR) into a unified theory of Quantum Gravity (QG).","QG is known to present the possibility of a quantum superposition of causal structure and event orderings.","In the literature of quantum information theory, this translates to a superposition of unitary evolution orders.   ","In this work we show a first example of a natural computational model based on QG, that provides an exponential speedup over standard quantum computation (under standard hardness assumptions).","We define a model and complexity measure for a quantum computer that has the ability to generate a superposition of unitary evolution orders, and show that such computer is able to solve in polynomial time two of the fundamental problems in computer science: The Graph Isomorphism Problem ($\\mathsf{GI}$) and the Gap Closest Vector Problem ($\\mathsf{GapCVP}$), with gap $O\\left( n^{2} \\right)$. These problems are believed by experts to be hard to solve for a regular quantum computer.","Interestingly, our model does not seem overpowered, and we found no obvious way to solve entire complexity classes that are considered hard in computer science, like the classes $\\mathbf{NP}$ and $\\mathbf{SZK}$."],"url":"http://arxiv.org/abs/2403.02937v1","category":"quant-ph"}
{"created":"2024-03-05 13:03:31","title":"AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators","abstract":"In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators.","sentences":["In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators."],"url":"http://arxiv.org/abs/2403.02936v1","category":"cs.AI"}
{"created":"2024-03-05 12:51:40","title":"Fuzzy Datalog$^\\exists$ over Arbitrary t-Norms","abstract":"One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data. This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more. To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting. In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting.","sentences":["One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data.","This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more.","To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies.","The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting.","In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting."],"url":"http://arxiv.org/abs/2403.02933v1","category":"cs.AI"}
{"created":"2024-03-05 12:50:36","title":"RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules","abstract":"Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task. Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.","sentences":["Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data.","With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus.","In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories.","Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task.","Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner.","That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point.","Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods.","What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories."],"url":"http://arxiv.org/abs/2403.02932v1","category":"cs.CL"}
{"created":"2024-03-05 12:44:39","title":"Risk-Constrained Community Battery Utilisation Optimisation for Electric Vehicle Charging with Photovoltaic Resources","abstract":"High penetration of renewable generation in the electricity grid presents power system operators with challenges including voltage instability mainly due to fluctuating power generation. To cope with intermittent generation, community batteries introduce an elegant solution for storing excess generation of renewable resources and reverting to the grid in peak demand periods. The question of the right battery type and size coupled with the right investment is challenging. Furthermore, the growth in adapting EVs imposes additional demand challenges on the power system compared to traditional industrial and household demand. This paper introduces long-term planning for community batteries to capture the surplus generation of PV resources for a given area and redirect these resources to charge EVs, without direct injection to the grid. For long-term investment planning on batteries, we consider 15 years' worth of historical data associated with solar irradiance, temperature, EV demands, and household demands. A novel stochastic mathematical model is proposed for decision-making on battery specifications (the type and capacity per year) based on the four standard battery types provided by the CSIRO in Australia. Uncertainties related to the EVs and RESs are captured by a non-parametric robust technique, named information gap decision theory, from optimistic and pessimistic perspectives. The investment decision-making part is formulated as mixed-integer linear programming taking advantage of the powerful commercial solver -- GUROBI -- which leads to finding feasible global solutions with low computational burden. The outcomes of this investigation not only detect optimal battery installation strategies to improve the stability profile of the grid by capturing the excess generation of PV resources but also facilitate EV integration in the community toward reaching net-zero emissions targets.","sentences":["High penetration of renewable generation in the electricity grid presents power system operators with challenges including voltage instability mainly due to fluctuating power generation.","To cope with intermittent generation, community batteries introduce an elegant solution for storing excess generation of renewable resources and reverting to the grid in peak demand periods.","The question of the right battery type and size coupled with the right investment is challenging.","Furthermore, the growth in adapting EVs imposes additional demand challenges on the power system compared to traditional industrial and household demand.","This paper introduces long-term planning for community batteries to capture the surplus generation of PV resources for a given area and redirect these resources to charge EVs, without direct injection to the grid.","For long-term investment planning on batteries, we consider 15 years' worth of historical data associated with solar irradiance, temperature, EV demands, and household demands.","A novel stochastic mathematical model is proposed for decision-making on battery specifications (the type and capacity per year) based on the four standard battery types provided by the CSIRO in Australia.","Uncertainties related to the EVs and RESs are captured by a non-parametric robust technique, named information gap decision theory, from optimistic and pessimistic perspectives.","The investment decision-making part is formulated as mixed-integer linear programming taking advantage of the powerful commercial solver -- GUROBI -- which leads to finding feasible global solutions with low computational burden.","The outcomes of this investigation not only detect optimal battery installation strategies to improve the stability profile of the grid by capturing the excess generation of PV resources but also facilitate EV integration in the community toward reaching net-zero emissions targets."],"url":"http://arxiv.org/abs/2403.02927v1","category":"math.OC"}
{"created":"2024-03-05 12:38:54","title":"From Spectra to Biophysical Insights: End-to-End Learning with a Biased Radiative Transfer Model","abstract":"Advances in machine learning have boosted the use of Earth observation data for climate change research. Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change. Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests. We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach. Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression. Furthermore, our framework has potential generally for inverting biased physical models. The code is available on https://github.com/yihshe/ai-refined-rtm.git.","sentences":["Advances in machine learning have boosted the use of Earth observation data for climate change research.","Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change.","Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests.","We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach.","Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression.","Furthermore, our framework has potential generally for inverting biased physical models.","The code is available on https://github.com/yihshe/ai-refined-rtm.git."],"url":"http://arxiv.org/abs/2403.02922v1","category":"cs.LG"}
{"created":"2024-03-05 12:38:21","title":"Quantum for Good and the Societal Impact of Quantum Computing","abstract":"Quantum computing promises to help humanity solve problems that would otherwise be intractable on classical computers. Unlike today's machines, quantum computers use a novel computing process that leverages the foundational quantum mechanical laws of nature. This unlocks unparalleled compute power for certain applications and promises to help solve some of our generation's gravest challenges, including the climate crisis, food insecurity, and widespread disease. No one entity will be able to realize this end state alone. Developing a fault-tolerant quantum supercomputer and a vibrant ecosystem around it will require deep partnerships between industry, governments, and academia. It will also require collective action to enable and promote positive applications of quantum computing and ensure that the safe and responsible use of the technology is at the center of its development and deployment. Achieving these objectives will require focusing on three priorities:   1. Impact. Ensure quantum computing benefits all of humankind by developing quantum solutions to solve critical, global problems.   2. Use. Protect against malicious use by accelerating the deployment of quantum-safe cryptography and developing governance processes and controls for the responsible use of quantum machines.   3. Access. Democratize the potential for economic growth across all of society through skilling, workforce and ecosystem development, and digital infrastructure.   This paper discusses each in turn.","sentences":["Quantum computing promises to help humanity solve problems that would otherwise be intractable on classical computers.","Unlike today's machines, quantum computers use a novel computing process that leverages the foundational quantum mechanical laws of nature.","This unlocks unparalleled compute power for certain applications and promises to help solve some of our generation's gravest challenges, including the climate crisis, food insecurity, and widespread disease.","No one entity will be able to realize this end state alone.","Developing a fault-tolerant quantum supercomputer and a vibrant ecosystem around it will require deep partnerships between industry, governments, and academia.","It will also require collective action to enable and promote positive applications of quantum computing and ensure that the safe and responsible use of the technology is at the center of its development and deployment.","Achieving these objectives will require focusing on three priorities:   1.","Impact.","Ensure quantum computing benefits all of humankind by developing quantum solutions to solve critical, global problems.   ","2. Use.","Protect against malicious use by accelerating the deployment of quantum-safe cryptography and developing governance processes and controls for the responsible use of quantum machines.   ","3.","Access.","Democratize the potential for economic growth across all of society through skilling, workforce and ecosystem development, and digital infrastructure.   ","This paper discusses each in turn."],"url":"http://arxiv.org/abs/2403.02921v1","category":"physics.soc-ph"}
{"created":"2024-03-05 12:38:14","title":"TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax","abstract":"The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification benchmark across five tasks involving long sequences reveals no degradation in accuracy when employing Transformers equipped with TaylorShift. For reproducibility, we provide access to our code under https://github.com/tobna/TaylorShift.","sentences":["The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers.","Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance.","This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space.","We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements.","Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond.","For shorter sequences, TaylorShift scales comparably with the vanilla attention.","Furthermore, a classification benchmark across five tasks involving long sequences reveals no degradation in accuracy when employing Transformers equipped with TaylorShift.","For reproducibility, we provide access to our code under https://github.com/tobna/TaylorShift."],"url":"http://arxiv.org/abs/2403.02920v1","category":"cs.LG"}
{"created":"2024-03-05 12:34:36","title":"A Miniaturized Device for Ultrafast On-demand Drug Release based on a Gigahertz Ultrasonic Resonator","abstract":"On-demand controlled drug delivery is essential for the treatment of a wide range of chronic diseases. As the drug is released at the time when required, its efficacy is boosted and the side effects are minimized. However, so far, drug delivery devices often rely on the passive diffusion process for a sustained release, which is slow and uncontrollable. Here, we present a miniaturized microfluidic device for wirelessly controlled ultrafast active drug delivery, driven by an oscillating solid-liquid interface. The oscillation generates acoustic streaming in the drug reservoir, which opens an elastic valve to deliver the drug. High-speed microscopy reveals the fast response of the valve on the order of 1 ms, which is more than three orders of magnitude faster than the start-of-the-art. The amount of the released drug exhibits a linear relationship with the working time and the electric power applied to the ultrasonic resonator. The trigger of the release is wirelessly controlled via a magnetic field, and the system shows stable output in a continuous experiment for two weeks. The integrated system shows great promise as a long-term controlled drug delivery implant for chronic diseases.","sentences":["On-demand controlled drug delivery is essential for the treatment of a wide range of chronic diseases.","As the drug is released at the time when required, its efficacy is boosted and the side effects are minimized.","However, so far, drug delivery devices often rely on the passive diffusion process for a sustained release, which is slow and uncontrollable.","Here, we present a miniaturized microfluidic device for wirelessly controlled ultrafast active drug delivery, driven by an oscillating solid-liquid interface.","The oscillation generates acoustic streaming in the drug reservoir, which opens an elastic valve to deliver the drug.","High-speed microscopy reveals the fast response of the valve on the order of 1 ms, which is more than three orders of magnitude faster than the start-of-the-art.","The amount of the released drug exhibits a linear relationship with the working time and the electric power applied to the ultrasonic resonator.","The trigger of the release is wirelessly controlled via a magnetic field, and the system shows stable output in a continuous experiment for two weeks.","The integrated system shows great promise as a long-term controlled drug delivery implant for chronic diseases."],"url":"http://arxiv.org/abs/2403.02917v1","category":"cs.RO"}
{"created":"2024-03-05 12:31:24","title":"DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting","abstract":"The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level. However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.","sentences":["The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment.","Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment.","To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors.","These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region.","Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical.","Worse still, the complex technical design may ultimately lead to a model with weak generalizability.","In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions.","To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level.","However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps.","To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect.","During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions."],"url":"http://arxiv.org/abs/2403.02914v1","category":"cs.AI"}
{"created":"2024-03-05 12:21:57","title":"ImgTrojan: Jailbreaking Vision-Language Models with ONE Image","abstract":"There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.","sentences":["There has been an increasing interest in the alignment of large language models (LLMs) with human values.","However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored.","In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions.","A scenario where our poisoned (image, text) data pairs are included in the training data is assumed.","By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images.","Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate.","For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack.","Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided.","We demonstrate the efficacy of our attack by comparing it with baseline methods."],"url":"http://arxiv.org/abs/2403.02910v1","category":"cs.CV"}
{"created":"2024-03-05 12:18:12","title":"Gaze-Vector Estimation in the Dark with Temporally Encoded Event-driven Neural Networks","abstract":"In this paper, we address the intricate challenge of gaze vector prediction, a pivotal task with applications ranging from human-computer interaction to driver monitoring systems. Our innovative approach is designed for the demanding setting of extremely low-light conditions, leveraging a novel temporal event encoding scheme, and a dedicated neural network architecture. The temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS) events with grayscale guide frames, generating consecutively encoded images for input into our neural network. This unique solution not only captures diverse gaze responses from participants within the active age group but also introduces a curated dataset tailored for low-light conditions. The encoded temporal frames paired with our network showcase impressive spatial localization and reliable gaze direction in their predictions. Achieving a remarkable 100-pixel accuracy of 100%, our research underscores the potency of our neural network to work with temporally consecutive encoded images for precise gaze vector predictions in challenging low-light videos, contributing to the advancement of gaze prediction technologies.","sentences":["In this paper, we address the intricate challenge of gaze vector prediction, a pivotal task with applications ranging from human-computer interaction to driver monitoring systems.","Our innovative approach is designed for the demanding setting of extremely low-light conditions, leveraging a novel temporal event encoding scheme, and a dedicated neural network architecture.","The temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS) events with grayscale guide frames, generating consecutively encoded images for input into our neural network.","This unique solution not only captures diverse gaze responses from participants within the active age group but also introduces a curated dataset tailored for low-light conditions.","The encoded temporal frames paired with our network showcase impressive spatial localization and reliable gaze direction in their predictions.","Achieving a remarkable 100-pixel accuracy of 100%, our research underscores the potency of our neural network to work with temporally consecutive encoded images for precise gaze vector predictions in challenging low-light videos, contributing to the advancement of gaze prediction technologies."],"url":"http://arxiv.org/abs/2403.02909v1","category":"cs.CV"}
{"created":"2024-03-05 12:18:08","title":"Preserving Tangible and Intangible Cultural Heritage: the Cases of Volterra and Atari","abstract":"At first glance, the ruins of the Roman Theatre in the Italian town of Volterra have little in common with cassette tapes containing Atari games. One is certainly considered an important historical landmark, while the consensus on the importance of the other is partial at best. Still, both are remnants of times vastly different from the present and are at risk of oblivion. Unearthed architectural structures are exposed to the elements just as the deteriorating signals stored on magnetic tapes. However, the rate of deterioration is much faster with the magnetic media, as their life expectancy is counted in decades, whereas the Roman Theater, which is already in ruin, measures its lifespan in centuries. Hence, both would benefit from some form of digital preservation and reconstruction. In this panel, we discuss how to sustainably preserve tangible and intangible cultural artifacts for future generations.","sentences":["At first glance, the ruins of the Roman Theatre in the Italian town of Volterra have little in common with cassette tapes containing Atari games.","One is certainly considered an important historical landmark, while the consensus on the importance of the other is partial at best.","Still, both are remnants of times vastly different from the present and are at risk of oblivion.","Unearthed architectural structures are exposed to the elements just as the deteriorating signals stored on magnetic tapes.","However, the rate of deterioration is much faster with the magnetic media, as their life expectancy is counted in decades, whereas the Roman Theater, which is already in ruin, measures its lifespan in centuries.","Hence, both would benefit from some form of digital preservation and reconstruction.","In this panel, we discuss how to sustainably preserve tangible and intangible cultural artifacts for future generations."],"url":"http://arxiv.org/abs/2403.02908v1","category":"cs.CY"}
{"created":"2024-03-05 12:15:45","title":"The Future of Primordial Black Holes: Open Questions and Roadmap","abstract":"We discuss some of the the open questions and the roadmap in the physics of primordial black holes. Black holes are the only dark matter candidate that is known to actually exit. Their conjectured primordial role is admittedly based on hypothesis rather than fact, most straightforwardly as a simple extension to the standard models of inflation, or even, in homage to quantum physics, more controversially via a slowing-down of Hawking evaporation. Regardless of one's stance on the theoretical basis for their existence, the possibility of primordial black holes playing a novel role in dark matter physics and gravitational wave astronomy opens up a rich astrophysical phenomenology that we lay out in this brief overview.","sentences":["We discuss some of the the open questions and the roadmap in the physics of primordial black holes.","Black holes are the only dark matter candidate that is known to actually exit.","Their conjectured primordial role is admittedly based on hypothesis rather than fact, most straightforwardly as a simple extension to the standard models of inflation, or even, in homage to quantum physics, more controversially via a slowing-down of Hawking evaporation.","Regardless of one's stance on the theoretical basis for their existence, the possibility of primordial black holes playing a novel role in dark matter physics and gravitational wave astronomy opens up a rich astrophysical phenomenology that we lay out in this brief overview."],"url":"http://arxiv.org/abs/2403.02907v1","category":"astro-ph.CO"}
{"created":"2024-03-05 12:13:18","title":"MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model","abstract":"The body movements accompanying speech aid speakers in expressing their ideas. Co-speech motion generation is one of the important approaches for synthesizing realistic avatars. Due to the intricate correspondence between speech and motion, generating realistic and diverse motion is a challenging task. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion generation framework based on the diffusion model to ensure both the authenticity and diversity of generated motion. We propose a progressive fusion strategy to enhance the interaction of inter-modal and intra-modal, efficiently integrating multi-modal information. Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles. Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features. Besides, we propose a geometric loss to enforce the joints' velocity and acceleration coherence among frames. Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion. Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body.","sentences":["The body movements accompanying speech aid speakers in expressing their ideas.","Co-speech motion generation is one of the important approaches for synthesizing realistic avatars.","Due to the intricate correspondence between speech and motion, generating realistic and diverse motion is a challenging task.","In this paper, we propose MMoFusion, a Multi-modal co-speech Motion generation framework based on the diffusion model to ensure both the authenticity and diversity of generated motion.","We propose a progressive fusion strategy to enhance the interaction of inter-modal and intra-modal, efficiently integrating multi-modal information.","Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles.","Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features.","Besides, we propose a geometric loss to enforce the joints' velocity and acceleration coherence among frames.","Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion.","Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body."],"url":"http://arxiv.org/abs/2403.02905v1","category":"cs.MM"}
{"created":"2024-03-05 12:11:07","title":"A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods","abstract":"Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.","sentences":["Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text.","ATS has drawn considerable interest in both academic and industrial circles.","Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint.","Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods.","In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature.","To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods."],"url":"http://arxiv.org/abs/2403.02901v1","category":"cs.AI"}
{"created":"2024-03-05 12:06:48","title":"Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation","abstract":"Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches.","sentences":["Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts.","A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation.","Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer.","Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically.","To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings.","Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way.","Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings.","These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss.","Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.02899v1","category":"cs.AI"}
{"created":"2024-03-05 12:05:23","title":"Rare $B$ and $K$ decays in a scotogenic model","abstract":"A scotogenic model can radiatively generate the observed neutrino mass, provide a dark matter candidate, and lead to rare lepton flavor-violating processes. We aim to extend the model to establish a potential connection to the quark flavor-related processes within the framework of scotogenesis, enhancing the unexpectedly large branching ratio (BR) of $B^+\\to K^+ \\nu \\bar\\nu$, observed by Belle II Collaboration. Meanwhile, the model can address tensions between some experimental measurements and standard model (SM) predictions in flavor physics, such as the muon $g-2$ excess and the higher BR of $B_s \\to \\mu^- \\mu^+$. We introduce in the model the following dark particles: a neutral singlet Dirac-type lepton ($N$); two inert Higgs doublets ($\\eta_{1,2}$), with one of which carrying a lepton number; a charged singlet dark scalar $(\\chi^+)$, and a singlet vector-like up-type dark quark ($T$). The first two entities are responsible for the radiative neutrino mass, and $\\chi^+$ couples to right-handed quarks and leptons and can resolve the tensions existing in muon $g-2$ and $B_s\\to \\mu^- \\mu^+$. Furthermore, the BR of $B^+ \\to K^+ \\nu \\bar\\nu$ can be enhanced up to a factor of 2 compared to the SM prediction through the mediations of the dark $T$ and the charged scalars. In addition, we also study the impacts on the $K\\to \\pi \\nu \\bar\\nu$ decays.","sentences":["A scotogenic model can radiatively generate the observed neutrino mass, provide a dark matter candidate, and lead to rare lepton flavor-violating processes.","We aim to extend the model to establish a potential connection to the quark flavor-related processes within the framework of scotogenesis, enhancing the unexpectedly large branching ratio (BR) of $B^+\\to K^+ \\nu \\bar\\nu$, observed by Belle II Collaboration.","Meanwhile, the model can address tensions between some experimental measurements and standard model (SM) predictions in flavor physics, such as the muon $g-2$ excess and the higher BR of $B_s \\to \\mu^- \\mu^+$. We introduce in the model the following dark particles: a neutral singlet Dirac-type lepton ($N$); two inert Higgs doublets ($\\eta_{1,2}$), with one of which carrying a lepton number; a charged singlet dark scalar $(\\chi^+)$, and a singlet vector-like up-type dark quark ($T$).","The first two entities are responsible for the radiative neutrino mass, and $\\chi^+$ couples to right-handed quarks and leptons and can resolve the tensions existing in muon $g-2$ and $B_s\\to \\mu^- \\mu^+$.","Furthermore, the BR of $B^+ \\to K^+ \\nu \\bar\\nu$ can be enhanced up to a factor of 2 compared to the SM prediction through the mediations of the dark $T$ and the charged scalars.","In addition, we also study the impacts on the $K\\to \\pi \\nu \\bar\\nu$ decays."],"url":"http://arxiv.org/abs/2403.02897v1","category":"hep-ph"}
{"created":"2024-03-05 11:59:06","title":"DIFNet: SAR RFI suppression based on domain invariant features","abstract":"Synthetic aperture radar is a high-resolution two-dimensional imaging radar, however, during the imaging process, SAR is susceptible to intentional and unintentional interference, with radio frequency interference (RFI) being the most common type, leading to a severe degradation in image quality. Although inpainting networks have achieved excellent results, their generalization is unclear, and whether they still work effectively in cross-sensor experiments needs further verification. Through time-frequency analysis of interference signals, we find that interference holds domain invariant features between different sensors. Therefore, this paper reconstructs the loss function and extracts the domain invariant features to improve the generalization. Ultimately, this paper proposes a SAR RFI suppression method based on domain invariant features, and embeds the RFI suppression into SAR imaging process. Compared to traditional notch filtering methods, the proposed approach not only removes interference but also effectively preserves strong scattering targets. Compared to PISNet, our method can extract domain invariant features and holds better generalization ability, and even in the cross-sensor experiment, our method can still achieve excellent results.","sentences":["Synthetic aperture radar is a high-resolution two-dimensional imaging radar, however, during the imaging process, SAR is susceptible to intentional and unintentional interference, with radio frequency interference (RFI) being the most common type, leading to a severe degradation in image quality.","Although inpainting networks have achieved excellent results, their generalization is unclear, and whether they still work effectively in cross-sensor experiments needs further verification.","Through time-frequency analysis of interference signals, we find that interference holds domain invariant features between different sensors.","Therefore, this paper reconstructs the loss function and extracts the domain invariant features to improve the generalization.","Ultimately, this paper proposes a SAR RFI suppression method based on domain invariant features, and embeds the RFI suppression into SAR imaging process.","Compared to traditional notch filtering methods, the proposed approach not only removes interference but also effectively preserves strong scattering targets.","Compared to PISNet, our method can extract domain invariant features and holds better generalization ability, and even in the cross-sensor experiment, our method can still achieve excellent results."],"url":"http://arxiv.org/abs/2403.02894v1","category":"eess.SP"}
{"created":"2024-03-05 11:57:21","title":"Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning","abstract":"Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios respectively. Notably, in multilingual scenario, our zero-shot framework even exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance.","sentences":["Event Causality Identification (ECI) refers to detect causal relations between events in texts.","However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored.","In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI.","Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document.","Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages.","Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios respectively.","Notably, in multilingual scenario, our zero-shot framework even exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance."],"url":"http://arxiv.org/abs/2403.02893v1","category":"cs.CL"}
{"created":"2024-03-05 11:57:10","title":"Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams","abstract":"This work addresses the task of long-term person re-identification. Typically, person re-identification assumes that people do not change their clothes, which limits its applications to short-term scenarios. To overcome this limitation, we investigate long-term person re-identification, which considers both clothes-changing and clothes-consistent scenarios. In this paper, we propose a novel framework that effectively learns and utilizes both global and local information. The proposed framework consists of three streams: global, local body part, and head streams. The global and head streams encode identity-relevant information from an entire image and a cropped image of the head region, respectively. Both streams encode the most distinct, less distinct, and average features using the combinations of adversarial erasing, max pooling, and average pooling. The local body part stream extracts identity-related information for each body part, allowing it to be compared with the same body part from another image. Since body part annotations are not available in re-identification datasets, pseudo-labels are generated using clustering. These labels are then utilized to train a body part segmentation head in the local body part stream. The proposed framework is trained by backpropagating the weighted summation of the identity classification loss, the pair-based loss, and the pseudo body part segmentation loss. To demonstrate the effectiveness of the proposed method, we conducted experiments on three publicly available datasets (Celeb-reID, PRCC, and VC-Clothes). The experimental results demonstrate that the proposed method outperforms the previous state-of-the-art method.","sentences":["This work addresses the task of long-term person re-identification.","Typically, person re-identification assumes that people do not change their clothes, which limits its applications to short-term scenarios.","To overcome this limitation, we investigate long-term person re-identification, which considers both clothes-changing and clothes-consistent scenarios.","In this paper, we propose a novel framework that effectively learns and utilizes both global and local information.","The proposed framework consists of three streams: global, local body part, and head streams.","The global and head streams encode identity-relevant information from an entire image and a cropped image of the head region, respectively.","Both streams encode the most distinct, less distinct, and average features using the combinations of adversarial erasing, max pooling, and average pooling.","The local body part stream extracts identity-related information for each body part, allowing it to be compared with the same body part from another image.","Since body part annotations are not available in re-identification datasets, pseudo-labels are generated using clustering.","These labels are then utilized to train a body part segmentation head in the local body part stream.","The proposed framework is trained by backpropagating the weighted summation of the identity classification loss, the pair-based loss, and the pseudo body part segmentation loss.","To demonstrate the effectiveness of the proposed method, we conducted experiments on three publicly available datasets (Celeb-reID, PRCC, and VC-Clothes).","The experimental results demonstrate that the proposed method outperforms the previous state-of-the-art method."],"url":"http://arxiv.org/abs/2403.02892v1","category":"cs.CV"}
{"created":"2024-03-05 11:48:35","title":"Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders","abstract":"Learned image compression codecs have recently achieved impressive compression performances surpassing the most efficient image coding architectures. However, most approaches are trained to minimize rate and distortion which often leads to unsatisfactory visual results at low bitrates since perceptual metrics are not taken into account. In this paper, we show that conditional diffusion models can lead to promising results in the generative compression task when used as a decoder, and that, given a compressed representation, they allow creating new tradeoff points between distortion and perception at the decoder side based on the sampling method.","sentences":["Learned image compression codecs have recently achieved impressive compression performances surpassing the most efficient image coding architectures.","However, most approaches are trained to minimize rate and distortion which often leads to unsatisfactory visual results at low bitrates since perceptual metrics are not taken into account.","In this paper, we show that conditional diffusion models can lead to promising results in the generative compression task when used as a decoder, and that, given a compressed representation, they allow creating new tradeoff points between distortion and perception at the decoder side based on the sampling method."],"url":"http://arxiv.org/abs/2403.02887v1","category":"cs.CV"}
{"created":"2024-03-05 11:44:14","title":"Revisiting Confidence Estimation: Towards Reliable Failure Prediction","abstract":"Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios. Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction. The code is available at \\url{https://github.com/Impression2805/FMFP}.","sentences":["Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications.","However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes.","In recent years, many confidence calibration and OOD detection methods have been developed.","In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors.","We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not.","Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios.","Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction.","The code is available at \\url{https://github.com/Impression2805/FMFP}."],"url":"http://arxiv.org/abs/2403.02886v1","category":"cs.CV"}
{"created":"2024-03-05 11:42:59","title":"MathScale: Scaling Instruction Tuning for Mathematical Reasoning","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving.","However, their proficiency in solving mathematical problems remains inadequate.","We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}).","Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions.","MathScale exhibits effective scalability along the size axis of the math dataset that we generate.","As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs.","To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems.","We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning.","Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively."],"url":"http://arxiv.org/abs/2403.02884v1","category":"cs.CL"}
{"created":"2024-03-05 11:42:43","title":"Canonical Hamiltonian Guiding Center Dynamics and Its Intrinsic Magnetic Moment","abstract":"The concept of guiding center is potent in astrophysics, space plasmas, fusion researches, and arc plasmas to solve the multi-scale dynamics of magnetized plasmas. In this letter, we rigorously prove that the guiding center dynamics can generally be described as a constrained canonical Hamiltonian system with two constraints in six dimensional phase space, and that the solution flow of the guiding center lies on a canonical symplectic sub-manifold. The guiding center can thus be modeled as a pseudo-particle with an intrinsic magnetic moment, which properly replaces the charged particle dynamics on time scales larger than the gyro-period. The complete dynamical behaviors, such as the velocity and force, of the guiding center pseudo-particle can be clearly deduced from the model. Furthermore, a series of related theories, such as symplectic numerical methods, the canonical gyro-kinetic theory, and canonical particle-in-cell algorithms can be systematically developed based on the canonical guiding center system. The canonical guiding center theory also provides an enlightenment for the origin of the intrinsic magnetic moment.","sentences":["The concept of guiding center is potent in astrophysics, space plasmas, fusion researches, and arc plasmas to solve the multi-scale dynamics of magnetized plasmas.","In this letter, we rigorously prove that the guiding center dynamics can generally be described as a constrained canonical Hamiltonian system with two constraints in six dimensional phase space, and that the solution flow of the guiding center lies on a canonical symplectic sub-manifold.","The guiding center can thus be modeled as a pseudo-particle with an intrinsic magnetic moment, which properly replaces the charged particle dynamics on time scales larger than the gyro-period.","The complete dynamical behaviors, such as the velocity and force, of the guiding center pseudo-particle can be clearly deduced from the model.","Furthermore, a series of related theories, such as symplectic numerical methods, the canonical gyro-kinetic theory, and canonical particle-in-cell algorithms can be systematically developed based on the canonical guiding center system.","The canonical guiding center theory also provides an enlightenment for the origin of the intrinsic magnetic moment."],"url":"http://arxiv.org/abs/2403.02883v1","category":"physics.plasm-ph"}
{"created":"2024-03-05 11:39:17","title":"Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement","abstract":"Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper.","sentences":["Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application.","Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation.","To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED.","It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning.","Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions.","Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement.","In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations.","Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities.","We will open the source code upon acceptance of the paper."],"url":"http://arxiv.org/abs/2403.02879v1","category":"cs.CV"}
{"created":"2024-03-05 11:39:11","title":"Primordial black hole formation processes with full numerical relativity","abstract":"See thesis for complete abstract.   Primordial black holes (PBHs) can form in the early universe, and there are several mass windows in which their abundance today may be large enough to comprise a significant part of the dark matter density. Additionally, numerical relativity (NR) allows one to investigate the formation processes of PBHs in the fully nonlinear strong-gravity regime. In this thesis, we will describe the use of NR methods to study PBH formation, motivated in particular by open questions about the nonspherical effects PBH formation in a matter-dominated early universe.   We demonstrate that superhorizon non-linear perturbations can collapse and form PBHs via the direct collapse or the accretion collapse mechanisms in a matter-dominated universe. The heaviest perturbations collapse via the direct collapse mechanism, while lighter perturbations trigger an accretion process that causes a rapid collapse of the ambient DM. From the hoop conjecture we propose an analytic criterion to determine whether a given perturbation will collapse via the direct or accretion mechanism and we compute the timescale of collapse. Independent of the formation mechanism, the PBH forms within an efold after collapse is initiated and with a small initial mass compared to the Hubble horizon, $M_\\textrm{BH} H_0\\sim 10^{-2}m_\\mathrm{Pl}^2$. Finally, we find that PBH formation is followed by extremely rapid growth $M_\\textrm{BH}\\propto H^{-\\beta}$ with $\\beta\\gg 1$, during which the PBH acquires most of its mass.   Furthermore, we study the formation of spinning primordial black holes during an early matter-dominated era. Using non-linear 3+1D general relativistic simulations, we compute the efficiency of mass and angular momentum transfer in the process -- which we find to be $\\mathcal{O}(10\\%)$.   Abstract continues in thesis.","sentences":["See thesis for complete abstract.   ","Primordial black holes (PBHs) can form in the early universe, and there are several mass windows in which their abundance today may be large enough to comprise a significant part of the dark matter density.","Additionally, numerical relativity (NR) allows one to investigate the formation processes of PBHs in the fully nonlinear strong-gravity regime.","In this thesis, we will describe the use of NR methods to study PBH formation, motivated in particular by open questions about the nonspherical effects PBH formation in a matter-dominated early universe.   ","We demonstrate that superhorizon non-linear perturbations can collapse and form PBHs via the direct collapse or the accretion collapse mechanisms in a matter-dominated universe.","The heaviest perturbations collapse via the direct collapse mechanism, while lighter perturbations trigger an accretion process that causes a rapid collapse of the ambient DM.","From the hoop conjecture we propose an analytic criterion to determine whether a given perturbation will collapse via the direct or accretion mechanism and we compute the timescale of collapse.","Independent of the formation mechanism, the PBH forms within an efold after collapse is initiated and with a small initial mass compared to the Hubble horizon,","$M_\\textrm{BH} H_0\\sim 10^{-2}m_\\mathrm{Pl}^2$.","Finally, we find that PBH formation is followed by extremely rapid growth $M_\\textrm{BH}\\propto H^{-\\beta}$ with $\\beta\\gg 1$, during which the PBH acquires most of its mass.   ","Furthermore, we study the formation of spinning primordial black holes during an early matter-dominated era.","Using non-linear 3+1D general relativistic simulations, we compute the efficiency of mass and angular momentum transfer in the process -- which we find to be $\\mathcal{O}(10\\%)$.   Abstract continues in thesis."],"url":"http://arxiv.org/abs/2403.02878v1","category":"astro-ph.CO"}
{"created":"2024-03-05 11:39:07","title":"ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving","abstract":"End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm. One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate. The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution. In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical. In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD. Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes. Empirically, we show that our planning-oriented approach could outperform general active learning methods by a large margin. Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data. We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts.","sentences":["End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm.","One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate.","The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution.","In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical.","In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD.","Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes.","Empirically, we show that our planning-oriented approach could outperform general active learning methods by a large margin.","Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data.","We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts."],"url":"http://arxiv.org/abs/2403.02877v1","category":"cs.CV"}
{"created":"2024-03-05 11:38:48","title":"Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples","abstract":"Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show significant improvements in fine-grained concept understanding across a wide range of vision-language datasets, including our InpaintCOCO dataset.","sentences":["Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding.","This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function.","Consequently, the models struggle with fine-grained semantic differences.","To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples.","The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment.","Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models.","We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions.","Our results show significant improvements in fine-grained concept understanding across a wide range of vision-language datasets, including our InpaintCOCO dataset."],"url":"http://arxiv.org/abs/2403.02875v1","category":"cs.CV"}
{"created":"2024-03-05 11:38:20","title":"A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails","abstract":"This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.","sentences":["This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization.","We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze.","This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities.","Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique."],"url":"http://arxiv.org/abs/2403.02873v1","category":"cs.LG"}
{"created":"2024-03-05 11:26:22","title":"Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices","abstract":"With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.","sentences":["With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models.","Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models.","Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices.","As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID).","Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others.","Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model.","Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions.","The results show up to 5.8 times better performance than when the adversary has no model information about the victim model."],"url":"http://arxiv.org/abs/2403.02870v1","category":"cs.AI"}
{"created":"2024-03-05 11:21:51","title":"Autonomous frequency locking for zero-offset microcomb","abstract":"The stabilization of optical frequency comb conventionally relies on active electronic feedback loops and stable frequency references. Here, we propose a new approach for autonomous frequency locking (AFL) to generate a zero-offset frequency comb based on cooperative nonlinear optical processes in a microcavity. In a simplified few-mode system, AFL enables the concept of fractional harmonic generation as a zero-offset multi-laser reference for measuring the carrier envelope offset frequency ($f_{\\mathrm{ceo}}$) of frequency combs spanning less than one octave, such as 1/3 octave. Combining with Kerr comb generation in a microcaivity, AFL is further applied to directly generate zero-$f_{\\mathrm{ceo}}$ soliton comb that is robust against fluctuations in pump laser and cavity resonances. Numerical simulations validate the AFL scheme, showing good agreement with analytical prediction of the locking condition. This work presents a new pathway for exploring novel frequency locking mechanisms and technologies using integrated photonic devices, and also appeals further investigations of cooperative nonlinear optics processes in microcavities.","sentences":["The stabilization of optical frequency comb conventionally relies on active electronic feedback loops and stable frequency references.","Here, we propose a new approach for autonomous frequency locking (AFL) to generate a zero-offset frequency comb based on cooperative nonlinear optical processes in a microcavity.","In a simplified few-mode system, AFL enables the concept of fractional harmonic generation as a zero-offset multi-laser reference for measuring the carrier envelope offset frequency ($f_{\\mathrm{ceo}}$) of frequency combs spanning less than one octave, such as 1/3 octave.","Combining with Kerr comb generation in a microcaivity, AFL is further applied to directly generate zero-$f_{\\mathrm{ceo}}$ soliton comb that is robust against fluctuations in pump laser and cavity resonances.","Numerical simulations validate the AFL scheme, showing good agreement with analytical prediction of the locking condition.","This work presents a new pathway for exploring novel frequency locking mechanisms and technologies using integrated photonic devices, and also appeals further investigations of cooperative nonlinear optics processes in microcavities."],"url":"http://arxiv.org/abs/2403.02868v1","category":"physics.optics"}
{"created":"2024-03-05 11:18:06","title":"SCINE -- Software for Chemical Interaction Networks","abstract":"The software for chemical interaction networks (SCINE) project aims at pushing the frontier of quantum chemical calculations on molecular structures to a new level. While calculations on individual structures as well as on simple relations between them (\\textit{e.g.}, as given by an intrinsic reaction coordinate) have become routine in chemistry, new developments have pushed the frontier in the field to high-throughput calculations. Chemical relations may be created by a search for specific molecular properties in a molecular design attempt or they can be defined by a set of elementary reaction steps that form a chemical reaction network. The software modules of SCINE have been designed to facilitate such studies. The features of the modules are (i) general applicability of the applied methodologies ranging from electronic structure (no restriction to specific elements of the periodic table) to microkinetic modeling (with little restrictions on molecularity), full modularity so that SCINE modules can also be applied as stand-alone programs or be exchanged for external software packages that fulfill a similar purpose (to increase options for computational campaigns and to provide alternatives in case of tasks that are hard or impossible to accomplish with certain programs), (ii) high stability and autonomous operations so that control and steering by an operator is as easy as possible, and (iii) easy embedding into complex heterogeneous environments for molecular structures taken individually or in the context of a reaction network. A graphical user interface unites all modules and ensures interoperability. All components of the software have been made available open source and free of charge.","sentences":["The software for chemical interaction networks (SCINE) project aims at pushing the frontier of quantum chemical calculations on molecular structures to a new level.","While calculations on individual structures as well as on simple relations between them (\\textit{e.g.}, as given by an intrinsic reaction coordinate) have become routine in chemistry, new developments have pushed the frontier in the field to high-throughput calculations.","Chemical relations may be created by a search for specific molecular properties in a molecular design attempt or they can be defined by a set of elementary reaction steps that form a chemical reaction network.","The software modules of SCINE have been designed to facilitate such studies.","The features of the modules are (i) general applicability of the applied methodologies ranging from electronic structure (no restriction to specific elements of the periodic table) to microkinetic modeling (with little restrictions on molecularity), full modularity so that SCINE modules can also be applied as stand-alone programs or be exchanged for external software packages that fulfill a similar purpose (to increase options for computational campaigns and to provide alternatives in case of tasks that are hard or impossible to accomplish with certain programs), (ii) high stability and autonomous operations so that control and steering by an operator is as easy as possible, and (iii) easy embedding into complex heterogeneous environments for molecular structures taken individually or in the context of a reaction network.","A graphical user interface unites all modules and ensures interoperability.","All components of the software have been made available open source and free of charge."],"url":"http://arxiv.org/abs/2403.02865v1","category":"physics.chem-ph"}
{"created":"2024-03-05 11:14:06","title":"Recent Developments in Holographic Black Hole Chemistry","abstract":"One of the major developments in classical black hole thermodynamics is the inclusion of vacuum energy in the form of thermodynamic pressure. Known as Black Hole Chemistry, this subdiscipline has led to the realization that anti de Sitter black holes exhibit a broad variety of phase transitions that are essentially the same as those observed in chemical systems. Since the pressure is given in terms of a negative cosmological constant (which parametrizes the vacuum energy), the holographic interpretation of Black Hole Chemistry has remained unclear. In the last few years there has been considerable progress in developing an exact dictionary between the bulk laws of Black Hole Chemistry and the laws of the dual Conformal Field Theory (CFT). Holographic Black Hole Chemistry is now becoming an established subfield, with a full thermodynamic bulk/boundary correspondence, and an emergent understanding of CFT phase behaviour and its correspondence in the bulk. Here I review these developments, highlighting key advances and briefly discussing future prospects for further research.","sentences":["One of the major developments in classical black hole thermodynamics is the inclusion of vacuum energy in the form of thermodynamic pressure.","Known as Black Hole Chemistry, this subdiscipline has led to the realization that anti de Sitter black holes exhibit a broad variety of phase transitions that are essentially the same as those observed in chemical systems.","Since the pressure is given in terms of a negative cosmological constant (which parametrizes the vacuum energy), the holographic interpretation of Black Hole Chemistry has remained unclear.","In the last few years there has been considerable progress in developing an exact dictionary between the bulk laws of Black Hole Chemistry and the laws of the dual Conformal Field Theory (CFT).","Holographic Black Hole Chemistry is now becoming an established subfield, with a full thermodynamic bulk/boundary correspondence, and an emergent understanding of CFT phase behaviour and its correspondence in the bulk.","Here I review these developments, highlighting key advances and briefly discussing future prospects for further research."],"url":"http://arxiv.org/abs/2403.02864v1","category":"hep-th"}
{"created":"2024-03-05 11:11:55","title":"Numerical investigation of stabilization in the Hybridizable Discontinuous Galerkin method for linear anisotropic elastic equation","abstract":"This work concerns the implementation of the hybridizable discontinuous Galerkin (HDG) method to solve the linear anisotropic elastic equation in the frequency domain. First-order formulation with the compliance tensor and Voigt notation are employed to provide a compact description of the discretized problem and flexibility with highly heterogeneous media. We further focus on the question of optimal choice of stabilization in the definition of HDG numerical traces. For this purpose, we construct a hybridized Godunov-upwind flux for anisotropic elasticity possessing three distinct wavespeeds. This stabilization removes the need to choose scaling factors, contrary to identity and Kelvin-Christoffel based stabilizations which are popular choices in literature. We carry out comparisons among these families for isotropic and anisotropic material, with constant background and highly heterogeneous ones, in two and three dimensions. They establish the optimality of the Godunov stabilization which can be used as a reference choice for generic material and different types of waves.","sentences":["This work concerns the implementation of the hybridizable discontinuous Galerkin (HDG) method to solve the linear anisotropic elastic equation in the frequency domain.","First-order formulation with the compliance tensor and Voigt notation are employed to provide a compact description of the discretized problem and flexibility with highly heterogeneous media.","We further focus on the question of optimal choice of stabilization in the definition of HDG numerical traces.","For this purpose, we construct a hybridized Godunov-upwind flux for anisotropic elasticity possessing three distinct wavespeeds.","This stabilization removes the need to choose scaling factors, contrary to identity and Kelvin-Christoffel based stabilizations which are popular choices in literature.","We carry out comparisons among these families for isotropic and anisotropic material, with constant background and highly heterogeneous ones, in two and three dimensions.","They establish the optimality of the Godunov stabilization which can be used as a reference choice for generic material and different types of waves."],"url":"http://arxiv.org/abs/2403.02862v1","category":"math.AP"}
{"created":"2024-03-05 11:09:45","title":"Novel Limited Memory Quasi-Newton Methods Based On Optimal Matrix Approximation","abstract":"Update formulas for the Hessian approximations in quasi-Newton methods such as BFGS can be derived as analytical solutions to certain nearest-matrix problems. In this article, we propose a similar idea for deriving new limited memory versions of quasi-Newton methods. Most limited memory quasi-Newton methods make use of Hessian approximations that can be written as a scaled identity matrix plus a symmetric matrix with limited rank. We derive a way of finding the nearest matrix of this type to an arbitrary symmetric matrix, in either the Frobenius norm, the induced $l^2$ norm, or a dissimilarity measure for positive definite matrices in terms of trace and determinant. In doing so, we lay down a framework for more general matrix optimization problems with unitarily invariant matrix norms and arbitrary constraints on the set of eigenvalues. We then propose a trust region method in which the Hessian approximation, after having been updated by a Broyden class formula and used to solve a trust-region problem, is replaced by one of its closest limited memory approximations. We propose to store the Hessian approximation in terms of its eigenvectors and eigenvalues in a way that completely defines its eigenvalue decomposition, as this simplifies both the solution of the trust region subproblem and the nearest limited memory matrix problem. Our method is compared to a reference trust region method with the usual limited memory BFGS updates, and is shown to require fewer iterations and the storage of fewer vectors for a variety of test problems.","sentences":["Update formulas for the Hessian approximations in quasi-Newton methods such as BFGS can be derived as analytical solutions to certain nearest-matrix problems.","In this article, we propose a similar idea for deriving new limited memory versions of quasi-Newton methods.","Most limited memory quasi-Newton methods make use of Hessian approximations that can be written as a scaled identity matrix plus a symmetric matrix with limited rank.","We derive a way of finding the nearest matrix of this type to an arbitrary symmetric matrix, in either the Frobenius norm, the induced $l^2$ norm, or a dissimilarity measure for positive definite matrices in terms of trace and determinant.","In doing so, we lay down a framework for more general matrix optimization problems with unitarily invariant matrix norms and arbitrary constraints on the set of eigenvalues.","We then propose a trust region method in which the Hessian approximation, after having been updated by a Broyden class formula and used to solve a trust-region problem, is replaced by one of its closest limited memory approximations.","We propose to store the Hessian approximation in terms of its eigenvectors and eigenvalues in a way that completely defines its eigenvalue decomposition, as this simplifies both the solution of the trust region subproblem and the nearest limited memory matrix problem.","Our method is compared to a reference trust region method with the usual limited memory BFGS updates, and is shown to require fewer iterations and the storage of fewer vectors for a variety of test problems."],"url":"http://arxiv.org/abs/2403.02860v1","category":"math.OC"}
{"created":"2024-03-05 10:53:21","title":"Refining the grading of irreducible Lie colour algebra representations","abstract":"We define the refining extension of representations for Lie colour algebras which simultaneously extends the representations and refines the grading to a larger grading group.We show that the refining extension provides a general method for deriving all finite-dimensional irreducible Lie colour algebra representations from those for Lie superalgebras. This method yields a bijection between equivalence classes but, despite this, Lie colour algebras maintain a non-trivial representation theory distinct from that of Lie superalgebras. We expect this result to be useful for the many applications of Lie colour algebras that make use of irreducible representations. In addition, we show that the refining extension provides a natural way to construct the Hilbert space realisation for colour algebra quantum-mechanical models by giving an example that has previously appeared in the literature.","sentences":["We define the refining extension of representations for Lie colour algebras which simultaneously extends the representations and refines the grading to a larger grading group.","We show that the refining extension provides a general method for deriving all finite-dimensional irreducible Lie colour algebra representations from those for Lie superalgebras.","This method yields a bijection between equivalence classes but, despite this, Lie colour algebras maintain a non-trivial representation theory distinct from that of Lie superalgebras.","We expect this result to be useful for the many applications of Lie colour algebras that make use of irreducible representations.","In addition, we show that the refining extension provides a natural way to construct the Hilbert space realisation for colour algebra quantum-mechanical models by giving an example that has previously appeared in the literature."],"url":"http://arxiv.org/abs/2403.02855v1","category":"math-ph"}
{"created":"2024-03-05 10:52:13","title":"STAR-RIS Assisted Wireless-Powered and Backscattering Mobile Edge Computing Networks","abstract":"Wireless powered and backscattering mobile edge computing (WPB-MEC) network is a novel network paradigm to supply energy supplies and computing resource to wireless sensors (WSs). However, its performance is seriously affected by severe attenuations and inappropriate assumptions of infinite computing capability at the hybrid access point (HAP). To address the above issues, in this paper, we propose a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided scheme for boosting the performance of WPB-MEC network under the constraint of finite computing capability. Specifically, energy-constrained WSs are able to offload tasks actively or passively from them to the HAP. In this process, the STAR-RIS is utilized to improve the quantity of harvested energy and strengthen the offloading efficiency by adapting its operating protocols. We then maximize the sum computational bits (SCBs) under the finite computing capability constraint. To handle the solving challenges, we first present interesting results in closed-form and then design a block coordinate descent (BCD) based algorithm, ensuring a near-optimal solution. Finally, simulation results are provided to confirm that our proposed scheme can improve the SCBs by 9.9 times compared to the local computing only scheme.","sentences":["Wireless powered and backscattering mobile edge computing (WPB-MEC) network is a novel network paradigm to supply energy supplies and computing resource to wireless sensors (WSs).","However, its performance is seriously affected by severe attenuations and inappropriate assumptions of infinite computing capability at the hybrid access point (HAP).","To address the above issues, in this paper, we propose a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided scheme for boosting the performance of WPB-MEC network under the constraint of finite computing capability.","Specifically, energy-constrained WSs are able to offload tasks actively or passively from them to the HAP.","In this process, the STAR-RIS is utilized to improve the quantity of harvested energy and strengthen the offloading efficiency by adapting its operating protocols.","We then maximize the sum computational bits (SCBs) under the finite computing capability constraint.","To handle the solving challenges, we first present interesting results in closed-form and then design a block coordinate descent (BCD) based algorithm, ensuring a near-optimal solution.","Finally, simulation results are provided to confirm that our proposed scheme can improve the SCBs by 9.9 times compared to the local computing only scheme."],"url":"http://arxiv.org/abs/2403.02854v1","category":"eess.SP"}
{"created":"2024-03-05 10:48:41","title":"Asymptotically safe cosmology with non-canonical scalar field","abstract":"We investigate the quantum modified cosmological dynamical equations in a Friedmann-Robertson-Walker universe filled with a barotropic fluid and a general non-canonical scalar field characterized by a Lagrangian similar to k-essence model but with a potential term. Quantum corrections are incorporated by considering the running of gravitational and potential couplings, employing the functional renormalization group approach. Covariant conservation of the non-canonical scalar field and the background barotropic fluid is considered separately, imposing a constraint resulting from the Bianchi identity. This constraint determines the evolution of the cut-off scale with the scale factor and also reveals cosmic fixed points, depending on whether the flow ceases or continues to evolve. We explore how the general non-canonical scalar field parameter affects the different types of cosmic fixed points and how it differs from the canonical case. Furthermore, we establish a bound on the ratio of the RG parameters involving the non-canonical parameter for which the universe may exhibit accelerated expansion for mixed fixed points. This bound indicates the non-canonical scalar field includes larger sets of RG fixed point which may give rise to an accelerated universe.","sentences":["We investigate the quantum modified cosmological dynamical equations in a Friedmann-Robertson-Walker universe filled with a barotropic fluid and a general non-canonical scalar field characterized by a Lagrangian similar to k-essence model but with a potential term.","Quantum corrections are incorporated by considering the running of gravitational and potential couplings, employing the functional renormalization group approach.","Covariant conservation of the non-canonical scalar field and the background barotropic fluid is considered separately, imposing a constraint resulting from the Bianchi identity.","This constraint determines the evolution of the cut-off scale with the scale factor and also reveals cosmic fixed points, depending on whether the flow ceases or continues to evolve.","We explore how the general non-canonical scalar field parameter affects the different types of cosmic fixed points and how it differs from the canonical case.","Furthermore, we establish a bound on the ratio of the RG parameters involving the non-canonical parameter for which the universe may exhibit accelerated expansion for mixed fixed points.","This bound indicates the non-canonical scalar field includes larger sets of RG fixed point which may give rise to an accelerated universe."],"url":"http://arxiv.org/abs/2403.02852v1","category":"gr-qc"}
{"created":"2024-03-05 10:36:27","title":"FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models","abstract":"Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose FLGuard, a novel byzantine-robust FL method that detects malicious clients and discards malicious local updates by utilizing the contrastive learning technique, which showed a tremendous improvement as a self-supervised learning method. With contrastive models, we design FLGuard as an ensemble scheme to maximize the defensive capability. We evaluate FLGuard extensively under various poisoning attacks and compare the accuracy of the global model with existing byzantine-robust FL methods. FLGuard outperforms the state-of-the-art defense methods in most cases and shows drastic improvement, especially in non-IID settings. https://github.com/201younghanlee/FLGuard","sentences":["Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets.","Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance.","However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients.","Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system.","However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID).","In this work, we propose FLGuard, a novel byzantine-robust FL method that detects malicious clients and discards malicious local updates by utilizing the contrastive learning technique, which showed a tremendous improvement as a self-supervised learning method.","With contrastive models, we design FLGuard as an ensemble scheme to maximize the defensive capability.","We evaluate FLGuard extensively under various poisoning attacks and compare the accuracy of the global model with existing byzantine-robust FL methods.","FLGuard outperforms the state-of-the-art defense methods in most cases and shows drastic improvement, especially in non-IID settings.","https://github.com/201younghanlee/FLGuard"],"url":"http://arxiv.org/abs/2403.02846v1","category":"cs.LG"}
{"created":"2024-03-05 10:29:45","title":"Generalized Hyperbolicity, Stability and Expansivity for Operators on Locally Convex Spaces","abstract":"We introduce and study the notions of (generalized) hyperbolicity, topological stability and (uniform) topological expansivity for operators on locally convex spaces. We prove that every generalized hyperbolic operator on a locally convex space has the finite shadowing property. Contrary to what happens in the Banach space setting, hyperbolic operators on Fr\\'echet spaces may fail to have the shadowing property, but we find additional conditions that ensure the validity of the shadowing property. Assuming that the space is sequentially complete, we prove that generalized hyperbolicity implies the strict periodic shadowing property, but we also show that the hypothesis of sequential completeness is essential. We show that operators with the periodic shadowing property on topological vector spaces have other interesting dynamical behaviors, including the fact that the restriction of such an operator to its chain recurrent set is topologically mixing and Devaney chaotic. We prove that topologically stable operators on locally convex spaces have the finite shadowing property and the strict periodic shadowing property. As a consequence, topologically stable operators on Banach spaces have the shadowing property. Moreover, we prove that generalized hyperbolicity implies topological stability for operators on Banach spaces. We prove that uniformly topologically expansive operators on locally convex spaces are neither Li-Yorke chaotic nor topologically transitive. Finally, we characterize the notion of topological expansivity for weighted shifts on Fr\\'echet sequence spaces. Several examples are provided.","sentences":["We introduce and study the notions of (generalized) hyperbolicity, topological stability and (uniform) topological expansivity for operators on locally convex spaces.","We prove that every generalized hyperbolic operator on a locally convex space has the finite shadowing property.","Contrary to what happens in the Banach space setting, hyperbolic operators on Fr\\'echet spaces may fail to have the shadowing property, but we find additional conditions that ensure the validity of the shadowing property.","Assuming that the space is sequentially complete, we prove that generalized hyperbolicity implies the strict periodic shadowing property, but we also show that the hypothesis of sequential completeness is essential.","We show that operators with the periodic shadowing property on topological vector spaces have other interesting dynamical behaviors, including the fact that the restriction of such an operator to its chain recurrent set is topologically mixing and Devaney chaotic.","We prove that topologically stable operators on locally convex spaces have the finite shadowing property and the strict periodic shadowing property.","As a consequence, topologically stable operators on Banach spaces have the shadowing property.","Moreover, we prove that generalized hyperbolicity implies topological stability for operators on Banach spaces.","We prove that uniformly topologically expansive operators on locally convex spaces are neither Li-Yorke chaotic nor topologically transitive.","Finally, we characterize the notion of topological expansivity for weighted shifts on Fr\\'echet sequence spaces.","Several examples are provided."],"url":"http://arxiv.org/abs/2403.02843v1","category":"math.DS"}
{"created":"2024-03-05 10:25:30","title":"On a theory of martingales for censoring","abstract":"A theory of martingales for censoring is developed. The Doob-Meyer martingale is shown to be inadequate in general, and a repaired martingale is proposed with a non-predictable centering term. Associated martingale transforms, variation processes, and covariation processes are developed based on a measure of half-predictability that generalizes predictability. The development is applied to study the Kaplan Meier estimator.","sentences":["A theory of martingales for censoring is developed.","The Doob-Meyer martingale is shown to be inadequate in general, and a repaired martingale is proposed with a non-predictable centering term.","Associated martingale transforms, variation processes, and covariation processes are developed based on a measure of half-predictability that generalizes predictability.","The development is applied to study the Kaplan Meier estimator."],"url":"http://arxiv.org/abs/2403.02840v1","category":"math.ST"}
{"created":"2024-03-05 10:00:38","title":"A chip-integrated comb-based microwave oscillator","abstract":"Low-noise microwave oscillators are cornerstones for wireless communication, radar and clocks. Optical frequency combs have enabled photonic microwaves with unrivalled noise performance and bandwidth. Emerging interest is to generate microwaves using chip-based frequency combs, namely microcombs. Here, we demonstrate the first, fully integrated, microcomb-based, microwave oscillator chip. The chip, powered by a microelectronic circuit, leverages hybrid integration of a DFB laser, a nonlinear microresonator, and a high-speed photodetector. Each component represents the best of its own class, yet allows large-volume manufacturing with low cost in CMOS foundries. The hybrid chip outputs an ultralow-noise laser of 6.9 Hz linewidth, a microcomb of 10.7 GHz repetition rate, and a 10.7 GHz microwave of 6.3 mHz linewidth -- all three in one entity of 76 mm$^2$ size.The microwave phase noise reaches -75/-105/-130 dBc/Hz at 1/10/100 kHz Fourier offset frequency. Our results can reinvigorate our information society for communication, sensing, timing and precision measurement.","sentences":["Low-noise microwave oscillators are cornerstones for wireless communication, radar and clocks.","Optical frequency combs have enabled photonic microwaves with unrivalled noise performance and bandwidth.","Emerging interest is to generate microwaves using chip-based frequency combs, namely microcombs.","Here, we demonstrate the first, fully integrated, microcomb-based, microwave oscillator chip.","The chip, powered by a microelectronic circuit, leverages hybrid integration of a DFB laser, a nonlinear microresonator, and a high-speed photodetector.","Each component represents the best of its own class, yet allows large-volume manufacturing with low cost in CMOS foundries.","The hybrid chip outputs an ultralow-noise laser of 6.9 Hz linewidth, a microcomb of 10.7 GHz repetition rate, and a 10.7 GHz microwave of 6.3 mHz linewidth -- all three in one entity of 76 mm$^2$ size.","The microwave phase noise reaches -75/-105/-130 dBc/Hz at 1/10/100 kHz Fourier offset frequency.","Our results can reinvigorate our information society for communication, sensing, timing and precision measurement."],"url":"http://arxiv.org/abs/2403.02828v1","category":"physics.optics"}
{"created":"2024-03-05 09:57:47","title":"Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation","abstract":"Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: https://noise-rectification.github.io.","sentences":["Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains.","Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains.","Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity.","We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process.","To this end, we propose an effective method that can be applied to mainstream video diffusion models.","This method achieves high fidelity based on supplementing more precise image information and noise rectification.","Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases.","Our method is tuning-free and plug-and-play.","The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos.","For more image-to-video generated results, please refer to the project website: https://noise-rectification.github.io."],"url":"http://arxiv.org/abs/2403.02827v1","category":"cs.CV"}
{"created":"2024-03-05 09:52:38","title":"Contrastive Pre-training for Deep Session Data Understanding","abstract":"Session data has been widely used for understanding user's behavior in e-commerce. Researchers are trying to leverage session data for different tasks, such as purchase intention prediction, remaining length prediction, recommendation, etc., as it provides context clues about the user's dynamic interests. However, online shopping session data is semi-structured and complex in nature, which contains both unstructured textual data about the products, search queries, and structured user action sequences. Most existing works focus on leveraging the coarse-grained item sequences for specific tasks, while largely ignore the fine-grained information from text and user action details. In this work, we delve into deep session data understanding via scrutinizing the various clues inside the rich information in user sessions. Specifically, we propose to pre-train a general-purpose User Behavior Model (UBM) over large-scale session data with rich details, such as product title, attributes and various kinds of user actions. A two-stage pre-training scheme is introduced to encourage the model to self-learn from various augmentations with contrastive learning objectives, which spans different granularity levels of session data. Then the well-trained session understanding model can be easily fine-tuned for various downstream tasks. Extensive experiments show that UBM better captures the complex intra-item semantic relations, inter-item connections and inter-interaction dependencies, leading to large performance gains as compared to the baselines on several downstream tasks. And it also demonstrates strong robustness when data is sparse.","sentences":["Session data has been widely used for understanding user's behavior in e-commerce.","Researchers are trying to leverage session data for different tasks, such as purchase intention prediction, remaining length prediction, recommendation, etc., as it provides context clues about the user's dynamic interests.","However, online shopping session data is semi-structured and complex in nature, which contains both unstructured textual data about the products, search queries, and structured user action sequences.","Most existing works focus on leveraging the coarse-grained item sequences for specific tasks, while largely ignore the fine-grained information from text and user action details.","In this work, we delve into deep session data understanding via scrutinizing the various clues inside the rich information in user sessions.","Specifically, we propose to pre-train a general-purpose User Behavior Model (UBM) over large-scale session data with rich details, such as product title, attributes and various kinds of user actions.","A two-stage pre-training scheme is introduced to encourage the model to self-learn from various augmentations with contrastive learning objectives, which spans different granularity levels of session data.","Then the well-trained session understanding model can be easily fine-tuned for various downstream tasks.","Extensive experiments show that UBM better captures the complex intra-item semantic relations, inter-item connections and inter-interaction dependencies, leading to large performance gains as compared to the baselines on several downstream tasks.","And it also demonstrates strong robustness when data is sparse."],"url":"http://arxiv.org/abs/2403.02825v1","category":"cs.IR"}
{"created":"2024-03-05 09:51:14","title":"Fluorescent nano- and microparticles for sensing cellular microenvironment: past, present and future applications","abstract":"The tumor microenvironment (TME) features distinct hallmarks, including acidosis, hypoxia, reactive oxygen species (ROS) generation, and altered ion fluxes, which are crucial targets for early cancer biomarker detection, tumor diagnosis, and therapeutic strategies. A variety of imaging and sensing techniques have been developed and employed in both research and clinical settings to visualize and monitor cellular and TME dynamics. Among these, ratiometric fluorescence-based sensors have emerged as powerful analytical tools, providing precise and sensitive insights into the TME and enabling real-time detection and tracking of dynamic changes. In this comprehensive review, we discuss the latest advancements in ratiometric fluorescent probes designed for optical mapping of pH, oxygen, ROS, ions, and biomarkers within the TME. We elucidate their structural designs and sensing mechanisms, as well as their applications in in vitro and in vivo detection. Furthermore, we explore integrated sensing platforms that reveal the spatiotemporal behavior of complex tumor cultures, highlighting the potential of high-resolution imaging techniques combined with computational methods. This review aims to provide a solid foundation for understanding the current state of the art and the future potential of fluorescent nano- and microparticles in the field of cellular microenvironment sensing.","sentences":["The tumor microenvironment (TME) features distinct hallmarks, including acidosis, hypoxia, reactive oxygen species (ROS) generation, and altered ion fluxes, which are crucial targets for early cancer biomarker detection, tumor diagnosis, and therapeutic strategies.","A variety of imaging and sensing techniques have been developed and employed in both research and clinical settings to visualize and monitor cellular and TME dynamics.","Among these, ratiometric fluorescence-based sensors have emerged as powerful analytical tools, providing precise and sensitive insights into the TME and enabling real-time detection and tracking of dynamic changes.","In this comprehensive review, we discuss the latest advancements in ratiometric fluorescent probes designed for optical mapping of pH, oxygen, ROS, ions, and biomarkers within the TME.","We elucidate their structural designs and sensing mechanisms, as well as their applications in in vitro and in vivo detection.","Furthermore, we explore integrated sensing platforms that reveal the spatiotemporal behavior of complex tumor cultures, highlighting the potential of high-resolution imaging techniques combined with computational methods.","This review aims to provide a solid foundation for understanding the current state of the art and the future potential of fluorescent nano- and microparticles in the field of cellular microenvironment sensing."],"url":"http://arxiv.org/abs/2403.02824v1","category":"physics.app-ph"}
{"created":"2024-03-05 09:48:11","title":"Highly Reproducible and CMOS-compatible VO2-based Oscillators for Brain-inspired Computing","abstract":"With remarkable electrical and optical switching properties induced at low power and near room temperature (68C), vanadium dioxide (VO2) has sparked rising interest in unconventional computing among the phase-change materials research community. The scalability and the potential to compute beyond the von Neumann model make VO2 especially appealing for implementation in oscillating neural networks for artificial intelligence (AI) applications, to solve constraint satisfaction problems, and for pattern recognition. Its integration into large networks of oscillators on a Silicon platform still poses challenges associated with the stabilization in the correct oxidation state and the ability to fabricate a structure with predictable electrical behavior showing very low variability. In this work, the role played by the different annealing parameters applied by three methods (slow thermal annealing, flash annealing, and rapid thermal annealing), following the vanadium oxide atomic layer deposition (ALD), on the formation of VO2 grains is studied and an optimal substrate stack configuration that minimizes variability between devices is proposed. Material and electrical characterizations are performed on the different films and a step-by-step recipe to build reproducible VO2-based oscillators is presented, which is argued to be made possible thanks to the introduction of a hafnium oxide (HfO2) layer between the silicon substrate and the vanadium oxide layer. Up to seven nearly identical VO2-based devices are contacted simultaneously to create a network of oscillators, paving the way for large-scale implementation of VO2 oscillating neural networks.","sentences":["With remarkable electrical and optical switching properties induced at low power and near room temperature (68C), vanadium dioxide (VO2) has sparked rising interest in unconventional computing among the phase-change materials research community.","The scalability and the potential to compute beyond the von Neumann model make VO2 especially appealing for implementation in oscillating neural networks for artificial intelligence (AI) applications, to solve constraint satisfaction problems, and for pattern recognition.","Its integration into large networks of oscillators on a Silicon platform still poses challenges associated with the stabilization in the correct oxidation state and the ability to fabricate a structure with predictable electrical behavior showing very low variability.","In this work, the role played by the different annealing parameters applied by three methods (slow thermal annealing, flash annealing, and rapid thermal annealing), following the vanadium oxide atomic layer deposition (ALD), on the formation of VO2 grains is studied and an optimal substrate stack configuration that minimizes variability between devices is proposed.","Material and electrical characterizations are performed on the different films and a step-by-step recipe to build reproducible VO2-based oscillators is presented, which is argued to be made possible thanks to the introduction of a hafnium oxide (HfO2) layer between the silicon substrate and the vanadium oxide layer.","Up to seven nearly identical VO2-based devices are contacted simultaneously to create a network of oscillators, paving the way for large-scale implementation of VO2 oscillating neural networks."],"url":"http://arxiv.org/abs/2403.02822v1","category":"physics.app-ph"}
{"created":"2024-03-05 09:44:19","title":"Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry","abstract":"In the wood industry, logs are commonly quality screened by discrete X-ray scans on a moving conveyor belt from a few source positions. Typically, two-dimensional (2D) slice-wise measurements are obtained by a sequential scanning geometry. Each 2D slice alone does not carry sufficient information for a three-dimensional tomographic reconstruction in which biological features of interest in the log are well preserved. In the present work, we propose a learned iterative reconstruction method based on the Learned Primal-Dual neural network, suited for sequential scanning geometries. Our method accumulates information between neighbouring slices, instead of only accounting for single slices during reconstruction. Our quantitative and qualitative evaluations with as few as five source positions show that our method yields reconstructions of logs that are sufficiently accurate to identify biological features like knots (branches), heartwood and sapwood.","sentences":["In the wood industry, logs are commonly quality screened by discrete X-ray scans on a moving conveyor belt from a few source positions.","Typically, two-dimensional (2D) slice-wise measurements are obtained by a sequential scanning geometry.","Each 2D slice alone does not carry sufficient information for a three-dimensional tomographic reconstruction in which biological features of interest in the log are well preserved.","In the present work, we propose a learned iterative reconstruction method based on the Learned Primal-Dual neural network, suited for sequential scanning geometries.","Our method accumulates information between neighbouring slices, instead of only accounting for single slices during reconstruction.","Our quantitative and qualitative evaluations with as few as five source positions show that our method yields reconstructions of logs that are sufficiently accurate to identify biological features like knots (branches), heartwood and sapwood."],"url":"http://arxiv.org/abs/2403.02820v1","category":"cs.AI"}
{"created":"2024-03-05 09:42:47","title":"Unraveling spin texture and spin-orbit coupling contributions in spin triplet superconductivity","abstract":"Over the past decade, it has been proposed theoretically and confirmed experimentally that long-range spin triplet (LRT) superconductivity can be generated in ferromagnet-superconductor hybrids either by the presence of spin textures (ST-LRT) or thanks to spin-orbit coupling (SOC-LRT). Nevertheless, there has been no theoretical or experimental investigation to date suggesting that both contributions could simultaneously exist within an experimental system. To disentangle these contributions, we present a comprehensive study of superconducting quasiparticle interference effects taking place inside a ferromagnetic layer interfacing a superconductor, through the investigation of above-gap conductance anomalies (CAs) related to MacMillan-Rowell resonances. The bias dependence of the CAs has been studied under a wide range of in-plane (IP) and out-of-plane (OOP) magnetic fields in two types of epitaxial, V/MgO/Fe-based ferromagnet-superconductor junctions with interfacial spin-orbit coupling. We observe an anisotropy of the CAs amplitude under small IP and OOP magnetic fields while remaining weakly affected by high fields, and implement micromagnetic simulations to help us distinguish between the ST-LRT and SOC-LRT contributions. Our findings suggest that further exploration of Fabry-P\\'erot type interference effects in electron transport could yield valuable insights into the hybridization between superconductors and ferromagnets induced by spin-orbit coupling and spin textures.","sentences":["Over the past decade, it has been proposed theoretically and confirmed experimentally that long-range spin triplet (LRT) superconductivity can be generated in ferromagnet-superconductor hybrids either by the presence of spin textures (ST-LRT) or thanks to spin-orbit coupling (SOC-LRT).","Nevertheless, there has been no theoretical or experimental investigation to date suggesting that both contributions could simultaneously exist within an experimental system.","To disentangle these contributions, we present a comprehensive study of superconducting quasiparticle interference effects taking place inside a ferromagnetic layer interfacing a superconductor, through the investigation of above-gap conductance anomalies (CAs) related to MacMillan-Rowell resonances.","The bias dependence of the CAs has been studied under a wide range of in-plane (IP) and out-of-plane (OOP) magnetic fields in two types of epitaxial, V/MgO/Fe-based ferromagnet-superconductor junctions with interfacial spin-orbit coupling.","We observe an anisotropy of the CAs amplitude under small IP and OOP magnetic fields while remaining weakly affected by high fields, and implement micromagnetic simulations to help us distinguish between the ST-LRT and SOC-LRT contributions.","Our findings suggest that further exploration of Fabry-P\\'erot type interference effects in electron transport could yield valuable insights into the hybridization between superconductors and ferromagnets induced by spin-orbit coupling and spin textures."],"url":"http://arxiv.org/abs/2403.02819v1","category":"cond-mat.supr-con"}
{"created":"2024-03-05 09:38:11","title":"Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?","abstract":"Current state-of-the-art (SOTA) 3D object detection methods often require a large amount of 3D bounding box annotations for training. However, collecting such large-scale densely-supervised datasets is notoriously costly. To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D object per scene. Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance. To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme. Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module. Our proposed method produces competitive results when compared with SOTA weakly-supervised methods using the same or even more annotation costs. Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost. The additional unlabeled training scenes could further boost the performance. The code will be available at https://github.com/gaocq/SS3D2.","sentences":["Current state-of-the-art (SOTA) 3D object detection methods often require a large amount of 3D bounding box annotations for training.","However, collecting such large-scale densely-supervised datasets is notoriously costly.","To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D object per scene.","Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance.","To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme.","Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module.","Our proposed method produces competitive results when compared with SOTA weakly-supervised methods using the same or even more annotation costs.","Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost.","The additional unlabeled training scenes could further boost the performance.","The code will be available at https://github.com/gaocq/SS3D2."],"url":"http://arxiv.org/abs/2403.02818v1","category":"cs.CV"}
{"created":"2024-03-05 09:37:13","title":"Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications","abstract":"In the past year, numerous companies have incorporated Generative AI (GenAI) capabilities into new and existing applications, forming interconnected Generative AI (GenAI) ecosystems consisting of semi/fully autonomous agents powered by GenAI services. While ongoing research highlighted risks associated with the GenAI layer of agents (e.g., dialog poisoning, membership inference, prompt leaking, jailbreaking), a critical question emerges: Can attackers develop malware to exploit the GenAI component of an agent and launch cyber-attacks on the entire GenAI ecosystem? This paper introduces Morris II, the first worm designed to target GenAI ecosystems through the use of adversarial self-replicating prompts. The study demonstrates that attackers can insert such prompts into inputs that, when processed by GenAI models, prompt the model to replicate the input as output (replication), engaging in malicious activities (payload). Additionally, these inputs compel the agent to deliver them (propagate) to new agents by exploiting the connectivity within the GenAI ecosystem. We demonstrate the application of Morris II against GenAIpowered email assistants in two use cases (spamming and exfiltrating personal data), under two settings (black-box and white-box accesses), using two types of input data (text and images). The worm is tested against three different GenAI models (Gemini Pro, ChatGPT 4.0, and LLaVA), and various factors (e.g., propagation rate, replication, malicious activity) influencing the performance of the worm are evaluated.","sentences":["In the past year, numerous companies have incorporated Generative AI (GenAI) capabilities into new and existing applications, forming interconnected Generative AI (GenAI) ecosystems consisting of semi/fully autonomous agents powered by GenAI services.","While ongoing research highlighted risks associated with the GenAI layer of agents (e.g., dialog poisoning, membership inference, prompt leaking, jailbreaking), a critical question emerges: Can attackers develop malware to exploit the GenAI component of an agent and launch cyber-attacks on the entire GenAI ecosystem?","This paper introduces Morris II, the first worm designed to target GenAI ecosystems through the use of adversarial self-replicating prompts.","The study demonstrates that attackers can insert such prompts into inputs that, when processed by GenAI models, prompt the model to replicate the input as output (replication), engaging in malicious activities (payload).","Additionally, these inputs compel the agent to deliver them (propagate) to new agents by exploiting the connectivity within the GenAI ecosystem.","We demonstrate the application of Morris II against GenAIpowered email assistants in two use cases (spamming and exfiltrating personal data), under two settings (black-box and white-box accesses), using two types of input data (text and images).","The worm is tested against three different GenAI models (Gemini Pro, ChatGPT 4.0, and LLaVA), and various factors (e.g., propagation rate, replication, malicious activity) influencing the performance of the worm are evaluated."],"url":"http://arxiv.org/abs/2403.02817v1","category":"cs.CR"}
{"created":"2024-03-05 09:33:36","title":"InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting","abstract":"Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels in a selective way. A channel identifier, a global mixing module and a self-contextual attention module are devised in InjectTST. The channel identifier can help Transformer distinguish channels for better representation. The global mixing module produces cross-channel global information. Through the self-contextual attention module, the independent channels can selectively concentrate on useful global information without robustness degradation, and channel mixing is achieved implicitly. Experiments indicate that InjectTST can achieve stable improvement compared with state-of-the-art models.","sentences":["Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting.","Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness.","Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information.","Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum.","To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper.","Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels in a selective way.","A channel identifier, a global mixing module and a self-contextual attention module are devised in InjectTST.","The channel identifier can help Transformer distinguish channels for better representation.","The global mixing module produces cross-channel global information.","Through the self-contextual attention module, the independent channels can selectively concentrate on useful global information without robustness degradation, and channel mixing is achieved implicitly.","Experiments indicate that InjectTST can achieve stable improvement compared with state-of-the-art models."],"url":"http://arxiv.org/abs/2403.02814v1","category":"cs.LG"}
{"created":"2024-03-05 09:25:31","title":"Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems","abstract":"Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data. It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions. However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain. In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric space. The DGG integral kernel is parameterized by Gaussian kernel weighted Riemann sum approximating and using dynamic message passing graph to depict the interrelation within the integral term. Fourier Neural Operator is selected to localize the metric vectors on spatial and frequency domains. Metric vectors are regarded as located on latent uniform domain, wherein spatial and spectral transformation offer highly regular constraints on solution space. The efficiency and robustness of DGGO are validated by applying it to solve numerical arbitrary discrete mechanics problems in comparison with mainstream neural operators. Ablation experiments are implemented to demonstrate the effectiveness of spatial transformation in the DGG kernel. The proposed method is utilized to forecast stress field of hyper-elastic material with geometrically variable void as engineering application.","sentences":["Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data.","It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions.","However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain.","In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems.","The Dynamic Gaussian Graph (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric space.","The DGG integral kernel is parameterized by Gaussian kernel weighted Riemann sum approximating and using dynamic message passing graph to depict the interrelation within the integral term.","Fourier Neural Operator is selected to localize the metric vectors on spatial and frequency domains.","Metric vectors are regarded as located on latent uniform domain, wherein spatial and spectral transformation offer highly regular constraints on solution space.","The efficiency and robustness of DGGO are validated by applying it to solve numerical arbitrary discrete mechanics problems in comparison with mainstream neural operators.","Ablation experiments are implemented to demonstrate the effectiveness of spatial transformation in the DGG kernel.","The proposed method is utilized to forecast stress field of hyper-elastic material with geometrically variable void as engineering application."],"url":"http://arxiv.org/abs/2403.02810v1","category":"cs.LG"}
{"created":"2024-03-05 09:24:11","title":"Face-hitting Dominating Sets in Planar Graphs","abstract":"A dominating set of a graph $G$ is a subset $S$ of its vertices such that each vertex of $G$ not in $S$ has a neighbor in $S$. A face-hitting set of a plane graph $G$ is a set $T$ of vertices in $G$ such that every face of $G$ contains at least one vertex of $T$. We show that the vertex-set of every plane (multi-)graph without isolated vertices, self-loops or $2$-faces can be partitioned into two disjoint sets so that both the sets are dominating and face-hitting. We also show that all the three assumptions above are necessary for the conclusion.   As a corollary, we show that every $n$-vertex simple plane triangulation has a dominating set of size at most $(1 - \\alpha)n/2$, where $\\alpha n$ is the maximum size of an independent set in the triangulation. Matheson and Tarjan [European J. Combin., 1996] conjectured that every plane triangulation with a sufficiently large number of vertices $n$ has a dominating set of size at most $n / 4$. Currently, the best known general bound for this is by Christiansen, Rotenberg and Rutschmann [SODA, 2024] who showed that every plane triangulation on $n > 10$ vertices has a dominating set of size at most $2n/7$. Our corollary improves their bound for $n$-vertex plane triangulations which contain a maximal independent set of size either less than $2n/7$ or more than $3n/7$.","sentences":["A dominating set of a graph $G$ is a subset $S$ of its vertices such that each vertex of $G$ not in $S$ has a neighbor in $S$. A face-hitting set of a plane graph $G$ is a set $T$ of vertices in $G$ such that every face of $G$ contains at least one vertex of $T$. We show that the vertex-set of every plane (multi-)graph without isolated vertices, self-loops or $2$-faces can be partitioned into two disjoint sets so that both the sets are dominating and face-hitting.","We also show that all the three assumptions above are necessary for the conclusion.   ","As a corollary, we show that every $n$-vertex simple plane triangulation has a dominating set of size at most $(1 - \\alpha)n/2$, where $\\alpha n$ is the maximum size of an independent set in the triangulation.","Matheson and Tarjan [European J. Combin., 1996] conjectured that every plane triangulation with a sufficiently large number of vertices $n$ has a dominating set of size at most $n / 4$.","Currently, the best known general bound for this is by Christiansen, Rotenberg and Rutschmann [SODA, 2024] who showed that every plane triangulation on $n > 10$ vertices has a dominating set of size at most $2n/7$. Our corollary improves their bound for $n$-vertex plane triangulations which contain a maximal independent set of size either less than $2n/7$ or more than $3n/7$."],"url":"http://arxiv.org/abs/2403.02808v1","category":"math.CO"}
{"created":"2024-03-05 09:21:39","title":"On Conormal Lie Algebras of Feigin-Odesskii Poisson Structures","abstract":"The main result of the paper is a description of conormal Lie algebras of Feigin-Odesskii Poisson structures. In order to obtain it we introduce a new variant of a definition of a Feigin-Odesskii Poisson structure: we define it using a differential on the second page of a certain spectral sequence. In the general case this spectral sequence computes morphisms and higher Ext's between filtered objects in an abelian category. Moreover, we use our definition to give another proof of the description of symplectic leaves of Feigin-Odesskii Poisson structures.","sentences":["The main result of the paper is a description of conormal Lie algebras of Feigin-Odesskii Poisson structures.","In order to obtain it we introduce a new variant of a definition of a Feigin-Odesskii Poisson structure: we define it using a differential on the second page of a certain spectral sequence.","In the general case this spectral sequence computes morphisms and higher Ext's between filtered objects in an abelian category.","Moreover, we use our definition to give another proof of the description of symplectic leaves of Feigin-Odesskii Poisson structures."],"url":"http://arxiv.org/abs/2403.02805v1","category":"math.AG"}
{"created":"2024-03-05 09:18:02","title":"Community Detection on Block Models with Geometric Kernels","abstract":"We consider the community recovery problem on a one-dimensional random geometric graph where every node has two independent labels: an observed location label and a hidden community label. A geometric kernel maps the locations of pairs of nodes to probabilities. Edges are drawn between pairs of nodes based on their communities and the value of the kernel corresponding to the respective node locations. Given the graph so generated along with the location labels, the latent communities of the nodes are to be inferred. In this work, we will look into the fundamental statistical limits for recovering the communities in such models. Additionally, we propose a linear-time algorithm (in the number of edges) and show that it recovers the communities of nodes exactly up to the information theoretic threshold.","sentences":["We consider the community recovery problem on a one-dimensional random geometric graph where every node has two independent labels: an observed location label and a hidden community label.","A geometric kernel maps the locations of pairs of nodes to probabilities.","Edges are drawn between pairs of nodes based on their communities and the value of the kernel corresponding to the respective node locations.","Given the graph so generated along with the location labels, the latent communities of the nodes are to be inferred.","In this work, we will look into the fundamental statistical limits for recovering the communities in such models.","Additionally, we propose a linear-time algorithm (in the number of edges) and show that it recovers the communities of nodes exactly up to the information theoretic threshold."],"url":"http://arxiv.org/abs/2403.02802v1","category":"math.PR"}
{"created":"2024-03-05 09:12:49","title":"DPPA: Pruning Method for Large Language Model to Model Merging","abstract":"Model merging is to combine fine-tuned models derived from multiple domains, with the intent of enhancing the model's proficiency across various domains. The principal concern is the resolution of parameter conflicts. A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the pruning stage. The DARE approach has exhibited promising outcomes when applied to a simplistic fine-tuned model. However, the efficacy of this method tends to wane when employed on complex fine-tuned models that show a significant parameter bias relative to the baseline model. In this paper, we introduce a dual-stage method termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the challenge of merging complex fine-tuned models. Initially, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning, which aim is to enhance performance at higher pruning rates. Subsequently, we propose Dynamically Partition Amplification (DPA), a rescaling strategy, is designed to dynamically amplify parameter partitions in relation to their significance levels. The experimental results show that our method maintains a mere 20% of domain-specific parameters and yet delivers a performance comparable to other methodologies that preserve up to 90% of parameters. Furthermore, our method displays outstanding performance post-pruning, leading to a significant improvement of nearly 20% performance in model merging. We make our code on Github.","sentences":["Model merging is to combine fine-tuned models derived from multiple domains, with the intent of enhancing the model's proficiency across various domains.","The principal concern is the resolution of parameter conflicts.","A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the pruning stage.","The DARE approach has exhibited promising outcomes when applied to a simplistic fine-tuned model.","However, the efficacy of this method tends to wane when employed on complex fine-tuned models that show a significant parameter bias relative to the baseline model.","In this paper, we introduce a dual-stage method termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the challenge of merging complex fine-tuned models.","Initially, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning, which aim is to enhance performance at higher pruning rates.","Subsequently, we propose Dynamically Partition Amplification (DPA), a rescaling strategy, is designed to dynamically amplify parameter partitions in relation to their significance levels.","The experimental results show that our method maintains a mere 20% of domain-specific parameters and yet delivers a performance comparable to other methodologies that preserve up to 90% of parameters.","Furthermore, our method displays outstanding performance post-pruning, leading to a significant improvement of nearly 20% performance in model merging.","We make our code on Github."],"url":"http://arxiv.org/abs/2403.02799v1","category":"cs.CL"}
{"created":"2024-03-05 09:09:15","title":"Evaluating and Optimizing Educational Content with Large Language Model Judgments","abstract":"Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instructional materials using the judgments of another LM as a reward function. We apply this approach to create math word problem worksheets aimed at maximizing student learning gains. Human teachers' evaluations of these LM-generated worksheets show a significant alignment between the LM judgments and human teacher preferences. We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design.","sentences":["Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes.","To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials.","However, it is difficult to model the cognitive processes of learning dynamics.","We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes.","Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect.","This demonstrates the potential of LMs as reliable evaluators of educational content.","Building on this insight, we introduce an instruction optimization approach in which one LM generates instructional materials using the judgments of another LM as a reward function.","We apply this approach to create math word problem worksheets aimed at maximizing student learning gains.","Human teachers' evaluations of these LM-generated worksheets show a significant alignment between the LM judgments and human teacher preferences.","We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design."],"url":"http://arxiv.org/abs/2403.02795v1","category":"cs.AI"}
{"created":"2024-03-05 09:08:20","title":"A Distance Metric Learning Model Based On Variational Information Bottleneck","abstract":"In recent years, personalized recommendation technology has flourished and become one of the hot research directions. The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied. The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector. While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited recommendation quality of the model. In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the mutual information of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean distance by decoupling the latent space feature vector. In this paper, the experimental results are compared with the root mean square error (RMSE) on the three public datasets. The results show that the generalization ability of VIB-DML is excellent. Compared with the general metric learning model MetricF, the prediction error is reduced by 7.29%. Finally, the paper proves the strong robustness of VIBDML through experiments.","sentences":["In recent years, personalized recommendation technology has flourished and become one of the hot research directions.","The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied.","The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector.","While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited recommendation quality of the model.","In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the mutual information of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean distance by decoupling the latent space feature vector.","In this paper, the experimental results are compared with the root mean square error (RMSE) on the three public datasets.","The results show that the generalization ability of VIB-DML is excellent.","Compared with the general metric learning model MetricF, the prediction error is reduced by 7.29%.","Finally, the paper proves the strong robustness of VIBDML through experiments."],"url":"http://arxiv.org/abs/2403.02794v1","category":"cs.IR"}
{"created":"2024-03-05 09:07:37","title":"Differential cross-sections for events with missing transverse momentum and jets measured with the ATLAS detector in 13 TeV proton-proton collisions","abstract":"Measurements of inclusive, differential cross-sections for the production of events with missing transverse momentum in association with jets in proton-proton collisions at $\\sqrt{s}=13$~TeV are presented. The measurements are made with the ATLAS detector using an integrated luminosity of 140~fb$^{-1}$ and include measurements of dijet distributions in a region in which vector-boson fusion processes are enhanced. They are unfolded to correct for detector resolution and efficiency within the fiducial acceptance, and are designed to allow robust comparisons with a wide range of theoretical predictions. A measurement of differential cross sections for the $Z~\\to \\nu\\nu$ process is made. The measurements are generally well-described by Standard Model predictions except for the dijet invariant mass distribution. Auxiliary measurements of the hadronic system recoiling against isolated leptons, and photons, are also made in the same phase space. Ratios between the measured distributions are then derived, to take advantage of cancellations in modelling effects and some of the major systematic uncertainties. These measurements are sensitive to new phenomena, and provide a mechanism to easily set constraints on phenomenological models. To illustrate the robustness of the approach, these ratios are compared with two common Dark Matter models, where the constraints derived from the measurement are comparable to those set by dedicated detector-level searches.","sentences":["Measurements of inclusive, differential cross-sections for the production of events with missing transverse momentum in association with jets in proton-proton collisions at $\\sqrt{s}=13$~TeV are presented.","The measurements are made with the ATLAS detector using an integrated luminosity of 140~fb$^{-1}$ and include measurements of dijet distributions in a region in which vector-boson fusion processes are enhanced.","They are unfolded to correct for detector resolution and efficiency within the fiducial acceptance, and are designed to allow robust comparisons with a wide range of theoretical predictions.","A measurement of differential cross sections for the $Z~\\to \\nu\\nu$ process is made.","The measurements are generally well-described by Standard Model predictions except for the dijet invariant mass distribution.","Auxiliary measurements of the hadronic system recoiling against isolated leptons, and photons, are also made in the same phase space.","Ratios between the measured distributions are then derived, to take advantage of cancellations in modelling effects and some of the major systematic uncertainties.","These measurements are sensitive to new phenomena, and provide a mechanism to easily set constraints on phenomenological models.","To illustrate the robustness of the approach, these ratios are compared with two common Dark Matter models, where the constraints derived from the measurement are comparable to those set by dedicated detector-level searches."],"url":"http://arxiv.org/abs/2403.02793v1","category":"hep-ex"}
{"created":"2024-03-05 09:06:20","title":"Intelligent Traffic Monitoring with Distributed Acoustic Sensing","abstract":"Distributed Acoustic Sensing (DAS) is promising for traffic monitoring, but its extensive data and sensitivity to vibrations, causing noise, pose computational challenges. To address this, we propose a two-step deep-learning workflow with high efficiency and noise immunity for DAS-based traffic monitoring, focusing on instance vehicle trajectory segmentation and velocity estimation. Our approach begins by generating a diverse synthetic DAS dataset with labeled vehicle signals, tackling the issue of missing training labels in this field. This dataset is used to train a Convolutional Neural Network (CNN) to detect linear vehicle trajectories from the noisy DAS data in the time-space domain. However, due to significant noise, these trajectories are often fragmented and incomplete. To enhance accuracy, we introduce a second step involving the Hough transform. This converts detected linear features into point-like energy clusters in the Hough domain. Another CNN is then employed to focus on these energies, identifying the most significant points. The inverse Hough transform is applied to these points to reconstruct complete, distinct, and noise-free linear vehicle trajectories in the time-space domain. The Hough transform plays a crucial role by enforcing a local linearity constraint on the trajectories, enhancing continuity and noise immunity, and facilitating the separation of individual trajectories and estimation of vehicle velocities (indicated by trajectory slopes in the Hough domain). Our method has shown effectiveness in real-world datasets, proving its value in real-time processing of DAS data and applicability in similar traffic monitoring scenarios. All related codes and data are available at https://github.com/TTMuTian/itm/.","sentences":["Distributed Acoustic Sensing (DAS) is promising for traffic monitoring, but its extensive data and sensitivity to vibrations, causing noise, pose computational challenges.","To address this, we propose a two-step deep-learning workflow with high efficiency and noise immunity for DAS-based traffic monitoring, focusing on instance vehicle trajectory segmentation and velocity estimation.","Our approach begins by generating a diverse synthetic DAS dataset with labeled vehicle signals, tackling the issue of missing training labels in this field.","This dataset is used to train a Convolutional Neural Network (CNN) to detect linear vehicle trajectories from the noisy DAS data in the time-space domain.","However, due to significant noise, these trajectories are often fragmented and incomplete.","To enhance accuracy, we introduce a second step involving the Hough transform.","This converts detected linear features into point-like energy clusters in the Hough domain.","Another CNN is then employed to focus on these energies, identifying the most significant points.","The inverse Hough transform is applied to these points to reconstruct complete, distinct, and noise-free linear vehicle trajectories in the time-space domain.","The Hough transform plays a crucial role by enforcing a local linearity constraint on the trajectories, enhancing continuity and noise immunity, and facilitating the separation of individual trajectories and estimation of vehicle velocities (indicated by trajectory slopes in the Hough domain).","Our method has shown effectiveness in real-world datasets, proving its value in real-time processing of DAS data and applicability in similar traffic monitoring scenarios.","All related codes and data are available at https://github.com/TTMuTian/itm/."],"url":"http://arxiv.org/abs/2403.02791v1","category":"physics.geo-ph"}
{"created":"2024-03-05 08:59:45","title":"Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease","abstract":"Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of graph representation learning within a semi-supervised learning framework. Leveraging graph neural networks (GNNs), our approach constructs a subject similarity graph to identify risk patterns from health checkup data. The effectiveness of various GNN approaches in this context is demonstrated, even with minimal labeled samples. Central to our methodology is the inclusion of human-centric explanations through explainable GNNs, providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on graph representation learning and human-centric explanation.","sentences":["Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of graph representation learning within a semi-supervised learning framework.","Leveraging graph neural networks (GNNs), our approach constructs a subject similarity graph to identify risk patterns from health checkup data.","The effectiveness of various GNN approaches in this context is demonstrated, even with minimal labeled samples.","Central to our methodology is the inclusion of human-centric explanations through explainable GNNs, providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on graph representation learning and human-centric explanation."],"url":"http://arxiv.org/abs/2403.02786v1","category":"cs.LG"}
{"created":"2024-03-05 08:56:30","title":"Where the Really Hard Quadratic Assignment Problems Are: the QAP-SAT instances","abstract":"The Quadratic Assignment Problem (QAP) is one of the major domains in the field of evolutionary computation, and more widely in combinatorial optimization. This paper studies the phase transition of the QAP, which can be described as a dramatic change in the problem's computational complexity and satisfiability, within a narrow range of the problem parameters. To approach this phenomenon, we introduce a new QAP-SAT design of the initial problem based on submodularity to capture its difficulty with new features. This decomposition is studied experimentally using branch-and-bound and tabu search solvers. A phase transition parameter is then proposed. The critical parameter of phase transition satisfaction and that of the solving effort are shown to be highly correlated for tabu search, thus allowing the prediction of difficult instances.","sentences":["The Quadratic Assignment Problem (QAP) is one of the major domains in the field of evolutionary computation, and more widely in combinatorial optimization.","This paper studies the phase transition of the QAP, which can be described as a dramatic change in the problem's computational complexity and satisfiability, within a narrow range of the problem parameters.","To approach this phenomenon, we introduce a new QAP-SAT design of the initial problem based on submodularity to capture its difficulty with new features.","This decomposition is studied experimentally using branch-and-bound and tabu search solvers.","A phase transition parameter is then proposed.","The critical parameter of phase transition satisfaction and that of the solving effort are shown to be highly correlated for tabu search, thus allowing the prediction of difficult instances."],"url":"http://arxiv.org/abs/2403.02783v1","category":"cs.AI"}
{"created":"2024-03-05 08:53:30","title":"PromptKD: Unsupervised Prompt Distillation for Vision-Language Models","abstract":"Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method.","sentences":["Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains.","Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models.","In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images.","Specifically, our framework consists of two distinct stages.","In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels.","After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder.","In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits.","Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts.","The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain.","Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference.","To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student.","Extensive experiments on 11 datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.02781v1","category":"cs.CV"}
{"created":"2024-03-05 08:50:32","title":"Abstracting Denotational Interpreters","abstract":"We explore denotational interpreters: denotational semantics that produce coinductive traces of a corresponding small-step operational semantics. By parameterising our denotational interpreter over the semantic domain and then varying it, we recover dynamic semantics with different evaluation strategies as well as summary-based static analyses such as type analysis, all from the same generic interpreter. Among our contributions is the first provably adequate denotational semantics for call-by-need. The generated traces lend themselves well to describe operational properties such as evaluation cardinality, and hence to static analyses abstracting these operational properties. Since static analysis and dynamic semantics share the same generic interpreter definition, soundness proofs via abstract interpretation decompose into showing small abstraction laws about the abstract domain, thus obviating complicated ad-hoc preservation-style proof frameworks.","sentences":["We explore denotational interpreters: denotational semantics that produce coinductive traces of a corresponding small-step operational semantics.","By parameterising our denotational interpreter over the semantic domain and then varying it, we recover dynamic semantics with different evaluation strategies as well as summary-based static analyses such as type analysis, all from the same generic interpreter.","Among our contributions is the first provably adequate denotational semantics for call-by-need.","The generated traces lend themselves well to describe operational properties such as evaluation cardinality, and hence to static analyses abstracting these operational properties.","Since static analysis and dynamic semantics share the same generic interpreter definition, soundness proofs via abstract interpretation decompose into showing small abstraction laws about the abstract domain, thus obviating complicated ad-hoc preservation-style proof frameworks."],"url":"http://arxiv.org/abs/2403.02778v1","category":"cs.PL"}
{"created":"2024-03-05 08:46:54","title":"A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation","abstract":"Purpose: The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter. This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation. Deep Reinforcement Learning approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions. Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the geometry changes. Methods: In this paper, we propose a zero-shot learning strategy for three-dimensional autonomous endovascular navigation. Using a very small training set of branching patterns, our reinforcement learning algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining. Results: We demonstrate our method on 4 different vascular systems, with an average success rate of 95% at reaching random targets on these anatomies. Our strategy is also computationally efficient, allowing the training of our controller to be performed in only 2 hours. Conclusion: Our training method proved its ability to navigate unseen geometries with different characteristics, thanks to a nearly shape-invariant observation space.","sentences":["Purpose: The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter.","This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation.","Deep Reinforcement Learning approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions.","Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the geometry changes.","Methods: In this paper, we propose a zero-shot learning strategy for three-dimensional autonomous endovascular navigation.","Using a very small training set of branching patterns, our reinforcement learning algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining.","Results:","We demonstrate our method on 4 different vascular systems, with an average success rate of 95% at reaching random targets on these anatomies.","Our strategy is also computationally efficient, allowing the training of our controller to be performed in only 2 hours.","Conclusion: Our training method proved its ability to navigate unseen geometries with different characteristics, thanks to a nearly shape-invariant observation space."],"url":"http://arxiv.org/abs/2403.02777v1","category":"cs.LG"}
{"created":"2024-03-05 08:45:30","title":"EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs","abstract":"Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.","sentences":["Large language models (LLMs) have proven to be very superior to conventional methods in various tasks.","However, their expensive computations and high memory requirements are prohibitive for deployment.","Model quantization is an effective method for reducing this overhead.","The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks.","Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance?","In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs.","Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error.","Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error.","With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model.","Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed.","Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods."],"url":"http://arxiv.org/abs/2403.02775v1","category":"cs.AI"}
{"created":"2024-03-05 08:41:41","title":"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models","abstract":"Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on the downscaling task. Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints.","sentences":["Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive.","Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches.","However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training.","We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner.","Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data.","We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on the downscaling task.","Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints."],"url":"http://arxiv.org/abs/2403.02774v1","category":"physics.ao-ph"}
{"created":"2024-03-05 18:57:06","title":"Performance of a modular ton-scale pixel-readout liquid argon time projection chamber","abstract":"The Module-0 Demonstrator is a single-phase 600 kg liquid argon time projection chamber operated as a prototype for the DUNE liquid argon near detector. Based on the ArgonCube design concept, Module-0 features a novel 80k-channel pixelated charge readout and advanced high-coverage photon detection system. In this paper, we present an analysis of an eight-day data set consisting of 25 million cosmic ray events collected in the spring of 2021. We use this sample to demonstrate the imaging performance of the charge and light readout systems as well as the signal correlations between the two. We also report argon purity and detector uniformity measurements, and provide comparisons to detector simulations.","sentences":["The Module-0 Demonstrator is a single-phase 600 kg liquid argon time projection chamber operated as a prototype for the DUNE liquid argon near detector.","Based on the ArgonCube design concept, Module-0 features a novel 80k-channel pixelated charge readout and advanced high-coverage photon detection system.","In this paper, we present an analysis of an eight-day data set consisting of 25 million cosmic ray events collected in the spring of 2021.","We use this sample to demonstrate the imaging performance of the charge and light readout systems as well as the signal correlations between the two.","We also report argon purity and detector uniformity measurements, and provide comparisons to detector simulations."],"url":"http://arxiv.org/abs/2403.03212v1","category":"physics.ins-det"}
{"created":"2024-03-05 18:49:29","title":"Anomalous continuum scattering and higher-order van Hove singularity in the strongly anisotropic S = 1/2 triangular lattice antiferromagnet","abstract":"The S = 1/2 triangular lattice antiferromagnet (TLAF) stands out as a paradigmatic example of frustrated quantum magnetism. An ongoing challenge involves understanding the influence of exchange anisotropy on the collective behavior within such systems. Using inelastic neutron scattering (INS) and advanced calculation techniques, we have studied the low and high temperature spin dynamics of Ba2La2CoTe2O12 (BLCTO): a Co2+-based Jeff = 1/2 TLAF that exhibits 120 deg order below TN = 3.26 K. The spin Hamiltonian was determined by fitting the energy-resolved paramagnetic excitations measured at T > TN, revealing an exceptionally strong easy-plane XXZ exchange anisotropy. Below TN, the excitation spectrum exhibits a high energy continuum having a larger spectral weight than the single-magnon modes. Combined with advanced theoretical calculations of magnetic excitations based on magnons and spinons, this observation suggests a scenario characterized by a spinon confinement length that markedly exceeds the lattice spacing. We conjecture that this phenomenon arises due to the proximity to a quantum melting point, which persists even in the presence of strong easy-plane XXZ anisotropy. Finally, we highlight characteristic flat features in the excitation spectrum, which are connected to higher-order van Hove singularities in the magnon dispersion and are directly induced by easy-plane XXZ anisotropy. Our results provide a rare experimental insight into the nature of highly anisotropic S = 1/2 TLAFs between the Heisenberg and XY limits.","sentences":["The S = 1/2 triangular lattice antiferromagnet (TLAF) stands out as a paradigmatic example of frustrated quantum magnetism.","An ongoing challenge involves understanding the influence of exchange anisotropy on the collective behavior within such systems.","Using inelastic neutron scattering (INS) and advanced calculation techniques, we have studied the low and high temperature spin dynamics of Ba2La2CoTe2O12 (BLCTO): a Co2+-based Jeff = 1/2 TLAF that exhibits 120 deg order below TN = 3.26 K.","The spin Hamiltonian was determined by fitting the energy-resolved paramagnetic excitations measured at T > TN, revealing an exceptionally strong easy-plane XXZ exchange anisotropy.","Below TN, the excitation spectrum exhibits a high energy continuum having a larger spectral weight than the single-magnon modes.","Combined with advanced theoretical calculations of magnetic excitations based on magnons and spinons, this observation suggests a scenario characterized by a spinon confinement length that markedly exceeds the lattice spacing.","We conjecture that this phenomenon arises due to the proximity to a quantum melting point, which persists even in the presence of strong easy-plane XXZ anisotropy.","Finally, we highlight characteristic flat features in the excitation spectrum, which are connected to higher-order van Hove singularities in the magnon dispersion and are directly induced by easy-plane XXZ anisotropy.","Our results provide a rare experimental insight into the nature of highly anisotropic S = 1/2 TLAFs between the Heisenberg and XY limits."],"url":"http://arxiv.org/abs/2403.03210v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 18:46:50","title":"Active Statistical Inference","abstract":"Inspired by the concept of active learning, we propose active inference$\\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.","sentences":["Inspired by the concept of active learning, we propose active inference$\\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection.","Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget.","It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident.","Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution.","The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data.","This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values.","We evaluate active inference on datasets from public opinion research, census analysis, and proteomics."],"url":"http://arxiv.org/abs/2403.03208v1","category":"stat.ML"}
{"created":"2024-03-05 18:29:12","title":"On Projective Planes of Order 16 Associated with 1-rotational 2-(52, 4, 1) Designs","abstract":"A maximal arc of degree k in a finite projective plane P of order q = ks is a set of (q-s+1)k points that meets every line of P in either k or 0 points. The collection of the nonempty intersections of a maximal arc with the lines of P is a resolvable Steiner 2-((q-s+1)k, k, 1) design. Necessary and sufficient conditions for a resolvable Steiner 2- design to be embeddable as a maximal arc in a projective plane were proved recently in [8]. Steiner designs associated with maximal arcs in the known projective planes of order 16 were analyzed in [6], where it was shown that some of the associated designs are embeddable in two non-isomorphic planes. Using MAGMA, we conducted an analysis to ascertain whether any of the 22 non-isomorphic 1-rotational 2-(52,4,1) designs, previously classified in [3], could be embedded in maximal arcs of degree 4 within projective planes of order 16. This paper presents a summary of our findings, revealing that precisely only one out of the the twenty-two 1-rotational designs from [3] is embeddable in a plane of order 16, being the Desarguesian plane P G(2, 16).","sentences":["A maximal arc of degree k in a finite projective plane P of order q = ks is a set of (q-s+1)k points that meets every line of P in either k or 0 points.","The collection of the nonempty intersections of a maximal arc with the lines of P is a resolvable Steiner 2-((q-s+1)k, k, 1) design.","Necessary and sufficient conditions for a resolvable Steiner 2- design to be embeddable as a maximal arc in a projective plane were proved recently in [8].","Steiner designs associated with maximal arcs in the known projective planes of order 16 were analyzed in [6], where it was shown that some of the associated designs are embeddable in two non-isomorphic planes.","Using MAGMA, we conducted an analysis to ascertain whether any of the 22 non-isomorphic 1-rotational 2-(52,4,1) designs, previously classified in [3], could be embedded in maximal arcs of degree 4 within projective planes of order 16.","This paper presents a summary of our findings, revealing that precisely only one out of the the twenty-two 1-rotational designs from [3] is embeddable in a plane of order 16, being the Desarguesian plane P G(2, 16)."],"url":"http://arxiv.org/abs/2403.03189v1","category":"math.CO"}
{"created":"2024-03-05 17:16:37","title":"Jovian sodium nebula and Io plasma torus S$^+$ and brightnesses 2017 -- 2023: insights into volcanic vs. sublimation supply","abstract":"We present first results derived from the largest collection of contemporaneously recorded Jovian sodium nebula and Io plasma torus (IPT) in [S II] 673.1 nm images assembled to date. The data were recorded by the Planetary Science Institute's Io Input/Output observatory (IoIO) and provide important context to Io geologic and atmospheric studies as well as the Juno mission and supporting observations. Enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from Io is determined by the enhancements, not any quiescent state. The enhancements are not seen at periodicities associated with modulation in solar insolation of Io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of Io's atmospheric escape. We suggest that geologic activity, likely involving volcanic plumes, drives escape.","sentences":["We present first results derived from the largest collection of contemporaneously recorded Jovian sodium nebula and Io plasma torus (IPT) in [S II] 673.1 nm images assembled to date.","The data were recorded by the Planetary Science Institute's Io Input/Output observatory (IoIO) and provide important context to Io geologic and atmospheric studies as well as the Juno mission and supporting observations.","Enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from Io is determined by the enhancements, not any quiescent state.","The enhancements are not seen at periodicities associated with modulation in solar insolation of Io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of Io's atmospheric escape.","We suggest that geologic activity, likely involving volcanic plumes, drives escape."],"url":"http://arxiv.org/abs/2403.03131v1","category":"astro-ph.EP"}
{"created":"2024-03-05 16:46:18","title":"Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand","abstract":"Current studies on human locomotion focus mainly on solid ground walking conditions. In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand. A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected. We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics. A comprehensive analysis of human gait and joint stiffness profiles is presented. The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground. These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand. The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains.","sentences":["Current studies on human locomotion focus mainly on solid ground walking conditions.","In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand.","A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected.","We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics.","A comprehensive analysis of human gait and joint stiffness profiles is presented.","The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground.","These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand.","The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains."],"url":"http://arxiv.org/abs/2403.03105v1","category":"cs.RO"}
{"created":"2024-03-05 16:43:25","title":"Emergent Equivariance in Deep Ensembles","abstract":"We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.","sentences":["We demonstrate that deep ensembles are secretly equivariant models.","More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation.","Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit.","The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is.","Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments."],"url":"http://arxiv.org/abs/2403.03103v1","category":"cs.LG"}
{"created":"2024-03-05 15:57:11","title":"Enhancing single-atom loading in tightly confined dipole traps with ancillary dipole beam","abstract":"Single atoms trapped in tightly focused optical dipole traps provide an excellent experimental platform for quantum computing, precision measurement, and fundamental physics research. In this work, we propose and demonstrate a novel approach to enhancing the loading of single atoms by introducing a weak ancillary dipole beam. The loading rate of single atoms in a dipole trap can be significantly improved by only a few tens of microwatts of counter-propagating beam. It was also demonstrated that multiple atoms could be loaded with the assistance of a counter-propagating beam. By reducing the power requirements for trapping single atoms and enabling the trapping of multiple atoms, our method facilitates the extension of single-atom arrays and the investigation of collective light-atom interactions.","sentences":["Single atoms trapped in tightly focused optical dipole traps provide an excellent experimental platform for quantum computing, precision measurement, and fundamental physics research.","In this work, we propose and demonstrate a novel approach to enhancing the loading of single atoms by introducing a weak ancillary dipole beam.","The loading rate of single atoms in a dipole trap can be significantly improved by only a few tens of microwatts of counter-propagating beam.","It was also demonstrated that multiple atoms could be loaded with the assistance of a counter-propagating beam.","By reducing the power requirements for trapping single atoms and enabling the trapping of multiple atoms, our method facilitates the extension of single-atom arrays and the investigation of collective light-atom interactions."],"url":"http://arxiv.org/abs/2403.03068v1","category":"quant-ph"}
{"created":"2024-03-05 15:44:41","title":"Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal","abstract":"Runtime Verification (RV) refers to a family of techniques in which system executions are observed and confronted to formal specifications, with the aim of identifying faults. In Offline RV, observation is done in a first step and verification in a second, on a static artifact collected during observation. In this paper, we define an approach to offline RV of Distributed Systems (DS) against interactions. Interactions are formal models describing communications within a DS. DS are composed of subsystems deployed on different machines and interacting via message passing. Therefore, observing executions of a DS entails logging a collection of local execution traces, one for each subsystem, that we call a multi-trace. A major challenge in analyzing multi-traces is that there are no practical means to synchronize the ends of observations of all local traces. We address this via an operation, called lifeline removal, which we apply on-the-fly on the specification during verification once a local trace has been entirely analyzed. This operation removes from the interaction the specification of actions occurring on the subsystem that is no-longer observed. This may allow further execution of the specification via removing deadlocks due to the partial orders of actions. We prove the correctness of the resulting RV algorithm and introduce two optimization techniques which we also prove correct. We implement a Partial Order Reduction (POR) technique via the selection of a one-unambiguous action (as a unique first step to a linearization) which existence is determined via another use of the lifeline removal operator. Additionally, Local Analyses (LOC) i.e., the verification of local traces, can be leveraged during the global multi-trace analysis to prove failure more quickly. Experiments illustrate the application of our RV approach and the benefits of our optimizations.","sentences":["Runtime Verification (RV) refers to a family of techniques in which system executions are observed and confronted to formal specifications, with the aim of identifying faults.","In Offline RV, observation is done in a first step and verification in a second, on a static artifact collected during observation.","In this paper, we define an approach to offline RV of Distributed Systems (DS) against interactions.","Interactions are formal models describing communications within a DS.","DS are composed of subsystems deployed on different machines and interacting via message passing.","Therefore, observing executions of a DS entails logging a collection of local execution traces, one for each subsystem, that we call a multi-trace.","A major challenge in analyzing multi-traces is that there are no practical means to synchronize the ends of observations of all local traces.","We address this via an operation, called lifeline removal, which we apply on-the-fly on the specification during verification once a local trace has been entirely analyzed.","This operation removes from the interaction the specification of actions occurring on the subsystem that is no-longer observed.","This may allow further execution of the specification via removing deadlocks due to the partial orders of actions.","We prove the correctness of the resulting RV algorithm and introduce two optimization techniques which we also prove correct.","We implement a Partial Order Reduction (POR) technique via the selection of a one-unambiguous action (as a unique first step to a linearization) which existence is determined via another use of the lifeline removal operator.","Additionally, Local Analyses (LOC) i.e., the verification of local traces, can be leveraged during the global multi-trace analysis to prove failure more quickly.","Experiments illustrate the application of our RV approach and the benefits of our optimizations."],"url":"http://arxiv.org/abs/2403.03057v1","category":"cs.FL"}
{"created":"2024-03-05 13:16:37","title":"Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?","abstract":"The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs). By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group. However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner. In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset. We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study. We found limited similarities between our results and those of the previous study, providing evidence against the hypothesis. Our findings imply that there is significant variation between ICUs; thus, a standardised restructuring approach is unlikely to be appropriate. Instead, potential efficiency gains might be greater when the number and nature of the subunits are tailored to each ICU individually.","sentences":["The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs).","By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group.","However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner.","In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset.","We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study.","We found limited similarities between our results and those of the previous study, providing evidence against the hypothesis.","Our findings imply that there is significant variation between ICUs; thus, a standardised restructuring approach is unlikely to be appropriate.","Instead, potential efficiency gains might be greater when the number and nature of the subunits are tailored to each ICU individually."],"url":"http://arxiv.org/abs/2403.02945v1","category":"cs.LG"}
{"created":"2024-03-05 12:49:25","title":"Improving the quality of individual-level online information tracking: challenges of existing approaches and introduction of a new content- and long-tail sensitive academic solution","abstract":"This article evaluates the quality of data collection in individual-level desktop information tracking used in the social sciences and shows that the existing approaches face sampling issues, validity issues due to the lack of content-level data and their disregard of the variety of devices and long-tail consumption patterns as well as transparency and privacy issues. To overcome some of these problems, the article introduces a new academic tracking solution, WebTrack, an open source tracking tool maintained by a major European research institution. The design logic, the interfaces and the backend requirements for WebTrack, followed by a detailed examination of strengths and weaknesses of the tool, are discussed. Finally, using data from 1185 participants, the article empirically illustrates how an improvement in the data collection through WebTrack leads to new innovative shifts in the processing of tracking data. As WebTrack allows collecting the content people are exposed to on more than classical news platforms, we can strongly improve the detection of politics-related information consumption in tracking data with the application of automated content analysis compared to traditional approaches that rely on the list-based identification of news.","sentences":["This article evaluates the quality of data collection in individual-level desktop information tracking used in the social sciences and shows that the existing approaches face sampling issues, validity issues due to the lack of content-level data and their disregard of the variety of devices and long-tail consumption patterns as well as transparency and privacy issues.","To overcome some of these problems, the article introduces a new academic tracking solution, WebTrack, an open source tracking tool maintained by a major European research institution.","The design logic, the interfaces and the backend requirements for WebTrack, followed by a detailed examination of strengths and weaknesses of the tool, are discussed.","Finally, using data from 1185 participants, the article empirically illustrates how an improvement in the data collection through WebTrack leads to new innovative shifts in the processing of tracking data.","As WebTrack allows collecting the content people are exposed to on more than classical news platforms, we can strongly improve the detection of politics-related information consumption in tracking data with the application of automated content analysis compared to traditional approaches that rely on the list-based identification of news."],"url":"http://arxiv.org/abs/2403.02931v1","category":"cs.CY"}
{"created":"2024-03-05 12:41:48","title":"Cascade enhancement of magnetic dipole emission and efficient collection of photons by the hybrid topological structure","abstract":"High photon emission rate, high collection efficiency and stability are all important for single photon source applications. Here we demonstrate that both cascade enhancement of magnetic dipole emission and efficient collection of emitted photons can be realized simultaneously in a hybrid structure of a two-dimensional honeycomb topological photonic crystal containing dielectric nanodisk. The magnetic dipole resonance of nanodisk can effectively excite topological edgestate due to their near field overlapping. If a magnetic emitter is used to excite the nanodisk, then its magnetic dipole resonance can be viewed as a large equivalent dipole interacting with the edgestates, which achieves the cascade enhancement of the magnetic emitter. In addition, almost all the scattered photons around the nanodisk can be collected through topological edge state with the collection efficiency up to 95%. The proposed mechanism may provide the practical applications of on-chip robust single photon sources and nanolasers.","sentences":["High photon emission rate, high collection efficiency and stability are all important for single photon source applications.","Here we demonstrate that both cascade enhancement of magnetic dipole emission and efficient collection of emitted photons can be realized simultaneously in a hybrid structure of a two-dimensional honeycomb topological photonic crystal containing dielectric nanodisk.","The magnetic dipole resonance of nanodisk can effectively excite topological edgestate due to their near field overlapping.","If a magnetic emitter is used to excite the nanodisk, then its magnetic dipole resonance can be viewed as a large equivalent dipole interacting with the edgestates, which achieves the cascade enhancement of the magnetic emitter.","In addition, almost all the scattered photons around the nanodisk can be collected through topological edge state with the collection efficiency up to 95%.","The proposed mechanism may provide the practical applications of on-chip robust single photon sources and nanolasers."],"url":"http://arxiv.org/abs/2403.02926v1","category":"physics.optics"}
{"created":"2024-03-05 10:37:05","title":"Fast Numerical Approximation of Parabolic Problems Using Model Order Reduction and the Laplace Transform","abstract":"We introduce a novel, fast method for the numerical approximation of parabolic partial differential equations (PDEs for short) based on model order reduction techniques and the Laplace transform. We start by applying said transform to the evolution problem, thus yielding a time-independent boundary value problem solely depending on the complex Laplace parameter. In an offline stage, we judiciously sample the Laplace parameter and numerically solve the corresponding collection of high-fidelity or full-order problems. Next, we apply a proper orthogonal decomposition (POD) to this collection of solutions in order to obtain a reduced basis in the Laplace domain. We project the linear parabolic problem onto this basis, and then using any suitable time-stepping method, we solve the evolution problem. A key insight to justify the implementation and analysis of the proposed method corresponds to resorting to Hardy spaces of analytic functions and establishing, through the Paley-Wiener theorem, an isometry between the solution of the time-dependent problem and its Laplace transform. As a result, one may conclude that computing a POD with samples taken in the Laplace domain produces an exponentially accurate reduced basis for the time-dependent problem. Numerical experiments portray the performance of the method in terms of accuracy and, in particular, speed-up when compared to the solution obtained by solving the full-order model.","sentences":["We introduce a novel, fast method for the numerical approximation of parabolic partial differential equations (PDEs for short) based on model order reduction techniques and the Laplace transform.","We start by applying said transform to the evolution problem, thus yielding a time-independent boundary value problem solely depending on the complex Laplace parameter.","In an offline stage, we judiciously sample the Laplace parameter and numerically solve the corresponding collection of high-fidelity or full-order problems.","Next, we apply a proper orthogonal decomposition (POD) to this collection of solutions in order to obtain a reduced basis in the Laplace domain.","We project the linear parabolic problem onto this basis, and then using any suitable time-stepping method, we solve the evolution problem.","A key insight to justify the implementation and analysis of the proposed method corresponds to resorting to Hardy spaces of analytic functions and establishing, through the Paley-Wiener theorem, an isometry between the solution of the time-dependent problem and its Laplace transform.","As a result, one may conclude that computing a POD with samples taken in the Laplace domain produces an exponentially accurate reduced basis for the time-dependent problem.","Numerical experiments portray the performance of the method in terms of accuracy and, in particular, speed-up when compared to the solution obtained by solving the full-order model."],"url":"http://arxiv.org/abs/2403.02847v1","category":"math.NA"}
{"created":"2024-03-05 10:35:52","title":"OORD: The Oxford Offroad Radar Dataset","abstract":"There is a growing academic interest as well as commercial exploitation of millimetre-wave scanning radar for autonomous vehicle localisation and scene understanding. Although several datasets to support this research area have been released, they are primarily focused on urban or semi-urban environments. Nevertheless, rugged offroad deployments are important application areas which also present unique challenges and opportunities for this sensor technology. Therefore, the Oxford Offroad Radar Dataset (OORD) presents data collected in the rugged Scottish highlands in extreme weather. The radar data we offer to the community are accompanied by GPS/INS reference - to further stimulate research in radar place recognition. In total we release over 90GiB of radar scans as well as GPS and IMU readings by driving a diverse set of four routes over 11 forays, totalling approximately 154km of rugged driving. This is an area increasingly explored in literature, and we therefore present and release examples of recent open-sourced radar place recognition systems and their performance on our dataset. This includes a learned neural network, the weights of which we also release. The data and tools are made freely available to the community at https://oxford-robotics-institute.github.io/oord-dataset.","sentences":["There is a growing academic interest as well as commercial exploitation of millimetre-wave scanning radar for autonomous vehicle localisation and scene understanding.","Although several datasets to support this research area have been released, they are primarily focused on urban or semi-urban environments.","Nevertheless, rugged offroad deployments are important application areas which also present unique challenges and opportunities for this sensor technology.","Therefore, the Oxford Offroad Radar Dataset (OORD) presents data collected in the rugged Scottish highlands in extreme weather.","The radar data we offer to the community are accompanied by GPS/INS reference - to further stimulate research in radar place recognition.","In total we release over 90GiB of radar scans as well as GPS and IMU readings by driving a diverse set of four routes over 11 forays, totalling approximately 154km of rugged driving.","This is an area increasingly explored in literature, and we therefore present and release examples of recent open-sourced radar place recognition systems and their performance on our dataset.","This includes a learned neural network, the weights of which we also release.","The data and tools are made freely available to the community at https://oxford-robotics-institute.github.io/oord-dataset."],"url":"http://arxiv.org/abs/2403.02845v1","category":"cs.RO"}
{"created":"2024-03-05 10:29:15","title":"Search for a $\u03bc^+\u03bc^-$ resonance in four-muon final states at Belle II","abstract":"We report on a search for a resonance $X$ decaying to a pair of muons in $e^{+}e^{-}\\rightarrow \\mu^+ \\mu^- X$ events in the 0.212-9.000 GeV/$c^{2}$ mass range, using 178 fb$^{-1}$ of data collected by the BelleII experiment at the SuperKEKB collider at a center of mass energy of 10.58 GeV. The analysis probes two different models of $X$ beyond the standard model: a $Z^{\\prime}$ vector boson in the $L_{\\mu}-L_{\\tau}$ model and a muonphilic scalar. We observe no evidence for a signal and set exclusion limits at the 90\\% confidence level on the products of cross section and branching fraction for these processes, ranging from 0.046 fb to 0.97 fb for the $L_{\\mu}-L_{\\tau}$ model and from 0.055 fb to 1.3 fb for the muonphilic scalar model. For masses below 6 GeV/$c^{2}$, the corresponding constraints on the couplings of these processes to the standard model range from 0.0008 to 0.039 for the $L_{\\mu}-L_{\\tau}$ model and from 0.0018 to 0.040 for the muonphilic scalar model. These are the first constraints on the muonphilic scalar from a dedicated search.","sentences":["We report on a search for a resonance $X$ decaying to a pair of muons in $e^{+}e^{-}\\rightarrow \\mu^+ \\mu^- X$ events in the 0.212-9.000 GeV/$c^{2}$ mass range, using 178 fb$^{-1}$ of data collected by the BelleII experiment at the SuperKEKB collider at a center of mass energy of 10.58 GeV.","The analysis probes two different models of $X$ beyond the standard model: a $Z^{\\prime}$ vector boson in the $L_{\\mu}-L_{\\tau}$ model and a muonphilic scalar.","We observe no evidence for a signal and set exclusion limits at the 90\\% confidence level on the products of cross section and branching fraction for these processes, ranging from 0.046 fb to 0.97 fb for the $L_{\\mu}-L_{\\tau}$ model and from 0.055 fb to 1.3 fb for the muonphilic scalar model.","For masses below 6 GeV/$c^{2}$, the corresponding constraints on the couplings of these processes to the standard model range from 0.0008 to 0.039 for the $L_{\\mu}-L_{\\tau}$ model and from 0.0018 to 0.040 for the muonphilic scalar model.","These are the first constraints on the muonphilic scalar from a dedicated search."],"url":"http://arxiv.org/abs/2403.02841v1","category":"hep-ex"}
{"created":"2024-03-05 09:22:30","title":"Atacama Large Aperture Submillimeter Telescope (AtLAST) Science: Surveying the distant Universe","abstract":"During the most active period of star formation in galaxies, which occurs in the redshift range 1<z<3, strong bursts of star formation result in significant quantities of dust, which obscures new stars being formed as their UV/optical light is absorbed and then re-emitted in the infrared, which redshifts into the mm/sub-mm bands for these early times. To get a complete picture of the high-z galaxy population, we need to survey a large patch of the sky in the sub-mm with sufficient angular resolution to resolve all galaxies, but we also need the depth to fully sample their cosmic evolution, and therefore obtain their redshifts using direct mm spectroscopy with a very wide frequency coverage. This requires a large single-dish sub-mm telescope with fast mapping speeds at high sensitivity and angular resolution, a large bandwidth with good spectral resolution and multiplex spectroscopic capabilities. The proposed 50-m Atacama Large Aperture Submillimeter Telescope (AtLAST) will deliver these specifications. We discuss how AtLAST allows us to study the whole population of high-z galaxies, including the dusty star-forming ones which can only be detected and studied in the sub-mm, and obtain a wealth of information for each of these up to z~7: gas content, cooling budget, star formation rate, dust mass, and dust temperature. We present worked examples of surveys that AtLAST can perform, both deep and wide, and also focused on galaxies in proto-clusters. In addition we show how such surveys with AtLAST can measure the growth rate and the Hubble constant with high accuracy, and demonstrate the power of the line-intensity mapping method in the mm/sub-mm wavebands to constrain the cosmic expansion history at high redshifts, as good examples of what can uniquely be done by AtLAST in this research field.","sentences":["During the most active period of star formation in galaxies, which occurs in the redshift range 1<z<3, strong bursts of star formation result in significant quantities of dust, which obscures new stars being formed as their UV/optical light is absorbed and then re-emitted in the infrared, which redshifts into the mm/sub-mm bands for these early times.","To get a complete picture of the high-z galaxy population, we need to survey a large patch of the sky in the sub-mm with sufficient angular resolution to resolve all galaxies, but we also need the depth to fully sample their cosmic evolution, and therefore obtain their redshifts using direct mm spectroscopy with a very wide frequency coverage.","This requires a large single-dish sub-mm telescope with fast mapping speeds at high sensitivity and angular resolution, a large bandwidth with good spectral resolution and multiplex spectroscopic capabilities.","The proposed 50-m Atacama Large Aperture Submillimeter Telescope (AtLAST) will deliver these specifications.","We discuss how AtLAST allows us to study the whole population of high-z galaxies, including the dusty star-forming ones which can only be detected and studied in the sub-mm, and obtain a wealth of information for each of these up to z~7: gas content, cooling budget, star formation rate, dust mass, and dust temperature.","We present worked examples of surveys that AtLAST can perform, both deep and wide, and also focused on galaxies in proto-clusters.","In addition we show how such surveys with AtLAST can measure the growth rate and the Hubble constant with high accuracy, and demonstrate the power of the line-intensity mapping method in the mm/sub-mm wavebands to constrain the cosmic expansion history at high redshifts, as good examples of what can uniquely be done by AtLAST in this research field."],"url":"http://arxiv.org/abs/2403.02806v1","category":"astro-ph.CO"}
{"created":"2024-03-05 08:46:43","title":"Ultrahigh Resolution X-ray Thomson Scattering Measurements of Electronic Structures","abstract":"Using a novel ultrahigh resolution ($\\Delta E \\sim 0.1\\,$eV) setup to measure electronic features in x-ray Thomson scattering (XRTS) experiments at the European XFEL in Germany, we have studied the collective plasmon excitation in aluminium at ambient conditions, which we can measure very accurately even at low momentum transfers. As a result, we can resolve previously reported discrepancies between ab initio time-dependent density functional theory simulations and experimental observations. The demonstrated capability for high-resolution XRTS measurements will be a game changer for the diagnosis of experiments with matter under extreme densities, temperatures, and pressures, and unlock the full potential of state-of-the-art x-ray free electron laser (XFEL) facilities to study planetary interior conditions, to understand inertial confinement fusion applications, and for material science and discovery.","sentences":["Using a novel ultrahigh resolution ($\\Delta E \\sim 0.1\\,$eV) setup to measure electronic features in x-ray Thomson scattering (XRTS) experiments at the European XFEL in Germany, we have studied the collective plasmon excitation in aluminium at ambient conditions, which we can measure very accurately even at low momentum transfers.","As a result, we can resolve previously reported discrepancies between ab initio time-dependent density functional theory simulations and experimental observations.","The demonstrated capability for high-resolution XRTS measurements will be a game changer for the diagnosis of experiments with matter under extreme densities, temperatures, and pressures, and unlock the full potential of state-of-the-art x-ray free electron laser (XFEL) facilities to study planetary interior conditions, to understand inertial confinement fusion applications, and for material science and discovery."],"url":"http://arxiv.org/abs/2403.02776v1","category":"physics.plasm-ph"}
{"created":"2024-03-05 08:38:25","title":"Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives","abstract":"Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entire dataset to train a single model applicable to all exercise types. This model, with a Spatial-Temporal Graph Convolutional Network (ST-GCN) architecture, demonstrated enhanced generalizability across exercises and a decrease in overall complexity. Through extensive experiments on three publicly available rehabilitation exercise assessment datasets, the University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD), IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation (KIMORE), our method has shown to surpass existing methods, setting a new benchmark in rehabilitation exercise assessment accuracy.","sentences":["Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates.","AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress.","These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type.","This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise.","Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entire dataset to train a single model applicable to all exercise types.","This model, with a Spatial-Temporal Graph Convolutional Network (ST-GCN) architecture, demonstrated enhanced generalizability across exercises and a decrease in overall complexity.","Through extensive experiments on three publicly available rehabilitation exercise assessment datasets, the University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD), IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation (KIMORE), our method has shown to surpass existing methods, setting a new benchmark in rehabilitation exercise assessment accuracy."],"url":"http://arxiv.org/abs/2403.02772v1","category":"cs.LG"}
{"created":"2024-03-05 08:37:05","title":"HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes","abstract":"Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics. However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions. With limited labeled data, supervised methods are difficult to generalize to general scenarios, hindering real-life applications. Mimicking human intelligence, we propose an unsupervised 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes. To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment. Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving a substantial 87.8\\% improvement in mAP and closely approaching the performance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLife.","sentences":["Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics.","However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions.","With limited labeled data, supervised methods are difficult to generalize to general scenarios, hindering real-life applications.","Mimicking human intelligence, we propose an unsupervised 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes.","To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment.","Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving a substantial 87.8\\% improvement in mAP and closely approaching the performance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLife."],"url":"http://arxiv.org/abs/2403.02769v1","category":"cs.CV"}
{"created":"2024-03-05 08:36:35","title":"An Empirical Analysis on the Use and Reporting of National Security Letters","abstract":"National Security Letters (NSLs) are similar to administrative subpoenas and can be issued directly by elements of the executive branch without requiring prior approval from a court or grand jury. Importantly, NSLs authorize the imposition of nondisclosure orders (aka \"gag orders\") on the receiving party. Controversy about potential abuses of this authority has driven a range of legal and policy discussions. To address these concerns, both the public sector and the private sector have sought to document the usage of NSLs in aggregated form. However, each data source is limited in scope, time, and kind.   In this paper, we consolidate the available data around NSLs and answer two questions: (1) what can the public effectively learn from the reported data and does this information suffice to assess the NSL usage? (2) how accessible is this data collection? We show that longitudinal trends in the usage of NSLs can be observed. For instance, we find a significant increase in NSL requests for non-US persons and that the policy reforms to decrease the mandated nondisclosure period appear to be effective. The observed trends suggest that the current transparency mechanisms are viable safeguards against the excessive use of NSLs. However, aggregating and normalizing the data requires manual reviewing, parsing, and validating. We even find inconsistencies within and across official data sources. Overall, the laborious data collection process hinders external and internal auditing efforts and demonstrates the need for a unified and more usable dataset for NSLs.","sentences":["National Security Letters (NSLs) are similar to administrative subpoenas and can be issued directly by elements of the executive branch without requiring prior approval from a court or grand jury.","Importantly, NSLs authorize the imposition of nondisclosure orders (aka \"gag orders\") on the receiving party.","Controversy about potential abuses of this authority has driven a range of legal and policy discussions.","To address these concerns, both the public sector and the private sector have sought to document the usage of NSLs in aggregated form.","However, each data source is limited in scope, time, and kind.   ","In this paper, we consolidate the available data around NSLs and answer two questions: (1) what can the public effectively learn from the reported data and does this information suffice to assess the NSL usage?","(2) how accessible is this data collection?","We show that longitudinal trends in the usage of NSLs can be observed.","For instance, we find a significant increase in NSL requests for non-US persons and that the policy reforms to decrease the mandated nondisclosure period appear to be effective.","The observed trends suggest that the current transparency mechanisms are viable safeguards against the excessive use of NSLs.","However, aggregating and normalizing the data requires manual reviewing, parsing, and validating.","We even find inconsistencies within and across official data sources.","Overall, the laborious data collection process hinders external and internal auditing efforts and demonstrates the need for a unified and more usable dataset for NSLs."],"url":"http://arxiv.org/abs/2403.02768v1","category":"cs.CY"}
{"created":"2024-03-05 08:31:00","title":"Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations","abstract":"With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions. At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities. As a result, recent research has sought to harness the power of LLM to improve recommendation systems. Given the rapid development of this research direction in the field of recommendation systems, there is an urgent need for a systematic review of existing LLM-driven recommendation systems for researchers and practitioners in related fields to gain insight into. More specifically, we first introduced a representative approach to learning user and item representations using LLM as a feature encoder. We then reviewed the latest advances in LLMs techniques for collaborative filtering enhanced recommendation systems from the three paradigms of pre-training, fine-tuning, and prompting. Finally, we had a comprehensive discussion on the future direction of this emerging field.","sentences":["With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences.","Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information.","It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions.","At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities.","As a result, recent research has sought to harness the power of LLM to improve recommendation systems.","Given the rapid development of this research direction in the field of recommendation systems, there is an urgent need for a systematic review of existing LLM-driven recommendation systems for researchers and practitioners in related fields to gain insight into.","More specifically, we first introduced a representative approach to learning user and item representations using LLM as a feature encoder.","We then reviewed the latest advances in LLMs techniques for collaborative filtering enhanced recommendation systems from the three paradigms of pre-training, fine-tuning, and prompting.","Finally, we had a comprehensive discussion on the future direction of this emerging field."],"url":"http://arxiv.org/abs/2403.02760v1","category":"cs.AI"}
{"created":"2024-03-05 08:28:42","title":"Kinetic temperature and radial flow velocity estimation using identified hadrons and light (anti-)nuclei produced in relativistic heavy-ion collisions at RHIC and LHC","abstract":"We report the investigation of the kinetic freeze-out properties of identified hadrons ($\\pi^\\pm$, $K^\\pm$ and $p(\\bar p)$) along with light (anti-)nuclei $d (\\bar d)$, $t (\\bar t)$ and ${}^{3}He$ in relativistic heavy-ion collisions at RHIC and LHC energies. A simultaneous fit is performed with the Blast-Wave (BW) model to the transverse momentum ({\\ppt}) spectra of identified hadrons together with light (anti-)nuclei produced in {\\auau} collisions at {\\sqrtsNN} = 7.7 -- 200 GeV and in {\\pbpb} collisions at {\\sqrtsNN} = 2.76. The energy and centrality dependence of kinetic freeze-out temperature ($T_{kin}$) and collective flow velocity $\\langle \\beta \\rangle$ has been studied. It is observed that light (anti-)nuclei also participate in the collective expansion of the medium created in the collision when included in a common fit with the light hadrons. We observe a marginal rise in the $T_{kin}$ and a slight decrease in $\\langle \\beta \\rangle$ when compared to the values obtained from the fit to light hadrons, indicating that heavier particles decouple earlier in time from the fireball compared to the light hadrons due to higher masses and are more sensitive to the effects of the radial flow. A similar $\\langle \\beta \\rangle$ and significant larger $T_{kin}$ is observed when fit is performed to only protons and light (anti-)nuclei. Additionally, we also observe that the $T_{kin}$ increases from central to peripheral collisions, which is consistent with the argument of short-lived fireball in peripheral collisions. Whereas the $\\langle \\beta \\rangle$ shows a decreasing trend from central to peripheral collisions indicating a more rapid expansion in the central collisions. Both, $T_{kin}$ and $\\langle \\beta \\rangle$ show a weak dependence on the collision energy at most energies. We also observe a strong anti-correlation between $T_{kin}$ and $\\langle \\beta \\rangle$.","sentences":["We report the investigation of the kinetic freeze-out properties of identified hadrons ($\\pi^\\pm$, $K^\\pm$ and $p(\\bar p)$) along with light (anti-)nuclei $d (\\bar d)$, $t (\\bar t)$ and ${}^{3}He$ in relativistic heavy-ion collisions at RHIC and LHC energies.","A simultaneous fit is performed with the Blast-Wave (BW) model to the transverse momentum ({\\ppt}) spectra of identified hadrons together with light (anti-)nuclei produced in {\\auau} collisions at {\\sqrtsNN} = 7.7 -- 200 GeV and in {\\pbpb} collisions at {\\sqrtsNN} = 2.76.","The energy and centrality dependence of kinetic freeze-out temperature ($T_{kin}$) and collective flow velocity $\\langle \\beta \\rangle$ has been studied.","It is observed that light (anti-)nuclei also participate in the collective expansion of the medium created in the collision when included in a common fit with the light hadrons.","We observe a marginal rise in the $T_{kin}$ and a slight decrease in $\\langle \\beta \\rangle$ when compared to the values obtained from the fit to light hadrons, indicating that heavier particles decouple earlier in time from the fireball compared to the light hadrons due to higher masses and are more sensitive to the effects of the radial flow.","A similar $\\langle \\beta \\rangle$ and significant larger $T_{kin}$ is observed when fit is performed to only protons and light (anti-)nuclei.","Additionally, we also observe that the $T_{kin}$ increases from central to peripheral collisions, which is consistent with the argument of short-lived fireball in peripheral collisions.","Whereas the $\\langle \\beta \\rangle$ shows a decreasing trend from central to peripheral collisions indicating a more rapid expansion in the central collisions.","Both, $T_{kin}$ and $\\langle \\beta \\rangle$ show a weak dependence on the collision energy at most energies.","We also observe a strong anti-correlation between $T_{kin}$ and $\\langle \\beta \\rangle$."],"url":"http://arxiv.org/abs/2403.02759v1","category":"nucl-th"}
{"created":"2024-03-05 08:25:11","title":"In-Memory Learning: A Declarative Learning Framework for Large Language Models","abstract":"The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.","sentences":["The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic.","Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework.","The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment.","This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning.","We also delve into the key features of benchmarks designed to evaluate the self-improvement process.","Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem."],"url":"http://arxiv.org/abs/2403.02757v1","category":"cs.CL"}
{"created":"2024-03-05 08:15:57","title":"HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents","abstract":"Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.","sentences":["Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc.","Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment.","In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent.","By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus.","The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity.","The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking.","To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study.","We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking.","We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents.","We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis."],"url":"http://arxiv.org/abs/2403.02752v1","category":"cs.HC"}
{"created":"2024-03-05 08:08:59","title":"Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection","abstract":"Ultrasound is a widely used medical tool for non-invasive diagnosis, but its images often contain speckle noise which can lower their resolution and contrast-to-noise ratio. This can make it more difficult to extract, recognize, and analyze features in the images, as well as impair the accuracy of computer-assisted diagnostic techniques and the ability of doctors to interpret the images. Reducing speckle noise, therefore, is a crucial step in the preprocessing of ultrasound images. Researchers have proposed several speckle reduction methods, but no single method takes all relevant factors into account. In this paper, we compare seven such methods: Median, Gaussian, Bilateral, Average, Weiner, Anisotropic and Denoising auto-encoder without and with skip connections in terms of their ability to preserve features and edges while effectively reducing noise. In an experimental study, a convolutional noise-removing auto-encoder with skip connection, a deep learning method, was used to improve ultrasound images of breast cancer. This method involved adding speckle noise at various levels. The results of the deep learning method were compared to those of traditional image enhancement methods, and it was found that the proposed method was more effective. To assess the performance of these algorithms, we use three established evaluation metrics and present both filtered images and statistical data.","sentences":["Ultrasound is a widely used medical tool for non-invasive diagnosis, but its images often contain speckle noise which can lower their resolution and contrast-to-noise ratio.","This can make it more difficult to extract, recognize, and analyze features in the images, as well as impair the accuracy of computer-assisted diagnostic techniques and the ability of doctors to interpret the images.","Reducing speckle noise, therefore, is a crucial step in the preprocessing of ultrasound images.","Researchers have proposed several speckle reduction methods, but no single method takes all relevant factors into account.","In this paper, we compare seven such methods: Median, Gaussian, Bilateral, Average, Weiner, Anisotropic and Denoising auto-encoder without and with skip connections in terms of their ability to preserve features and edges while effectively reducing noise.","In an experimental study, a convolutional noise-removing auto-encoder with skip connection, a deep learning method, was used to improve ultrasound images of breast cancer.","This method involved adding speckle noise at various levels.","The results of the deep learning method were compared to those of traditional image enhancement methods, and it was found that the proposed method was more effective.","To assess the performance of these algorithms, we use three established evaluation metrics and present both filtered images and statistical data."],"url":"http://arxiv.org/abs/2403.02750v1","category":"eess.IV"}
{"created":"2024-03-05 07:58:12","title":"CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models","abstract":"This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.","sentences":["This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets.","We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues.","In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it.","To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response.","Furthermore, we show robust recovery results in the partially observed setting.","Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings.","This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs."],"url":"http://arxiv.org/abs/2403.02745v1","category":"cs.AI"}
{"created":"2024-03-05 07:44:13","title":"Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery","abstract":"Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling. We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%. This advancement enables effective machine learning mapping even with minimal labeling budgets, exemplified by an F1 score on the boma detection task of 0.51 with a budget of 300 total patches.","sentences":["Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with.","This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest.","We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling.","We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania.","We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%.","This advancement enables effective machine learning mapping even with minimal labeling budgets, exemplified by an F1 score on the boma detection task of 0.51 with a budget of 300 total patches."],"url":"http://arxiv.org/abs/2403.02736v1","category":"cs.CV"}
{"created":"2024-03-05 07:34:51","title":"HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?","abstract":"There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR). Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and think step-by-step strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets. Our findings indicate that by effective prompting, LLMs can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively.","sentences":["There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world.","In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR).","Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts.","HARGPT inputs raw IMU data into LLMs and utilizes the role-play and think step-by-step strategies for prompting.","We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models.","Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets.","Our findings indicate that by effective prompting, LLMs can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively."],"url":"http://arxiv.org/abs/2403.02727v1","category":"cs.CL"}
{"created":"2024-03-05 07:34:41","title":"Bias in Generative AI","abstract":"This study analyzed images generated by three popular generative artificial intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators. Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances. Firstly, we found that all three AI generators exhibited bias against women and African Americans. Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society. Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances. For example, women were depicted as younger with more smiles and happiness, while men were depicted as older with more neutral expressions and anger, posing a risk that generative AI models may unintentionally depict women as more submissive and less competent than men. Such nuanced biases, by their less overt nature, might be more problematic as they can permeate perceptions unconsciously and may be more difficult to rectify. Although the extent of bias varied depending on the model, the direction of bias remained consistent in both commercial and open-source AI generators. As these tools become commonplace, our study highlights the urgency to identify and mitigate various biases in generative AI, reinforcing the commitment to ensuring that AI technologies benefit all of humanity for a more inclusive future.","sentences":["This study analyzed images generated by three popular generative artificial intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators.","Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances.","Firstly, we found that all three AI generators exhibited bias against women and African Americans.","Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society.","Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances.","For example, women were depicted as younger with more smiles and happiness, while men were depicted as older with more neutral expressions and anger, posing a risk that generative AI models may unintentionally depict women as more submissive and less competent than men.","Such nuanced biases, by their less overt nature, might be more problematic as they can permeate perceptions unconsciously and may be more difficult to rectify.","Although the extent of bias varied depending on the model, the direction of bias remained consistent in both commercial and open-source AI generators.","As these tools become commonplace, our study highlights the urgency to identify and mitigate various biases in generative AI, reinforcing the commitment to ensuring that AI technologies benefit all of humanity for a more inclusive future."],"url":"http://arxiv.org/abs/2403.02726v1","category":"econ.GN"}
{"created":"2024-03-05 07:29:12","title":"Minimum Topology Attacks for Graph Neural Networks","abstract":"With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology. Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges. Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer.","sentences":["With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention.","Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node.","However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility.","To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node.","To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology.","Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges.","Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer."],"url":"http://arxiv.org/abs/2403.02723v1","category":"cs.AI"}
{"created":"2024-03-05 07:17:18","title":"Multi-Scale Subgraph Contrastive Learning","abstract":"Graph-level contrastive learning, aiming to learn the representations for each graph by contrasting two augmented graphs, has attracted considerable attention. Previous studies usually simply assume that a graph and its augmented graph as a positive pair, otherwise as a negative pair. However, it is well known that graph structure is always complex and multi-scale, which gives rise to a fundamental question: after graph augmentation, will the previous assumption still hold in reality? By an experimental analysis, we discover the semantic information of an augmented graph structure may be not consistent as original graph structure, and whether two augmented graphs are positive or negative pairs is highly related with the multi-scale structures. Based on this finding, we propose a multi-scale subgraph contrastive learning method which is able to characterize the fine-grained semantic information. Specifically, we generate global and local views at different scales based on subgraph sampling, and construct multiple contrastive relationships according to their semantic associations to provide richer self-supervised signals. Extensive experiments and parametric analysis on eight graph classification real-world datasets well demonstrate the effectiveness of the proposed method.","sentences":["Graph-level contrastive learning, aiming to learn the representations for each graph by contrasting two augmented graphs, has attracted considerable attention.","Previous studies usually simply assume that a graph and its augmented graph as a positive pair, otherwise as a negative pair.","However, it is well known that graph structure is always complex and multi-scale, which gives rise to a fundamental question: after graph augmentation, will the previous assumption still hold in reality?","By an experimental analysis, we discover the semantic information of an augmented graph structure may be not consistent as original graph structure, and whether two augmented graphs are positive or negative pairs is highly related with the multi-scale structures.","Based on this finding, we propose a multi-scale subgraph contrastive learning method which is able to characterize the fine-grained semantic information.","Specifically, we generate global and local views at different scales based on subgraph sampling, and construct multiple contrastive relationships according to their semantic associations to provide richer self-supervised signals.","Extensive experiments and parametric analysis on eight graph classification real-world datasets well demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.02719v1","category":"cs.AI"}
{"created":"2024-03-05 07:13:28","title":"Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models","abstract":"Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights underscore the significance of meticulous fine-tuning with high-quality datasets in enhancing LLM performance.","sentences":["Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence.","However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese.","The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation.","To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics.","Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese.","Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets.","These insights underscore the significance of meticulous fine-tuning with high-quality datasets in enhancing LLM performance."],"url":"http://arxiv.org/abs/2403.02715v1","category":"cs.CL"}
{"created":"2024-03-05 06:55:43","title":"DeepBioisostere: Discovering Bioisosteres with Deep Learning for a Fine Control of Multiple Molecular Properties","abstract":"Optimizing molecules to improve their properties is a fundamental challenge in drug design. For a fine-tuning of molecular properties without losing bio-activity validated in advance, the concept of bioisosterism has emerged. Many in silico methods have been proposed for discovering bioisosteres, but they require expert knowledge for their applications or are restricted to known databases. Here, we introduce DeepBioisostere, a deep generative model to design suitable bioisosteric replacements. Our model allows an end-to-end chemical replacement by intelligently selecting fragments for removal and insertion along with their attachment orientation. Through various scenarios of multiple property control, we showcase the model's capability to modulate specific properties, addressing the challenge in molecular optimization. Our model's innovation lies in its capacity to design a bioisosteric replacement reflecting the compatibility with the surroundings of the modification site, facilitating the control of sophisticated properties like drug-likeness. DeepBioisostere can also provide previously unseen bioisosteric replacements, highlighting its capability for exploring diverse chemical modifications rather than just mining them from known databases. Lastly, we employed DeepBioisostere to improve the sensitivity of a known SARS-CoV-2 main protease inhibitor to the E166V mutant that exhibits drug resistance to the inhibitor, demonstrating its potential application in lead optimization.","sentences":["Optimizing molecules to improve their properties is a fundamental challenge in drug design.","For a fine-tuning of molecular properties without losing bio-activity validated in advance, the concept of bioisosterism has emerged.","Many in silico methods have been proposed for discovering bioisosteres, but they require expert knowledge for their applications or are restricted to known databases.","Here, we introduce DeepBioisostere, a deep generative model to design suitable bioisosteric replacements.","Our model allows an end-to-end chemical replacement by intelligently selecting fragments for removal and insertion along with their attachment orientation.","Through various scenarios of multiple property control, we showcase the model's capability to modulate specific properties, addressing the challenge in molecular optimization.","Our model's innovation lies in its capacity to design a bioisosteric replacement reflecting the compatibility with the surroundings of the modification site, facilitating the control of sophisticated properties like drug-likeness.","DeepBioisostere can also provide previously unseen bioisosteric replacements, highlighting its capability for exploring diverse chemical modifications rather than just mining them from known databases.","Lastly, we employed DeepBioisostere to improve the sensitivity of a known SARS-CoV-2 main protease inhibitor to the E166V mutant that exhibits drug resistance to the inhibitor, demonstrating its potential application in lead optimization."],"url":"http://arxiv.org/abs/2403.02706v1","category":"q-bio.BM"}
{"created":"2024-03-05 06:46:43","title":"Fighting Game Adaptive Background Music for Improved Gameplay","abstract":"This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features. The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition. The BGM adapts by changing the volume of each category of instruments. Each category is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.","sentences":["This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features.","The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition.","The BGM adapts by changing the volume of each category of instruments.","Each category is connected to a different element of the game.","We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI).","The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM."],"url":"http://arxiv.org/abs/2403.02701v1","category":"cs.SD"}
{"created":"2024-03-05 06:23:50","title":"Privacy-Aware Semantic Cache for Large Language Models","abstract":"Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model in a distributed manner across numerous users without violating privacy. By placing a local cache in each user's device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower cache false hit rates. Our experiments, benchmarked against the GPTCache, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%, while still surpassing GPTCache.","sentences":["Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics.","However, these models incur exceptionally high computational costs.","For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations.","Caching is a natural solution to reduce LLM inference costs on repeated queries.","However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   ","This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss.","Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact.","MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model in a distributed manner across numerous users without violating privacy.","By placing a local cache in each user's device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower cache false hit rates.","Our experiments, benchmarked against the GPTCache, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions.","Furthermore, MeanCache reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%, while still surpassing GPTCache."],"url":"http://arxiv.org/abs/2403.02694v1","category":"cs.LG"}
{"created":"2024-03-05 06:17:13","title":"DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators","abstract":"Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally drifting noises. The DOCTOR framework intelligently monitors the chip status using adaptive probing and performs fast in-situ training-free calibration to restore accuracy when necessary. Recognizing nonuniform spatial variation distributions across devices and tensor cores, we also propose a variation-aware architectural remapping strategy to avoid executing critical tasks on noisy devices. Extensive experiments show that our proposed framework can guarantee sustained performance under drifting variations with 34% higher accuracy and 2-3 orders-of-magnitude lower overhead compared to state-of-the-art on-chip training methods.","sentences":["Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments.","However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations.","While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism.","To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally drifting noises.","The DOCTOR framework intelligently monitors the chip status using adaptive probing and performs fast in-situ training-free calibration to restore accuracy when necessary.","Recognizing nonuniform spatial variation distributions across devices and tensor cores, we also propose a variation-aware architectural remapping strategy to avoid executing critical tasks on noisy devices.","Extensive experiments show that our proposed framework can guarantee sustained performance under drifting variations with 34% higher accuracy and 2-3 orders-of-magnitude lower overhead compared to state-of-the-art on-chip training methods."],"url":"http://arxiv.org/abs/2403.02688v1","category":"cs.ET"}
{"created":"2024-03-05 06:15:48","title":"Enhanced DareFightingICE Competitions: Sound Design and AI Competitions","abstract":"This paper presents a new and improved DareFightingICE platform, a fighting game platform with a focus on visually impaired players (VIPs), in the Unity game engine. It also introduces the separation of the DareFightingICE Competition into two standalone competitions called DareFightingICE Sound Design Competition and DareFightingICE AI Competition--at the 2024 IEEE Conference on Games (CoG)--in which a new platform will be used. This new platform is an enhanced version of the old DareFightingICE platform, having a better audio system to convey 3D sound and a better way to send audio data to AI agents. With this enhancement and by utilizing Unity, the new DareFightingICE platform is more accessible in terms of adding new features for VIPs and future audio research. This paper also improves the evaluation method for evaluating sound designs in the Sound Design Competition which will ensure a better sound design for VIPs as this competition continues to run at future CoG. To the best of our knowledge, both of our competitions are first of their kind, and the connection between the competitions to mutually improve the entries' quality with time makes these competitions an important part of representing an often overlooked segment within the broader gaming community, VIPs.","sentences":["This paper presents a new and improved DareFightingICE platform, a fighting game platform with a focus on visually impaired players (VIPs), in the Unity game engine.","It also introduces the separation of the DareFightingICE Competition into two standalone competitions called DareFightingICE Sound Design Competition and DareFightingICE AI Competition--at the 2024 IEEE Conference on Games (CoG)--in which a new platform will be used.","This new platform is an enhanced version of the old DareFightingICE platform, having a better audio system to convey 3D sound and a better way to send audio data to AI agents.","With this enhancement and by utilizing Unity, the new DareFightingICE platform is more accessible in terms of adding new features for VIPs and future audio research.","This paper also improves the evaluation method for evaluating sound designs in the Sound Design Competition which will ensure a better sound design for VIPs as this competition continues to run at future CoG. To the best of our knowledge, both of our competitions are first of their kind, and the connection between the competitions to mutually improve the entries' quality with time makes these competitions an important part of representing an often overlooked segment within the broader gaming community, VIPs."],"url":"http://arxiv.org/abs/2403.02687v1","category":"cs.HC"}
{"created":"2024-03-05 06:10:28","title":"Learning to Defer to a Population: A Meta-Learning Approach","abstract":"The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks.","sentences":["The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert.","All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained.","In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time.","We accomplish this by using meta-learning, considering both optimization- and model-based variants.","Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy.","For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities.","In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks."],"url":"http://arxiv.org/abs/2403.02683v1","category":"cs.LG"}
{"created":"2024-03-05 05:38:54","title":"Elemental abundances in the diffuse ISM from joint FUV and X-ray spectroscopy: iron, oxygen, carbon and sulfur","abstract":"In this study, we investigate interstellar absorption lines along the line of sight toward the galactic low-mass X-ray binary Cygnus X-2. We combine absorption line data obtained from high-resolution X-ray spectra collected with Chandra and XMM-Newton satellites, along with Far-UV absorption lines observed by the Hubble Space Telescope's (HST) Cosmic Origins Spectrograph (COS) Instrument. Our primary objective is to understand the abundance and depletion of oxygen, iron, sulfur, and carbon. To achieve this, we have developed an analysis pipeline that simultaneously fits both the UV and X-ray datasets. This novel approach takes into account the line spread function (LSF) of HST/COS, enhancing the precision of our results. We examine the absorption lines of FeII, SII, CII, and CI present in the FUV spectrum of Cygnus X-2, revealing the presence of at least two distinct absorbers characterized by different velocities. Additionally, we employ Cloudy simulations to compare our findings concerning the ionic ratios for the studied elements. We find that gaseous iron and sulfur exist in their singly ionized forms, Fe II and S II, respectively, while the abundances of CII and CI do not agree with the Cloudy simulations of the neutral ISM. Finally, we explore discrepancies in the X-ray atomic data of iron and discuss their impact on the overall abundance and depletion of iron.","sentences":["In this study, we investigate interstellar absorption lines along the line of sight toward the galactic low-mass X-ray binary Cygnus X-2.","We combine absorption line data obtained from high-resolution X-ray spectra collected with Chandra and XMM-Newton satellites, along with Far-UV absorption lines observed by the Hubble Space Telescope's (HST) Cosmic Origins Spectrograph (COS) Instrument.","Our primary objective is to understand the abundance and depletion of oxygen, iron, sulfur, and carbon.","To achieve this, we have developed an analysis pipeline that simultaneously fits both the UV and X-ray datasets.","This novel approach takes into account the line spread function (LSF) of HST/COS, enhancing the precision of our results.","We examine the absorption lines of FeII, SII, CII, and CI present in the FUV spectrum of Cygnus X-2, revealing the presence of at least two distinct absorbers characterized by different velocities.","Additionally, we employ Cloudy simulations to compare our findings concerning the ionic ratios for the studied elements.","We find that gaseous iron and sulfur exist in their singly ionized forms, Fe II and S II, respectively, while the abundances of CII and CI do not agree with the Cloudy simulations of the neutral ISM.","Finally, we explore discrepancies in the X-ray atomic data of iron and discuss their impact on the overall abundance and depletion of iron."],"url":"http://arxiv.org/abs/2403.02664v1","category":"astro-ph.GA"}
{"created":"2024-03-05 04:48:24","title":"Learning at the Speed of Wireless: Online Real-Time Learning for AI-Enabled MIMO in NextG","abstract":"Integration of artificial intelligence (AI) and machine learning (ML) into the air interface has been envisioned as a key technology for next-generation (NextG) cellular networks. At the air interface, multiple-input multiple-output (MIMO) and its variants such as multi-user MIMO (MU-MIMO) and massive/full-dimension MIMO have been key enablers across successive generations of cellular networks with evolving complexity and design challenges. Initiating active investigation into leveraging AI/ML tools to address these challenges for MIMO becomes a critical step towards an AI-enabled NextG air interface. At the NextG air interface, the underlying wireless environment will be extremely dynamic with operation adaptations performed on a sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and rank/link adaptation. Given the enormously large number of operation adaptation possibilities, we contend that online real-time AI/ML-based approaches constitute a promising paradigm. To this end, we outline the inherent challenges and offer insights into the design of such online real-time AI/ML-based solutions for MIMO operations. An online real-time AI/ML-based method for MIMO-OFDM channel estimation is then presented, serving as a potential roadmap for developing similar techniques across various MIMO operations in NextG.","sentences":["Integration of artificial intelligence (AI) and machine learning (ML) into the air interface has been envisioned as a key technology for next-generation (NextG) cellular networks.","At the air interface, multiple-input multiple-output (MIMO) and its variants such as multi-user MIMO (MU-MIMO) and massive/full-dimension MIMO have been key enablers across successive generations of cellular networks with evolving complexity and design challenges.","Initiating active investigation into leveraging AI/ML tools to address these challenges for MIMO becomes a critical step towards an AI-enabled NextG air interface.","At the NextG air interface, the underlying wireless environment will be extremely dynamic with operation adaptations performed on a sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and rank/link adaptation.","Given the enormously large number of operation adaptation possibilities, we contend that online real-time AI/ML-based approaches constitute a promising paradigm.","To this end, we outline the inherent challenges and offer insights into the design of such online real-time AI/ML-based solutions for MIMO operations.","An online real-time AI/ML-based method for MIMO-OFDM channel estimation is then presented, serving as a potential roadmap for developing similar techniques across various MIMO operations in NextG."],"url":"http://arxiv.org/abs/2403.02651v1","category":"eess.SP"}
{"created":"2024-03-05 04:35:59","title":"Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad","abstract":"Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \\left(\\frac{\\log T}{\\sqrt{T}} \\right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.","sentences":["Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive.","This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm.","We prove the scale-invariance of KATE for the case of Generalized Linear Models.","Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \\left(\\frac{\\log T}{\\sqrt{T}} \\right)$ for KATE, matching the best-known ones for AdaGrad and Adam.","We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data.","The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios."],"url":"http://arxiv.org/abs/2403.02648v1","category":"cs.LG"}
{"created":"2024-03-05 04:33:36","title":"FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model","abstract":"The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing.   Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news information and combining it with stock factors, the return forecasting module aim to analysis the impact of news on market sentiment, and the risk assessment module is adopted to control investment risk. Extensive experiments on real-world datasets have well verified the effectiveness and explainability of our proposed FinReport. Our codes and datasets are available at https://github.com/frinkleko/FinReport.","sentences":["The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios.","However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news.","On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions.","To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing.   ","Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report.","The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module.","The news factorization module involves understanding news information and combining it with stock factors, the return forecasting module aim to analysis the impact of news on market sentiment, and the risk assessment module is adopted to control investment risk.","Extensive experiments on real-world datasets have well verified the effectiveness and explainability of our proposed FinReport.","Our codes and datasets are available at https://github.com/frinkleko/FinReport."],"url":"http://arxiv.org/abs/2403.02647v1","category":"cs.CL"}
{"created":"2024-03-05 04:08:19","title":"HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative","abstract":"Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.","sentences":["Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years.","Vehicle-infrastructure cooperation (VIC) becomes one of the important research area.","Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems.","To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC.","Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections.","Each intersection is equipped with 6-18 sensors to capture synchronous data.","While autonomous vehicles pass through these intersections for collecting VIC data.","HoloVIC contains in total on 100k+ synchronous frames from different sensors.","Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar.","We also associate the IDs of the same objects across different devices and consecutive frames in sequence.","Based on HoloVIC, we formulated four tasks to facilitate the development of related research.","We also provide benchmarks for these tasks."],"url":"http://arxiv.org/abs/2403.02640v1","category":"cs.CV"}
{"created":"2024-03-05 03:59:01","title":"PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning","abstract":"Training for multi-agent reinforcement learning(MARL) is a time-consuming process caused by distribution shift of each agent. One drawback is that strategy of each agent in MARL is independent but actually in cooperation. Thus, a vertical issue in multi-agent reinforcement learning is how to efficiently accelerate training process. To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent. However, CF based methods introduce joint error from other agents in estimation of value network. In so doing, inspired by federated learning, we propose three simple novel approaches called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL. Agents share Q-value network periodically during the training process. Agents which has same identity adapt collected reward as scalability and update partial neural network during period to share different parameters. We apply our approaches in classical MARL method QMIX and evaluate our approaches on various tasks in StarCraft Multi-Agent Challenge(SMAC) environment. Performance of numerical experiments yield enormous enhancement, with an average improvement of 10\\%-30\\%, and enable to win tasks that QMIX cannot. Our code can be downloaded from https://github.com/ColaZhang22/PPS-QMIX","sentences":["Training for multi-agent reinforcement learning(MARL) is a time-consuming process caused by distribution shift of each agent.","One drawback is that strategy of each agent in MARL is independent but actually in cooperation.","Thus, a vertical issue in multi-agent reinforcement learning is how to efficiently accelerate training process.","To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent.","However, CF based methods introduce joint error from other agents in estimation of value network.","In so doing, inspired by federated learning, we propose three simple novel approaches called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL.","Agents share Q-value network periodically during the training process.","Agents which has same identity adapt collected reward as scalability and update partial neural network during period to share different parameters.","We apply our approaches in classical MARL method QMIX and evaluate our approaches on various tasks in StarCraft Multi-Agent Challenge(SMAC) environment.","Performance of numerical experiments yield enormous enhancement, with an average improvement of 10\\%-30\\%, and enable to win tasks that QMIX cannot.","Our code can be downloaded from https://github.com/ColaZhang22/PPS-QMIX"],"url":"http://arxiv.org/abs/2403.02635v1","category":"cs.AI"}
{"created":"2024-03-05 03:40:39","title":"Privacy in Multi-agent Systems","abstract":"With the increasing awareness of privacy and the deployment of legislations in various multi-agent system application domains such as power systems and intelligent transportation, the privacy protection problem for multi-agent systems is gaining increased traction in recent years. This article discusses some of the representative advancements in the filed.","sentences":["With the increasing awareness of privacy and the deployment of legislations in various multi-agent system application domains such as power systems and intelligent transportation, the privacy protection problem for multi-agent systems is gaining increased traction in recent years.","This article discusses some of the representative advancements in the filed."],"url":"http://arxiv.org/abs/2403.02631v1","category":"eess.SY"}
{"created":"2024-03-05 03:32:02","title":"Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects","abstract":"This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL), to tackle them. POE incorporates a continuous Pareto module with representation balancing, enhancing estimation efficiency across multiple tasks. As for POPL, it involves deriving short-term and long-term outcomes linked with various treatment levels, facilitating an exploration of the Pareto frontier emanating from these outcomes. Results on both the synthetic and real-world datasets demonstrate the superiority of our method.","sentences":["This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other.","For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects.","Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge.","Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well.","In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL), to tackle them.","POE incorporates a continuous Pareto module with representation balancing, enhancing estimation efficiency across multiple tasks.","As for POPL, it involves deriving short-term and long-term outcomes linked with various treatment levels, facilitating an exploration of the Pareto frontier emanating from these outcomes.","Results on both the synthetic and real-world datasets demonstrate the superiority of our method."],"url":"http://arxiv.org/abs/2403.02624v1","category":"cs.LG"}
{"created":"2024-03-05 03:23:55","title":"World Models for Autonomous Driving: An Initial Survey","abstract":"In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and comprehension of this burgeoning field, and inspiring continued innovation and exploration.","sentences":["In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process.","World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps.","This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations.","Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and comprehension of this burgeoning field, and inspiring continued innovation and exploration."],"url":"http://arxiv.org/abs/2403.02622v1","category":"cs.LG"}
{"created":"2024-03-05 03:18:43","title":"Training Machine Learning models at the Edge: A Survey","abstract":"Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, libraries, and simulation tools available for EL. In doing so, the paper contributes to a holistic understanding of the current landscape and future directions in the intersection of edge computing and machine learning, paving the way for informed comparisons between optimization methods and techniques designed for edge learning.","sentences":["Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge.","While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored.","This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge.","The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends.","Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL).","This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, libraries, and simulation tools available for EL.","In doing so, the paper contributes to a holistic understanding of the current landscape and future directions in the intersection of edge computing and machine learning, paving the way for informed comparisons between optimization methods and techniques designed for edge learning."],"url":"http://arxiv.org/abs/2403.02619v1","category":"cs.LG"}
{"created":"2024-03-05 03:11:02","title":"Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems","abstract":"Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS). However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals. To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension. Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation relationships within the system. Subsequently, based on these two types of state matrices, a three-branch structure of series-temporal-spatial attention module is designed to simultaneously capture the series, temporal, and space dependencies among MTS. Afterwards, three associated alignment loss functions and a reconstruction loss are constructed to jointly optimize the model. Finally, anomalies are determined and diagnosed by comparing the residual matrices with the original matrices. We conducted comparative experiments on five publicly datasets spanning three application domains (service monitoring, spatial and earth exploration, and water treatment), along with a petroleum refining simulation dataset collected by ourselves. The results demonstrate that MAD-Transformer can adaptively detect fine-grained anomalies with short duration, and outperforms the state-of-the-art baselines in terms of noise robustness and localization performance.","sentences":["Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS).","However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals.","To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS.","MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension.","Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation relationships within the system.","Subsequently, based on these two types of state matrices, a three-branch structure of series-temporal-spatial attention module is designed to simultaneously capture the series, temporal, and space dependencies among MTS.","Afterwards, three associated alignment loss functions and a reconstruction loss are constructed to jointly optimize the model.","Finally, anomalies are determined and diagnosed by comparing the residual matrices with the original matrices.","We conducted comparative experiments on five publicly datasets spanning three application domains (service monitoring, spatial and earth exploration, and water treatment), along with a petroleum refining simulation dataset collected by ourselves.","The results demonstrate that MAD-Transformer can adaptively detect fine-grained anomalies with short duration, and outperforms the state-of-the-art baselines in terms of noise robustness and localization performance."],"url":"http://arxiv.org/abs/2403.02616v1","category":"cs.LG"}
{"created":"2024-03-05 03:04:35","title":"Large Language Models and Video Games: A Preliminary Scoping Review","abstract":"Large language models (LLMs) hold interesting potential for the design, development, and research of video games. Building on the decades of prior research on generative AI in games, many researchers have sped to investigate the power and potential of LLMs for games. Given the recent spike in LLM-related research in games, there is already a wealth of relevant research to survey. In order to capture a snapshot of the state of LLM research in games, and to help lay the foundation for future work, we carried out an initial scoping review of relevant papers published so far. In this paper, we review 76 papers published between 2022 to early 2024 on LLMs and video games, with key focus areas in game AI, game development, narrative, and game research and reviews. Our paper provides an early state of the field and lays the groundwork for future research and reviews on this topic.","sentences":["Large language models (LLMs) hold interesting potential for the design, development, and research of video games.","Building on the decades of prior research on generative AI in games, many researchers have sped to investigate the power and potential of LLMs for games.","Given the recent spike in LLM-related research in games, there is already a wealth of relevant research to survey.","In order to capture a snapshot of the state of LLM research in games, and to help lay the foundation for future work, we carried out an initial scoping review of relevant papers published so far.","In this paper, we review 76 papers published between 2022 to early 2024 on LLMs and video games, with key focus areas in game AI, game development, narrative, and game research and reviews.","Our paper provides an early state of the field and lays the groundwork for future research and reviews on this topic."],"url":"http://arxiv.org/abs/2403.02613v1","category":"cs.HC"}
{"created":"2024-03-05 02:59:35","title":"A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning","abstract":"Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, improving deblur performance for labeled and unlabeled data. Extensive experiments and downstream task validation show the framework achieves state-of-the-art performance across multiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.","sentences":["Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery.","To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency.","The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context.","The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands.","It also enables deblur knowledge transfer to learn cross-domain information from extra data, improving deblur performance for labeled and unlabeled data.","Extensive experiments and downstream task validation show the framework achieves state-of-the-art performance across multiple datasets.","Project page: https://github.com/PieceZhang/MPT-CataBlur."],"url":"http://arxiv.org/abs/2403.02611v1","category":"cs.CV"}
{"created":"2024-03-05 02:58:57","title":"ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation","abstract":"This paper presents the second ChatGPT4PCG competition at the 2024 IEEE Conference on Games. In this edition of the competition, we follow the first edition, but make several improvements and changes. We introduce a new evaluation metric along with allowing a more flexible format for participants' submissions and making several improvements to the evaluation pipeline. Continuing from the first edition, we aim to foster and explore the realm of prompt engineering (PE) for procedural content generation (PCG). While the first competition saw success, it was hindered by various limitations; we aim to mitigate these limitations in this edition. We introduce diversity as a new metric to discourage submissions aimed at producing repetitive structures. Furthermore, we allow submission of a Python program instead of a prompt text file for greater flexibility in implementing advanced PE approaches, which may require control flow, including conditions and iterations. We also make several improvements to the evaluation pipeline with a better classifier for similarity evaluation and better-performing function signatures. We thoroughly evaluate the effectiveness of the new metric and the improved classifier. Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation. Finally, we provide implementation examples of various PE techniques in Python and evaluate their preliminary performance. We hope this competition serves as a resource and platform for learning about PE and PCG in general.","sentences":["This paper presents the second ChatGPT4PCG competition at the 2024 IEEE Conference on Games.","In this edition of the competition, we follow the first edition, but make several improvements and changes.","We introduce a new evaluation metric along with allowing a more flexible format for participants' submissions and making several improvements to the evaluation pipeline.","Continuing from the first edition, we aim to foster and explore the realm of prompt engineering (PE) for procedural content generation (PCG).","While the first competition saw success, it was hindered by various limitations; we aim to mitigate these limitations in this edition.","We introduce diversity as a new metric to discourage submissions aimed at producing repetitive structures.","Furthermore, we allow submission of a Python program instead of a prompt text file for greater flexibility in implementing advanced PE approaches, which may require control flow, including conditions and iterations.","We also make several improvements to the evaluation pipeline with a better classifier for similarity evaluation and better-performing function signatures.","We thoroughly evaluate the effectiveness of the new metric and the improved classifier.","Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation.","Finally, we provide implementation examples of various PE techniques in Python and evaluate their preliminary performance.","We hope this competition serves as a resource and platform for learning about PE and PCG in general."],"url":"http://arxiv.org/abs/2403.02610v1","category":"cs.AI"}
{"created":"2024-03-05 02:49:00","title":"DNNLasso: Scalable Graph Learning for Matrix-Variate Data","abstract":"We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices. Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of graphs. However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets. In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time. Our code is available at https://github.com/YangjingZhang/DNNLasso.","sentences":["We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices.","Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of graphs.","However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets.","In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time.","Our code is available at https://github.com/YangjingZhang/DNNLasso."],"url":"http://arxiv.org/abs/2403.02608v1","category":"cs.LG"}
{"created":"2024-03-05 02:44:58","title":"MEBS: Multi-task End-to-end Bid Shading for Multi-slot Display Advertising","abstract":"Online bidding and auction are crucial aspects of the online advertising industry. Conventionally, there is only one slot for ad display and most current studies focus on it. Nowadays, multi-slot display advertising is gradually becoming popular where many ads could be displayed in a list and shown as a whole to users. However, multi-slot display advertising leads to different cost-effectiveness. Advertisers have the incentive to adjust bid prices so as to win the most economical ad positions. In this study, we introduce bid shading into multi-slot display advertising for bid price adjustment with a Multi-task End-to-end Bid Shading(MEBS) method. We prove the optimality of our method theoretically and examine its performance experimentally. Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a 7.01% lift in Gross Merchandise Volume, a 7.42% lift in Return on Investment, and a 3.26% lift in ad buy count.","sentences":["Online bidding and auction are crucial aspects of the online advertising industry.","Conventionally, there is only one slot for ad display and most current studies focus on it.","Nowadays, multi-slot display advertising is gradually becoming popular where many ads could be displayed in a list and shown as a whole to users.","However, multi-slot display advertising leads to different cost-effectiveness.","Advertisers have the incentive to adjust bid prices so as to win the most economical ad positions.","In this study, we introduce bid shading into multi-slot display advertising for bid price adjustment with a Multi-task End-to-end Bid Shading(MEBS) method.","We prove the optimality of our method theoretically and examine its performance experimentally.","Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a 7.01% lift in Gross Merchandise Volume, a 7.42% lift in Return on Investment, and a 3.26% lift in ad buy count."],"url":"http://arxiv.org/abs/2403.02607v1","category":"cs.GT"}
{"created":"2024-03-05 02:04:14","title":"Measurement of $CP$ asymmetries in $B^0 \\rightarrow K^0_S K^0_S K^0_S$ decays at Belle II","abstract":"We report a measurement of decay-time dependent charge-parity ($CP$) asymmetries in $B^0 \\rightarrow K^0_S K^0_S K^0_S$ decays. We use $387 \\times 10^6 B\\bar{B}$ pairs collected at the $\\Upsilon(4S)$ resonance with the Belle II detector at the SuperKEKB asymmetric-energy electron-positron collider. We reconstruct 220 signal events and extract the $CP$-violating parameters $S$ and $C$ from a fit to the distribution of the decay-time difference between the two $B$ mesons. The resulting confidence region is consistent with previous measurements in $B^0 \\rightarrow K^0_S K^0_S K^0_S$ and $B^0 \\rightarrow (c\\bar{c})K^0$ decays, and with predictions based on the standard model.","sentences":["We report a measurement of decay-time dependent charge-parity ($CP$) asymmetries in $B^0 \\rightarrow K^0_S K^0_S K^0_S$ decays.","We use $387 \\times 10^6 B\\bar{B}$ pairs collected at the $\\Upsilon(4S)$ resonance with the Belle II detector at the SuperKEKB asymmetric-energy electron-positron collider.","We reconstruct 220 signal events and extract the $CP$-violating parameters $S$ and $C$ from a fit to the distribution of the decay-time difference between the two $B$ mesons.","The resulting confidence region is consistent with previous measurements in $B^0 \\rightarrow K^0_S K^0_S K^0_S$ and $B^0 \\rightarrow (c\\bar{c})K^0$ decays, and with predictions based on the standard model."],"url":"http://arxiv.org/abs/2403.02590v1","category":"hep-ex"}
{"created":"2024-03-05 02:02:00","title":"MUSIC: Accelerated Convergence for Distributed Optimization With Inexact and Exact Methods","abstract":"Gradient-type distributed optimization methods have blossomed into one of the most important tools for solving a minimization learning task over a networked agent system. However, only one gradient update per iteration is difficult to achieve a substantive acceleration of convergence. In this paper, we propose an accelerated framework named as MUSIC allowing each agent to perform multiple local updates and a single combination in each iteration. More importantly, we equip inexact and exact distributed optimization methods into this framework, thereby developing two new algorithms that exhibit accelerated linear convergence and high communication efficiency. Our rigorous convergence analysis reveals the sources of steady-state errors arising from inexact policies and offers effective solutions. Numerical results based on synthetic and real datasets demonstrate both our theoretical motivations and analysis, as well as performance advantages.","sentences":["Gradient-type distributed optimization methods have blossomed into one of the most important tools for solving a minimization learning task over a networked agent system.","However, only one gradient update per iteration is difficult to achieve a substantive acceleration of convergence.","In this paper, we propose an accelerated framework named as MUSIC allowing each agent to perform multiple local updates and a single combination in each iteration.","More importantly, we equip inexact and exact distributed optimization methods into this framework, thereby developing two new algorithms that exhibit accelerated linear convergence and high communication efficiency.","Our rigorous convergence analysis reveals the sources of steady-state errors arising from inexact policies and offers effective solutions.","Numerical results based on synthetic and real datasets demonstrate both our theoretical motivations and analysis, as well as performance advantages."],"url":"http://arxiv.org/abs/2403.02589v1","category":"math.OC"}
{"created":"2024-03-05 01:44:03","title":"A Direct Sampling Method and Its Integration with Deep Learning for Inverse Scattering Problems with Phaseless Data","abstract":"We consider in this work an inverse acoustic scattering problem when only phaseless data is available. The inverse problem is highly nonlinear and ill-posed due to the lack of the phase information. Solving inverse scattering problems with phaseless data is important in applications as the collection of physically acceptable phased data is usually difficult and expensive. A novel direct sampling method (DSM) will be developed to effectively estimate the locations and geometric shapes of the unknown scatterers from phaseless data generated by a very limited number of incident waves. With a careful theoretical analysis of the behavior of the index function and some representative numerical examples, the new DSM is shown to be computationally efficient, easy to implement, robust to large noise, and does not require any prior knowledge of the unknown scatterers. Furthermore, to fully exploit the index functions obtained from the DSM, we also propose to integrate the DSM with a deep learning technique (DSM-DL) to achieve high-quality reconstructions. Several challenging and representative numerical experiments are carried out to demonstrate the accuracy and robustness of reconstructions by DSM-DL. The DSM-DL networks trained by phased data are further theoretically and numerically shown to be able to solve problems with phaseless data. Additionally, our numerical experiments also show the DSM-DL can solve inverse scattering problems with mixed types of scatterers, which renders its applications in many important practical scenarios.","sentences":["We consider in this work an inverse acoustic scattering problem when only phaseless data is available.","The inverse problem is highly nonlinear and ill-posed due to the lack of the phase information.","Solving inverse scattering problems with phaseless data is important in applications as the collection of physically acceptable phased data is usually difficult and expensive.","A novel direct sampling method (DSM) will be developed to effectively estimate the locations and geometric shapes of the unknown scatterers from phaseless data generated by a very limited number of incident waves.","With a careful theoretical analysis of the behavior of the index function and some representative numerical examples, the new DSM is shown to be computationally efficient, easy to implement, robust to large noise, and does not require any prior knowledge of the unknown scatterers.","Furthermore, to fully exploit the index functions obtained from the DSM, we also propose to integrate the DSM with a deep learning technique (DSM-DL) to achieve high-quality reconstructions.","Several challenging and representative numerical experiments are carried out to demonstrate the accuracy and robustness of reconstructions by DSM-DL.","The DSM-DL networks trained by phased data are further theoretically and numerically shown to be able to solve problems with phaseless data.","Additionally, our numerical experiments also show the DSM-DL can solve inverse scattering problems with mixed types of scatterers, which renders its applications in many important practical scenarios."],"url":"http://arxiv.org/abs/2403.02584v1","category":"math.NA"}
{"created":"2024-03-05 01:30:23","title":"Canonical aspects of pregeometric vector-based first order gauge theory","abstract":"A recently proposed pregeometric auxiliary vector mediated gauge theory is studied in its canonical domain, by performing the Legendre transform on a curved background and by considering its covariant phase space. This uncovers several curious properties and problems, even if the viability of the theory itself is questionable at best. The constraints become differential equations, although the Dirac-Bergmann algorithm appears consistent with electromagnetic degrees of freedom, background permitting. Solving the consistency conditions provides what appears to be spontaneous Lorentz symmetry breaking to obtain a secondary constraint. In parallel, the covariant phase space defines the symplectic structure, immediately relevant for quantization. Conserved currents naturally follow: some details are collected and elaborated. Properties of degenerate metric geometry are discussed throughout. The full phenomenology of the vector theory, classical and quantum, is established to be that of gauge theory on a classical electromagnetic background. This leads into the fundamental question of background independence in all physical theories.","sentences":["A recently proposed pregeometric auxiliary vector mediated gauge theory is studied in its canonical domain, by performing the Legendre transform on a curved background and by considering its covariant phase space.","This uncovers several curious properties and problems, even if the viability of the theory itself is questionable at best.","The constraints become differential equations, although the Dirac-Bergmann algorithm appears consistent with electromagnetic degrees of freedom, background permitting.","Solving the consistency conditions provides what appears to be spontaneous Lorentz symmetry breaking to obtain a secondary constraint.","In parallel, the covariant phase space defines the symplectic structure, immediately relevant for quantization.","Conserved currents naturally follow: some details are collected and elaborated.","Properties of degenerate metric geometry are discussed throughout.","The full phenomenology of the vector theory, classical and quantum, is established to be that of gauge theory on a classical electromagnetic background.","This leads into the fundamental question of background independence in all physical theories."],"url":"http://arxiv.org/abs/2403.02578v1","category":"hep-th"}
{"created":"2024-03-05 01:13:56","title":"ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary","abstract":"The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.","sentences":["The literature review is an indispensable step in the research process.","It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works.","However, literature summary is challenging and time consuming.","The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization.","However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary.","In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary.","This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism.","In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria.","The ChatCite agent outperformed other models in various dimensions in the experiments.","The literature summaries generated by ChatCite can also be directly used for drafting literature reviews."],"url":"http://arxiv.org/abs/2403.02574v1","category":"cs.IR"}
{"created":"2024-03-05 00:52:01","title":"Dynamics of magnetic self-propelled particles in a harmonic trap","abstract":"Artificial active particles, exemplified by Hexbugs (HB), serve as valuable tools for investigating the intricate dynamics of active matter systems. Leveraging their stochastic motion, Hexbugs provides an excellent experimental model. In this study, we utilize Hexbugs equipped with disk-like armor and embedded magnetic dipoles, transforming them into Magnetic Self-Propelled Particles (MSPP). We explore the emergence of collective and stationary states numerically and experimentally by confining these MSPPs within a parabolic domain acting as a harmonic potential. Our findings unveil a diverse range of metastable configurations intricately linked to the complex dynamics inherent in the system. We discern that particle number, activity, and the balance between magnetic and harmonic potential strengths predominantly influence the metastability of these structures. By employing these parameters as control factors, we compare and contrast the behavior of MSPPs with disk-like magnetic Active Brownian Particles (ABPs) in the overdamped limit of vanishing inertia. Our numerical predictions reproduce most of the experimental observations, highlighting the crucial role of magnetic dipole interactions in developing novel configurations for active particles within parabolic domains. These configurations include chains, clusters, and vortex formations characterized by a specific pattern in the particle spatial distribution. Notably, we observe that the influence of inertia is not fundamental in generating metastable configurations in these confined systems. Instead, the particle's shape, activity, and orientation are dominant factors. This comparative analysis provides insights into the distinctive features and dynamics of MSPP within confined environments, shedding light on the role of short-range polar interactions.","sentences":["Artificial active particles, exemplified by Hexbugs (HB), serve as valuable tools for investigating the intricate dynamics of active matter systems.","Leveraging their stochastic motion, Hexbugs provides an excellent experimental model.","In this study, we utilize Hexbugs equipped with disk-like armor and embedded magnetic dipoles, transforming them into Magnetic Self-Propelled Particles (MSPP).","We explore the emergence of collective and stationary states numerically and experimentally by confining these MSPPs within a parabolic domain acting as a harmonic potential.","Our findings unveil a diverse range of metastable configurations intricately linked to the complex dynamics inherent in the system.","We discern that particle number, activity, and the balance between magnetic and harmonic potential strengths predominantly influence the metastability of these structures.","By employing these parameters as control factors, we compare and contrast the behavior of MSPPs with disk-like magnetic Active Brownian Particles (ABPs) in the overdamped limit of vanishing inertia.","Our numerical predictions reproduce most of the experimental observations, highlighting the crucial role of magnetic dipole interactions in developing novel configurations for active particles within parabolic domains.","These configurations include chains, clusters, and vortex formations characterized by a specific pattern in the particle spatial distribution.","Notably, we observe that the influence of inertia is not fundamental in generating metastable configurations in these confined systems.","Instead, the particle's shape, activity, and orientation are dominant factors.","This comparative analysis provides insights into the distinctive features and dynamics of MSPP within confined environments, shedding light on the role of short-range polar interactions."],"url":"http://arxiv.org/abs/2403.02569v1","category":"cond-mat.soft"}
{"created":"2024-03-05 00:48:56","title":"Eliciting Better Multilingual Structured Reasoning from LLMs through Code","abstract":"Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks. We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus showing our techniques maintain general-purpose abilities.","sentences":["Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks.","We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages.","xSTREET","exposes a gap in base LLM performance between English and non-English reasoning tasks.","We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners.","First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is.","Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution.","Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask.","Furthermore, the models show no regression on non-reasoning tasks, thus showing our techniques maintain general-purpose abilities."],"url":"http://arxiv.org/abs/2403.02567v1","category":"cs.CL"}
{"created":"2024-03-05 00:43:31","title":"Deep Cooperation in ISAC System: Resource, Node and Infrastructure Perspectives","abstract":"With the mobile communication system evolving into 6th-generation (6G), the Internet of Everything (IoE) is becoming reality, which connects human, big data and intelligent machines to support the intelligent decision making, reconfiguring the traditional industries and human life. The applications of IoE require not only pure communication capability, but also high-accuracy and large-scale sensing capability. With the emerging integrated sensing and communication (ISAC) technique, exploiting the mobile communication system with multi-domain resources, multiple network elements, and large-scale infrastructures to realize cooperative sensing is a crucial approach to satisfy the requirements of high-accuracy and large-scale sensing in IoE. In this article, the deep cooperation in ISAC system including three perspectives is investigated. In the microscopic perspective, namely, within a single node, the cooperation at the resource-level is performed to improve sensing accuracy by fusing the sensing information carried in the time-frequency-space-code multi-domain resources. In the mesoscopic perspective, the sensing accuracy could be improved through the cooperation of multiple nodes including Base Station (BS), User Equipment (UE), and Reconfigurable Intelligence Surface (RIS), etc. In the macroscopic perspective, the massive number of infrastructures from the same operator or different operators could perform cooperative sensing to extend the sensing coverage and improve the sensing continuity. This article may provide a deep and comprehensive view on the cooperative sensing in ISAC system to enhance the performance of sensing, supporting the applications of IoE.","sentences":["With the mobile communication system evolving into 6th-generation (6G), the Internet of Everything (IoE) is becoming reality, which connects human, big data and intelligent machines to support the intelligent decision making, reconfiguring the traditional industries and human life.","The applications of IoE require not only pure communication capability, but also high-accuracy and large-scale sensing capability.","With the emerging integrated sensing and communication (ISAC) technique, exploiting the mobile communication system with multi-domain resources, multiple network elements, and large-scale infrastructures to realize cooperative sensing is a crucial approach to satisfy the requirements of high-accuracy and large-scale sensing in IoE.","In this article, the deep cooperation in ISAC system including three perspectives is investigated.","In the microscopic perspective, namely, within a single node, the cooperation at the resource-level is performed to improve sensing accuracy by fusing the sensing information carried in the time-frequency-space-code multi-domain resources.","In the mesoscopic perspective, the sensing accuracy could be improved through the cooperation of multiple nodes including Base Station (BS), User Equipment (UE), and Reconfigurable Intelligence Surface (RIS), etc.","In the macroscopic perspective, the massive number of infrastructures from the same operator or different operators could perform cooperative sensing to extend the sensing coverage and improve the sensing continuity.","This article may provide a deep and comprehensive view on the cooperative sensing in ISAC system to enhance the performance of sensing, supporting the applications of IoE."],"url":"http://arxiv.org/abs/2403.02565v1","category":"eess.SP"}
{"created":"2024-03-05 00:27:43","title":"Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research","abstract":"Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the \"Minimum information about clinical artificial intelligence modeling\" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards.","sentences":["Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed.","While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks.","In particular, the ability of these models to produce useful outputs with little to no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models.","In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the \"Minimum information about clinical artificial intelligence modeling\" (MI-CLAIM) checklist.","The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine.","Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research.","This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards."],"url":"http://arxiv.org/abs/2403.02558v1","category":"cs.CL"}
{"created":"2024-03-05 00:02:54","title":"Femtoscopy analysis of ultra-soft pion trap at LHC energies","abstract":"Femtoscopy studies of pion radiation in heavy-ion collisions have been conducted extensively at all available collider energies, both theoretically and experimentally. In all these studies a special interest is given to $m_{T}$ dependency of pion femtoscopy radii, usually approximated by a power-law function at transverse momenta above 200~MeV/$c$. However, the radii behaviour has been much less explored for the ultra-soft pions, possessing the transverse momentum comparable to or lower than the pion mass. For many experimental setups this region is difficult to measure. In this work we present theoretical calculations of pion emission in the ultra-soft region in the two hybrid models -- iHKM and LHYQUID+THERMINATOR2. Along with the particle transverse momentum spectra, we present the calculated femtoscopy radii, both in one-dimensional and three-dimensional representations. We investigate the radii dependence on pair $m_{T}$ and observe, in particular, a departure from the power-law behaviour at ultra-soft momenta, potentially reflecting a decoupling of such slow pions from the rest of collectively expanding system. We provide the theoretical interpretation of this result and discuss its significance, in particular, for the ongoing non-identical particle femtoscopy analysis for pairs consisting of a pion and a baryon (or of a pion and a charmed meson).","sentences":["Femtoscopy studies of pion radiation in heavy-ion collisions have been conducted extensively at all available collider energies, both theoretically and experimentally.","In all these studies a special interest is given to $m_{T}$ dependency of pion femtoscopy radii, usually approximated by a power-law function at transverse momenta above 200~MeV/$c$. However, the radii behaviour has been much less explored for the ultra-soft pions, possessing the transverse momentum comparable to or lower than the pion mass.","For many experimental setups this region is difficult to measure.","In this work we present theoretical calculations of pion emission in the ultra-soft region in the two hybrid models -- iHKM and LHYQUID+THERMINATOR2.","Along with the particle transverse momentum spectra, we present the calculated femtoscopy radii, both in one-dimensional and three-dimensional representations.","We investigate the radii dependence on pair $m_{T}$ and observe, in particular, a departure from the power-law behaviour at ultra-soft momenta, potentially reflecting a decoupling of such slow pions from the rest of collectively expanding system.","We provide the theoretical interpretation of this result and discuss its significance, in particular, for the ongoing non-identical particle femtoscopy analysis for pairs consisting of a pion and a baryon (or of a pion and a charmed meson)."],"url":"http://arxiv.org/abs/2403.02551v1","category":"hep-ph"}
{"created":"2024-03-04 23:40:20","title":"Wukong: Towards a Scaling Law for Large-Scale Recommendation","abstract":"Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wukong's scalability on an internal, large-scale dataset. The results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the scaling law across two orders of magnitude in model complexity, extending beyond 100 Gflop or equivalently up to GPT-3/LLaMa-2 scale of total training compute, where prior arts fall short.","sentences":["Scaling laws play an instrumental role in the sustainable improvement in model quality.","Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms.","This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets.","In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation.","Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers.","We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise.","Further, we assessed Wukong's scalability on an internal, large-scale dataset.","The results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the scaling law across two orders of magnitude in model complexity, extending beyond 100 Gflop or equivalently up to GPT-3/LLaMa-2 scale of total training compute, where prior arts fall short."],"url":"http://arxiv.org/abs/2403.02545v1","category":"cs.LG"}
{"created":"2024-03-04 22:47:58","title":"DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation","abstract":"Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities. To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm. Data and code are released at https://github.com/shirley-wu/daco","sentences":["Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data.","In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task.","However, collecting data analysis annotations curated by experts can be prohibitively expensive.","We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique.","We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark.","We train a 6B supervised fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns reasonable data analysis capabilities.","To further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps.","Our DACO-RL algorithm is evaluated by human annotators to produce more helpful answers than SFT model in 57.72% cases, validating the effectiveness of our proposed algorithm.","Data and code are released at https://github.com/shirley-wu/daco"],"url":"http://arxiv.org/abs/2403.02528v1","category":"cs.CL"}
{"created":"2024-03-04 22:36:12","title":"A Tunable Reflection Surface with Independently Variable Phase and Slope","abstract":"A reconfigurable intelligent surface (RIS) is an essential component in the architecture of the next generation of wireless communication systems. An RIS is deployed to provide a controllability to the multi-path environment between the transmitter and the receiver, which becomes critical when the line-of-sight signal between them is blocked. In this work, we design an electrically tunable linearly polarized RIS at 2.5 GHz that yields a controllable reflection phase and phase-frequency slope; in other words, we add tunability of the phase-frequency slope to the tunability of the resonance center frequency. The proposed design consists of two layers of unit cells placed over a ground plane, with dog-bone-shaped elements in the top layer and patch elements in the bottom layer. Each patch and dog-bone element is loaded with a varactor, whose reverse bias voltage is controlled to provide a phase-frequency profile with a slope value of 9 degrees/MHz or 0.95 degrees/MHz, and a phase shift range of 320 degrees.","sentences":["A reconfigurable intelligent surface (RIS) is an essential component in the architecture of the next generation of wireless communication systems.","An RIS is deployed to provide a controllability to the multi-path environment between the transmitter and the receiver, which becomes critical when the line-of-sight signal between them is blocked.","In this work, we design an electrically tunable linearly polarized RIS at 2.5 GHz that yields a controllable reflection phase and phase-frequency slope; in other words, we add tunability of the phase-frequency slope to the tunability of the resonance center frequency.","The proposed design consists of two layers of unit cells placed over a ground plane, with dog-bone-shaped elements in the top layer and patch elements in the bottom layer.","Each patch and dog-bone element is loaded with a varactor, whose reverse bias voltage is controlled to provide a phase-frequency profile with a slope value of 9 degrees/MHz or 0.95 degrees/MHz, and a phase shift range of 320 degrees."],"url":"http://arxiv.org/abs/2403.02526v1","category":"eess.SP"}
{"created":"2024-03-04 22:27:11","title":"Transformer for Times Series: an Application to the S&P500","abstract":"The transformer models have been extensively used with good results in a wide area of machine learning applications including Large Language Models and image generation. Here, we inquire on the applicability of this approach to financial time series. We first describe the dataset construction for two prototypical situations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand and real S&P500 data on the other hand. Then, we present in detail the proposed Transformer architecture and finally we discuss some encouraging results. For the synthetic data we predict rather accurately the next move, and for the S&P500 we get some interesting results related to quadratic variation and volatility prediction.","sentences":["The transformer models have been extensively used with good results in a wide area of machine learning applications including Large Language Models and image generation.","Here, we inquire on the applicability of this approach to financial time series.","We first describe the dataset construction for two prototypical situations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand and real S&P500 data on the other hand.","Then, we present in detail the proposed Transformer architecture and finally we discuss some encouraging results.","For the synthetic data we predict rather accurately the next move, and for the S&P500 we get some interesting results related to quadratic variation and volatility prediction."],"url":"http://arxiv.org/abs/2403.02523v1","category":"cs.AI"}
{"created":"2024-03-04 22:26:25","title":"HeAR -- Health Acoustic Representations","abstract":"Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and disease, yet are underexplored in the medical machine learning community. The existing deep learning systems for health acoustics are often narrowly trained and evaluated on a single task, which is limited by data and may hinder generalization to other tasks. To mitigate these gaps, we develop HeAR, a scalable self-supervised learning-based deep learning system using masked autoencoders trained on a large dataset of 313 million two-second long audio clips. Through linear probes, we establish HeAR as a state-of-the-art health audio embedding model on a benchmark of 33 health acoustic tasks across 6 datasets. By introducing this work, we hope to enable and accelerate further health acoustics research.","sentences":["Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and disease, yet are underexplored in the medical machine learning community.","The existing deep learning systems for health acoustics are often narrowly trained and evaluated on a single task, which is limited by data and may hinder generalization to other tasks.","To mitigate these gaps, we develop HeAR, a scalable self-supervised learning-based deep learning system using masked autoencoders trained on a large dataset of 313 million two-second long audio clips.","Through linear probes, we establish HeAR as a state-of-the-art health audio embedding model on a benchmark of 33 health acoustic tasks across 6 datasets.","By introducing this work, we hope to enable and accelerate further health acoustics research."],"url":"http://arxiv.org/abs/2403.02522v1","category":"cs.LG"}
{"created":"2024-03-04 22:03:49","title":"Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation","abstract":"Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a computational framework on purpose in two ways. First, it formalises a framework on purpose based on a three-level motivational hierarchy involving: (a) the purposes; (b) the desires, which are domain independent; (c) specific domain dependent state-goals. Second, the work highlights key challenges highlighted by the framework such as: the `purpose-desire alignment problem', the `purpose-goal grounding problem', and the `arbitration between desires'. Overall, the approach enables OEL robots to learn in an autonomous way but also to focus on acquiring goals and skills that meet the purposes of the designers and users.","sentences":["Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals.","OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users.","OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks.","This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'.","Purposes indicate what the designers and/or users want from the robot.","The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them.","This work contributes to develop a computational framework on purpose in two ways.","First, it formalises a framework on purpose based on a three-level motivational hierarchy involving: (a) the purposes; (b) the desires, which are domain independent; (c) specific domain dependent state-goals.","Second, the work highlights key challenges highlighted by the framework such as: the `purpose-desire alignment problem', the `purpose-goal grounding problem', and the `arbitration between desires'.","Overall, the approach enables OEL robots to learn in an autonomous way but also to focus on acquiring goals and skills that meet the purposes of the designers and users."],"url":"http://arxiv.org/abs/2403.02514v1","category":"cs.RO"}
{"created":"2024-03-04 22:01:03","title":"Hybrid quantum programming with PennyLane Lightning on HPC platforms","abstract":"We introduce PennyLane's Lightning suite, a collection of high-performance state-vector simulators targeting CPU, GPU, and HPC-native architectures and workloads. Quantum applications such as QAOA, VQE, and synthetic workloads are implemented to demonstrate the supported classical computing architectures and showcase the scale of problems that can be simulated using our tooling. We benchmark the performance of Lightning with backends supporting CPUs, as well as NVidia and AMD GPUs, and compare the results to other commonly used high-performance simulator packages, demonstrating where Lightning's implementations give performance leads. We show improved CPU performance by employing explicit SIMD intrinsics and multi-threading, batched task-based execution across multiple GPUs, and distributed forward and gradient-based quantum circuit executions across multiple nodes. Our data shows we can comfortably simulate a variety of circuits, giving examples with up to 30 qubits on a single device or node, and up to 41 qubits using multiple nodes.","sentences":["We introduce PennyLane's Lightning suite, a collection of high-performance state-vector simulators targeting CPU, GPU, and HPC-native architectures and workloads.","Quantum applications such as QAOA, VQE, and synthetic workloads are implemented to demonstrate the supported classical computing architectures and showcase the scale of problems that can be simulated using our tooling.","We benchmark the performance of Lightning with backends supporting CPUs, as well as NVidia and AMD GPUs, and compare the results to other commonly used high-performance simulator packages, demonstrating where Lightning's implementations give performance leads.","We show improved CPU performance by employing explicit SIMD intrinsics and multi-threading, batched task-based execution across multiple GPUs, and distributed forward and gradient-based quantum circuit executions across multiple nodes.","Our data shows we can comfortably simulate a variety of circuits, giving examples with up to 30 qubits on a single device or node, and up to 41 qubits using multiple nodes."],"url":"http://arxiv.org/abs/2403.02512v1","category":"quant-ph"}
{"created":"2024-03-04 21:55:22","title":"SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models","abstract":"In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50\\% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.","sentences":["In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities.","However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs.","While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored.","Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties.","The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks.","Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques.","Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50\\% on average.","Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs."],"url":"http://arxiv.org/abs/2403.02509v1","category":"cs.CL"}
{"created":"2024-03-04 21:51:11","title":"A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing","abstract":"The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.","sentences":["The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP).","This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data.","This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited.","Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm.","We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications.","We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression.","Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm.","To this end, we have provided open access to all our code and datasets.","The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach."],"url":"http://arxiv.org/abs/2403.02504v1","category":"cs.CL"}
{"created":"2024-03-04 21:50:29","title":"Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents","abstract":"Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.","sentences":["Large Language Models (LLMs) have become integral components in various autonomous agent systems.","In this study, we present an exploration-based trajectory optimization approach, referred to as ETO.","This learning method is designed to enhance the performance of open LLM agents.","Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures.","This leads to improved performance through an iterative optimization framework.","During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs.","In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO.","This iterative cycle of exploration and training fosters continued improvement in the agents.","Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin.","Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.02502v1","category":"cs.CL"}
{"created":"2024-03-04 21:44:43","title":"Magnetic Localization for In-body Nano-communication Medical Systems","abstract":"Nano-machines circulating inside the human body, collecting data on tissue conditions, represent a vital part of next-generation medical diagnostic systems. However, for these devices to operate effectively, they need to relay not only their medical measurements but also their positions. This paper introduces a novel localization method for in-body nano-machines based on the magnetic field, leveraging the advantageous magnetic permeability of all human tissues. The entire proposed localization system is described, starting from 10x10 ${\\mu}m^2$ magnetometers to be integrated into the nano-machines, to a set of external wires generating the magnetic field. Mathematical equations for the localization algorithm are also provided, assuming the nano-machines do not execute the computations themselves, but transmit their magnetic field measurements together with medical data outside of the body. The whole system is validated with computer simulations that capture the measurement error of the magnetometers, the error induced by the Earth magnetic field, and a human body model assuming different possible positions of nano-machines. The results show a very high system accuracy with localization errors even below 1 cm.","sentences":["Nano-machines circulating inside the human body, collecting data on tissue conditions, represent a vital part of next-generation medical diagnostic systems.","However, for these devices to operate effectively, they need to relay not only their medical measurements but also their positions.","This paper introduces a novel localization method for in-body nano-machines based on the magnetic field, leveraging the advantageous magnetic permeability of all human tissues.","The entire proposed localization system is described, starting from 10x10 ${\\mu}m^2$ magnetometers to be integrated into the nano-machines, to a set of external wires generating the magnetic field.","Mathematical equations for the localization algorithm are also provided, assuming the nano-machines do not execute the computations themselves, but transmit their magnetic field measurements together with medical data outside of the body.","The whole system is validated with computer simulations that capture the measurement error of the magnetometers, the error induced by the Earth magnetic field, and a human body model assuming different possible positions of nano-machines.","The results show a very high system accuracy with localization errors even below 1 cm."],"url":"http://arxiv.org/abs/2403.02497v1","category":"cs.IR"}
{"created":"2024-03-04 21:41:27","title":"Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking","abstract":"The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method. We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper. Video: https://youtu.be/OAro5pg8I9U","sentences":["The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios.","These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors.","In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning.","By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning.","In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method.","We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper.","Video: https://youtu.be/OAro5pg8I9U"],"url":"http://arxiv.org/abs/2403.02495v1","category":"cs.RO"}
{"created":"2024-03-04 21:05:52","title":"Encodings for Prediction-based Neural Architecture Search","abstract":"Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \\textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present our predictor \\textbf{FLAN}: \\textbf{Fl}ow \\textbf{A}ttention for \\textbf{N}AS. FLAN integrates critical insights on predictor design, transfer learning, and \\textit{unified encodings} to enable more than an order of magnitude cost reduction for training NAS accuracy predictors. Our implementation and encodings for all neural networks are open-sourced at \\href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\\_nas}.","sentences":["Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization.","The efficacy of these predictors is largely influenced by the method of encoding neural network architectures.","While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies.","In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based.","Furthermore, we extend these encodings and introduce \\textit{unified encodings}, that extend NAS predictors to multiple search spaces.","Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101.","Building on our study, we present our predictor \\textbf{FLAN}: \\textbf{Fl}ow \\textbf{A}ttention for \\textbf{N}AS.","FLAN integrates critical insights on predictor design, transfer learning, and \\textit{unified encodings} to enable more than an order of magnitude cost reduction for training NAS accuracy predictors.","Our implementation and encodings for all neural networks are open-sourced at \\href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\\_nas}."],"url":"http://arxiv.org/abs/2403.02484v1","category":"cs.LG"}
{"created":"2024-03-04 21:04:54","title":"MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify","abstract":"In multicriteria decision-making, a user seeks a set of non-dominated solutions to a (constrained) multiobjective optimization problem, the so-called Pareto frontier. In this work, we seek to bring a state-of-the-art method for exact multiobjective integer linear programming into the heuristic realm. We focus on binary decision diagrams (BDDs) which first construct a graph that represents all feasible solutions to the problem and then traverse the graph to extract the Pareto frontier. Because the Pareto frontier may be exponentially large, enumerating it over the BDD can be time-consuming. We explore how restricted BDDs, which have already been shown to be effective as heuristics for single-objective problems, can be adapted to multiobjective optimization through the use of machine learning (ML). MORBDD, our ML-based BDD sparsifier, first trains a binary classifier to eliminate BDD nodes that are unlikely to contribute to Pareto solutions, then post-processes the sparse BDD to ensure its connectivity via optimization. Experimental results on multiobjective knapsack problems show that MORBDD is highly effective at producing very small restricted BDDs with excellent approximation quality, outperforming width-limited restricted BDDs and the well-known evolutionary algorithm NSGA-II.","sentences":["In multicriteria decision-making, a user seeks a set of non-dominated solutions to a (constrained) multiobjective optimization problem, the so-called Pareto frontier.","In this work, we seek to bring a state-of-the-art method for exact multiobjective integer linear programming into the heuristic realm.","We focus on binary decision diagrams (BDDs) which first construct a graph that represents all feasible solutions to the problem and then traverse the graph to extract the Pareto frontier.","Because the Pareto frontier may be exponentially large, enumerating it over the BDD can be time-consuming.","We explore how restricted BDDs, which have already been shown to be effective as heuristics for single-objective problems, can be adapted to multiobjective optimization through the use of machine learning (ML).","MORBDD, our ML-based BDD sparsifier, first trains a binary classifier to eliminate BDD nodes that are unlikely to contribute to Pareto solutions, then post-processes the sparse BDD to ensure its connectivity via optimization.","Experimental results on multiobjective knapsack problems show that MORBDD is highly effective at producing very small restricted BDDs with excellent approximation quality, outperforming width-limited restricted BDDs and the well-known evolutionary algorithm NSGA-II."],"url":"http://arxiv.org/abs/2403.02482v1","category":"cs.AI"}
{"created":"2024-03-04 20:34:58","title":"OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering","abstract":"The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in generating offensive texts using ChatGPT due to ethical constraints, we present a prompt-based approach that effectively generates implicit offensive languages. To ensure data quality, we evaluate our data with human. Additionally, we employ a prompt-based Zero-Shot method with ChatGPT and compare the detection results between human annotation and ChatGPT annotation. We utilize existing state-of-the-art models to see how effective they are in detecting such languages. We will make our code and dataset public for other researchers.","sentences":["The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being.","As a result, it has become very important to address this issue with high priority.","Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect.","Current research in this domain encounters several challenges.","Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords.","Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide.","In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups.","Despite limitations in generating offensive texts using ChatGPT due to ethical constraints, we present a prompt-based approach that effectively generates implicit offensive languages.","To ensure data quality, we evaluate our data with human.","Additionally, we employ a prompt-based Zero-Shot method with ChatGPT and compare the detection results between human annotation and ChatGPT annotation.","We utilize existing state-of-the-art models to see how effective they are in detecting such languages.","We will make our code and dataset public for other researchers."],"url":"http://arxiv.org/abs/2403.02472v1","category":"cs.CL"}
{"created":"2024-03-04 20:14:38","title":"The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer","abstract":"This paper studies how large language models (LLMs) can act as effective, high-level creative collaborators and ``muses'' for game design. We model the design of this study after the exercises artists use by looking at amorphous ink splotches for creative inspiration. Our goal is to determine whether AI-assistance can improve, hinder, or provide an alternative quality to games when compared to the creative intents implemented by human designers. The capabilities of LLMs as game designers are stress tested by placing it at the forefront of the decision making process. Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from prompted outputs of the LLM, ChatGPT. A user study was conducted and participants were asked to blindly evaluate the quality and their preference of these games. We discuss both the development process of communicating creative intent to an AI chatbot and the synthesized open feedback of the participants. We use this data to determine both the benefits and shortcomings of AI in a more design-centric role.","sentences":["This paper studies how large language models (LLMs) can act as effective, high-level creative collaborators and ``muses'' for game design.","We model the design of this study after the exercises artists use by looking at amorphous ink splotches for creative inspiration.","Our goal is to determine whether AI-assistance can improve, hinder, or provide an alternative quality to games when compared to the creative intents implemented by human designers.","The capabilities of LLMs as game designers are stress tested by placing it at the forefront of the decision making process.","Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from prompted outputs of the LLM, ChatGPT.","A user study was conducted and participants were asked to blindly evaluate the quality and their preference of these games.","We discuss both the development process of communicating creative intent to an AI chatbot and the synthesized open feedback of the participants.","We use this data to determine both the benefits and shortcomings of AI in a more design-centric role."],"url":"http://arxiv.org/abs/2403.02454v1","category":"cs.AI"}
{"created":"2024-03-04 20:02:38","title":"Soft quasi-Newton: Guaranteed positive definiteness by relaxing the secant constraint","abstract":"We propose a novel algorithm, termed soft quasi-Newton (soft QN), for optimization in the presence of bounded noise. Traditional quasi-Newton algorithms are vulnerable to such perturbations. To develop a more robust quasi-Newton method, we replace the secant condition in the matrix optimization problem for the Hessian update with a penalty term in its objective and derive a closed-form update formula. A key feature of our approach is its ability to maintain positive definiteness of the Hessian inverse approximation. Furthermore, we establish the following properties of soft QN: it recovers the BFGS method under specific limits, it treats positive and negative curvature equally, and it is scale invariant. Collectively, these features enhance the efficacy of soft QN in noisy environments. For strongly convex objective functions and Hessian approximations obtained using soft QN, we develop an algorithm that exhibits linear convergence toward a neighborhood of the optimal solution, even if gradient and function evaluations are subject to bounded perturbations. Through numerical experiments, we demonstrate superior performance of soft QN compared to state-of-the-art methods in various scenarios.","sentences":["We propose a novel algorithm, termed soft quasi-Newton (soft QN), for optimization in the presence of bounded noise.","Traditional quasi-Newton algorithms are vulnerable to such perturbations.","To develop a more robust quasi-Newton method, we replace the secant condition in the matrix optimization problem for the Hessian update with a penalty term in its objective and derive a closed-form update formula.","A key feature of our approach is its ability to maintain positive definiteness of the Hessian inverse approximation.","Furthermore, we establish the following properties of soft QN: it recovers the BFGS method under specific limits, it treats positive and negative curvature equally, and it is scale invariant.","Collectively, these features enhance the efficacy of soft QN in noisy environments.","For strongly convex objective functions and Hessian approximations obtained using soft QN, we develop an algorithm that exhibits linear convergence toward a neighborhood of the optimal solution, even if gradient and function evaluations are subject to bounded perturbations.","Through numerical experiments, we demonstrate superior performance of soft QN compared to state-of-the-art methods in various scenarios."],"url":"http://arxiv.org/abs/2403.02448v1","category":"math.OC"}
{"created":"2024-03-04 19:58:04","title":"Free Proxies Unmasked: A Vulnerability and Longitudinal Analysis of Free Proxy Services","abstract":"Free-proxies have been widespread since the early days of the Web, helping users bypass geo-blocked content and conceal their IP addresses. Various proxy providers promise faster Internet or increased privacy while advertising their lists comprised of hundreds of readily available free proxies. However, while paid proxy services advertise the support of encrypted connections and high stability, free proxies often lack such guarantees, making them prone to malicious activities such as eavesdropping or modifying content. Furthermore, there is a market that encourages exploiting devices to install proxies.   In this paper, we present a 30-month longitudinal study analyzing the stability, security, and potential manipulation of free web proxies that we collected from 11 providers. Our collection resulted in over 640,600 proxies, that we cumulatively tested daily. We find that only 34.5% of proxies were active at least once during our tests, showcasing the general instability of free proxies. Geographically, a majority of proxies originate from the US and China. Leveraging the Shodan search engine, we identified 4,452 distinct vulnerabilities on the proxies' IP addresses, including 1,755 vulnerabilities that allow unauthorized remote code execution and 2,036 that enable privilege escalation on the host device. Through the software analysis on the proxies' IP addresses, we find that 42,206 of them appear to run on MikroTik routers. Worryingly, we also discovered 16,923 proxies that manipulate content, indicating potential malicious intent by proxy owners. Ultimately, our research reveals that the use of free web proxies poses significant risks to users' privacy and security. The instability, vulnerabilities, and potential for malicious actions uncovered in our analysis lead us to strongly caution users against relying on free proxies.","sentences":["Free-proxies have been widespread since the early days of the Web, helping users bypass geo-blocked content and conceal their IP addresses.","Various proxy providers promise faster Internet or increased privacy while advertising their lists comprised of hundreds of readily available free proxies.","However, while paid proxy services advertise the support of encrypted connections and high stability, free proxies often lack such guarantees, making them prone to malicious activities such as eavesdropping or modifying content.","Furthermore, there is a market that encourages exploiting devices to install proxies.   ","In this paper, we present a 30-month longitudinal study analyzing the stability, security, and potential manipulation of free web proxies that we collected from 11 providers.","Our collection resulted in over 640,600 proxies, that we cumulatively tested daily.","We find that only 34.5% of proxies were active at least once during our tests, showcasing the general instability of free proxies.","Geographically, a majority of proxies originate from the US and China.","Leveraging the Shodan search engine, we identified 4,452 distinct vulnerabilities on the proxies' IP addresses, including 1,755 vulnerabilities that allow unauthorized remote code execution and 2,036 that enable privilege escalation on the host device.","Through the software analysis on the proxies' IP addresses, we find that 42,206 of them appear to run on MikroTik routers.","Worryingly, we also discovered 16,923 proxies that manipulate content, indicating potential malicious intent by proxy owners.","Ultimately, our research reveals that the use of free web proxies poses significant risks to users' privacy and security.","The instability, vulnerabilities, and potential for malicious actions uncovered in our analysis lead us to strongly caution users against relying on free proxies."],"url":"http://arxiv.org/abs/2403.02445v1","category":"cs.CR"}
{"created":"2024-03-04 19:56:19","title":"Anatomically Constrained Tractography of the Fetal Brain","abstract":"Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to study the fetal brain in utero. An important computation enabled by dMRI is streamline tractography, which has unique applications such as tract-specific analysis of the brain white matter and structural connectivity assessment. However, due to the low fetal dMRI data quality and the challenging nature of tractography, existing methods tend to produce highly inaccurate results. They generate many false streamlines while failing to reconstruct streamlines that constitute the major white matter tracts. In this paper, we advocate for anatomically constrained tractography based on an accurate segmentation of the fetal brain tissue directly in the dMRI space. We develop a deep learning method to compute the segmentation automatically. Experiments on independent test data show that this method can accurately segment the fetal brain tissue and drastically improve tractography results. It enables the reconstruction of highly curved tracts such as optic radiations. Importantly, our method infers the tissue segmentation and streamline propagation direction from a diffusion tensor fit to the dMRI data, making it applicable to routine fetal dMRI scans. The proposed method can lead to significant improvements in the accuracy and reproducibility of quantitative assessment of the fetal brain with dMRI.","sentences":["Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to study the fetal brain in utero.","An important computation enabled by dMRI is streamline tractography, which has unique applications such as tract-specific analysis of the brain white matter and structural connectivity assessment.","However, due to the low fetal dMRI data quality and the challenging nature of tractography, existing methods tend to produce highly inaccurate results.","They generate many false streamlines while failing to reconstruct streamlines that constitute the major white matter tracts.","In this paper, we advocate for anatomically constrained tractography based on an accurate segmentation of the fetal brain tissue directly in the dMRI space.","We develop a deep learning method to compute the segmentation automatically.","Experiments on independent test data show that this method can accurately segment the fetal brain tissue and drastically improve tractography results.","It enables the reconstruction of highly curved tracts such as optic radiations.","Importantly, our method infers the tissue segmentation and streamline propagation direction from a diffusion tensor fit to the dMRI data, making it applicable to routine fetal dMRI scans.","The proposed method can lead to significant improvements in the accuracy and reproducibility of quantitative assessment of the fetal brain with dMRI."],"url":"http://arxiv.org/abs/2403.02444v1","category":"cs.CV"}
{"created":"2024-03-04 19:38:50","title":"Root Causing Prediction Anomalies Using Explainable AI","abstract":"This paper presents a novel application of explainable AI (XAI) for root-causing performance degradation in machine learning models that learn continuously from user engagement data. In such systems a single feature corruption can cause cascading feature, label and concept drifts. We have successfully applied this technique to improve the reliability of models used in personalized advertising. Performance degradation in such systems manifest as prediction anomalies in the models. These models are typically trained continuously using features that are produced by hundreds of real time data processing pipelines or derived from other upstream models. A failure in any of these pipelines or an instability in any of the upstream models can cause feature corruption, causing the model's predicted output to deviate from the actual output and the training data to become corrupted. The causal relationship between the features and the predicted output is complex, and root-causing is challenging due to the scale and dynamism of the system. We demonstrate how temporal shifts in the global feature importance distribution can effectively isolate the cause of a prediction anomaly, with better recall than model-to-feature correlation methods. The technique appears to be effective even when approximating the local feature importance using a simple perturbation-based method, and aggregating over a few thousand examples. We have found this technique to be a model-agnostic, cheap and effective way to monitor complex data pipelines in production and have deployed a system for continuously analyzing the global feature importance distribution of continuously trained models.","sentences":["This paper presents a novel application of explainable AI (XAI) for root-causing performance degradation in machine learning models that learn continuously from user engagement data.","In such systems a single feature corruption can cause cascading feature, label and concept drifts.","We have successfully applied this technique to improve the reliability of models used in personalized advertising.","Performance degradation in such systems manifest as prediction anomalies in the models.","These models are typically trained continuously using features that are produced by hundreds of real time data processing pipelines or derived from other upstream models.","A failure in any of these pipelines or an instability in any of the upstream models can cause feature corruption, causing the model's predicted output to deviate from the actual output and the training data to become corrupted.","The causal relationship between the features and the predicted output is complex, and root-causing is challenging due to the scale and dynamism of the system.","We demonstrate how temporal shifts in the global feature importance distribution can effectively isolate the cause of a prediction anomaly, with better recall than model-to-feature","correlation methods.","The technique appears to be effective even when approximating the local feature importance using a simple perturbation-based method, and aggregating over a few thousand examples.","We have found this technique to be a model-agnostic, cheap and effective way to monitor complex data pipelines in production and have deployed a system for continuously analyzing the global feature importance distribution of continuously trained models."],"url":"http://arxiv.org/abs/2403.02439v1","category":"cs.LG"}
{"created":"2024-03-04 19:35:08","title":"SoK: Challenges and Opportunities in Federated Unlearning","abstract":"Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \\emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \\emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK paper aims to take a deep look at the \\emph{federated unlearning} literature, with the goal of identifying research trends and challenges in this emerging field. By carefully categorizing papers published on FL unlearning (since 2020), we aim to pinpoint the unique complexities of federated unlearning, highlighting limitations on directly applying centralized unlearning methods. We compare existing federated unlearning methods regarding influence removal and performance recovery, compare their threat models and assumptions, and discuss their implications and limitations. For instance, we analyze the experimental setup of FL unlearning studies from various perspectives, including data heterogeneity and its simulation, the datasets used for demonstration, and evaluation metrics. Our work aims to offer insights and suggestions for future research on federated unlearning.","sentences":["Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves.","This allows training models on user data while respecting privacy regulations such as GDPR and CPRA.","However, emerging privacy requirements may mandate model owners to be able to \\emph{forget} some learned data, e.g., when requested by data owners or law enforcement.","This has given birth to an active field of research called \\emph{machine unlearning}.","In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable!","This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL.","In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   ","This SoK paper aims to take a deep look at the \\emph{federated unlearning} literature, with the goal of identifying research trends and challenges in this emerging field.","By carefully categorizing papers published on FL unlearning (since 2020), we aim to pinpoint the unique complexities of federated unlearning, highlighting limitations on directly applying centralized unlearning methods.","We compare existing federated unlearning methods regarding influence removal and performance recovery, compare their threat models and assumptions, and discuss their implications and limitations.","For instance, we analyze the experimental setup of FL unlearning studies from various perspectives, including data heterogeneity and its simulation, the datasets used for demonstration, and evaluation metrics.","Our work aims to offer insights and suggestions for future research on federated unlearning."],"url":"http://arxiv.org/abs/2403.02437v1","category":"cs.LG"}
{"created":"2024-03-04 19:22:09","title":"Towards efficient deep autoencoders for multivariate time series anomaly detection","abstract":"Multivariate time series anomaly detection is a crucial problem in many industrial and research applications. Timely detection of anomalies allows, for instance, to prevent defects in manufacturing processes and failures in cyberphysical systems. Deep learning methods are preferred among others for their accuracy and robustness for the analysis of complex multivariate data. However, a key aspect is being able to extract predictions in a timely manner, to accommodate real-time requirements in different applications. In the case of deep learning models, model reduction is extremely important to achieve optimal results in real-time systems with limited time and memory constraints. In this paper, we address this issue by proposing a novel compression method for deep autoencoders that involves three key factors. First, pruning reduces the number of weights, while preventing catastrophic drops in accuracy by means of a fast search process that identifies high sparsity levels. Second, linear and non-linear quantization reduces model complexity by reducing the number of bits for every single weight. The combined contribution of these three aspects allow the model size to be reduced, by removing a subset of the weights (pruning), and decreasing their bit-width (quantization). As a result, the compressed model is faster and easier to adopt in highly constrained hardware environments. Experiments performed on popular multivariate anomaly detection benchmarks, show that our method is capable of achieving significant model compression ratio (between 80% and 95%) without a significant reduction in the anomaly detection performance.","sentences":["Multivariate time series anomaly detection is a crucial problem in many industrial and research applications.","Timely detection of anomalies allows, for instance, to prevent defects in manufacturing processes and failures in cyberphysical systems.","Deep learning methods are preferred among others for their accuracy and robustness for the analysis of complex multivariate data.","However, a key aspect is being able to extract predictions in a timely manner, to accommodate real-time requirements in different applications.","In the case of deep learning models, model reduction is extremely important to achieve optimal results in real-time systems with limited time and memory constraints.","In this paper, we address this issue by proposing a novel compression method for deep autoencoders that involves three key factors.","First, pruning reduces the number of weights, while preventing catastrophic drops in accuracy by means of a fast search process that identifies high sparsity levels.","Second, linear and non-linear quantization reduces model complexity by reducing the number of bits for every single weight.","The combined contribution of these three aspects allow the model size to be reduced, by removing a subset of the weights (pruning), and decreasing their bit-width (quantization).","As a result, the compressed model is faster and easier to adopt in highly constrained hardware environments.","Experiments performed on popular multivariate anomaly detection benchmarks, show that our method is capable of achieving significant model compression ratio (between 80% and 95%) without a significant reduction in the anomaly detection performance."],"url":"http://arxiv.org/abs/2403.02429v1","category":"cs.LG"}
{"created":"2024-03-04 19:13:23","title":"Situated Understanding of Older Adults' Interactions with Voice Assistants: A Month-long In-home Study","abstract":"Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults' interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults' homes with Amazon smart speakers integrated with custom audio recorders to collect ``in-the-wild'' audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs. Midway through our study, we deployed ChatGPT-powered Alexa skill to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with LLMs' contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults' expectations.","sentences":["Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling.","Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults' interactions with VAs, particularly regarding their reactions to errors.","To bridge this gap, we equipped 15 older adults' homes with Amazon smart speakers integrated with custom audio recorders to collect ``in-the-wild'' audio interaction data for detailed error analysis.","Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs.","Midway through our study, we deployed ChatGPT-powered Alexa skill to investigate its efficacy for older adults.","Our research suggests leveraging vocal and verbal responses combined with LLMs' contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults' expectations."],"url":"http://arxiv.org/abs/2403.02421v1","category":"cs.HC"}
{"created":"2024-03-04 19:12:48","title":"Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems","abstract":"Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LLM calls lead to higher performance on \"easy\" queries, but lower performance on \"hard\" queries, and non-monotone behavior emerges when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LLM calls that maximizes system performance, and define a scaling law of Voting Inference Systems. Experiments show that our scaling law can predict the performance of Voting Inference Systems and find the optimal number of LLM calls to make.","sentences":["Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses.","However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance.","In this paper, we initiate the study of scaling laws of compound inference systems.","We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting.","We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls.","Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LLM calls lead to higher performance on \"easy\" queries, but lower performance on \"hard\" queries, and non-monotone behavior emerges when a task contains both types of queries.","This insight then allows us to compute, from a small number of samples, the number of LLM calls that maximizes system performance, and define a scaling law of Voting Inference Systems.","Experiments show that our scaling law can predict the performance of Voting Inference Systems and find the optimal number of LLM calls to make."],"url":"http://arxiv.org/abs/2403.02419v1","category":"cs.LG"}
{"created":"2024-03-04 19:11:39","title":"Successive quasienergy collapse and the driven Dicke phase transition in the few-emitter limit","abstract":"The emergent behavior that arises in many-body systems of increasing size follows universal laws that become apparent in order-to-disorder transitions. While this behavior has been traditionally explored for large numbers of emitters, recent progress allows for the exploration of the few-emitter limit, where correlations can be measured and connected to microscopic models to gain further insight into order-to-disorder transitions. We explore this few-body limit in the driven and damped Tavis--Cummings model, which describes a collection of atoms interacting with a driven and damped cavity mode. Our exploration revolves around the dressed states of the atomic ensemble and field, whose energies are shown to collapse as the driving field is increased to mark the onset of a dissipative quantum phase transition. The collapse occurs in stages and is an effect of light-matter correlations that are overlooked for single atoms and neglected in mean-field models. The implications of these correlations over the macroscopic observables of the system are presented. We encounter a shift in the expected transition point and an increased number of parity-broken states to choose from once the ordered phase is reached.","sentences":["The emergent behavior that arises in many-body systems of increasing size follows universal laws that become apparent in order-to-disorder transitions.","While this behavior has been traditionally explored for large numbers of emitters, recent progress allows for the exploration of the few-emitter limit, where correlations can be measured and connected to microscopic models to gain further insight into order-to-disorder transitions.","We explore this few-body limit in the driven and damped Tavis--Cummings model, which describes a collection of atoms interacting with a driven and damped cavity mode.","Our exploration revolves around the dressed states of the atomic ensemble and field, whose energies are shown to collapse as the driving field is increased to mark the onset of a dissipative quantum phase transition.","The collapse occurs in stages and is an effect of light-matter correlations that are overlooked for single atoms and neglected in mean-field models.","The implications of these correlations over the macroscopic observables of the system are presented.","We encounter a shift in the expected transition point and an increased number of parity-broken states to choose from once the ordered phase is reached."],"url":"http://arxiv.org/abs/2403.02417v1","category":"quant-ph"}
{"created":"2024-03-04 19:10:39","title":"Arrays in Practice: An Empirical Study of Array Access Patterns on the JVM","abstract":"The array is a data structure used in a wide range of programs. Its compact storage and constant time random access makes it highly efficient, but arbitrary indexing complicates the analysis of code containing array accesses. Such analyses are important for compiler optimisations such as bounds check elimination. The aim of this work is to gain a better understanding of how arrays are used in real-world programs. While previous work has applied static analyses to understand how arrays are accessed and used, we take a dynamic approach. We empirically examine various characteristics of array usage by instrumenting programs to log all array accesses, allowing for analysis of array sizes, element types, from where arrays are accessed and to which extent sequences of array accesses form recognizable patterns. The programs in the study were collected from the Renaissance benchmark suite, all running on the Java Virtual Machine.   We account for characteristics displayed by the arrays investigated, finding that most arrays have a small size, are accessed by only one or two classes and by a single thread. On average over the benchmarks, 69.8% of the access patterns consist of uncomplicated traversals. Most of the instrumented classes (over 95%) do not use arrays directly at all. These results come from tracing data covering 3,803,043,390 array accesses made across 168,686 classes. While our analysis has only been applied to the Renaissance benchmark suite, the methodology can be applied to any program running on the Java Virtual Machine. This study, and the methodology in general, can inform future runtime implementations and compiler optimisations.","sentences":["The array is a data structure used in a wide range of programs.","Its compact storage and constant time random access makes it highly efficient, but arbitrary indexing complicates the analysis of code containing array accesses.","Such analyses are important for compiler optimisations such as bounds check elimination.","The aim of this work is to gain a better understanding of how arrays are used in real-world programs.","While previous work has applied static analyses to understand how arrays are accessed and used, we take a dynamic approach.","We empirically examine various characteristics of array usage by instrumenting programs to log all array accesses, allowing for analysis of array sizes, element types, from where arrays are accessed and to which extent sequences of array accesses form recognizable patterns.","The programs in the study were collected from the Renaissance benchmark suite, all running on the Java Virtual Machine.   ","We account for characteristics displayed by the arrays investigated, finding that most arrays have a small size, are accessed by only one or two classes and by a single thread.","On average over the benchmarks, 69.8% of the access patterns consist of uncomplicated traversals.","Most of the instrumented classes (over 95%) do not use arrays directly at all.","These results come from tracing data covering 3,803,043,390 array accesses made across 168,686 classes.","While our analysis has only been applied to the Renaissance benchmark suite, the methodology can be applied to any program running on the Java Virtual Machine.","This study, and the methodology in general, can inform future runtime implementations and compiler optimisations."],"url":"http://arxiv.org/abs/2403.02416v1","category":"cs.PL"}
{"created":"2024-03-04 19:07:42","title":"Privacy-Respecting Type Error Telemetry at Scale","abstract":"Context: Roblox Studio lets millions of creators build interactive experiences by programming in a variant of Lua called Luau. The creators form a broad group, ranging from novices writing their first script to professional developers; thus, Luau must support a wide audience. As part of its efforts to support all kinds of programmers, Luau includes an optional, gradual type system and goes to great lengths to minimize false positive errors.   Inquiry: Since Luau is currently being used by many creators, we want to collect data to improve the language and, in particular, the type system. The standard way to collect data is to deploy client-side telemetry; however, we cannot scrape personal data or proprietary information, which means we cannot collect source code fragments, error messages, or even filepaths. The research questions are thus about how to conduct telemetry that is not invasive and obtain insights from it about type errors.   Approach: We designed and implemented a pseudonymized, randomly-sampling telemetry system for Luau. Telemetry records include a timestamp, a session id, a reason for sending, and a numeric summary of the most recent type analyses. This information lets us study type errors over time without revealing private data. We deployed the system in Roblox Studio during Spring 2023 and collected over 1.5 million telemetry records from over 340,000 sessions.   Knowledge: We present several findings about Luau, all of which suggest that telemetry is an effective way to study type error pragmatics. One of the less-surprising findings is that opt-in gradual types are unpopular: there is an 100x gap between the number of untyped Luau sessions and the number of typed ones. One surprise is that the strict mode for type analysis is overly conservative about interactions with data assets. A reassuring finding is that type analysis rarely hits its internal limits on problem size.   Grounding: Our findings are supported by a dataset of over 1.5 million telemetry records. The data and scripts for analyzing it are available in an artifact.   Importance: Beyond the immediate benefits to Luau, our findings about types and type errors have implications for adoption and ergonomics in other gradual languages such as TypeScript, Elixir, and Typed Racket. Our telemetry design is of broad interest, as it reports on type errors without revealing sensitive information.","sentences":["Context: Roblox Studio lets millions of creators build interactive experiences by programming in a variant of Lua called Luau.","The creators form a broad group, ranging from novices writing their first script to professional developers; thus, Luau must support a wide audience.","As part of its efforts to support all kinds of programmers, Luau includes an optional, gradual type system and goes to great lengths to minimize false positive errors.   ","Inquiry: Since Luau is currently being used by many creators, we want to collect data to improve the language and, in particular, the type system.","The standard way to collect data is to deploy client-side telemetry; however, we cannot scrape personal data or proprietary information, which means we cannot collect source code fragments, error messages, or even filepaths.","The research questions are thus about how to conduct telemetry that is not invasive and obtain insights from it about type errors.   ","Approach: We designed and implemented a pseudonymized, randomly-sampling telemetry system for Luau.","Telemetry records include a timestamp, a session id, a reason for sending, and a numeric summary of the most recent type analyses.","This information lets us study type errors over time without revealing private data.","We deployed the system in Roblox Studio during Spring 2023 and collected over 1.5 million telemetry records from over 340,000 sessions.   ","Knowledge: We present several findings about Luau, all of which suggest that telemetry is an effective way to study type error pragmatics.","One of the less-surprising findings is that opt-in gradual types are unpopular: there is an 100x gap between the number of untyped Luau sessions and the number of typed ones.","One surprise is that the strict mode for type analysis is overly conservative about interactions with data assets.","A reassuring finding is that type analysis rarely hits its internal limits on problem size.   ","Grounding: Our findings are supported by a dataset of over 1.5 million telemetry records.","The data and scripts for analyzing it are available in an artifact.   ","Importance:","Beyond the immediate benefits to Luau, our findings about types and type errors have implications for adoption and ergonomics in other gradual languages such as TypeScript, Elixir, and Typed Racket.","Our telemetry design is of broad interest, as it reports on type errors without revealing sensitive information."],"url":"http://arxiv.org/abs/2403.02409v1","category":"cs.PL"}
{"created":"2024-03-04 19:00:01","title":"Final Moments II: Observational Properties and Physical Modeling of CSM-Interacting Type II Supernovae","abstract":"We present ultraviolet/optical/near-infrared observations and modeling of Type II supernovae (SNe II) whose early-time ($\\delta t < 2$ days) spectra show transient, narrow emission lines from shock ionization of confined ($r < 10^{15}$ cm) circumstellar material (CSM). The observed electron-scattering broadened line profiles (i.e., IIn-like) of HI, He I/II, C III/IV, and N III/IV/V from the CSM persist on a characteristic timescale ($t_{\\rm IIn}$) that marks a transition to a lower-density CSM and the emergence of Doppler-broadened features from the fast-moving SN ejecta. Our sample, the largest to date, consists of 39 SNe with early-time IIn-like features in addition to 35 \"comparison\" SNe with no evidence of early-time IIn-like features, all with ultraviolet observations. The total sample consists of 50 unpublished objects with 474 previously unpublished spectra and 50 multiband light curves, collected primarily through the Young Supernova Experiment and Global Supernova Project collaborations. For all sample objects, we find a significant correlation between peak ultraviolet brightness and both $t_{\\rm IIn}$ and the rise time, as well as evidence for enhanced peak luminosities in SNe II with IIn-like features. We quantify mass-loss rates and CSM density for the sample through matching of peak multiband absolute magnitudes, rise times, $t_{\\rm IIn}$ and optical SN spectra with a grid of radiation hydrodynamics and non-local thermodynamic equilibrium (nLTE) radiative-transfer simulations. For our grid of models, all with the same underlying explosion, there is a trend between the duration of the electron-scattering broadened line profiles and inferred mass-loss rate: $t_{\\rm IIn} \\approx 3.8[\\dot{M}/(0.01 \\textrm{M}_{\\odot} \\textrm{yr}^{-1})]$ days.","sentences":["We present ultraviolet/optical/near-infrared observations and modeling of Type II supernovae (SNe II) whose early-time ($\\delta t < 2$ days) spectra show transient, narrow emission lines from shock ionization of confined ($r < 10^{15}$ cm) circumstellar material (CSM).","The observed electron-scattering broadened line profiles (i.e., IIn-like) of HI, He I/II, C III/IV, and N III/IV/V from the CSM persist on a characteristic timescale ($t_{\\rm IIn}$) that marks a transition to a lower-density CSM and the emergence of Doppler-broadened features from the fast-moving SN ejecta.","Our sample, the largest to date, consists of 39 SNe with early-time IIn-like features in addition to 35 \"comparison\" SNe with no evidence of early-time IIn-like features, all with ultraviolet observations.","The total sample consists of 50 unpublished objects with 474 previously unpublished spectra and 50 multiband light curves, collected primarily through the Young Supernova Experiment and Global Supernova Project collaborations.","For all sample objects, we find a significant correlation between peak ultraviolet brightness and both $t_{\\rm IIn}$ and the rise time, as well as evidence for enhanced peak luminosities in SNe II with IIn-like features.","We quantify mass-loss rates and CSM density for the sample through matching of peak multiband absolute magnitudes, rise times, $t_{\\rm IIn}$ and optical SN spectra with a grid of radiation hydrodynamics and non-local thermodynamic equilibrium (nLTE) radiative-transfer simulations.","For our grid of models, all with the same underlying explosion, there is a trend between the duration of the electron-scattering broadened line profiles and inferred mass-loss rate: $t_{\\rm IIn} \\approx 3.8[\\dot{M}/(0.01 \\textrm{M}_{\\odot} \\textrm{yr}^{-1})]$ days."],"url":"http://arxiv.org/abs/2403.02382v1","category":"astro-ph.HE"}
{"created":"2024-03-04 18:59:30","title":"Twisting Lids Off with Two Hands","abstract":"Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.","sentences":["Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system.","In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world.","With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors.","Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity."],"url":"http://arxiv.org/abs/2403.02338v1","category":"cs.RO"}
{"created":"2024-03-04 18:58:53","title":"Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis","abstract":"In the highly competitive area of product marketing, the visibility of brand logos on packaging plays a crucial role in shaping consumer perception, directly influencing the success of the product. This paper introduces a comprehensive framework to measure the brand logo's attention on a packaging design. The proposed method consists of three steps. The first step leverages YOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500 and LogoDet-3K. The second step involves modeling the user's visual attention with a novel saliency prediction model tailored for the packaging context. The proposed saliency model combines the visual elements with text maps employing a transformers-based architecture to predict user attention maps. In the third step, by integrating logo detection with a saliency map generation, the framework provides a comprehensive brand attention score. The effectiveness of the proposed method is assessed module by module, ensuring a thorough evaluation of each component. Comparing logo detection and saliency map prediction with state-of-the-art models shows the superiority of the proposed methods. To investigate the robustness of the proposed brand attention score, we collected a unique dataset to examine previous psychophysical hypotheses related to brand visibility. the results show that the brand attention score is in line with all previous studies. Also, we introduced seven new hypotheses to check the impact of position, orientation, presence of person, and other visual elements on brand attention. This research marks a significant stride in the intersection of cognitive psychology, computer vision, and marketing, paving the way for advanced, consumer-centric packaging designs.","sentences":["In the highly competitive area of product marketing, the visibility of brand logos on packaging plays a crucial role in shaping consumer perception, directly influencing the success of the product.","This paper introduces a comprehensive framework to measure the brand logo's attention on a packaging design.","The proposed method consists of three steps.","The first step leverages YOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500 and LogoDet-3K. The second step involves modeling the user's visual attention with a novel saliency prediction model tailored for the packaging context.","The proposed saliency model combines the visual elements with text maps employing a transformers-based architecture to predict user attention maps.","In the third step, by integrating logo detection with a saliency map generation, the framework provides a comprehensive brand attention score.","The effectiveness of the proposed method is assessed module by module, ensuring a thorough evaluation of each component.","Comparing logo detection and saliency map prediction with state-of-the-art models shows the superiority of the proposed methods.","To investigate the robustness of the proposed brand attention score, we collected a unique dataset to examine previous psychophysical hypotheses related to brand visibility.","the results show that the brand attention score is in line with all previous studies.","Also, we introduced seven new hypotheses to check the impact of position, orientation, presence of person, and other visual elements on brand attention.","This research marks a significant stride in the intersection of cognitive psychology, computer vision, and marketing, paving the way for advanced, consumer-centric packaging designs."],"url":"http://arxiv.org/abs/2403.02336v1","category":"cs.CV"}
{"created":"2024-03-04 18:58:46","title":"Gradient Correlation Subspace Learning against Catastrophic Forgetting","abstract":"Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}","sentences":["Efficient continual learning techniques have been a topic of significant research over the last few years.","A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting.","This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL).","The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace.","The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task.","Code will be available at \\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}"],"url":"http://arxiv.org/abs/2403.02334v1","category":"cs.LG"}
{"created":"2024-03-04 18:58:30","title":"Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning","abstract":"Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance that not only outpaces other finetuned 7B models but also exceeds that of certain 34B models. Our ablation studies further confirm the substantial enhancement in mathematical reasoning across various subtopics, marking a significant stride in LLMs' reasoning capabilities.","sentences":["Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets.","Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources.","KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability.","As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs.","Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset.","Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance that not only outpaces other finetuned 7B models but also exceeds that of certain 34B models.","Our ablation studies further confirm the substantial enhancement in mathematical reasoning across various subtopics, marking a significant stride in LLMs' reasoning capabilities."],"url":"http://arxiv.org/abs/2403.02333v1","category":"cs.CL"}
{"created":"2024-03-04 18:55:50","title":"Model Lakes","abstract":"Given a set of deep learning models, it can be hard to find models appropriate to a task, understand the models, and characterize how models are different one from another. Currently, practitioners rely on manually-written documentation to understand and choose models. However, not all models have complete and reliable documentation. As the number of machine learning models increases, this issue of finding, differentiating, and understanding models is becoming more crucial. Inspired from research on data lakes, we introduce and define the concept of model lakes. We discuss fundamental research challenges in the management of large models. And we discuss what principled data management techniques can be brought to bear on the study of large model management.","sentences":["Given a set of deep learning models, it can be hard to find models appropriate to a task, understand the models, and characterize how models are different one from another.","Currently, practitioners rely on manually-written documentation to understand and choose models.","However, not all models have complete and reliable documentation.","As the number of machine learning models increases, this issue of finding, differentiating, and understanding models is becoming more crucial.","Inspired from research on data lakes, we introduce and define the concept of model lakes.","We discuss fundamental research challenges in the management of large models.","And we discuss what principled data management techniques can be brought to bear on the study of large model management."],"url":"http://arxiv.org/abs/2403.02327v1","category":"cs.DB"}
{"created":"2024-03-04 18:55:30","title":"Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training","abstract":"Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a \"visual prompt\", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse region-based tasks such as recognition, math, and object relationship reasoning. We also show CRG's applicability to spatial reasoning, with 10% improvement on What'sUp, as well as to compositional generalization -- improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe -- and to image-text alignment for generated images, where we improve by up to 8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG allows us to re-rank proposed regions in referring expression comprehension and phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an average gain of 3.2% in accuracy. Our analysis explores alternative masking strategies for CRG, quantifies CRG's probability shift, and evaluates the role of region guidance strength, empirically validating CRG's design choices.","sentences":["Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest.","For example, VLMs can be given a \"visual prompt\", where visual markers such as bounding boxes delineate key image regions.","However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts.","We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts.","CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior).","CRG achieves substantial improvements in a wide variety of VL tasks: When region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse region-based tasks such as recognition, math, and object relationship reasoning.","We also show CRG's applicability to spatial reasoning, with 10% improvement on What'sUp, as well as to compositional generalization -- improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe -- and to image-text alignment for generated images, where we improve by up to 8.4 AUROC and 6.8 F1 points on SeeTRUE.","When reference regions are absent, CRG allows us to re-rank proposed regions in referring expression comprehension and phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an average gain of 3.2% in accuracy.","Our analysis explores alternative masking strategies for CRG, quantifies CRG's probability shift, and evaluates the role of region guidance strength, empirically validating CRG's design choices."],"url":"http://arxiv.org/abs/2403.02325v1","category":"cs.CV"}
{"created":"2024-03-04 18:32:12","title":"Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation","abstract":"Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune the ShareGPT4V model for this specific task, aiming to achieve state-of-the-art results in this particular challenge. Although such a model would not be practical in production, as it is incredibly expensive compared to a specialized model like MiVOLO, it could be very useful in some tasks, like data annotation.","sentences":["Multimodal Large Language Models (MLLMs) have recently gained immense popularity.","Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision.","These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained.","We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO.","We also updated MiVOLO and provide details and new metrics in this article.","This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models.","Furthermore, we attempted various ways to fine-tune the ShareGPT4V model for this specific task, aiming to achieve state-of-the-art results in this particular challenge.","Although such a model would not be practical in production, as it is incredibly expensive compared to a specialized model like MiVOLO, it could be very useful in some tasks, like data annotation."],"url":"http://arxiv.org/abs/2403.02302v1","category":"cs.CV"}
{"created":"2024-03-04 18:27:47","title":"Reactive Programming without Functions","abstract":"Context: Reactive programming (RP) is a declarative programming paradigm suitable for expressing the handling of events. It enables programmers to create applications that react automatically to changes over time. Whenever a time-varying signal changes -- e.g. in response to values produced by event stream (e.g., sensor data, user input...) -- the program state is updated automatically in tandem with that change. This makes RP well-suited for building interactive applications and reactive (soft real-time) systems. Inquiry: RP Language implementations are often built on top of an existing (host) language as an Embedded Domain Specific Language (EDSL). This results in application code in which reactive code and non-reactive code is inherently entangled. Using a mechanism known as lifting, one usually has access to the full feature set of the (non-reactive) host language in the RP program. However, lifting is also dangerous. First, host code expressed in a Turing-complete language may diverge, resulting in unresponsive programs: i.e. reactive programs that are not actually reactive. Second, the bi-directional integration of reactive and non-reactive code results in a paradigmatic mismatch that, when unchecked, leads to faulty behaviour in programs. Approach: We propose a new reactive programming language, that has been meticulously designed to be reactive-only. We start with a simple (first-order) model for reactivity, based on reactors (i.e. uninstantiated descriptions of signals and their dependencies) and deployments (i.e. instances of reactors) that consist of signals. The language does not have the notion of functions, and thus unlike other RP languages there is no lifting either. We extend this simple model incrementally with additional features found in other programming languages, RP or otherwise. These features include stateful reactors (that allow for time-based accumulation), signals with dynamic dependencies by means of conditionals and polymorphic deployments, recursively-defined reactors, and (anonymous) reactors with lexical scope. Knowledge: In our description of these language features, we not only describe the syntax and semantics, but also how each features compares to the problems that exist in (EDSL) RP languages. I.e. by starting from a reactive-only model, we identify which reactive features (that, in other RP languages are typically expressed in non-reactive code) affect the reactive guarantees that can be enforced by the language. Grounding: We base our arguments by analysing the effect that each feature has on our language: e.g., by analysing how signals are updated, how they are created and how dependencies between signals can be affected. When applicable, we draw parallels with other languages: i.e. similarities shared with other RP languages will be highlighted and thoroughly analysed, and where relevant the same will also be done with non-reactive languages. Importance: Our language shows how a purely reactive programming is able to express the same kinds of programs as in other RP languages that require the use of (unchecked) functions. By considering reactive programs as a collection of pure (reactive-only) reactors, we aim to increase how reactive programming is comprehended by both language designers and its users.","sentences":["Context: Reactive programming (RP) is a declarative programming paradigm suitable for expressing the handling of events.","It enables programmers to create applications that react automatically to changes over time.","Whenever a time-varying signal changes -- e.g. in response to values produced by event stream (e.g., sensor data, user input...) -- the program state is updated automatically in tandem with that change.","This makes RP well-suited for building interactive applications and reactive (soft real-time) systems.","Inquiry: RP Language implementations are often built on top of an existing (host) language as an Embedded Domain Specific Language (EDSL).","This results in application code in which reactive code and non-reactive code is inherently entangled.","Using a mechanism known as lifting, one usually has access to the full feature set of the (non-reactive) host language in the RP program.","However, lifting is also dangerous.","First, host code expressed in a Turing-complete language may diverge, resulting in unresponsive programs: i.e. reactive programs that are not actually reactive.","Second, the bi-directional integration of reactive and non-reactive code results in a paradigmatic mismatch that, when unchecked, leads to faulty behaviour in programs.","Approach:","We propose a new reactive programming language, that has been meticulously designed to be reactive-only.","We start with a simple (first-order) model for reactivity, based on reactors (i.e. uninstantiated descriptions of signals and their dependencies) and deployments (i.e. instances of reactors) that consist of signals.","The language does not have the notion of functions, and thus unlike other RP languages there is no lifting either.","We extend this simple model incrementally with additional features found in other programming languages, RP or otherwise.","These features include stateful reactors (that allow for time-based accumulation), signals with dynamic dependencies by means of conditionals and polymorphic deployments, recursively-defined reactors, and (anonymous) reactors with lexical scope.","Knowledge:","In our description of these language features, we not only describe the syntax and semantics, but also how each features compares to the problems that exist in (EDSL) RP languages.","I.e. by starting from a reactive-only model, we identify which reactive features (that, in other RP languages are typically expressed in non-reactive code) affect the reactive guarantees that can be enforced by the language.","Grounding: We base our arguments by analysing the effect that each feature has on our language: e.g., by analysing how signals are updated, how they are created and how dependencies between signals can be affected.","When applicable, we draw parallels with other languages: i.e. similarities shared with other RP languages will be highlighted and thoroughly analysed, and where relevant the same will also be done with non-reactive languages.","Importance:","Our language shows how a purely reactive programming is able to express the same kinds of programs as in other RP languages that require the use of (unchecked) functions.","By considering reactive programs as a collection of pure (reactive-only) reactors, we aim to increase how reactive programming is comprehended by both language designers and its users."],"url":"http://arxiv.org/abs/2403.02296v1","category":"cs.PL"}
{"created":"2024-03-04 18:23:55","title":"OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport","abstract":"Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models. In this paper, we introduce \\sys, a framework that harnesses optimal transport theory for data repair under CI constraints. Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility. We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution. However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance. To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which efficiently addresses high-dimensional and large-scale data. Through extensive experiments, we demonstrate the efficacy and efficiency of our proposed methods, showcasing their practical utility in real-world data cleaning and preprocessing tasks. Furthermore, we provide comparisons with traditional approaches, highlighting the superiority of our techniques in terms of preserving data utility while ensuring adherence to the desired CI constraints.","sentences":["Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models.","In this paper, we introduce \\sys, a framework that harnesses optimal transport theory for data repair under CI constraints.","Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility.","We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution.","However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance.","To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which efficiently addresses high-dimensional and large-scale data.","Through extensive experiments, we demonstrate the efficacy and efficiency of our proposed methods, showcasing their practical utility in real-world data cleaning and preprocessing tasks.","Furthermore, we provide comparisons with traditional approaches, highlighting the superiority of our techniques in terms of preserving data utility while ensuring adherence to the desired CI constraints."],"url":"http://arxiv.org/abs/2403.02372v1","category":"cs.LG"}
{"created":"2024-03-04 18:19:48","title":"Koopman-Assisted Reinforcement Learning","abstract":"The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory. However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity. This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes (MDPs), resulting in the development of two new RL algorithms to address these limitations. We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable. In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor'' that facilitates the estimation of the optimal value function. Then, a transformation of Bellman's framework in terms of the Koopman tensor enables us to reformulate two max-entropy RL algorithms: soft value iteration and soft actor-critic (SAC). This highly flexible framework can be used for deterministic or stochastic systems as well as for discrete or continuous-time dynamics. Finally, we show that these Koopman Assisted Reinforcement Learning (KARL) algorithms attain state-of-the-art (SOTA) performance with respect to traditional neural network-based SAC and linear quadratic regulator (LQR) baselines on four controlled dynamical systems: a linear state-space system, the Lorenz system, fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing.","sentences":["The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory.","However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity.","This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes (MDPs), resulting in the development of two new RL algorithms to address these limitations.","We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable.","In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates.","By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor'' that facilitates the estimation of the optimal value function.","Then, a transformation of Bellman's framework in terms of the Koopman tensor enables us to reformulate two max-entropy RL algorithms: soft value iteration and soft actor-critic (SAC).","This highly flexible framework can be used for deterministic or stochastic systems as well as for discrete or continuous-time dynamics.","Finally, we show that these Koopman Assisted Reinforcement Learning (KARL) algorithms attain state-of-the-art (SOTA) performance with respect to traditional neural network-based SAC and linear quadratic regulator (LQR) baselines on four controlled dynamical systems: a linear state-space system, the Lorenz system, fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing."],"url":"http://arxiv.org/abs/2403.02290v1","category":"cs.AI"}
{"created":"2024-03-04 17:56:28","title":"Subjective $\\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection","abstract":"Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling. This approach understands each annotator's view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., sentiment analysis. However, this construction may be inappropriate for tasks such as hate speech detection, as it affords equal validity to all positions on e.g., sexism or racism. We argue that the conflation of hate and offence can invalidate findings on hate speech, and call for future work to be situated in theory, disentangling hate from its orthogonal concept, offence.","sentences":["Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling.","This approach understands each annotator's view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., sentiment analysis.","However, this construction may be inappropriate for tasks such as hate speech detection, as it affords equal validity to all positions on e.g., sexism or racism.","We argue that the conflation of hate and offence can invalidate findings on hate speech, and call for future work to be situated in theory, disentangling hate from its orthogonal concept, offence."],"url":"http://arxiv.org/abs/2403.02268v1","category":"cs.CL"}
{"created":"2024-03-04 17:40:03","title":"Probabilistic Fault-Tolerant Robust Traffic Grooming in OTN-over-DWDM Networks","abstract":"The development of next-generation networks is revolutionizing network operators' management and orchestration practices worldwide. The critical services supported by these networks require increasingly stringent performance requirements, especially when considering the aspect of network reliability. This increase in reliability, coupled with the mass generation and consumption of information stemming from the increasing complexity of the network and the integration of artificial intelligence agents, affects transport networks, which will be required to allow the feasibility of such services to materialize. To this end, traditional recovery schemes are inadequate to ensure the resilience requirements of next-generation critical services given the increasingly dynamic nature of the network. The work presented in this paper proposes a probabilistic and fault-tolerant robust traffic grooming model for OTN-over-DWDM networks. The model's parameterization gives network operators the ability to control the level of protection and reliability required to meet their quality of service and service level agreement guarantees. The results demonstrate that the robust solution can ensure fault tolerance even in the face of demand uncertainty without service disruptions and the need for reactive network maintenance.","sentences":["The development of next-generation networks is revolutionizing network operators' management and orchestration practices worldwide.","The critical services supported by these networks require increasingly stringent performance requirements, especially when considering the aspect of network reliability.","This increase in reliability, coupled with the mass generation and consumption of information stemming from the increasing complexity of the network and the integration of artificial intelligence agents, affects transport networks, which will be required to allow the feasibility of such services to materialize.","To this end, traditional recovery schemes are inadequate to ensure the resilience requirements of next-generation critical services given the increasingly dynamic nature of the network.","The work presented in this paper proposes a probabilistic and fault-tolerant robust traffic grooming model for OTN-over-DWDM networks.","The model's parameterization gives network operators the ability to control the level of protection and reliability required to meet their quality of service and service level agreement guarantees.","The results demonstrate that the robust solution can ensure fault tolerance even in the face of demand uncertainty without service disruptions and the need for reactive network maintenance."],"url":"http://arxiv.org/abs/2403.02254v1","category":"cs.NI"}
{"created":"2024-03-04 17:38:32","title":"KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection","abstract":"Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second limitation of existing RBPDs is that they solely rely on the image modality, ignoring useful textual information present in the webpage HTML. To utilize this textual information, we propose a Large Language Model (LLM)-based approach to extract brand information of webpages from text. Our resulting multimodal phishing detection approach, KnowPhish Detector (KPD), can detect phishing webpages with or without logos. We evaluate KnowPhish and KPD on a manually validated dataset, and on a field study under Singapore's local context, showing substantial improvements in effectiveness and efficiency compared to state-of-the-art baselines.","sentences":["Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches.","Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach.","However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base.","To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand.","KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner.","A second limitation of existing RBPDs is that they solely rely on the image modality, ignoring useful textual information present in the webpage HTML.","To utilize this textual information, we propose a Large Language Model (LLM)-based approach to extract brand information of webpages from text.","Our resulting multimodal phishing detection approach, KnowPhish Detector (KPD), can detect phishing webpages with or without logos.","We evaluate KnowPhish and KPD on a manually validated dataset, and on a field study under Singapore's local context, showing substantial improvements in effectiveness and efficiency compared to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2403.02253v1","category":"cs.CR"}
{"created":"2024-03-04 17:34:59","title":"Non-autoregressive Sequence-to-Sequence Vision-Language Models","abstract":"Sequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.","sentences":["Sequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions.","We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder.","This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model.","The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference."],"url":"http://arxiv.org/abs/2403.02249v1","category":"cs.CV"}
{"created":"2024-03-04 17:33:39","title":"Better Schedules for Low Precision Training of Deep Neural Networks","abstract":"Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout training according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine schedules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency and model performance, as well as derive a set of best practices for choosing CPT schedules. Going further, we find that a correlation exists between model performance and training cost, and that changing the underlying CPT schedule can control the tradeoff between these two variables. To explain the direct correlation between model performance and training cost, we draw a connection between quantized training and critical learning periods, suggesting that aggressive quantization is a form of learning impairment that can permanently damage model performance.","sentences":["Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs).","Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout training according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance.","Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine schedules) and use them for low precision training without adequate comparisons to alternative scheduling options.","We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks).","From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency and model performance, as well as derive a set of best practices for choosing CPT schedules.","Going further, we find that a correlation exists between model performance and training cost, and that changing the underlying CPT schedule can control the tradeoff between these two variables.","To explain the direct correlation between model performance and training cost, we draw a connection between quantized training and critical learning periods, suggesting that aggressive quantization is a form of learning impairment that can permanently damage model performance."],"url":"http://arxiv.org/abs/2403.02243v1","category":"cs.LG"}
{"created":"2024-03-04 17:33:20","title":"Neural Redshift: Random Networks are not Random Functions","abstract":"Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent \"simplicity bias\". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inherit all these properties from their building blocks.   Implications. We provide a fresh explanation for the success of deep learning independent from gradient-based training. It points at promising avenues for controlling the solutions implemented by trained models.","sentences":["Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete.","Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks.","This paper seeks other sources of generalization in NNs.   Findings.","To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks.","Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity.","But unlike common wisdom, NNs do not have an inherent \"simplicity bias\".","This property depends on components such as ReLUs, residual connections, and layer normalizations.","Alternative architectures can be built with a bias for any level of complexity.","Transformers also inherit all these properties from their building blocks.   Implications.","We provide a fresh explanation for the success of deep learning independent from gradient-based training.","It points at promising avenues for controlling the solutions implemented by trained models."],"url":"http://arxiv.org/abs/2403.02241v2","category":"cs.LG"}
{"created":"2024-03-04 17:29:57","title":"Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks","abstract":"The integration of Machine Learning and Artificial Intelligence (ML/AI) into fifth-generation (5G) networks has made evident the limitations of network intelligence with ever-increasing, strenuous requirements for current and next-generation devices. This transition to ubiquitous intelligence demands high connectivity, synchronicity, and end-to-end communication between users and network operators, and will pave the way towards full network automation without human intervention. Intent-based networking is a key factor in the reduction of human actions, roles, and responsibilities while shifting towards novel extraction and interpretation of automated network management. This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network intelligence.","sentences":["The integration of Machine Learning and Artificial Intelligence (ML/AI) into fifth-generation (5G) networks has made evident the limitations of network intelligence with ever-increasing, strenuous requirements for current and next-generation devices.","This transition to ubiquitous intelligence demands high connectivity, synchronicity, and end-to-end communication between users and network operators, and will pave the way towards full network automation without human intervention.","Intent-based networking is a key factor in the reduction of human actions, roles, and responsibilities while shifting towards novel extraction and interpretation of automated network management.","This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network intelligence."],"url":"http://arxiv.org/abs/2403.02238v1","category":"cs.NI"}
{"created":"2024-03-04 17:29:03","title":"Interpretable Models for Detecting and Monitoring Elevated Intracranial Pressure","abstract":"Detecting elevated intracranial pressure (ICP) is crucial in diagnosing and managing various neurological conditions. These fluctuations in pressure are transmitted to the optic nerve sheath (ONS), resulting in changes to its diameter, which can then be detected using ultrasound imaging devices. However, interpreting sonographic images of the ONS can be challenging. In this work, we propose two systems that actively monitor the ONS diameter throughout an ultrasound video and make a final prediction as to whether ICP is elevated. To construct our systems, we leverage subject matter expert (SME) guidance, structuring our processing pipeline according to their collection procedure, while also prioritizing interpretability and computational efficiency. We conduct a number of experiments, demonstrating that our proposed systems are able to outperform various baselines. One of our SMEs then manually validates our top system's performance, lending further credibility to our approach while demonstrating its potential utility in a clinical setting.","sentences":["Detecting elevated intracranial pressure (ICP) is crucial in diagnosing and managing various neurological conditions.","These fluctuations in pressure are transmitted to the optic nerve sheath (ONS), resulting in changes to its diameter, which can then be detected using ultrasound imaging devices.","However, interpreting sonographic images of the ONS can be challenging.","In this work, we propose two systems that actively monitor the ONS diameter throughout an ultrasound video and make a final prediction as to whether ICP is elevated.","To construct our systems, we leverage subject matter expert (SME) guidance, structuring our processing pipeline according to their collection procedure, while also prioritizing interpretability and computational efficiency.","We conduct a number of experiments, demonstrating that our proposed systems are able to outperform various baselines.","One of our SMEs then manually validates our top system's performance, lending further credibility to our approach while demonstrating its potential utility in a clinical setting."],"url":"http://arxiv.org/abs/2403.02236v1","category":"eess.IV"}
{"created":"2024-03-04 17:22:43","title":"Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection","abstract":"This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation to address the evolving nature of malware. This research contributes to ongoing discussions in cybersecurity and provides practical insights for developing more robust malware detection systems in the digital era.","sentences":["This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset.","The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively.","Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored.","Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance.","Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection.","The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation to address the evolving nature of malware.","This research contributes to ongoing discussions in cybersecurity and provides practical insights for developing more robust malware detection systems in the digital era."],"url":"http://arxiv.org/abs/2403.02232v1","category":"cs.CR"}
{"created":"2024-03-04 17:21:19","title":"CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking","abstract":"Automatic Compliance Checking (ACC) within the Architecture, Engineering, and Construction (AEC) sector necessitates automating the interpretation of building regulations to achieve its full potential. However, extracting information from textual rules to convert them to a machine-readable format has been a challenge due to the complexities associated with natural language and the limited resources that can support advanced machine-learning techniques. To address this challenge, we introduce CODE-ACCORD, a unique dataset compiled under the EU Horizon ACCORD project. CODE-ACCORD comprises 862 self-contained sentences extracted from the building regulations of England and Finland. Aligned with our core objective of facilitating information extraction from text for machine-readable rule generation, each sentence was annotated with entities and relations. Entities represent specific components such as \"window\" and \"smoke detectors\", while relations denote semantic associations between these entities, collectively capturing the conveyed ideas in natural language. We manually annotated all the sentences using a group of 12 annotators. Each sentence underwent annotations by multiple annotators and subsequently careful data curation to finalise annotations, ensuring their accuracy and reliability, thereby establishing the dataset as a solid ground truth. CODE-ACCORD offers a rich resource for diverse machine learning and natural language processing (NLP) related tasks in ACC, including text classification, entity recognition and relation extraction. To the best of our knowledge, this is the first entity and relation-annotated dataset in compliance checking, which is also publicly available.","sentences":["Automatic Compliance Checking (ACC) within the Architecture, Engineering, and Construction (AEC) sector necessitates automating the interpretation of building regulations to achieve its full potential.","However, extracting information from textual rules to convert them to a machine-readable format has been a challenge due to the complexities associated with natural language and the limited resources that can support advanced machine-learning techniques.","To address this challenge, we introduce CODE-ACCORD, a unique dataset compiled under the EU Horizon ACCORD project.","CODE-ACCORD comprises 862 self-contained sentences extracted from the building regulations of England and Finland.","Aligned with our core objective of facilitating information extraction from text for machine-readable rule generation, each sentence was annotated with entities and relations.","Entities represent specific components such as \"window\" and \"smoke detectors\", while relations denote semantic associations between these entities, collectively capturing the conveyed ideas in natural language.","We manually annotated all the sentences using a group of 12 annotators.","Each sentence underwent annotations by multiple annotators and subsequently careful data curation to finalise annotations, ensuring their accuracy and reliability, thereby establishing the dataset as a solid ground truth.","CODE-ACCORD offers a rich resource for diverse machine learning and natural language processing (NLP) related tasks in ACC, including text classification, entity recognition and relation extraction.","To the best of our knowledge, this is the first entity and relation-annotated dataset in compliance checking, which is also publicly available."],"url":"http://arxiv.org/abs/2403.02231v1","category":"cs.IR"}
{"created":"2024-03-05 18:58:22","title":"Out of Time Order Correlation of the Hubbard Model with Random Local Disorder","abstract":"The out of time order correlator (OTOC) serves as a powerful tool for investigating quantum information spreading and chaos in complex systems. We present a method employing non-equilibrium dynamical mean-field theory (DMFT) and coherent potential approximation (CPA) combined with diagrammatic perturbation on the Schwinger-Keldysh contour to calculate the OTOC for correlated fermionic systems subjected to both random disorder and electrons interaction. Our key finding is that random disorder enhances the OTOC decay in the Hubbard model for the metallic phase in the weak coupling limit. However, the current limitation of our perturbative solver restricts the applicability to weak interaction regimes.","sentences":["The out of time order correlator (OTOC) serves as a powerful tool for investigating quantum information spreading and chaos in complex systems.","We present a method employing non-equilibrium dynamical mean-field theory (DMFT) and coherent potential approximation (CPA) combined with diagrammatic perturbation on the Schwinger-Keldysh contour to calculate the OTOC for correlated fermionic systems subjected to both random disorder and electrons interaction.","Our key finding is that random disorder enhances the OTOC decay in the Hubbard model for the metallic phase in the weak coupling limit.","However, the current limitation of our perturbative solver restricts the applicability to weak interaction regimes."],"url":"http://arxiv.org/abs/2403.03214v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 18:39:01","title":"Quantum superpositions of current states in Rydberg-atom networks","abstract":"Quantum simulation of many-body quantum systems using Rydberg-atom platforms has become of extreme interest in the last years. The possibility to realize spin Hamiltonians and the accurate control at the single atom level paved the way for the study of quantum phases of matter and dynamics. Here, we propose a quantum optimal control protocol based on the GRAPE algorithm to engineer quantum current states. Besides current states characterized by a single winding number, our approach allow to access superposition of quantum current states. The single current states are eigenstates of the current operator that therefore can define an observable that remains persistent at any time. In particular, the features of the excitations dynamics reflects the nature of current states, a fact that in principle can be used to characterize the nature of the flow experimentally.","sentences":["Quantum simulation of many-body quantum systems using Rydberg-atom platforms has become of extreme interest in the last years.","The possibility to realize spin Hamiltonians and the accurate control at the single atom level paved the way for the study of quantum phases of matter and dynamics.","Here, we propose a quantum optimal control protocol based on the GRAPE algorithm to engineer quantum current states.","Besides current states characterized by a single winding number, our approach allow to access superposition of quantum current states.","The single current states are eigenstates of the current operator that therefore can define an observable that remains persistent at any time.","In particular, the features of the excitations dynamics reflects the nature of current states, a fact that in principle can be used to characterize the nature of the flow experimentally."],"url":"http://arxiv.org/abs/2403.03202v1","category":"quant-ph"}
{"created":"2024-03-05 18:33:32","title":"Metallic mean Wang tiles II: the dynamics of an aperiodic computer chip","abstract":"We consider a new family $(\\mathcal{T}_n)_{n\\geq1}$ of aperiodic sets of Wang tiles and we describe the dynamical properties of the set $\\Omega_n$ of valid configurations $\\mathbb{Z}^2\\to\\mathcal{T}_n$. The tiles can be defined as the different instances of a square shape computer chip whose inputs and outputs are 3-dimensional integer vectors. The family include the Ammann aperiodic set of 16 Wang tiles and gathers the hallmarks of other small aperiodic sets of Wang tiles. Notably, the tiles satisfy additive versions of equations verified by the Kari--Culik aperiodic sets of 14 and 13 Wang tiles. Also configurations in $\\Omega_n$ are the codings of a $\\mathbb{Z}^2$-action on a 2-dimensional torus like the Jeandel--Rao aperiodic set of 11 Wang tiles. The family broadens the relation between quadratic integers and aperiodic tilings beyond the omnipresent golden ratio as the dynamics of $\\Omega_n$ involves the positive root $\\beta$ of the polynomial $x^2-nx-1$, also known as the $n$-th metallic mean. We show the existence of an almost one-to-one factor map $\\Omega_n\\to\\mathbb{T}^2$ which commutes the shift action on $\\Omega_n$ with horizontal and vertical translations by $\\beta$ on $\\mathbb{T}^2$. The factor map can be explicitely defined by the average of the top labels from the same row of tiles as in Kari and Culik examples. The proofs are based on the minimality of $\\Omega_n$ (proved in a previous article) and a polygonal partition of $\\mathbb{T}^2$ which we show is a Markov partition for the toral $\\mathbb{Z}^2$-action. The partition and the sets of Wang tiles are symmetric which makes them, like Penrose tilings, worthy of investigation.","sentences":["We consider a new family $(\\mathcal{T}_n)_{n\\geq1}$ of aperiodic sets of Wang tiles and we describe the dynamical properties of the set $\\Omega_n$ of valid configurations $\\mathbb{Z}^2\\to\\mathcal{T}_n$. The tiles can be defined as the different instances of a square shape computer chip whose inputs and outputs are 3-dimensional integer vectors.","The family include the Ammann aperiodic set of 16 Wang tiles and gathers the hallmarks of other small aperiodic sets of Wang tiles.","Notably, the tiles satisfy additive versions of equations verified by the Kari--Culik aperiodic sets of 14 and 13 Wang tiles.","Also configurations in $\\Omega_n$ are the codings of a $\\mathbb{Z}^2$-action on a 2-dimensional torus like the Jeandel--Rao aperiodic set of 11 Wang tiles.","The family broadens the relation between quadratic integers and aperiodic tilings beyond the omnipresent golden ratio as the dynamics of $\\Omega_n$ involves the positive root $\\beta$ of the polynomial $x^2-nx-1$, also known as the $n$-th metallic mean.","We show the existence of an almost one-to-one factor map $\\Omega_n\\to\\mathbb{T}^2$ which commutes the shift action on $\\Omega_n$ with horizontal and vertical translations by $\\beta$ on $\\mathbb{T}^2$. The factor map can be explicitely defined by the average of the top labels from the same row of tiles as in Kari and Culik examples.","The proofs are based on the minimality of $\\Omega_n$ (proved in a previous article) and a polygonal partition of $\\mathbb{T}^2$ which we show is a Markov partition for the toral $\\mathbb{Z}^2$-action.","The partition and the sets of Wang tiles are symmetric which makes them, like Penrose tilings, worthy of investigation."],"url":"http://arxiv.org/abs/2403.03197v1","category":"math.DS"}
{"created":"2024-03-05 18:32:00","title":"SmartSantander: IoT Experimentation over a Smart City Testbed","abstract":"This paper describes the deployment and experimentation architecture of the Internet of Things experimentation facility being deployed at Santander city. The facility is implemented within the SmartSantander project, one of the projects of the Future Internet Research and Experimentation initiative of the European Commission and represents a unique in the world city-scale experimental research facility. Additionally, this facility supports typical applications and services of a smart city. Tangible results are expected to influence the definition and specification of Future Internet architecture design from viewpoints of Internet of Things and Internet of Services. The facility comprises a large number of Internet of Things devices deployed in several urban scenarios which will be federated into a single testbed. In this paper the deployment being carried out at the main location, namely Santander city, is described. Besides presenting the current deployment, in this article the main insights in terms of the architectural design of a large-scale IoT testbed are presented as well. Furthermore, solutions adopted for implementation of the different components addressing the required testbed functionalities are also sketched out. The IoT experimentation facility described in this paper is conceived to provide a suitable platform for large scale experimentation and evaluation of IoT concepts under real-life conditions.","sentences":["This paper describes the deployment and experimentation architecture of the Internet of Things experimentation facility being deployed at Santander city.","The facility is implemented within the SmartSantander project, one of the projects of the Future Internet Research and Experimentation initiative of the European Commission and represents a unique in the world city-scale experimental research facility.","Additionally, this facility supports typical applications and services of a smart city.","Tangible results are expected to influence the definition and specification of Future Internet architecture design from viewpoints of Internet of Things and Internet of Services.","The facility comprises a large number of Internet of Things devices deployed in several urban scenarios which will be federated into a single testbed.","In this paper the deployment being carried out at the main location, namely Santander city, is described.","Besides presenting the current deployment, in this article the main insights in terms of the architectural design of a large-scale IoT testbed are presented as well.","Furthermore, solutions adopted for implementation of the different components addressing the required testbed functionalities are also sketched out.","The IoT experimentation facility described in this paper is conceived to provide a suitable platform for large scale experimentation and evaluation of IoT concepts under real-life conditions."],"url":"http://arxiv.org/abs/2403.03196v1","category":"cs.NI"}
{"created":"2024-03-05 18:31:32","title":"Concentration-compactness via profile decomposition for systems of coupled Schr\u00f6dinger equations of Hamiltonian type","abstract":"We analyse Hamiltonian-type systems of second-order elliptic PDE invariant under a non-compact group and, consequently, involve a lack of compactness of the Sobolev embedding. We show that the loss of compactness can be compensated by using a concentration-compactness principle via weak profile decomposition for bounded Palais-Smale sequences in Banach spaces. Our analysis to prove the existence of ground states involves a reduction by the inversion method of the system to a fourth-order equation combined with a variational principle of a minimax nature. Among other results, including regularity and a Pohozaev-type identity, we also prove the non-existence of weak solutions for a class of Lane-Emden systems.","sentences":["We analyse Hamiltonian-type systems of second-order elliptic PDE invariant under a non-compact group and, consequently, involve a lack of compactness of the Sobolev embedding.","We show that the loss of compactness can be compensated by using a concentration-compactness principle via weak profile decomposition for bounded Palais-Smale sequences in Banach spaces.","Our analysis to prove the existence of ground states involves a reduction by the inversion method of the system to a fourth-order equation combined with a variational principle of a minimax nature.","Among other results, including regularity and a Pohozaev-type identity, we also prove the non-existence of weak solutions for a class of Lane-Emden systems."],"url":"http://arxiv.org/abs/2403.03195v1","category":"math.AP"}
{"created":"2024-03-05 18:29:34","title":"GA-NIFS: NIRSpec reveals evidence for non-circular motions and AGN feedback in GN20","abstract":"We present rest-frame optical data of the z~4 sub-millimeter galaxy GN20 obtained with JWST/NIRSpec in integral field spectroscopy (IFS) mode. The H$\\alpha$ emission is asymmetric and clumpy and extends over a projected distance of more than 15 kpc. To first order, the large-scale ionised gas kinematics are consistent with a turbulent ($\\sigma\\sim90$ km/s), rotating disc ($v_{\\rm rot}\\sim500$ km/s), congruent with previous studies of its molecular and ionised gas kinematics. However, we also find clear evidence for non-circular motions in the H$\\alpha$ kinematics. We discuss their possible connection with various scenarios, such as external perturbations, accretion or radial flows. In the centre of GN20, we find broad line emission (FWHM $\\sim1000-2000$ km/s) in the H$\\alpha$+[N II] complex, suggestive of fast, AGN-driven winds or, alternatively, of the broad-line region of an active black hole. Elevated values of [N II]$\\lambda6583$/H$\\alpha>0.4$ and EW(H$\\alpha)>6$ \\r{A}, throughout large parts of GN20 suggest that feedback from the active black hole is able to photo-ionise the interstellar medium. Our data corroborates that GN20 offers a unique opportunity to observe key processes in the evolution of the most massive present-day galaxies acting in concert, over 12 billion years ago.","sentences":["We present rest-frame optical data of the z~4 sub-millimeter galaxy GN20 obtained with JWST/NIRSpec in integral field spectroscopy (IFS) mode.","The H$\\alpha$ emission is asymmetric and clumpy and extends over a projected distance of more than 15 kpc.","To first order, the large-scale ionised gas kinematics are consistent with a turbulent ($\\sigma\\sim90$ km/s), rotating disc ($v_{\\rm rot}\\sim500$ km/s), congruent with previous studies of its molecular and ionised gas kinematics.","However, we also find clear evidence for non-circular motions in the H$\\alpha$ kinematics.","We discuss their possible connection with various scenarios, such as external perturbations, accretion or radial flows.","In the centre of GN20, we find broad line emission (FWHM $\\sim1000-2000$ km/s) in the H$\\alpha$+[N II] complex, suggestive of fast, AGN-driven winds or, alternatively, of the broad-line region of an active black hole.","Elevated values of [N II]$\\lambda6583$/H$\\alpha>0.4$ and EW(H$\\alpha)>6$ \\r{A}, throughout large parts of GN20 suggest that feedback from the active black hole is able to photo-ionise the interstellar medium.","Our data corroborates that GN20 offers a unique opportunity to observe key processes in the evolution of the most massive present-day galaxies acting in concert, over 12 billion years ago."],"url":"http://arxiv.org/abs/2403.03192v1","category":"astro-ph.GA"}
{"created":"2024-03-05 18:19:55","title":"On the computation of stable coupled state-space models for dynamic substructuring applications","abstract":"This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons's second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models. The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model). Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model. Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles. The validity of the proposed methodologies is presented and discussed by exploiting experimental data. Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton's second law are computed. Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable. Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper. The methodology proved to work well on these data. Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and simulations.","sentences":["This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons's second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models.","The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model).","Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model.","Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles.","The validity of the proposed methodologies is presented and discussed by exploiting experimental data.","Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton's second law are computed.","Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable.","Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper.","The methodology proved to work well on these data.","Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and simulations."],"url":"http://arxiv.org/abs/2403.03182v1","category":"eess.SY"}
{"created":"2024-03-05 18:09:05","title":"Remote sensing of soil moisture using Rydberg atoms and satellite signals of opportunity","abstract":"Spaceborne radar remote sensing of the earth system is essential to study natural and man-made changes in the ecosystem, water and energy cycles, weather and air quality, sea level, and surface dynamics. A major challenge with current approaches is the lack of broad spectrum tunability due to narrow band microwave electronics, that limit systems to specific science variable retrievals. This results in a significant limitation in studying dynamic coupled earth system processes such as surface and subsurface hydrology, where broad spectrum radar remote sensing is needed to sense multiple variables simultaneously. Rydberg atomic sensors are highly sensitive broad-spectrum quantum detectors that can be dynamically tuned to cover micro-to-millimeter waves with no requirement for band-specific electronics. Rydberg atomic sensors can use existing transmitted signals such as navigation and communication satellites to enable remote sensing. We demonstrate remote sensing of soil moisture, an important earth system variable, via ground-based radar reflectometry with Rydberg atomic systems. To do this, we sensitize the atoms to XM satellite radio signals and use signal correlations to demonstrate use of these satellite signals for remote sensing of soil moisture. Our approach provides a step towards satellite-based broad-spectrum Rydberg atomic remote sensing.","sentences":["Spaceborne radar remote sensing of the earth system is essential to study natural and man-made changes in the ecosystem, water and energy cycles, weather and air quality, sea level, and surface dynamics.","A major challenge with current approaches is the lack of broad spectrum tunability due to narrow band microwave electronics, that limit systems to specific science variable retrievals.","This results in a significant limitation in studying dynamic coupled earth system processes such as surface and subsurface hydrology, where broad spectrum radar remote sensing is needed to sense multiple variables simultaneously.","Rydberg atomic sensors are highly sensitive broad-spectrum quantum detectors that can be dynamically tuned to cover micro-to-millimeter waves with no requirement for band-specific electronics.","Rydberg atomic sensors can use existing transmitted signals such as navigation and communication satellites to enable remote sensing.","We demonstrate remote sensing of soil moisture, an important earth system variable, via ground-based radar reflectometry with Rydberg atomic systems.","To do this, we sensitize the atoms to XM satellite radio signals and use signal correlations to demonstrate use of these satellite signals for remote sensing of soil moisture.","Our approach provides a step towards satellite-based broad-spectrum Rydberg atomic remote sensing."],"url":"http://arxiv.org/abs/2403.03175v1","category":"physics.app-ph"}
{"created":"2024-03-05 18:08:29","title":"Solving the bongard-logo problem by modeling a probabilistic model","abstract":"Abstract reasoning problems challenge the perceptual and cognitive abilities of AI algorithms, demanding deeper pattern discernment and inductive reasoning beyond explicit image features. This study introduces PMoC, a tailored probability model for the Bongard-Logo problem, achieving high reasoning accuracy by constructing independent probability models. Additionally, we present Pose-Transformer, an enhanced Transformer-Encoder designed for complex abstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM. Pose-Transformer incorporates positional information learning, inspired by capsule networks' pose matrices, enhancing its focus on local positional relationships in image data processing. When integrated with PMoC, it further improves reasoning accuracy. Our approach effectively addresses reasoning difficulties associated with abstract entities' positional changes, outperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM databases. This research contributes to advancing AI's capabilities in abstract reasoning and cognitive pattern recognition.","sentences":["Abstract reasoning problems challenge the perceptual and cognitive abilities of AI algorithms, demanding deeper pattern discernment and inductive reasoning beyond explicit image features.","This study introduces PMoC, a tailored probability model for the Bongard-Logo problem, achieving high reasoning accuracy by constructing independent probability models.","Additionally, we present Pose-Transformer, an enhanced Transformer-Encoder designed for complex abstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.","Pose-Transformer incorporates positional information learning, inspired by capsule networks' pose matrices, enhancing its focus on local positional relationships in image data processing.","When integrated with PMoC, it further improves reasoning accuracy.","Our approach effectively addresses reasoning difficulties associated with abstract entities' positional changes, outperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM databases.","This research contributes to advancing AI's capabilities in abstract reasoning and cognitive pattern recognition."],"url":"http://arxiv.org/abs/2403.03173v1","category":"cs.CV"}
{"created":"2024-03-05 18:03:57","title":"Dynamical decoding of the competition between charge density waves in a kagome superconductor","abstract":"The kagome superconductor CsV$_3$Sb$_5$ hosts a variety of charge density wave (CDW) phases, which play a fundamental role in the formation of other exotic electronic instabilities. However, identifying the precise structure of these CDW phases and their intricate relationships remain the subject of intense debate, due to the lack of static probes that can distinguish the CDW phases with identical spatial periodicity. Here, we unveil the competition between two coexisting $2\\times2\\times2$ CDWs in CsV$_3$Sb$_5$ harnessing time-resolved X-ray diffraction. By analyzing the light-induced changes in the intensity of CDW superlattice peaks, we demonstrate the presence of both phases, each displaying a significantly different amount of melting upon excitation. The anomalous light-induced sharpening of peak width further shows that the phase that is more resistant to photo-excitation exhibits an increase in domain size at the expense of the other, thereby showcasing a hallmark of phase competition. Our results not only shed light on the interplay between the multiple CDW phases in CsV$_3$Sb$_5$, but also establish a non-equilibrium framework for comprehending complex phase relationships that are challenging to disentangle using static techniques.","sentences":["The kagome superconductor CsV$_3$Sb$_5$ hosts a variety of charge density wave (CDW) phases, which play a fundamental role in the formation of other exotic electronic instabilities.","However, identifying the precise structure of these CDW phases and their intricate relationships remain the subject of intense debate, due to the lack of static probes that can distinguish the CDW phases with identical spatial periodicity.","Here, we unveil the competition between two coexisting $2\\times2\\times2$ CDWs in CsV$_3$Sb$_5$ harnessing time-resolved X-ray diffraction.","By analyzing the light-induced changes in the intensity of CDW superlattice peaks, we demonstrate the presence of both phases, each displaying a significantly different amount of melting upon excitation.","The anomalous light-induced sharpening of peak width further shows that the phase that is more resistant to photo-excitation exhibits an increase in domain size at the expense of the other, thereby showcasing a hallmark of phase competition.","Our results not only shed light on the interplay between the multiple CDW phases in CsV$_3$Sb$_5$, but also establish a non-equilibrium framework for comprehending complex phase relationships that are challenging to disentangle using static techniques."],"url":"http://arxiv.org/abs/2403.03169v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 17:58:32","title":"Hybrid data assimilation techniques using the adjoint method in a coupled Lorenz system","abstract":"A hybrid 4D-variational data assimilation method for numerical climate models is introduced using the Lorenz '63 model. This new approach has the potential to optimise a high complexity Earth system model (ESM) by utilising the adjoint equations of an intermediate complexity ESM. The method is conceptually demonstrated by consecutively synchronising two Lorenz '63 systems to observations before optimisation. The first represents a 'high complexity' model and the second an 'intermediate complexity' model which has adjoint equations. This method will save computational power for a full ESM and has negligible error and uncertainty change compared to the optimisation of a single model with adjoint equations. A similar setup can be applied to sparse observations. An alternative assimilation setup, with two identical models, is used to filter noisy data. This reduces optimised parametric model uncertainty by approximately one third. Such a precision gain could prove valuable for seasonal, annual, and decadal predictions.","sentences":["A hybrid 4D-variational data assimilation method for numerical climate models is introduced using the Lorenz '63 model.","This new approach has the potential to optimise a high complexity Earth system model (ESM) by utilising the adjoint equations of an intermediate complexity ESM.","The method is conceptually demonstrated by consecutively synchronising two Lorenz '63 systems to observations before optimisation.","The first represents a 'high complexity' model and the second an 'intermediate complexity' model which has adjoint equations.","This method will save computational power for a full ESM and has negligible error and uncertainty change compared to the optimisation of a single model with adjoint equations.","A similar setup can be applied to sparse observations.","An alternative assimilation setup, with two identical models, is used to filter noisy data.","This reduces optimised parametric model uncertainty by approximately one third.","Such a precision gain could prove valuable for seasonal, annual, and decadal predictions."],"url":"http://arxiv.org/abs/2403.03166v1","category":"physics.ao-ph"}
{"created":"2024-03-05 17:48:43","title":"Four-band effective square lattice model for Bernal-stacked bilayer graphene","abstract":"Bernal-stacked bilayer graphene (BLG) provides an ideal basis for gate-controlled, and free of etching, electronic devices. Theoretical modeling of realistic devices is an essential part of research, however, simulations of large-scale BLG devices continue to be extremely challenging. Micrometer-sized systems are predominantly beyond the reach of the commonly used atomistic tight-binding method, while other numerical approaches based on the two dimensional Dirac equation are not straightforward to conduct due to the fermion doubling problem. Here we present an approach based on the continuum model, unharmed by the fermion doubling. The discretization of the BLG continuum Hamiltonian leads to an effective four-band model, with both valleys built-in. We demonstrate its performance with realistic, large-scale systems, and obtain results consistent with experiments and with the tight-binding model, over a broad range of magnetic field.","sentences":["Bernal-stacked bilayer graphene (BLG) provides an ideal basis for gate-controlled, and free of etching, electronic devices.","Theoretical modeling of realistic devices is an essential part of research, however, simulations of large-scale BLG devices continue to be extremely challenging.","Micrometer-sized systems are predominantly beyond the reach of the commonly used atomistic tight-binding method, while other numerical approaches based on the two dimensional Dirac equation are not straightforward to conduct due to the fermion doubling problem.","Here we present an approach based on the continuum model, unharmed by the fermion doubling.","The discretization of the BLG continuum Hamiltonian leads to an effective four-band model, with both valleys built-in.","We demonstrate its performance with realistic, large-scale systems, and obtain results consistent with experiments and with the tight-binding model, over a broad range of magnetic field."],"url":"http://arxiv.org/abs/2403.03155v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 17:26:42","title":"Characterization of a novel time-resolved, real-time scintillation dosimetry system for ultra-high dose rate radiation therapy applications","abstract":"Background: Scintillation dosimetry has promising qualities for ultra-high dose rate (UHDR) radiotherapy (RT), but no system has shown compatibility with mean dose rates ($\\bar{DR}$) above 100 Gy/s and doses per pulse ($D_p$) exceeding 1.5 Gy typical of UHDR (FLASH)-RT. The aim of this study was to characterize a novel scintillator dosimetry system with the potential of accommodating UHDRs. Methods: A thorough dosimetric characterization of the system was performed on an UHDR electron beamline. The system's response as a function of dose, $\\bar{DR}$, $D_p$, and the pulse dose rate ${DR}_p$ was investigated, together with the system's dose sensitivity (signal per unit dose) as a function of dose history. The capabilities of the system for time-resolved dosimetric readout were also evaluated. Results: Within a tolerance of $\\pm$3% the system exhibited dose linearity and was independent of $\\bar{DR}$ and $D_p$ within the tested ranges of 1.8-1341 Gy/s and 0.005-7.68 Gy, respectively. A 6% reduction in the signal per unit dose was observed as ${DR}_p$ was increased from 8.9e4-1.8e6 Gy/s. Additionally, the dose delivered per integration window of the continuously sampling photodetector had to remain between 0.028 and 11.64 Gy to preserve a stable signal response per unit dose. The system accurately measured $D_p$ of individual pulses delivered at up to 120 Hz. The day-to-day variation of the signal per unit dose at a reference setup varied by up to $\\pm$13% but remained consistent (<$\\pm$2%) within each day of measurements and showed no signal loss as a function of dose history. Conclusions: With daily calibrations and ${DR}_p$ specific correction factors, the system reliably provides real-time, millisecond-resolved dosimetric measurements of pulsed conventional and UHDR beams from typical electron linacs, marking an important advancement in UHDR dosimetry.","sentences":["Background: Scintillation dosimetry has promising qualities for ultra-high dose rate (UHDR) radiotherapy (RT), but no system has shown compatibility with mean dose rates ($\\bar{DR}$) above 100 Gy/s and doses per pulse ($D_p$) exceeding 1.5 Gy typical of UHDR (FLASH)-RT.","The aim of this study was to characterize a novel scintillator dosimetry system with the potential of accommodating UHDRs.","Methods: A thorough dosimetric characterization of the system was performed on an UHDR electron beamline.","The system's response as a function of dose, $\\bar{DR}$, $D_p$, and the pulse dose rate ${DR}_p$ was investigated, together with the system's dose sensitivity (signal per unit dose) as a function of dose history.","The capabilities of the system for time-resolved dosimetric readout were also evaluated.","Results:","Within a tolerance of $\\pm$3% the system exhibited dose linearity and was independent of $\\bar{DR}$ and $D_p$ within the tested ranges of 1.8-1341 Gy/s and 0.005-7.68 Gy, respectively.","A 6% reduction in the signal per unit dose was observed as ${DR}_p$ was increased from 8.9e4-1.8e6 Gy/s. Additionally, the dose delivered per integration window of the continuously sampling photodetector had to remain between 0.028 and 11.64 Gy to preserve a stable signal response per unit dose.","The system accurately measured $D_p$ of individual pulses delivered at up to 120 Hz.","The day-to-day variation of the signal per unit dose at a reference setup varied by up to $\\pm$13% but remained consistent (<$\\pm$2%) within each day of measurements and showed no signal loss as a function of dose history.","Conclusions: With daily calibrations and ${DR}_p$ specific correction factors, the system reliably provides real-time, millisecond-resolved dosimetric measurements of pulsed conventional and UHDR beams from typical electron linacs, marking an important advancement in UHDR dosimetry."],"url":"http://arxiv.org/abs/2403.03142v1","category":"physics.med-ph"}
{"created":"2024-03-05 17:26:41","title":"Language Guided Exploration for RL Agents in Text Environments","abstract":"Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.","sentences":["Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents.","Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts.","In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER).","We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer."],"url":"http://arxiv.org/abs/2403.03141v1","category":"cs.CL"}
{"created":"2024-03-05 17:12:42","title":"A Federated Deep Learning Approach for Privacy-Preserving Real-Time Transient Stability Predictions in Power Systems","abstract":"Maintaining the privacy of power system data is essential for protecting sensitive information and ensuring the operation security of critical infrastructure. Therefore, the adoption of centralized deep learning (DL) transient stability assessment (TSA) frameworks can introduce risks to electric utilities. This is because these frameworks make utility data susceptible to cyber threats and communication issues when transmitting data to a central server for training a single TSA model. Additionally, the centralized approach demands significant computational resources, which may not always be readily available. In light of these challenges, this paper introduces a federated DL-based TSA framework designed to identify the operating states of the power system. Instead of local utilities transmitting their data to a central server for centralized model training, they independently train their own TSA models using their respective datasets. Subsequently, the parameters of each local TSA model are sent to a central server for model aggregation, and the resulting model is shared back with the local clients. This approach not only preserves the integrity of local utility data, making it resilient against cyber threats but also reduces the computational demands for local TSA model training. The proposed approach is tested on four local clients each having the IEEE 39-bus test system.","sentences":["Maintaining the privacy of power system data is essential for protecting sensitive information and ensuring the operation security of critical infrastructure.","Therefore, the adoption of centralized deep learning (DL) transient stability assessment (TSA) frameworks can introduce risks to electric utilities.","This is because these frameworks make utility data susceptible to cyber threats and communication issues when transmitting data to a central server for training a single TSA model.","Additionally, the centralized approach demands significant computational resources, which may not always be readily available.","In light of these challenges, this paper introduces a federated DL-based TSA framework designed to identify the operating states of the power system.","Instead of local utilities transmitting their data to a central server for centralized model training, they independently train their own TSA models using their respective datasets.","Subsequently, the parameters of each local TSA model are sent to a central server for model aggregation, and the resulting model is shared back with the local clients.","This approach not only preserves the integrity of local utility data, making it resilient against cyber threats but also reduces the computational demands for local TSA model training.","The proposed approach is tested on four local clients each having the IEEE 39-bus test system."],"url":"http://arxiv.org/abs/2403.03126v1","category":"eess.SY"}
{"created":"2024-03-05 17:07:55","title":"Testbeam analysis of biasing structures for irradiated hybrid pixel detectors","abstract":"Following the Phase-II upgrade during Long Shutdown (LS3), the LHC aims to reach a peak instantaneous luminosity of $7.5\\times 10^{34}$cm$^{-2}$s$^{-1}$, which corresponds to an average of around 200 inelastic proton-proton collisions per beam-crossing (every 25 ns). To cope with these conditions, the ATLAS Inner Detector will be replaced by a new all-silicon system -- the Inner Tracker (ITk). The ITk will be operational for more than ten years, during which time ATLAS is expected to record approximately 4000 fb$^{-1}$ of data. The ITk's pixel sub-system is based on hybrid pixel modules with new silicon sensors and readout chips. These studies focus on testbeam campaigns undertaken to study the spatial resolution and efficiencies of hybrid pixel detector modules based on the first large-structure prototype front-end readout chip -- the RD53A -- using planar silicon sensors. These devices have been irradiated to replicate the effect of the high radiation environment present during operation in the ATLAS detector. Results for devices using sensors with different punch-through bias structures and using different readout chips are summarised. Those with sensors incorporating a punch-through bias structure are found to exhibit systematically lower efficiency than those without, as a result of local areas of relative inefficiency around the punch-through dots. Despite this, all devices measured are found to satisfy the requirement of 97% efficiency at $V_\\mathrm{bias}=400$ V after being irradiated to end-of-life fluence.","sentences":["Following the Phase-II upgrade during Long Shutdown (LS3), the LHC aims to reach a peak instantaneous luminosity of $7.5\\times 10^{34}$cm$^{-2}$s$^{-1}$, which corresponds to an average of around 200 inelastic proton-proton collisions per beam-crossing (every 25 ns).","To cope with these conditions, the ATLAS Inner Detector will be replaced by a new all-silicon system -- the Inner Tracker (ITk).","The ITk will be operational for more than ten years, during which time ATLAS is expected to record approximately 4000 fb$^{-1}$ of data.","The ITk's pixel sub-system is based on hybrid pixel modules with new silicon sensors and readout chips.","These studies focus on testbeam campaigns undertaken to study the spatial resolution and efficiencies of hybrid pixel detector modules based on the first large-structure prototype front-end readout chip -- the RD53A -- using planar silicon sensors.","These devices have been irradiated to replicate the effect of the high radiation environment present during operation in the ATLAS detector.","Results for devices using sensors with different punch-through bias structures and using different readout chips are summarised.","Those with sensors incorporating a punch-through bias structure are found to exhibit systematically lower efficiency than those without, as a result of local areas of relative inefficiency around the punch-through dots.","Despite this, all devices measured are found to satisfy the requirement of 97% efficiency at $V_\\mathrm{bias}=400$ V after being irradiated to end-of-life fluence."],"url":"http://arxiv.org/abs/2403.03124v1","category":"physics.ins-det"}
{"created":"2024-03-05 16:58:33","title":"Exploring Quantum Phases of Dipolar Gases through Quasicrystalline Confinement","abstract":"The effects of frustration on extended supersolid states is a largely unexplored subject in the realm of cold-atom systems. In this work, we explore the impact of quasicrystalline lattices on the supersolid phases of dipolar bosons. Our findings reveal that weak quasicrystalline lattices can induce a variety of modulated phases, merging the inherent solid pattern with a quasiperiodic decoration induced by the external potential. As the lattice becomes stronger, we observe a super quasicrystal phase and a Bose glass phase. Our results, supported by a detailed discussion on experimental feasibility using dysprosium atoms and quasicrystalline optical lattice potentials, open a new avenue in the exploration of long-range interacting quantum systems in aperiodic environments. We provide a solid foundation for future experimental investigations, potentially confirming our theoretical predictions and contributing profoundly to the field of quantum gases in complex external potentials.","sentences":["The effects of frustration on extended supersolid states is a largely unexplored subject in the realm of cold-atom systems.","In this work, we explore the impact of quasicrystalline lattices on the supersolid phases of dipolar bosons.","Our findings reveal that weak quasicrystalline lattices can induce a variety of modulated phases, merging the inherent solid pattern with a quasiperiodic decoration induced by the external potential.","As the lattice becomes stronger, we observe a super quasicrystal phase and a Bose glass phase.","Our results, supported by a detailed discussion on experimental feasibility using dysprosium atoms and quasicrystalline optical lattice potentials, open a new avenue in the exploration of long-range interacting quantum systems in aperiodic environments.","We provide a solid foundation for future experimental investigations, potentially confirming our theoretical predictions and contributing profoundly to the field of quantum gases in complex external potentials."],"url":"http://arxiv.org/abs/2403.03118v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-05 16:58:13","title":"Necessary and Sufficient Conditions to the Problem of Input-Output Extension of Internally Controlled Underactuated Nonlinear Systems","abstract":"In this letter, we address the problem of re-targeting a commercial under-actuated robotic system to a higher dimensional output task. Commercial platforms are equipped with an on-board low-level internal controller that provides the user some virtual references inputs and cannot be replaced. Such controller may control in practice only a subset of the total degrees-of-freedom of the system. Our primary objective is to augment these systems by introducing supplementary inputs, attaining increased output capability. Integrating such additional inputs into the control framework introduces a layer of complexity, raising questions about the compatibility and cohesiveness of the overall system. We derive necessary and sufficient conditions under which it is possible to extend the controlled outputs by adding extra inputs when one is forced to keep the internal controller unmodified. We underscore the efficacy and universality of our proposed methodology through the presentation of two relevant examples.","sentences":["In this letter, we address the problem of re-targeting a commercial under-actuated robotic system to a higher dimensional output task.","Commercial platforms are equipped with an on-board low-level internal controller that provides the user some virtual references inputs and cannot be replaced.","Such controller may control in practice only a subset of the total degrees-of-freedom of the system.","Our primary objective is to augment these systems by introducing supplementary inputs, attaining increased output capability.","Integrating such additional inputs into the control framework introduces a layer of complexity, raising questions about the compatibility and cohesiveness of the overall system.","We derive necessary and sufficient conditions under which it is possible to extend the controlled outputs by adding extra inputs when one is forced to keep the internal controller unmodified.","We underscore the efficacy and universality of our proposed methodology through the presentation of two relevant examples."],"url":"http://arxiv.org/abs/2403.03117v1","category":"eess.SY"}
{"created":"2024-03-05 16:51:59","title":"Charge-order melting in the one-dimensional Edwards model","abstract":"We use infinite matrix-product-state techniques to study the time evolution of the charge-density-wave (CDW) order after a quench or a light pulse in a fundamental fermion-boson model. The motion of fermions in the model is linked to the creation of bosonic excitations, which counteracts the melting of the CDW order. For low-energy quenches corresponding to a change of the boson relaxation rate, we find behavior similar to that in an effective $t$-$V$ model. When the boson energy is quenched instead or a light pulse is applied to the system, the transient dynamics are more complex, with the CDW order first quickly decreasing to an intermediate value while the density-wave-like order of the bosons rises. In the case of pulse irradiation, the subsequent time-evolution of the CDW order depends strongly on the photon frequency. For frequencies slightly below the boson energy, we observe a temporary increase of the CDW order parameter. Our results reveal the complex physics of driven Mott insulators in low-dimensional systems with strong correlations.","sentences":["We use infinite matrix-product-state techniques to study the time evolution of the charge-density-wave (CDW) order after a quench or a light pulse in a fundamental fermion-boson model.","The motion of fermions in the model is linked to the creation of bosonic excitations, which counteracts the melting of the CDW order.","For low-energy quenches corresponding to a change of the boson relaxation rate, we find behavior similar to that in an effective $t$-$V$ model.","When the boson energy is quenched instead or a light pulse is applied to the system, the transient dynamics are more complex, with the CDW order first quickly decreasing to an intermediate value while the density-wave-like order of the bosons rises.","In the case of pulse irradiation, the subsequent time-evolution of the CDW order depends strongly on the photon frequency.","For frequencies slightly below the boson energy, we observe a temporary increase of the CDW order parameter.","Our results reveal the complex physics of driven Mott insulators in low-dimensional systems with strong correlations."],"url":"http://arxiv.org/abs/2403.03108v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 16:13:57","title":"An Empirical Calibration of the Tip of the Red Giant Branch Distance Method in the Near Infrared. I. HST WFC3/IR F110W and F160W Filters","abstract":"The Tip of the Red Giant Branch (TRGB)-based distance method in the I band is one of the most efficient and precise techniques for measuring distances to nearby galaxies (D <= 15 Mpc). The TRGB in the near infrared (NIR) is 1 to 2 magnitudes brighter relative to the I band, and has the potential to expand the range over which distance measurements to nearby galaxies are feasible. Using Hubble Space Telescope (HST) imaging of 12 fields in 8 nearby galaxies, we determine color-based corrections and zero points of the TRGB in the Wide Field Camera 3 IR (WFC3/IR) F110W and F160W filters. First, we measure TRGB distances in the I band equivalent Advanced Camera System (ACS) F814W filter from resolved stellar populations with the HST. The TRGB in the ACS F814W filter is used for our distance anchor and to place the WFC3/IR magnitudes on an absolute scale. We then determine the color dependence (a proxy for metallicity/age) and zero point of the NIR TRGB from photometry of WFC3/IR fields which overlap with the ACS fields. The new calibration is accurate to ~1% in distance, relative to the F814W TRGB. Validating the accuracy of the calibrations, we find that the distance modulus for each field using the NIR TRGB calibration agrees with the distance modulus of the same fields as determined from the F814W TRGB. This is a JWST preparatory program and the work done here will directly inform our approach to calibrating the TRGB in JWST NIRCam and NIRISS photometric filters.","sentences":["The Tip of the Red Giant Branch (TRGB)-based distance method in the I band is one of the most efficient and precise techniques for measuring distances to nearby galaxies (D <= 15 Mpc).","The TRGB in the near infrared (NIR) is 1 to 2 magnitudes brighter relative to the I band, and has the potential to expand the range over which distance measurements to nearby galaxies are feasible.","Using Hubble Space Telescope (HST) imaging of 12 fields in 8 nearby galaxies, we determine color-based corrections and zero points of the TRGB in the Wide Field Camera 3 IR (WFC3/IR) F110W and F160W filters.","First, we measure TRGB distances in the I band equivalent Advanced Camera System (ACS)","F814W","filter from resolved stellar populations with the HST.","The TRGB in the ACS F814W filter is used for our distance anchor and to place the WFC3/IR magnitudes on an absolute scale.","We then determine the color dependence (a proxy for metallicity/age) and zero point of the NIR TRGB from photometry of WFC3/IR fields which overlap with the ACS fields.","The new calibration is accurate to ~1% in distance, relative to the F814W TRGB.","Validating the accuracy of the calibrations, we find that the distance modulus for each field using the NIR TRGB calibration agrees with the distance modulus of the same fields as determined from the F814W TRGB.","This is a JWST preparatory program and the work done here will directly inform our approach to calibrating the TRGB in JWST NIRCam and NIRISS photometric filters."],"url":"http://arxiv.org/abs/2403.03086v1","category":"astro-ph.GA"}
{"created":"2024-03-05 16:05:02","title":"Demonstrating efficient and robust bosonic state reconstruction via optimized excitation counting","abstract":"Quantum state reconstruction is an essential element in quantum information processing. However, efficient and reliable reconstruction of non-trivial quantum states in the presence of hardware imperfections can be challenging. This task is particularly demanding for high-dimensional states encoded in continuous-variable (CV) systems, as many error-prone measurements are needed to cover the relevant degrees of freedom of the system in phase space. In this work, we introduce an efficient and robust technique for optimized reconstruction based on excitation number sampling (ORENS). We use a standard bosonic circuit quantum electrodynamics (cQED) setup to experimentally demonstrate the robustness of ORENS and show that it outperforms the existing cQED reconstruction techniques such as Wigner tomography and Husimi Q-function. Our investigation highlights that ORENS is naturally free of parasitic system dynamics and resilient to decoherence effects in the hardware. Finally, ORENS relies only on the ability to accurately measure the excitation number of the state, making it a versatile and accessible tool for a wide range of CV platforms and readily scalable to multimode systems. Thus, our work provides a crucial and valuable primitive for practical quantum information processing using bosonic modes.","sentences":["Quantum state reconstruction is an essential element in quantum information processing.","However, efficient and reliable reconstruction of non-trivial quantum states in the presence of hardware imperfections can be challenging.","This task is particularly demanding for high-dimensional states encoded in continuous-variable (CV) systems, as many error-prone measurements are needed to cover the relevant degrees of freedom of the system in phase space.","In this work, we introduce an efficient and robust technique for optimized reconstruction based on excitation number sampling (ORENS).","We use a standard bosonic circuit quantum electrodynamics (cQED) setup to experimentally demonstrate the robustness of ORENS and show that it outperforms the existing cQED reconstruction techniques such as Wigner tomography and Husimi Q-function.","Our investigation highlights that ORENS is naturally free of parasitic system dynamics and resilient to decoherence effects in the hardware.","Finally, ORENS relies only on the ability to accurately measure the excitation number of the state, making it a versatile and accessible tool for a wide range of CV platforms and readily scalable to multimode systems.","Thus, our work provides a crucial and valuable primitive for practical quantum information processing using bosonic modes."],"url":"http://arxiv.org/abs/2403.03080v1","category":"quant-ph"}
{"created":"2024-03-05 16:01:55","title":"MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding","abstract":"3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.   The source code and additional resources for this project are available on GitHub: https://github.com/birdy666/MiKASA-3DVG","sentences":["3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces.","Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent.","In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer.","Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships.","Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis.","Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.   ","The source code and additional resources for this project are available on GitHub: https://github.com/birdy666/MiKASA-3DVG"],"url":"http://arxiv.org/abs/2403.03077v1","category":"cs.CV"}
{"created":"2024-03-05 16:01:09","title":"Detecting Concrete Visual Tokens for Multimodal Machine Translation","abstract":"The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.","sentences":["The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking.","We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique.","We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens.","We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model."],"url":"http://arxiv.org/abs/2403.03075v1","category":"cs.CL"}
{"created":"2024-03-05 15:57:52","title":"Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families","abstract":"We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions. We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data.","sentences":["We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete.","We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case.","The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions.","We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity.","Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data."],"url":"http://arxiv.org/abs/2403.03069v1","category":"cs.LG"}
{"created":"2024-03-05 15:56:32","title":"Tracking-in-range Formulations for Numerical Optimal Control","abstract":"In contrast to set-point tracking which aims to reduce the tracking error between the tracker and the reference, tracking-in-range problems only focus on whether the tracker is within a given range around the reference, making it more suitable for the mission specifications of many practical applications. In this work, we present novel optimal control formulations to solve tracking-in-range problems, for both problems requiring the tracker to be always in range, and problems allowing the tracker to go out of range to yield overall better outcomes. As the problem naturally involves discontinuous functions, we present alternative formulations and regularisation strategies to improve the performance of numerical solvers. The extension to in-range tracking with multiple trackers and in-range tracking in high dimensional space are also discussed and illustrated with numerical examples, demonstrating substantial increases in mission duration in comparison to traditional set-point tracking.","sentences":["In contrast to set-point tracking which aims to reduce the tracking error between the tracker and the reference, tracking-in-range problems only focus on whether the tracker is within a given range around the reference, making it more suitable for the mission specifications of many practical applications.","In this work, we present novel optimal control formulations to solve tracking-in-range problems, for both problems requiring the tracker to be always in range, and problems allowing the tracker to go out of range to yield overall better outcomes.","As the problem naturally involves discontinuous functions, we present alternative formulations and regularisation strategies to improve the performance of numerical solvers.","The extension to in-range tracking with multiple trackers and in-range tracking in high dimensional space are also discussed and illustrated with numerical examples, demonstrating substantial increases in mission duration in comparison to traditional set-point tracking."],"url":"http://arxiv.org/abs/2403.03066v1","category":"eess.SY"}
{"created":"2024-03-05 15:38:54","title":"Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range","abstract":"This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify our theoretical findings.","sentences":["This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems.","The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents.","On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting.","We show that it is possible to approximate the exact gradient only using local information.","Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase.","We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off.","The simulation results verify our theoretical findings."],"url":"http://arxiv.org/abs/2403.03055v1","category":"cs.MA"}
{"created":"2024-03-05 15:33:34","title":"Microscopic origin of abrupt transition in interdependent superconducting networks","abstract":"The paradigm of interdependent networks has recently been manifested in experimentally testable lab setup of interdependent superconducting networks. This system experiences an abrupt transition due to the thermal dissipation between the networks but its underlying mechanism remains elusive. Here we study the critical behavior and the underlying mechanism of the transition, unveiling its unique microscopic nature. The microscopic characteristics of the transition result in a macroscopic long-living plateau that lasts for thousands of seconds and increases with the size of the system. We characterize the critical behavior of the transition and find that the critical exponents are consistent with those predicted theoretically for percolation of abstract interdependent networks and interdependent ferromagnetic networks, supporting a common universal origin of interdependent systems.","sentences":["The paradigm of interdependent networks has recently been manifested in experimentally testable lab setup of interdependent superconducting networks.","This system experiences an abrupt transition due to the thermal dissipation between the networks but its underlying mechanism remains elusive.","Here we study the critical behavior and the underlying mechanism of the transition, unveiling its unique microscopic nature.","The microscopic characteristics of the transition result in a macroscopic long-living plateau that lasts for thousands of seconds and increases with the size of the system.","We characterize the critical behavior of the transition and find that the critical exponents are consistent with those predicted theoretically for percolation of abstract interdependent networks and interdependent ferromagnetic networks, supporting a common universal origin of interdependent systems."],"url":"http://arxiv.org/abs/2403.03050v1","category":"physics.soc-ph"}
{"created":"2024-03-05 15:31:35","title":"Design of Stochastic Quantizers for Privacy Preservation","abstract":"In this paper, we examine the role of stochastic quantizers for privacy preservation. We first employ a static stochastic quantizer and investigate its corresponding privacy-preserving properties. Specifically, we demonstrate that a sufficiently large quantization step guarantees $(0, \\delta)$ differential privacy. Additionally, the degradation of control performance caused by quantization is evaluated as the tracking error of output regulation. These two analyses characterize the trade-off between privacy and control performance, determined by the quantization step. This insight enables us to use quantization intentionally as a means to achieve the seemingly conflicting two goals of maintaining control performance and preserving privacy at the same time; towards this end, we further investigate a dynamic stochastic quantizer. Under a stability assumption, the dynamic stochastic quantizer can enhance privacy, more than the static one, while achieving the same control performance. We further handle the unstable case by additionally applying input Gaussian noise.","sentences":["In this paper, we examine the role of stochastic quantizers for privacy preservation.","We first employ a static stochastic quantizer and investigate its corresponding privacy-preserving properties.","Specifically, we demonstrate that a sufficiently large quantization step guarantees $(0, \\delta)$ differential privacy.","Additionally, the degradation of control performance caused by quantization is evaluated as the tracking error of output regulation.","These two analyses characterize the trade-off between privacy and control performance, determined by the quantization step.","This insight enables us to use quantization intentionally as a means to achieve the seemingly conflicting two goals of maintaining control performance and preserving privacy at the same time; towards this end, we further investigate a dynamic stochastic quantizer.","Under a stability assumption, the dynamic stochastic quantizer can enhance privacy, more than the static one, while achieving the same control performance.","We further handle the unstable case by additionally applying input Gaussian noise."],"url":"http://arxiv.org/abs/2403.03048v1","category":"eess.SY"}
{"created":"2024-03-05 15:30:24","title":"The Exchange Problem","abstract":"Auctions are widely used in exchanges to match buy and sell requests. Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched. The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing. In this work, we study the algorithmic complexity of the problems arising from these matching tasks.   We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\\log n)$ time to match $n$ requests. For dynamic price matching, we establish a lower bound of $\\Omega(n \\log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal.","sentences":["Auctions are widely used in exchanges to match buy and sell requests.","Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched.","The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing.","In this work, we study the algorithmic complexity of the problems arising from these matching tasks.   ","We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\\log n)$ time to match $n$ requests.","For dynamic price matching, we establish a lower bound of $\\Omega(n \\log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal."],"url":"http://arxiv.org/abs/2403.03046v1","category":"cs.DS"}
{"created":"2024-03-05 15:22:49","title":"Proof-of-concept for a nonadditive stochastic model of supercooled liquids","abstract":"The recently proposed non-additive stochastic model (NSM) offers a coherent physical interpretation for diffusive phenomena in glass-forming systems. This model presents non-exponential relationships between viscosity, activation energy, and temperature, characterizing the non-Arrhenius behavior observed in supercooled liquids. In this work, we fit the NSM viscosity equation to experimental temperature-dependent viscosity data corresponding to twenty-five glass-forming liquids and compare the fit parameters with those obtained using the Vogel-Fulcher-Tammann (VFT), Avramov-Milchev (AM), and Mauro-Yue-Ellison-Gupta-Allan (MYEGA) models. The results demonstrate that the NSM provides an effective fitting equation for modeling viscosity experimental data in comparison with other established models (VFT, AM and MYEGA), characterizing the activation energy in fragile liquids, presenting a reliable indicator of the degree of fragility of the glass-forming liquids.","sentences":["The recently proposed non-additive stochastic model (NSM) offers a coherent physical interpretation for diffusive phenomena in glass-forming systems.","This model presents non-exponential relationships between viscosity, activation energy, and temperature, characterizing the non-Arrhenius behavior observed in supercooled liquids.","In this work, we fit the NSM viscosity equation to experimental temperature-dependent viscosity data corresponding to twenty-five glass-forming liquids and compare the fit parameters with those obtained using the Vogel-Fulcher-Tammann (VFT), Avramov-Milchev (AM), and Mauro-Yue-Ellison-Gupta-Allan (MYEGA) models.","The results demonstrate that the NSM provides an effective fitting equation for modeling viscosity experimental data in comparison with other established models (VFT, AM and MYEGA), characterizing the activation energy in fragile liquids, presenting a reliable indicator of the degree of fragility of the glass-forming liquids."],"url":"http://arxiv.org/abs/2403.03041v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 15:14:38","title":"Superconductivity in Ca-intercalated bilayer silicene","abstract":"Within first-principles calculations, we explore superconductivity in Ca-intercalated bilayer silicene compound, Si2CaSi2. This arises from the coupling of interlayer flower-like \\Gamma-centered Fermi surface formed by the hybridization of Ca-3d and Si-3pz orbitals with low-energy out-of-plane vibrations enabled by silicene's buckling. The consequent large electron-phonon coupling, as evident from the Eliashberg spectral function leads to superconductivity below 5.4 K in this two-dimensional covalent system. Our results reveal the key control parameters to achieve superconductivity in experimentally synthesizable silicon-based thin materials that can find diverse applications.","sentences":["Within first-principles calculations, we explore superconductivity in Ca-intercalated bilayer silicene compound, Si2CaSi2.","This arises from the coupling of interlayer flower-like \\Gamma-centered Fermi surface formed by the hybridization of Ca-3d and Si-3pz orbitals with low-energy out-of-plane vibrations enabled by silicene's buckling.","The consequent large electron-phonon coupling, as evident from the Eliashberg spectral function leads to superconductivity below 5.4 K in this two-dimensional covalent system.","Our results reveal the key control parameters to achieve superconductivity in experimentally synthesizable silicon-based thin materials that can find diverse applications."],"url":"http://arxiv.org/abs/2403.03036v1","category":"cond-mat.supr-con"}
{"created":"2024-03-05 15:08:16","title":"Learning to Use Tools via Cooperative and Interactive Agents","abstract":"Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.","sentences":["Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability.","Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction.","However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails.","To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents.","We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment.","Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline).","We further provide fine-granularity analysis for the efficiency and consistency of our framework."],"url":"http://arxiv.org/abs/2403.03031v1","category":"cs.CL"}
{"created":"2024-03-05 14:57:23","title":"On Airy solutions of P$_\\mathrm{II}$ and the complex cubic ensemble of random matrices, II","abstract":"We describe the pole-free regions of the one-parameter family of special solutions of P$_\\mathrm{II}$, the second Painlev\\'e equation, constructed from the Airy functions. This is achieved by exploiting the connection between these solutions and the recurrence coefficients of orthogonal polynomials that appear in the analysis of the ensemble of random matrices corresponding to the cubic potential.","sentences":["We describe the pole-free regions of the one-parameter family of special solutions of P$_\\mathrm{II}$, the second Painlev\\'e equation, constructed from the Airy functions.","This is achieved by exploiting the connection between these solutions and the recurrence coefficients of orthogonal polynomials that appear in the analysis of the ensemble of random matrices corresponding to the cubic potential."],"url":"http://arxiv.org/abs/2403.03023v1","category":"math-ph"}
{"created":"2024-03-05 14:49:52","title":"The Case for Evaluating Multimodal Translation Models on Text Datasets","abstract":"A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation. However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against.   Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance against complex sentences, and 3) the Multi30k test sets, for measuring MMT model performance against a real MMT dataset. Finally, we evaluate recent MMT models trained solely against the Multi30k dataset against our proposed evaluation framework and demonstrate the dramatic drop performance against text-only testing sets compared to recent text-only MT models.","sentences":["A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation.","However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties.","Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against.   ","Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance against complex sentences, and 3) the Multi30k test sets, for measuring MMT model performance against a real MMT dataset.","Finally, we evaluate recent MMT models trained solely against the Multi30k dataset against our proposed evaluation framework and demonstrate the dramatic drop performance against text-only testing sets compared to recent text-only MT models."],"url":"http://arxiv.org/abs/2403.03014v1","category":"cs.CL"}
{"created":"2024-03-05 14:34:13","title":"Implicit-Explicit simulation of Mass-Spring-Charge Systems","abstract":"Point masses connected by springs, or mass-spring systems, are widely used in computer animation to approximate the behavior of deformable objects. One of the restrictions imposed by these models is that points that are not topologically constrained (linked by a spring) are unable to interact with each other explicitly. Such interactions would introduce a new dimension for artistic control and animation within the computer graphics community. Beyond graphics, such a model could be an effective proxy to use for model-based learning of complex physical systems such as molecular biology. We propose to imbue masses in a mass-spring system with electrostatic charge leading a system with internal forces between all pairs of charged points -- regardless of whether they are linked by a spring. We provide a practical and stable algorithm to simulate charged mass-spring systems over long time horizons. We demonstrate how these systems may be controlled via parameters such as guidance electric fields or external charges, thus presenting fresh opportunities for artistic authoring. Our method is especially appropriate for computer graphics applications due to its robustness at larger simulation time steps.","sentences":["Point masses connected by springs, or mass-spring systems, are widely used in computer animation to approximate the behavior of deformable objects.","One of the restrictions imposed by these models is that points that are not topologically constrained (linked by a spring) are unable to interact with each other explicitly.","Such interactions would introduce a new dimension for artistic control and animation within the computer graphics community.","Beyond graphics, such a model could be an effective proxy to use for model-based learning of complex physical systems such as molecular biology.","We propose to imbue masses in a mass-spring system with electrostatic charge leading a system with internal forces between all pairs of charged points -- regardless of whether they are linked by a spring.","We provide a practical and stable algorithm to simulate charged mass-spring systems over long time horizons.","We demonstrate how these systems may be controlled via parameters such as guidance electric fields or external charges, thus presenting fresh opportunities for artistic authoring.","Our method is especially appropriate for computer graphics applications due to its robustness at larger simulation time steps."],"url":"http://arxiv.org/abs/2403.03005v1","category":"cs.GR"}
{"created":"2024-03-05 14:21:58","title":"A Convex Optimization Framework for Computing Robustness Margins of Kalman Filters","abstract":"This paper proposes a novel convex optimization framework for designing robust Kalman filters that guarantee a user-specified steady-state error while maximizing process and sensor noise. The proposed framework simultaneously determines the Kalman gain and the robustness margin in terms of the process and sensor noise. This is the first paper to present such a joint formulation for Kalman filtering. The proposed methodology is validated through two distinct examples: the Clohessy-Wiltshire-Hill equations for a chaser spacecraft in an elliptical orbit and the longitudinal motion model of an F-16 aircraft.","sentences":["This paper proposes a novel convex optimization framework for designing robust Kalman filters that guarantee a user-specified steady-state error while maximizing process and sensor noise.","The proposed framework simultaneously determines the Kalman gain and the robustness margin in terms of the process and sensor noise.","This is the first paper to present such a joint formulation for Kalman filtering.","The proposed methodology is validated through two distinct examples: the Clohessy-Wiltshire-Hill equations for a chaser spacecraft in an elliptical orbit and the longitudinal motion model of an F-16 aircraft."],"url":"http://arxiv.org/abs/2403.02996v1","category":"eess.SY"}
{"created":"2024-03-05 14:13:50","title":"MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer","abstract":"Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.","sentences":["Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens.","Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch.","Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples.","To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs.","Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities.","We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances.","Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance.","Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation."],"url":"http://arxiv.org/abs/2403.02991v1","category":"cs.CV"}
{"created":"2024-03-05 14:07:02","title":"Exciplex-driven blue OLEDs: unlocking multifunctionality applications","abstract":"We present the development of multifunctional blue-emission organic light-emitting diodes (OLEDs) using TADF-exciplex materials. These OLEDs exhibit sensitivity to external stimuli and achieve a maximum external quantum efficiency (EQE) of 11.6 % through partly liquid processing. This technique allows for large-scale production on arbitrary geometries.   The potential multifunctionality of the devices arises from their response to low external magnetic fields (up to 100 mT) with an efficiency up to 2.5 % for magnetoconductance, while maximum magneto-electroluminescence effects of 4.1 % were detected. We investigated novel aspects, including the utilization of two organic materials without further doping and the investigation of the impact of 2,2',2''-(1,3,5-Benzinetriyl)-tris(1phenyl-1-H-benzimidazole) (TPBi) processing in liquid and vapor form. The insights gained provide a fundamental understanding regarding the applicability of exciplex (EX) materials for fully solution-processed OLEDs through a deliberate omission of doping. Our work represents a significant advancement on the path towards multifunctional OLED technology, with potential applications in cost-efficient, scalable organic full-color displays and advanced sensing system","sentences":["We present the development of multifunctional blue-emission organic light-emitting diodes (OLEDs) using TADF-exciplex materials.","These OLEDs exhibit sensitivity to external stimuli and achieve a maximum external quantum efficiency (EQE) of 11.6 % through partly liquid processing.","This technique allows for large-scale production on arbitrary geometries.   ","The potential multifunctionality of the devices arises from their response to low external magnetic fields (up to 100 mT) with an efficiency up to 2.5 % for magnetoconductance, while maximum magneto-electroluminescence effects of 4.1 % were detected.","We investigated novel aspects, including the utilization of two organic materials without further doping and the investigation of the impact of 2,2',2''-(1,3,5-Benzinetriyl)-tris(1phenyl-1-H-benzimidazole) (TPBi) processing in liquid and vapor form.","The insights gained provide a fundamental understanding regarding the applicability of exciplex (EX) materials for fully solution-processed OLEDs through a deliberate omission of doping.","Our work represents a significant advancement on the path towards multifunctional OLED technology, with potential applications in cost-efficient, scalable organic full-color displays and advanced sensing system"],"url":"http://arxiv.org/abs/2403.02987v1","category":"physics.app-ph"}
{"created":"2024-03-05 13:58:18","title":"System performance of a TDM test-bed with long flex harness towards the new X-IFU FPA-DM","abstract":"SRON (Netherlands Institute for Space Research) is developing the Focal Plane Assembly (FPA) for Athena X-IFU, whose Demonstration Model (DM) will use for the first time a time domain multiplexing (TDM)-based readout system for the on-board transition-edge sensors (TES). We report on the characterization activities on a TDM setup provided by NASA Goddard Space Flight Center (GSFC) and National Institute for Standards and Technology (NIST) and tested in SRON cryogenic test facilities. The goal of these activities is to study the impact of the longer harness, closer to X-IFU specs, in a different EMI environment and switching from a single-ended to a differential readout scheme. In this contribution we describe the advancement in the debugging of the system in the SRON cryostat, which led to the demonstration of the nominal spectral performance of 2.8 eV at 5.9~keV with 16-row multiplexing, as well as an outlook for the future endeavours for the TDM readout integration on X-IFU's FPA-DM at SRON.","sentences":["SRON (Netherlands Institute for Space Research) is developing the Focal Plane Assembly (FPA) for Athena X-IFU, whose Demonstration Model (DM) will use for the first time a time domain multiplexing (TDM)-based readout system for the on-board transition-edge sensors (TES).","We report on the characterization activities on a TDM setup provided by NASA Goddard Space Flight Center (GSFC) and National Institute for Standards and Technology (NIST) and tested in SRON cryogenic test facilities.","The goal of these activities is to study the impact of the longer harness, closer to X-IFU specs, in a different EMI environment and switching from a single-ended to a differential readout scheme.","In this contribution we describe the advancement in the debugging of the system in the SRON cryostat, which led to the demonstration of the nominal spectral performance of 2.8 eV at 5.9~keV with 16-row multiplexing, as well as an outlook for the future endeavours for the TDM readout integration on X-IFU's FPA-DM at SRON."],"url":"http://arxiv.org/abs/2403.02978v1","category":"astro-ph.IM"}
{"created":"2024-03-05 13:52:48","title":"Model Predictive Control for setpoint tracking","abstract":"The main objective of tracking control is to steer the tracking error, that is the difference between the reference and the output, to zero while the plant's operation limits are satisfied. This requires that some assumptions on the evolution of the future values of the reference must be taken into account. Typically a simple evolution of the reference is considered, such as step, ramp, or parabolic reference signals. It is important to notice that the tracking problem considers possible variations in the reference to be tracked, such as steps or slope variations of the ramps. Then the tracking control problem is inherently uncertain, since the reference may differ from what is expected. If the value of the reference is changed, then there is no guarantee that the feasibility and stability properties of the resulting control law hold. This report presents the MPC for tracking (MPCT) approach, which ensures recursive feasibility and asymptotic stability of the setpoint when the value of the reference is changed.","sentences":["The main objective of tracking control is to steer the tracking error, that is the difference between the reference and the output, to zero while the plant's operation limits are satisfied.","This requires that some assumptions on the evolution of the future values of the reference must be taken into account.","Typically a simple evolution of the reference is considered, such as step, ramp, or parabolic reference signals.","It is important to notice that the tracking problem considers possible variations in the reference to be tracked, such as steps or slope variations of the ramps.","Then the tracking control problem is inherently uncertain, since the reference may differ from what is expected.","If the value of the reference is changed, then there is no guarantee that the feasibility and stability properties of the resulting control law hold.","This report presents the MPC for tracking (MPCT) approach, which ensures recursive feasibility and asymptotic stability of the setpoint when the value of the reference is changed."],"url":"http://arxiv.org/abs/2403.02973v1","category":"math.OC"}
{"created":"2024-03-05 13:49:32","title":"Space Complexity of Euclidean Clustering","abstract":"The $(k, z)$-Clustering problem in Euclidean space $\\mathbb{R}^d$ has been extensively studied. Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\\varepsilon$, remains unclear in existing literature.   This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds. Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size. Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.","sentences":["The $(k, z)$-Clustering problem in Euclidean space $\\mathbb{R}^d$ has been extensively studied.","Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature.","However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\\varepsilon$, remains unclear in existing literature.   ","This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds.","Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme.","Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size.","Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest."],"url":"http://arxiv.org/abs/2403.02971v1","category":"cs.CG"}
{"created":"2024-03-05 13:25:47","title":"Location of the zeros of quaternionic polynomials using matrix tools","abstract":"Using a variety of matrix techniques, the problem of locating the left eigenvalues of the quaternion companion matrices are investigated in this paper. In a recent paper, Dar et al. [6], proved that the zeros of a quaternionic polynomial and the left eigenvalues of corresponding companion matrix are same. In view of this, we use various newly developed matrix techniques to prove various results concerning the location of the zeros of regular polynomials of a quaternionic variable with quaternionic coefficients, which include an extension of the result of A. L. Cauchy as well.","sentences":["Using a variety of matrix techniques, the problem of locating the left eigenvalues of the quaternion companion matrices are investigated in this paper.","In a recent paper, Dar et al.","[6], proved that the zeros of a quaternionic polynomial and the left eigenvalues of corresponding companion matrix are same.","In view of this, we use various newly developed matrix techniques to prove various results concerning the location of the zeros of regular polynomials of a quaternionic variable with quaternionic coefficients, which include an extension of the result of A. L. Cauchy as well."],"url":"http://arxiv.org/abs/2403.02958v1","category":"math.CV"}
{"created":"2024-03-05 13:24:33","title":"Exploring language endangerment: historical, geographical, and economic insights from multilayer language-country bipartite network analysis","abstract":"Language endangerment is a phenomenon in which approximately 40% of languages spoken worldwide are predicted to disappear within the next few decades, resulting in the loss of cultures associated with these languages. To take effective measures against language endangerment, it is essential to quantitatively understand its characteristics because it is a phenomenon in which historical, geographical, and economic factors are intricately intertwined. In this study, multilayer language-country bipartite networks are constructed using information about which countries each language is spoken in and two types of linguistic features, namely the existence of a writing system and the function within a country. In addition, percolation simulations are conducted to measure how language and country networks break down according to the extinction of languages and to identify vulnerable connections in them. In the language network of officially used languages with their writing system, the community analysis indicated that there were communities composed of languages spoken over geographically separated distances. The strength of languages revealed that the official languages in the former colonial nations, namely English, French, Spanish, Dutch, Portuguese, and Russian, still played significant roles in the formation of these communities. In the language and country networks of unofficially used languages without their writing system, the percolation simulation revealed that languages were likely to severely disappear in the Americas, and that linguistic diversity was vulnerable in affluent countries. The findings show that the analysis of multilayer language-country bipartite networks has enabled a quantitative understanding of the language endangerment occurring worldwide from historical, geographical, and economic perspectives.","sentences":["Language endangerment is a phenomenon in which approximately 40% of languages spoken worldwide are predicted to disappear within the next few decades, resulting in the loss of cultures associated with these languages.","To take effective measures against language endangerment, it is essential to quantitatively understand its characteristics because it is a phenomenon in which historical, geographical, and economic factors are intricately intertwined.","In this study, multilayer language-country bipartite networks are constructed using information about which countries each language is spoken in and two types of linguistic features, namely the existence of a writing system and the function within a country.","In addition, percolation simulations are conducted to measure how language and country networks break down according to the extinction of languages and to identify vulnerable connections in them.","In the language network of officially used languages with their writing system, the community analysis indicated that there were communities composed of languages spoken over geographically separated distances.","The strength of languages revealed that the official languages in the former colonial nations, namely English, French, Spanish, Dutch, Portuguese, and Russian, still played significant roles in the formation of these communities.","In the language and country networks of unofficially used languages without their writing system, the percolation simulation revealed that languages were likely to severely disappear in the Americas, and that linguistic diversity was vulnerable in affluent countries.","The findings show that the analysis of multilayer language-country bipartite networks has enabled a quantitative understanding of the language endangerment occurring worldwide from historical, geographical, and economic perspectives."],"url":"http://arxiv.org/abs/2403.02954v1","category":"physics.soc-ph"}
{"created":"2024-03-05 13:24:17","title":"Single-level Robust Bidding of Renewable-only Virtual Power Plant in Energy and Ancillary Service Markets for Worst-case Profit","abstract":"This paper proposes a novel single-level robust mathematical approach to model the RES-only Virtual Power Plant (RVPP) bidding problem in the simultaneous Day Ahead Market (DAM) and Secondary Reserve Market (SRM). The worst-case profit of RVPP due to uncertainties related to electricity prices, Non-dispatchable Renewable Energy Sources (ND-RES) production, and flexible demand is captured. In order to find the worst-case profit in a single-level model, the relationship between price and energy uncertainties leads to some non-linear constraints, which are appropriately linearized. The simulation results show the superiority of the proposed robust model compared to those in the literature, as well as its computational efficiency.","sentences":["This paper proposes a novel single-level robust mathematical approach to model the RES-only Virtual Power Plant (RVPP) bidding problem in the simultaneous Day","Ahead Market (DAM) and Secondary Reserve Market (SRM).","The worst-case profit of RVPP due to uncertainties related to electricity prices, Non-dispatchable Renewable Energy Sources (ND-RES) production, and flexible demand is captured.","In order to find the worst-case profit in a single-level model, the relationship between price and energy uncertainties leads to some non-linear constraints, which are appropriately linearized.","The simulation results show the superiority of the proposed robust model compared to those in the literature, as well as its computational efficiency."],"url":"http://arxiv.org/abs/2403.02953v1","category":"eess.SY"}
{"created":"2024-03-05 13:11:28","title":"ISC: an RADI-type method for stochastic continuous-time algebraic Riccati equations","abstract":"In this paper, we propose an RADI-type method for large-scale stochastic continuous-time algebraic Riccati equations with sparse and low-rank structures. The so-called ISC method is developed by using the Incorporation idea together with different Shifts to accelerate the convergence and Compressions to reduce the storage and complexity. Numerical experiments are given to show its efficiency.","sentences":["In this paper, we propose an RADI-type method for large-scale stochastic continuous-time algebraic Riccati equations with sparse and low-rank structures.","The so-called ISC method is developed by using the Incorporation idea together with different Shifts to accelerate the convergence and Compressions to reduce the storage and complexity.","Numerical experiments are given to show its efficiency."],"url":"http://arxiv.org/abs/2403.02940v1","category":"math.NA"}
{"created":"2024-03-05 12:53:42","title":"Self-interacting quantum particles","abstract":"The real Hilbert space formalism developed within the quaternionic quantum mechanics ($\\mathbb H$QM) is fully applied to the simple model of the autonomous particle. This framework permits novel insights within the usual description of the complex autonomous particle, particulaly concening the energy of a non-stationary motion. Through the appraisal of the physical role played by a fully quaternionic scalar potential, a original self-interaction within the quaternionic autonomous particle has been determined as well. Scattering processes are considered to illustrate these novel features.","sentences":["The real Hilbert space formalism developed within the quaternionic quantum mechanics ($\\mathbb H$QM) is fully applied to the simple model of the autonomous particle.","This framework permits novel insights within the usual description of the complex autonomous particle, particulaly concening the energy of a non-stationary motion.","Through the appraisal of the physical role played by a fully quaternionic scalar potential, a original self-interaction within the quaternionic autonomous particle has been determined as well.","Scattering processes are considered to illustrate these novel features."],"url":"http://arxiv.org/abs/2403.02935v1","category":"quant-ph"}
{"created":"2024-03-05 12:53:00","title":"iSummary: Workload-based, Personalized Summaries for Knowledge Graphs","abstract":"The explosion in the size and the complexity of the available Knowledge Graphs on the web has led to the need for efficient and effective methods for their understanding and exploration. Semantic summaries have recently emerged as methods to quickly explore and understand the contents of various sources. However in most cases they are static not incorporating user needs and preferences and cannot scale. In this paper we present iSummary a novel scalable approach for constructing personalized summaries. As the size and the complexity of the Knowledge Graphs for constructing personalized summaries prohibit efficient summary construction, in our approach we exploit query logs. The main idea behind our approach is to exploit knowledge captured in existing user queries for identifying the most interesting resources and linking them constructing as such highquality personalized summaries. We present an algorithm with theoretical guarantees on the summarys quality linear in the number of queries available in the query log. We evaluate our approach using three realworld datasets and several baselines showing that our approach dominates other methods in terms of both quality and efficiency.","sentences":["The explosion in the size and the complexity of the available Knowledge Graphs on the web has led to the need for efficient and effective methods for their understanding and exploration.","Semantic summaries have recently emerged as methods to quickly explore and understand the contents of various sources.","However in most cases they are static not incorporating user needs and preferences and cannot scale.","In this paper we present iSummary a novel scalable approach for constructing personalized summaries.","As the size and the complexity of the Knowledge Graphs for constructing personalized summaries prohibit efficient summary construction, in our approach we exploit query logs.","The main idea behind our approach is to exploit knowledge captured in existing user queries for identifying the most interesting resources and linking them constructing as such highquality personalized summaries.","We present an algorithm with theoretical guarantees on the summarys quality linear in the number of queries available in the query log.","We evaluate our approach using three realworld datasets and several baselines showing that our approach dominates other methods in terms of both quality and efficiency."],"url":"http://arxiv.org/abs/2403.02934v1","category":"cs.DB"}
{"created":"2024-03-05 12:48:29","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study","abstract":"We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.","sentences":["We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs.","Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components.","Our findings reveal discrepancies in performance compared to the original work.","We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers."],"url":"http://arxiv.org/abs/2403.02930v1","category":"cs.CL"}
{"created":"2024-03-05 12:48:02","title":"Loss Design for Single-carrier Joint Communication and Neural Network-based Sensing","abstract":"We evaluate the influence of multi-snapshot sensing and varying signal-to-noise ratio (SNR) on the overall performance of neural network (NN)-based joint communication and sensing (JCAS) systems. To enhance the training behavior, we decouple the loss functions from the respective SNR values and the number of sensing snapshots, using bounds of the sensing performance. Pre-processing is done through conventional sensing signal processing steps on the inputs to the sensing NN. The proposed method outperforms classical algorithms, such as a Neyman-Pearson-based power detector for object detection and ESPRIT for angle of arrival (AoA) estimation for quadrature amplitude modulation (QAM) at low SNRs.","sentences":["We evaluate the influence of multi-snapshot sensing and varying signal-to-noise ratio (SNR) on the overall performance of neural network (NN)-based joint communication and sensing (JCAS) systems.","To enhance the training behavior, we decouple the loss functions from the respective SNR values and the number of sensing snapshots, using bounds of the sensing performance.","Pre-processing is done through conventional sensing signal processing steps on the inputs to the sensing NN.","The proposed method outperforms classical algorithms, such as a Neyman-Pearson-based power detector for object detection and ESPRIT for angle of arrival (AoA) estimation for quadrature amplitude modulation (QAM) at low SNRs."],"url":"http://arxiv.org/abs/2403.02929v1","category":"eess.SP"}
{"created":"2024-03-05 12:44:54","title":"User-Driven Adaptation: Tailoring Autonomous Driving Systems with Dynamic Preferences","abstract":"In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate. Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically. The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a ``human-on-the-loop'' mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap. In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation. The findings affirm the approach's ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants' subjective satisfaction in autonomous systems.","sentences":["In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate.","Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically.","The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a ``human-on-the-loop'' mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap.","In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation.","The findings affirm the approach's ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants' subjective satisfaction in autonomous systems."],"url":"http://arxiv.org/abs/2403.02928v1","category":"cs.HC"}
{"created":"2024-03-05 12:28:04","title":"Scientific machine learning for closure models in multiscale problems: a review","abstract":"Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the simulation's accuracy. Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks. This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning. The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed. The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed. In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods. In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain. In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further.","sentences":["Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the simulation's accuracy.","Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks.","This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning.","The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed.","The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed.","In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods.","In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain.","In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further."],"url":"http://arxiv.org/abs/2403.02913v1","category":"math.NA"}
{"created":"2024-03-05 12:22:33","title":"Search for giant planets in M67 V: a warm Jupiter orbiting the turn-off star S1429","abstract":"Planets orbiting members of open or globular clusters offer a great opportunity to study exoplanet populations systematically as stars within clusters provide a mostly homogeneous sample at least in chemical composition and stellar age. However, even though there have been coordinated efforts to search for exoplanets in stellar clusters, only a small number of planets has been detected. One successful example is the seven-year radial velocity (RV) survey \"Search for giant planets in M67\" of 88 stars in the open cluster M67 which led to the discovery of five giant planets, including three close-in ($P < 10$ days) hot-Jupiters. In this work, we continue and extend the observation of stars in M67 with the aim to search for additional planets. We conducted spectroscopic observations with the HPF, HARPS, HARPS-North, and SOPHIE spectrographs of 11 stars in M67. Six of our targets showed a variation or long-term trends in their RV during the original survey, while the other five were not observed in the original sample bringing the total number of stars to 93. An analysis of the radial velocities revealed one additional planet around the turn-off point star S1429 and gave solutions for the orbits of stellar companions around S2207 and YBP2018. S1429 b is a warm Jupiter on a likely circular orbit with a period of $77.48_{-0.19}^{+0.18}$ days and a minimum mass $\\text{M} \\sin i = 1.80 \\pm 0.2$ M$_\\text{J}$. We update the hot-Jupiter occurrence rate in M67 to include the five new stars, deriving $4.2_{-2.3}^{+4.1} \\%$ when considering all stars, and $5.4_{-3.0}^{+5.1} \\%$ if binary star systems are removed.","sentences":["Planets orbiting members of open or globular clusters offer a great opportunity to study exoplanet populations systematically as stars within clusters provide a mostly homogeneous sample at least in chemical composition and stellar age.","However, even though there have been coordinated efforts to search for exoplanets in stellar clusters, only a small number of planets has been detected.","One successful example is the seven-year radial velocity (RV) survey \"Search for giant planets in M67\" of 88 stars in the open cluster M67 which led to the discovery of five giant planets, including three close-in ($P < 10$ days) hot-Jupiters.","In this work, we continue and extend the observation of stars in M67 with the aim to search for additional planets.","We conducted spectroscopic observations with the HPF, HARPS, HARPS-North, and SOPHIE spectrographs of 11 stars in M67.","Six of our targets showed a variation or long-term trends in their RV during the original survey, while the other five were not observed in the original sample bringing the total number of stars to 93.","An analysis of the radial velocities revealed one additional planet around the turn-off point star S1429 and gave solutions for the orbits of stellar companions around S2207 and YBP2018.","S1429 b is a warm Jupiter on a likely circular orbit with a period of $77.48_{-0.19}^{+0.18}$ days and a minimum mass $\\text{M} \\sin","i = 1.80 \\pm 0.2$ M$_\\text{J}$.","We update the hot-Jupiter occurrence rate in M67 to include the five new stars, deriving $4.2_{-2.3}^{+4.1} \\%$ when considering all stars, and $5.4_{-3.0}^{+5.1} \\%$ if binary star systems are removed."],"url":"http://arxiv.org/abs/2403.02911v1","category":"astro-ph.EP"}
{"created":"2024-03-05 11:55:10","title":"Design of full order proportional-integral observer for the state estimation of discrete-time linear time-invariant systems","abstract":"This paper is devoted to the design of full order proportional-integral observer for the state estimation of discrete-time linear time-invariant systems. In particular, explicit necessary and sufficient conditions are established for the existence of proportional-integral observer for the state estimation of discrete-time linear time-invariant systems and a simple procedure is given for the construction of the observer. Our approach is based on properties of real and polynomial matrices.","sentences":["This paper is devoted to the design of full order proportional-integral observer for the state estimation of discrete-time linear time-invariant systems.","In particular, explicit necessary and sufficient conditions are established for the existence of proportional-integral observer for the state estimation of discrete-time linear time-invariant systems and a simple procedure is given for the construction of the observer.","Our approach is based on properties of real and polynomial matrices."],"url":"http://arxiv.org/abs/2403.02891v1","category":"math.OC"}
{"created":"2024-03-05 11:41:43","title":"Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization","abstract":"Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and high-fidelity microscopic traffic flow. Results indicate that the policy trained under domain randomization traffic flow has significantly better success rate and calculative reward compared to the models trained under other microscopic traffic flows.","sentences":["Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments.","The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance.","It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes.","In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO.","We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and high-fidelity microscopic traffic flow.","Results indicate that the policy trained under domain randomization traffic flow has significantly better success rate and calculative reward compared to the models trained under other microscopic traffic flows."],"url":"http://arxiv.org/abs/2403.02882v1","category":"eess.SY"}
{"created":"2024-03-05 11:36:33","title":"SIC-POVMs from Stark Units: Dimensions n^2+3=4p, p prime","abstract":"The existence problem for maximal sets of equiangular lines (or SICs) in complex Hilbert space of dimension $d$ remains largely open. In a previous publication (arXiv:2112.05552) we gave a conjectural algorithm for how to construct a SIC if $d = n^2+3 = p$, a prime number. Perhaps the most surprising number-theoretical aspect of that algorithm is the appearance of Stark units in a key role: a single Stark unit from a ray class field extension of a real quadratic field serves as a seed from which the SIC is constructed. The algorithm can be modified to apply to all dimensions $d = n^2+3$. Here we focus on the case when $d= n^2+3 = 4p$, $p$ prime, for two reasons. First, special measures have to be taken on the Hilbert space side of the problem when the dimension is even. Second, the degrees of the relevant ray class fields are `smooth' in a sense that facilitates exact calculations. As a result the algorithm becomes easier to explain. We give solutions for seventeen different dimensions of this form, reaching $d = 39604$. Several improvements relative to our previous publication are reported, but we cannot offer a proof that the algorithm works for any dimensions where it has not been tested.","sentences":["The existence problem for maximal sets of equiangular lines (or SICs) in complex Hilbert space of dimension $d$ remains largely open.","In a previous publication (arXiv:2112.05552) we gave a conjectural algorithm for how to construct a SIC if $d = n^2+3 = p$, a prime number.","Perhaps the most surprising number-theoretical aspect of that algorithm is the appearance of Stark units in a key role: a single Stark unit from a ray class field extension of a real quadratic field serves as a seed from which the SIC is constructed.","The algorithm can be modified to apply to all dimensions $d = n^2+3$. Here we focus on the case when $d= n^2+3 = 4p$, $p$ prime, for two reasons.","First, special measures have to be taken on the Hilbert space side of the problem when the dimension is even.","Second, the degrees of the relevant ray class fields are `smooth' in a sense that facilitates exact calculations.","As a result the algorithm becomes easier to explain.","We give solutions for seventeen different dimensions of this form, reaching $d = 39604$. Several improvements relative to our previous publication are reported, but we cannot offer a proof that the algorithm works for any dimensions where it has not been tested."],"url":"http://arxiv.org/abs/2403.02872v1","category":"quant-ph"}
{"created":"2024-03-05 11:29:05","title":"Quantum Mixed-State Self-Attention Network","abstract":"The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum positional encoding scheme, implemented through fixed quantum gates within the quantum circuit, to enhance the model's accuracy. Experimental validation on various datasets demonstrates that QMSAN model outperforms existing quantum and classical models in text classification, achieving significant performance improvements. QMSAN model not only significantly reduces the number of parameters but also exceeds classical self-attention networks in performance, showcasing its strong capability in data representation and information extraction. Furthermore, our study investigates the model's robustness in different quantum noise environments, showing that QMSAN possesses commendable robustness to low noise.","sentences":["The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks.","Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges.","This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks.","QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition.","Additionally, we propose an innovative quantum positional encoding scheme, implemented through fixed quantum gates within the quantum circuit, to enhance the model's accuracy.","Experimental validation on various datasets demonstrates that QMSAN model outperforms existing quantum and classical models in text classification, achieving significant performance improvements.","QMSAN model not only significantly reduces the number of parameters but also exceeds classical self-attention networks in performance, showcasing its strong capability in data representation and information extraction.","Furthermore, our study investigates the model's robustness in different quantum noise environments, showing that QMSAN possesses commendable robustness to low noise."],"url":"http://arxiv.org/abs/2403.02871v1","category":"quant-ph"}
{"created":"2024-03-05 11:25:01","title":"Classification of 2-node Excitatory-Inhibitory Networks","abstract":"We classify connected 2-node excitatory-inhibitory networks under various conditions. We assume that, as well as for connections, there are two distinct node-types, excitatory and inhibitory. In our classification we consider four different types of excitatory-inhibitory networks: restricted, partially restricted, unrestricted and completely unrestricted. For each type we give two different classifications. Using results on ODE-equivalence and minimality, we classify the ODE-classes and present a minimal representative for each ODE-class. We also classify all the networks with valence $\\le 2$. These classifications are up to renumbering of nodes and the interchange of `excitatory' and `inhibitory' on nodes and arrows.These classifications constitute a first step towards analysing dynamics and bifurcations of excitatory-inhibitory networks. The results have potential applications to biological network models, especially neuronal networks, gene regulatory networks, and synthetic gene networks.","sentences":["We classify connected 2-node excitatory-inhibitory networks under various conditions.","We assume that, as well as for connections, there are two distinct node-types, excitatory and inhibitory.","In our classification we consider four different types of excitatory-inhibitory networks: restricted, partially restricted, unrestricted and completely unrestricted.","For each type we give two different classifications.","Using results on ODE-equivalence and minimality, we classify the ODE-classes and present a minimal representative for each ODE-class.","We also classify all the networks with valence $\\le 2$.","These classifications are up to renumbering of nodes and the interchange of `excitatory' and `inhibitory' on nodes and arrows.","These classifications constitute a first step towards analysing dynamics and bifurcations of excitatory-inhibitory networks.","The results have potential applications to biological network models, especially neuronal networks, gene regulatory networks, and synthetic gene networks."],"url":"http://arxiv.org/abs/2403.02869v1","category":"math.DS"}
{"created":"2024-03-05 11:21:18","title":"Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation","abstract":"The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation.","sentences":["The study of continuous-time information diffusion has been an important area of research for many applications in recent years.","When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore.","Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues.","In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model.","Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure.","Furthermore, we undertake an analysis of the approximation error of FIM for network inference.","To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency.","We also quantify the effect of the approximation error on influence estimation theoretically.","Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation."],"url":"http://arxiv.org/abs/2403.02867v1","category":"cs.SI"}
{"created":"2024-03-05 11:13:04","title":"Spintronic Implementation of UNet for Image Segmentation","abstract":"Image segmentation plays a crucial role in computer vision applications like self-driving cars, satellite imagery analysis, and medical diagnosis. Implementing these complex deep neural networks on conventional hardware is highly inefficient. In this work, we propose hardware implementation of UNet for segmentation tasks, using spintronic devices. Our approach involves designing hardware for convolution, deconvolution, ReLU, and max pooling layers of the UNet architecture. We demonstrate the synaptic behavior of the domain wall MTJ, and design convolution and deconvolution layers using the domain wall-based crossbar array. We utilize the orthogonal current injected MTJ with its continuous resistance change and showcase the ReLU and max pooling functions. We employ a hybrid simulation setup by coupling micromagnetic simulation, non-equilibrium Green's function, Landau-Lifshitz-Gilbert-Slonczewski equations, and circuit simulation with Python programming to incorporate the diverse physics of spin-transport, magnetization dynamics, and CMOS elements in our proposed designs. We evaluate our UNet design on the CamVid dataset and achieve segmentation accuracies that are comparable to software implementation. During training, our design consumes 43.59pJ of energy for synaptic weight updates.","sentences":["Image segmentation plays a crucial role in computer vision applications like self-driving cars, satellite imagery analysis, and medical diagnosis.","Implementing these complex deep neural networks on conventional hardware is highly inefficient.","In this work, we propose hardware implementation of UNet for segmentation tasks, using spintronic devices.","Our approach involves designing hardware for convolution, deconvolution, ReLU, and max pooling layers of the UNet architecture.","We demonstrate the synaptic behavior of the domain wall MTJ, and design convolution and deconvolution layers using the domain wall-based crossbar array.","We utilize the orthogonal current injected MTJ with its continuous resistance change and showcase the ReLU and max pooling functions.","We employ a hybrid simulation setup by coupling micromagnetic simulation, non-equilibrium Green's function, Landau-Lifshitz-Gilbert-Slonczewski equations, and circuit simulation with Python programming to incorporate the diverse physics of spin-transport, magnetization dynamics, and CMOS elements in our proposed designs.","We evaluate our UNet design on the CamVid dataset and achieve segmentation accuracies that are comparable to software implementation.","During training, our design consumes 43.59pJ of energy for synaptic weight updates."],"url":"http://arxiv.org/abs/2403.02863v1","category":"cs.ET"}
{"created":"2024-03-05 10:56:25","title":"Quantum Data Management: From Theory to Opportunities","abstract":"Quantum computing has emerged as a transformative tool for future data management. Classical problems in database domains, including query optimization, data integration, and transaction management, have recently been addressed using quantum computing techniques. This tutorial aims to establish the theoretical foundation essential for enhancing methodologies and practical implementations in this line of research. Moreover, this tutorial takes a forward-looking approach by delving into recent strides in quantum internet technologies and the nonlocality theory. We aim to shed light on the uncharted territory of future data systems tailored for the quantum internet.","sentences":["Quantum computing has emerged as a transformative tool for future data management.","Classical problems in database domains, including query optimization, data integration, and transaction management, have recently been addressed using quantum computing techniques.","This tutorial aims to establish the theoretical foundation essential for enhancing methodologies and practical implementations in this line of research.","Moreover, this tutorial takes a forward-looking approach by delving into recent strides in quantum internet technologies and the nonlocality theory.","We aim to shed light on the uncharted territory of future data systems tailored for the quantum internet."],"url":"http://arxiv.org/abs/2403.02856v1","category":"cs.DB"}
{"created":"2024-03-05 10:51:01","title":"Observing the relative sign of excited-state dipole transitions by combining attosecond streaking and transient absorption spectroscopy","abstract":"The electronic structure of atomic quantum systems and their dynamical interaction with light is reflected in transition dipole matrix elements coupling the system's energy eigenstates. In this work, we measure phase shifts of the time-dependent ultrafast absorption to determine the relative signs of. the transition-dipole matrix elements. The measurement relies on precise absolute calibration of the relative timing between the used light pulses, which is achieved by combining attosecond transient absorption and attosecond streaking spectroscopy to simultaneously measure the resonant photoabsorption spectra of laser-coupled doubly excited states in helium, together with the attosecond streaked photoelectron spectra. The streaking measurement reveals the absolute timing and the full temporal profile of the interacting electric fields which is then used to quantify the state-specific dynamics of the measured photoabsorption spectra. By comparing the 1-fs time-scale modulation across the absorption lines corresponding to the 2s2p (1P) and sp2,3+ (1P) doubly excited states between simulation and measurement, we quantify the signs of the transition dipole matrix elements for the laser-coupled autoionizing states 2s2p-2p2 and 2p2-sp2,3+ to be opposite of each other.","sentences":["The electronic structure of atomic quantum systems and their dynamical interaction with light is reflected in transition dipole matrix elements coupling the system's energy eigenstates.","In this work, we measure phase shifts of the time-dependent ultrafast absorption to determine the relative signs of.","the transition-dipole matrix elements.","The measurement relies on precise absolute calibration of the relative timing between the used light pulses, which is achieved by combining attosecond transient absorption and attosecond streaking spectroscopy to simultaneously measure the resonant photoabsorption spectra of laser-coupled doubly excited states in helium, together with the attosecond streaked photoelectron spectra.","The streaking measurement reveals the absolute timing and the full temporal profile of the interacting electric fields which is then used to quantify the state-specific dynamics of the measured photoabsorption spectra.","By comparing the 1-fs time-scale modulation across the absorption lines corresponding to the 2s2p (1P) and sp2,3+ (1P) doubly excited states between simulation and measurement, we quantify the signs of the transition dipole matrix elements for the laser-coupled autoionizing states 2s2p-2p2 and 2p2-sp2,3+ to be opposite of each other."],"url":"http://arxiv.org/abs/2403.02853v1","category":"physics.atom-ph"}
{"created":"2024-03-05 10:41:58","title":"Quaternionic Mahler measure","abstract":"We introduce the quaternionic Mahler measure for non-commutative polynomials, extending the classical complex Mahler measure. We establish the existence of quaternionic Mahler measure for slice regular polynomials in one and two variables. We study the quaternionic Mahler measure for real and slice regular polynomials, and consider the associated Lehmer problem. Various formulas of quaternionic Mahler measures are proved.","sentences":["We introduce the quaternionic Mahler measure for non-commutative polynomials, extending the classical complex Mahler measure.","We establish the existence of quaternionic Mahler measure for slice regular polynomials in one and two variables.","We study the quaternionic Mahler measure for real and slice regular polynomials, and consider the associated Lehmer problem.","Various formulas of quaternionic Mahler measures are proved."],"url":"http://arxiv.org/abs/2403.02851v1","category":"math.NT"}
{"created":"2024-03-05 10:40:15","title":"Scalable Syndrome-based Neural Decoders for Bit-Interleaved Coded Modulations","abstract":"In this work, we introduce a framework that enables the use of Syndrome-Based Neural Decoders (SBND) for high-order Bit-Interleaved Coded Modulations (BICM). To this end, we extend the previous results on SBND, for which the validity is limited to Binary Phase-Shift Keying (BPSK), by means of a theoretical channel modeling of the bit Log-Likelihood Ratio (bit-LLR) induced outputs. We implement the proposed SBND system for two polar codes $(64,32)$ and $(128,64)$, using a Recurrent Neural Network (RNN) and a Transformer-based architecture. Both implementations are compared in Bit Error Rate (BER) performance and computational complexity.","sentences":["In this work, we introduce a framework that enables the use of Syndrome-Based Neural Decoders (SBND) for high-order Bit-Interleaved Coded Modulations (BICM).","To this end, we extend the previous results on SBND, for which the validity is limited to Binary Phase-Shift Keying (BPSK), by means of a theoretical channel modeling of the bit Log-Likelihood Ratio (bit-LLR) induced outputs.","We implement the proposed SBND system for two polar codes $(64,32)$ and $(128,64)$, using a Recurrent Neural Network (RNN) and a Transformer-based architecture.","Both implementations are compared in Bit Error Rate (BER) performance and computational complexity."],"url":"http://arxiv.org/abs/2403.02850v1","category":"cs.IT"}
{"created":"2024-03-05 10:31:02","title":"Rotational-state-selected Carbon Astrochemistry","abstract":"The addition of individual quanta of rotational excitation to a molecule has been shown to markedly change its reactivity by significantly modifying the intermolecular interactions. So far, it has only been possible to observe these rotational effects in a very limited number of systems due to lack of rotational selectivity in chemical reaction experiments. The recent development of rotationally controlled molecular beams now makes such investigations possible for a wide range of systems. This is particularly crucial in order to understand the chemistry occurring in the interstellar medium, such as exploring the formation of carbon-based astrochemical molecules and the emergence of molecular complexity in interstellar space from the reaction of small atomic and molecular fragments.","sentences":["The addition of individual quanta of rotational excitation to a molecule has been shown to markedly change its reactivity by significantly modifying the intermolecular interactions.","So far, it has only been possible to observe these rotational effects in a very limited number of systems due to lack of rotational selectivity in chemical reaction experiments.","The recent development of rotationally controlled molecular beams now makes such investigations possible for a wide range of systems.","This is particularly crucial in order to understand the chemistry occurring in the interstellar medium, such as exploring the formation of carbon-based astrochemical molecules and the emergence of molecular complexity in interstellar space from the reaction of small atomic and molecular fragments."],"url":"http://arxiv.org/abs/2403.02844v1","category":"physics.chem-ph"}
{"created":"2024-03-05 10:20:32","title":"A pH Sensor Scaffold for Mapping Spatiotemporal Gradients in Three Dimensional In Vitro Tumour Models","abstract":"The detection of extracellular pH at single cell resolution is challenging and requires advanced sensibility. Sensing pH at a high spatial and temporal resolution might provide crucial information in understanding the role of pH and its fluctuations in a wide range of physio-pathological cellular processes, including cancer. Here, a method to embed silica-based fluorescent pH sensors into alginate-based three-dimensional (3D) microgels tumour models, coupled with a computational method for fine data analysis, is presented. By means of confocal laser scanning microscopy, live-cell time-lapse imaging of 3D alginate microgels was performed and the extracellular pH metabolic variations were monitored in both in vitro 3D mono- and 3D co-cultures of tumour and stromal pancreatic cells. The results show that the extracellular pH is cell line-specific and time-dependent. Moreover, differences in pH were also detected between 3D monocultures versus 3D co-cultures, thus suggesting the existence of a metabolic crosstalk between tumour and stromal cells. In conclusion, the system has the potential to image multiple live cells types in a 3D environment and to decipher in real-time their pH metabolic interplay under controlled experimental conditions, thus being also a suitable platform for drug screening and personalized medicine.","sentences":["The detection of extracellular pH at single cell resolution is challenging and requires advanced sensibility.","Sensing pH at a high spatial and temporal resolution might provide crucial information in understanding the role of pH and its fluctuations in a wide range of physio-pathological cellular processes, including cancer.","Here, a method to embed silica-based fluorescent pH sensors into alginate-based three-dimensional (3D) microgels tumour models, coupled with a computational method for fine data analysis, is presented.","By means of confocal laser scanning microscopy, live-cell time-lapse imaging of 3D alginate microgels was performed and the extracellular pH metabolic variations were monitored in both in vitro 3D mono- and 3D co-cultures of tumour and stromal pancreatic cells.","The results show that the extracellular pH is cell line-specific and time-dependent.","Moreover, differences in pH were also detected between 3D monocultures versus 3D co-cultures, thus suggesting the existence of a metabolic crosstalk between tumour and stromal cells.","In conclusion, the system has the potential to image multiple live cells types in a 3D environment and to decipher in real-time their pH metabolic interplay under controlled experimental conditions, thus being also a suitable platform for drug screening and personalized medicine."],"url":"http://arxiv.org/abs/2403.02838v1","category":"physics.bio-ph"}
{"created":"2024-03-05 10:17:18","title":"Forced Symmetric Formation Control","abstract":"This work considers the distance constrained formation control problem with an additional constraint requiring that the formation exhibits a specified spatial symmetry. We employ recent results from the theory of symmetry-forced rigidity to construct an appropriate potential function that leads to a gradient dynamical system driving the agents to the desired formation. We show that only $(1+1/|\\Gamma|)n$ edges are sufficient to implement the control strategy when there are $n$ agents and the underlying symmetry group is $\\Gamma$. This number is considerably smaller than what is typically required from classic rigidity-theory based strategies ($2n-3$ edges). We also provide an augmented control strategy that ensures the agents can converge to a formation with respect to an arbitrary centroid. Numerous numerical examples are provided to illustrate the main results.","sentences":["This work considers the distance constrained formation control problem with an additional constraint requiring that the formation exhibits a specified spatial symmetry.","We employ recent results from the theory of symmetry-forced rigidity to construct an appropriate potential function that leads to a gradient dynamical system driving the agents to the desired formation.","We show that only $(1+1/|\\Gamma|)n$ edges are sufficient to implement the control strategy when there are $n$ agents and the underlying symmetry group is $\\Gamma$.","This number is considerably smaller than what is typically required from classic rigidity-theory based strategies ($2n-3$ edges).","We also provide an augmented control strategy that ensures the agents can converge to a formation with respect to an arbitrary centroid.","Numerous numerical examples are provided to illustrate the main results."],"url":"http://arxiv.org/abs/2403.02836v1","category":"math.OC"}
{"created":"2024-03-05 10:14:29","title":"Low-rank Tensor Autoregressive Predictor for Third-Order Time-Series Forecasting","abstract":"Recently, tensor time-series forecasting has gained increasing attention, whose core requirement is how to perform dimensionality reduction. Among all multidimensional data, third-order tensor is the most prevalent structure in real-world scenarios, such as RGB images and network traffic data. Previous studies in this field are mainly based on tensor Tucker decomposition and such methods have limitations in terms of computational cost, with iteration complexity of approximately $O(2n^3r)$, where $n$ and $r$ are the dimension and rank of original tensor data. Moreover, many real-world data does not exhibit the low-rank property under Tucker decomposition, which may fail the dimensionality reduction. In this paper, we pioneer the application of tensor singular value decomposition (t-SVD) to third-order time-series, which builds an efficient forecasting algorithm, called Low-rank Tensor Autoregressive Predictor (LOTAP). We observe that tensor tubal rank in t-SVD is always less than Tucker rank, which leads to great benefit in computational complexity. By combining it with the autoregressive (AR) model, the forecasting problem is formulated as a least squares optimization. We divide such an optimization problem by fast Fourier transformation into four decoupled subproblems, whose variables include regressive coefficient, f-diagonal tensor, left and right orthogonal tensors. The alternating minimization algorithm is proposed with iteration complexity of about $O(n^3 + n^2r^2)$, in which each subproblem has a closed-form solution. Numerical experiments show that, compared to Tucker-decomposition-based algorithms, LOTAP achieves a speed improvement ranging from 2 to 6 times while maintaining accurate forecasting performance in all four baseline tasks. In addition, LOTAP is applicable to a wider range of tensor forecasting tasks due to its more effective dimensionality reduction ability.","sentences":["Recently, tensor time-series forecasting has gained increasing attention, whose core requirement is how to perform dimensionality reduction.","Among all multidimensional data, third-order tensor is the most prevalent structure in real-world scenarios, such as RGB images and network traffic data.","Previous studies in this field are mainly based on tensor Tucker decomposition and such methods have limitations in terms of computational cost, with iteration complexity of approximately $O(2n^3r)$, where $n$ and $r$ are the dimension and rank of original tensor data.","Moreover, many real-world data does not exhibit the low-rank property under Tucker decomposition, which may fail the dimensionality reduction.","In this paper, we pioneer the application of tensor singular value decomposition (t-SVD) to third-order time-series, which builds an efficient forecasting algorithm, called Low-rank Tensor Autoregressive Predictor (LOTAP).","We observe that tensor tubal rank in t-SVD is always less than Tucker rank, which leads to great benefit in computational complexity.","By combining it with the autoregressive (AR) model, the forecasting problem is formulated as a least squares optimization.","We divide such an optimization problem by fast Fourier transformation into four decoupled subproblems, whose variables include regressive coefficient, f-diagonal tensor, left and right orthogonal tensors.","The alternating minimization algorithm is proposed with iteration complexity of about $O(n^3 +","n^2r^2)$, in which each subproblem has a closed-form solution.","Numerical experiments show that, compared to Tucker-decomposition-based algorithms, LOTAP achieves a speed improvement ranging from 2 to 6 times while maintaining accurate forecasting performance in all four baseline tasks.","In addition, LOTAP is applicable to a wider range of tensor forecasting tasks due to its more effective dimensionality reduction ability."],"url":"http://arxiv.org/abs/2403.02835v1","category":"math.OC"}
{"created":"2024-03-05 10:09:31","title":"SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix","abstract":"This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (SGD) with momentum. The extensive experiments on training deep learning models on several benchmark image classification datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and several state-of-the-art Newton optimization methods, such as Nystrom-SGD, L-BFGS, and AdaHessian, in term of the convergence speed for achieving the pre-specified objectives of training and test losses as well as test accuracy.","sentences":["This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models.","It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion.","Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data.","The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (SGD) with momentum.","The extensive experiments on training deep learning models on several benchmark image classification datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and several state-of-the-art Newton optimization methods, such as Nystrom-SGD, L-BFGS, and AdaHessian, in term of the convergence speed for achieving the pre-specified objectives of training and test losses as well as test accuracy."],"url":"http://arxiv.org/abs/2403.02833v1","category":"cs.LG"}
{"created":"2024-03-05 10:03:21","title":"SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies","abstract":"We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components. Furthermore, SpaceHopper's design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep Reinforcement Learning policies. In a simulation of Ceres' gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth's gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments.","sentences":["We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons.","The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components.","Furthermore, SpaceHopper's design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases.","Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing.","We control the leg motion for reorientation using Deep Reinforcement Learning policies.","In a simulation of Ceres' gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away.","Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth's gravity.","Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments."],"url":"http://arxiv.org/abs/2403.02831v1","category":"cs.RO"}
{"created":"2024-03-05 09:36:28","title":"Efficient simulation of complex Ginzburg--Landau equations using high-order exponential-type methods","abstract":"In this paper, we consider the task of efficiently computing the numerical solution of evolutionary complex Ginzburg--Landau equations. To this aim, we employ high-order exponential methods of splitting and Lawson type for the time integration. These schemes enjoy favorable stability properties and, in particular, do not show restrictions on the time step size due to the underlying stiffness of the models. The needed actions of matrix exponentials are efficiently realized with pointwise operations in Fourier space (when the model is considered with periodic boundary conditions) or by using a tensor-oriented approach that suitably employs the so-called $\\mu$-mode products (when the semidiscretization in space is performed with finite differences). The overall effectiveness of the approach is demonstrated by running simulations on a variety of two- and three-dimensional (systems of) complex Ginzburg--Landau equations with cubic and cubic-quintic nonlinearities, which are widely considered in literature to model relevant physical phenomena. In fact, in all instances high-order exponential-type schemes can outperform standard techniques to integrate in time the models under consideration, i.e., the well-known split-step method and the explicit fourth-order Runge--Kutta integrator.","sentences":["In this paper, we consider the task of efficiently computing the numerical solution of evolutionary complex Ginzburg--Landau equations.","To this aim, we employ high-order exponential methods of splitting and Lawson type for the time integration.","These schemes enjoy favorable stability properties and, in particular, do not show restrictions on the time step size due to the underlying stiffness of the models.","The needed actions of matrix exponentials are efficiently realized with pointwise operations in Fourier space (when the model is considered with periodic boundary conditions) or by using a tensor-oriented approach that suitably employs the so-called $\\mu$-mode products (when the semidiscretization in space is performed with finite differences).","The overall effectiveness of the approach is demonstrated by running simulations on a variety of two- and three-dimensional (systems of) complex Ginzburg--Landau equations with cubic and cubic-quintic nonlinearities, which are widely considered in literature to model relevant physical phenomena.","In fact, in all instances high-order exponential-type schemes can outperform standard techniques to integrate in time the models under consideration, i.e., the well-known split-step method and the explicit fourth-order Runge--Kutta integrator."],"url":"http://arxiv.org/abs/2403.02816v1","category":"math.NA"}
{"created":"2024-03-05 09:28:40","title":"Linear quadratic control of nonlinear systems with Koopman operator learning and the Nystr\u00f6m method","abstract":"In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\\\"om approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\\\"om approximation. More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.","sentences":["In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems.","While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\\\"om approximation) can be used to achieve huge computational savings while preserving accuracy.","Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\\\"om approximation.","More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size.","Theoretical findings are complemented by numerical experiments corroborating our results."],"url":"http://arxiv.org/abs/2403.02811v1","category":"math.OC"}
{"created":"2024-03-05 09:19:18","title":"Dimorphos orbit determination from mutual events photometry","abstract":"The NASA Double Asteroid Redirection Test (DART) spacecraft successfully impacted the Didymos-Dimorphos binary asteroid system on 2022 September 26 UTC. We provide an update to its pre-impact mutual orbit and estimate the post-impact physical and orbital parameters, derived using ground-based photometric observations taken from July 2022 to February 2023. We found that the total change of the orbital period was $-33.240 \\pm 0.072$ min. (all uncertainties are 3$\\sigma$). We obtained the eccentricity of the post-impact orbit to be $0.028 \\pm 0.016$ and the apsidal precession rate of $7.3 \\pm 2.0$ deg./day from the impact to 2022 December 2. The data taken later in December to February suggest that the eccentricity dropped close to zero or the orbit became chaotic approximately 70 days after the impact. Most of the period change took place immediately after the impact but in a few weeks following the impact it was followed by additional change of $-27^{+19}_{-58}$ seconds or $-19 \\pm 18$ seconds (the two values depend on the approach we used to describe the evolution of the orbital period after the impact -- an exponentially decreasing angular acceleration or an assumption of a constant orbital period, which changed abruptly some time after the impact, respectively). We estimate the pre-impact Dimorphos-Didymos size ratio was $0.223 \\pm 0.012$ and the post-impact is $0.202 \\pm 0.018$, which indicates a marginally significant reduction of Dimorphos' volume by ($9 \\pm 9) \\%$ as the result of the impact.","sentences":["The NASA Double Asteroid Redirection Test (DART) spacecraft successfully impacted the Didymos-Dimorphos binary asteroid system on 2022 September 26 UTC.","We provide an update to its pre-impact mutual orbit and estimate the post-impact physical and orbital parameters, derived using ground-based photometric observations taken from July 2022 to February 2023.","We found that the total change of the orbital period was $-33.240 \\pm 0.072$ min. (all uncertainties are 3$\\sigma$).","We obtained the eccentricity of the post-impact orbit to be $0.028 \\pm 0.016$ and the apsidal precession rate of $7.3 \\pm 2.0$ deg./day from the impact to 2022 December 2.","The data taken later in December to February suggest that the eccentricity dropped close to zero or the orbit became chaotic approximately 70 days after the impact.","Most of the period change took place immediately after the impact but in a few weeks following the impact it was followed by additional change of $-27^{+19}_{-58}$ seconds or $-19 \\pm 18$ seconds (the two values depend on the approach we used to describe the evolution of the orbital period after the impact -- an exponentially decreasing angular acceleration or an assumption of a constant orbital period, which changed abruptly some time after the impact, respectively).","We estimate the pre-impact Dimorphos-Didymos size ratio was $0.223 \\pm 0.012$ and the post-impact is $0.202 \\pm 0.018$, which indicates a marginally significant reduction of Dimorphos' volume by ($9 \\pm 9) \\%$ as the result of the impact."],"url":"http://arxiv.org/abs/2403.02804v1","category":"astro-ph.EP"}
{"created":"2024-03-05 09:12:45","title":"Analytic mappings of the unit disk which almost preserve hyperbolic area","abstract":"In this paper, we study analytic self-maps of the unit disk which distort hyperbolic area of large hyperbolic disks by a bounded amount. We give a number of characterizations involving angular derivatives, Lipschitz extensions, M\\\"obius distortion, the distribution of critical points and Aleksandrov-Clark measures. We also study Lyapunov exponents of their Aleksandrov-Clark measures.","sentences":["In this paper, we study analytic self-maps of the unit disk which distort hyperbolic area of large hyperbolic disks by a bounded amount.","We give a number of characterizations involving angular derivatives, Lipschitz extensions, M\\\"obius distortion, the distribution of critical points and Aleksandrov-Clark measures.","We also study Lyapunov exponents of their Aleksandrov-Clark measures."],"url":"http://arxiv.org/abs/2403.02798v1","category":"math.CV"}
{"created":"2024-03-05 09:06:39","title":"The summatory function of the M\u00f6bius function involving the greatest common divisor","abstract":"Let $\\gcd(m,n)$ denote the greatest common divisor of the positive integers $m$ and $n$, and let $\\mu$ represent the M\\\" obius function. For any real number $x>5$, we define the summatory function of the M\\\" obius function involving the greatest common divisor as $ S_{\\mu}(x) := \\sum_{mn\\leq x} \\mu(\\gcd(m,n)). $ In this paper, we present an asymptotic formula for $S_{\\mu}(x)$. Assuming the Riemann Hypothesis, we delve further into the asymptotic behavior of $S_{\\mu}(x)$ and derive a mean square estimate for its error term. Our proof employs the Perron formula, Parseval's theorem, complex integration techniques, and the properties of the Riemann zeta-function.","sentences":["Let $\\gcd(m,n)$ denote the greatest common divisor of the positive integers $m$ and $n$, and let $\\mu$ represent the M\\\" obius function.","For any real number $x>5$, we define the summatory function of the M\\\" obius function involving the greatest common divisor as $ S_{\\mu}(x) := \\sum_{mn\\leq x} \\mu(\\gcd(m,n)).","$","In this paper, we present an asymptotic formula for $S_{\\mu}(x)$. Assuming the Riemann Hypothesis, we delve further into the asymptotic behavior of $S_{\\mu}(x)$ and derive a mean square estimate for its error term.","Our proof employs the Perron formula, Parseval's theorem, complex integration techniques, and the properties of the Riemann zeta-function."],"url":"http://arxiv.org/abs/2403.02792v1","category":"math.NT"}
{"created":"2024-03-05 09:00:44","title":"What Can We Learn from Directed Flow at STAR-FTX Energies?","abstract":"We present results of simulations of directed flow of various hadrons in Au+Au collisions at collision energies of $\\sqrt{s_{NN}}=$ 3 and 4.5 GeV. Simulations are performed within the model three-fluid dynamics (3FD) and the event simulator based on it (THESEUS). The results are compared with recent STAR data. The directed flows of various particles provide information on dynamics in various parts and at various stages of the colliding system depending on the particle. However, the information on the equation of state is not always directly accessible because of strong influence of the afterburner stage or insufficient equilibration of the matter. It is found that the crossover scenario gives the best overall description of the data. This crossover EoS is soft in the hadronic phase. The transition into QGP in Au+Au collisions occurs at collision energies between 3 and 4.5 GeV, at baryon densities $n_B \\geq 4 n_0$ and temperatures $\\approx 150$ MeV. In-medium effects in the directed flow of (anti)kaons are discussed.","sentences":["We present results of simulations of directed flow of various hadrons in Au+Au collisions at collision energies of $\\sqrt{s_{NN}}=$ 3 and 4.5 GeV. Simulations are performed within the model three-fluid dynamics (3FD) and the event simulator based on it (THESEUS).","The results are compared with recent STAR data.","The directed flows of various particles provide information on dynamics in various parts and at various stages of the colliding system depending on the particle.","However, the information on the equation of state is not always directly accessible because of strong influence of the afterburner stage or insufficient equilibration of the matter.","It is found that the crossover scenario gives the best overall description of the data.","This crossover EoS is soft in the hadronic phase.","The transition into QGP in Au+Au collisions occurs at collision energies between 3 and 4.5 GeV, at baryon densities $n_B \\geq 4 n_0$ and temperatures $\\approx 150$ MeV.","In-medium effects in the directed flow of (anti)kaons are discussed."],"url":"http://arxiv.org/abs/2403.02787v1","category":"nucl-th"}
{"created":"2024-03-05 08:55:51","title":"Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos","abstract":"In this paper, we explore the capability of an agent to construct a logical sequence of action steps, thereby assembling a strategic procedural plan. This plan is crucial for navigating from an initial visual observation to a target visual outcome, as depicted in real-life instructional videos. Existing works have attained partial success by extensively leveraging various sources of information available in the datasets, such as heavy intermediate visual observations, procedural names, or natural language step-by-step instructions, for features or supervision signals. However, the task remains formidable due to the implicit causal constraints in the sequencing of steps and the variability inherent in multiple feasible plans. To tackle these intricacies that previous efforts have overlooked, we propose to enhance the capabilities of the agent by infusing it with procedural knowledge. This knowledge, sourced from training procedure plans and structured as a directed weighted graph, equips the agent to better navigate the complexities of step sequencing and its potential variations. We coin our approach KEPP, a novel Knowledge-Enhanced Procedure Planning system, which harnesses a probabilistic procedural knowledge graph extracted from training data, effectively acting as a comprehensive textbook for the training domain. Experimental evaluations across three widely-used datasets under settings of varying complexity reveal that KEPP attains superior, state-of-the-art results while requiring only minimal supervision.","sentences":["In this paper, we explore the capability of an agent to construct a logical sequence of action steps, thereby assembling a strategic procedural plan.","This plan is crucial for navigating from an initial visual observation to a target visual outcome, as depicted in real-life instructional videos.","Existing works have attained partial success by extensively leveraging various sources of information available in the datasets, such as heavy intermediate visual observations, procedural names, or natural language step-by-step instructions, for features or supervision signals.","However, the task remains formidable due to the implicit causal constraints in the sequencing of steps and the variability inherent in multiple feasible plans.","To tackle these intricacies that previous efforts have overlooked, we propose to enhance the capabilities of the agent by infusing it with procedural knowledge.","This knowledge, sourced from training procedure plans and structured as a directed weighted graph, equips the agent to better navigate the complexities of step sequencing and its potential variations.","We coin our approach KEPP, a novel Knowledge-Enhanced Procedure Planning system, which harnesses a probabilistic procedural knowledge graph extracted from training data, effectively acting as a comprehensive textbook for the training domain.","Experimental evaluations across three widely-used datasets under settings of varying complexity reveal that KEPP attains superior, state-of-the-art results while requiring only minimal supervision."],"url":"http://arxiv.org/abs/2403.02782v1","category":"cs.CV"}
{"created":"2024-03-05 08:50:56","title":"Reverse inequalities for quasi-Riesz transform on the Vicsek cable system","abstract":"This work is devoted to the study of so-called ``reverse Riesz'' inequalities and suitable variants in the context of some fractal-like cable systems. It was already proved by L. Chen, T. Coulhon, J. Feneuil and the second author that, in the Vicsek cable system, the inequality $\\left\\Vert \\Delta^{1/2}f\\right\\Vert_p\\lesssim \\left\\Vert \\nabla f\\right\\Vert_p$ is false for all $p\\in [1,2)$. Following a recent joint paper by the two authors and M. Yang, we examine the validity of ``reverse quasi-Riesz'' inequalities, of the form $\\left\\Vert \\Delta^{\\gamma}e^{-\\Delta}f\\right\\Vert_p\\lesssim \\left\\Vert \\nabla f\\right\\Vert_p$, in the (unbounded) Vicsek cable system, for $p\\in (1,+\\infty)$ and $\\gamma>0$. These reverse inequalities are strongly related to the problem of $L^p$ boundedness of the operators $\\nabla e^{-\\Delta}\\Delta^{-\\varepsilon}$, the so-called ``quasi-Riesz transforms'' (at infinity), introduced by L. Chen in her PhD thesis. Our main result is an almost complete characterization of the sets of $\\gamma\\in (0,1)$ and $p\\in (1,+\\infty)$ such that the reverse quasi-Riesz inequality holds in the Vicsek cable system. It remains an open question to investigate reverse quasi-Riesz inequalities for other cable systems, or for manifolds built out of these.","sentences":["This work is devoted to the study of so-called ``reverse Riesz'' inequalities and suitable variants in the context of some fractal-like cable systems.","It was already proved by L. Chen, T. Coulhon, J. Feneuil and the second author that, in the Vicsek cable system, the inequality $\\left\\Vert \\Delta^{1/2}f\\right\\Vert_p\\lesssim \\left\\Vert \\nabla f\\right\\Vert_p$ is false for all $p\\in [1,2)$. Following a recent joint paper by the two authors and M. Yang, we examine the validity of ``reverse quasi-Riesz'' inequalities, of the form $\\left\\Vert \\Delta^{\\gamma}e^{-\\Delta}f\\right\\Vert_p\\lesssim \\left\\Vert \\nabla f\\right\\Vert_p$, in the (unbounded) Vicsek cable system, for $p\\in (1,+\\infty)$ and $\\gamma>0$. These reverse inequalities are strongly related to the problem of $L^p$ boundedness of the operators $\\nabla e^{-\\Delta}\\Delta^{-\\varepsilon}$, the so-called ``quasi-Riesz transforms'' (at infinity), introduced by L. Chen in her PhD thesis.","Our main result is an almost complete characterization of the sets of $\\gamma\\in (0,1)$ and $p\\in (1,+\\infty)$ such that the reverse quasi-Riesz inequality holds in the Vicsek cable system.","It remains an open question to investigate reverse quasi-Riesz inequalities for other cable systems, or for manifolds built out of these."],"url":"http://arxiv.org/abs/2403.02779v1","category":"math.AP"}
{"created":"2024-03-05 08:33:55","title":"Unbalanced L1 optimal transport for vector valued measures and application to Full Waveform Inversion","abstract":"Optimal transport has recently started to be successfully employed to define misfit or loss functions in inverse problems. However, it is a problem intrinsically defined for positive (probability) measures and therefore strategies are needed for its applications in more general settings of interest. In this paper we introduce an unbalanced optimal transport problem for vector valued measures starting from the $L^1$ optimal transport. By lifting data in a self-dual cone of a higher dimensional vector space, we show that one can recover a meaningful transport problem. We show that the favorable computational complexity of the $L^1$ problem, an advantage compared to other formulations of optimal transport, is inherited by our vector extension. We consider both a one-homogeneous and a two-homogeneous penalization for the imbalance of mass, the latter being potentially relevant for applications to physics based problems. In particular, we demonstrate the potential of our strategy for full waveform inversion, an inverse problem for high resolution seismic imaging.","sentences":["Optimal transport has recently started to be successfully employed to define misfit or loss functions in inverse problems.","However, it is a problem intrinsically defined for positive (probability) measures and therefore strategies are needed for its applications in more general settings of interest.","In this paper we introduce an unbalanced optimal transport problem for vector valued measures starting from the $L^1$ optimal transport.","By lifting data in a self-dual cone of a higher dimensional vector space, we show that one can recover a meaningful transport problem.","We show that the favorable computational complexity of the $L^1$ problem, an advantage compared to other formulations of optimal transport, is inherited by our vector extension.","We consider both a one-homogeneous and a two-homogeneous penalization for the imbalance of mass, the latter being potentially relevant for applications to physics based problems.","In particular, we demonstrate the potential of our strategy for full waveform inversion, an inverse problem for high resolution seismic imaging."],"url":"http://arxiv.org/abs/2403.02764v1","category":"math.OC"}
{"created":"2024-03-05 08:32:17","title":"Quantum Zeno Monte Carlo for observable measurement","abstract":"The advent of logical quantum processors marks the beginning of the early stages of error-corrected quantum computation. As a bridge between the noisy intermediate scale quantum (NISQ) era and the fault-tolerant quantum computing (FTQC) era, these devices and their successors have the potential to revolutionize the solution of classically challenging problems. An important application of quantum computers is to calculate observables of quantum systems. This problem is crucial for solving quantum many-body and optimization problems. However, due to limited error correction capabilities, this new era are still susceptible to noise, thereby necessitating new quantum algorithms with polynomial complexity as well as noisy-resilency. This paper proposes a new noise-resilient and ansatz-free algorithm, called Quantum Zeno Monte Carlo. It utilizes the quantum Zeno effect and Monte Carlo integration for multi-step adiabatic transitions to the target eigenstates. It can efficiently find static as well as dynamic physical properties such as ground state energy, excited state energies, and Green's function without the use of variational parameters. This algorithm offers a polynomial computational cost and quantum circuit depth that is significantly lower than the quantum phase estimation.","sentences":["The advent of logical quantum processors marks the beginning of the early stages of error-corrected quantum computation.","As a bridge between the noisy intermediate scale quantum (NISQ) era and the fault-tolerant quantum computing (FTQC) era, these devices and their successors have the potential to revolutionize the solution of classically challenging problems.","An important application of quantum computers is to calculate observables of quantum systems.","This problem is crucial for solving quantum many-body and optimization problems.","However, due to limited error correction capabilities, this new era are still susceptible to noise, thereby necessitating new quantum algorithms with polynomial complexity as well as noisy-resilency.","This paper proposes a new noise-resilient and ansatz-free algorithm, called Quantum Zeno Monte Carlo.","It utilizes the quantum Zeno effect and Monte Carlo integration for multi-step adiabatic transitions to the target eigenstates.","It can efficiently find static as well as dynamic physical properties such as ground state energy, excited state energies, and Green's function without the use of variational parameters.","This algorithm offers a polynomial computational cost and quantum circuit depth that is significantly lower than the quantum phase estimation."],"url":"http://arxiv.org/abs/2403.02763v1","category":"quant-ph"}
{"created":"2024-03-05 08:31:49","title":"Noise-induced transition in optimal solutions of variational quantum algorithms","abstract":"Variational quantum algorithms are promising candidates for delivering practical quantum advantage on noisy intermediate-scale quantum (NISQ) hardware. However, optimizing the noisy cost functions associated with these algorithms is challenging for system sizes relevant to quantum advantage. In this work, we investigate the effect of noise on optimization by studying a variational quantum eigensolver (VQE) algorithm calculating the ground state of a spin chain model, and we observe an abrupt transition induced by noise to the optimal solutions. We will present numerical simulations, a demonstration using an IBM quantum processor unit (QPU), and a theoretical analysis indicating the origin of this transition. Our findings suggest that careful analysis is crucial to avoid misinterpreting the noise-induced features as genuine algorithm results.","sentences":["Variational quantum algorithms are promising candidates for delivering practical quantum advantage on noisy intermediate-scale quantum (NISQ) hardware.","However, optimizing the noisy cost functions associated with these algorithms is challenging for system sizes relevant to quantum advantage.","In this work, we investigate the effect of noise on optimization by studying a variational quantum eigensolver (VQE) algorithm calculating the ground state of a spin chain model, and we observe an abrupt transition induced by noise to the optimal solutions.","We will present numerical simulations, a demonstration using an IBM quantum processor unit (QPU), and a theoretical analysis indicating the origin of this transition.","Our findings suggest that careful analysis is crucial to avoid misinterpreting the noise-induced features as genuine algorithm results."],"url":"http://arxiv.org/abs/2403.02762v1","category":"quant-ph"}
{"created":"2024-03-05 08:19:44","title":"Learning Group Activity Features Through Person Attribute Prediction","abstract":"This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector. Unlike prior work in which the manual annotation of group activities is required for supervised learning, our method learns the GAF through person attribute prediction without group activity annotations. By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity. As a person attribute, we propose to use a person's action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation. In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly. Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets. Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes. Code: https://github.com/chihina/GAFL-CVPR2024.","sentences":["This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector.","Unlike prior work in which the manual annotation of group activities is required for supervised learning, our method learns the GAF through person attribute prediction without group activity annotations.","By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity.","As a person attribute, we propose to use a person's action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation.","In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly.","Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets.","Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes.","Code: https://github.com/chihina/GAFL-CVPR2024."],"url":"http://arxiv.org/abs/2403.02753v1","category":"cs.CV"}
{"created":"2024-03-05 08:02:18","title":"A numerical algorithm for solving the coupled Schr\u00f6dinger equations using inverse power method","abstract":"The inverse power method is a numerical algorithm to obtain the eigenvectors of a matrix. In this work, we develop an iteration algorithm, based on the inverse power method, to numerically solve the Schr\\\"odinger equation that couples an arbitrary number of components. Such an algorithm can also be applied to the multi-body systems. To show the power and accuracy of this method, we also present an example of solving the Dirac equation under the presence of an external scalar potential and a constant magnetic field, with source code publicly available.","sentences":["The inverse power method is a numerical algorithm to obtain the eigenvectors of a matrix.","In this work, we develop an iteration algorithm, based on the inverse power method, to numerically solve the Schr\\\"odinger equation that couples an arbitrary number of components.","Such an algorithm can also be applied to the multi-body systems.","To show the power and accuracy of this method, we also present an example of solving the Dirac equation under the presence of an external scalar potential and a constant magnetic field, with source code publicly available."],"url":"http://arxiv.org/abs/2403.02747v1","category":"physics.comp-ph"}
{"created":"2024-03-05 08:02:00","title":"Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels","abstract":"Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.","sentences":["Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity.","However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area.","In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR).","Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms.","Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information.","Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images.","Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels."],"url":"http://arxiv.org/abs/2403.02746v1","category":"cs.CV"}
{"created":"2024-03-05 07:58:02","title":"Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments","abstract":"With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased. Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks. However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices. Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks. In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices. This paper proposes a self-adaptive anomaly detection system for IoT traffic, including unknown attacks. The proposed system comprises a honeypot server and a gateway. The honeypot server continuously captures traffic and adaptively generates an anomaly detection model using real-time captured traffic. Thereafter, the gateway uses the generated model to detect anomalous traffic. Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic. Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware. The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against both known and unknown attacks.","sentences":["With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased.","Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks.","However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices.","Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks.","In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices.","This paper proposes a self-adaptive anomaly detection system for IoT traffic, including unknown attacks.","The proposed system comprises a honeypot server and a gateway.","The honeypot server continuously captures traffic and adaptively generates an anomaly detection model using real-time captured traffic.","Thereafter, the gateway uses the generated model to detect anomalous traffic.","Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic.","Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware.","The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against both known and unknown attacks."],"url":"http://arxiv.org/abs/2403.02744v1","category":"cs.CR"}
{"created":"2024-03-05 07:47:34","title":"Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment","abstract":"Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to obtain the representation of the samples precisely and estimate the causal effect more accurately, contrastive learning is used to fine-tune the encoder of the samples by aligning the space of the encoder with the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source LLMs.","sentences":["Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases.","Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs.","To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs.","In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.","The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-door adjustment to mitigate model biases.","Moreover, to obtain the representation of the samples precisely and estimate the causal effect more accurately, contrastive learning is used to fine-tune the encoder of the samples by aligning the space of the encoder with the LLM.","Experimental results show that the proposed causal prompting approach achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source LLMs."],"url":"http://arxiv.org/abs/2403.02738v1","category":"cs.CL"}
{"created":"2024-03-05 07:45:29","title":"Neural Fractional Differential Equations","abstract":"Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours.   This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions.   Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data.   This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural ODE in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems.","sentences":["Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering.","They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours.   ","This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions.   ","Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data.   ","This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture.","The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural ODE in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems."],"url":"http://arxiv.org/abs/2403.02737v1","category":"cs.LG"}
{"created":"2024-03-05 07:43:24","title":"Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators","abstract":"Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations. This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading. Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs. We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids). The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases.","sentences":["Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science.","Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations.","This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading.","Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs.","We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids).","The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases."],"url":"http://arxiv.org/abs/2403.02735v1","category":"cs.DC"}
{"created":"2024-03-05 07:42:13","title":"Strain tunable electronic ground states in two-dimensional iridate thin films","abstract":"Quantum phases of matter such as superconducting, ferromagnetic and Wigner crystal states are often driven by the two-dimensionality (2D) of correlated systems. Meanwhile, spin-orbit coupling (SOC) is a fundamental element leading to nontrivial topology which gives rise to quantum phenomena such as the large anomalous Hall effect and nontrivial superconductivity. However, the search for controllable platforms with both 2D and SOC has been relatively overlooked so far. Here, we control and study the electronic ground states of iridate ultrathin films having both 2D and SOC by angle-resolved photoemission spectroscopy (ARPES) and dynamical mean field theory (DMFT) calculations. The metallicity of SrIrO$_3$ ultrathin films is controlled down to a monolayer by dimensional and strain manipulation. Our results suggest that the iridate ultrathin films can be a controllable 2D SOC platform exhibiting a variety of phenomena for future functional devices.","sentences":["Quantum phases of matter such as superconducting, ferromagnetic and Wigner crystal states are often driven by the two-dimensionality (2D) of correlated systems.","Meanwhile, spin-orbit coupling (SOC) is a fundamental element leading to nontrivial topology which gives rise to quantum phenomena such as the large anomalous Hall effect and nontrivial superconductivity.","However, the search for controllable platforms with both 2D and SOC has been relatively overlooked so far.","Here, we control and study the electronic ground states of iridate ultrathin films having both 2D and SOC by angle-resolved photoemission spectroscopy (ARPES) and dynamical mean field theory (DMFT) calculations.","The metallicity of SrIrO$_3$ ultrathin films is controlled down to a monolayer by dimensional and strain manipulation.","Our results suggest that the iridate ultrathin films can be a controllable 2D SOC platform exhibiting a variety of phenomena for future functional devices."],"url":"http://arxiv.org/abs/2403.02734v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 07:37:47","title":"A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks","abstract":"Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimentally demonstrate that our method produces models that satisfy the constraints and also improves their predictive performance. Thus, ensuring compliance with critical system properties and also contributing to reducing data quantity requirements. Furthermore, we show that the proposed method improves the convergence to an optimal solution and improves the explainability of Neural ODE models. Our proposed two-stage training method can be used with any NN architectures.","sentences":["Real-world systems are often formulated as constrained optimization problems.","Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used.","However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model.","This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems.","In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages.","The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation.","The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region.","We experimentally demonstrate that our method produces models that satisfy the constraints and also improves their predictive performance.","Thus, ensuring compliance with critical system properties and also contributing to reducing data quantity requirements.","Furthermore, we show that the proposed method improves the convergence to an optimal solution and improves the explainability of Neural ODE models.","Our proposed two-stage training method can be used with any NN architectures."],"url":"http://arxiv.org/abs/2403.02730v1","category":"cs.LG"}
{"created":"2024-03-05 07:31:46","title":"A genome-scale deep learning model to predict gene expression changes of genetic perturbations from multiplex biological networks","abstract":"Systematic characterization of biological effects to genetic perturbation is essential to the application of molecular biology and biomedicine. However, the experimental exhaustion of genetic perturbations on the genome-wide scale is challenging. Here, we show that TranscriptionNet, a deep learning model that integrates multiple biological networks to systematically predict transcriptional profiles to three types of genetic perturbations based on transcriptional profiles induced by genetic perturbations in the L1000 project: RNA interference (RNAi), clustered regularly interspaced short palindromic repeat (CRISPR) and overexpression (OE). TranscriptionNet performs better than existing approaches in predicting inducible gene expression changes for all three types of genetic perturbations. TranscriptionNet can predict transcriptional profiles for all genes in existing biological networks and increases perturbational gene expression changes for each type of genetic perturbation from a few thousand to 26,945 genes. TranscriptionNet demonstrates strong generalization ability when comparing predicted and true gene expression changes on different external tasks. Overall, TranscriptionNet can systemically predict transcriptional consequences induced by perturbing genes on a genome-wide scale and thus holds promise to systemically detect gene function and enhance drug development and target discovery.","sentences":["Systematic characterization of biological effects to genetic perturbation is essential to the application of molecular biology and biomedicine.","However, the experimental exhaustion of genetic perturbations on the genome-wide scale is challenging.","Here, we show that TranscriptionNet, a deep learning model that integrates multiple biological networks to systematically predict transcriptional profiles to three types of genetic perturbations based on transcriptional profiles induced by genetic perturbations in the L1000 project: RNA interference (RNAi), clustered regularly interspaced short palindromic repeat (CRISPR) and overexpression (OE).","TranscriptionNet performs better than existing approaches in predicting inducible gene expression changes for all three types of genetic perturbations.","TranscriptionNet can predict transcriptional profiles for all genes in existing biological networks and increases perturbational gene expression changes for each type of genetic perturbation from a few thousand to 26,945 genes.","TranscriptionNet demonstrates strong generalization ability when comparing predicted and true gene expression changes on different external tasks.","Overall, TranscriptionNet can systemically predict transcriptional consequences induced by perturbing genes on a genome-wide scale and thus holds promise to systemically detect gene function and enhance drug development and target discovery."],"url":"http://arxiv.org/abs/2403.02724v1","category":"q-bio.GN"}
{"created":"2024-03-05 07:28:47","title":"Numerical simulation of charging up, accumulation of space charge and formation of discharges","abstract":"Aging and stability of gaseous ionization detectors are intricately related to charging up, accumulation of space charge and formation of discharges. All these phenomena, in their turn, depend on the dynamics of charged particles within the device. Because of the large number of particles involved and their complex interactions, the dynamic processes of generation and loss of charged particles, and their transport within the detector volume are extremely expensive to simulate numerically. In this work, we propose and evaluate possible algorithms / approaches that show some promise in relation to the above-mentioned problems. Several important ionization detectors having parallel plate configurations, such as GEM, Micromegas, RPCs and THGEMs, are considered for this purpose. Information related to primary ionization is obtained from HEED, while all the transport properties are evaluated using MAGBOLTZ. The transport dynamics have been followed using two different approaches. In one, particle description using neBEM-Garfield++ combination has been used. For this purpose, the neBEM solver has been significantly improved such that perturbations due to the charged particles present within the device are considered while estimating electric field. In the other approach, the transport is simulated following hydrodynamic model using COMSOL during which the electric field is also provided by COMSOL where it is easy to set up space charge effects. A comparison between these possible approaches will be presented. Effect of different simulation parameters will also be demonstrated using simple examples.","sentences":["Aging and stability of gaseous ionization detectors are intricately related to charging up, accumulation of space charge and formation of discharges.","All these phenomena, in their turn, depend on the dynamics of charged particles within the device.","Because of the large number of particles involved and their complex interactions, the dynamic processes of generation and loss of charged particles, and their transport within the detector volume are extremely expensive to simulate numerically.","In this work, we propose and evaluate possible algorithms / approaches that show some promise in relation to the above-mentioned problems.","Several important ionization detectors having parallel plate configurations, such as GEM, Micromegas, RPCs and THGEMs, are considered for this purpose.","Information related to primary ionization is obtained from HEED, while all the transport properties are evaluated using MAGBOLTZ.","The transport dynamics have been followed using two different approaches.","In one, particle description using neBEM-Garfield++ combination has been used.","For this purpose, the neBEM solver has been significantly improved such that perturbations due to the charged particles present within the device are considered while estimating electric field.","In the other approach, the transport is simulated following hydrodynamic model using COMSOL during which the electric field is also provided by COMSOL where it is easy to set up space charge effects.","A comparison between these possible approaches will be presented.","Effect of different simulation parameters will also be demonstrated using simple examples."],"url":"http://arxiv.org/abs/2403.02722v1","category":"physics.ins-det"}
{"created":"2024-03-05 07:08:06","title":"Breeze-7B Technical Report","abstract":"Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class.","sentences":["Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese.","This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model.","The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class."],"url":"http://arxiv.org/abs/2403.02712v1","category":"cs.CL"}
{"created":"2024-03-05 06:58:49","title":"Backfire Effect Reveals Early Controversy in Online Media","abstract":"The rapid development of online media has significantly facilitated the public's information consumption, knowledge acquisition, and opinion exchange. However, it has also led to more violent conflicts in online discussions. Therefore, controversy detection becomes important for computational and social sciences. Previous research on detection methods has primarily focused on larger datasets and more complex computational models but has rarely examined the underlying mechanisms of conflict, particularly the psychological motivations behind them. In this paper, we present evidence that conflicting posts tend to have a high proportion of \"ascending gradient of likes\", i.e., replies get more likes than comments. Additionally, there is a gradient in the number of replies between the neighboring tiers as well. We develop two new gradient features and demonstrate the common enhancement effect of our features in terms of controversy detection models. Further, multiple evaluation algorithms are used to compare structural, interactive, and textual features with the new features across multiple Chinese and English media. The results show that it is a general case that gradient features are significantly different in terms of controversy and are more important than other features. More thoroughly, we discuss the mechanism by which the ascending gradient emerges, suggesting that the case is related to the \"backfire effect\" in ideological conflicts that have received recent attention. The features formed by the psychological mechanism also show excellent detection performance in application scenarios where only a few hot information or early information are considered. Our findings can provide a new perspective for online conflict behavior analysis and early detection.","sentences":["The rapid development of online media has significantly facilitated the public's information consumption, knowledge acquisition, and opinion exchange.","However, it has also led to more violent conflicts in online discussions.","Therefore, controversy detection becomes important for computational and social sciences.","Previous research on detection methods has primarily focused on larger datasets and more complex computational models but has rarely examined the underlying mechanisms of conflict, particularly the psychological motivations behind them.","In this paper, we present evidence that conflicting posts tend to have a high proportion of \"ascending gradient of likes\", i.e., replies get more likes than comments.","Additionally, there is a gradient in the number of replies between the neighboring tiers as well.","We develop two new gradient features and demonstrate the common enhancement effect of our features in terms of controversy detection models.","Further, multiple evaluation algorithms are used to compare structural, interactive, and textual features with the new features across multiple Chinese and English media.","The results show that it is a general case that gradient features are significantly different in terms of controversy and are more important than other features.","More thoroughly, we discuss the mechanism by which the ascending gradient emerges, suggesting that the case is related to the \"backfire effect\" in ideological conflicts that have received recent attention.","The features formed by the psychological mechanism also show excellent detection performance in application scenarios where only a few hot information or early information are considered.","Our findings can provide a new perspective for online conflict behavior analysis and early detection."],"url":"http://arxiv.org/abs/2403.02708v1","category":"cs.SI"}
{"created":"2024-03-05 06:37:17","title":"Existence of de Almeida-Thouless-type instability in the transverse field Sherrington-Kirkpatrick model","abstract":"The interpolation method for mean field spin glass models developed by Guerra and Talagrand is extended to a quantum mean field spin glass model. This extension enables us to obtain both replica-symmetric (RS) and one step replica-symmetry breaking (1RSB) solutions of the free energy density in the transverse field Sherrington-Kirkpatrick model. It is shown that the RS solution is exact in the paramagnetic phase. We provide a sufficient condition on coupling constants where the 1RSB solution gives better bound than the RS one. This condition reduced to physical quantities in disordered single spin systems allows a simple computer-assisted proof for the existence of the de Almeida-Thouless-type instability.","sentences":["The interpolation method for mean field spin glass models developed by Guerra and Talagrand is extended to a quantum mean field spin glass model.","This extension enables us to obtain both replica-symmetric (RS) and one step replica-symmetry breaking (1RSB) solutions of the free energy density in the transverse field Sherrington-Kirkpatrick model.","It is shown that the RS solution is exact in the paramagnetic phase.","We provide a sufficient condition on coupling constants where the 1RSB solution gives better bound than the RS one.","This condition reduced to physical quantities in disordered single spin systems allows a simple computer-assisted proof for the existence of the de Almeida-Thouless-type instability."],"url":"http://arxiv.org/abs/2403.02699v1","category":"math-ph"}
{"created":"2024-03-05 06:24:05","title":"Low-rank matrix estimation via nonconvex spectral regularized methods in errors-in-variables matrix regression","abstract":"High-dimensional matrix regression has been studied in various aspects, such as statistical properties, computational efficiency and application to specific instances including multivariate regression, system identification and matrix compressed sensing. Current studies mainly consider the idealized case that the covariate matrix is obtained without noise, while the more realistic scenario that the covariates may always be corrupted with noise or missing data has received little attention. We consider the general errors-in-variables matrix regression model and proposed a unified framework for low-rank estimation based on nonconvex spectral regularization. Then in the statistical aspect, recovery bounds for any stationary points are provided to achieve statistical consistency. In the computational aspect, the proximal gradient method is applied to solve the nonconvex optimization problem and is proved to converge in polynomial time. Consequences for specific matrix compressed sensing models with additive noise and missing data are obtained via verifying corresponding regularity conditions. Finally, the performance of the proposed nonconvex estimation method is illustrated by numerical experiments.","sentences":["High-dimensional matrix regression has been studied in various aspects, such as statistical properties, computational efficiency and application to specific instances including multivariate regression, system identification and matrix compressed sensing.","Current studies mainly consider the idealized case that the covariate matrix is obtained without noise, while the more realistic scenario that the covariates may always be corrupted with noise or missing data has received little attention.","We consider the general errors-in-variables matrix regression model and proposed a unified framework for low-rank estimation based on nonconvex spectral regularization.","Then in the statistical aspect, recovery bounds for any stationary points are provided to achieve statistical consistency.","In the computational aspect, the proximal gradient method is applied to solve the nonconvex optimization problem and is proved to converge in polynomial time.","Consequences for specific matrix compressed sensing models with additive noise and missing data are obtained via verifying corresponding regularity conditions.","Finally, the performance of the proposed nonconvex estimation method is illustrated by numerical experiments."],"url":"http://arxiv.org/abs/2403.02696v1","category":"math.ST"}
{"created":"2024-03-05 06:23:23","title":"Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming","abstract":"Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to introduce minimal transmission and computation overhead for mobile terminals. We also propose a model-agnostic meta-learning (MAML) based saliency prediction network trainer, which provides a few-sample fast training solution to obtain the prediction model by utilizing the information from the past models. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results show that our prediction approach can work in real-time for live video streaming and can achieve higher accuracies compared to other existing prediction methods on mobile end, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. We observe the accuracy of MFVP is 8.1$\\%$ to 28.7$\\%$ higher than other algorithms and achieves 3.73$\\%$ to 14.96$\\%$ higher average quality level and 49.6$\\%$ to 74.97$\\%$ less quality level change than other algorithms.","sentences":["Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames.","Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks.","Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability.","In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to introduce minimal transmission and computation overhead for mobile terminals.","We also propose a model-agnostic meta-learning (MAML) based saliency prediction network trainer, which provides a few-sample fast training solution to obtain the prediction model by utilizing the information from the past models.","We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem.","Extensive experiment results show that our prediction approach can work in real-time for live video streaming and can achieve higher accuracies compared to other existing prediction methods on mobile end, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects.","We observe the accuracy of MFVP is 8.1$\\%$ to 28.7$\\%$ higher than other algorithms and achieves 3.73$\\%$ to 14.96$\\%$ higher average quality level and 49.6$\\%$ to 74.97$\\%$ less quality level change than other algorithms."],"url":"http://arxiv.org/abs/2403.02693v1","category":"cs.MM"}
{"created":"2024-03-05 06:23:00","title":"Uplift Modeling for Target User Attacks on Recommender Systems","abstract":"Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA.","sentences":["Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users.","In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies.","Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance.","To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group.","In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework.","UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance.","Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA.","By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA."],"url":"http://arxiv.org/abs/2403.02692v1","category":"cs.IR"}
{"created":"2024-03-05 06:15:42","title":"Hierarchy of the echo state property in quantum reservoir computing","abstract":"The echo state property (ESP) represents a fundamental concept in the reservoir computing (RC) framework that ensures output-only training of reservoir networks by being agnostic to the initial states and far past inputs. However, the traditional definition of ESP does not describe possible non-stationary systems in which statistical properties evolve. To address this issue, we introduce two new categories of ESP: \\textit{non-stationary ESP}, designed for potentially non-stationary systems, and \\textit{subspace/subset ESP}, designed for systems whose subsystems have ESP. Following the definitions, we numerically demonstrate the correspondence between non-stationary ESP in the quantum reservoir computer (QRC) framework with typical Hamiltonian dynamics and input encoding methods using non-linear autoregressive moving-average (NARMA) tasks. We also confirm the correspondence by computing linear/non-linear memory capacities that quantify input-dependent components within reservoir states. Our study presents a new understanding of the practical design of QRC and other possibly non-stationary RC systems in which non-stationary systems and subsystems are exploited.","sentences":["The echo state property (ESP) represents a fundamental concept in the reservoir computing (RC) framework that ensures output-only training of reservoir networks by being agnostic to the initial states and far past inputs.","However, the traditional definition of ESP does not describe possible non-stationary systems in which statistical properties evolve.","To address this issue, we introduce two new categories of ESP: \\textit{non-stationary ESP}, designed for potentially non-stationary systems, and \\textit{subspace/subset ESP}, designed for systems whose subsystems have ESP.","Following the definitions, we numerically demonstrate the correspondence between non-stationary ESP in the quantum reservoir computer (QRC) framework with typical Hamiltonian dynamics and input encoding methods using non-linear autoregressive moving-average (NARMA) tasks.","We also confirm the correspondence by computing linear/non-linear memory capacities that quantify input-dependent components within reservoir states.","Our study presents a new understanding of the practical design of QRC and other possibly non-stationary RC systems in which non-stationary systems and subsystems are exploited."],"url":"http://arxiv.org/abs/2403.02686v1","category":"quant-ph"}
{"created":"2024-03-05 06:14:38","title":"Radiation and Heat Transport in Divergent Shock-Bubble Interactions","abstract":"Shock-bubble interactions (SBI) are important across a wide range of physical systems. In inertial confinement fusion, interactions between laser-driven shocks and micro-voids in both ablators and foam targets generate instabilities that are a major obstacle in achieving ignition. Experiments imaging the collapse of such voids at high energy densities (HED) are constrained by spatial and temporal resolution, making simulations a vital tool in understanding these systems. In this study, we benchmark several radiation and thermal transport models in the xRAGE hydrodynamic code against experimental images of a collapsing mesoscale void during the passage of a 300 GPa shock. We also quantitatively examine the role of transport physics in the evolution of the SBI. This allows us to understand the dynamics of the interaction at timescales shorter than experimental imaging framerates. We find that all radiation models examined reproduce empirical shock velocities within experimental error. Radiation transport is found to reduce shock pressures by providing an additional energy pathway in the ablation region, but this effect is small ($\\sim$1\\% of total shock pressure). Employing a flux-limited Spitzer model for heat conduction, we find that flux limiters between 0.03 and 0.10 produce agreement with experimental velocities, suggesting that the system is well-within the Spitzer regime. Higher heat conduction is found to lower temperatures in the ablated plasma and to prevent secondary shocks at the ablation front, resulting in weaker primary shocks. Finally, we confirm that the SBI-driven instabilities observed in the HED regime are baroclinically driven, as in the low energy case.","sentences":["Shock-bubble interactions (SBI) are important across a wide range of physical systems.","In inertial confinement fusion, interactions between laser-driven shocks and micro-voids in both ablators and foam targets generate instabilities that are a major obstacle in achieving ignition.","Experiments imaging the collapse of such voids at high energy densities (HED) are constrained by spatial and temporal resolution, making simulations a vital tool in understanding these systems.","In this study, we benchmark several radiation and thermal transport models in the xRAGE hydrodynamic code against experimental images of a collapsing mesoscale void during the passage of a 300 GPa shock.","We also quantitatively examine the role of transport physics in the evolution of the SBI.","This allows us to understand the dynamics of the interaction at timescales shorter than experimental imaging framerates.","We find that all radiation models examined reproduce empirical shock velocities within experimental error.","Radiation transport is found to reduce shock pressures by providing an additional energy pathway in the ablation region, but this effect is small ($\\sim$1\\% of total shock pressure).","Employing a flux-limited Spitzer model for heat conduction, we find that flux limiters between 0.03 and 0.10 produce agreement with experimental velocities, suggesting that the system is well-within the Spitzer regime.","Higher heat conduction is found to lower temperatures in the ablated plasma and to prevent secondary shocks at the ablation front, resulting in weaker primary shocks.","Finally, we confirm that the SBI-driven instabilities observed in the HED regime are baroclinically driven, as in the low energy case."],"url":"http://arxiv.org/abs/2403.02684v1","category":"physics.plasm-ph"}
{"created":"2024-03-05 06:09:35","title":"A Dual-Level Cancelable Framework for Palmprint Verification and Hack-Proof Data Storage","abstract":"In recent years, palmprints have been widely used for individual verification. The rich privacy information in palmprint data necessitates its protection to ensure security and privacy without sacrificing system performance. Existing systems often use cancelable technologies to protect templates, but these technologies ignore the potential risk of data leakage. Upon breaching the system and gaining access to the stored database, a hacker could easily manipulate the stored templates, compromising the security of the verification system. To address this issue, we propose a dual-level cancelable palmprint verification framework in this paper. Specifically, the raw template is initially encrypted using a competition hashing network with a first-level token, facilitating the end-to-end generation of cancelable templates. Different from previous works, the protected template undergoes further encryption to differentiate the second-level protected template from the first-level one. The system specifically creates a negative database (NDB) with the second-level token for dual-level protection during the enrollment stage. Reversing the NDB is NP-hard and a fine-grained algorithm for NDB generation is introduced to manage the noise and specified bits. During the verification stage, we propose an NDB matching algorithm based on matrix operation to accelerate the matching process of previous NDB methods caused by dictionary-based matching rules. This approach circumvents the need to store templates identical to those utilized for verification, reducing the risk of potential data leakage. Extensive experiments conducted on public palmprint datasets have confirmed the effectiveness and generality of the proposed framework. Upon acceptance of the paper, the code will be accessible at https://github.com/Deep-Imaging-Group/NPR.","sentences":["In recent years, palmprints have been widely used for individual verification.","The rich privacy information in palmprint data necessitates its protection to ensure security and privacy without sacrificing system performance.","Existing systems often use cancelable technologies to protect templates, but these technologies ignore the potential risk of data leakage.","Upon breaching the system and gaining access to the stored database, a hacker could easily manipulate the stored templates, compromising the security of the verification system.","To address this issue, we propose a dual-level cancelable palmprint verification framework in this paper.","Specifically, the raw template is initially encrypted using a competition hashing network with a first-level token, facilitating the end-to-end generation of cancelable templates.","Different from previous works, the protected template undergoes further encryption to differentiate the second-level protected template from the first-level one.","The system specifically creates a negative database (NDB) with the second-level token for dual-level protection during the enrollment stage.","Reversing the NDB is NP-hard and a fine-grained algorithm for NDB generation is introduced to manage the noise and specified bits.","During the verification stage, we propose an NDB matching algorithm based on matrix operation to accelerate the matching process of previous NDB methods caused by dictionary-based matching rules.","This approach circumvents the need to store templates identical to those utilized for verification, reducing the risk of potential data leakage.","Extensive experiments conducted on public palmprint datasets have confirmed the effectiveness and generality of the proposed framework.","Upon acceptance of the paper, the code will be accessible at https://github.com/Deep-Imaging-Group/NPR."],"url":"http://arxiv.org/abs/2403.02680v1","category":"cs.CR"}
{"created":"2024-03-05 06:06:40","title":"An Optimal Baseband Delay-Based Beam Squint Removal Scheme across a Range of Steering Angles for Digital Wideband Beamformers in Radars","abstract":"This paper is an attempt to mitigate the beam squint happening due to frequency-dependent phase shifts in the wideband beamforming scenario, specifically in radar applications. The estimation of the direction of arrival is significant for precise target detection in radars. The undesirable beam squint effect due to the phase shift-only mechanism in conventional phased array systems, which becomes exacerbated when dealing with wide bandwidth signals, is analyzed for a large set of steering angles in this paper. An optimum baseband delay combined with the phase shift technique is proposed for wideband radar beamforming to mitigate beam squint effectively. This technique has been demonstrated to function properly with 1-GHz carrier frequency for signals with wide bandwidths of up to +/-250MHz and for steering angles ranging from 0 to 90 degrees.","sentences":["This paper is an attempt to mitigate the beam squint happening due to frequency-dependent phase shifts in the wideband beamforming scenario, specifically in radar applications.","The estimation of the direction of arrival is significant for precise target detection in radars.","The undesirable beam squint effect due to the phase shift-only mechanism in conventional phased array systems, which becomes exacerbated when dealing with wide bandwidth signals, is analyzed for a large set of steering angles in this paper.","An optimum baseband delay combined with the phase shift technique is proposed for wideband radar beamforming to mitigate beam squint effectively.","This technique has been demonstrated to function properly with 1-GHz carrier frequency for signals with wide bandwidths of up to +/-250MHz","and for steering angles ranging from 0 to 90 degrees."],"url":"http://arxiv.org/abs/2403.02679v1","category":"eess.SP"}
{"created":"2024-03-05 05:53:09","title":"Revisiting Meta-evaluation for Grammatical Error Correction","abstract":"Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.","sentences":["Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments.","However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems.","These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques.","To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation.","SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses.","The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, suggest that edit-based metrics may have been underestimated in existing studies.","Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits."],"url":"http://arxiv.org/abs/2403.02674v1","category":"cs.CL"}
{"created":"2024-03-05 05:50:30","title":"On the convergence of conditional gradient method for unbounded multiobjective optimization problems","abstract":"This paper focuses on developing a conditional gradient algorithm for multiobjective optimization problems with an unbounded feasible region. We employ the concept of recession cone to establish the well-defined nature of the algorithm. The asymptotic convergence property and the iteration-complexity bound are established under mild assumptions. Numerical examples are provided to verify the algorithmic performance.","sentences":["This paper focuses on developing a conditional gradient algorithm for multiobjective optimization problems with an unbounded feasible region.","We employ the concept of recession cone to establish the well-defined nature of the algorithm.","The asymptotic convergence property and the iteration-complexity bound are established under mild assumptions.","Numerical examples are provided to verify the algorithmic performance."],"url":"http://arxiv.org/abs/2403.02671v1","category":"math.OC"}
{"created":"2024-03-05 05:47:24","title":"Coexistence of topological semimetal states in holography","abstract":"We introduce a holographic model that exhibits a coexistence state of the Weyl semimetal and the topological nodal line state, providing us with a valuable tool to investigate the system's behavior in the strong coupling regime. Nine types of bulk solutions exhibiting different IR behaviors have been identified, corresponding to nine different types of boundary states. These nine states include four distinct phases, namely the Weyl-nodal phase, the gap-nodal phase, the Weyl gap phase and the gap-gap phase, four phase boundaries, which are the Weyl-Dirac phase, the gap-Dirac phase, the Dirac-gap phase and the Dirac-nodal phase, and finally a double critical point. A phase diagram is plotted that exhibits qualitative similarity to the one obtained in the weak coupling limit. The anomalous Hall conductivity, which serves as an order parameter, and the free energy are calculated, with the latter showing the continuity of the topological phase transitions within the system. Our study highlights the similarities and differences in such a topological system between the weak and strong coupling regimes, paving the way for further experimental observations.","sentences":["We introduce a holographic model that exhibits a coexistence state of the Weyl semimetal and the topological nodal line state, providing us with a valuable tool to investigate the system's behavior in the strong coupling regime.","Nine types of bulk solutions exhibiting different IR behaviors have been identified, corresponding to nine different types of boundary states.","These nine states include four distinct phases, namely the Weyl-nodal phase, the gap-nodal phase, the Weyl gap phase and the gap-gap phase, four phase boundaries, which are the Weyl-Dirac phase, the gap-Dirac phase, the Dirac-gap phase and the Dirac-nodal phase, and finally a double critical point.","A phase diagram is plotted that exhibits qualitative similarity to the one obtained in the weak coupling limit.","The anomalous Hall conductivity, which serves as an order parameter, and the free energy are calculated, with the latter showing the continuity of the topological phase transitions within the system.","Our study highlights the similarities and differences in such a topological system between the weak and strong coupling regimes, paving the way for further experimental observations."],"url":"http://arxiv.org/abs/2403.02669v1","category":"hep-th"}
{"created":"2024-03-05 05:44:38","title":"G-EvoNAS: Evolutionary Neural Architecture Search Based on Network Growth","abstract":"The evolutionary paradigm has been successfully applied to neural network search(NAS) in recent years. Due to the vast search complexity of the global space, current research mainly seeks to repeatedly stack partial architectures to build the entire model or to seek the entire model based on manually designed benchmark modules. The above two methods are attempts to reduce the search difficulty by narrowing the search space. To efficiently search network architecture in the global space, this paper proposes another solution, namely a computationally efficient neural architecture evolutionary search framework based on network growth (G-EvoNAS). The complete network is obtained by gradually deepening different Blocks. The process begins from a shallow network, grows and evolves, and gradually deepens into a complete network, reducing the search complexity in the global space. Then, to improve the ranking accuracy of the network, we reduce the weight coupling of each network in the SuperNet by pruning the SuperNet according to elite groups at different growth stages. The G-EvoNAS is tested on three commonly used image classification datasets, CIFAR10, CIFAR100, and ImageNet, and compared with various state-of-the-art algorithms, including hand-designed networks and NAS networks. Experimental results demonstrate that G-EvoNAS can find a neural network architecture comparable to state-of-the-art designs in 0.2 GPU days.","sentences":["The evolutionary paradigm has been successfully applied to neural network search(NAS) in recent years.","Due to the vast search complexity of the global space, current research mainly seeks to repeatedly stack partial architectures to build the entire model or to seek the entire model based on manually designed benchmark modules.","The above two methods are attempts to reduce the search difficulty by narrowing the search space.","To efficiently search network architecture in the global space, this paper proposes another solution, namely a computationally efficient neural architecture evolutionary search framework based on network growth (G-EvoNAS).","The complete network is obtained by gradually deepening different Blocks.","The process begins from a shallow network, grows and evolves, and gradually deepens into a complete network, reducing the search complexity in the global space.","Then, to improve the ranking accuracy of the network, we reduce the weight coupling of each network in the SuperNet by pruning the SuperNet according to elite groups at different growth stages.","The G-EvoNAS is tested on three commonly used image classification datasets, CIFAR10, CIFAR100, and ImageNet, and compared with various state-of-the-art algorithms, including hand-designed networks and NAS networks.","Experimental results demonstrate that G-EvoNAS can find a neural network architecture comparable to state-of-the-art designs in 0.2 GPU days."],"url":"http://arxiv.org/abs/2403.02667v1","category":"cs.NE"}
{"created":"2024-03-05 05:44:23","title":"Passive and active suppression of transduced noise in silicon spin qubits","abstract":"Addressing and mitigating decoherence sources plays an essential role in the development of a scalable quantum computing system, which requires low gate errors to be consistently maintained throughout the circuit execution. While nuclear spin-free materials, such as isotopically purified silicon, exhibit intrinsically promising coherence properties for electron spin qubits, the omnipresent charge noise, when converted to magnetic noise under a strong magnetic field gradient, often hinders stable qubit operation within a time frame comparable to the data acquisition time. Here, we demonstrate both open- and closed-loop suppression techniques for the transduced noise in silicon spin qubits, resulting in a more than two-fold (ten-fold) improvement of the inhomogeneous coherence time (Rabi oscillation quality) that leads to a single-qubit gate fidelity of over 99.6% even in the presence of a strong decoherence field gradient. Utilizing gate set tomography, we show that adaptive qubit control also reduces the non-Markovian noise in the system, which validates the stability of the gate fidelity. The technique can be used to learn multiple Hamiltonian parameters and is useful for the intermittent calibration of the circuit parameters with affordable experimental overhead, providing a useful subroutine during the repeated execution of general quantum circuits.","sentences":["Addressing and mitigating decoherence sources plays an essential role in the development of a scalable quantum computing system, which requires low gate errors to be consistently maintained throughout the circuit execution.","While nuclear spin-free materials, such as isotopically purified silicon, exhibit intrinsically promising coherence properties for electron spin qubits, the omnipresent charge noise, when converted to magnetic noise under a strong magnetic field gradient, often hinders stable qubit operation within a time frame comparable to the data acquisition time.","Here, we demonstrate both open- and closed-loop suppression techniques for the transduced noise in silicon spin qubits, resulting in a more than two-fold (ten-fold) improvement of the inhomogeneous coherence time (Rabi oscillation quality) that leads to a single-qubit gate fidelity of over 99.6% even in the presence of a strong decoherence field gradient.","Utilizing gate set tomography, we show that adaptive qubit control also reduces the non-Markovian noise in the system, which validates the stability of the gate fidelity.","The technique can be used to learn multiple Hamiltonian parameters and is useful for the intermittent calibration of the circuit parameters with affordable experimental overhead, providing a useful subroutine during the repeated execution of general quantum circuits."],"url":"http://arxiv.org/abs/2403.02666v1","category":"quant-ph"}
{"created":"2024-03-05 05:40:45","title":"DGAP: Efficient Dynamic Graph Analysis on Persistent Memory","abstract":"Dynamic graphs, featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications. To accommodate this, graph frameworks, particularly their internal data structures, must support both persistent graph updates and rapid graph analysis simultaneously, leading to complex designs to orchestrate `fast but volatile' and `persistent but slow' storage devices. Emerging persistent memory technologies, such as Optane DCPMM, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high IOPS together. In light of this, we propose DGAP, a framework for efficient dynamic graph analysis on persistent memory. Unlike traditional dynamic graph frameworks, which combine multiple graph data structures (e.g., edge list or adjacency list) to achieve the required performance, DGAP utilizes a single mutable Compressed Sparse Row (CSR) graph structure with new designs for persistent memory to construct the framework. Specifically, DGAP introduces a \\textit{per-section edge log} to reduce write amplification on persistent memory; a \\textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory. Our extensive evaluation results demonstrate that DGAP can achieve up to $3.2\\times$ better graph update performance and up to $3.77\\times$ better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, such as XPGraph, LLAMA, and GraphOne.","sentences":["Dynamic graphs, featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications.","To accommodate this, graph frameworks, particularly their internal data structures, must support both persistent graph updates and rapid graph analysis simultaneously, leading to complex designs to orchestrate `fast but volatile' and `persistent but slow' storage devices.","Emerging persistent memory technologies, such as Optane DCPMM, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high IOPS together.","In light of this, we propose DGAP, a framework for efficient dynamic graph analysis on persistent memory.","Unlike traditional dynamic graph frameworks, which combine multiple graph data structures (e.g., edge list or adjacency list) to achieve the required performance, DGAP utilizes a single mutable Compressed Sparse Row (CSR) graph structure with new designs for persistent memory to construct the framework.","Specifically, DGAP introduces a \\textit{per-section edge log} to reduce write amplification on persistent memory; a \\textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory.","Our extensive evaluation results demonstrate that DGAP can achieve up to $3.2\\times$ better graph update performance and up to $3.77\\times$ better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, such as XPGraph, LLAMA, and GraphOne."],"url":"http://arxiv.org/abs/2403.02665v1","category":"cs.DS"}
{"created":"2024-03-05 05:22:05","title":"The Influence of Validation Data on Logical and Scientific Interpretations of Forensic Expert Opinions","abstract":"Forensic experts use specialized training and knowledge to enable other members of the judicial system to make better informed and more just decisions. Factfinders, in particular, are tasked with judging how much weight to give to experts' reports and opinions. Many references describe assessing evidential weight from the perspective of a forensic expert. Some recognize that stakeholders are each responsible for evaluating their own weight of evidence. Morris (1971, 1974, 1977) provided a general framework for recipients to update their own uncertainties after learning an expert's opinion. Although this framework is normative under Bayesian axioms and several forensic scholars advocate the use of Bayesian reasoning, few resources describe its application in forensic science. This paper addresses this gap by examining how recipients can combine principles of science and Bayesian reasoning to evaluate their own likelihood ratios for expert opinions. This exercise helps clarify how an expert's role depends on whether one envisions recipients to be logical and scientific or deferential. Illustrative examples with an expert's opinion expressed as a categorical conclusion, likelihood ratio, or range of likelihood ratios, or with likelihood ratios from multiple experts, each reveal the importance and influence of validation data for logical recipients' interpretations.","sentences":["Forensic experts use specialized training and knowledge to enable other members of the judicial system to make better informed and more just decisions.","Factfinders, in particular, are tasked with judging how much weight to give to experts' reports and opinions.","Many references describe assessing evidential weight from the perspective of a forensic expert.","Some recognize that stakeholders are each responsible for evaluating their own weight of evidence.","Morris (1971, 1974, 1977) provided a general framework for recipients to update their own uncertainties after learning an expert's opinion.","Although this framework is normative under Bayesian axioms and several forensic scholars advocate the use of Bayesian reasoning, few resources describe its application in forensic science.","This paper addresses this gap by examining how recipients can combine principles of science and Bayesian reasoning to evaluate their own likelihood ratios for expert opinions.","This exercise helps clarify how an expert's role depends on whether one envisions recipients to be logical and scientific or deferential.","Illustrative examples with an expert's opinion expressed as a categorical conclusion, likelihood ratio, or range of likelihood ratios, or with likelihood ratios from multiple experts, each reveal the importance and influence of validation data for logical recipients' interpretations."],"url":"http://arxiv.org/abs/2403.02663v1","category":"stat.AP"}
{"created":"2024-03-05 05:12:10","title":"How to Save My Gas Fees: Understanding and Detecting Real-world Gas Issues in Solidity Programs","abstract":"The execution of smart contracts on Ethereum, a public blockchain system, incurs a fee called gas fee for its computation and data-store consumption. When programmers develop smart contracts (e.g., in the Solidity programming language), they could unknowingly write code snippets that unnecessarily cause more gas fees. These issues, or what we call gas wastes, could lead to significant monetary waste for users. Yet, there have been no systematic examination of them or effective tools for detecting them. This paper takes the initiative in helping Ethereum users reduce their gas fees in two important steps: we conduct the first empirical study on gas wastes in popular smart contracts written in Solidity by understanding their root causes and fixing strategies; we then develop a static tool, PeCatch, to effectively detect gas wastes with simple fixes in Solidity programs based on our study findings. Overall, we make seven insights and four suggestions from our gas-waste study, which could foster future tool development, language improvement, and programmer awareness, and develop eight gas-waste checkers, which pinpoint 383 previously unknown gas wastes from famous Solidity libraries.","sentences":["The execution of smart contracts on Ethereum, a public blockchain system, incurs a fee called gas fee for its computation and data-store consumption.","When programmers develop smart contracts (e.g., in the Solidity programming language), they could unknowingly write code snippets that unnecessarily cause more gas fees.","These issues, or what we call gas wastes, could lead to significant monetary waste for users.","Yet, there have been no systematic examination of them or effective tools for detecting them.","This paper takes the initiative in helping Ethereum users reduce their gas fees in two important steps: we conduct the first empirical study on gas wastes in popular smart contracts written in Solidity by understanding their root causes and fixing strategies; we then develop a static tool, PeCatch, to effectively detect gas wastes with simple fixes in Solidity programs based on our study findings.","Overall, we make seven insights and four suggestions from our gas-waste study, which could foster future tool development, language improvement, and programmer awareness, and develop eight gas-waste checkers, which pinpoint 383 previously unknown gas wastes from famous Solidity libraries."],"url":"http://arxiv.org/abs/2403.02661v1","category":"cs.SE"}
{"created":"2024-03-05 18:38:10","title":"Site Symmetry and Multiorbital Flat Bands on Kagome and Pyrochlore Lattices","abstract":"Flat bands in electronic band structures are intriguing platforms for strong correlation and topological physics, primarily due to the suppressed kinetic energy of electrons. Various methods have been developed to create flat bands, utilizing lattice geometry or finely tuned parameters. Despite this, the investigation of orbital symmetry in multiorbital materials is a relatively new area of focus. In this work, we propose a site symmetry based systematic approach to emerging multiorbital flat bands in lattices made of corner-connecting motifs such as the kagome and pyrochlore lattices. As a conceptual advance, the one-orbital flat bands are shown to originate as mutual eigenstates of isolated molecular motifs. Further developing the mutual eigenstate method for multiorbitals transforming differently under the site symmetries such as mirror and inversion, we derive multiorbital flat bands from the skew-symmetric interorbital Hamiltonian and introduce an isolated molecule enabled group-theoretic description of the flat band wavefunctions. Realizations of the multiorbital flatbands in relevant materials are shown to be possible under the Slater-Koster formalism. Our findings provide new directions for exploring flatband electronic structures for novel correlated and topological quantum states.","sentences":["Flat bands in electronic band structures are intriguing platforms for strong correlation and topological physics, primarily due to the suppressed kinetic energy of electrons.","Various methods have been developed to create flat bands, utilizing lattice geometry or finely tuned parameters.","Despite this, the investigation of orbital symmetry in multiorbital materials is a relatively new area of focus.","In this work, we propose a site symmetry based systematic approach to emerging multiorbital flat bands in lattices made of corner-connecting motifs such as the kagome and pyrochlore lattices.","As a conceptual advance, the one-orbital flat bands are shown to originate as mutual eigenstates of isolated molecular motifs.","Further developing the mutual eigenstate method for multiorbitals transforming differently under the site symmetries such as mirror and inversion, we derive multiorbital flat bands from the skew-symmetric interorbital Hamiltonian and introduce an isolated molecule enabled group-theoretic description of the flat band wavefunctions.","Realizations of the multiorbital flatbands in relevant materials are shown to be possible under the Slater-Koster formalism.","Our findings provide new directions for exploring flatband electronic structures for novel correlated and topological quantum states."],"url":"http://arxiv.org/abs/2403.03201v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 18:37:53","title":"Concavity Properties of Solutions of Elliptic Equations under Conformal Deformations","abstract":"We study the Dirichlet problem for the weighted Schr\\\"odinger operator \\[-\\Delta u +Vu = \\lambda \\rho u,\\] where $\\rho$ is a positive weighting function and $V$ is a potential. Such equations appear naturally in conformal geometry and in the composite membrane problem. Our primary goal is to establish concavity estimates for the principle eigenfunction with respect to conformal connections. Doing so, we obtain new bounds on the fundamental gap problem, which is the difference between the first and second eigenvalues. In particular, we partially resolve a conjecture of Nguyen, Stancu and Wei [IMRN 2022] on the fundamental gap of horoconvex domains. In addition, we obtain a power convexity estimate for solutions to the torsion problem in spherical geometry on convex domains which are not too large.","sentences":["We study the Dirichlet problem for the weighted Schr\\\"odinger operator \\[-\\Delta u +Vu = \\lambda \\rho u,\\] where $\\rho$ is a positive weighting function and $V$ is a potential.","Such equations appear naturally in conformal geometry and in the composite membrane problem.","Our primary goal is to establish concavity estimates for the principle eigenfunction with respect to conformal connections.","Doing so, we obtain new bounds on the fundamental gap problem, which is the difference between the first and second eigenvalues.","In particular, we partially resolve a conjecture of Nguyen, Stancu and Wei [IMRN 2022] on the fundamental gap of horoconvex domains.","In addition, we obtain a power convexity estimate for solutions to the torsion problem in spherical geometry on convex domains which are not too large."],"url":"http://arxiv.org/abs/2403.03200v1","category":"math.DG"}
{"created":"2024-03-05 17:33:47","title":"On dynamics of gasless combustion in slowly varying periodic media: periodic fronts, their stability and propagation-extinction-diffusion-reignition pattern","abstract":"In this paper we consider a classical model of gasless combustion in a one dimensional formulation under the assumption of ignition temperature kinetics. We study the propagation of flame fronts in this model when the initial distribution of the solid fuel is a spatially periodic function that varies on a large scale. It is shown that in certain parametric regimes the model supports periodic traveling fronts. An accurate asymptotic formula for the velocity of the flame front is derived and studied. The stability of periodic fronts is also explored, and a critical condition in terms of parameters of the problem is derived. It is also shown that the instability of periodic fronts, in a certain parametric regimes, results in a propagation-extinction-diffusion-reignition pattern which is studied numerically.","sentences":["In this paper we consider a classical model of gasless combustion in a one dimensional formulation under the assumption of ignition temperature kinetics.","We study the propagation of flame fronts in this model when the initial distribution of the solid fuel is a spatially periodic function that varies on a large scale.","It is shown that in certain parametric regimes the model supports periodic traveling fronts.","An accurate asymptotic formula for the velocity of the flame front is derived and studied.","The stability of periodic fronts is also explored, and a critical condition in terms of parameters of the problem is derived.","It is also shown that the instability of periodic fronts, in a certain parametric regimes, results in a propagation-extinction-diffusion-reignition pattern which is studied numerically."],"url":"http://arxiv.org/abs/2403.03144v1","category":"nlin.PS"}
{"created":"2024-03-05 17:27:05","title":"Using Smartphones to Study Vaccination Decisions in the Wild","abstract":"One of the most important tools available to limit the spread and impact of infectious diseases is vaccination. It is therefore important to understand what factors determine people's vaccination decisions. To this end, previous behavioural research made use of, (i) controlled but often abstract or hypothetical studies (e.g., vignettes) or, (ii) realistic but typically less flexible studies that make it difficult to understand individual decision processes (e.g., clinical trials). Combining the best of these approaches, we propose integrating real-world Bluetooth contacts via smartphones in several rounds of a game scenario, as a novel methodology to study vaccination decisions and disease spread. In our 12-week proof-of-concept study conducted with $N$ = 494 students, we found that participants strongly responded to some of the information provided to them during or after each decision round, particularly those related to their individual health outcomes. In contrast, information related to others' decisions and outcomes (e.g., the number of vaccinated or infected individuals) appeared to be less important. We discuss the potential of this novel method and point to fruitful areas for future research.","sentences":["One of the most important tools available to limit the spread and impact of infectious diseases is vaccination.","It is therefore important to understand what factors determine people's vaccination decisions.","To this end, previous behavioural research made use of, (i) controlled but often abstract or hypothetical studies (e.g., vignettes) or, (ii) realistic but typically less flexible studies that make it difficult to understand individual decision processes (e.g., clinical trials).","Combining the best of these approaches, we propose integrating real-world Bluetooth contacts via smartphones in several rounds of a game scenario, as a novel methodology to study vaccination decisions and disease spread.","In our 12-week proof-of-concept study conducted with $N$ = 494 students, we found that participants strongly responded to some of the information provided to them during or after each decision round, particularly those related to their individual health outcomes.","In contrast, information related to others' decisions and outcomes (e.g., the number of vaccinated or infected individuals) appeared to be less important.","We discuss the potential of this novel method and point to fruitful areas for future research."],"url":"http://arxiv.org/abs/2403.03143v1","category":"physics.soc-ph"}
{"created":"2024-03-05 17:24:46","title":"Nematic Ising superconductivity in twisted bilayer graphene under hydrostatic pressure","abstract":"High hydrostatic pressures can be used to induce flat bands in twisted bilayer graphene, at twist angles larger than those realizing the usual magic-angle condition. Here, we characterize the emerging spin-degenerate correlated insulator phases for a (magic) twist angle of $\\theta=3.5^\\circ$ at even integer filling factors $\\nu=0,\\pm2$ by relying on an exact self-consistent real-space Hartree-Fock approach that accounts for the screened long-range Coulomb interaction $\\epsilon$ as well as the on-site Hubbard interaction $U$. We further present a novel algorithm that maps the full real-space density matrix to a reduced density matrix based on a $SU(4)$ symmetry of sublattice and valley degree of freedom. At charge-neutrality, we obtain a pure state of a Kramers intervalley coherent insulator localized on the moir\\'e unit cell with a trivial gap. For $\\nu=\\pm2$, we obtain a mixed state, either valley polarized or valley coherent with Chern number $|C|=1$ pero spin-channel. In the weak coupling limit $\\epsilon=54$ and $U=4$eV, this leads to nematic Ising superconductivity for $\\nu=-2.4$ and conventional triplet superconductivity for $\\nu=2.3$, that can experimentally be tested.","sentences":["High hydrostatic pressures can be used to induce flat bands in twisted bilayer graphene, at twist angles larger than those realizing the usual magic-angle condition.","Here, we characterize the emerging spin-degenerate correlated insulator phases for a (magic) twist angle of $\\theta=3.5^\\circ$ at even integer filling factors $\\nu=0,\\pm2$ by relying on an exact self-consistent real-space Hartree-Fock approach that accounts for the screened long-range Coulomb interaction $\\epsilon$ as well as the on-site Hubbard interaction $U$. We further present a novel algorithm that maps the full real-space density matrix to a reduced density matrix based on a $SU(4)$ symmetry of sublattice and valley degree of freedom.","At charge-neutrality, we obtain a pure state of a Kramers intervalley coherent insulator localized on the moir\\'e unit cell with a trivial gap.","For $\\nu=\\pm2$, we obtain a mixed state, either valley polarized or valley coherent with Chern number $|C|=1$ pero spin-channel.","In the weak coupling limit $\\epsilon=54$ and $U=4$eV, this leads to nematic Ising superconductivity for $\\nu=-2.4$ and conventional triplet superconductivity for $\\nu=2.3$, that can experimentally be tested."],"url":"http://arxiv.org/abs/2403.03140v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 17:20:50","title":"Isostructural Softening of Vulcanized Nanocomposites","abstract":"Following a previous work evidencing that short poly-propylene glycol (PPG) chains incorporated to crude SBR-silica nanocomposites act as filler-network softeners without changing their structure, we propose in the present letter to examine more operative cross-linked materials. We first evidence that the adsorption of PPG onto silica deactivates progressively the particle's catalytic effect on vulcanization, without perturbing however the cross-link density distribution that we investigate through multiple-quantum NMR. In addition, electron microscopy confirms that silica structure is conserved after vulcanization and that it does not depend on the PPG content either. Composites containing various amount of PPG can thus be seen as structurally identical, both from a matrix and filler point of view - which is confirmed by small and medium amplitude oscillation shear rheology showing strikingly identical viscoelastic properties. PPG signature only appears above 100% in tensile deformation where it is seen to soften dramatically the filler network. Our discovery makes it consequently possible to decorrelate the mechanical behavior of reinforced rubbers under normal conditions of use and urgent needs of energy dissipation.","sentences":["Following a previous work evidencing that short poly-propylene glycol (PPG) chains incorporated to crude SBR-silica nanocomposites act as filler-network softeners without changing their structure, we propose in the present letter to examine more operative cross-linked materials.","We first evidence that the adsorption of PPG onto silica deactivates progressively the particle's catalytic effect on vulcanization, without perturbing however the cross-link density distribution that we investigate through multiple-quantum NMR.","In addition, electron microscopy confirms that silica structure is conserved after vulcanization and that it does not depend on the PPG content either.","Composites containing various amount of PPG can thus be seen as structurally identical, both from a matrix and filler point of view - which is confirmed by small and medium amplitude oscillation shear rheology showing strikingly identical viscoelastic properties.","PPG signature only appears above 100% in tensile deformation where it is seen to soften dramatically the filler network.","Our discovery makes it consequently possible to decorrelate the mechanical behavior of reinforced rubbers under normal conditions of use and urgent needs of energy dissipation."],"url":"http://arxiv.org/abs/2403.03133v1","category":"physics.chem-ph"}
{"created":"2024-03-05 17:15:40","title":"A Comprehensive Stochastic Programming Model for Transfer Synchronization in Transit Networks","abstract":"We investigate the stochastic transfer synchronization problem, which seeks to synchronize the timetables of different routes in a transit network to reduce transfer waiting times, delay times, and unnecessary in-vehicle times. We present a sophisticated two-stage stochastic mixed-integer programming model that takes into account variability in passenger walking times between bus stops, bus running times, dwell times, and demand uncertainty. Our model incorporates new features related to dwell time determination by considering passenger arrival patterns at bus stops which have been neglected in the literature on transfer synchronization and timetabling. We solve a sample average approximation of our model using a problem-based scenario reduction approach, and the progressive hedging algorithm. As a proof of concept, our computational experiments on two single transfer nodes in the City of Toronto, with a mixture of low- and high-frequency routes, demonstrate the potential advantages of the proposed model. Our findings highlight the necessity and value of incorporating stochasticity in transfer-based timetabling models.","sentences":["We investigate the stochastic transfer synchronization problem, which seeks to synchronize the timetables of different routes in a transit network to reduce transfer waiting times, delay times, and unnecessary in-vehicle times.","We present a sophisticated two-stage stochastic mixed-integer programming model that takes into account variability in passenger walking times between bus stops, bus running times, dwell times, and demand uncertainty.","Our model incorporates new features related to dwell time determination by considering passenger arrival patterns at bus stops which have been neglected in the literature on transfer synchronization and timetabling.","We solve a sample average approximation of our model using a problem-based scenario reduction approach, and the progressive hedging algorithm.","As a proof of concept, our computational experiments on two single transfer nodes in the City of Toronto, with a mixture of low- and high-frequency routes, demonstrate the potential advantages of the proposed model.","Our findings highlight the necessity and value of incorporating stochasticity in transfer-based timetabling models."],"url":"http://arxiv.org/abs/2403.03130v1","category":"math.OC"}
{"created":"2024-03-05 17:12:13","title":"A3COSMOS & A3GOODSS: Continuum Source Catalogues and Multi-band Number Counts","abstract":"Galaxy submillimetre number counts are a fundamental measurement in our understanding of galaxy evolution models. Most early measurements are obtained via single-dish telescopes with substantial source confusion, whereas recent interferometric observations are limited to small areas. We used a large database of ALMA continuum observations to accurately measure galaxy number counts in multiple (sub)millimetre bands, thus bridging the flux density range between single-dish surveys and deep interferometric studies. We continued the Automated Mining of the ALMA Archive in the COSMOS Field project (A3COSMOS) and extended it with observations from the GOODS-South field (A3GOODSS). The database consists of ~4,000 pipeline-processed continuum images from the public ALMA archive, yielding 2,050 unique detected sources. To infer galaxy number counts, we constructed a method to reduce the observational bias inherent to targeted pointings that dominate the database. This method comprises a combination of image selection, masking, and source weighting. The effective area was calculated by accounting for inhomogeneous wavelengths, sensitivities, and resolutions and for spatial overlap between images. We tested and calibrated our method with simulations. We obtained the first number counts derived in a consistent and homogeneous way in four different ALMA bands covering a relatively large area. The results are consistent with number counts from the literature within the uncertainties. We extended the available depth in ALMA Band 4 by 0.4 dex with respect to previous studies. In Band 7, at the depth of the inferred number counts, ~40% of the cosmic infrared background is resolved into discrete sources. This fraction, however, decreases with wavelength, reaching ~4% in Band 3. Finally, we used the number counts to test models of dusty galaxy evolution, and find a good agreement within the uncertainties.","sentences":["Galaxy submillimetre number counts are a fundamental measurement in our understanding of galaxy evolution models.","Most early measurements are obtained via single-dish telescopes with substantial source confusion, whereas recent interferometric observations are limited to small areas.","We used a large database of ALMA continuum observations to accurately measure galaxy number counts in multiple (sub)millimetre bands, thus bridging the flux density range between single-dish surveys and deep interferometric studies.","We continued the Automated Mining of the ALMA Archive in the COSMOS Field project (A3COSMOS) and extended it with observations from the GOODS-South field (A3GOODSS).","The database consists of ~4,000 pipeline-processed continuum images from the public ALMA archive, yielding 2,050 unique detected sources.","To infer galaxy number counts, we constructed a method to reduce the observational bias inherent to targeted pointings that dominate the database.","This method comprises a combination of image selection, masking, and source weighting.","The effective area was calculated by accounting for inhomogeneous wavelengths, sensitivities, and resolutions and for spatial overlap between images.","We tested and calibrated our method with simulations.","We obtained the first number counts derived in a consistent and homogeneous way in four different ALMA bands covering a relatively large area.","The results are consistent with number counts from the literature within the uncertainties.","We extended the available depth in ALMA Band 4 by 0.4 dex with respect to previous studies.","In Band 7, at the depth of the inferred number counts, ~40% of the cosmic infrared background is resolved into discrete sources.","This fraction, however, decreases with wavelength, reaching ~4% in Band 3.","Finally, we used the number counts to test models of dusty galaxy evolution, and find a good agreement within the uncertainties."],"url":"http://arxiv.org/abs/2403.03125v1","category":"astro-ph.GA"}
{"created":"2024-03-05 16:56:00","title":"Effect of particle stiffness and surface properties on the nonlinear viscoelasticity of dense microgel suspensions","abstract":"Particle surface chemistry and internal softness are two fundamental parameters in governing the mechanical properties of dense colloidal suspensions, dictating structure and flow, therefore of interest from materials fabrication to processing. Here, we modulate softness by tuning the crosslinker content of poly(N-isopropylacrylamide) microgels, and we adjust their surface properties by co-polymerization with polyethylene glycol (PEG) chains, controlling adhesion, friction and fuzziness. We investigate the distinct effects of these parameters on the entire mechanical response from restructuring to complete fluidization of jammed samples at varying packing fractions under large-amplitude oscillatory shear experiments, and we complement rheological data with colloidal-probe atomic force microscopy to unravel variations in the particles' surface properties. We find that surface properties play a fundamental role at smaller packings; decreasing adhesion and friction at contact causes the samples to yield and fluidify in a lower deformation range. Instead, increasing softness or fuzziness has a similar effect at ultra-high densities, making suspensions able to better adapt to the applied shear and reach complete fluidization over a larger deformation range. These findings shed new light on the single-particle parameters governing the mechanical response of dense suspensions subjected to deformation, offering synthetic approaches to design materials with tailored mechanical properties.","sentences":["Particle surface chemistry and internal softness are two fundamental parameters in governing the mechanical properties of dense colloidal suspensions, dictating structure and flow, therefore of interest from materials fabrication to processing.","Here, we modulate softness by tuning the crosslinker content of poly(N-isopropylacrylamide) microgels, and we adjust their surface properties by co-polymerization with polyethylene glycol (PEG) chains, controlling adhesion, friction and fuzziness.","We investigate the distinct effects of these parameters on the entire mechanical response from restructuring to complete fluidization of jammed samples at varying packing fractions under large-amplitude oscillatory shear experiments, and we complement rheological data with colloidal-probe atomic force microscopy to unravel variations in the particles' surface properties.","We find that surface properties play a fundamental role at smaller packings; decreasing adhesion and friction at contact causes the samples to yield and fluidify in a lower deformation range.","Instead, increasing softness or fuzziness has a similar effect at ultra-high densities, making suspensions able to better adapt to the applied shear and reach complete fluidization over a larger deformation range.","These findings shed new light on the single-particle parameters governing the mechanical response of dense suspensions subjected to deformation, offering synthetic approaches to design materials with tailored mechanical properties."],"url":"http://arxiv.org/abs/2403.03113v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:55:39","title":"Characterizing the 3D Structure of Molecular Cloud Envelopes in the \"Cloud Factory\" Simulations","abstract":"We leverage recent numerical simulations of highly resolved star-forming regions in a Milky Way-like Galaxy to explore the nature of extended gaseous envelopes around molecular clouds. We extract a sample of two dozen star-forming clouds from the feedback-dominated suite of the \"Cloud Factory'' simulations. With the goal of exploring the 3D thermal and chemical structure of the gas, we measure and fit the clouds' radial profiles in multiple tracers, including $n_{H_I}$, $n_{H_2}$, $n_{H_{tot}}$, $n_{CO}$, and gas temperature. We find that while solar neighborhood clouds recently detected via 3D dust mapping have radially symmetric, low-density envelopes that extend $\\sim$ 10-15 pc, the simulated cloud envelopes are primarily radially asymmetric with low-density envelopes that extend only $\\sim$ 2-3 pc. One potential explanation for the absence of extended envelopes in the simulated clouds may be the lack of magnetic fields, while a stronger local feedback prescription compared to solar neighborhood conditions may drive the radially asymmetric cloud morphologies. We make the pipeline to extract and characterize the radial profile of clouds publicly available, which can be used in complementary and future simulations to shed additional light on the key physics shaping the formation and evolution of star-forming structures in the Milky Way.","sentences":["We leverage recent numerical simulations of highly resolved star-forming regions in a Milky Way-like Galaxy to explore the nature of extended gaseous envelopes around molecular clouds.","We extract a sample of two dozen star-forming clouds from the feedback-dominated suite of the \"Cloud Factory'' simulations.","With the goal of exploring the 3D thermal and chemical structure of the gas, we measure and fit the clouds' radial profiles in multiple tracers, including $n_{H_I}$, $n_{H_2}$, $n_{H_{tot}}$, $n_{CO}$, and gas temperature.","We find that while solar neighborhood clouds recently detected via 3D dust mapping have radially symmetric, low-density envelopes that extend $\\sim$ 10-15 pc, the simulated cloud envelopes are primarily radially asymmetric with low-density envelopes that extend only $\\sim$ 2-3 pc.","One potential explanation for the absence of extended envelopes in the simulated clouds may be the lack of magnetic fields, while a stronger local feedback prescription compared to solar neighborhood conditions may drive the radially asymmetric cloud morphologies.","We make the pipeline to extract and characterize the radial profile of clouds publicly available, which can be used in complementary and future simulations to shed additional light on the key physics shaping the formation and evolution of star-forming structures in the Milky Way."],"url":"http://arxiv.org/abs/2403.03112v1","category":"astro-ph.GA"}
{"created":"2024-03-05 16:34:29","title":"Galaxies in the Zone of Avoidance: Misclassifications using machine learning tools","abstract":"Automated methods for classifying extragalactic objects in large surveys offer significant advantages compared to manual approaches in terms of efficiency and consistency. However, the existence of the Galactic disk raises additional concerns. These regions are known for high levels of interstellar extinction, star crowding, and limited data sets and studies. In this study, we explore the identification and classification of galaxies in the Zone of Avoidance (ZoA). In particular, we compare our results in the near-infrared with X-ray data. We analize the appearance of the objects classified as galaxies using machine learning by Zhang et al. (2021) in the Galactic disk and make a comparison with the visually confirmed galaxies from the VVV NIRGC (Baravalle et al. (2021). Our analysis, which includes the visual inspection of all sources catalogued as galaxies throughout the Galactic disk using machine learning techniques reveals significant differences. Only 4 galaxies were found in both the near-Infrared and X-ray data sets. Several specific regions of interest within the ZoA exhibit a high probability of being galaxies in X-ray data but closely resemble extended Galactic objects. The results indicate the difficulty of using machine learning methods for galaxy classification in the ZoA mainly due to the scarce information on galaxies behind the Galactic plane in the training set. They also stress the importance of considering specific factors that are present to improve the reliability and accuracy of future studies in this challenging region.","sentences":["Automated methods for classifying extragalactic objects in large surveys offer significant advantages compared to manual approaches in terms of efficiency and consistency.","However, the existence of the Galactic disk raises additional concerns.","These regions are known for high levels of interstellar extinction, star crowding, and limited data sets and studies.","In this study, we explore the identification and classification of galaxies in the Zone of Avoidance (ZoA).","In particular, we compare our results in the near-infrared with X-ray data.","We analize the appearance of the objects classified as galaxies using machine learning by Zhang et al. (2021) in the Galactic disk and make a comparison with the visually confirmed galaxies from the VVV NIRGC (Baravalle et al. (2021).","Our analysis, which includes the visual inspection of all sources catalogued as galaxies throughout the Galactic disk using machine learning techniques reveals significant differences.","Only 4 galaxies were found in both the near-Infrared and X-ray data sets.","Several specific regions of interest within the ZoA exhibit a high probability of being galaxies in X-ray data but closely resemble extended Galactic objects.","The results indicate the difficulty of using machine learning methods for galaxy classification in the ZoA mainly due to the scarce information on galaxies behind the Galactic plane in the training set.","They also stress the importance of considering specific factors that are present to improve the reliability and accuracy of future studies in this challenging region."],"url":"http://arxiv.org/abs/2403.03098v1","category":"astro-ph.GA"}
{"created":"2024-03-05 15:59:29","title":"Impact of $\u03b1$ enhancement on the asteroseismic age determination of field stars. Application to the APO-K2 catalogue","abstract":"We investigated the theoretical biases affecting the asteroseismic grid-based estimates of stellar parameters in the presence of a mismatch between the heavy element mixture of observed stars and stellar models. We performed a controlled simulation adopting effective temperature, [Fe/H], average large frequency spacing, and frequency of maximum oscillation power as observational constraints. Synthetic stars were sampled from grids of stellar models computed with different [alpha/Fe] values from 0.0 to 0.4. The mass, radius, and age of these objects were then estimated by adopting a grid of models with a fixed [alpha/Fe] value of 0.0. The experiment was repeated assuming different sets of observational uncertainties. In the reference scenario, we adopted an uncertainty of 1.5% in seismic parameters, 50 K in effective temperature, and 0.05 dex in [Fe/H]. A higher uncertainty in the atmospheric constraints was also adopted in order to explore the impact on the precision of the observations of the estimated stellar parameters. Our simulations showed that estimated parameters are biased up to 3% in mass, 1.5% in radius, and 4% in age when the reference uncertainty scenario was adopted. These values correspond to 45%, 48%, and 16% of the estimated uncertainty in the stellar parameters. These biases in mass and radius disappear when adopting larger observational uncertainties because of the possibility of the fitting algorithm exploring a wider range of possible solutions. However, in this scenario, the age is significantly biased by -8%. Finally, we verified that the stellar mass, radius, and age can be estimated with a high accuracy by adopting a grid with the incorrect value of [alpha/Fe] if the metallicity [Fe/H] of the target is adjusted to match the Z in the fitting grid. In this scenario, the maximum bias in the age was reduced to 1.5%.","sentences":["We investigated the theoretical biases affecting the asteroseismic grid-based estimates of stellar parameters in the presence of a mismatch between the heavy element mixture of observed stars and stellar models.","We performed a controlled simulation adopting effective temperature, [Fe/H], average large frequency spacing, and frequency of maximum oscillation power as observational constraints.","Synthetic stars were sampled from grids of stellar models computed with different [alpha/Fe] values from 0.0 to 0.4.","The mass, radius, and age of these objects were then estimated by adopting a grid of models with a fixed [alpha/Fe] value of 0.0.","The experiment was repeated assuming different sets of observational uncertainties.","In the reference scenario, we adopted an uncertainty of 1.5% in seismic parameters, 50 K in effective temperature, and 0.05 dex in [Fe/H].","A higher uncertainty in the atmospheric constraints was also adopted in order to explore the impact on the precision of the observations of the estimated stellar parameters.","Our simulations showed that estimated parameters are biased up to 3% in mass, 1.5% in radius, and 4% in age when the reference uncertainty scenario was adopted.","These values correspond to 45%, 48%, and 16% of the estimated uncertainty in the stellar parameters.","These biases in mass and radius disappear when adopting larger observational uncertainties because of the possibility of the fitting algorithm exploring a wider range of possible solutions.","However, in this scenario, the age is significantly biased by -8%.","Finally, we verified that the stellar mass, radius, and age can be estimated with a high accuracy by adopting a grid with the incorrect value of [alpha/Fe] if the metallicity [Fe/H] of the target is adjusted to match the Z in the fitting grid.","In this scenario, the maximum bias in the age was reduced to 1.5%."],"url":"http://arxiv.org/abs/2403.03070v1","category":"astro-ph.SR"}
{"created":"2024-03-05 15:52:54","title":"CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections","abstract":"Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to help the model learn a unified illumination-invariant representation. Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at https://github.com/zy1296/CrackNex.","sentences":["Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure.","Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health.","Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings.","However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes.","In addition, conventional approaches require many annotated low-light crack images which is time-consuming.","In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to help the model learn a unified illumination-invariant representation.","Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem.","In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set.","Then, a prototype fusion module is designed to integrate the features from both prototypes.","CrackNex outperforms the SOTA methods on multiple datasets.","Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation.","LCSD consists of 102 well-illuminated crack images and 41 low-light crack images.","The dataset and code are available at https://github.com/zy1296/CrackNex."],"url":"http://arxiv.org/abs/2403.03063v1","category":"cs.CV"}
{"created":"2024-03-05 15:48:07","title":"Machine Learning Assisted Adjustment Boosts Inferential Efficiency of Randomized Controlled Trials","abstract":"In this work, we proposed a novel inferential procedure assisted by machine learning based adjustment for randomized control trials. The method was developed under the Rosenbaum's framework of exact tests in randomized experiments with covariate adjustments. Through extensive simulation experiments, we showed the proposed method can robustly control the type I error and can boost the inference efficiency for a randomized controlled trial (RCT). This advantage was further demonstrated in a real world example. The simplicity and robustness of the proposed method makes it a competitive candidate as a routine inference procedure for RCTs, especially when the number of baseline covariates is large, and when nonlinear association or interaction among covariates is expected. Its application may remarkably reduce the required sample size and cost of RCTs, such as phase III clinical trials.","sentences":["In this work, we proposed a novel inferential procedure assisted by machine learning based adjustment for randomized control trials.","The method was developed under the Rosenbaum's framework of exact tests in randomized experiments with covariate adjustments.","Through extensive simulation experiments, we showed the proposed method can robustly control the type I error and can boost the inference efficiency for a randomized controlled trial (RCT).","This advantage was further demonstrated in a real world example.","The simplicity and robustness of the proposed method makes it a competitive candidate as a routine inference procedure for RCTs, especially when the number of baseline covariates is large, and when nonlinear association or interaction among covariates is expected.","Its application may remarkably reduce the required sample size and cost of RCTs, such as phase III clinical trials."],"url":"http://arxiv.org/abs/2403.03058v1","category":"stat.ME"}
{"created":"2024-03-05 14:58:31","title":"Predictive power of the Berezinskii-Kosterlitz-Thouless theory based on Renormalization Group throughout the BCS-BEC crossover in 2D superconductors","abstract":"Recent experiments on 2D superconductors allow the characterization of the critical temperature and of the phase diagram across the BCS-BEC crossover as a function of density. We obtain from these experiments the microscopic parameters of the superconducting state at low temperatures by the BCS mean-field approach. For Li$_x$ZrNCl, the extracted parameters are used to evaluate the superconducting phase stiffness and the Berezinskii-Kosterlitz-Thouless (BKT) critical temperature throughout the BCS-BEC crossover, by implementing the corresponding Renormalization Group (RG) approach. In this way, we make a quantitative test of the predictive power of the BKT theory for evaluating the critical temperature. The RG flow equations turn out to give a sizable renormalization of the phase stiffness and of the critical temperature, which is crucial to obtain a satisfactory agreement between the BKT theory and the experiments, in particular in the BCS-BEC crossover regime. We predict the temperature range where phase stiffness renormalization can be measured in Li$_x$ZrNCl across the BCS-BEC crossover. Contrary to other microscopic theories of superconductivity, we find that the BKT theory can be exploited to evaluate quantitatively the critical temperature of 2D superconductors in different pairing regimes.","sentences":["Recent experiments on 2D superconductors allow the characterization of the critical temperature and of the phase diagram across the BCS-BEC crossover as a function of density.","We obtain from these experiments the microscopic parameters of the superconducting state at low temperatures by the BCS mean-field approach.","For Li$_x$ZrNCl, the extracted parameters are used to evaluate the superconducting phase stiffness and the Berezinskii-Kosterlitz-Thouless (BKT) critical temperature throughout the BCS-BEC crossover, by implementing the corresponding Renormalization Group (RG) approach.","In this way, we make a quantitative test of the predictive power of the BKT theory for evaluating the critical temperature.","The RG flow equations turn out to give a sizable renormalization of the phase stiffness and of the critical temperature, which is crucial to obtain a satisfactory agreement between the BKT theory and the experiments, in particular in the BCS-BEC crossover regime.","We predict the temperature range where phase stiffness renormalization can be measured in Li$_x$ZrNCl across the BCS-BEC crossover.","Contrary to other microscopic theories of superconductivity, we find that the BKT theory can be exploited to evaluate quantitatively the critical temperature of 2D superconductors in different pairing regimes."],"url":"http://arxiv.org/abs/2403.03025v1","category":"cond-mat.supr-con"}
{"created":"2024-03-05 14:55:17","title":"Pushing single atoms near an optical cavity","abstract":"Optical scattering force is used to reduce the loading time of single atoms to a cavity mode. Releasing a cold atomic ensemble above the resonator, we apply a push beam along the direction of gravity, offering fast atomic transport with narrow velocity distribution. We also observe in real time that, when the push beam is illuminated against gravity, single atoms slow down and even turn around in the mode, through the cavity-transmission measurement. Our method can be employed to make atom-cavity experiments more efficient.","sentences":["Optical scattering force is used to reduce the loading time of single atoms to a cavity mode.","Releasing a cold atomic ensemble above the resonator, we apply a push beam along the direction of gravity, offering fast atomic transport with narrow velocity distribution.","We also observe in real time that, when the push beam is illuminated against gravity, single atoms slow down and even turn around in the mode, through the cavity-transmission measurement.","Our method can be employed to make atom-cavity experiments more efficient."],"url":"http://arxiv.org/abs/2403.03019v1","category":"quant-ph"}
{"created":"2024-03-05 14:08:49","title":"Developments on frequency domain multiplexing readout for large arrays of transition-edge sensor X-ray micro-calorimeters","abstract":"At SRON we have been developing X-ray TES micro-calorimeters as backup technology for the X-ray Integral Field Unit (X-IFU) of the Athena mission, demonstrating excellent resolving powers both under DC and AC bias. We also developed a frequency-domain multiplexing (FDM) readout technology, where each TES is coupled to a superconducting band-pass LC resonator and AC biased at MHz frequencies through a common readout line. The TES signals are summed at the input of a superconducting quantum interference device (SQUID), which performs a first amplification at cryogenic stage. Custom analog front-end electronics and digital boards take care of further amplifying the signals at room temperature and of the modulation/demodulation of the TES signals and bias carrier, respectively. We report on the most recent developments on our FDM technology, which involves a two-channel demonstration with a total of 70 pixels with a summed energy resolution of 2.34 +/- 0.02 eV at 5.9 keV without spectral performance degradation with respect to single-channel operation. Moreover, we discuss prospects towards the scaling-up to a larger multiplexing factor up to 78 pixels per channel in a 1-6 MHz readout bandwidth.","sentences":["At SRON we have been developing X-ray TES micro-calorimeters as backup technology for the X-ray Integral Field Unit (X-IFU) of the Athena mission, demonstrating excellent resolving powers both under DC and AC bias.","We also developed a frequency-domain multiplexing (FDM) readout technology, where each TES is coupled to a superconducting band-pass LC resonator and AC biased at MHz frequencies through a common readout line.","The TES signals are summed at the input of a superconducting quantum interference device (SQUID), which performs a first amplification at cryogenic stage.","Custom analog front-end electronics and digital boards take care of further amplifying the signals at room temperature and of the modulation/demodulation of the TES signals and bias carrier, respectively.","We report on the most recent developments on our FDM technology, which involves a two-channel demonstration with a total of 70 pixels with a summed energy resolution of 2.34 +/- 0.02 eV at 5.9 keV without spectral performance degradation with respect to single-channel operation.","Moreover, we discuss prospects towards the scaling-up to a larger multiplexing factor up to 78 pixels per channel in a 1-6 MHz readout bandwidth."],"url":"http://arxiv.org/abs/2403.02989v1","category":"astro-ph.IM"}
{"created":"2024-03-05 14:05:48","title":"Quasi-diagrams and gentle algebras","abstract":"Any gentle algebra $A$ with one maximal path corresponds to a unique quasi-diagram $\\alpha$. We introduce the regularity for $\\alpha$, and show that $A$ has finite global dimension if and only if $\\alpha$ is regular. We characterize regular quasi-diagrams which remain regular under the dihedral group action. We prove that the set of maximal chord diagrams is the \"biggest\" one among the sets closed under taking Koszul dual and rotations.","sentences":["Any gentle algebra $A$ with one maximal path corresponds to a unique quasi-diagram $\\alpha$. We introduce the regularity for $\\alpha$, and show that $A$ has finite global dimension if and only if $\\alpha$ is regular.","We characterize regular quasi-diagrams which remain regular under the dihedral group action.","We prove that the set of maximal chord diagrams is the \"biggest\" one among the sets closed under taking Koszul dual and rotations."],"url":"http://arxiv.org/abs/2403.02986v1","category":"math.RA"}
{"created":"2024-03-05 13:50:25","title":"Bodioid: philosophical reflections on the hybrid of bodies and artefacts towards post-human","abstract":"The advent of the post-human era has blurred the boundary between the body and artifacts. Further, external materials and information are more deeply integrated into the body, making emerging technology a key driving force for shaping post-human existence and promoting bodily evolution. Based on this, this study analyses the transformation process of three technological forms, namely tools, machines, and cyborgs, and reveals the construction of bodies and artifacts. From the phenomenological perspective, the essences of body and artifact existences are reflected upon, and the existence is construction viewpoint is proposed. Furthermore, a technological design concept, bodioid, is proposed to meticulously depict the characteristics of integrating similarities and differences towards unity between the body and artifacts, based on the theoretical foundation of technology mediation and the materialization of morality. Finally, through analogizing the organizational form of language, the two key forms and specific mechanisms of bodioid construction, namely extension and mirroring, are indicated. With this in mind, the post-human existence landscape is discussed with the objective of providing theoretical insights into the study of the underlying philosophical principles of technological design.","sentences":["The advent of the post-human era has blurred the boundary between the body and artifacts.","Further, external materials and information are more deeply integrated into the body, making emerging technology a key driving force for shaping post-human existence and promoting bodily evolution.","Based on this, this study analyses the transformation process of three technological forms, namely tools, machines, and cyborgs, and reveals the construction of bodies and artifacts.","From the phenomenological perspective, the essences of body and artifact existences are reflected upon, and the existence is construction viewpoint is proposed.","Furthermore, a technological design concept, bodioid, is proposed to meticulously depict the characteristics of integrating similarities and differences towards unity between the body and artifacts, based on the theoretical foundation of technology mediation and the materialization of morality.","Finally, through analogizing the organizational form of language, the two key forms and specific mechanisms of bodioid construction, namely extension and mirroring, are indicated.","With this in mind, the post-human existence landscape is discussed with the objective of providing theoretical insights into the study of the underlying philosophical principles of technological design."],"url":"http://arxiv.org/abs/2403.02972v1","category":"cs.HC"}
{"created":"2024-03-05 13:30:55","title":"Regret-based budgeted decision rules under severe uncertainty","abstract":"One way to make decisions under uncertainty is to select an optimal option from a possible range of options, by maximizing the expected utilities derived from a probability model. However, under severe uncertainty, identifying precise probabilities is hard. For this reason, imprecise probability models uncertainty through convex sets of probabilities, and considers decision rules that can return multiple options to reflect insufficient information. Many well-founded decision rules have been studied in the past, but none of those standard rules are able to control the number of returned alternatives. This can be a problem for large decision problems, due to the cognitive burden decision makers have to face when presented with a large number of alternatives. Our contribution proposes regret-based ideas to construct new decision rules which return a bounded number of options, where the limit on the number of options is set in advance by the decision maker as an expression of their cognitive limitation. We also study their consistency and numerical behaviour.","sentences":["One way to make decisions under uncertainty is to select an optimal option from a possible range of options, by maximizing the expected utilities derived from a probability model.","However, under severe uncertainty, identifying precise probabilities is hard.","For this reason, imprecise probability models uncertainty through convex sets of probabilities, and considers decision rules that can return multiple options to reflect insufficient information.","Many well-founded decision rules have been studied in the past, but none of those standard rules are able to control the number of returned alternatives.","This can be a problem for large decision problems, due to the cognitive burden decision makers have to face when presented with a large number of alternatives.","Our contribution proposes regret-based ideas to construct new decision rules which return a bounded number of options, where the limit on the number of options is set in advance by the decision maker as an expression of their cognitive limitation.","We also study their consistency and numerical behaviour."],"url":"http://arxiv.org/abs/2403.02960v1","category":"math.ST"}
{"created":"2024-03-05 12:34:29","title":"Microscopic parametrization of the near threshold oscillations of the nucleon time-like effective electromagnetic form factors","abstract":"We present an analysis of the recent near threshold BESIII data for the nucleon time-like effective form factors. The damped oscillation emerging from the subtraction of the dipole formula is treated in non-perturbative-QCD, making use of the light cone distribution amplitudes expansion. Non-perturbative effects are accounted for by considering Q2-dependent coefficients in such expansions, whose free parameters are determined by fitting to the proton and neutron data. Possible implications and future analysis have been discussed.","sentences":["We present an analysis of the recent near threshold BESIII data for the nucleon time-like effective form factors.","The damped oscillation emerging from the subtraction of the dipole formula is treated in non-perturbative-QCD, making use of the light cone distribution amplitudes expansion.","Non-perturbative effects are accounted for by considering Q2-dependent coefficients in such expansions, whose free parameters are determined by fitting to the proton and neutron data.","Possible implications and future analysis have been discussed."],"url":"http://arxiv.org/abs/2403.02916v1","category":"hep-ph"}
{"created":"2024-03-05 12:32:19","title":"Photon-exciton strong coupling induced by topological edge state","abstract":"Strong coupling between single quantum emitter and dielectric nanoparticle has not been achieved due to large scattering loss of dielectric nanoparticle. Here we propose strong photon-exciton coupling mechanism under topological protection. In hybrid two-dimensional topological photonic crystal structure containing gap dielectric nanoantenna, owing to the robustness of the topological pattern, the photons originally scattered to the surrounding area can be guided into the edge-state channel, so the linewidth of nanoantenna is greatly narrowed. Simultaneously, the stronger local field in the nanoantenna gap provides a relatively large coupling coefficient. Therefore, strong coupling condition is achieved. Our results demonstrate the potential of dielectric structures as a platform for studying cavity quantum electrodynamics in a single quantum level.","sentences":["Strong coupling between single quantum emitter and dielectric nanoparticle has not been achieved due to large scattering loss of dielectric nanoparticle.","Here we propose strong photon-exciton coupling mechanism under topological protection.","In hybrid two-dimensional topological photonic crystal structure containing gap dielectric nanoantenna, owing to the robustness of the topological pattern, the photons originally scattered to the surrounding area can be guided into the edge-state channel, so the linewidth of nanoantenna is greatly narrowed.","Simultaneously, the stronger local field in the nanoantenna gap provides a relatively large coupling coefficient.","Therefore, strong coupling condition is achieved.","Our results demonstrate the potential of dielectric structures as a platform for studying cavity quantum electrodynamics in a single quantum level."],"url":"http://arxiv.org/abs/2403.02915v1","category":"physics.optics"}
{"created":"2024-03-05 12:28:00","title":"Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems","abstract":"We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\\varepsilon, \\delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} + \\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability. This result provides evidence of the near-optimality of the approach. Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest.","sentences":["We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting.","We propose $(\\varepsilon, \\delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives.","For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size.","Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} + \\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability.","This result provides evidence of the near-optimality of the approach.","Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods.","For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest."],"url":"http://arxiv.org/abs/2403.02912v1","category":"math.OC"}
{"created":"2024-03-05 12:01:34","title":"Ices on pebbles in protoplanetary discs","abstract":"The formation of solid macroscopic grains (pebbles) in protoplanetary discs is the first step toward planet formation. We aim to study the distribution of pebbles and the chemical composition of their ice mantles in a young protoplanetary disc. We use the two-dimensional hydrodynamical code FEOSAD in the thin-disc approximation, which is designed to model the global evolution of a self-gravitating viscous protoplanetary disc taking into account dust coagulation and fragmentation, thermal balance, and phase transitions and transport of the main volatiles (H$_2$O, CO$_{2}$, CH$_{4}$ and CO), which can reside in the gas, on small dust ($<1$ $\\mu$m), on grown dust ($>1$ $\\mu$m) and on pebbles. We model the dynamics of the protoplanetary disc from the cloud collapse to the 500 kyr moment. We determine the spatial distribution of pebbles and composition of their ice mantles and estimate the mass of volatiles on pebbles, grown dust and small dust. We show that pebbles form as early as 50 kyr after the disc formation and exist until the end of simulation (500 kyr), providing prerequisites for planet formation. All pebbles formed in the model are covered by icy mantles. Using a model considering accretion and desorption of volatiles onto dust/pebbles, we find that the ice mantles on pebbles consist mainly of H$_2$O and CO$_{2}$, and are carbon-depleted compared to gas and ices on small and grown dust, which contain more CO and CH$_4$. This suggests a possible dominance of oxygen in the composition of planets formed from pebbles under these conditions.","sentences":["The formation of solid macroscopic grains (pebbles) in protoplanetary discs is the first step toward planet formation.","We aim to study the distribution of pebbles and the chemical composition of their ice mantles in a young protoplanetary disc.","We use the two-dimensional hydrodynamical code FEOSAD in the thin-disc approximation, which is designed to model the global evolution of a self-gravitating viscous protoplanetary disc taking into account dust coagulation and fragmentation, thermal balance, and phase transitions and transport of the main volatiles (H$_2$O, CO$_{2}$, CH$_{4}$ and CO), which can reside in the gas, on small dust ($<1$ $\\mu$m), on grown dust ($>1$ $\\mu$m) and on pebbles.","We model the dynamics of the protoplanetary disc from the cloud collapse to the 500 kyr moment.","We determine the spatial distribution of pebbles and composition of their ice mantles and estimate the mass of volatiles on pebbles, grown dust and small dust.","We show that pebbles form as early as 50 kyr after the disc formation and exist until the end of simulation (500 kyr), providing prerequisites for planet formation.","All pebbles formed in the model are covered by icy mantles.","Using a model considering accretion and desorption of volatiles onto dust/pebbles, we find that the ice mantles on pebbles consist mainly of H$_2$O and CO$_{2}$, and are carbon-depleted compared to gas and ices on small and grown dust, which contain more CO and CH$_4$.","This suggests a possible dominance of oxygen in the composition of planets formed from pebbles under these conditions."],"url":"http://arxiv.org/abs/2403.02895v1","category":"astro-ph.EP"}
{"created":"2024-03-05 11:19:43","title":"Unlocking Electro-optic Resonant Phase Shifting for Multi-dimensional, Ultra-dynamic Photonic Switches","abstract":"Optical circuit switching is connection-oriented, being deterministic through the reservation of a complete wavelength channel or spatial path for a certain period. However, this comes at a trade-off against link dynamics, and overall capacity can thus be constrained by the time slot reservations, especially for switches with microsecond- to millisecond-scale reconfiguration times. For data-intensive applications, the communication patterns associated with random data sets typically yield short-lived flows. This situation calls for a new multi-dimensional switching paradigm that fully exploits not only the space and wavelength domains but also with nanosecond-scale reconfigurable capability in the time domain to enable ultra-dynamic links. In this work, we focus on the exploitation of micro-ring resonant phase shifters (RPSs) that are wavelength selective for optical switching in a single plane. By proposing an innovative analytical method with transmission circle chart, we fully unlock the power of RPS with nanosecond-scale reconfigurability and the capability to arbitrarily manipulate its phase and amplitude. Such a compact model offers fresh insights into designs with under and critically coupled RPSs beyond the commonly explored over-coupling condition. This creates not only versatile switch elements but also perfect absorbers for robust multi-wavelength operations. The proposed device can bring about a breakthrough in the optical switching capacity that potentially addresses the challenges faced by modern data center networks, as well as other photonic circuits for high-throughput signal processing.","sentences":["Optical circuit switching is connection-oriented, being deterministic through the reservation of a complete wavelength channel or spatial path for a certain period.","However, this comes at a trade-off against link dynamics, and overall capacity can thus be constrained by the time slot reservations, especially for switches with microsecond- to millisecond-scale reconfiguration times.","For data-intensive applications, the communication patterns associated with random data sets typically yield short-lived flows.","This situation calls for a new multi-dimensional switching paradigm that fully exploits not only the space and wavelength domains but also with nanosecond-scale reconfigurable capability in the time domain to enable ultra-dynamic links.","In this work, we focus on the exploitation of micro-ring resonant phase shifters (RPSs) that are wavelength selective for optical switching in a single plane.","By proposing an innovative analytical method with transmission circle chart, we fully unlock the power of RPS with nanosecond-scale reconfigurability and the capability to arbitrarily manipulate its phase and amplitude.","Such a compact model offers fresh insights into designs with under and critically coupled RPSs beyond the commonly explored over-coupling condition.","This creates not only versatile switch elements but also perfect absorbers for robust multi-wavelength operations.","The proposed device can bring about a breakthrough in the optical switching capacity that potentially addresses the challenges faced by modern data center networks, as well as other photonic circuits for high-throughput signal processing."],"url":"http://arxiv.org/abs/2403.02866v1","category":"physics.optics"}
{"created":"2024-03-05 11:10:46","title":"Efficient sparse probability measures recovery via Bregman gradient","abstract":"This paper presents an algorithm tailored for the efficient recovery of sparse probability measures incorporating $\\ell_0$-sparse regularization within the probability simplex constraint. Employing the Bregman proximal gradient method, our algorithm achieves sparsity by explicitly solving underlying subproblems. We rigorously establish the convergence properties of the algorithm, showcasing its capacity to converge to a local minimum with a convergence rate of $O(1/k)$ under mild assumptions. To substantiate the efficacy of our algorithm, we conduct numerical experiments, offering a compelling demonstration of its efficiency in recovering sparse probability measures.","sentences":["This paper presents an algorithm tailored for the efficient recovery of sparse probability measures incorporating $\\ell_0$-sparse regularization within the probability simplex constraint.","Employing the Bregman proximal gradient method, our algorithm achieves sparsity by explicitly solving underlying subproblems.","We rigorously establish the convergence properties of the algorithm, showcasing its capacity to converge to a local minimum with a convergence rate of $O(1/k)$ under mild assumptions.","To substantiate the efficacy of our algorithm, we conduct numerical experiments, offering a compelling demonstration of its efficiency in recovering sparse probability measures."],"url":"http://arxiv.org/abs/2403.02861v1","category":"math.OC"}
{"created":"2024-03-05 09:18:29","title":"Towards Robust Federated Learning via Logits Calibration on Non-IID Data","abstract":"Federated learning (FL) is a privacy-preserving distributed management framework based on collaborative model training of distributed devices in edge networks. However, recent studies have shown that FL is vulnerable to adversarial examples (AEs), leading to a significant drop in its performance. Meanwhile, the non-independent and identically distributed (non-IID) challenge of data distribution between edge devices can further degrade the performance of models. Consequently, both AEs and non-IID pose challenges to deploying robust learning models at the edge. In this work, we adopt the adversarial training (AT) framework to improve the robustness of FL models against adversarial example (AE) attacks, which can be termed as federated adversarial training (FAT). Moreover, we address the non-IID challenge by implementing a simple yet effective logits calibration strategy under the FAT framework, which can enhance the robustness of models when subjected to adversarial attacks. Specifically, we employ a direct strategy to adjust the logits output by assigning higher weights to classes with small samples during training. This approach effectively tackles the class imbalance in the training data, with the goal of mitigating biases between local and global models. Experimental results on three dataset benchmarks, MNIST, Fashion-MNIST, and CIFAR-10 show that our strategy achieves competitive results in natural and robust accuracy compared to several baselines.","sentences":["Federated learning (FL) is a privacy-preserving distributed management framework based on collaborative model training of distributed devices in edge networks.","However, recent studies have shown that FL is vulnerable to adversarial examples (AEs), leading to a significant drop in its performance.","Meanwhile, the non-independent and identically distributed (non-IID) challenge of data distribution between edge devices can further degrade the performance of models.","Consequently, both AEs and non-IID pose challenges to deploying robust learning models at the edge.","In this work, we adopt the adversarial training (AT) framework to improve the robustness of FL models against adversarial example (AE) attacks, which can be termed as federated adversarial training (FAT).","Moreover, we address the non-IID challenge by implementing a simple yet effective logits calibration strategy under the FAT framework, which can enhance the robustness of models when subjected to adversarial attacks.","Specifically, we employ a direct strategy to adjust the logits output by assigning higher weights to classes with small samples during training.","This approach effectively tackles the class imbalance in the training data, with the goal of mitigating biases between local and global models.","Experimental results on three dataset benchmarks, MNIST, Fashion-MNIST, and CIFAR-10 show that our strategy achieves competitive results in natural and robust accuracy compared to several baselines."],"url":"http://arxiv.org/abs/2403.02803v1","category":"cs.CV"}
{"created":"2024-03-05 09:12:33","title":"Depth resolution in piezoresponse force microscopy","abstract":"Piezoresponse Force Microscopy (PFM) is one of the most widespread methods for investigating and visualizing ferroelectric domain structures down to the nanometer length scale. PFM makes use of the direct coupling of the piezoelectric response to the crystal lattice, and hence is most often applied to spatially map the 3-dimensional (3D) near-surface domain distribution of any polar or ferroic sample. Nonetheless, since most samples investigated by PFM are at least semiconducting or fully insulating, the electric ac field emerging from the conductive scanning force microscopy (SFM) tip, penetrates the sample, and hence may also couple to polar features that are deeply buried into the bulk of the sample under investigation. Thus, in the work presented here, we experimentally and theoretically explore the contrast and depth resolution capabilities of PFM, by analyzing the dependence of several key parameters. These key parameters include the depth of the buried feature, i.e. here a domain wall (DW), as well as PFM-relevant technical parameters such as the tip radius, the PFM drive voltage and frequency, and the signal-to-noise ratio. The theoretical predictions are experimentally verified using x-cut periodically-poled lithium niobate single crystals that are specially prepared into wedge-shaped samples, in order to allow the buried feature, here the DW, to be `positioned' at any depth into the bulk. This inspection essentially contributes to the fundamental understanding in PFM contrast analysis, and to the reconstruction of 3D domain structures down to a 1-$\\mu$m-penetration depth into the sample.","sentences":["Piezoresponse Force Microscopy (PFM) is one of the most widespread methods for investigating and visualizing ferroelectric domain structures down to the nanometer length scale.","PFM makes use of the direct coupling of the piezoelectric response to the crystal lattice, and hence is most often applied to spatially map the 3-dimensional (3D) near-surface domain distribution of any polar or ferroic sample.","Nonetheless, since most samples investigated by PFM are at least semiconducting or fully insulating, the electric ac field emerging from the conductive scanning force microscopy (SFM) tip, penetrates the sample, and hence may also couple to polar features that are deeply buried into the bulk of the sample under investigation.","Thus, in the work presented here, we experimentally and theoretically explore the contrast and depth resolution capabilities of PFM, by analyzing the dependence of several key parameters.","These key parameters include the depth of the buried feature, i.e. here a domain wall (DW), as well as PFM-relevant technical parameters such as the tip radius, the PFM drive voltage and frequency, and the signal-to-noise ratio.","The theoretical predictions are experimentally verified using x-cut periodically-poled lithium niobate single crystals that are specially prepared into wedge-shaped samples, in order to allow the buried feature, here the DW, to be `positioned' at any depth into the bulk.","This inspection essentially contributes to the fundamental understanding in PFM contrast analysis, and to the reconstruction of 3D domain structures down to a 1-$\\mu$m-penetration depth into the sample."],"url":"http://arxiv.org/abs/2403.02797v1","category":"physics.app-ph"}
{"created":"2024-03-05 09:04:11","title":"Ne22 distillation and the cooling sequence of the old metal-rich open cluster NGC 6791","abstract":"Recent Monte Carlo plasma simulations to study in crystallizing carbon-oxygen (CO) white dwarfs (WDs) the phase separation of Ne22 (the most abundant metal after carbon and oxygen) have shown that, under the right conditions, a distillation process that transports Ne22 toward the WD centre is efficient and releases a considerable amount of gravitational energy that can lead to cooling delays of up to several Gyr. Here we present the first CO WD stellar evolution models that self-consistently include the effect of neon distillation, and cover the full range of CO WD masses, for a progenitor metallicity twice-solar appropriate for the old open cluster NGC 6791. The old age (about 8.5 Gyr) and high metallicity of this cluster -- hence the high neon content (about 3% by mass) in the cores of its WDs -- maximize the effect of neon distillation in the models to be compared with the observed cooling sequence. We discuss the effect of distillation on the internal chemical stratification and cooling time of the models, confirming that distillation causes cooling delays up to several Gyr, that depend in a non-monotonic way on the mass. We also show how our models produce luminosity functions (LFs) that can match the faint end of the observed WD LF in NGC 6791, for ages consistent with the range determined from a sample of cluster's eclipsing binary stars, and the main sequence turn-off. Without the inclusion of distillation the theoretical WD cooling sequences reach too faint magnitudes compared to the observations. We also propose James Webb Space Telescope observations that can independently demonstrate the efficiency of neon distillation in the interiors of NGC 6791 WDs, and help resolve the current uncertainty on the treatment of the electron conduction opacities for the hydrogen-helium envelope of the WD models.","sentences":["Recent Monte Carlo plasma simulations to study in crystallizing carbon-oxygen (CO) white dwarfs (WDs) the phase separation of Ne22 (the most abundant metal after carbon and oxygen) have shown that, under the right conditions, a distillation process that transports Ne22 toward the WD centre is efficient and releases a considerable amount of gravitational energy that can lead to cooling delays of up to several Gyr.","Here we present the first CO WD stellar evolution models that self-consistently include the effect of neon distillation, and cover the full range of CO WD masses, for a progenitor metallicity twice-solar appropriate for the old open cluster NGC 6791.","The old age (about 8.5 Gyr) and high metallicity of this cluster -- hence the high neon content (about 3% by mass) in the cores of its WDs -- maximize the effect of neon distillation in the models to be compared with the observed cooling sequence.","We discuss the effect of distillation on the internal chemical stratification and cooling time of the models, confirming that distillation causes cooling delays up to several Gyr, that depend in a non-monotonic way on the mass.","We also show how our models produce luminosity functions (LFs) that can match the faint end of the observed WD LF in NGC 6791, for ages consistent with the range determined from a sample of cluster's eclipsing binary stars, and the main sequence turn-off.","Without the inclusion of distillation the theoretical WD cooling sequences reach too faint magnitudes compared to the observations.","We also propose James Webb Space Telescope observations that can independently demonstrate the efficiency of neon distillation in the interiors of NGC 6791 WDs, and help resolve the current uncertainty on the treatment of the electron conduction opacities for the hydrogen-helium envelope of the WD models."],"url":"http://arxiv.org/abs/2403.02790v1","category":"astro-ph.SR"}
{"created":"2024-03-05 08:22:41","title":"Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models","abstract":"The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains. Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.","sentences":["The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience.","Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains.","In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy.","This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting.","2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training.","3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt.","The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains.","Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion.","This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities."],"url":"http://arxiv.org/abs/2403.02756v1","category":"cs.CL"}
{"created":"2024-03-05 08:19:52","title":"Learning to Ask Critical Questions for Assisting Product Search","abstract":"Product search plays an essential role in eCommerce. It was treated as a special type of information retrieval problem. Most existing works make use of historical data to improve the search performance, which do not take the opportunity to ask for user's current interest directly. Some session-aware methods take the user's clicks within the session as implicit feedback, but it is still just a guess on user's preference. To address this problem, recent conversational or question-based search models interact with users directly for understanding the user's interest explicitly. However, most users do not have a clear picture on what to buy at the initial stage. Asking critical attributes that the user is looking for after they explored for a while should be a more efficient way to help them searching for the target items. In this paper, we propose a dual-learning model that hybrids the best from both implicit session feedback and proactively clarifying with users on the most critical questions. We first establish a novel utility score to measure whether a clicked item provides useful information for finding the target. Then we develop the dual Selection Net and Ranking Net for choosing the critical questions and ranking the items. It innovatively links traditional click-stream data and text-based questions together. To verify our proposal, we did extensive experiments on a public dataset, and our model largely outperformed other state-of-the-art methods.","sentences":["Product search plays an essential role in eCommerce.","It was treated as a special type of information retrieval problem.","Most existing works make use of historical data to improve the search performance, which do not take the opportunity to ask for user's current interest directly.","Some session-aware methods take the user's clicks within the session as implicit feedback, but it is still just a guess on user's preference.","To address this problem, recent conversational or question-based search models interact with users directly for understanding the user's interest explicitly.","However, most users do not have a clear picture on what to buy at the initial stage.","Asking critical attributes that the user is looking for after they explored for a while should be a more efficient way to help them searching for the target items.","In this paper, we propose a dual-learning model that hybrids the best from both implicit session feedback and proactively clarifying with users on the most critical questions.","We first establish a novel utility score to measure whether a clicked item provides useful information for finding the target.","Then we develop the dual Selection Net and Ranking Net for choosing the critical questions and ranking the items.","It innovatively links traditional click-stream data and text-based questions together.","To verify our proposal, we did extensive experiments on a public dataset, and our model largely outperformed other state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.02754v1","category":"cs.IR"}
{"created":"2024-03-05 08:05:23","title":"Relativistic mean-field study of alpha decay in superheavy isotopes with 100 \\texorpdfstring{$\\leq$ Z $\\leq$}-120","abstract":"The $\\alpha$-decay half-lives of superheavy nuclei with $100 \\leq Z \\leq 120$ are comprehensively analyzed using the axially deformed relativistic mean field (RMF) formalism for the NL3$^*$ parameter set. We employ RMF binding energies to determine the $\\alpha$-decay energies and make a comparison with both the available experimental data and the theoretical results obtained from the global nuclear mass model WS4. The four distinct formulae, specifically the modified scaling law Brown, modified Viola-Seaborg, Yibin {\\it et al.} formula, and its modified form are used to calculate the decay half-lives and examine the numerical correlation between the half-life ($T_{1/2}$) for each $\\alpha$-decay energy. We notice that $T_{1/2}$ is significantly dependent on the decay formula in terms of isospin asymmetry and decay energy. We also noticed that modified scaling law Brown formula estimates of half-lives agreed comparatively better with the experiment as compared to others. Moreover, the present investigation provides significant information on the stability of the superheavy island considered for ongoing and/or future experiments.","sentences":["The $\\alpha$-decay half-lives of superheavy nuclei with $100 \\leq Z \\leq 120$ are comprehensively analyzed using the axially deformed relativistic mean field (RMF) formalism for the NL3$^*$ parameter set.","We employ RMF binding energies to determine the $\\alpha$-decay energies and make a comparison with both the available experimental data and the theoretical results obtained from the global nuclear mass model WS4.","The four distinct formulae, specifically the modified scaling law Brown, modified Viola-Seaborg, Yibin {\\it et al.} formula, and its modified form are used to calculate the decay half-lives and examine the numerical correlation between the half-life ($T_{1/2}$) for each $\\alpha$-decay energy.","We notice that $T_{1/2}$ is significantly dependent on the decay formula in terms of isospin asymmetry and decay energy.","We also noticed that modified scaling law Brown formula estimates of half-lives agreed comparatively better with the experiment as compared to others.","Moreover, the present investigation provides significant information on the stability of the superheavy island considered for ongoing and/or future experiments."],"url":"http://arxiv.org/abs/2403.02748v1","category":"nucl-th"}
{"created":"2024-03-05 07:26:29","title":"Light Thermal Dark Matter Beyond $p$-Wave Annihilation in Minimal Higgs Portal Model","abstract":"This study explores a minimal renormalizable dark matter (DM) model, incorporating a sub-GeV Majorana DM and a singlet scalar particle $\\phi$. Using scalar and pseudo-scalar interactions (couplings $c_s$ and $c_p$), we investigate implications for DM detection, considering $s$-wave, $p$-wave, and combined ($s$+$p$ wave) contributions in DM annihilation cross-section, as well as loop-correction contributions to DM-nucleon elastic scattering. Identifying a broad parameter space ($10 \\,\\rm{MeV} < m_\\chi \\lesssim m_\\phi$) within the $2\\sigma$ allowed region, we explore scenarios ($\\left|c_s\\right|\\gg \\left|c_p\\right|$, $\\left|c_s\\right|\\ll \\left|c_p\\right|$, and $\\left|c_s\\right|\\approx \\left|c_p\\right|$). We find that (i) a non-zero pseudo-scalar coupling alleviates direct detection constraints as a comparison with the previous pure scalar coupling case; (ii) CMB observations set stringent limits on pseudo-scalar interaction dominant cases, making $s$-wave annihilation viable only for $m_\\chi>1\\,\\rm{GeV}$; (iii) the preferred $\\phi$-resonance region can be tested in the future indirect detection experiments, such as e-ASTROGAM.","sentences":["This study explores a minimal renormalizable dark matter (DM) model, incorporating a sub-GeV Majorana DM and a singlet scalar particle $\\phi$. Using scalar and pseudo-scalar interactions (couplings $c_s$ and $c_p$), we investigate implications for DM detection, considering $s$-wave, $p$-wave, and combined ($s$+$p$ wave) contributions in DM annihilation cross-section, as well as loop-correction contributions to DM-nucleon elastic scattering.","Identifying a broad parameter space ($10 \\,\\rm{MeV} < m_\\chi \\lesssim m_\\phi$) within the $2\\sigma$ allowed region, we explore scenarios ($\\left|c_s\\right|\\gg \\left|c_p\\right|$, $\\left|c_s\\right|\\ll \\left|c_p\\right|$, and $\\left|c_s\\right|\\approx \\left|c_p\\right|$).","We find that (i) a non-zero pseudo-scalar coupling alleviates direct detection constraints as a comparison with the previous pure scalar coupling case; (ii) CMB observations set stringent limits on pseudo-scalar interaction dominant cases, making $s$-wave annihilation viable only for $m_\\chi>1\\,\\rm{GeV}$; (iii) the preferred $\\phi$-resonance region can be tested in the future indirect detection experiments, such as e-ASTROGAM."],"url":"http://arxiv.org/abs/2403.02721v1","category":"hep-ph"}
{"created":"2024-03-05 07:25:38","title":"A mid-infrared dual-comb spectrometer in step-sweep mode for high-resolution molecular spectroscopy","abstract":"To meet the challenges of high-resolution molecular spectroscopy, increasingly sophisticated spectroscopic techniques were developed. For a long time FTIR and laser-based spectroscopies were used for these studies. The recent development of dual-comb spectroscopy at high-resolution makes this technique a powerful tool for gas phase studies. We report on the use and characterization of the IRis-F1, a tabletop mid-infrared dual-comb spectrometer, in the newly developed step-sweep mode. The resolution of the wavenumber axis is increased by step-wise tuning (interleaving) and accurate measurement of the laser center wavelength and repetition frequency. Doppler limited measurements of N2O and CH4 reveal a wavenumber accuracy of 1E-4 cm-1 on the covered range of > 50 cm-1. Measured half-widths of absorption lines show no systematic broadening, indicating a negligible instrument response function. Finally, measurements of nitrogen pressure broadening coefficients in the v4 band of methane show that quantum cascade laser dual-comb spectroscopy in step-sweep mode is well adapted for measurements of precision spectroscopic data, in particular line shape parameters.","sentences":["To meet the challenges of high-resolution molecular spectroscopy, increasingly sophisticated spectroscopic techniques were developed.","For a long time FTIR and laser-based spectroscopies were used for these studies.","The recent development of dual-comb spectroscopy at high-resolution makes this technique a powerful tool for gas phase studies.","We report on the use and characterization of the IRis-F1, a tabletop mid-infrared dual-comb spectrometer, in the newly developed step-sweep mode.","The resolution of the wavenumber axis is increased by step-wise tuning (interleaving) and accurate measurement of the laser center wavelength and repetition frequency.","Doppler limited measurements of N2O and CH4 reveal a wavenumber accuracy of 1E-4 cm-1 on the covered range of > 50 cm-1.","Measured half-widths of absorption lines show no systematic broadening, indicating a negligible instrument response function.","Finally, measurements of nitrogen pressure broadening coefficients in the v4 band of methane show that quantum cascade laser dual-comb spectroscopy in step-sweep mode is well adapted for measurements of precision spectroscopic data, in particular line shape parameters."],"url":"http://arxiv.org/abs/2403.02720v1","category":"physics.optics"}
{"created":"2024-03-05 06:54:49","title":"Projected Gradient Descent Algorithm for Low-Rank Matrix Estimation","abstract":"Most existing methodologies of estimating low-rank matrices rely on Burer-Monteiro factorization, but these approaches can suffer from slow convergence, especially when dealing with solutions characterized by a large condition number, defined by the ratio of the largest to the $r$-th singular values, where $r$ is the search rank. While methods such as Scaled Gradient Descent have been proposed to address this issue, such methods are more complicated and sometimes have weaker theoretical guarantees, for example, in the rank-deficient setting. In contrast, this paper demonstrates the effectiveness of the projected gradient descent algorithm. Firstly, its local convergence rate is independent of the condition number. Secondly, under conditions where the objective function is rank-$2r$ restricted $L$-smooth and $\\mu$-strongly convex, with $L/\\mu < 3$, projected gradient descent with appropriate step size converges linearly to the solution. Moreover, a perturbed version of this algorithm effectively navigates away from saddle points, converging to an approximate solution or a second-order local minimizer across a wide range of step sizes. Furthermore, we establish that there are no spurious local minimizers in estimating asymmetric low-rank matrices when the objective function satisfies $L/\\mu<3.$","sentences":["Most existing methodologies of estimating low-rank matrices rely on Burer-Monteiro factorization, but these approaches can suffer from slow convergence, especially when dealing with solutions characterized by a large condition number, defined by the ratio of the largest to the $r$-th singular values, where $r$ is the search rank.","While methods such as Scaled Gradient Descent have been proposed to address this issue, such methods are more complicated and sometimes have weaker theoretical guarantees, for example, in the rank-deficient setting.","In contrast, this paper demonstrates the effectiveness of the projected gradient descent algorithm.","Firstly, its local convergence rate is independent of the condition number.","Secondly, under conditions where the objective function is rank-$2r$ restricted $L$-smooth and $\\mu$-strongly convex, with $L/\\mu < 3$, projected gradient descent with appropriate step size converges linearly to the solution.","Moreover, a perturbed version of this algorithm effectively navigates away from saddle points, converging to an approximate solution or a second-order local minimizer across a wide range of step sizes.","Furthermore, we establish that there are no spurious local minimizers in estimating asymmetric low-rank matrices when the objective function satisfies $L/\\mu<3.$"],"url":"http://arxiv.org/abs/2403.02704v1","category":"math.OC"}
{"created":"2024-03-05 06:23:55","title":"Controllable Prompt Tuning For Balancing Group Distributional Robustness","abstract":"Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring only 0.4% tunable parameters.","sentences":["Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts.","While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups.","To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them.","However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging.","Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques.","On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring only 0.4% tunable parameters."],"url":"http://arxiv.org/abs/2403.02695v1","category":"cs.LG"}
{"created":"2024-03-05 06:20:49","title":"Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning","abstract":"For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various benchmark datasets. Our code is available at \\url{https://github.com/BaeHeeSun/RENT}.","sentences":["For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk.","Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it.","We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT.","Specifically, we first demonstrate current utilizations can have potential limitations for implementation.","As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework.","With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix.","Empirically, RENT consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various benchmark datasets.","Our code is available at \\url{https://github.com/BaeHeeSun/RENT}."],"url":"http://arxiv.org/abs/2403.02690v1","category":"cs.LG"}
{"created":"2024-03-05 06:00:43","title":"Probabilistic assessment of the reactor vessel lifetime","abstract":"A simple and rapid method is proposed for assessing the reduction in the lifetime of steel walls of the reactor vessel under neutron irradiation. The method is based on modeling the number of radiation defects by the behavior of a general time-dependent random process of death and birth and queuing theory. Necessary data for assessments: the estimated operating time of the reactor (in years), the actual operating time of the reactor, the accumulated fluence depending on time, the temperature on the walls of the reactor vessel, the neutron absorption cross section of the steel of the reactor walls, the energy of fast neutrons striking the walls. The main problem: getting this accurate data.","sentences":["A simple and rapid method is proposed for assessing the reduction in the lifetime of steel walls of the reactor vessel under neutron irradiation.","The method is based on modeling the number of radiation defects by the behavior of a general time-dependent random process of death and birth and queuing theory.","Necessary data for assessments: the estimated operating time of the reactor (in years), the actual operating time of the reactor, the accumulated fluence depending on time, the temperature on the walls of the reactor vessel, the neutron absorption cross section of the steel of the reactor walls, the energy of fast neutrons striking the walls.","The main problem: getting this accurate data."],"url":"http://arxiv.org/abs/2403.02675v1","category":"physics.ins-det"}
{"created":"2024-03-05 05:03:38","title":"Hardware requirements for trapped-ion based verifiable blind quantum computing with a measurement-only client","abstract":"In blind quantum computing, a user with a simple client device can perform a quantum computation on a remote quantum server such that the server cannot gain knowledge about the computation. Here, we numerically investigate hardware requirements for verifiable blind quantum computing using an ion trap as server and a distant measurement-only client. While the client has no direct access to quantum-computing resources, it can remotely execute quantum programs on the server by measuring photons emitted by the trapped ion. We introduce a numerical model for trapped-ion quantum devices in NetSquid, a discrete-event simulator for quantum networks. Using this, we determine the minimal hardware requirements on a per-parameter basis to perform the verifiable blind quantum computing protocol. We benchmark these for a five-qubit linear graph state, with which any single-qubit rotation can be performed, where client and server are separated by 50 km. Current state-of-the-art ion traps satisfy the minimal requirements on a per-parameter basis, but all current imperfections combined make it impossible to perform the blind computation securely over 50 km using existing technology. Using a genetic algorithm, we determine the set of hardware parameters that minimises the total improvements required, finding directions along which to improve hardware to reach our threshold error probability that would enable experimental demonstration. In this way, we lay a path for the near-term experimental progress required to realise the implementation of verifiable blind quantum computing over a 50 km distance.","sentences":["In blind quantum computing, a user with a simple client device can perform a quantum computation on a remote quantum server such that the server cannot gain knowledge about the computation.","Here, we numerically investigate hardware requirements for verifiable blind quantum computing using an ion trap as server and a distant measurement-only client.","While the client has no direct access to quantum-computing resources, it can remotely execute quantum programs on the server by measuring photons emitted by the trapped ion.","We introduce a numerical model for trapped-ion quantum devices in NetSquid, a discrete-event simulator for quantum networks.","Using this, we determine the minimal hardware requirements on a per-parameter basis to perform the verifiable blind quantum computing protocol.","We benchmark these for a five-qubit linear graph state, with which any single-qubit rotation can be performed, where client and server are separated by 50 km.","Current state-of-the-art ion traps satisfy the minimal requirements on a per-parameter basis, but all current imperfections combined make it impossible to perform the blind computation securely over 50 km using existing technology.","Using a genetic algorithm, we determine the set of hardware parameters that minimises the total improvements required, finding directions along which to improve hardware to reach our threshold error probability that would enable experimental demonstration.","In this way, we lay a path for the near-term experimental progress required to realise the implementation of verifiable blind quantum computing over a 50 km distance."],"url":"http://arxiv.org/abs/2403.02656v1","category":"quant-ph"}
{"created":"2024-03-05 04:38:13","title":"Few-shot Learner Parameterization by Diffusion Time-steps","abstract":"Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in https://github.com/yue-zhongqi/tif.","sentences":["Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels.","To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent.","Building on this, we propose Time-step Few-shot (TiF) learner.","We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt.","Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes.","For a test image, we can use the parameterization to only extract the nuanced class attributes for classification.","TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks.","Codes are in https://github.com/yue-zhongqi/tif."],"url":"http://arxiv.org/abs/2403.02649v1","category":"cs.CV"}
{"created":"2024-03-05 04:20:03","title":"UFO: Uncertainty-aware LiDAR-image Fusion for Off-road Semantic Terrain Map Estimation","abstract":"Autonomous off-road navigation requires an accurate semantic understanding of the environment, often converted into a bird's-eye view (BEV) representation for various downstream tasks. While learning-based methods have shown success in generating local semantic terrain maps directly from sensor data, their efficacy in off-road environments is hindered by challenges in accurately representing uncertain terrain features. This paper presents a learning-based fusion method for generating dense terrain classification maps in BEV. By performing LiDAR-image fusion at multiple scales, our approach enhances the accuracy of semantic maps generated from an RGB image and a single-sweep LiDAR scan. Utilizing uncertainty-aware pseudo-labels further enhances the network's ability to learn reliably in off-road environments without requiring precise 3D annotations. By conducting thorough experiments using off-road driving datasets, we demonstrate that our method can improve accuracy in off-road terrains, validating its efficacy in facilitating reliable and safe autonomous navigation in challenging off-road settings.","sentences":["Autonomous off-road navigation requires an accurate semantic understanding of the environment, often converted into a bird's-eye view (BEV) representation for various downstream tasks.","While learning-based methods have shown success in generating local semantic terrain maps directly from sensor data, their efficacy in off-road environments is hindered by challenges in accurately representing uncertain terrain features.","This paper presents a learning-based fusion method for generating dense terrain classification maps in BEV.","By performing LiDAR-image fusion at multiple scales, our approach enhances the accuracy of semantic maps generated from an RGB image and a single-sweep LiDAR scan.","Utilizing uncertainty-aware pseudo-labels further enhances the network's ability to learn reliably in off-road environments without requiring precise 3D annotations.","By conducting thorough experiments using off-road driving datasets, we demonstrate that our method can improve accuracy in off-road terrains, validating its efficacy in facilitating reliable and safe autonomous navigation in challenging off-road settings."],"url":"http://arxiv.org/abs/2403.02642v1","category":"cs.RO"}
{"created":"2024-03-05 04:00:50","title":"BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection","abstract":"Humans can easily distinguish the known and unknown categories and can recognize the unknown object by learning it once instead of repeating it many times without forgetting the learned object. Hence, we aim to make deep learning models simulate the way people learn. We refer to such a learning manner as OnLine Open World Object Detection(OLOWOD). Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important. Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency. In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge. Therefore, we propose a simple plug-and-play method, called Brain-inspired Streaming Dual-level Perturbations(BSDP), to solve the OLOWOD problem. Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories. We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner.","sentences":["Humans can easily distinguish the known and unknown categories and can recognize the unknown object by learning it once instead of repeating it many times without forgetting the learned object.","Hence, we aim to make deep learning models simulate the way people learn.","We refer to such a learning manner as OnLine Open World Object Detection(OLOWOD).","Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important.","Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency.","In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge.","Therefore, we propose a simple plug-and-play method, called Brain-inspired Streaming Dual-level Perturbations(BSDP), to solve the OLOWOD problem.","Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories.","We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner."],"url":"http://arxiv.org/abs/2403.02637v1","category":"cs.CV"}
{"created":"2024-03-05 03:59:20","title":"Algorithms for Galois Words: Detection, Factorization, and Rotation","abstract":"Lyndon words are extensively studied in combinatorics on words -- they play a crucial role on upper bounding the number of runs a word can have [Bannai+, SIAM J. Comput.'17]. We can determine Lyndon words, factorize a word into Lyndon words in lexicographically decreasing order, and find the Lyndon rotation of a word, all in linear time within constant additional working space. A recent research interest emerged from the question of what happens when we change the lexicographic order, which is at the heart of the definition of Lyndon words. In particular, the alternating order, where the order of all odd positions becomes reversed, has been recently proposed. While a Lyndon word is, among all its cyclic rotations, the smallest one with respect to the lexicographic order, a Galois word exhibits the same property by exchanging the lexicographic order with the alternating order. Unfortunately, this exchange has a large impact on the properties Galois words exhibit, which makes it a nontrivial task to translate results from Lyndon words to Galois words. Up until now, it has only been conjectured that linear-time algorithms with constant additional working space in the spirit of Duval's algorithm are possible for computing the Galois factorization or the Galois rotation.   Here, we affirm this conjecture as follows. Given a word $T$ of length $n$, we can determine whether $T$ is a Galois word, in $O(n)$ time with constant additional working space. Within the same complexities, we can also determine the Galois rotation of $T$, and compute the Galois factorization of $T$ online. The last result settles Open Problem~1 in [Dolce et al., TCS'2019] for Galois words.","sentences":["Lyndon words are extensively studied in combinatorics on words -- they play a crucial role on upper bounding the number of runs a word can have [Bannai+, SIAM J. Comput.'17].","We can determine Lyndon words, factorize a word into Lyndon words in lexicographically decreasing order, and find the Lyndon rotation of a word, all in linear time within constant additional working space.","A recent research interest emerged from the question of what happens when we change the lexicographic order, which is at the heart of the definition of Lyndon words.","In particular, the alternating order, where the order of all odd positions becomes reversed, has been recently proposed.","While a Lyndon word is, among all its cyclic rotations, the smallest one with respect to the lexicographic order, a Galois word exhibits the same property by exchanging the lexicographic order with the alternating order.","Unfortunately, this exchange has a large impact on the properties Galois words exhibit, which makes it a nontrivial task to translate results from Lyndon words to Galois words.","Up until now, it has only been conjectured that linear-time algorithms with constant additional working space in the spirit of Duval's algorithm are possible for computing the Galois factorization or the Galois rotation.   ","Here, we affirm this conjecture as follows.","Given a word $T$ of length $n$, we can determine whether $T$ is a Galois word, in $O(n)$ time with constant additional working space.","Within the same complexities, we can also determine the Galois rotation of $T$, and compute the Galois factorization of $T$ online.","The last result settles Open Problem~1 in [Dolce et al., TCS'2019] for Galois words."],"url":"http://arxiv.org/abs/2403.02636v1","category":"cs.DS"}
{"created":"2024-03-05 03:39:23","title":"Towards Geometric-Photometric Joint Alignment for Facial Mesh Registration","abstract":"This paper presents a Geometric-Photometric Joint Alignment(GPJA) method, for accurately aligning human expressions by combining geometry and photometric information. Common practices for registering human heads typically involve aligning landmarks with facial template meshes using geometry processing approaches, but often overlook photometric consistency. GPJA overcomes this limitation by leveraging differentiable rendering to align vertices with target expressions, achieving joint alignment in geometry and photometric appearances automatically, without the need for semantic annotation or aligned meshes for training. It features a holistic rendering alignment strategy and a multiscale regularized optimization for robust and fast convergence. The method utilizes derivatives at vertex positions for supervision and employs a gradient-based algorithm which guarantees smoothness and avoids topological defects during the geometry evolution. Experimental results demonstrate faithful alignment under various expressions, surpassing the conventional ICP-based methods and the state-of-the-art deep learning based method. In practical, our method enhances the efficiency of obtaining topology-consistent face models from multi-view stereo facial scanning.","sentences":["This paper presents a Geometric-Photometric Joint Alignment(GPJA) method, for accurately aligning human expressions by combining geometry and photometric information.","Common practices for registering human heads typically involve aligning landmarks with facial template meshes using geometry processing approaches, but often overlook photometric consistency.","GPJA overcomes this limitation by leveraging differentiable rendering to align vertices with target expressions, achieving joint alignment in geometry and photometric appearances automatically, without the need for semantic annotation or aligned meshes for training.","It features a holistic rendering alignment strategy and a multiscale regularized optimization for robust and fast convergence.","The method utilizes derivatives at vertex positions for supervision and employs a gradient-based algorithm which guarantees smoothness and avoids topological defects during the geometry evolution.","Experimental results demonstrate faithful alignment under various expressions, surpassing the conventional ICP-based methods and the state-of-the-art deep learning based method.","In practical, our method enhances the efficiency of obtaining topology-consistent face models from multi-view stereo facial scanning."],"url":"http://arxiv.org/abs/2403.02629v1","category":"cs.GR"}
{"created":"2024-03-05 03:22:37","title":"CSS_J154915.7+375506: A low-mass-ratio marginal contact binary system with a hierarchical third body","abstract":"We presented the multi-filter light curves of CSS_J154915.7+375506 inaugurally, which were observed by the 1.5 m AZT-22 telescope at Maidanak Astronomical Observatory. A low-resolution spectrum obtained by LAMOST reveals it is an A-type close binary. By analyzing the BVRI total-eclipse light curves, we are able to derive a reliable photometric solution for this system, which indicates that CSS_J154915.7+375506 is an extremely low-mass-ratio (q=0.138) marginal contact binary system. The location in the HR diagram shows that its secondary component with a much smaller mass is the more evolved one, indicating the mass ratio reversal occurred. The present secondary component had transferred a significant amount of mass to the present primary one. By the combination of a total of 20 times of minimum, we investigated its O-C curve. A periodic oscillation and a possible period decrease have been detected. As the period decreases, the system will evolve towards the contact phase. This makes CSS\\_J154915.7+375506 a valuable case to study the formation scenario of contact binaries through mass reversal. The periodic oscillation suggested a third body with a minimal mass of $0.91\\,M_{\\odot}$, which is larger than that of the less massive component in the central binary. This implies that the secondary body was not replaced by the third body during early stellar interactions, indicating that it is a fossil system and retains its original dynamical information.","sentences":["We presented the multi-filter light curves of CSS_J154915.7+375506 inaugurally, which were observed by the 1.5 m AZT-22 telescope at Maidanak Astronomical Observatory.","A low-resolution spectrum obtained by LAMOST reveals it is an A-type close binary.","By analyzing the BVRI total-eclipse light curves, we are able to derive a reliable photometric solution for this system, which indicates that CSS_J154915.7+375506 is an extremely low-mass-ratio (q=0.138) marginal contact binary system.","The location in the HR diagram shows that its secondary component with a much smaller mass is the more evolved one, indicating the mass ratio reversal occurred.","The present secondary component had transferred a significant amount of mass to the present primary one.","By the combination of a total of 20 times of minimum, we investigated its O-C curve.","A periodic oscillation and a possible period decrease have been detected.","As the period decreases, the system will evolve towards the contact phase.","This makes CSS\\_J154915.7+375506 a valuable case to study the formation scenario of contact binaries through mass reversal.","The periodic oscillation suggested a third body with a minimal mass of $0.91\\,M_{\\odot}$, which is larger than that of the less massive component in the central binary.","This implies that the secondary body was not replaced by the third body during early stellar interactions, indicating that it is a fossil system and retains its original dynamical information."],"url":"http://arxiv.org/abs/2403.02621v1","category":"astro-ph.SR"}
{"created":"2024-03-05 03:15:27","title":"A Reduced-Order Resistive Force Model for Robotic Foot-Mud Interactions","abstract":"Legged robots are well-suited for broad exploration tasks in complex environments with yielding terrain. Understanding robotic foot-terrain interactions is critical for safe locomotion and walking efficiency for legged robots. This paper presents a reduced-order resistive-force model for robotic-foot/mud interactions. We focus on vertical robot locomotion on mud and propose a visco-elasto-plastic analog to model the foot/mud interaction forces. Dynamic behaviors such as mud visco-elasticity, withdrawing cohesive suction, and yielding are explicitly discussed with the proposed model. Besides comparing with dry/wet granular materials, mud intrusion experiments are conducted to validate the force model. The dependency of the model parameter on water content and foot velocity is also studied to reveal in-depth model properties under various conditions. The proposed force model potentially provides an enabling tool for legged robot locomotion and control on muddy terrain.","sentences":["Legged robots are well-suited for broad exploration tasks in complex environments with yielding terrain.","Understanding robotic foot-terrain interactions is critical for safe locomotion and walking efficiency for legged robots.","This paper presents a reduced-order resistive-force model for robotic-foot/mud interactions.","We focus on vertical robot locomotion on mud and propose a visco-elasto-plastic analog to model the foot/mud interaction forces.","Dynamic behaviors such as mud visco-elasticity, withdrawing cohesive suction, and yielding are explicitly discussed with the proposed model.","Besides comparing with dry/wet granular materials, mud intrusion experiments are conducted to validate the force model.","The dependency of the model parameter on water content and foot velocity is also studied to reveal in-depth model properties under various conditions.","The proposed force model potentially provides an enabling tool for legged robot locomotion and control on muddy terrain."],"url":"http://arxiv.org/abs/2403.02617v1","category":"cs.RO"}
{"created":"2024-03-05 03:03:58","title":"Modification to the Jeans criterion by external tides: Anisotropic fragmentation and formation of filaments","abstract":"The Jeans criterion sets the foundation of our understanding of gravitational collapse. Jog studied the fragmentation of gas under external tides and derived a dispersion relation $$   l' = l_{\\rm Jeans} \\frac{1} {(1 + \\lambda_0' / 4 \\pi G \\rho_0)^{1/2}} \\;. $$ She further concludes that the Jeans mass is $m_{\\rm incorrect}'=m_{\\rm Jeans} ( 1/(1 + \\lambda_0' / 4 \\pi G \\rho_0)^{3/2})$. We clarify that due to the inhomogeneous nature of tides, this characteristic mass is incorrect. Under weak tides, the mass is $m \\approx \\rho\\, l_1 l_2 l_3$, where the modifications to Jeans lengths along all three dimensions need to be considered; when the tide is strong enough, collapse can only occur once 1 or 2 dimensions. In the latter case, tides can stretch the gas, leading to the formation of filaments.","sentences":["The Jeans criterion sets the foundation of our understanding of gravitational collapse.","Jog studied the fragmentation of gas under external tides and derived a dispersion relation $$   l' = l_{\\rm Jeans} \\frac{1} {(1 + \\lambda_0' / 4 \\pi G \\rho_0)^{1/2}} \\;.","$$ She further concludes that the Jeans mass is $m_{\\rm incorrect}'=m_{\\rm Jeans} ( 1/(1 + \\lambda_0' / 4 \\pi G \\rho_0)^{3/2})$.","We clarify that due to the inhomogeneous nature of tides, this characteristic mass is incorrect.","Under weak tides, the mass is $m \\approx \\rho\\, l_1 l_2 l_3$, where the modifications to Jeans lengths along all three dimensions need to be considered; when the tide is strong enough, collapse can only occur once 1 or 2 dimensions.","In the latter case, tides can stretch the gas, leading to the formation of filaments."],"url":"http://arxiv.org/abs/2403.02612v1","category":"astro-ph.GA"}
{"created":"2024-03-05 02:53:24","title":"Search Intenion Network for Personalized Query Auto-Completion in E-Commerce","abstract":"Query Auto-Completion(QAC), as an important part of the modern search engine, plays a key role in complementing user queries and helping them refine their search intentions.Today's QAC systems in real-world scenarios face two major challenges:1)intention equivocality(IE): during the user's typing process,the prefix often contains a combination of characters and subwords, which makes the current intention ambiguous and difficult to model.2)intention transfer (IT):previous works make personalized recommendations based on users' historical sequences, but ignore the search intention transfer.However, the current intention extracted from prefix may be contrary to the historical preferences.","sentences":["Query Auto-Completion(QAC), as an important part of the modern search engine, plays a key role in complementing user queries and helping them refine their search intentions.","Today's QAC systems in real-world scenarios face two major challenges:1)intention equivocality(IE): during the user's typing process,the prefix often contains a combination of characters and subwords, which makes the current intention ambiguous and difficult to model.2)intention transfer (IT):previous works make personalized recommendations based on users' historical sequences, but ignore the search intention transfer.","However, the current intention extracted from prefix may be contrary to the historical preferences."],"url":"http://arxiv.org/abs/2403.02609v1","category":"cs.IR"}
{"created":"2024-03-05 02:44:57","title":"Fast Radio Bursts in the Disks of Active Galactic Nuclei","abstract":"Fast radio bursts (FRBs) are luminous millisecond-duration radio pulses with extragalactic origin, which were discovered more than a decade ago. Despite the numerous samples, the physical origin of FRBs remains poorly understood. FRBs have been thought to originate from young magnetars or accreting compact objects (COs). Massive stars or COs are predicted to be embedded in the accretion disks of active galactic nuclei (AGNs). The dense disk absorbs FRBs severely, making them difficult to observe. However, progenitors ejecta or outflow feedback from the accreting COs interact with the disk material to form a cavity. The existence of the cavity can reduce the absorption by the dense disk materials, making FRBs escape. Here we investigate the production and propagation of FRBs in AGN disks and find that the AGN environments lead to the following unique observational properties, which can be verified in future observation. First, the dense material in the disk can cause large dispersion measure (DM) and rotation measure (RM). Second, the toroidal magnetic field in the AGN disk can cause Faraday conversion. Third, during the shock breakout, DM and RM show non-power-law evolution patterns over time. Fourth, for accreting-powered models, higher accretion rates lead to more bright bursts in AGN disks.","sentences":["Fast radio bursts (FRBs) are luminous millisecond-duration radio pulses with extragalactic origin, which were discovered more than a decade ago.","Despite the numerous samples, the physical origin of FRBs remains poorly understood.","FRBs have been thought to originate from young magnetars or accreting compact objects (COs).","Massive stars or COs are predicted to be embedded in the accretion disks of active galactic nuclei (AGNs).","The dense disk absorbs FRBs severely, making them difficult to observe.","However, progenitors ejecta or outflow feedback from the accreting COs interact with the disk material to form a cavity.","The existence of the cavity can reduce the absorption by the dense disk materials, making FRBs escape.","Here we investigate the production and propagation of FRBs in AGN disks and find that the AGN environments lead to the following unique observational properties, which can be verified in future observation.","First, the dense material in the disk can cause large dispersion measure (DM) and rotation measure (RM).","Second, the toroidal magnetic field in the AGN disk can cause Faraday conversion.","Third, during the shock breakout, DM and RM show non-power-law evolution patterns over time.","Fourth, for accreting-powered models, higher accretion rates lead to more bright bursts in AGN disks."],"url":"http://arxiv.org/abs/2403.02606v1","category":"astro-ph.HE"}
{"created":"2024-03-05 02:38:46","title":"UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments","abstract":"Learning a universal manipulation policy encompassing doors with diverse categories, geometries and mechanisms, is crucial for future embodied agents to effectively work in complex and broad real-world scenarios. Due to the limited datasets and unrealistic simulation environments, previous works fail to achieve good performance across various doors. In this work, we build a novel door manipulation environment reflecting different realistic door manipulation mechanisms, and further equip this environment with a large-scale door dataset covering 6 door categories with hundreds of door bodies and handles, making up thousands of different door instances. Additionally, to better emulate real-world scenarios, we introduce a mobile robot as the agent and use the partial and occluded point cloud as the observation, which are not considered in previous works while possessing significance for real-world implementations. To learn a universal policy over diverse doors, we propose a novel framework disentangling the whole manipulation process into three stages, and integrating them by training in the reversed order of inference. Extensive experiments validate the effectiveness of our designs and demonstrate our framework's strong performance.","sentences":["Learning a universal manipulation policy encompassing doors with diverse categories, geometries and mechanisms, is crucial for future embodied agents to effectively work in complex and broad real-world scenarios.","Due to the limited datasets and unrealistic simulation environments, previous works fail to achieve good performance across various doors.","In this work, we build a novel door manipulation environment reflecting different realistic door manipulation mechanisms, and further equip this environment with a large-scale door dataset covering 6 door categories with hundreds of door bodies and handles, making up thousands of different door instances.","Additionally, to better emulate real-world scenarios, we introduce a mobile robot as the agent and use the partial and occluded point cloud as the observation, which are not considered in previous works while possessing significance for real-world implementations.","To learn a universal policy over diverse doors, we propose a novel framework disentangling the whole manipulation process into three stages, and integrating them by training in the reversed order of inference.","Extensive experiments validate the effectiveness of our designs and demonstrate our framework's strong performance."],"url":"http://arxiv.org/abs/2403.02604v1","category":"cs.RO"}
{"created":"2024-03-05 02:29:18","title":"Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning","abstract":"For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel \"Low-Res Leads the Way\" (LWay) training framework, merging Supervised Pre-training with Self-supervised Learning to enhance the adaptability of SR models to real-world images. Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-resolved outputs for LR reconstruction. Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on high-frequency details. Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen real-world datasets, outperforming existing methods. Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications.","sentences":["For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge.","This work introduces a novel \"Low-Res Leads the Way\" (LWay) training framework, merging Supervised Pre-training with Self-supervised Learning to enhance the adaptability of SR models to real-world images.","Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-resolved outputs for LR reconstruction.","Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images.","The integration of Discrete Wavelet Transform (DWT) further refines the focus on high-frequency details.","Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen real-world datasets, outperforming existing methods.","Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications."],"url":"http://arxiv.org/abs/2403.02601v1","category":"eess.IV"}
{"created":"2024-03-05 02:19:59","title":"Effect of dark matter on the shadow of a distorted and deformed compact object","abstract":"This work investigates observational properties, namely the shadow and photon ring structure, of emission profiles originated near compact objects. In particular, we consider a distorted and deformed compact object characterized by quadrupoles and surrounded by an optically thin and geometrically thin accretion disk with different emission profiles modeled by the Johnson's Standard-Unbound (SU) distribution in the reference frame of the emitter. Under these assumptions, we produce the observed intensity profiles and shadow images for a face-on observer. Our results indicate that, due to the fact that modifications of the quadrupole parameters affect the radius of the innermost stable circular orbit (ISCO) and the photon sphere (PS), the observed shadow images and their properties are significantly influenced by the quadrupole parameters and emission profiles. Furthermore, we analyze the impact of the presence of a dark matter halo in the observational imprints considered and verify that both the increase in the matter contained in the halo or decrease in the length-scale of the halo lead to an increase in the size of the observed shadow. Our results indicate potential degeneracies between the observational features of distorded and deformed compact objects with those of spherically symmetric blackholes, which could be assessed by a comparison with the current and future generation of optical experiments in gravitational physics.","sentences":["This work investigates observational properties, namely the shadow and photon ring structure, of emission profiles originated near compact objects.","In particular, we consider a distorted and deformed compact object characterized by quadrupoles and surrounded by an optically thin and geometrically thin accretion disk with different emission profiles modeled by the Johnson's Standard-Unbound (SU) distribution in the reference frame of the emitter.","Under these assumptions, we produce the observed intensity profiles and shadow images for a face-on observer.","Our results indicate that, due to the fact that modifications of the quadrupole parameters affect the radius of the innermost stable circular orbit (ISCO) and the photon sphere (PS), the observed shadow images and their properties are significantly influenced by the quadrupole parameters and emission profiles.","Furthermore, we analyze the impact of the presence of a dark matter halo in the observational imprints considered and verify that both the increase in the matter contained in the halo or decrease in the length-scale of the halo lead to an increase in the size of the observed shadow.","Our results indicate potential degeneracies between the observational features of distorded and deformed compact objects with those of spherically symmetric blackholes, which could be assessed by a comparison with the current and future generation of optical experiments in gravitational physics."],"url":"http://arxiv.org/abs/2403.02597v1","category":"astro-ph.HE"}
{"created":"2024-03-05 01:37:37","title":"Generative Software Engineering","abstract":"The rapid development of deep learning techniques, improved computational power, and the availability of vast training data have led to significant advancements in pre-trained models and large language models (LLMs). Pre-trained models based on architectures such as BERT and Transformer, as well as LLMs like ChatGPT, have demonstrated remarkable language capabilities and found applications in Software engineering. Software engineering tasks can be divided into many categories, among which generative tasks are the most concern by researchers, where pre-trained models and LLMs possess powerful language representation and contextual awareness capabilities, enabling them to leverage diverse training data and adapt to generative tasks through fine-tuning, transfer learning, and prompt engineering. These advantages make them effective tools in generative tasks and have demonstrated excellent performance. In this paper, we present a comprehensive literature review of generative tasks in SE using pre-trained models and LLMs. We accurately categorize SE generative tasks based on software engineering methodologies and summarize the advanced pre-trained models and LLMs involved, as well as the datasets and evaluation metrics used. Additionally, we identify key strengths, weaknesses, and gaps in existing approaches, and propose potential research directions. This review aims to provide researchers and practitioners with an in-depth analysis and guidance on the application of pre-trained models and LLMs in generative tasks within SE.","sentences":["The rapid development of deep learning techniques, improved computational power, and the availability of vast training data have led to significant advancements in pre-trained models and large language models (LLMs).","Pre-trained models based on architectures such as BERT and Transformer, as well as LLMs like ChatGPT, have demonstrated remarkable language capabilities and found applications in Software engineering.","Software engineering tasks can be divided into many categories, among which generative tasks are the most concern by researchers, where pre-trained models and LLMs possess powerful language representation and contextual awareness capabilities, enabling them to leverage diverse training data and adapt to generative tasks through fine-tuning, transfer learning, and prompt engineering.","These advantages make them effective tools in generative tasks and have demonstrated excellent performance.","In this paper, we present a comprehensive literature review of generative tasks in SE using pre-trained models and LLMs.","We accurately categorize SE generative tasks based on software engineering methodologies and summarize the advanced pre-trained models and LLMs involved, as well as the datasets and evaluation metrics used.","Additionally, we identify key strengths, weaknesses, and gaps in existing approaches, and propose potential research directions.","This review aims to provide researchers and practitioners with an in-depth analysis and guidance on the application of pre-trained models and LLMs in generative tasks within SE."],"url":"http://arxiv.org/abs/2403.02583v1","category":"cs.SE"}
{"created":"2024-03-05 01:32:49","title":"VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing","abstract":"Visual entailment (VE) is a multimodal reasoning task consisting of image-sentence pairs whereby a promise is defined by an image, and a hypothesis is described by a sentence. The goal is to predict whether the image semantically entails the sentence. VE systems have been widely adopted in many downstream tasks. Metamorphic testing is the commonest technique for AI algorithms, but it poses a significant challenge for VE testing. They either only consider perturbations on single modality which would result in ineffective tests due to the destruction of the relationship of image-text pair, or just conduct shallow perturbations on the inputs which can hardly detect the decision error made by VE systems. Motivated by the fact that objects in the image are the fundamental element for reasoning, we propose VEglue, an object-aligned joint erasing approach for VE systems testing. It first aligns the object regions in the premise and object descriptions in the hypothesis to identify linked and un-linked objects. Then, based on the alignment information, three Metamorphic Relations are designed to jointly erase the objects of the two modalities. We evaluate VEglue on four widely-used VE systems involving two public datasets. Results show that VEglue could detect 11,609 issues on average, which is 194%-2,846% more than the baselines. In addition, VEglue could reach 52.5% Issue Finding Rate (IFR) on average, and significantly outperform the baselines by 17.1%-38.2%. Furthermore, we leverage the tests generated by VEglue to retrain the VE systems, which largely improves model performance (50.8% increase in accuracy) on newly generated tests without sacrificing the accuracy on the original test set.","sentences":["Visual entailment (VE) is a multimodal reasoning task consisting of image-sentence pairs whereby a promise is defined by an image, and a hypothesis is described by a sentence.","The goal is to predict whether the image semantically entails the sentence.","VE systems have been widely adopted in many downstream tasks.","Metamorphic testing is the commonest technique for AI algorithms, but it poses a significant challenge for VE testing.","They either only consider perturbations on single modality which would result in ineffective tests due to the destruction of the relationship of image-text pair, or just conduct shallow perturbations on the inputs which can hardly detect the decision error made by VE systems.","Motivated by the fact that objects in the image are the fundamental element for reasoning, we propose VEglue, an object-aligned joint erasing approach for VE systems testing.","It first aligns the object regions in the premise and object descriptions in the hypothesis to identify linked and un-linked objects.","Then, based on the alignment information, three Metamorphic Relations are designed to jointly erase the objects of the two modalities.","We evaluate VEglue on four widely-used VE systems involving two public datasets.","Results show that VEglue could detect 11,609 issues on average, which is 194%-2,846% more than the baselines.","In addition, VEglue could reach 52.5% Issue Finding Rate (IFR) on average, and significantly outperform the baselines by 17.1%-38.2%.","Furthermore, we leverage the tests generated by VEglue to retrain the VE systems, which largely improves model performance (50.8% increase in accuracy) on newly generated tests without sacrificing the accuracy on the original test set."],"url":"http://arxiv.org/abs/2403.02581v1","category":"cs.CV"}
{"created":"2024-03-05 00:37:36","title":"Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas","abstract":"Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies. While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers. Therefore, we conduct a systematic review of 101 recent papers in sign language AI. Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models. We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by what decisions are the most convenient or perceived as important to hearing researchers. We end with a call to action: the field must make space for Deaf researchers to lead the conversation in sign language AI.","sentences":["Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies.","While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers.","Therefore, we conduct a systematic review of 101 recent papers in sign language AI.","Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models.","We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by what decisions are the most convenient or perceived as important to hearing researchers.","We end with a call to action: the field must make space for Deaf researchers to lead the conversation in sign language AI."],"url":"http://arxiv.org/abs/2403.02563v1","category":"cs.CV"}
{"created":"2024-03-05 00:34:05","title":"Semantic Human Mesh Reconstruction with Textures","abstract":"The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods.","sentences":["The field of 3D detailed human mesh reconstruction has made significant progress in recent years.","However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights.","In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details.","SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks.","Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts.","Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information.","The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs.","The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands.","Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.02561v1","category":"cs.CV"}
{"created":"2024-03-05 00:29:08","title":"Regularized Benders Decomposition for High Performance Capacity Expansion Models","abstract":"We consider electricity capacity expansion models, which optimize investment and retirement decisions by minimizing both investment and operation costs. In order to provide credible support for planning and policy decisions, these models need to include detailed operations and time-coupling constraints, and allow modeling of discrete planning decisions. Such requirements result in large-scale mixed integer optimization problems that are intractable with off-the-shelf solvers. Hence, practical solution approaches often rely on carefully designed abstraction techniques to find the best compromise between reduced temporal and spatial resolutions and model accuracy. Benders decomposition methods offer scalable approaches to leverage distributed computing resources and enable models with both high resolution and computational performance. Unfortunately, such algorithms are known to suffer from instabilities, resulting in oscillations between extreme planning decisions that slows convergence. In this study, we implement and evaluate several level-set regularization schemes to avoid the selection of extreme planning decisions. Using a large capacity expansion model of the Continental United States with over 70 million variables as a case study, we find that a regularization scheme that selects planning decisions in the interior of the feasible set shows superior performance compared to previously published methods, enabling high-resolution, mixed-integer planning problems with unprecedented computational performance.","sentences":["We consider electricity capacity expansion models, which optimize investment and retirement decisions by minimizing both investment and operation costs.","In order to provide credible support for planning and policy decisions, these models need to include detailed operations and time-coupling constraints, and allow modeling of discrete planning decisions.","Such requirements result in large-scale mixed integer optimization problems that are intractable with off-the-shelf solvers.","Hence, practical solution approaches often rely on carefully designed abstraction techniques to find the best compromise between reduced temporal and spatial resolutions and model accuracy.","Benders decomposition methods offer scalable approaches to leverage distributed computing resources and enable models with both high resolution and computational performance.","Unfortunately, such algorithms are known to suffer from instabilities, resulting in oscillations between extreme planning decisions that slows convergence.","In this study, we implement and evaluate several level-set regularization schemes to avoid the selection of extreme planning decisions.","Using a large capacity expansion model of the Continental United States with over 70 million variables as a case study, we find that a regularization scheme that selects planning decisions in the interior of the feasible set shows superior performance compared to previously published methods, enabling high-resolution, mixed-integer planning problems with unprecedented computational performance."],"url":"http://arxiv.org/abs/2403.02559v1","category":"math.OC"}
{"created":"2024-03-04 23:54:53","title":"Projection Mapping under Environmental Lighting by Replacing Room Lights with Heterogeneous Projectors","abstract":"Projection mapping (PM) is a technique that enhances the appearance of real-world surfaces using projected images, enabling multiple people to view augmentations simultaneously, thereby facilitating communication and collaboration. However, PM typically requires a dark environment to achieve high-quality projections, limiting its practicality. In this paper, we overcome this limitation by replacing conventional room lighting with heterogeneous projectors. These projectors replicate environmental lighting by selectively illuminating the scene, excluding the projection target. Our contributions include a distributed projector optimization framework designed to effectively replicate environmental lighting and the incorporation of a large-aperture projector, in addition to standard projectors, to reduce high-luminance emitted rays and hard shadows -- undesirable factors for collaborative tasks in PM. We conducted a series of quantitative and qualitative experiments, including user studies, to validate our approach. Our findings demonstrate that our projector-based lighting system significantly enhances the contrast and realism of PM results even under environmental lighting compared to typical lights. Furthermore, our method facilitates a substantial shift in the perceived color mode from the undesirable aperture-color mode, where observers perceive the projected object as self-luminous, to the surface-color mode in PM.","sentences":["Projection mapping (PM) is a technique that enhances the appearance of real-world surfaces using projected images, enabling multiple people to view augmentations simultaneously, thereby facilitating communication and collaboration.","However, PM typically requires a dark environment to achieve high-quality projections, limiting its practicality.","In this paper, we overcome this limitation by replacing conventional room lighting with heterogeneous projectors.","These projectors replicate environmental lighting by selectively illuminating the scene, excluding the projection target.","Our contributions include a distributed projector optimization framework designed to effectively replicate environmental lighting and the incorporation of a large-aperture projector, in addition to standard projectors, to reduce high-luminance emitted rays and hard shadows -- undesirable factors for collaborative tasks in PM.","We conducted a series of quantitative and qualitative experiments, including user studies, to validate our approach.","Our findings demonstrate that our projector-based lighting system significantly enhances the contrast and realism of PM results even under environmental lighting compared to typical lights.","Furthermore, our method facilitates a substantial shift in the perceived color mode from the undesirable aperture-color mode, where observers perceive the projected object as self-luminous, to the surface-color mode in PM."],"url":"http://arxiv.org/abs/2403.02547v1","category":"cs.GR"}
{"created":"2024-03-04 23:46:19","title":"Catch'em all: Classification of Rare, Prominent, and Novel Malware Families","abstract":"National security is threatened by malware, which remains one of the most dangerous and costly cyber threats. As of last year, researchers reported 1.3 billion known malware specimens, motivating the use of data-driven machine learning (ML) methods for analysis. However, shortcomings in existing ML approaches hinder their mass adoption. These challenges include detection of novel malware and the ability to perform malware classification in the face of class imbalance: a situation where malware families are not equally represented in the data. Our work addresses these shortcomings with MalwareDNA: an advanced dimensionality reduction and feature extraction framework. We demonstrate stable task performance under class imbalance for the following tasks: malware family classification and novel malware detection with a trade-off in increased abstention or reject-option rate.","sentences":["National security is threatened by malware, which remains one of the most dangerous and costly cyber threats.","As of last year, researchers reported 1.3 billion known malware specimens, motivating the use of data-driven machine learning (ML) methods for analysis.","However, shortcomings in existing ML approaches hinder their mass adoption.","These challenges include detection of novel malware and the ability to perform malware classification in the face of class imbalance: a situation where malware families are not equally represented in the data.","Our work addresses these shortcomings with MalwareDNA: an advanced dimensionality reduction and feature extraction framework.","We demonstrate stable task performance under class imbalance for the following tasks: malware family classification and novel malware detection with a trade-off in increased abstention or reject-option rate."],"url":"http://arxiv.org/abs/2403.02546v1","category":"cs.CR"}
{"created":"2024-03-04 23:36:05","title":"PDQMA = DQMA = NEXP: QMA With Hidden Variables and Non-collapsing Measurements","abstract":"We define and study a variant of QMA (Quantum Merlin Arthur) in which Arthur can make multiple non-collapsing measurements to Merlin's witness state, in addition to ordinary collapsing measurements. By analogy to the class PDQP defined by Aaronson, Bouland, Fitzsimons, and Lee (2014), we call this class PDQMA. Our main result is that PDQMA = NEXP; this result builds on the MIP = NEXP Theorem and complements the result of Aaronson (2018) that PDQP/qpoly = ALL. While the result has little to do with quantum mechanics, we also show a more \"quantum\" result: namely, that QMA with the ability to inspect the entire history of a hidden variable is equal to NEXP, under mild assumptions on the hidden-variable theory. We also observe that a quantum computer, augmented with quantum advice and the ability to inspect the history of a hidden variable, can solve any decision problem in polynomial time.","sentences":["We define and study a variant of QMA (Quantum Merlin Arthur) in which Arthur can make multiple non-collapsing measurements to Merlin's witness state, in addition to ordinary collapsing measurements.","By analogy to the class PDQP defined by Aaronson, Bouland, Fitzsimons, and Lee (2014), we call this class PDQMA.","Our main result is that PDQMA = NEXP; this result builds on the MIP = NEXP Theorem and complements the result of Aaronson (2018) that PDQP/qpoly = ALL.","While the result has little to do with quantum mechanics, we also show a more \"quantum\" result: namely, that QMA with the ability to inspect the entire history of a hidden variable is equal to NEXP, under mild assumptions on the hidden-variable theory.","We also observe that a quantum computer, augmented with quantum advice and the ability to inspect the history of a hidden variable, can solve any decision problem in polynomial time."],"url":"http://arxiv.org/abs/2403.02543v1","category":"quant-ph"}
{"created":"2024-03-04 23:17:16","title":"Addressing the Influence of Unmeasured Confounding in Observational Studies with Time-to-Event Outcomes: A Semiparametric Sensitivity Analysis Approach","abstract":"In this paper, we develop a semiparametric sensitivity analysis approach designed to address unmeasured confounding in observational studies with time-to-event outcomes. We target estimation of the marginal distributions of potential outcomes under competing exposures using influence function-based techniques. We derived the non-parametric influence function for uncensored data and mapped the uncensored data influence function to the observed data influence function. Our methodology is motivated by and applied to an observational study evaluating the effectiveness of radical prostatectomy (RP) versus external beam radiotherapy with androgen deprivation (EBRT+AD) for the treatment of prostate cancer. We also present a simulation study to evaluate the statistical properties of our methodology.","sentences":["In this paper, we develop a semiparametric sensitivity analysis approach designed to address unmeasured confounding in observational studies with time-to-event outcomes.","We target estimation of the marginal distributions of potential outcomes under competing exposures using influence function-based techniques.","We derived the non-parametric influence function for uncensored data and mapped the uncensored data influence function to the observed data influence function.","Our methodology is motivated by and applied to an observational study evaluating the effectiveness of radical prostatectomy (RP) versus external beam radiotherapy with androgen deprivation (EBRT+AD) for the treatment of prostate cancer.","We also present a simulation study to evaluate the statistical properties of our methodology."],"url":"http://arxiv.org/abs/2403.02539v1","category":"stat.ME"}
{"created":"2024-03-04 23:12:17","title":"Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable Machine Learning","abstract":"Prediction of the Solar Energetic Particle (SEP) events garner increasing interest as space missions extend beyond Earth's protective magnetosphere. These events, which are, in most cases, products of magnetic reconnection-driven processes during solar flares or fast coronal-mass-ejection-driven shock waves, pose significant radiation hazards to aviation, space-based electronics, and particularly, space exploration. In this work, we utilize the recently developed dataset that combines the Solar Dynamics Observatory/Helioseismic and Magnetic Imager's (SDO/HMI) Space weather HMI Active Region Patches (SHARP) and the Solar and Heliospheric Observatory/Michelson Doppler Imager's (SoHO/MDI) Space Weather MDI Active Region Patches (SMARP). We employ a suite of machine learning strategies, including Support Vector Machines (SVM) and regression models, to evaluate the predictive potential of this new data product for a forecast of post-solar flare SEP events. Our study indicates that despite the augmented volume of data, the prediction accuracy reaches 0.7 +- 0.1, which aligns with but does not exceed these published benchmarks. A linear SVM model with training and testing configurations that mimic an operational setting (positive-negative imbalance) reveals a slight increase (+ 0.04 +- 0.05) in the accuracy of a 14-hour SEP forecast compared to previous studies. This outcome emphasizes the imperative for more sophisticated, physics-informed models to better understand the underlying processes leading to SEP events.","sentences":["Prediction of the Solar Energetic Particle (SEP) events garner increasing interest as space missions extend beyond Earth's protective magnetosphere.","These events, which are, in most cases, products of magnetic reconnection-driven processes during solar flares or fast coronal-mass-ejection-driven shock waves, pose significant radiation hazards to aviation, space-based electronics, and particularly, space exploration.","In this work, we utilize the recently developed dataset that combines the Solar Dynamics Observatory/Helioseismic and Magnetic Imager's (SDO/HMI) Space weather HMI Active Region Patches (SHARP) and the Solar and Heliospheric Observatory/Michelson Doppler Imager's (SoHO/MDI) Space Weather MDI Active Region Patches (SMARP).","We employ a suite of machine learning strategies, including Support Vector Machines (SVM) and regression models, to evaluate the predictive potential of this new data product for a forecast of post-solar flare SEP events.","Our study indicates that despite the augmented volume of data, the prediction accuracy reaches 0.7 +- 0.1, which aligns with but does not exceed these published benchmarks.","A linear SVM model with training and testing configurations that mimic an operational setting (positive-negative imbalance) reveals a slight increase (+ 0.04 +- 0.05) in the accuracy of a 14-hour SEP forecast compared to previous studies.","This outcome emphasizes the imperative for more sophisticated, physics-informed models to better understand the underlying processes leading to SEP events."],"url":"http://arxiv.org/abs/2403.02536v1","category":"astro-ph.SR"}
{"created":"2024-03-04 23:03:17","title":"Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?","abstract":"The industry is rich in cases when we are required to make forecasting for large amounts of time series at once. However, we might be in a situation where we can not afford to train a separate model for each of them. Such issue in time series modeling remains without due attention. The remedy for this setting is the establishment of a foundation model. Such a model is expected to work in zero-shot and few-shot regimes. However, what should we take as a training dataset for such kind of model?   Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series. In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples. In this work, we consider the essential question if it is advantageous to train a foundation model on synthetic data or it is better to utilize only a limited number of real-life examples. Our experiments are conducted only for regular time series and speak in favor of leveraging solely the real time series. Moreover, the choice of the proper source dataset strongly influences the performance during inference. When provided access even to a limited quantity of short time series data, employing it within a supervised framework yields more favorable results than training on a larger volume of synthetic data. The code for our experiments is publicly available on Github \\url{https://github.com/sb-ai-lab/synthesize_or_not}.","sentences":["The industry is rich in cases when we are required to make forecasting for large amounts of time series at once.","However, we might be in a situation where we can not afford to train a separate model for each of them.","Such issue in time series modeling remains without due attention.","The remedy for this setting is the establishment of a foundation model.","Such a model is expected to work in zero-shot and few-shot regimes.","However, what should we take as a training dataset for such kind of model?   ","Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series.","In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples.","In this work, we consider the essential question if it is advantageous to train a foundation model on synthetic data or it is better to utilize only a limited number of real-life examples.","Our experiments are conducted only for regular time series and speak in favor of leveraging solely the real time series.","Moreover, the choice of the proper source dataset strongly influences the performance during inference.","When provided access even to a limited quantity of short time series data, employing it within a supervised framework yields more favorable results than training on a larger volume of synthetic data.","The code for our experiments is publicly available on Github \\url{https://github.com/sb-ai-lab/synthesize_or_not}."],"url":"http://arxiv.org/abs/2403.02534v1","category":"cs.LG"}
{"created":"2024-03-04 22:56:26","title":"Superposition detection and QMA with non-collapsing measurements","abstract":"We prove that QMA where the verifier may also make a single non-collapsing measurement is equal to NEXP, resolving an open question of Aaronson. We show this is a corollary to a modified proof of QMA+ = NEXP [arXiv:2306.13247]. At the core of many results inspired by Blier and Tapp [arXiv:0709.0738] is an unphysical property testing problem deciding whether a quantum state is close to an element of a fixed basis.","sentences":["We prove that QMA where the verifier may also make a single non-collapsing measurement is equal to NEXP, resolving an open question of Aaronson.","We show this is a corollary to a modified proof of QMA+ =","NEXP","[arXiv:2306.13247].","At the core of many results inspired by Blier and Tapp [arXiv:0709.0738] is an unphysical property testing problem deciding whether a quantum state is close to an element of a fixed basis."],"url":"http://arxiv.org/abs/2403.02532v1","category":"quant-ph"}
{"created":"2024-03-04 22:10:05","title":"Position operators in terms of converging finite-dimensional matrices: Exploring their interplay with geometry, transport, and gauge theory","abstract":"Position operator $\\hat{r}$ appears as $i{\\partial_p}$ in wave mechanics, while its matrix form is well known diverging in diagonals, causing serious difficulties in basis transformation, observable yielding, etc. We aim to find a convergent $r$-matrix (CRM) to improve the existing divergent $r$-matrix (DRM), and investigate its influence at both the conceptual and the application levels. Unlike the spin matrix, which affords a Lie algebra representation as the solution of $[s_i,s_j]={\\epsilon}_{i,j,k}s_k$, the $r$-matrix cannot be a solution for $[\\hat{r},p]=i\\hbar$, namely Weyl algebra. Indeed: matrix representations of Weyl algebras prove not existing; thus, neither CRM nor DRM would afford a representation. Instead, the CRM should be viewed as a procedure of encoding $\\hat{r}$ using matrices of arbitrary finite dimensions. Deriving CRM recognizes that the limited understanding about Weyl algebra has led to the divergence. A key modification is increasing the 1-st Weyl algebra (the familiar substitution $\\hat{r}{\\rightarrow}i{\\partial_p}$) to the $N$-th Weyl algebra. Resolving the divergence makes $r$-matrix rigorously defined, and we are able to show $r$-matrix is distinct from a spin matrix in terms of its defining principles, transformation behavior, and the observable it yields. At the conceptual level, the CRM fills the logical gap between the $r$-matrix and the Berry connection; and helps to show that Bloch space $\\mathcal{H}_B$ is incomplete for $\\hat{r}$. At the application level, we focus on transport, and discover that the Hermitian matrix is not identical with the associative Hermitian operator, i.e., $r_{m,n}=r_{n,m}^*{\\nLeftrightarrow}\\hat{r}=\\hat{r}^{\\dagger}$. We also discuss how such a non-representation CRM can contribute to building a unified transport theory.","sentences":["Position operator $\\hat{r}$ appears as $i{\\partial_p}$ in wave mechanics, while its matrix form is well known diverging in diagonals, causing serious difficulties in basis transformation, observable yielding, etc.","We aim to find a convergent $r$-matrix (CRM) to improve the existing divergent $r$-matrix (DRM), and investigate its influence at both the conceptual and the application levels.","Unlike the spin matrix, which affords a Lie algebra representation as the solution of $[s_i,s_j]={\\epsilon}_{i,j,k}s_k$, the $r$-matrix cannot be a solution for $[\\hat{r},p]=i\\hbar$, namely Weyl algebra.","Indeed: matrix representations of Weyl algebras prove not existing; thus, neither CRM nor DRM would afford a representation.","Instead, the CRM should be viewed as a procedure of encoding $\\hat{r}$ using matrices of arbitrary finite dimensions.","Deriving CRM recognizes that the limited understanding about Weyl algebra has led to the divergence.","A key modification is increasing the 1-st Weyl algebra (the familiar substitution $\\hat{r}{\\rightarrow}i{\\partial_p}$) to the $N$-th Weyl algebra.","Resolving the divergence makes $r$-matrix rigorously defined, and we are able to show $r$-matrix is distinct from a spin matrix in terms of its defining principles, transformation behavior, and the observable it yields.","At the conceptual level, the CRM fills the logical gap between the $r$-matrix and the Berry connection; and helps to show that Bloch space $\\mathcal{H}_B$ is incomplete for $\\hat{r}$. At the application level, we focus on transport, and discover that the Hermitian matrix is not identical with the associative Hermitian operator, i.e., $r_{m,n}=r_{n,m}^*{\\nLeftrightarrow}\\hat{r}=\\hat{r}^{\\dagger}$. We also discuss how such a non-representation CRM can contribute to building a unified transport theory."],"url":"http://arxiv.org/abs/2403.02519v1","category":"quant-ph"}
{"created":"2024-03-04 22:08:06","title":"Structural origin of relaxation in dense colloidal suspensions","abstract":"Amorphous solids relax via slow molecular rearrangement induced by thermal fluctuations or applied stress. Although microscopic structural signatures predicting these structural relaxations have long been sought, a physically motivated structural measure relevant to diverse systems remains elusive. Here, we introduce a structural order parameter derived from the mean-field caging potential experienced by the particles due to their neighbors, which reliably predicts the occurrence of structural relaxations. The parameter, derived from density functional theory, is a measure of susceptibility to particle rearrangements that can effectively identify weak or defect-like regions in disordered systems. Using experiments on dense colloidal suspensions, we demonstrate a causal relationship between this order parameter and the structural relaxations of the amorphous solid. In quiescent suspensions, increasing the density leads to stronger correlations between the structure and dynamics. Under applied shear, the mean structural order parameter increases with increasing strain, signaling shear-induced softening, which is accompanied by the proliferation of plastic events. In both cases, the order parameter reliably identifies weak regions where the plastic rearrangements due to thermal fluctuation or applied shear preferentially occur. Our study paves the way to a structural understanding of the relaxation of a wide range of amorphous solids, from suspensions to metallic glasses.","sentences":["Amorphous solids relax via slow molecular rearrangement induced by thermal fluctuations or applied stress.","Although microscopic structural signatures predicting these structural relaxations have long been sought, a physically motivated structural measure relevant to diverse systems remains elusive.","Here, we introduce a structural order parameter derived from the mean-field caging potential experienced by the particles due to their neighbors, which reliably predicts the occurrence of structural relaxations.","The parameter, derived from density functional theory, is a measure of susceptibility to particle rearrangements that can effectively identify weak or defect-like regions in disordered systems.","Using experiments on dense colloidal suspensions, we demonstrate a causal relationship between this order parameter and the structural relaxations of the amorphous solid.","In quiescent suspensions, increasing the density leads to stronger correlations between the structure and dynamics.","Under applied shear, the mean structural order parameter increases with increasing strain, signaling shear-induced softening, which is accompanied by the proliferation of plastic events.","In both cases, the order parameter reliably identifies weak regions where the plastic rearrangements due to thermal fluctuation or applied shear preferentially occur.","Our study paves the way to a structural understanding of the relaxation of a wide range of amorphous solids, from suspensions to metallic glasses."],"url":"http://arxiv.org/abs/2403.02517v1","category":"cond-mat.soft"}
{"created":"2024-03-04 21:52:25","title":"Differentially Private Representation Learning via Image Captioning","abstract":"Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For example, under a privacy budget of $\\varepsilon=8$, a linear classifier trained on top of learned DP-Cap features attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA of 56.5%. Our work challenges the prevailing sentiment that high-utility DP representation learning cannot be achieved by training from scratch.","sentences":["Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy.","However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning.","Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features.","In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets.","Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks.","For example, under a privacy budget of $\\varepsilon=8$, a linear classifier trained on top of learned DP-Cap features attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA of 56.5%.","Our work challenges the prevailing sentiment that high-utility DP representation learning cannot be achieved by training from scratch."],"url":"http://arxiv.org/abs/2403.02506v1","category":"cs.CV"}
{"created":"2024-03-04 21:44:49","title":"Unifying uncertainties for rotor-like quantum systems","abstract":"Quantum rotor represents the second simplest model after the harmonic oscillator with profound interdisciplinary but yet unexplored implications. It overreaches its mechanical meaning with significant consequences and promising applications in, e.g., singular optics, super-conductive circuits with Josephson junction or optimal pulse shaping in time frequency domains at ultimate quantum limit. Quantification of complementarity between angular momentum and angular variable is essential for exploitation of this canonical pair in quantum metrology. Whereas the natural measures for position and momentum are variances, the uncertainty associated with angular variable can be linked to moments of inertia of the unit ring about axes passing through its center of mass. This interpretation provides variants for saturable uncertainty relations which can be further used in quantum metrology of the quantum rotor and explains ambiguities in choice of possible uncertainty measures associated with sine and cosine functions of angular variable. Special attention will be payed to uncertainty products which are exactly or approximately minimised by von Mises states, which play the role of squeezed states for quantum rotor and allow optimal detection of quantum states at the ultimate limits.","sentences":["Quantum rotor represents the second simplest model after the harmonic oscillator with profound interdisciplinary but yet unexplored implications.","It overreaches its mechanical meaning with significant consequences and promising applications in, e.g., singular optics, super-conductive circuits with Josephson junction or optimal pulse shaping in time frequency domains at ultimate quantum limit.","Quantification of complementarity between angular momentum and angular variable is essential for exploitation of this canonical pair in quantum metrology.","Whereas the natural measures for position and momentum are variances, the uncertainty associated with angular variable can be linked to moments of inertia of the unit ring about axes passing through its center of mass.","This interpretation provides variants for saturable uncertainty relations which can be further used in quantum metrology of the quantum rotor and explains ambiguities in choice of possible uncertainty measures associated with sine and cosine functions of angular variable.","Special attention will be payed to uncertainty products which are exactly or approximately minimised by von Mises states, which play the role of squeezed states for quantum rotor and allow optimal detection of quantum states at the ultimate limits."],"url":"http://arxiv.org/abs/2403.02498v1","category":"quant-ph"}
{"created":"2024-03-04 21:31:26","title":"The class of Gorenstein injective modules is covering if and only if it is closed under direct limits","abstract":"We prove that the class of Gorenstein injective modules is covering if and only if it is closed under direct limits. This adds to the list of examples that support Enochs conjecture: Every covering class of modules is closed under direct limits.   We also show that the class of Gorenstein injective left R-modules is covering if and only if R is left noetherian, and such that character modules of Gorenstein injective left R modules are Gorenstein flat.","sentences":["We prove that the class of Gorenstein injective modules is covering if and only if it is closed under direct limits.","This adds to the list of examples that support Enochs conjecture: Every covering class of modules is closed under direct limits.   ","We also show that the class of Gorenstein injective left R-modules is covering if and only if R is left noetherian, and such that character modules of Gorenstein injective left R modules are Gorenstein flat."],"url":"http://arxiv.org/abs/2403.02493v1","category":"math.AC"}
{"created":"2024-03-04 21:27:47","title":"Two results on complexities of decision problems of groups","abstract":"We answer two questions on the complexities of decision problems of groups, each related to a classical result. First, C. Miller characterized the complexity of the isomorphism problem for finitely presented groups in 1971. We do the same for the isomorphism problem for recursively presented groups. Second, the fact that every Turing degree appears as the degree of the word problem of a finitely presented group is shown independently by multiple people in the 1960s. We answer the analogous question for degrees of ceers instead of Turing degrees. We show that the set of ceers which are computably equivalent to a finitely presented group is $\\Sigma^0_3$-complete, which is the maximal possible complexity.","sentences":["We answer two questions on the complexities of decision problems of groups, each related to a classical result.","First, C. Miller characterized the complexity of the isomorphism problem for finitely presented groups in 1971.","We do the same for the isomorphism problem for recursively presented groups.","Second, the fact that every Turing degree appears as the degree of the word problem of a finitely presented group is shown independently by multiple people in the 1960s.","We answer the analogous question for degrees of ceers instead of Turing degrees.","We show that the set of ceers which are computably equivalent to a finitely presented group is $\\Sigma^0_3$-complete, which is the maximal possible complexity."],"url":"http://arxiv.org/abs/2403.02492v1","category":"math.LO"}
{"created":"2024-03-04 21:16:54","title":"Torsion-free abelian groups of finite rank and fields of finite transcendence degree","abstract":"Let $\\operatorname{TFAb}_r$ be the class of torsion-free abelian groups of rank $r$, and let $\\operatorname{FD}_r$ be the class of fields of characteristic $0$ and transcendence degree~$r$. We compare these classes using various notions. Considering Scott complexity of the structures in the classes and the complexity of the isomorphism relations on the classes, the classes seem very similar. Hjorth and Thomas showed that the $\\operatorname{TFAb}_r$ are strictly increasing under Borel reducibility. This is not so for the classes $\\operatorname{FD}_r$. Thomas and Velickovic showed that for sufficiently large $r$, the classes $\\operatorname{FD}_r$ are equivalent under Borel reducibility. We try to compare the groups with the fields, using Borel reducibility, and also using some effective variants. We give functorial Turing computable embeddings of $\\operatorname{TFAb}_r$ in $\\operatorname{FD}_r$, and of $\\operatorname{FD}_r$ in $\\operatorname{FD}_{r+1}$. We show that under computable countable reducibility, $\\operatorname{TFAb}_1$ lies on top among the classes we are considering. In fact, under computable countable reducibility, isomorphism on $\\operatorname{TFAb}_1$ lies on top among equivalence relations that are effective $\\Sigma_3$, along with the Vitali equivalence relation on $2^\\omega$.","sentences":["Let $\\operatorname{TFAb}_r$ be the class of torsion-free abelian groups of rank $r$, and let $\\operatorname{FD}_r$ be the class of fields of characteristic $0$ and transcendence degree~$r$. We compare these classes using various notions.","Considering Scott complexity of the structures in the classes and the complexity of the isomorphism relations on the classes, the classes seem very similar.","Hjorth and Thomas showed that the $\\operatorname{TFAb}_r$ are strictly increasing under Borel reducibility.","This is not so for the classes $\\operatorname{FD}_r$. Thomas and Velickovic showed that for sufficiently large $r$, the classes $\\operatorname{FD}_r$ are equivalent under Borel reducibility.","We try to compare the groups with the fields, using Borel reducibility, and also using some effective variants.","We give functorial Turing computable embeddings of $\\operatorname{TFAb}_r$ in $\\operatorname{FD}_r$, and of $\\operatorname{FD}_r$ in $\\operatorname{FD}_{r+1}$. We show that under computable countable reducibility, $\\operatorname{TFAb}_1$ lies on top among the classes we are considering.","In fact, under computable countable reducibility, isomorphism on $\\operatorname{TFAb}_1$ lies on top among equivalence relations that are effective $\\Sigma_3$, along with the Vitali equivalence relation on $2^\\omega$."],"url":"http://arxiv.org/abs/2403.02488v1","category":"math.LO"}
{"created":"2024-03-04 21:14:25","title":"Absorbing-state transitions in particulate systems under spatially varying driving","abstract":"We study the absorbing state transition in particulate systems under spatially inhomogeneous driving using a modified random organization model. For smoothly varying driving the steady state results map onto the homogeneous absorbing state phase diagram, with the position of the boundary between absorbing and diffusive states being insensitive to the driving wavelength. Here the phenomenology is well-described by a one-dimensional continuum model that we pose. For discontinuously varying driving the position of the absorbing phase boundary and the exponent characterising the fraction of active particles are altered relative to the homogeneous case.","sentences":["We study the absorbing state transition in particulate systems under spatially inhomogeneous driving using a modified random organization model.","For smoothly varying driving the steady state results map onto the homogeneous absorbing state phase diagram, with the position of the boundary between absorbing and diffusive states being insensitive to the driving wavelength.","Here the phenomenology is well-described by a one-dimensional continuum model that we pose.","For discontinuously varying driving the position of the absorbing phase boundary and the exponent characterising the fraction of active particles are altered relative to the homogeneous case."],"url":"http://arxiv.org/abs/2403.02487v1","category":"cond-mat.soft"}
{"created":"2024-03-04 21:08:25","title":"Demonstrating a Robust Walking Algorithm for Underactuated Bipedal Robots in Non-flat, Non-stationary Environments","abstract":"This work explores an innovative algorithm designed to enhance the mobility of underactuated bipedal robots across challenging terrains, especially when navigating through spaces with constrained opportunities for foot support, like steps or stairs. By combining ankle torque with a refined angular momentum-based linear inverted pendulum model (ALIP), our method allows variability in the robot's center of mass height. We employ a dual-strategy controller that merges virtual constraints for precise motion regulation across essential degrees of freedom with an ALIP-centric model predictive control (MPC) framework, aimed at enforcing gait stability. The effectiveness of our feedback design is demonstrated through its application on the Cassie bipedal robot, which features 20 degrees of freedom. Key to our implementation is the development of tailored nominal trajectories and an optimized MPC that reduces the execution time to under 500 microseconds--and, hence, is compatible with Cassie's controller update frequency. This paper not only showcases the successful hardware deployment but also demonstrates a new capability, a bipedal robot using a moving walkway.","sentences":["This work explores an innovative algorithm designed to enhance the mobility of underactuated bipedal robots across challenging terrains, especially when navigating through spaces with constrained opportunities for foot support, like steps or stairs.","By combining ankle torque with a refined angular momentum-based linear inverted pendulum model (ALIP), our method allows variability in the robot's center of mass height.","We employ a dual-strategy controller that merges virtual constraints for precise motion regulation across essential degrees of freedom with an ALIP-centric model predictive control (MPC) framework, aimed at enforcing gait stability.","The effectiveness of our feedback design is demonstrated through its application on the Cassie bipedal robot, which features 20 degrees of freedom.","Key to our implementation is the development of tailored nominal trajectories and an optimized MPC that reduces the execution time to under 500 microseconds--and, hence, is compatible with Cassie's controller update frequency.","This paper not only showcases the successful hardware deployment but also demonstrates a new capability, a bipedal robot using a moving walkway."],"url":"http://arxiv.org/abs/2403.02486v1","category":"cs.RO"}
{"created":"2024-03-04 20:40:02","title":"A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation","abstract":"We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \\textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads to an overall approach that considerably simplifies existing proofs. We conjecture that our inductive proof technique will find applications in the analyses of more complex stochastic approximation algorithms, and conclude by providing some examples of such applications.","sentences":["We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling.","Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates.","We ask: \\textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?}","Our main contribution is to show this is possible via a novel two-step argument.","In the first step, we use induction to prove that under a standard choice of a constant step-size $\\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation.","In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\\alpha^2)$ that captures the effect of Markovian sampling.","Combining these pieces leads to an overall approach that considerably simplifies existing proofs.","We conjecture that our inductive proof technique will find applications in the analyses of more complex stochastic approximation algorithms, and conclude by providing some examples of such applications."],"url":"http://arxiv.org/abs/2403.02476v1","category":"cs.LG"}
{"created":"2024-03-04 20:39:24","title":"Enhancing LLM Safety via Constrained Direct Preference Optimization","abstract":"The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to LLMs that is missing in DPO while achieving significantly higher rewards under the same safety constraint compared to a recently proposed safe RLHF approach.   Warning: This paper contains example data that may be offensive or harmful.","sentences":["The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals.","To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework.","This approach, however, is computationally expensive and often unstable.","In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight.","By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning.","Empirically, our approach provides a safety guarantee to LLMs that is missing in DPO while achieving significantly higher rewards under the same safety constraint compared to a recently proposed safe RLHF approach.   ","Warning:","This paper contains example data that may be offensive or harmful."],"url":"http://arxiv.org/abs/2403.02475v1","category":"cs.LG"}
{"created":"2024-03-04 20:28:28","title":"Cosmic-ray diffusion in two local filamentary clouds","abstract":"A fairly uniform cosmic-ray (CR) distribution is observed near the Sun, except in the nearby Eridu cloud, which shows an unexplained 30-50% deficit in GeV to TeV CR flux. To explore the origin of this deficit, we studied the Reticulum cloud, which shares notable traits with Eridu: a comparable distance in the low-density region of the Local Valley and a filamentary structure of atomic hydrogen extending along ordered magnetic-field lines that are steeply inclined to the Galactic plane. Using 14 years of Fermi-LAT data in the 0.16 to 63 GeV energy band, we found that the gamma-ray emissivity in the Reticulum cloud is fully consistent with the average spectrum measured in the solar neighbourhood, but this emissivity, and therefore the CR flux, is 1.57 $\\pm$ 0.09 times larger than in Eridu across the whole energy band. The difference cannot be attributed to uncertainties in gas mass. Nevertheless, we find that the two clouds are similar in many respects at a parsec scale: both have magnetic-field strengths of a few micro-Gauss in the plane of the sky; both are in approximate equilibrium between magnetic and thermal pressures; they have similar turbulent velocities and sonic Mach numbers; and both show magnetic-field regularity with a dispersion in orientation lower than 10-15 degrees over large zones. The gas in Reticulum is colder and denser than in Eridu, but we find similar parallel diffusion coefficients around a few times 1e28 cm2/s in both clouds if CRs above 1 GV in rigidity diffuse on resonant, self-excited Alfv\\'en waves that are damped by ion-neutral interactions. The loss of CRs in Eridu remains unexplained, but these two clouds provide important test cases to further study how magnetic turbulence, line tangling, and ion-neutral damping regulate CR diffusion in the dominant gas phase of the interstellar medium.","sentences":["A fairly uniform cosmic-ray (CR) distribution is observed near the Sun, except in the nearby Eridu cloud, which shows an unexplained 30-50% deficit in GeV to TeV CR flux.","To explore the origin of this deficit, we studied the Reticulum cloud, which shares notable traits with Eridu: a comparable distance in the low-density region of the Local Valley and a filamentary structure of atomic hydrogen extending along ordered magnetic-field lines that are steeply inclined to the Galactic plane.","Using 14 years of Fermi-LAT data in the 0.16 to 63 GeV energy band, we found that the gamma-ray emissivity in the Reticulum cloud is fully consistent with the average spectrum measured in the solar neighbourhood, but this emissivity, and therefore the CR flux, is 1.57 $\\pm$ 0.09 times larger than in Eridu across the whole energy band.","The difference cannot be attributed to uncertainties in gas mass.","Nevertheless, we find that the two clouds are similar in many respects at a parsec scale: both have magnetic-field strengths of a few micro-Gauss in the plane of the sky; both are in approximate equilibrium between magnetic and thermal pressures; they have similar turbulent velocities and sonic Mach numbers; and both show magnetic-field regularity with a dispersion in orientation lower than 10-15 degrees over large zones.","The gas in Reticulum is colder and denser than in Eridu, but we find similar parallel diffusion coefficients around a few times 1e28 cm2/s in both clouds if CRs above 1 GV in rigidity diffuse on resonant, self-excited Alfv\\'en waves that are damped by ion-neutral interactions.","The loss of CRs in Eridu remains unexplained, but these two clouds provide important test cases to further study how magnetic turbulence, line tangling, and ion-neutral damping regulate CR diffusion in the dominant gas phase of the interstellar medium."],"url":"http://arxiv.org/abs/2403.02466v1","category":"astro-ph.HE"}
{"created":"2024-03-04 20:24:58","title":"Tuning charge density wave of kagome metal ScV6Sn6","abstract":"Compounds with a kagome lattice exhibit intriguing properties and the charge density wave (CDW) adds an additional layer of interest to research on them. In this study, we investigate the temperature and magnetic field dependent electrical properties under a chemical substitution and hydrostatic pressure of ScV6Sn6, a non-magnetic charge density wave (CDW) compound. Substituting 5 % Cr at the V site or applying 1.5 GPa of pressure shifts the CDW to 50 K from 92 K. This shift is attributed to the movement of the imaginary phonon band, as revealed by the phonon dispersion relation. The longitudinal and Hall resistivities respond differently under these stimuli. The magnetoresistance (MR) maintains its quasilinear behavior under pressure, but it becomes quadratic after Cr substitution. The anomalous Hall-like behavior of the parent compound persists up to the respective CDW transition under pressure, after which it sharply declines. In contrast, the longitudinal and Hall resistivities of Cr substituted compounds follow a two-band model and originates from the multi carrier effect. These results clearly highlight the role of phonon contributions in the CDW transition and call for further investigation into the origin of the anomalous Hall-like behavior in the parent compound.","sentences":["Compounds with a kagome lattice exhibit intriguing properties and the charge density wave (CDW) adds an additional layer of interest to research on them.","In this study, we investigate the temperature and magnetic field dependent electrical properties under a chemical substitution and hydrostatic pressure of ScV6Sn6, a non-magnetic charge density wave (CDW) compound.","Substituting 5 % Cr at the V site or applying 1.5 GPa of pressure shifts the CDW to 50 K from 92 K. This shift is attributed to the movement of the imaginary phonon band, as revealed by the phonon dispersion relation.","The longitudinal and Hall resistivities respond differently under these stimuli.","The magnetoresistance (MR) maintains its quasilinear behavior under pressure, but it becomes quadratic after Cr substitution.","The anomalous Hall-like behavior of the parent compound persists up to the respective CDW transition under pressure, after which it sharply declines.","In contrast, the longitudinal and Hall resistivities of Cr substituted compounds follow a two-band model and originates from the multi carrier effect.","These results clearly highlight the role of phonon contributions in the CDW transition and call for further investigation into the origin of the anomalous Hall-like behavior in the parent compound."],"url":"http://arxiv.org/abs/2403.02463v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-04 20:23:43","title":"Edge states for tight-binding operators with soft walls","abstract":"We study one- and two-dimensional periodic tight-binding models under the presence of a potential that grows to infinity in one direction, hence preventing the particles to escape in this direction (the soft wall). We prove that a spectral flow appears in these corresponding edge models, as the wall is shifted. We identity this flow as a number of Bloch bands, and provide a lower bound for the number of edge states appearing in such models.","sentences":["We study one-","and two-dimensional periodic tight-binding models under the presence of a potential that grows to infinity in one direction, hence preventing the particles to escape in this direction (the soft wall).","We prove that a spectral flow appears in these corresponding edge models, as the wall is shifted.","We identity this flow as a number of Bloch bands, and provide a lower bound for the number of edge states appearing in such models."],"url":"http://arxiv.org/abs/2403.02462v1","category":"math-ph"}
{"created":"2024-03-04 20:18:19","title":"Saturated Partial Embeddings of Planar Graphs","abstract":"In this work, we study how far one can deviate from optimal behavior when embedding a planar graph. For a planar graph $G$, we say that a plane subgraph $H\\subseteq G$ is a \\textit{plane-saturated subgraph} if adding any edge (possibly with new vertices) to $H$ would either violate planarity or make the resulting graph no longer a subgraph of $G$. For a planar graph $G$, we define the \\textit{plane-saturation ratio}, $\\psr(G)$, as the minimum value of $\\frac{e(H)}{e(G)}$ for a plane-saturated subgraph $H$ of $G$ and investigate how small $\\psr(G)$ can be. While there exist planar graphs where $\\psr(G)$ is arbitrarily close to $0$, we show that for all twin-free planar graphs, $\\psr(G)>1/16$, and that there exist twin-free planar graphs where $\\psr(G)$ is arbitrarily close to $1/16$. In fact, we study a broader category of planar graphs, focusing on classes characterized by a bounded number of degree $1$ and degree $2$ twin vertices. We offer solutions for some instances of bounds while positing conjectures for the remaining ones.","sentences":["In this work, we study how far one can deviate from optimal behavior when embedding a planar graph.","For a planar graph $G$, we say that a plane subgraph $H\\subseteq G$ is a \\textit{plane-saturated subgraph} if adding any edge (possibly with new vertices) to $H$ would either violate planarity or make the resulting graph no longer a subgraph of $G$. For a planar graph $G$, we define the \\textit{plane-saturation ratio}, $\\psr(G)$, as the minimum value of $\\frac{e(H)}{e(G)}$ for a plane-saturated subgraph $H$ of $G$ and investigate how small $\\psr(G)$ can be.","While there exist planar graphs where $\\psr(G)$ is arbitrarily close to $0$, we show that for all twin-free planar graphs, $\\psr(G)>1/16$, and that there exist twin-free planar graphs where $\\psr(G)$ is arbitrarily close to $1/16$. In fact, we study a broader category of planar graphs, focusing on classes characterized by a bounded number of degree $1$ and degree $2$ twin vertices.","We offer solutions for some instances of bounds while positing conjectures for the remaining ones."],"url":"http://arxiv.org/abs/2403.02458v1","category":"math.CO"}
{"created":"2024-03-04 19:59:32","title":"On Latency Predictors for Neural Architecture Search","abstract":"Efficient deployment of neural networks (NN) requires the co-optimization of accuracy and latency. For example, hardware-aware neural architecture search has been used to automatically find NN architectures that satisfy a latency constraint on a specific hardware device. Central to these search algorithms is a prediction model that is designed to provide a hardware latency estimate for a candidate NN architecture. Recent research has shown that the sample efficiency of these predictive models can be greatly improved through pre-training on some \\textit{training} devices with many samples, and then transferring the predictor on the \\textit{test} (target) device. Transfer learning and meta-learning methods have been used for this, but often exhibit significant performance variability. Additionally, the evaluation of existing latency predictors has been largely done on hand-crafted training/test device sets, making it difficult to ascertain design features that compose a robust and general latency predictor. To address these issues, we introduce a comprehensive suite of latency prediction tasks obtained in a principled way through automated partitioning of hardware device sets. We then design a general latency predictor to comprehensively study (1) the predictor architecture, (2) NN sample selection methods, (3) hardware device representations, and (4) NN operation encoding schemes. Building on conclusions from our study, we present an end-to-end latency predictor training strategy that outperforms existing methods on 11 out of 12 difficult latency prediction tasks, improving latency prediction by 22.5\\% on average, and up to to 87.6\\% on the hardest tasks. Focusing on latency prediction, our HW-Aware NAS reports a $5.8\\times$ speedup in wall-clock time. Our code is available on \\href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat\\_latency}.","sentences":["Efficient deployment of neural networks (NN) requires the co-optimization of accuracy and latency.","For example, hardware-aware neural architecture search has been used to automatically find NN architectures that satisfy a latency constraint on a specific hardware device.","Central to these search algorithms is a prediction model that is designed to provide a hardware latency estimate for a candidate NN architecture.","Recent research has shown that the sample efficiency of these predictive models can be greatly improved through pre-training on some \\textit{training} devices with many samples, and then transferring the predictor on the \\textit{test} (target) device.","Transfer learning and meta-learning methods have been used for this, but often exhibit significant performance variability.","Additionally, the evaluation of existing latency predictors has been largely done on hand-crafted training/test device sets, making it difficult to ascertain design features that compose a robust and general latency predictor.","To address these issues, we introduce a comprehensive suite of latency prediction tasks obtained in a principled way through automated partitioning of hardware device sets.","We then design a general latency predictor to comprehensively study (1) the predictor architecture, (2) NN sample selection methods, (3) hardware device representations, and (4) NN operation encoding schemes.","Building on conclusions from our study, we present an end-to-end latency predictor training strategy that outperforms existing methods on 11 out of 12 difficult latency prediction tasks, improving latency prediction by 22.5\\% on average, and up to to 87.6\\% on the hardest tasks.","Focusing on latency prediction, our HW-Aware NAS reports a $5.8\\times$ speedup in wall-clock time.","Our code is available on \\href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat\\_latency}."],"url":"http://arxiv.org/abs/2403.02446v1","category":"cs.LG"}
{"created":"2024-03-04 19:42:49","title":"The vertical structure of galactic discs: nonlocal gravity versus dark matter","abstract":"Recent isolated galactic simulations show that the morphology of galactic discs in modified gravity differs from that of the standard dark matter model. In this study, we focused on the vertical structure of galactic discs and compared the bending instability in the vertical direction for both paradigms. To achieve this, we utilized high-resolution N-body simulations to construct two models in a specific nonlocal gravity theory (NLG) and the standard dark matter model and compared their stability against the bending perturbations. Our numerical results demonstrate that the outer regions of the disc are more susceptible to the instability in NLG, whereas the disc embedded in the dark matter halo is more unstable in the central regions. We then interpret these results based on the dispersion relation of the bending waves. To do so, we presented an analytical study to derive the dispersion relation in NLG. Our numerical results align with the predictions of our analytical models. Consequently, we conclude that the analysis of bending instability in galactic discs offers an explanation for the distinct vertical structures observed in simulated galactic discs under these two theories. These findings represent a significant step towards distinguishing between the modified gravity and dark matter models.","sentences":["Recent isolated galactic simulations show that the morphology of galactic discs in modified gravity differs from that of the standard dark matter model.","In this study, we focused on the vertical structure of galactic discs and compared the bending instability in the vertical direction for both paradigms.","To achieve this, we utilized high-resolution N-body simulations to construct two models in a specific nonlocal gravity theory (NLG) and the standard dark matter model and compared their stability against the bending perturbations.","Our numerical results demonstrate that the outer regions of the disc are more susceptible to the instability in NLG, whereas the disc embedded in the dark matter halo is more unstable in the central regions.","We then interpret these results based on the dispersion relation of the bending waves.","To do so, we presented an analytical study to derive the dispersion relation in NLG.","Our numerical results align with the predictions of our analytical models.","Consequently, we conclude that the analysis of bending instability in galactic discs offers an explanation for the distinct vertical structures observed in simulated galactic discs under these two theories.","These findings represent a significant step towards distinguishing between the modified gravity and dark matter models."],"url":"http://arxiv.org/abs/2403.02441v1","category":"astro-ph.GA"}
{"created":"2024-03-04 19:26:39","title":"On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation","abstract":"We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.","sentences":["We study a new technique for understanding convergence of learning agents under small modifications of data.","We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence.","We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning."],"url":"http://arxiv.org/abs/2403.02432v1","category":"stat.ML"}
{"created":"2024-03-04 19:12:55","title":"Dr Wenowdis: Specializing dynamic language C extensions using type information","abstract":"C-based interpreters such as CPython make extensive use of C \"extension\" code, which is opaque to static analysis tools and faster runtimes with JIT compilers, such as PyPy. Not only are the extensions opaque, but the interface between the dynamic language types and the C types can introduce impedance. We hypothesise that frequent calls to C extension code introduce significant overhead that is often unnecessary. We validate this hypothesis by introducing a simple technique, \"typed methods\", which allow selected C extension functions to have additional metadata attached to them in a backward-compatible way. This additional metadata makes it much easier for a JIT compiler (and as we show, even an interpreter!) to significantly reduce the call and return overhead. Although we have prototyped typed methods in PyPy, we suspect that the same technique is applicable to a wider variety of language runtimes and that the information can also be consumed by static analysis tooling.","sentences":["C-based interpreters such as CPython make extensive use of C \"extension\" code, which is opaque to static analysis tools and faster runtimes with JIT compilers, such as PyPy.","Not only are the extensions opaque, but the interface between the dynamic language types and the C types can introduce impedance.","We hypothesise that frequent calls to C extension code introduce significant overhead that is often unnecessary.","We validate this hypothesis by introducing a simple technique, \"typed methods\", which allow selected C extension functions to have additional metadata attached to them in a backward-compatible way.","This additional metadata makes it much easier for a JIT compiler (and as we show, even an interpreter!)","to significantly reduce the call and return overhead.","Although we have prototyped typed methods in PyPy, we suspect that the same technique is applicable to a wider variety of language runtimes and that the information can also be consumed by static analysis tooling."],"url":"http://arxiv.org/abs/2403.02420v1","category":"cs.PL"}
{"created":"2024-03-04 19:09:46","title":"Regularity of semi-valuation rings and homotopy invariance of algebraic K-theory","abstract":"We show that the algebraic K-theory of semi-valuation rings with stably coherent regular semi-fraction ring satisfies homotopy invariance. Moreover, we show that these rings are regular if their valuation is non-trivial. Thus they yield examples of regular rings which are not homotopy invariant for algebraic K-theory. On the other hand, they are not necessarily coherent, so that they provide a class of possibly non-coherent examples for homotopy invariance of algebraic K-theory. As an application, we show that Temkin's relative Riemann-Zariski spaces also satisfy homotopy invariance for K-theory under some finiteness assumption.","sentences":["We show that the algebraic K-theory of semi-valuation rings with stably coherent regular semi-fraction ring satisfies homotopy invariance.","Moreover, we show that these rings are regular if their valuation is non-trivial.","Thus they yield examples of regular rings which are not homotopy invariant for algebraic K-theory.","On the other hand, they are not necessarily coherent, so that they provide a class of possibly non-coherent examples for homotopy invariance of algebraic K-theory.","As an application, we show that Temkin's relative Riemann-Zariski spaces also satisfy homotopy invariance for K-theory under some finiteness assumption."],"url":"http://arxiv.org/abs/2403.02413v1","category":"math.KT"}
{"created":"2024-03-04 19:06:13","title":"A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement","abstract":"Distortions caused by low-light conditions are not only visually unpleasant but also degrade the performance of computer vision tasks. The restoration and enhancement have proven to be highly beneficial. However, there are only a limited number of enhancement methods explicitly designed for videos acquired in low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet) model using a Swin Transformer as a backbone to capture low light video features and exploit their spatio-temporal correlations. The STA-SUNet model is trained on a novel, fully registered dataset (BVI), which comprises dynamic scenes captured under varying light conditions. It is further analysed comparatively against various other models over three test datasets. The model demonstrates superior adaptivity across all datasets, obtaining the highest PSNR and SSIM values. It is particularly effective in extreme low-light conditions, yielding fairly good visualisation results.","sentences":["Distortions caused by low-light conditions are not only visually unpleasant but also degrade the performance of computer vision tasks.","The restoration and enhancement have proven to be highly beneficial.","However, there are only a limited number of enhancement methods explicitly designed for videos acquired in low-light conditions.","We propose a Spatio-Temporal Aligned SUNet (STA-SUNet) model using a Swin Transformer as a backbone to capture low light video features and exploit their spatio-temporal correlations.","The STA-SUNet model is trained on a novel, fully registered dataset (BVI), which comprises dynamic scenes captured under varying light conditions.","It is further analysed comparatively against various other models over three test datasets.","The model demonstrates superior adaptivity across all datasets, obtaining the highest PSNR and SSIM values.","It is particularly effective in extreme low-light conditions, yielding fairly good visualisation results."],"url":"http://arxiv.org/abs/2403.02408v1","category":"eess.IV"}
{"created":"2024-03-04 19:00:07","title":"The true number density of massive galaxies in the early Universe revealed by JWST/MIRI","abstract":"One of the main challenges in galaxy formation that has emerged recently is the early assembly of massive galaxies. The observed number density and the maximum stellar mass ($M_{\\star}$) of massive galaxies in the early Universe appear to be higher than model predictions, which may pose a serious problem to the LCDM cosmology. A major limitation in many previous studies is the large uncertainty in estimating $M_{\\star}$ due to the lack of constraints in the rest-frame near-infrared part of the spectral energy distribution, which is critical to determining $M_{\\star}$ accurately. Here we use data from a large JWST/MIRI survey in the PRIMER program to carry out a systematic analysis of massive galaxies at $z \\sim 3-8$, leveraging photometric constraints at rest-frame $\\gtrsim 1 \\mu$m. We find a significant reduction in the number and mass densities of massive galaxies at $z > 5$ compared to earlier results that did not use the MIRI photometry. Within the standard $\\Lambda$CDM cosmology, our results require a moderate increase in the baryon-to-star conversion efficiency ($\\epsilon$) towards higher redshifts and higher $M_{\\star}$. For the most massive galaxies at $z\\sim 8$, the required $\\epsilon$ is $\\sim 0.3$, in comparison to $\\epsilon \\sim 0.14$ for typical low-redshift galaxies. Our findings are consistent with models assuming suppressed stellar feedback due to the high gas density and the associated short free-fall time expected for massive halos at high redshift.","sentences":["One of the main challenges in galaxy formation that has emerged recently is the early assembly of massive galaxies.","The observed number density and the maximum stellar mass ($M_{\\star}$) of massive galaxies in the early Universe appear to be higher than model predictions, which may pose a serious problem to the LCDM cosmology.","A major limitation in many previous studies is the large uncertainty in estimating $M_{\\star}$ due to the lack of constraints in the rest-frame near-infrared part of the spectral energy distribution, which is critical to determining $M_{\\star}$ accurately.","Here we use data from a large JWST/MIRI survey in the PRIMER program to carry out a systematic analysis of massive galaxies at $z \\sim 3-8$, leveraging photometric constraints at rest-frame $\\gtrsim 1 \\mu$m.","We find a significant reduction in the number and mass densities of massive galaxies at $z > 5$ compared to earlier results that did not use the MIRI photometry.","Within the standard $\\Lambda$CDM cosmology, our results require a moderate increase in the baryon-to-star conversion efficiency ($\\epsilon$) towards higher redshifts and higher $M_{\\star}$. For the most massive galaxies at $z\\sim 8$, the required $\\epsilon$ is $\\sim 0.3$, in comparison to $\\epsilon \\sim 0.14$ for typical low-redshift galaxies.","Our findings are consistent with models assuming suppressed stellar feedback due to the high gas density and the associated short free-fall time expected for massive halos at high redshift."],"url":"http://arxiv.org/abs/2403.02399v1","category":"astro-ph.GA"}
{"created":"2024-03-05 18:59:51","title":"FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation","abstract":"Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.","sentences":["Estimating relative camera poses between images has been a central problem in computer vision.","Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases.","Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision.","We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales.","At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver.","A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization."],"url":"http://arxiv.org/abs/2403.03221v1","category":"cs.CV"}
{"created":"2024-03-05 17:41:35","title":"Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks","abstract":"Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods. The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks. Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios.","sentences":["Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data.","While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   ","In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks.","In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives.","A client's model update is considered malicious if it significantly deviates from the computed median update.","We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods.","The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks.","Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios."],"url":"http://arxiv.org/abs/2403.03149v1","category":"cs.CR"}
{"created":"2024-03-05 16:28:48","title":"Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization","abstract":"Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues. In our work, we focus on semi-supervised AVSL with pseudo-labeling. To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation. We equip XPL with two effective components. Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training. Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias. Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability.","sentences":["Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues.","In our work, we focus on semi-supervised AVSL with pseudo-labeling.","To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation.","We equip XPL with two effective components.","Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training.","Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias.","Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability."],"url":"http://arxiv.org/abs/2403.03095v1","category":"cs.CV"}
{"created":"2024-03-05 15:28:24","title":"Adding Multimodal Capabilities to a Text-only Translation Model","abstract":"While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.","sentences":["While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree.","Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets.","In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model.","We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k."],"url":"http://arxiv.org/abs/2403.03045v1","category":"cs.CL"}
{"created":"2024-03-05 15:12:25","title":"Global dissipative martingale solutions to the variational wave equation with stochastic forcing","abstract":"We consider the variational wave equation in one-dimensional space with stochastic forcing by an additive noise. Blow-up of local smooth solutions is established, and global existence is proved in the class of weak martingale solutions.","sentences":["We consider the variational wave equation in one-dimensional space with stochastic forcing by an additive noise.","Blow-up of local smooth solutions is established, and global existence is proved in the class of weak martingale solutions."],"url":"http://arxiv.org/abs/2403.03034v1","category":"math.AP"}
{"created":"2024-03-05 14:31:24","title":"Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models","abstract":"Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition. Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming. Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also greatly reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference speed than LLaVA-1.5. Source codes are released at: https://github.com/luogen1996/LLaVA-HR.","sentences":["Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition.","Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming.","Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA).","In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters).","This design also greatly reduces the input sequence length of MLLMs.","To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR.","We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA.","More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference speed than LLaVA-1.5.","Source codes are released at: https://github.com/luogen1996/LLaVA-HR."],"url":"http://arxiv.org/abs/2403.03003v1","category":"cs.CV"}
{"created":"2024-03-05 14:16:52","title":"Adaptive Integrate-and-Fire Time Encoding Machine","abstract":"An integrate-and-fire time-encoding machine (IF-TEM) is an effective asynchronous sampler that translates amplitude information into non-uniform time sequences. In this work, we propose a novel Adaptive IF-TEM (AIF-TEM) approach. This design dynamically adjusts the TEM's sensitivity to changes in the input signal's amplitude and frequency in real-time. We provide a comprehensive analysis of AIF-TEM's oversampling and distortion properties. By the adaptive adjustments, AIF-TEM as we show can achieve significant performance improvements in practical finite regime, in terms of sampling rate-distortion. We demonstrate empirically that in the scenarios tested AIF-TEM outperforms classical IF-TEM and traditional Nyquist (i.e., periodic) sampling methods for band-limited signals. In terms of Mean Square Error (MSE), the reduction reaches at least 12dB (fixing the oversampling rate).","sentences":["An integrate-and-fire time-encoding machine (IF-TEM) is an effective asynchronous sampler that translates amplitude information into non-uniform time sequences.","In this work, we propose a novel Adaptive IF-TEM (AIF-TEM) approach.","This design dynamically adjusts the TEM's sensitivity to changes in the input signal's amplitude and frequency in real-time.","We provide a comprehensive analysis of AIF-TEM's oversampling and distortion properties.","By the adaptive adjustments, AIF-TEM as we show can achieve significant performance improvements in practical finite regime, in terms of sampling rate-distortion.","We demonstrate empirically that in the scenarios tested AIF-TEM outperforms classical IF-TEM and traditional Nyquist (i.e., periodic) sampling methods for band-limited signals.","In terms of Mean Square Error (MSE), the reduction reaches at least 12dB (fixing the oversampling rate)."],"url":"http://arxiv.org/abs/2403.02992v1","category":"eess.SP"}
{"created":"2024-03-05 13:53:48","title":"Online Learning of Human Constraints from Feedback in Shared Autonomy","abstract":"Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints. Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task. In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators. We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions. The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator. Therefore, we propose an augmentative assistant agent capable of learning and adapting to human physical constraints, aligning its actions with the ergonomic preferences and limitations of the human operator.","sentences":["Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints.","Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task.","In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators.","We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions.","The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator.","Therefore, we propose an augmentative assistant agent capable of learning and adapting to human physical constraints, aligning its actions with the ergonomic preferences and limitations of the human operator."],"url":"http://arxiv.org/abs/2403.02974v1","category":"cs.RO"}
{"created":"2024-03-05 10:11:19","title":"Second-order robust parallel integrators for dynamical low-rank approximation","abstract":"Due to its reduced memory and computational demands, dynamical low-rank approximation (DLRA) has sparked significant interest in multiple research communities. A central challenge in DLRA is the development of time integrators that are robust to the curvature of the manifold of low-rank matrices. Recently, a parallel robust time integrator that permits dynamic rank adaptation and enables a fully parallel update of all low-rank factors was introduced. Despite its favorable computational efficiency, the construction as a first-order approximation to the augmented basis-update & Galerkin integrator restricts the parallel integrator's accuracy to order one. In this work, an extension to higher order is proposed by a careful basis augmentation before solving the matrix differential equations of the factorized solution. A robust error bound with an improved dependence on normal components of the vector field together with a norm preservation property up to small terms is derived. These analytic results are complemented and demonstrated through a series of numerical experiments.","sentences":["Due to its reduced memory and computational demands, dynamical low-rank approximation (DLRA) has sparked significant interest in multiple research communities.","A central challenge in DLRA is the development of time integrators that are robust to the curvature of the manifold of low-rank matrices.","Recently, a parallel robust time integrator that permits dynamic rank adaptation and enables a fully parallel update of all low-rank factors was introduced.","Despite its favorable computational efficiency, the construction as a first-order approximation to the augmented basis-update & Galerkin integrator restricts the parallel integrator's accuracy to order one.","In this work, an extension to higher order is proposed by a careful basis augmentation before solving the matrix differential equations of the factorized solution.","A robust error bound with an improved dependence on normal components of the vector field together with a norm preservation property up to small terms is derived.","These analytic results are complemented and demonstrated through a series of numerical experiments."],"url":"http://arxiv.org/abs/2403.02834v1","category":"math.NA"}
{"created":"2024-03-05 09:44:51","title":"An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation","abstract":"Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the electricity production.","sentences":["Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources.","However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems.","In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges.","To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time.","Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms.","This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the electricity production."],"url":"http://arxiv.org/abs/2403.02821v1","category":"cs.LG"}
{"created":"2024-03-05 08:57:28","title":"DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation","abstract":"Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data. Unsupervised domain adaptation (UDA) has proven to be advantageous in incorporating unclassified information from the target domain. However, independently fine-tuning UDA models on the source and target domains has a limited effect on the outcome. This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate domain information. Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy. The efficacy of our approach is substantiated by extensive benchmark experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.","sentences":["Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data.","Unsupervised domain adaptation (UDA) has proven to be advantageous in incorporating unclassified information from the target domain.","However, independently fine-tuning UDA models on the source and target domains has a limited effect on the outcome.","This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate domain information.","Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy.","The efficacy of our approach is substantiated by extensive benchmark experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets."],"url":"http://arxiv.org/abs/2403.02784v1","category":"cs.CV"}
{"created":"2024-03-05 07:10:25","title":"DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization","abstract":"Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.","sentences":["Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data.","With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG).","However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG.","Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains.","With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization.","Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2403.02714v1","category":"cs.CV"}
{"created":"2024-03-05 06:57:37","title":"Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation","abstract":"Leveraging pre-trained visual language models has become a widely adopted approach for improving performance in downstream visual question answering (VQA) applications. However, in the specialized field of medical VQA, the scarcity of available data poses a significant barrier to achieving reliable model generalization. Numerous methods have been proposed to enhance model generalization, addressing the issue from data-centric and model-centric perspectives. Data augmentation techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited data samples. In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the visual encoder of the multimodality model during both pre-training and fine-tuning phases, to improve model generalization for downstream medical VQA tasks. The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer's historical updates. It is subsequently injected into the model's visual encoder. The results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both VQA-RAD and SLAKE datasets.","sentences":["Leveraging pre-trained visual language models has become a widely adopted approach for improving performance in downstream visual question answering (VQA) applications.","However, in the specialized field of medical VQA, the scarcity of available data poses a significant barrier to achieving reliable model generalization.","Numerous methods have been proposed to enhance model generalization, addressing the issue from data-centric and model-centric perspectives.","Data augmentation techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited data samples.","In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the visual encoder of the multimodality model during both pre-training and fine-tuning phases, to improve model generalization for downstream medical VQA tasks.","The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer's historical updates.","It is subsequently injected into the model's visual encoder.","The results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both VQA-RAD and SLAKE datasets."],"url":"http://arxiv.org/abs/2403.02707v1","category":"cs.CV"}
{"created":"2024-03-05 06:10:22","title":"Time Weaver: A Conditional Time Series Generation Model","abstract":"Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that Time Weaver outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets.","sentences":["Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze.","Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.).","Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain.","To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation.","Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient.","These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series.","Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series.","We show that Time Weaver outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets."],"url":"http://arxiv.org/abs/2403.02682v1","category":"cs.LG"}
{"created":"2024-03-05 03:55:17","title":"Human Activity Recognition with Low-Resolution Infrared Array Sensor Using Semi-supervised Cross-domain Neural Networks for Indoor Environment","abstract":"Low-resolution infrared-based human activity recognition (HAR) attracted enormous interests due to its low-cost and private. In this paper, a novel semi-supervised crossdomain neural network (SCDNN) based on 8 $\\times$ 8 low-resolution infrared sensor is proposed for accurately identifying human activity despite changes in the environment at a low-cost. The SCDNN consists of feature extractor, domain discriminator and label classifier. In the feature extractor, the unlabeled and minimal labeled target domain data are trained for domain adaptation to achieve a mapping of the source domain and target domain data. The domain discriminator employs the unsupervised learning to migrate data from the source domain to the target domain. The label classifier obtained from training the source domain data improves the recognition of target domain activities due to the semi-supervised learning utilized in training the target domain data. Experimental results show that the proposed method achieves 92.12\\% accuracy for recognition of activities in the target domain by migrating the source and target domains. The proposed approach adapts superior to cross-domain scenarios compared to the existing deep learning methods, and it provides a low-cost yet highly adaptable solution for cross-domain scenarios.","sentences":["Low-resolution infrared-based human activity recognition (HAR) attracted enormous interests due to its low-cost and private.","In this paper, a novel semi-supervised crossdomain neural network (SCDNN) based on 8 $\\times$ 8 low-resolution infrared sensor is proposed for accurately identifying human activity despite changes in the environment at a low-cost.","The SCDNN consists of feature extractor, domain discriminator and label classifier.","In the feature extractor, the unlabeled and minimal labeled target domain data are trained for domain adaptation to achieve a mapping of the source domain and target domain data.","The domain discriminator employs the unsupervised learning to migrate data from the source domain to the target domain.","The label classifier obtained from training the source domain data improves the recognition of target domain activities due to the semi-supervised learning utilized in training the target domain data.","Experimental results show that the proposed method achieves 92.12\\% accuracy for recognition of activities in the target domain by migrating the source and target domains.","The proposed approach adapts superior to cross-domain scenarios compared to the existing deep learning methods, and it provides a low-cost yet highly adaptable solution for cross-domain scenarios."],"url":"http://arxiv.org/abs/2403.02632v1","category":"eess.SP"}
{"created":"2024-03-05 03:07:10","title":"Exploring the Limitations of Large Language Models in Compositional Relation Reasoning","abstract":"We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.","sentences":["We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other.","Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean.","Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts."],"url":"http://arxiv.org/abs/2403.02615v1","category":"cs.CL"}
{"created":"2024-03-05 02:27:52","title":"TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts","abstract":"Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels. Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic. We published the official code at https://github.com/HyunWookL/TESTAM","sentences":["Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events.","Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling.","In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph.","By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events.","For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels.","Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic.","We published the official code at https://github.com/HyunWookL/TESTAM"],"url":"http://arxiv.org/abs/2403.02600v1","category":"cs.LG"}
{"created":"2024-03-05 01:14:08","title":"Topological protection revealed by real-time longitudinal and transverse studies","abstract":"Topology provides an essential concept for achieving unchanged (or protected) quantum properties in the presence of perturbations. A challenge facing realistic applications is that the level of protection displayed in real systems is subject to substantial variations. Some key differences stem from mechanisms influencing the reconstruction behaviors of extended dissipationless modes. Despite various insightful results on potential causes of backscattering, the edge-state-based approach is limited because the bulk states, as shown by breakdown tests, contribute indispensably. This study investigates the influence of bulk reconstruction where dissipationless modes are global objects instead of being restricted to the sample edge. An integer quantum Hall effect (IQHE) hosted in a Corbino sample geometry is adopted and brought continuously to the verge of a breakdown. A detection technique is developed to include two independent setups capable of simultaneously capturing the onset of dissipation in both longitudinal and transverse directions. The real-time correspondence between orthogonal results confirms two facts. 1. Dissipationless charge modes undergo frequent reconstruction in response to electrochemical potential changes, causing dissipationless current paths to expand transversely into the bulk while preserving chirality. A breakdown only occurs when a backscattering emerges between reconfigured dissipationless current paths bridging opposite edge contacts. 2. Impurity screening is vital in enhancing protection, and topological protection is subject to an intriguing interplay of disorder, electron-electron interaction, and topology. The proposed reconstruction mechanism qualitatively explains the robustness variations, beneficial for developing means for optimization.","sentences":["Topology provides an essential concept for achieving unchanged (or protected) quantum properties in the presence of perturbations.","A challenge facing realistic applications is that the level of protection displayed in real systems is subject to substantial variations.","Some key differences stem from mechanisms influencing the reconstruction behaviors of extended dissipationless modes.","Despite various insightful results on potential causes of backscattering, the edge-state-based approach is limited because the bulk states, as shown by breakdown tests, contribute indispensably.","This study investigates the influence of bulk reconstruction where dissipationless modes are global objects instead of being restricted to the sample edge.","An integer quantum Hall effect (IQHE) hosted in a Corbino sample geometry is adopted and brought continuously to the verge of a breakdown.","A detection technique is developed to include two independent setups capable of simultaneously capturing the onset of dissipation in both longitudinal and transverse directions.","The real-time correspondence between orthogonal results confirms two facts.","1.","Dissipationless charge modes undergo frequent reconstruction in response to electrochemical potential changes, causing dissipationless current paths to expand transversely into the bulk while preserving chirality.","A breakdown only occurs when a backscattering emerges between reconfigured dissipationless current paths bridging opposite edge contacts.","2.","Impurity screening is vital in enhancing protection, and topological protection is subject to an intriguing interplay of disorder, electron-electron interaction, and topology.","The proposed reconstruction mechanism qualitatively explains the robustness variations, beneficial for developing means for optimization."],"url":"http://arxiv.org/abs/2403.02575v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-04 20:20:14","title":"MagicClay: Sculpting Meshes With Generative Neural Fields","abstract":"The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control - a fundamental requirement for artistic work. Triangular meshes, on the other hand, are the representation of choice for most geometry related tasks, offering efficiency and intuitive control, but do not lend themselves to neural optimization. To support downstream tasks, previous art typically proposes a two-step approach, where first a shape is generated using neural fields, and then a mesh is extracted for further processing. Instead, in this paper we introduce a hybrid approach that maintains both a mesh and a Signed Distance Field (SDF) representations consistently. Using this representation, we introduce MagicClay - an artist friendly tool for sculpting regions of a mesh according to textual prompts while keeping other regions untouched. Our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization; Relying on the mesh representation, we show how to render the SDF at higher resolutions and faster. In addition, we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required, as indicated by the SDF. Using an implemented prototype, we demonstrate superior generated geometry compared to the state-of-the-art, and novel consistent control, allowing sequential prompt-based edits to the same mesh for the first time.","sentences":["The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control - a fundamental requirement for artistic work.","Triangular meshes, on the other hand, are the representation of choice for most geometry related tasks, offering efficiency and intuitive control, but do not lend themselves to neural optimization.","To support downstream tasks, previous art typically proposes a two-step approach, where first a shape is generated using neural fields, and then a mesh is extracted for further processing.","Instead, in this paper we introduce a hybrid approach that maintains both a mesh and a Signed Distance Field (SDF) representations consistently.","Using this representation, we introduce MagicClay - an artist friendly tool for sculpting regions of a mesh according to textual prompts while keeping other regions untouched.","Our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization; Relying on the mesh representation, we show how to render the SDF at higher resolutions and faster.","In addition, we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required, as indicated by the SDF.","Using an implemented prototype, we demonstrate superior generated geometry compared to the state-of-the-art, and novel consistent control, allowing sequential prompt-based edits to the same mesh for the first time."],"url":"http://arxiv.org/abs/2403.02460v1","category":"cs.GR"}
{"created":"2024-03-04 19:23:50","title":"Bayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models","abstract":"It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies. However, explicitly specifying all constraints in an environment can be a challenging task. State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues. In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations. The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation. Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods.","sentences":["It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies.","However, explicitly specifying all constraints in an environment can be a challenging task.","State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues.","In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations.","The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation.","Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods."],"url":"http://arxiv.org/abs/2403.02431v1","category":"cs.RO"}
{"created":"2024-03-04 19:00:07","title":"Tunable quantum criticality and pseudocriticality across the fixed-point annihilation in the anisotropic spin-boson model","abstract":"Spin-boson models are simple examples of quantum dissipative systems, but also serve as effective models in quantum magnetism and exhibit nontrivial quantum criticality. Recently, they have been established as a platform to study the nontrivial renormalization-group (RG) scenario of fixed-point annihilation, in which two intermediate-coupling RG fixed points collide and generate an extremely slow RG flow near the collision. For the Bose Kondo model, a single $S=1/2$ spin where each spin component couples to an independent bosonic bath with power-law spectrum $\\propto \\omega^s$ via dissipation strengths $\\alpha_i$, $i\\in\\{x,y,z\\}$, such phenomena occur sequentially for the U(1)-symmetric model at $\\alpha_z=0$ and the SU(2)-symmetric case at $\\alpha_z = \\alpha_{xy}$, as the bath exponent $s<1$ is tuned. Here we use an exact wormhole quantum Monte Carlo method to show how fixed-point annihilations within symmetry-enhanced parameter manifolds affect the anisotropy-driven criticality across them. We find a tunable transition between two long-range-ordered localized phases that can be continuous or strongly first-order, and even becomes weakly first-order in an extended regime close to the fixed-point collision. We extract critical exponents at the continuous transition, but also find scaling behavior at the symmetry-enhanced first-order transition, for which the inverse correlation-length exponent is given by the bath exponent $s$. In particular, we provide direct numerical evidence for pseudocritical scaling on both sides of the fixed-point collision, which manifests in an extremely slow drift of the correlation-length exponent. In addition, we also study the crossover behavior away from the SU(2)-symmetric case and determine the phase boundary of an extended U(1)-symmetric critical phase for $\\alpha_z < \\alpha_{xy}$.","sentences":["Spin-boson models are simple examples of quantum dissipative systems, but also serve as effective models in quantum magnetism and exhibit nontrivial quantum criticality.","Recently, they have been established as a platform to study the nontrivial renormalization-group (RG) scenario of fixed-point annihilation, in which two intermediate-coupling RG fixed points collide and generate an extremely slow RG flow near the collision.","For the Bose Kondo model, a single $S=1/2$ spin where each spin component couples to an independent bosonic bath with power-law spectrum $\\propto \\omega^s$ via dissipation strengths $\\alpha_i$, $i\\in\\{x,y,z\\}$, such phenomena occur sequentially for the U(1)-symmetric model at $\\alpha_z=0$ and the SU(2)-symmetric case at $\\alpha_z = \\alpha_{xy}$, as the bath exponent $s<1$ is tuned.","Here we use an exact wormhole quantum Monte Carlo method to show how fixed-point annihilations within symmetry-enhanced parameter manifolds affect the anisotropy-driven criticality across them.","We find a tunable transition between two long-range-ordered localized phases that can be continuous or strongly first-order, and even becomes weakly first-order in an extended regime close to the fixed-point collision.","We extract critical exponents at the continuous transition, but also find scaling behavior at the symmetry-enhanced first-order transition, for which the inverse correlation-length exponent is given by the bath exponent $s$. In particular, we provide direct numerical evidence for pseudocritical scaling on both sides of the fixed-point collision, which manifests in an extremely slow drift of the correlation-length exponent.","In addition, we also study the crossover behavior away from the SU(2)-symmetric case and determine the phase boundary of an extended U(1)-symmetric critical phase for $\\alpha_z < \\alpha_{xy}$."],"url":"http://arxiv.org/abs/2403.02400v1","category":"cond-mat.str-el"}
{"created":"2024-03-04 19:00:04","title":"End-to-end variational quantum sensing","abstract":"Harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering. Real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging. Numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice. Here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively. The framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ans\\\"atze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation. End-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors.","sentences":["Harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering.","Real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging.","Numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice.","Here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively.","The framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ans\\\"atze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation.","End-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors."],"url":"http://arxiv.org/abs/2403.02394v1","category":"quant-ph"}
{"created":"2024-03-04 18:57:11","title":"COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks","abstract":"Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks. So far, there is no certified defense proposed for MSFs. In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks. In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and performs a grid-based splitting method to characterize complex semantic transformations. We also propose efficient algorithms to compute the certification in terms of object detection accuracy and IoU for large-scale MSF models. Empirically, we evaluate the efficacy of COMMIT in different settings and provide a comprehensive benchmark of certified robustness for different MSF models using the CARLA simulation platform. We show that the certification for MSF models is at most 48.39% higher than that of single-modal models, which validates the advantages of MSF models. We believe our certification framework and benchmark will contribute an important step towards certifiably robust AVs in practice.","sentences":["Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs).","Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs.","While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations.","Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks.","So far, there is no certified defense proposed for MSFs.","In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks.","In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and performs a grid-based splitting method to characterize complex semantic transformations.","We also propose efficient algorithms to compute the certification in terms of object detection accuracy and IoU for large-scale MSF models.","Empirically, we evaluate the efficacy of COMMIT in different settings and provide a comprehensive benchmark of certified robustness for different MSF models using the CARLA simulation platform.","We show that the certification for MSF models is at most 48.39% higher than that of single-modal models, which validates the advantages of MSF models.","We believe our certification framework and benchmark will contribute an important step towards certifiably robust AVs in practice."],"url":"http://arxiv.org/abs/2403.02329v1","category":"cs.LG"}
{"created":"2024-03-04 18:53:27","title":"Contract Design for Pandora's Box","abstract":"We study a natural application of contract design to search problems with probabilistic prior and exploration costs. These problems have a plethora of applications and are expressed concisely within the Pandora's Box model. Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979.   In our principal-agent setting, the search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal to maximize her expected reward, i.e., value minus payment. We show how to compute optimal contracts for the principal in several scenarios.   A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. Interestingly, a suitable adaptation of the index policy results in an optimal contract here. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.","sentences":["We study a natural application of contract design to search problems with probabilistic prior and exploration costs.","These problems have a plethora of applications and are expressed concisely within the Pandora's Box model.","Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979.   ","In our principal-agent setting, the search task is delegated to an agent.","The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome.","Agent and principal obtain an individual value based on the selected prize.","To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize.","The goal of the principal to maximize her expected reward, i.e., value minus payment.","We show how to compute optimal contracts for the principal in several scenarios.   ","A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time.","For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal.","Interestingly, a suitable adaptation of the index policy results in an optimal contract here.","More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal.","These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy."],"url":"http://arxiv.org/abs/2403.02317v1","category":"cs.GT"}
{"created":"2024-03-04 18:46:20","title":"Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures","abstract":"Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations in image classification demonstrate that VRWKV matches ViT's classification performance with significantly faster speeds and lower memory usage. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at \\url{https://github.com/OpenGVLab/Vision-RWKV}.","sentences":["Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis.","This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks.","Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets.","Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations.","Our evaluations in image classification demonstrate that VRWKV matches ViT's classification performance with significantly faster speeds and lower memory usage.","In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds.","These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks.","Code is released at \\url{https://github.com/OpenGVLab/Vision-RWKV}."],"url":"http://arxiv.org/abs/2403.02308v1","category":"cs.CV"}
{"created":"2024-03-04 18:44:30","title":"Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection","abstract":"Realizing sufficient separability between the distributions of healthy and pathological samples is a critical obstacle for pathology detection convolutional models. Moreover, these models exhibit a bias for contrast-based images, with diminished performance on texture-based medical images. This study introduces the notion of a population-level context for pathology detection and employs a graph theoretic approach to model and incorporate it into the latent code of an autoencoder via a refinement module we term PopuSense. PopuSense seeks to capture additional intra-group variations inherent in biomedical data that a local or global context of the convolutional model might miss or smooth out. Experiments on contrast-based and texture-based images, with minimal adaptation, encounter the existing preference for intensity-based input. Nevertheless, PopuSense demonstrates improved separability in contrast-based images, presenting an additional avenue for refining representations learned by a model.","sentences":["Realizing sufficient separability between the distributions of healthy and pathological samples is a critical obstacle for pathology detection convolutional models.","Moreover, these models exhibit a bias for contrast-based images, with diminished performance on texture-based medical images.","This study introduces the notion of a population-level context for pathology detection and employs a graph theoretic approach to model and incorporate it into the latent code of an autoencoder via a refinement module we term PopuSense.","PopuSense seeks to capture additional intra-group variations inherent in biomedical data that a local or global context of the convolutional model might miss or smooth out.","Experiments on contrast-based and texture-based images, with minimal adaptation, encounter the existing preference for intensity-based input.","Nevertheless, PopuSense demonstrates improved separability in contrast-based images, presenting an additional avenue for refining representations learned by a model."],"url":"http://arxiv.org/abs/2403.02307v1","category":"eess.IV"}
{"created":"2024-03-04 18:18:36","title":"PixIT: Joint Training of Speaker Diarization and Speech Separation from Real-world Multi-speaker Recordings","abstract":"A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization. Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio. We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep. With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD. We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them. PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning.","sentences":["A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization.","Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio.","We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep.","With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD.","We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them.","PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning."],"url":"http://arxiv.org/abs/2403.02288v1","category":"eess.AS"}
{"created":"2024-03-04 18:15:14","title":"Detection of Non-recorded Word Senses in English and Swedish","abstract":"This study addresses the task of Unknown Sense Detection in English and Swedish. The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not. For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a few-shot scenario. Additionally, we use human annotations to adapt and evaluate our models. Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses.","sentences":["This study addresses the task of Unknown Sense Detection in English and Swedish.","The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not.","For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a few-shot scenario.","Additionally, we use human annotations to adapt and evaluate our models.","Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses."],"url":"http://arxiv.org/abs/2403.02285v1","category":"cs.CL"}
{"created":"2024-03-04 18:06:07","title":"Zonostrophic turbulence in the subsurface oceans of the Jovian and Saturnian moons","abstract":"In order to characterize the global circulation of the subsurface ocean of Jovian and Saturnian moons, we analyze the properties of 21 three-dimensional simulations of Boussinesq thermal convection in a rapidly rotating spherical shell. Flow is driven by an adverse temperature contrast imposed across the domain, and is subjected to no-slip boundary conditions. We cover a region of parameter space previously unexplored by global simulations, both in terms of rapid rotation and vigor of convective forcing, closer to, yet still admittedly far from, the conditions appropriate for the subsurface ocean of Ganymede, Europa, Enceladus, and Titan. Our most extreme simulations exhibit a dynamic global circulation that combines powerful east-west zonal jets, planetary waves, and vortices. A spectral analysis of the kinetic energy distribution performed in cylindrical geometry reveals a high degree of anisotropy of the simulated flows. Specifically, the axisymmetric zonal energy spectra follow a steep $-5$ slope in wavenumber space, with the energy amplitude exclusively controlled by the rotation rate. In contrast, the non-axisymmetric residual spectra display a gentle $-5/3$ slope, with the energy amplitude controlled by the thermal buoyancy input power. This spectral behavior conforms with the theory of zonostrophic turbulence and allows us to propose tentative extrapolations of these findings to the more extreme conditions of icy satellites. By assuming that kinetic energy dissipates via Ekman friction we predict an upper bound for the zonal velocity ranging from a few centimeters per second for Enceladus to about one meter per second for Ganymede, with residual velocities smaller than the zonal velocity by an order of magnitude on each moon. These predictions yield typical jets size approaching the ocean depth of Titan, Ganymede and Europa and $10$ to $40\\%$ of the ocean depth on Enceladus.","sentences":["In order to characterize the global circulation of the subsurface ocean of Jovian and Saturnian moons, we analyze the properties of 21 three-dimensional simulations of Boussinesq thermal convection in a rapidly rotating spherical shell.","Flow is driven by an adverse temperature contrast imposed across the domain, and is subjected to no-slip boundary conditions.","We cover a region of parameter space previously unexplored by global simulations, both in terms of rapid rotation and vigor of convective forcing, closer to, yet still admittedly far from, the conditions appropriate for the subsurface ocean of Ganymede, Europa, Enceladus, and Titan.","Our most extreme simulations exhibit a dynamic global circulation that combines powerful east-west zonal jets, planetary waves, and vortices.","A spectral analysis of the kinetic energy distribution performed in cylindrical geometry reveals a high degree of anisotropy of the simulated flows.","Specifically, the axisymmetric zonal energy spectra follow a steep $-5$ slope in wavenumber space, with the energy amplitude exclusively controlled by the rotation rate.","In contrast, the non-axisymmetric residual spectra display a gentle $-5/3$ slope, with the energy amplitude controlled by the thermal buoyancy input power.","This spectral behavior conforms with the theory of zonostrophic turbulence and allows us to propose tentative extrapolations of these findings to the more extreme conditions of icy satellites.","By assuming that kinetic energy dissipates via Ekman friction we predict an upper bound for the zonal velocity ranging from a few centimeters per second for Enceladus to about one meter per second for Ganymede, with residual velocities smaller than the zonal velocity by an order of magnitude on each moon.","These predictions yield typical jets size approaching the ocean depth of Titan, Ganymede and Europa and $10$ to $40\\%$ of the ocean depth on Enceladus."],"url":"http://arxiv.org/abs/2403.02277v1","category":"physics.flu-dyn"}
{"created":"2024-03-04 18:04:52","title":"Bounded Depth Frege Lower Bounds for Random 3-CNFs via Deterministic Restrictions","abstract":"A major open problem in proof complexity is to show that random 3-CNFs with linear number of clauses require super-polynomial size refutations in bounded depth Frege. We make a first step towards this question by showing a super-linear lower bound: for every $k$, there exists $\\epsilon > 0$ such that any depth-$k$ Frege refutation of a random $n$-variable 3-CNF with $\\Theta(n)$ clauses has $\\Omega(n^{1 + \\epsilon})$ steps w.h.p. Our proof involves a novel adaptation of the deterministic restriction technique of Chaudhuri and Radhakrishnan (STOC'96).","sentences":["A major open problem in proof complexity is to show that random 3-CNFs with linear number of clauses require super-polynomial size refutations in bounded depth Frege.","We make a first step towards this question by showing a super-linear lower bound: for every $k$, there exists $\\epsilon > 0$ such that any depth-$k$ Frege refutation of a random $n$-variable 3-CNF with $\\Theta(n)$ clauses has $\\Omega(n^{1 + \\epsilon})$ steps w.h.p.","Our proof involves a novel adaptation of the deterministic restriction technique of Chaudhuri and Radhakrishnan (STOC'96)."],"url":"http://arxiv.org/abs/2403.02275v1","category":"cs.CC"}
{"created":"2024-03-04 17:34:57","title":"Electrical Control of Exciton-Polariton Condensate Josephson Junctions","abstract":"We propose the electrical control of a device probing the Josephson effect in exciton-polariton (EP) condensates, which can be switched between various dynamical modes. We model the device by a four-component Gross-Pitaevskii equation assuming that ideal EP condensates are established with well-balanced pumping and dissipation. All the model parameters are calculated microscopically. In particular, we obtain the polariton tunneling strength across the junction as a second-order process of electron-hole pair tunneling. We find that the EP condensates can be manipulated through distinctive degrees of freedom not present in other coherent quantum systems, and the dynamics of EP Josephson junctions are far richer than that of the conventional superconducting junctions.","sentences":["We propose the electrical control of a device probing the Josephson effect in exciton-polariton (EP) condensates, which can be switched between various dynamical modes.","We model the device by a four-component Gross-Pitaevskii equation assuming that ideal EP condensates are established with well-balanced pumping and dissipation.","All the model parameters are calculated microscopically.","In particular, we obtain the polariton tunneling strength across the junction as a second-order process of electron-hole pair tunneling.","We find that the EP condensates can be manipulated through distinctive degrees of freedom not present in other coherent quantum systems, and the dynamics of EP Josephson junctions are far richer than that of the conventional superconducting junctions."],"url":"http://arxiv.org/abs/2403.02248v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-04 17:34:46","title":"Birbal: An efficient 7B instruct-model fine-tuned with curated datasets","abstract":"LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible. To tackle these challenges, the LLM Efficiency Challenge was introduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe. In this system description paper, we introduce Birbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for 16 hours. Birbal's success lies in curating high-quality instructions covering diverse tasks, resulting in a 35% performance improvement over second-best Qwen-14B based submission.","sentences":["LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility.","Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible.","To tackle these challenges, the LLM Efficiency Challenge was introduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe.","In this system description paper, we introduce Birbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for 16 hours.","Birbal's success lies in curating high-quality instructions covering diverse tasks, resulting in a 35% performance improvement over second-best Qwen-14B based submission."],"url":"http://arxiv.org/abs/2403.02247v1","category":"cs.CL"}
{"created":"2024-03-04 17:08:57","title":"TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models","abstract":"Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning. Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs. In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively. These are subsequently integrated to form inputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands. Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data.","sentences":["Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management.","The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data.","However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention.","Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem.","It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning.","Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs.","In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively.","These are subsequently integrated to form inputs that are suitable for LLMs.","A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands.","Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data."],"url":"http://arxiv.org/abs/2403.02221v1","category":"cs.LG"}
{"created":"2024-03-04 17:01:28","title":"Global weak solutions of the Serre-Green-Naghdi equations with surface tension","abstract":"We consider in this paper the Serre--Green--Naghdi equations with surface tension. Smooth solutions of this system conserve an $H^1$-equivalent energy. We prove the existence of global weak dissipative solutions for any relatively small-energy initial data. We also prove that the Riemann invariants of the solutions satisfy a one-sided Oleinik inequality.","sentences":["We consider in this paper the Serre--Green--Naghdi equations with surface tension.","Smooth solutions of this system conserve an $H^1$-equivalent energy.","We prove the existence of global weak dissipative solutions for any relatively small-energy initial data.","We also prove that the Riemann invariants of the solutions satisfy a one-sided Oleinik inequality."],"url":"http://arxiv.org/abs/2403.02214v1","category":"math.AP"}
{"created":"2024-03-04 16:49:36","title":"Parametric multi-element coupling architecture for coherent and dissipative control of superconducting qubits","abstract":"As systems for quantum computing keep growing in size and number of qubits, challenges in scaling the control capabilities are becoming increasingly relevant. Efficient schemes to simultaneously mediate coherent interactions between multiple quantum systems and to reduce decoherence errors can minimize the control overhead in next-generation quantum processors. Here, we present a superconducting qubit architecture based on tunable parametric interactions to perform two-qubit gates, reset, leakage recovery and to read out the qubits. In this architecture, parametrically driven multi-element couplers selectively couple qubits to resonators and neighbouring qubits, according to the frequency of the drive. We consider a system with two qubits and one readout resonator interacting via a single coupling circuit and experimentally demonstrate a controlled-Z gate with a fidelity of $98.30\\pm 0.23 \\%$, a reset operation that unconditionally prepares the qubit ground state with a fidelity of $99.80\\pm 0.02 \\%$ and a leakage recovery operation with a $98.5\\pm 0.3 \\%$ success probability. Furthermore, we implement a parametric readout with a single-shot assignment fidelity of $88.0\\pm 0.4 \\%$. These operations are all realized using a single tunable coupler, demonstrating the experimental feasibility of the proposed architecture and its potential for reducing the system complexity in scalable quantum processors.","sentences":["As systems for quantum computing keep growing in size and number of qubits, challenges in scaling the control capabilities are becoming increasingly relevant.","Efficient schemes to simultaneously mediate coherent interactions between multiple quantum systems and to reduce decoherence errors can minimize the control overhead in next-generation quantum processors.","Here, we present a superconducting qubit architecture based on tunable parametric interactions to perform two-qubit gates, reset, leakage recovery and to read out the qubits.","In this architecture, parametrically driven multi-element couplers selectively couple qubits to resonators and neighbouring qubits, according to the frequency of the drive.","We consider a system with two qubits and one readout resonator interacting via a single coupling circuit and experimentally demonstrate a controlled-Z gate with a fidelity of $98.30\\pm 0.23 \\%$, a reset operation that unconditionally prepares the qubit ground state with a fidelity of $99.80\\pm 0.02 \\%$ and a leakage recovery operation with a $98.5\\pm 0.3 \\%$ success probability.","Furthermore, we implement a parametric readout with a single-shot assignment fidelity of $88.0\\pm 0.4 \\%$. These operations are all realized using a single tunable coupler, demonstrating the experimental feasibility of the proposed architecture and its potential for reducing the system complexity in scalable quantum processors."],"url":"http://arxiv.org/abs/2403.02203v1","category":"quant-ph"}
{"created":"2024-03-04 16:34:10","title":"Boosting Distributional Copula Regression for Bivariate Binary, Discrete and Mixed Responses","abstract":"Motivated by challenges in the analysis of biomedical data and observational studies, we develop statistical boosting for the general class of bivariate distributional copula regression with arbitrary marginal distributions, which is suited to model binary, count, continuous or mixed outcomes. In our framework, the joint distribution of arbitrary, bivariate responses is modelled through a parametric copula. To arrive at a model for the entire conditional distribution, not only the marginal distribution parameters but also the copula parameters are related to covariates through additive predictors. We suggest efficient and scalable estimation by means of an adapted component-wise gradient boosting algorithm with statistical models as base-learners. A key benefit of boosting as opposed to classical likelihood or Bayesian estimation is the implicit data-driven variable selection mechanism as well as shrinkage without additional input or assumptions from the analyst. To the best of our knowledge, our implementation is the only one that combines a wide range of covariate effects, marginal distributions, copula functions, and implicit data-driven variable selection. We showcase the versatility of our approach on data from genetic epidemiology, healthcare utilization and childhood undernutrition. Our developments are implemented in the R package gamboostLSS, fostering transparent and reproducible research.","sentences":["Motivated by challenges in the analysis of biomedical data and observational studies, we develop statistical boosting for the general class of bivariate distributional copula regression with arbitrary marginal distributions, which is suited to model binary, count, continuous or mixed outcomes.","In our framework, the joint distribution of arbitrary, bivariate responses is modelled through a parametric copula.","To arrive at a model for the entire conditional distribution, not only the marginal distribution parameters but also the copula parameters are related to covariates through additive predictors.","We suggest efficient and scalable estimation by means of an adapted component-wise gradient boosting algorithm with statistical models as base-learners.","A key benefit of boosting as opposed to classical likelihood or Bayesian estimation is the implicit data-driven variable selection mechanism as well as shrinkage without additional input or assumptions from the analyst.","To the best of our knowledge, our implementation is the only one that combines a wide range of covariate effects, marginal distributions, copula functions, and implicit data-driven variable selection.","We showcase the versatility of our approach on data from genetic epidemiology, healthcare utilization and childhood undernutrition.","Our developments are implemented in the R package gamboostLSS, fostering transparent and reproducible research."],"url":"http://arxiv.org/abs/2403.02194v1","category":"stat.ME"}
{"created":"2024-03-04 16:31:58","title":"Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans","abstract":"The paper presents the DEF-AI-MIA COV19D Competition, which is organized in the framework of the 'Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis (DEF-AI-MIA)' Workshop of the 2024 Computer Vision and Pattern Recognition (CVPR) Conference. The Competition is the 4th in the series, following the first three Competitions held in the framework of ICCV 2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It includes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain Adaptation. The Competition use data from COV19-CT-DB database, which is described in the paper and includes a large number of chest CT scan series. Each chest CT scan series consists of a sequence of 2-D CT slices, the number of which is between 50 and 700. Training, validation and test datasets have been extracted from COV19-CT-DB and provided to the participants in both Challenges. The paper presents the baseline models used in the Challenges and the performance which was obtained respectively.","sentences":["The paper presents the DEF-AI-MIA COV19D Competition, which is organized in the framework of the 'Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis (DEF-AI-MIA)'","Workshop of the 2024 Computer Vision and Pattern Recognition (CVPR) Conference.","The Competition is the 4th in the series, following the first three Competitions held in the framework of ICCV 2021, ECCV 2022 and ICASSP 2023 International Conferences respectively.","It includes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain Adaptation.","The Competition use data from COV19-CT-DB database, which is described in the paper and includes a large number of chest CT scan series.","Each chest CT scan series consists of a sequence of 2-D CT slices, the number of which is between 50 and 700.","Training, validation and test datasets have been extracted from COV19-CT-DB and provided to the participants in both Challenges.","The paper presents the baseline models used in the Challenges and the performance which was obtained respectively."],"url":"http://arxiv.org/abs/2403.02192v1","category":"eess.IV"}
{"created":"2024-03-04 16:23:58","title":"Not all Layers of LLMs are Necessary during Inference","abstract":"The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, \"During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?\" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment tasks, while maintaining comparable performance. Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further.","sentences":["The inference phase of Large Language Models (LLMs) is very expensive.","An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability).","In this paper, we try to answer the question, \"During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?\"","To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks.","Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively.","More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks.","Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment tasks, while maintaining comparable performance.","Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further."],"url":"http://arxiv.org/abs/2403.02181v1","category":"cs.CL"}
{"created":"2024-03-04 16:09:27","title":"LiveRec: Prototyping Probes by Framing Debug Protocols","abstract":"Context: In the first part of his 2012 presentation \"Inventing on Principle\", Bret Victor gives a demo of a live code editor for Javascript which shows the dynamic history of values of variables in real time. This form of live programming has become known as \"probes\". Probes provide the programmer with permanent and continuous insight into the dynamic evolution of function or method variables, thus improving feedback and developer experience.   Inquiry: Although Victor shows a working prototype of live probes in the context of Javascript, he does not discuss strategies for implementing them. Later work provides an implementation approach, but this requires a programming language to be implemented on top of the GraalVM runtime. In this paper we present **LiveRec**, a generic approach for implementing probes which can be applied in the context of many programming languages, without requiring the modification of compilers or run-time systems.   Approach: **LiveRec** is based on reusing existing debug protocols to implement probes. Methods or functions are compiled after every code change and executed inside the debugger. During execution the evolution of all local variables in the current stack frame are recorded and communicated back to the editor or IDE for display to the user.   Knowledge: It turns out that mainstream debug protocols are rich enough for implementing live probes. Step-wise execution, code hot swapping, and stack frame inspection provide the right granularity and sufficient information to realize live probes, without modifying compilers or language runtimes. Furthermore, it turns out that the recently proposed Debugger Adapter Protocol (DAP) provides an even more generic approach of implementing live probes, but, in some cases, at the cost of a significant performance penalty.   Grounding: We have applied **LiveRec** to implement probes using stack recording natively for Java through the Java Debug Interface (JDI), and through the DAP for Java, Python, C, and Javascript, all requiring just modest amounts of configuration code. We evaluate the run-time performance of all four probes prototypes, decomposed into: compile-after-change, hot swap, single step overhead, and stack recording overhead. Our initial results show that live probes on top of native debug APIs can be performant enough for interactive use. In the case of DAP, however, it highly depends on characteristics of the programming language implementation and its associated debugging infrastructure.   Importance: Live programming improves the programmer experience by providing immediate feedback about a program's execution and eliminating disruptive edit-compile-restart sequences. Probes are one way to shorten the programmer feedback loop at the level of functions and methods. Although probes are not new, and have been implemented in (prototype) systems, **LiveRec**'s approach of building live probes on top of existing and generic debug protocols promises a path towards probes for a host of mainstream programming languages, with reasonable effort.","sentences":["Context: In the first part of his 2012 presentation \"Inventing on Principle\", Bret Victor gives a demo of a live code editor for Javascript which shows the dynamic history of values of variables in real time.","This form of live programming has become known as \"probes\".","Probes provide the programmer with permanent and continuous insight into the dynamic evolution of function or method variables, thus improving feedback and developer experience.   ","Inquiry:","Although Victor shows a working prototype of live probes in the context of Javascript, he does not discuss strategies for implementing them.","Later work provides an implementation approach, but this requires a programming language to be implemented on top of the GraalVM runtime.","In this paper we present **LiveRec**, a generic approach for implementing probes which can be applied in the context of many programming languages, without requiring the modification of compilers or run-time systems.   ","Approach: **LiveRec** is based on reusing existing debug protocols to implement probes.","Methods or functions are compiled after every code change and executed inside the debugger.","During execution the evolution of all local variables in the current stack frame are recorded and communicated back to the editor or IDE for display to the user.   ","Knowledge: It turns out that mainstream debug protocols are rich enough for implementing live probes.","Step-wise execution, code hot swapping, and stack frame inspection provide the right granularity and sufficient information to realize live probes, without modifying compilers or language runtimes.","Furthermore, it turns out that the recently proposed Debugger Adapter Protocol (DAP) provides an even more generic approach of implementing live probes, but, in some cases, at the cost of a significant performance penalty.   ","Grounding: We have applied **LiveRec** to implement probes using stack recording natively for Java through the Java Debug Interface (JDI), and through the DAP for Java, Python, C, and Javascript, all requiring just modest amounts of configuration code.","We evaluate the run-time performance of all four probes prototypes, decomposed into: compile-after-change, hot swap, single step overhead, and stack recording overhead.","Our initial results show that live probes on top of native debug APIs can be performant enough for interactive use.","In the case of DAP, however, it highly depends on characteristics of the programming language implementation and its associated debugging infrastructure.   ","Importance: Live programming improves the programmer experience by providing immediate feedback about a program's execution and eliminating disruptive edit-compile-restart sequences.","Probes are one way to shorten the programmer feedback loop at the level of functions and methods.","Although probes are not new, and have been implemented in (prototype) systems, **LiveRec**'s approach of building live probes on top of existing and generic debug protocols promises a path towards probes for a host of mainstream programming languages, with reasonable effort."],"url":"http://arxiv.org/abs/2403.02161v1","category":"cs.PL"}
{"created":"2024-03-04 16:00:35","title":"Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling","abstract":"Time-series modeling in process industries faces the challenge of dealing with complex, multi-faceted, and evolving data characteristics. Conventional single model approaches often struggle to capture the interplay of diverse dynamics, resulting in suboptimal forecasts. Addressing this, we introduce the Recency-Weighted Temporally-Segmented (ReWTS, pronounced `roots') ensemble model, a novel chunk-based approach for multi-step forecasting. The key characteristics of the ReWTS model are twofold: 1) It facilitates specialization of models into different dynamics by segmenting the training data into `chunks' of data and training one model per chunk. 2) During inference, an optimization procedure assesses each model on the recent past and selects the active models, such that the appropriate mixture of previously learned dynamics can be recalled to forecast the future. This method not only captures the nuances of each period, but also adapts more effectively to changes over time compared to conventional `global' models trained on all data in one go. We present a comparative analysis, utilizing two years of data from a wastewater treatment plant and a drinking water treatment plant in Norway, demonstrating the ReWTS ensemble's superiority. It consistently outperforms the global model in terms of mean squared forecasting error across various model architectures by 10-70\\% on both datasets, notably exhibiting greater resilience to outliers. This approach shows promise in developing automatic, adaptable forecasting models for decision-making and control systems in process industries and other complex systems.","sentences":["Time-series modeling in process industries faces the challenge of dealing with complex, multi-faceted, and evolving data characteristics.","Conventional single model approaches often struggle to capture the interplay of diverse dynamics, resulting in suboptimal forecasts.","Addressing this, we introduce the Recency-Weighted Temporally-Segmented (ReWTS, pronounced `roots') ensemble model, a novel chunk-based approach for multi-step forecasting.","The key characteristics of the ReWTS model are twofold: 1) It facilitates specialization of models into different dynamics by segmenting the training data into `chunks' of data and training one model per chunk.","2) During inference, an optimization procedure assesses each model on the recent past and selects the active models, such that the appropriate mixture of previously learned dynamics can be recalled to forecast the future.","This method not only captures the nuances of each period, but also adapts more effectively to changes over time compared to conventional `global' models trained on all data in one go.","We present a comparative analysis, utilizing two years of data from a wastewater treatment plant and a drinking water treatment plant in Norway, demonstrating the ReWTS ensemble's superiority.","It consistently outperforms the global model in terms of mean squared forecasting error across various model architectures by 10-70\\% on both datasets, notably exhibiting greater resilience to outliers.","This approach shows promise in developing automatic, adaptable forecasting models for decision-making and control systems in process industries and other complex systems."],"url":"http://arxiv.org/abs/2403.02150v1","category":"stat.ML"}
{"created":"2024-03-04 15:53:44","title":"Multi-Derivative Runge-Kutta Flux Reconstruction for hyperbolic conservation laws","abstract":"We extend the fourth order, two stage Multi-Derivative Runge Kutta (MDRK) scheme of Li and Du to the Flux Reconstruction (FR) framework by writing both of the stages in terms of a time averaged flux and then use the approximate Lax-Wendroff procedure. Numerical flux is computed in each stage using D2 dissipation and EA flux, enhancing Fourier CFL stability and accuracy respectively. A subcell based blending limiter is developed for the MDRK scheme, which ensures that the limited scheme is provably admissibility preserving. Along with being admissibility preserving, the blending scheme is constructed to minimize dissipation errors by using Gauss-Legendre solution points and performing MUSCL-Hancock reconstruction on subcells. The accuracy enhancement of the blending scheme is numerically verified on compressible Euler's equations, with test cases involving shocks and small-scale structures.","sentences":["We extend the fourth order, two stage Multi-Derivative Runge Kutta (MDRK) scheme of Li and Du to the Flux Reconstruction (FR) framework by writing both of the stages in terms of a time averaged flux and then use the approximate Lax-Wendroff procedure.","Numerical flux is computed in each stage using D2 dissipation and EA flux, enhancing Fourier CFL stability and accuracy respectively.","A subcell based blending limiter is developed for the MDRK scheme, which ensures that the limited scheme is provably admissibility preserving.","Along with being admissibility preserving, the blending scheme is constructed to minimize dissipation errors by using Gauss-Legendre solution points and performing MUSCL-Hancock reconstruction on subcells.","The accuracy enhancement of the blending scheme is numerically verified on compressible Euler's equations, with test cases involving shocks and small-scale structures."],"url":"http://arxiv.org/abs/2403.02141v1","category":"math.NA"}
{"created":"2024-03-04 15:46:50","title":"Point2Building: Reconstructing Buildings from Airborne LiDAR Point Clouds","abstract":"We present a learning-based approach to reconstruct buildings as 3D polygonal meshes from airborne LiDAR point clouds. What makes 3D building reconstruction from airborne LiDAR hard is the large diversity of building designs and especially roof shapes, the low and varying point density across the scene, and the often incomplete coverage of building facades due to occlusions by vegetation or to the viewing angle of the sensor. To cope with the diversity of shapes and inhomogeneous and incomplete object coverage, we introduce a generative model that directly predicts 3D polygonal meshes from input point clouds. Our autoregressive model, called Point2Building, iteratively builds up the mesh by generating sequences of vertices and faces. This approach enables our model to adapt flexibly to diverse geometries and building structures. Unlike many existing methods that rely heavily on pre-processing steps like exhaustive plane detection, our model learns directly from the point cloud data, thereby reducing error propagation and increasing the fidelity of the reconstruction. We experimentally validate our method on a collection of airborne LiDAR data of Zurich, Berlin and Tallinn. Our method shows good generalization to diverse urban styles.","sentences":["We present a learning-based approach to reconstruct buildings as 3D polygonal meshes from airborne LiDAR point clouds.","What makes 3D building reconstruction from airborne LiDAR hard is the large diversity of building designs and especially roof shapes, the low and varying point density across the scene, and the often incomplete coverage of building facades due to occlusions by vegetation or to the viewing angle of the sensor.","To cope with the diversity of shapes and inhomogeneous and incomplete object coverage, we introduce a generative model that directly predicts 3D polygonal meshes from input point clouds.","Our autoregressive model, called Point2Building, iteratively builds up the mesh by generating sequences of vertices and faces.","This approach enables our model to adapt flexibly to diverse geometries and building structures.","Unlike many existing methods that rely heavily on pre-processing steps like exhaustive plane detection, our model learns directly from the point cloud data, thereby reducing error propagation and increasing the fidelity of the reconstruction.","We experimentally validate our method on a collection of airborne LiDAR data of Zurich, Berlin and Tallinn.","Our method shows good generalization to diverse urban styles."],"url":"http://arxiv.org/abs/2403.02136v1","category":"cs.CV"}
{"created":"2024-03-04 15:40:31","title":"UB-FineNet: Urban Building Fine-grained Classification Network for Open-access Satellite Images","abstract":"Fine classification of city-scale buildings from satellite remote sensing imagery is a crucial research area with significant implications for urban planning, infrastructure development, and population distribution analysis. However, the task faces big challenges due to low-resolution overhead images acquired from high altitude space-borne platforms and the long-tail sample distribution of fine-grained urban building categories, leading to severe class imbalance problem. To address these issues, we propose a deep network approach to fine-grained classification of urban buildings using open-access satellite images. A Denoising Diffusion Probabilistic Model (DDPM) based super-resolution method is first introduced to enhance the spatial resolution of satellite images, which benefits from domain-adaptive knowledge distillation. Then, a new fine-grained classification network with Category Information Balancing Module (CIBM) and Contrastive Supervision (CS) technique is proposed to mitigate the problem of class imbalance and improve the classification robustness and accuracy. Experiments on Hong Kong data set with 11 fine building types revealed promising classification results with a mean Top-1 accuracy of 60.45\\%, which is on par with street-view image based approaches. Extensive ablation study shows that CIBM and CS improve Top-1 accuracy by 2.6\\% and 3.5\\% compared to the baseline method, respectively. And both modules can be easily inserted into other classification networks and similar enhancements have been achieved. Our research contributes to the field of urban analysis by providing a practical solution for fine classification of buildings in challenging mega city scenarios solely using open-access satellite images. The proposed method can serve as a valuable tool for urban planners, aiding in the understanding of economic, industrial, and population distribution.","sentences":["Fine classification of city-scale buildings from satellite remote sensing imagery is a crucial research area with significant implications for urban planning, infrastructure development, and population distribution analysis.","However, the task faces big challenges due to low-resolution overhead images acquired from high altitude space-borne platforms and the long-tail sample distribution of fine-grained urban building categories, leading to severe class imbalance problem.","To address these issues, we propose a deep network approach to fine-grained classification of urban buildings using open-access satellite images.","A Denoising Diffusion Probabilistic Model (DDPM) based super-resolution method is first introduced to enhance the spatial resolution of satellite images, which benefits from domain-adaptive knowledge distillation.","Then, a new fine-grained classification network with Category Information Balancing Module (CIBM) and Contrastive Supervision (CS) technique is proposed to mitigate the problem of class imbalance and improve the classification robustness and accuracy.","Experiments on Hong Kong data set with 11 fine building types revealed promising classification results with a mean Top-1 accuracy of 60.45\\%, which is on par with street-view image based approaches.","Extensive ablation study shows that CIBM and CS improve Top-1 accuracy by 2.6\\% and 3.5\\% compared to the baseline method, respectively.","And both modules can be easily inserted into other classification networks and similar enhancements have been achieved.","Our research contributes to the field of urban analysis by providing a practical solution for fine classification of buildings in challenging mega city scenarios solely using open-access satellite images.","The proposed method can serve as a valuable tool for urban planners, aiding in the understanding of economic, industrial, and population distribution."],"url":"http://arxiv.org/abs/2403.02132v1","category":"cs.CV"}
{"created":"2024-03-04 15:29:55","title":"A Dynamic Model of Integration","abstract":"Thomas Schelling introduced his agent-based model of segregation in 1971 and concluded that even when there is a low amount of intolerance within society that segregation will develop if people follow their individual preferences. A large body of literature building of this framework has been built and has bolstered this claim. This paper aims to take the same framework but instead look for ways to get to an integrated state. We focus on Allport's contact hypothesis that states that if there is equal status among groups, common goals among groups, and an institutional mechanism supporting intergroup contact then intergroup contact can reduce prejudice. We incorporate the contact hypothesis by having individuals adjust their intolerance based on their current neighborhood composition and the ease of conforming to their surroundings. Furthermore, we add in positive and negative media effects, as individuals are likely to get information about an outgroup from the media (e.g., news, TV, movies, etc.) that they consume. We find that having a society composed of individuals who do not easily conform to their surroundings and displaying positive examples of both groups in media promote integration within society.","sentences":["Thomas Schelling introduced his agent-based model of segregation in 1971 and concluded that even when there is a low amount of intolerance within society that segregation will develop if people follow their individual preferences.","A large body of literature building of this framework has been built and has bolstered this claim.","This paper aims to take the same framework but instead look for ways to get to an integrated state.","We focus on Allport's contact hypothesis that states that if there is equal status among groups, common goals among groups, and an institutional mechanism supporting intergroup contact then intergroup contact can reduce prejudice.","We incorporate the contact hypothesis by having individuals adjust their intolerance based on their current neighborhood composition and the ease of conforming to their surroundings.","Furthermore, we add in positive and negative media effects, as individuals are likely to get information about an outgroup from the media (e.g., news, TV, movies, etc.) that they consume.","We find that having a society composed of individuals who do not easily conform to their surroundings and displaying positive examples of both groups in media promote integration within society."],"url":"http://arxiv.org/abs/2403.02122v1","category":"nlin.AO"}
{"created":"2024-03-04 15:20:19","title":"Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks","abstract":"Machine learning (ML) is vulnerable to inference (e.g., membership inference, property inference, and data reconstruction) attacks that aim to infer the private information of training data or dataset. Existing defenses are only designed for one specific type of attack and sacrifice significant utility or are soon broken by adaptive attacks. We address these limitations by proposing an information-theoretic defense framework, called Inf2Guard, against the three major types of inference attacks. Our framework, inspired by the success of representation learning, posits that learning shared representations not only saves time/costs but also benefits numerous downstream tasks. Generally, Inf2Guard involves two mutual information objectives, for privacy protection and utility preservation, respectively. Inf2Guard exhibits many merits: it facilitates the design of customized objectives against the specific inference attack; it provides a general defense framework which can treat certain existing defenses as special cases; and importantly, it aids in deriving theoretical results, e.g., inherent utility-privacy tradeoff and guaranteed privacy leakage. Extensive evaluations validate the effectiveness of Inf2Guard for learning privacy-preserving representations against inference attacks and demonstrate the superiority over the baselines.","sentences":["Machine learning (ML) is vulnerable to inference (e.g., membership inference, property inference, and data reconstruction) attacks that aim to infer the private information of training data or dataset.","Existing defenses are only designed for one specific type of attack and sacrifice significant utility or are soon broken by adaptive attacks.","We address these limitations by proposing an information-theoretic defense framework, called Inf2Guard, against the three major types of inference attacks.","Our framework, inspired by the success of representation learning, posits that learning shared representations not only saves time/costs but also benefits numerous downstream tasks.","Generally, Inf2Guard involves two mutual information objectives, for privacy protection and utility preservation, respectively.","Inf2Guard exhibits many merits: it facilitates the design of customized objectives against the specific inference attack; it provides a general defense framework which can treat certain existing defenses as special cases; and importantly, it aids in deriving theoretical results, e.g., inherent utility-privacy tradeoff and guaranteed privacy leakage.","Extensive evaluations validate the effectiveness of Inf2Guard for learning privacy-preserving representations against inference attacks and demonstrate the superiority over the baselines."],"url":"http://arxiv.org/abs/2403.02116v1","category":"cs.LG"}
{"created":"2024-03-04 14:53:50","title":"Daedalus: Self-Adaptive Horizontal Autoscaling for Resource Efficiency of Distributed Stream Processing Systems","abstract":"Distributed Stream Processing (DSP) systems are capable of processing large streams of unbounded data, offering high throughput and low latencies. To maintain a stable Quality of Service (QoS), these systems require a sufficient allocation of resources. At the same time, over-provisioning can result in wasted energy and high operating costs. Therefore, to maximize resource utilization, autoscaling methods have been proposed that aim to efficiently match the resource allocation with the incoming workload. However, determining when and by how much to scale remains a significant challenge. Given the long-running nature of DSP jobs, scaling actions need to be executed at runtime, and to maintain a good QoS, they should be both accurate and infrequent. To address the challenges of autoscaling, the concept of self-adaptive systems is particularly fitting. These systems monitor themselves and their environment, adapting to changes with minimal need for expert involvement.   This paper introduces Daedalus, a self-adaptive manager for autoscaling in DSP systems, which draws on the principles of self-adaption to address the challenge of efficient autoscaling. Daedalus monitors a running DSP job and builds performance models, aiming to predict the maximum processing capacity at different scale-outs. When combined with time series forecasting to predict future workloads, Daedalus proactively scales DSP jobs, optimizing for maximum throughput and minimizing both latencies and resource usage. We conducted experiments using Apache Flink and Kafka Streams to evaluate the performance of Daedalus against two state-of-the-art approaches. Daedalus was able to achieve comparable latencies while reducing resource usage by up to 71%.","sentences":["Distributed Stream Processing (DSP) systems are capable of processing large streams of unbounded data, offering high throughput and low latencies.","To maintain a stable Quality of Service (QoS), these systems require a sufficient allocation of resources.","At the same time, over-provisioning can result in wasted energy and high operating costs.","Therefore, to maximize resource utilization, autoscaling methods have been proposed that aim to efficiently match the resource allocation with the incoming workload.","However, determining when and by how much to scale remains a significant challenge.","Given the long-running nature of DSP jobs, scaling actions need to be executed at runtime, and to maintain a good QoS, they should be both accurate and infrequent.","To address the challenges of autoscaling, the concept of self-adaptive systems is particularly fitting.","These systems monitor themselves and their environment, adapting to changes with minimal need for expert involvement.   ","This paper introduces Daedalus, a self-adaptive manager for autoscaling in DSP systems, which draws on the principles of self-adaption to address the challenge of efficient autoscaling.","Daedalus monitors a running DSP job and builds performance models, aiming to predict the maximum processing capacity at different scale-outs.","When combined with time series forecasting to predict future workloads, Daedalus proactively scales DSP jobs, optimizing for maximum throughput and minimizing both latencies and resource usage.","We conducted experiments using Apache Flink and Kafka Streams to evaluate the performance of Daedalus against two state-of-the-art approaches.","Daedalus was able to achieve comparable latencies while reducing resource usage by up to 71%."],"url":"http://arxiv.org/abs/2403.02093v2","category":"cs.DC"}
{"created":"2024-03-04 14:37:52","title":"Keep the bees off the trees: The particular vulnerability of species in the periphery of mutualistic networks to shock perturbations","abstract":"We study the phenomenon of multistability in mutualistic networks of plants and pollinators, where one desired state in which all species coexist competes with multiple states in which some species are gone extinct. In this setting, we examine the relation between the endangerment of pollinator species and their position within the mutualistic network. To this end, we compare endangerment rankings which are derived from the species' probabilities of going extinct due to random shock perturbations with rankings obtained from different network theoretic centrality metrics. We find that a pollinator's endangerment is strongly linked to its degree of mutualistic specialization and its position within the core-periphery structure of its mutualistic network, with the most endangered species being specialists in the outer periphery. Since particularly well established instances of such peripheral areas are tree-shaped structures which stem from links between nodes/species in the outermost shell of the network, we summarized our findings in the admittedly ambiguous slogan 'keep the bees off the trees'. Finally, we challenge the generality of our findings by testing whether the title of this work still applies when being located in the outer periphery allows pollinators to avoid competitive pressure.","sentences":["We study the phenomenon of multistability in mutualistic networks of plants and pollinators, where one desired state in which all species coexist competes with multiple states in which some species are gone extinct.","In this setting, we examine the relation between the endangerment of pollinator species and their position within the mutualistic network.","To this end, we compare endangerment rankings which are derived from the species' probabilities of going extinct due to random shock perturbations with rankings obtained from different network theoretic centrality metrics.","We find that a pollinator's endangerment is strongly linked to its degree of mutualistic specialization and its position within the core-periphery structure of its mutualistic network, with the most endangered species being specialists in the outer periphery.","Since particularly well established instances of such peripheral areas are tree-shaped structures which stem from links between nodes/species in the outermost shell of the network, we summarized our findings in the admittedly ambiguous slogan 'keep the bees off the trees'.","Finally, we challenge the generality of our findings by testing whether the title of this work still applies when being located in the outer periphery allows pollinators to avoid competitive pressure."],"url":"http://arxiv.org/abs/2403.02085v1","category":"nlin.AO"}
{"created":"2024-03-04 14:36:56","title":"ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models","abstract":"Recent advancement in text-to-image models (e.g., Stable Diffusion) and corresponding personalized technologies (e.g., DreamBooth and LoRA) enables individuals to generate high-quality and imaginative images. However, they often suffer from limitations when generating images with resolutions outside of their trained domain. To overcome this limitation, we present the Resolution Adapter (ResAdapter), a domain-consistent adapter designed for diffusion models to generate images with unrestricted resolutions and aspect ratios. Unlike other multi-resolution generation methods that process images of static resolution with complex post-process operations, ResAdapter directly generates images with the dynamical resolution. Especially, after learning a deep understanding of pure resolution priors, ResAdapter trained on the general dataset, generates resolution-free images with personalized diffusion models while preserving their original style domain. Comprehensive experiments demonstrate that ResAdapter with only 0.5M can process images with flexible resolutions for arbitrary diffusion models. More extended experiments demonstrate that ResAdapter is compatible with other modules (e.g., ControlNet, IP-Adapter and LCM-LoRA) for image generation across a broad range of resolutions, and can be integrated into other multi-resolution model (e.g., ElasticDiffusion) for efficiently generating higher-resolution images. Project link is https://res-adapter.github.io","sentences":["Recent advancement in text-to-image models (e.g., Stable Diffusion) and corresponding personalized technologies (e.g., DreamBooth and LoRA) enables individuals to generate high-quality and imaginative images.","However, they often suffer from limitations when generating images with resolutions outside of their trained domain.","To overcome this limitation, we present the Resolution Adapter (ResAdapter), a domain-consistent adapter designed for diffusion models to generate images with unrestricted resolutions and aspect ratios.","Unlike other multi-resolution generation methods that process images of static resolution with complex post-process operations, ResAdapter directly generates images with the dynamical resolution.","Especially, after learning a deep understanding of pure resolution priors, ResAdapter trained on the general dataset, generates resolution-free images with personalized diffusion models while preserving their original style domain.","Comprehensive experiments demonstrate that ResAdapter with only 0.5M can process images with flexible resolutions for arbitrary diffusion models.","More extended experiments demonstrate that ResAdapter is compatible with other modules (e.g., ControlNet, IP-Adapter and LCM-LoRA) for image generation across a broad range of resolutions, and can be integrated into other multi-resolution model (e.g., ElasticDiffusion) for efficiently generating higher-resolution images.","Project link is https://res-adapter.github.io"],"url":"http://arxiv.org/abs/2403.02084v1","category":"cs.CV"}
{"created":"2024-03-04 13:47:51","title":"Time-Reversal of Stochastic Maximum Principle","abstract":"Stochastic maximum principle (SMP) specifies a necessary condition for the solution of a stochastic optimal control problem. The condition involves a coupled system of forward and backward stochastic differential equations (FBSDE) for the state and the adjoint processes. Numerical solution of the FBSDE is challenging because the boundary condition of the adjoint process is specified at the terminal time, while the solution should be adaptable to the forward in time filtration of a Wiener process. In this paper, a \"time-reversal\" of the FBSDE system is proposed that involves integration with respect to a backward in time Wiener process. The time-reversal is used to propose an iterative Monte-Carlo procedure to solves the FBSDE system and its time-reversal simultaneously. The procedure involves approximating the {F\\\"ollmer's drift} and solving a regression problem between the state and its adjoint at each time. The procedure is illustrated for the linear quadratic (LQ) optimal control problem with a numerical example.","sentences":["Stochastic maximum principle (SMP) specifies a necessary condition for the solution of a stochastic optimal control problem.","The condition involves a coupled system of forward and backward stochastic differential equations (FBSDE) for the state and the adjoint processes.","Numerical solution of the FBSDE is challenging because the boundary condition of the adjoint process is specified at the terminal time, while the solution should be adaptable to the forward in time filtration of a Wiener process.","In this paper, a \"time-reversal\" of the FBSDE system is proposed that involves integration with respect to a backward in time Wiener process.","The time-reversal is used to propose an iterative Monte-Carlo procedure to solves the FBSDE system and its time-reversal simultaneously.","The procedure involves approximating the {F\\\"ollmer's drift} and solving a regression problem between the state and its adjoint at each time.","The procedure is illustrated for the linear quadratic (LQ) optimal control problem with a numerical example."],"url":"http://arxiv.org/abs/2403.02044v1","category":"eess.SY"}
{"created":"2024-03-04 13:26:48","title":"Heat and Work in Quantum Thermodynamics: a Cybernetic Approach","abstract":"We present a new proposal for distinguishing heat from work based on a control-theoretic observability decomposition. We derive a Hermitian operator representing instantaneous dissipation of observable energy, and suggest a generalization of the von-Neumann entropy which can account for the model-uncertainty also present in pure states if the measured observables are informationally incomplete. In this view, the transition from a fundamental to a thermodynamic model consists in mapping the fundamental density matrix to an effective one, generally of lower dimension, encoding only what is observable given the constraints of our sensor and actuator capabilities. The generalized entropy captures the information loss incurred in this mapping. The theory is illustrated for the central spin model, where we show that the application of external controls can increase the size of thermal fluctuations and lower the entropy.","sentences":["We present a new proposal for distinguishing heat from work based on a control-theoretic observability decomposition.","We derive a Hermitian operator representing instantaneous dissipation of observable energy, and suggest a generalization of the von-Neumann entropy which can account for the model-uncertainty also present in pure states if the measured observables are informationally incomplete.","In this view, the transition from a fundamental to a thermodynamic model consists in mapping the fundamental density matrix to an effective one, generally of lower dimension, encoding only what is observable given the constraints of our sensor and actuator capabilities.","The generalized entropy captures the information loss incurred in this mapping.","The theory is illustrated for the central spin model, where we show that the application of external controls can increase the size of thermal fluctuations and lower the entropy."],"url":"http://arxiv.org/abs/2403.02022v1","category":"quant-ph"}
{"created":"2024-03-04 13:12:02","title":"Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks","abstract":"We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological networks when applied to the Spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias.","sentences":["We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias.","We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection.","We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology.","By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process.","Finally, we show how our approach can change our understanding of ecological networks when applied to the Spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias."],"url":"http://arxiv.org/abs/2403.02011v1","category":"stat.ML"}
{"created":"2024-03-04 13:04:06","title":"Complexified Synchrony","abstract":"The Kuramoto model and its generalizations have been broadly employed to characterize and mechanistically understand various collective dynamical phenomena, especially the emergence of synchrony among coupled oscillators. Despite almost five decades of research, many questions remain open, in particular for finite-size systems. Here, we generalize recent work [Phys. Rev. Lett. 130, 187201 (2023)] on the finite-size Kuramoto model with its state variables analytically continued to the complex domain and also complexify its system parameters. Intriguingly, systems of two units with purely imaginary coupling do not actively synchronize even for arbitrarily large magnitudes of the coupling strengths, $|K| \\rightarrow \\infty$, but exhibit conservative dynamics with asynchronous rotations or librations for all $|K|$. For generic complex coupling, both, traditional phase-locked states and asynchronous states generalize to complex locked states, fixed points off the real subspace that exist even for arbitrarily weak coupling. We analyze a new collective mode of rotations exhibiting finite, yet arbitrarily large winding numbers. Numerical simulations for large networks indicate a novel form of discontinuous phase transition. We close by pointing to a range of exciting questions for future research.","sentences":["The Kuramoto model and its generalizations have been broadly employed to characterize and mechanistically understand various collective dynamical phenomena, especially the emergence of synchrony among coupled oscillators.","Despite almost five decades of research, many questions remain open, in particular for finite-size systems.","Here, we generalize recent work [Phys. Rev. Lett.","130, 187201 (2023)] on the finite-size Kuramoto model with its state variables analytically continued to the complex domain and also complexify its system parameters.","Intriguingly, systems of two units with purely imaginary coupling do not actively synchronize even for arbitrarily large magnitudes of the coupling strengths, $|K| \\rightarrow \\infty$, but exhibit conservative dynamics with asynchronous rotations or librations for all $|K|$. For generic complex coupling, both, traditional phase-locked states and asynchronous states generalize to complex locked states, fixed points off the real subspace that exist even for arbitrarily weak coupling.","We analyze a new collective mode of rotations exhibiting finite, yet arbitrarily large winding numbers.","Numerical simulations for large networks indicate a novel form of discontinuous phase transition.","We close by pointing to a range of exciting questions for future research."],"url":"http://arxiv.org/abs/2403.02006v1","category":"nlin.AO"}
{"created":"2024-03-04 12:36:25","title":"Skater: A Novel Bi-modal Bi-copter Robot for Adaptive Locomotion in Air and Diverse Terrain","abstract":"In this letter, we present a novel bi-modal bi-copter robot called Skater, which is adaptable to air and various ground surfaces. Skater consists of a bi-copter moving along its longitudinal direction with two passive wheels on both sides. Using longitudinally arranged bi-copter as the unified actuation system for both aerial and ground modes, this robot not only keeps concise and lightweight mechanism, but also possesses exceptional terrain traversing capability and strong steering capacity. Moreover, leveraging the vectored thrust characteristic of bi-copters, Skater can actively generate the centripetal force needed for steering, enabling it to achieve stable movement even on slippery surfaces. Furthermore, we model the comprehensive dynamics of Skater, analyze its differential flatness and introduce a controller using nonlinear model predictive control for trajectory tracking. The outstanding performance of the system is verified by extensive real-world experiments and benchmark comparisons.","sentences":["In this letter, we present a novel bi-modal bi-copter robot called Skater, which is adaptable to air and various ground surfaces.","Skater consists of a bi-copter moving along its longitudinal direction with two passive wheels on both sides.","Using longitudinally arranged bi-copter as the unified actuation system for both aerial and ground modes, this robot not only keeps concise and lightweight mechanism, but also possesses exceptional terrain traversing capability and strong steering capacity.","Moreover, leveraging the vectored thrust characteristic of bi-copters, Skater can actively generate the centripetal force needed for steering, enabling it to achieve stable movement even on slippery surfaces.","Furthermore, we model the comprehensive dynamics of Skater, analyze its differential flatness and introduce a controller using nonlinear model predictive control for trajectory tracking.","The outstanding performance of the system is verified by extensive real-world experiments and benchmark comparisons."],"url":"http://arxiv.org/abs/2403.01991v1","category":"cs.RO"}
{"created":"2024-03-04 12:20:29","title":"TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions","abstract":"Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our \"plug-and-play\" method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model. Then, it feeds the reconstruction of a corrupted image back to the pre-trained model. Finally, the pre-trained model does forward pass again to output action. Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation. The pre-trained navigation model with our top-down decoder significantly enhances navigation performance across almost all visual corruptions in our benchmarks. Our method improves the success rate of point-goal navigation from the state-of-the-art result of 46% to 94% on the most severe corruption. This suggests its potential for broader application in robotic visual navigation.","sentences":["Robot navigation under visual corruption presents a formidable challenge.","To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions.","Our \"plug-and-play\" method incorporates a top-down decoder to a pre-trained navigation model.","Firstly, the pre-trained navigation model gets a corrupted image and extracts features.","Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model.","Then, it feeds the reconstruction of a corrupted image back to the pre-trained model.","Finally, the pre-trained model does forward pass again to output action.","Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation.","The pre-trained navigation model with our top-down decoder significantly enhances navigation performance across almost all visual corruptions in our benchmarks.","Our method improves the success rate of point-goal navigation from the state-of-the-art result of 46% to 94% on the most severe corruption.","This suggests its potential for broader application in robotic visual navigation."],"url":"http://arxiv.org/abs/2403.01977v1","category":"cs.RO"}
{"created":"2024-03-04 11:44:40","title":"On the Challenges of Transforming UVL to IVML","abstract":"Software product line techniques encourage the reuse and adaptation of software components for creating customized products or software systems. These different product variants have commonalities and differences, which are managed by variability modeling. Over the past three decades, both academia and industry have developed numerous variability modeling methods, each with its own advantages and disadvantages. Many of these methods have demonstrated their utility within specific domains or applications. However, comprehending the capabilities and differences among these approaches to pinpoint the most suitable one for a particular use case remains challenging. Thus, new modeling techniques and tailored tools for handling variability are frequently created. Transitioning between variability models through transformations from different approaches can help in understanding the benefits and drawbacks of different modeling approaches. However, implementing such transformations presents challenges, such as semantic preservation and avoiding information loss. TRAVART is a tool that helps with transitioning between different approaches by enabling the transformation of variability models into other variability models of different types. This paper discusses the challenges for such transformations between UVL and IVML. It also presents a one-way transformation from the UVL to IVML with as little information loss as possible.","sentences":["Software product line techniques encourage the reuse and adaptation of software components for creating customized products or software systems.","These different product variants have commonalities and differences, which are managed by variability modeling.","Over the past three decades, both academia and industry have developed numerous variability modeling methods, each with its own advantages and disadvantages.","Many of these methods have demonstrated their utility within specific domains or applications.","However, comprehending the capabilities and differences among these approaches to pinpoint the most suitable one for a particular use case remains challenging.","Thus, new modeling techniques and tailored tools for handling variability are frequently created.","Transitioning between variability models through transformations from different approaches can help in understanding the benefits and drawbacks of different modeling approaches.","However, implementing such transformations presents challenges, such as semantic preservation and avoiding information loss.","TRAVART is a tool that helps with transitioning between different approaches by enabling the transformation of variability models into other variability models of different types.","This paper discusses the challenges for such transformations between UVL and IVML.","It also presents a one-way transformation from the UVL to IVML with as little information loss as possible."],"url":"http://arxiv.org/abs/2403.01952v1","category":"cs.SE"}
{"created":"2024-03-04 11:14:28","title":"Beneath and beyond frustrated total reflection: a practical demonstration","abstract":"Frustrated total internal reflection is analyzed from an unusual point of view Unlike most similar works, incident angles are used here as the scanning variable, instead of the tunneled film thickness. The theoretical framework is presented defining our own normalized variables, better adapted to the problem at hand. A straightforward and captivating experiment, appropriate for undergraduate classroom demonstrations is also presented. The experiment involves measuring the reflection and transmission of light through a pair of prisms separated by an air or water layer, and the results are in fair agreement with theory.","sentences":["Frustrated total internal reflection is analyzed from an unusual point of view Unlike most similar works, incident angles are used here as the scanning variable, instead of the tunneled film thickness.","The theoretical framework is presented defining our own normalized variables, better adapted to the problem at hand.","A straightforward and captivating experiment, appropriate for undergraduate classroom demonstrations is also presented.","The experiment involves measuring the reflection and transmission of light through a pair of prisms separated by an air or water layer, and the results are in fair agreement with theory."],"url":"http://arxiv.org/abs/2403.01938v1","category":"physics.optics"}
{"created":"2024-03-04 10:48:13","title":"Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?","abstract":"Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that supervised instruction tuning has the best trade-off between performance and resource requirements. As another contribution, we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages.","sentences":["Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning.","ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency.","Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance.","In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups.","Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs.","Our observations show that supervised instruction tuning has the best trade-off between performance and resource requirements.","As another contribution, we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages."],"url":"http://arxiv.org/abs/2403.01929v1","category":"cs.CL"}
{"created":"2024-03-04 10:26:22","title":"PowerSkel: A Device-Free Framework Using CSI Signal for Human Skeleton Estimation in Power Station","abstract":"Safety monitoring of power operations in power stations is crucial for preventing accidents and ensuring stable power supply. However, conventional methods such as wearable devices and video surveillance have limitations such as high cost, dependence on light, and visual blind spots. WiFi-based human pose estimation is a suitable method for monitoring power operations due to its low cost, device-free, and robustness to various illumination conditions.In this paper, a novel Channel State Information (CSI)-based pose estimation framework, namely PowerSkel, is developed to address these challenges. PowerSkel utilizes self-developed CSI sensors to form a mutual sensing network and constructs a CSI acquisition scheme specialized for power scenarios. It significantly reduces the deployment cost and complexity compared to the existing solutions. To reduce interference with CSI in the electricity scenario, a sparse adaptive filtering algorithm is designed to preprocess the CSI. CKDformer, a knowledge distillation network based on collaborative learning and self-attention, is proposed to extract the features from CSI and establish the mapping relationship between CSI and keypoints. The experiments are conducted in a real-world power station, and the results show that the PowerSkel achieves high performance with a PCK@50 of 96.27%, and realizes a significant visualization on pose estimation, even in dark environments. Our work provides a novel low-cost and high-precision pose estimation solution for power operation.","sentences":["Safety monitoring of power operations in power stations is crucial for preventing accidents and ensuring stable power supply.","However, conventional methods such as wearable devices and video surveillance have limitations such as high cost, dependence on light, and visual blind spots.","WiFi-based human pose estimation is a suitable method for monitoring power operations due to its low cost, device-free, and robustness to various illumination conditions.","In this paper, a novel Channel State Information (CSI)-based pose estimation framework, namely PowerSkel, is developed to address these challenges.","PowerSkel utilizes self-developed CSI sensors to form a mutual sensing network and constructs a CSI acquisition scheme specialized for power scenarios.","It significantly reduces the deployment cost and complexity compared to the existing solutions.","To reduce interference with CSI in the electricity scenario, a sparse adaptive filtering algorithm is designed to preprocess the CSI.","CKDformer, a knowledge distillation network based on collaborative learning and self-attention, is proposed to extract the features from CSI and establish the mapping relationship between CSI and keypoints.","The experiments are conducted in a real-world power station, and the results show that the PowerSkel achieves high performance with a PCK@50 of 96.27%, and realizes a significant visualization on pose estimation, even in dark environments.","Our work provides a novel low-cost and high-precision pose estimation solution for power operation."],"url":"http://arxiv.org/abs/2403.01913v1","category":"eess.SP"}
{"created":"2024-03-04 09:59:48","title":"FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio","abstract":"In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at https://github.com/modelscope/facechain.","sentences":["In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio.","Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency.","To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor.","Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames.","In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost.","Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm.","The codes will be released at https://github.com/modelscope/facechain."],"url":"http://arxiv.org/abs/2403.01901v1","category":"cs.CV"}
{"created":"2024-03-04 09:31:56","title":"ICLN: Input Convex Loss Network for Decision Focused Learning","abstract":"In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some inputs, while keeping the global structure for the other inputs. This enables ICLN to admit general DFL through only a single surrogate loss without any sense for choosing appropriate parametric forms. We confirm effectiveness and flexibility of ICLN by evaluating our proposed model with three stochastic decision-making problems.","sentences":["In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part.","Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task.","Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters.","Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss.","However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time.","In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm.","ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some inputs, while keeping the global structure for the other inputs.","This enables ICLN to admit general DFL through only a single surrogate loss without any sense for choosing appropriate parametric forms.","We confirm effectiveness and flexibility of ICLN by evaluating our proposed model with three stochastic decision-making problems."],"url":"http://arxiv.org/abs/2403.01875v1","category":"cs.LG"}
{"created":"2024-03-04 09:18:13","title":"AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes","abstract":"Indoor scenes we are living in are visually homogenous or textureless, while they inherently have structural forms and provide enough structural priors for 3D scene reconstruction. Motivated by this fact, we propose a structure-aware online signed distance fields (SDF) reconstruction framework in indoor scenes, especially under the Atlanta world (AW) assumption. Thus, we dub this incremental SDF reconstruction for AW as AiSDF. Within the online framework, we infer the underlying Atlanta structure of a given scene and then estimate planar surfel regions supporting the Atlanta structure. This Atlanta-aware surfel representation provides an explicit planar map for a given scene. In addition, based on these Atlanta planar surfel regions, we adaptively sample and constrain the structural regularity in the SDF reconstruction, which enables us to improve the reconstruction quality by maintaining a high-level structure while enhancing the details of a given scene. We evaluate the proposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate that the proposed framework is capable of reconstructing fine details of objects implicitly, as well as structures explicitly in room-scale scenes.","sentences":["Indoor scenes we are living in are visually homogenous or textureless, while they inherently have structural forms and provide enough structural priors for 3D scene reconstruction.","Motivated by this fact, we propose a structure-aware online signed distance fields (SDF) reconstruction framework in indoor scenes, especially under the Atlanta world (AW) assumption.","Thus, we dub this incremental SDF reconstruction for AW as AiSDF.","Within the online framework, we infer the underlying Atlanta structure of a given scene and then estimate planar surfel regions supporting the Atlanta structure.","This Atlanta-aware surfel representation provides an explicit planar map for a given scene.","In addition, based on these Atlanta planar surfel regions, we adaptively sample and constrain the structural regularity in the SDF reconstruction, which enables us to improve the reconstruction quality by maintaining a high-level structure while enhancing the details of a given scene.","We evaluate the proposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate that the proposed framework is capable of reconstructing fine details of objects implicitly, as well as structures explicitly in room-scale scenes."],"url":"http://arxiv.org/abs/2403.01861v1","category":"cs.RO"}
{"created":"2024-03-04 09:03:16","title":"PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis","abstract":"Recent advancements in large-scale pre-trained text-to-image models have led to remarkable progress in semantic image synthesis. Nevertheless, synthesizing high-quality images with consistent semantics and layout remains a challenge. In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE) that harnesses pre-trained models to alleviate the aforementioned issues. Specifically, we first employ the layout control map to faithfully represent layouts in the feature space. Subsequently, we combine the layout and semantic features in a timestep-adaptive manner to synthesize images with realistic details. During fine-tuning, we propose the Semantic Alignment (SA) loss to further enhance layout alignment. Additionally, we introduce the Layout-Free Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the priors of pre-trained models, thereby improving the visual quality and semantic consistency of synthesized images. Extensive experiments demonstrate that our approach performs favorably in terms of visual quality, semantic consistency, and layout alignment. The source code and model are available at https://github.com/cszy98/PLACE/tree/main.","sentences":["Recent advancements in large-scale pre-trained text-to-image models have led to remarkable progress in semantic image synthesis.","Nevertheless, synthesizing high-quality images with consistent semantics and layout remains a challenge.","In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE) that harnesses pre-trained models to alleviate the aforementioned issues.","Specifically, we first employ the layout control map to faithfully represent layouts in the feature space.","Subsequently, we combine the layout and semantic features in a timestep-adaptive manner to synthesize images with realistic details.","During fine-tuning, we propose the Semantic Alignment (SA) loss to further enhance layout alignment.","Additionally, we introduce the Layout-Free Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the priors of pre-trained models, thereby improving the visual quality and semantic consistency of synthesized images.","Extensive experiments demonstrate that our approach performs favorably in terms of visual quality, semantic consistency, and layout alignment.","The source code and model are available at https://github.com/cszy98/PLACE/tree/main."],"url":"http://arxiv.org/abs/2403.01852v1","category":"cs.CV"}
{"created":"2024-03-04 09:01:10","title":"Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral","abstract":"Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning. Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on downstream tasks. Our resources are publicly available through \\url{https://github.com/ymcui/Chinese-Mixtral}.","sentences":["Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance.","Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning.","Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities.","Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis.","We also present the visualizations of each expert to examine their importance on downstream tasks.","Our resources are publicly available through \\url{https://github.com/ymcui/Chinese-Mixtral}."],"url":"http://arxiv.org/abs/2403.01851v1","category":"cs.CL"}
{"created":"2024-03-04 08:59:32","title":"One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models","abstract":"Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.","sentences":["Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples.","This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work).","We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt.","Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs.","The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient.","Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods.","APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets.","Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively.","The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness.","Code is available at https://github.com/TreeLLi/APT."],"url":"http://arxiv.org/abs/2403.01849v1","category":"cs.CV"}
{"created":"2024-03-04 08:38:15","title":"FreeA: Human-object Interaction Detection using Free Annotation Labels","abstract":"Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing and classifying the interactive actions than the newest weakly model, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively. Code will be available at https://drliuqi.github.io/.","sentences":["Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets.","In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels.","To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions.","In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels.","Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models.","Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing and classifying the interactive actions than the newest weakly model, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively.","Code will be available at https://drliuqi.github.io/."],"url":"http://arxiv.org/abs/2403.01840v1","category":"cs.CV"}
{"created":"2024-03-04 08:19:11","title":"A Novel Shortest Path Query Algorithm Based on Optimized Adaptive Topology Structure","abstract":"Urban rail transit is a fundamental component of public transportation, however, commonly station-based path search algorithms often overlook the impact of transfer times on search results, leading to decreased accuracy. To solve this problem, this paper proposes a novel shortest path query algorithm based on adaptive topology optimization called the Adaptive Topology Extension Road Network Structure (ATEN). This algorithm categorizes transfer stations into different types and treats travel time and transfer time equivalently as weights for edges in the topological graph. The proposed algorithm introduces virtual stations to differentiate between pedestrian paths and train paths, eliminating the need for additional operations on transfer stations. The algorithm controls the extent of expansion in the urban rail transit topology, overcoming query errors caused by mishandling of transfer stations in the existing algorithm. Finally, a series of simulation experiments were conducted on Beijing's urban rail transit network to validate both correctness and efficiency of the proposed adaptive topology optimization algorithm. The results demonstrate significant advantages compared to existing similar algorithms.","sentences":["Urban rail transit is a fundamental component of public transportation, however, commonly station-based path search algorithms often overlook the impact of transfer times on search results, leading to decreased accuracy.","To solve this problem, this paper proposes a novel shortest path query algorithm based on adaptive topology optimization called the Adaptive Topology Extension Road Network Structure (ATEN).","This algorithm categorizes transfer stations into different types and treats travel time and transfer time equivalently as weights for edges in the topological graph.","The proposed algorithm introduces virtual stations to differentiate between pedestrian paths and train paths, eliminating the need for additional operations on transfer stations.","The algorithm controls the extent of expansion in the urban rail transit topology, overcoming query errors caused by mishandling of transfer stations in the existing algorithm.","Finally, a series of simulation experiments were conducted on Beijing's urban rail transit network to validate both correctness and efficiency of the proposed adaptive topology optimization algorithm.","The results demonstrate significant advantages compared to existing similar algorithms."],"url":"http://arxiv.org/abs/2403.01826v1","category":"cs.CE"}
{"created":"2024-03-04 08:14:32","title":"Late time cosmological solutions in $f(R,T)$ gravity in a viscous Universe","abstract":"Considering the condition on conservation of energy momentum tensor (EMT), we study late time cosmological solutions in the context of $f(R,T)=R+\\alpha T^{n}$ gravity (where $\\alpha$ and $n$ are constants) in a flat FLRW spacetime. The present model discusses the case of a barotropic perfect fluid as the matter content of the Universe, along with the case when dissipative effects are taken into account. Briefly, assuming a single perfect fluid we find that the mentioned model is not capable of presenting an observationally consistent picture of the late time accelerated expansion of the Universe; nevertheless, the model leads to admissible solutions when the bulk viscosity is included. In this regard, a consistent setting is found considering the Eckart, Truncated and Full Israel-Stewart theories which determine the behavior of bulk viscosity. In the presence of bulk viscosity, the behavior of the deceleration parameter (DP) shows that the underlying model can describe an acceptable evolution even if the isotropic pressure of matter content of the Universe is negligible.","sentences":["Considering the condition on conservation of energy momentum tensor (EMT), we study late time cosmological solutions in the context of $f(R,T)=R+\\alpha T^{n}$ gravity (where $\\alpha$ and $n$ are constants) in a flat FLRW spacetime.","The present model discusses the case of a barotropic perfect fluid as the matter content of the Universe, along with the case when dissipative effects are taken into account.","Briefly, assuming a single perfect fluid we find that the mentioned model is not capable of presenting an observationally consistent picture of the late time accelerated expansion of the Universe; nevertheless, the model leads to admissible solutions when the bulk viscosity is included.","In this regard, a consistent setting is found considering the Eckart, Truncated and Full Israel-Stewart theories which determine the behavior of bulk viscosity.","In the presence of bulk viscosity, the behavior of the deceleration parameter (DP) shows that the underlying model can describe an acceptable evolution even if the isotropic pressure of matter content of the Universe is negligible."],"url":"http://arxiv.org/abs/2403.02364v1","category":"gr-qc"}
{"created":"2024-03-04 08:10:42","title":"Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations","abstract":"We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network (MA-APNN) method to solve the time-dependent linear radiative transfer equations (LRTEs), which have a multi-scale nature and high dimensionality. To achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework and design a new adaptive exponentially weighted Asymptotic-Preserving (AP) loss function, which incorporates the macroscopic auxiliary equation that is derived from the original transfer equation directly and explicitly contains the information of the diffusion limit equation. Thus, as the scale parameter tends to zero, the loss function gradually transitions from the transport state to the diffusion limit state. In addition, the initial data, boundary conditions, and conservation laws serve as the regularization terms for the loss. We present several numerical examples to demonstrate the effectiveness of MA-APNNs.","sentences":["We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network (MA-APNN) method to solve the time-dependent linear radiative transfer equations (LRTEs), which have a multi-scale nature and high dimensionality.","To achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework and design a new adaptive exponentially weighted Asymptotic-Preserving (AP) loss function, which incorporates the macroscopic auxiliary equation that is derived from the original transfer equation directly and explicitly contains the information of the diffusion limit equation.","Thus, as the scale parameter tends to zero, the loss function gradually transitions from the transport state to the diffusion limit state.","In addition, the initial data, boundary conditions, and conservation laws serve as the regularization terms for the loss.","We present several numerical examples to demonstrate the effectiveness of MA-APNNs."],"url":"http://arxiv.org/abs/2403.01820v1","category":"math.NA"}
{"created":"2024-03-04 08:04:41","title":"SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition","abstract":"Instead of making behavioral decisions directly from the exponentially expanding joint observational-action space, subtask-based multi-agent reinforcement learning (MARL) methods enable agents to learn how to tackle different subtasks. Most existing subtask-based MARL methods are based on hierarchical reinforcement learning (HRL). However, these approaches often limit the number of subtasks, perform subtask recognition periodically, and can only identify and execute a specific subtask within the predefined fixed time period, which makes them inflexible and not suitable for diverse and dynamic scenarios with constantly changing subtasks. To break through above restrictions, a \\textbf{S}liding \\textbf{M}ultidimensional t\\textbf{A}sk window based m\\textbf{U}ti-agent reinforcement learnin\\textbf{G} framework (SMAUG) is proposed for adaptive real-time subtask recognition. It leverages a sliding multidimensional task window to extract essential information of subtasks from trajectory segments concatenated based on observed and predicted trajectories in varying lengths. An inference network is designed to iteratively predict future trajectories with the subtask-oriented policy network. Furthermore, intrinsic motivation rewards are defined to promote subtask exploration and behavior diversity. SMAUG can be integrated with any Q-learning-based approach. Experiments on StarCraft II show that SMAUG not only demonstrates performance superiority in comparison with all baselines but also presents a more prominent and swift rise in rewards during the initial training stage.","sentences":["Instead of making behavioral decisions directly from the exponentially expanding joint observational-action space, subtask-based multi-agent reinforcement learning (MARL) methods enable agents to learn how to tackle different subtasks.","Most existing subtask-based MARL methods are based on hierarchical reinforcement learning (HRL).","However, these approaches often limit the number of subtasks, perform subtask recognition periodically, and can only identify and execute a specific subtask within the predefined fixed time period, which makes them inflexible and not suitable for diverse and dynamic scenarios with constantly changing subtasks.","To break through above restrictions, a \\textbf{S}liding \\textbf{M}ultidimensional t\\textbf{A}sk window based m\\textbf{U}ti-agent reinforcement learnin\\textbf{G} framework (SMAUG) is proposed for adaptive real-time subtask recognition.","It leverages a sliding multidimensional task window to extract essential information of subtasks from trajectory segments concatenated based on observed and predicted trajectories in varying lengths.","An inference network is designed to iteratively predict future trajectories with the subtask-oriented policy network.","Furthermore, intrinsic motivation rewards are defined to promote subtask exploration and behavior diversity.","SMAUG can be integrated with any Q-learning-based approach.","Experiments on StarCraft II show that SMAUG not only demonstrates performance superiority in comparison with all baselines but also presents a more prominent and swift rise in rewards during the initial training stage."],"url":"http://arxiv.org/abs/2403.01816v1","category":"cs.AI"}
{"created":"2024-03-04 08:00:20","title":"A Simple Baseline for Efficient Hand Mesh Reconstruction","abstract":"3D hand pose estimation has found broad application in areas such as gesture recognition and human-machine interaction tasks. As performance improves, the complexity of the systems also increases, which can limit the comparative analysis and practical implementation of these methods. In this paper, we propose a simple yet effective baseline that not only surpasses state-of-the-art (SOTA) methods but also demonstrates computational efficiency. To establish this baseline, we abstract existing work into two components: a token generator and a mesh regressor, and then examine their core structures. A core structure, in this context, is one that fulfills intrinsic functions, brings about significant improvements, and achieves excellent performance without unnecessary complexities. Our proposed approach is decoupled from any modifications to the backbone, making it adaptable to any modern models. Our method outperforms existing solutions, achieving state-of-the-art (SOTA) results across multiple datasets. On the FreiHAND dataset, our approach produced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm. Similarly, on the Dexycb dataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm. As for performance speed, our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36","sentences":["3D hand pose estimation has found broad application in areas such as gesture recognition and human-machine interaction tasks.","As performance improves, the complexity of the systems also increases, which can limit the comparative analysis and practical implementation of these methods.","In this paper, we propose a simple yet effective baseline that not only surpasses state-of-the-art (SOTA) methods but also demonstrates computational efficiency.","To establish this baseline, we abstract existing work into two components: a token generator and a mesh regressor, and then examine their core structures.","A core structure, in this context, is one that fulfills intrinsic functions, brings about significant improvements, and achieves excellent performance without unnecessary complexities.","Our proposed approach is decoupled from any modifications to the backbone, making it adaptable to any modern models.","Our method outperforms existing solutions, achieving state-of-the-art (SOTA) results across multiple datasets.","On the FreiHAND dataset, our approach produced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm.","Similarly, on the Dexycb dataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm.","As for performance speed, our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36"],"url":"http://arxiv.org/abs/2403.01813v1","category":"cs.CV"}
{"created":"2024-03-04 07:47:07","title":"SAQIEL: Ultra-Light and Safe Manipulator with Passive 3D Wire Alignment Mechanism","abstract":"Improving the safety of collaborative manipulators necessitates the reduction of inertia in the moving part. Within this paper, we introduce a novel approach in the form of a passive 3D wire aligner, serving as a lightweight and low-friction power transmission mechanism, thus achieving the desired low inertia in the manipulator's operation. Through the utilization of this innovation, the consolidation of hefty actuators onto the root link becomes feasible, consequently enabling a supple drive characterized by minimal friction. To demonstrate the efficacy of this device, we fabricate an ultralight 7 degrees of freedom (DoF) manipulator named SAQIEL, boasting a mere 1.5 kg weight for its moving components. Notably, to mitigate friction within SAQIEL's actuation system, we employ a distinctive mechanism that directly winds wires using motors, obviating the need for traditional gear or belt-based speed reduction mechanisms. Through a series of empirical trials, we substantiate that SAQIEL adeptly strikes balance between lightweight design, substantial payload capacity, elevated velocity, precision, and adaptability.","sentences":["Improving the safety of collaborative manipulators necessitates the reduction of inertia in the moving part.","Within this paper, we introduce a novel approach in the form of a passive 3D wire aligner, serving as a lightweight and low-friction power transmission mechanism, thus achieving the desired low inertia in the manipulator's operation.","Through the utilization of this innovation, the consolidation of hefty actuators onto the root link becomes feasible, consequently enabling a supple drive characterized by minimal friction.","To demonstrate the efficacy of this device, we fabricate an ultralight 7 degrees of freedom (DoF) manipulator named SAQIEL, boasting a mere 1.5 kg weight for its moving components.","Notably, to mitigate friction within SAQIEL's actuation system, we employ a distinctive mechanism that directly winds wires using motors, obviating the need for traditional gear or belt-based speed reduction mechanisms.","Through a series of empirical trials, we substantiate that SAQIEL adeptly strikes balance between lightweight design, substantial payload capacity, elevated velocity, precision, and adaptability."],"url":"http://arxiv.org/abs/2403.01803v1","category":"cs.RO"}
{"created":"2024-03-04 07:45:29","title":"COLA: Cross-city Mobility Transformer for Human Trajectory Simulation","abstract":"Human trajectory data produced by daily mobile devices has proven its usefulness in various substantial fields such as urban planning and epidemic prevention. In terms of the individual privacy concern, human trajectory simulation has attracted increasing attention from researchers, targeting at offering numerous realistic mobility data for downstream tasks. Nevertheless, the prevalent issue of data scarcity undoubtedly degrades the reliability of existing deep learning models. In this paper, we are motivated to explore the intriguing problem of mobility transfer across cities, grasping the universal patterns of human trajectories to augment the powerful Transformer with external mobility data. There are two crucial challenges arising in the knowledge transfer across cities: 1) how to transfer the Transformer to adapt for domain heterogeneity; 2) how to calibrate the Transformer to adapt for subtly different long-tail frequency distributions of locations. To address these challenges, we have tailored a Cross-city mObiLity trAnsformer (COLA) with a dedicated model-agnostic transfer framework by effectively transferring cross-city knowledge for human trajectory simulation. Firstly, COLA divides the Transformer into the private modules for city-specific characteristics and the shared modules for city-universal mobility patterns. Secondly, COLA leverages a lightweight yet effective post-hoc adjustment strategy for trajectory simulation, without disturbing the complex bi-level optimization of model-agnostic knowledge transfer. Extensive experiments of COLA compared to state-of-the-art single-city baselines and our implemented cross-city baselines have demonstrated its superiority and effectiveness. The code is available at https://github.com/Star607/Cross-city-Mobility-Transformer.","sentences":["Human trajectory data produced by daily mobile devices has proven its usefulness in various substantial fields such as urban planning and epidemic prevention.","In terms of the individual privacy concern, human trajectory simulation has attracted increasing attention from researchers, targeting at offering numerous realistic mobility data for downstream tasks.","Nevertheless, the prevalent issue of data scarcity undoubtedly degrades the reliability of existing deep learning models.","In this paper, we are motivated to explore the intriguing problem of mobility transfer across cities, grasping the universal patterns of human trajectories to augment the powerful Transformer with external mobility data.","There are two crucial challenges arising in the knowledge transfer across cities: 1) how to transfer the Transformer to adapt for domain heterogeneity; 2) how to calibrate the Transformer to adapt for subtly different long-tail frequency distributions of locations.","To address these challenges, we have tailored a Cross-city mObiLity trAnsformer (COLA) with a dedicated model-agnostic transfer framework by effectively transferring cross-city knowledge for human trajectory simulation.","Firstly, COLA divides the Transformer into the private modules for city-specific characteristics and the shared modules for city-universal mobility patterns.","Secondly, COLA leverages a lightweight yet effective post-hoc adjustment strategy for trajectory simulation, without disturbing the complex bi-level optimization of model-agnostic knowledge transfer.","Extensive experiments of COLA compared to state-of-the-art single-city baselines and our implemented cross-city baselines have demonstrated its superiority and effectiveness.","The code is available at https://github.com/Star607/Cross-city-Mobility-Transformer."],"url":"http://arxiv.org/abs/2403.01801v1","category":"cs.LG"}
{"created":"2024-03-04 07:41:50","title":"AtomoVideo: High Fidelity Image-to-Video Generation","abstract":"Recently, video generation has achieved significant rapid development based on superior text-to-image generation techniques. In this work, we propose a high fidelity framework for image-to-video generation, named AtomoVideo. Based on multi-granularity image injection, we achieve higher fidelity of the generated video to the given image. In addition, thanks to high quality datasets and training strategies, we achieve greater motion intensity while maintaining superior temporal consistency and stability. Our architecture extends flexibly to the video frame prediction task, enabling long sequence prediction through iterative generation. Furthermore, due to the design of adapter training, our approach can be well combined with existing personalized models and controllable modules. By quantitatively and qualitatively evaluation, AtomoVideo achieves superior results compared to popular methods, more examples can be found on our project website: https://atomo-video.github.io/.","sentences":["Recently, video generation has achieved significant rapid development based on superior text-to-image generation techniques.","In this work, we propose a high fidelity framework for image-to-video generation, named AtomoVideo.","Based on multi-granularity image injection, we achieve higher fidelity of the generated video to the given image.","In addition, thanks to high quality datasets and training strategies, we achieve greater motion intensity while maintaining superior temporal consistency and stability.","Our architecture extends flexibly to the video frame prediction task, enabling long sequence prediction through iterative generation.","Furthermore, due to the design of adapter training, our approach can be well combined with existing personalized models and controllable modules.","By quantitatively and qualitatively evaluation, AtomoVideo achieves superior results compared to popular methods, more examples can be found on our project website: https://atomo-video.github.io/."],"url":"http://arxiv.org/abs/2403.01800v2","category":"cs.CV"}
{"created":"2024-03-04 07:32:28","title":"Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making","abstract":"Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing recommendations that the AI deems correct. However, recent studies suggest this may diminish human analytical thinking and lead to humans' inappropriate reliance on AI, impairing the synergy in human-AI teams. In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking. This diversity of roles has not yet been empirically explored in AI assistance. In this paper, we examine three AI roles: Recommender, Analyzer, and Devil's Advocate, and evaluate their effects across two AI performance levels. Our results show each role's distinct strengths and limitations in task performance, reliance appropriateness, and user experience. Notably, the Recommender role is not always the most effective, especially if the AI performance level is low, the Analyzer role may be preferable. These insights offer valuable implications for designing AI assistants with adaptive functional roles according to different situations.","sentences":["Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing recommendations that the AI deems correct.","However, recent studies suggest this may diminish human analytical thinking and lead to humans' inappropriate reliance on AI, impairing the synergy in human-AI teams.","In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking.","This diversity of roles has not yet been empirically explored in AI assistance.","In this paper, we examine three AI roles: Recommender, Analyzer, and Devil's Advocate, and evaluate their effects across two AI performance levels.","Our results show each role's distinct strengths and limitations in task performance, reliance appropriateness, and user experience.","Notably, the Recommender role is not always the most effective, especially if the AI performance level is low, the Analyzer role may be preferable.","These insights offer valuable implications for designing AI assistants with adaptive functional roles according to different situations."],"url":"http://arxiv.org/abs/2403.01791v1","category":"cs.HC"}
{"created":"2024-03-04 07:21:07","title":"Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning","abstract":"In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.","sentences":["In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis.","Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT).","Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost.","Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework.","This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures.","We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences.","Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer.","The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module."],"url":"http://arxiv.org/abs/2403.01781v1","category":"cs.CV"}
{"created":"2024-03-04 07:18:05","title":"Graph neural network for in-network placement of real-time metaverse tasks in next-generation network","abstract":"This study addresses the challenge of real-time metaverse applications by proposing an in-network placement and task-offloading solution for delay-constrained computing tasks in next-generation networks. The metaverse, envisioned as a parallel virtual world, requires seamless real-time experiences across diverse applications. The study introduces a software-defined networking (SDN)-based architecture and employs graph neural network (GNN) techniques for intelligent and adaptive task allocation in in-network computing (INC). Considering time constraints and computing capabilities, the proposed model optimally decides whether to offload rendering tasks to INC nodes or edge server. Extensive experiments demonstrate the superior performance of the proposed GNN model, achieving 97% accuracy compared to 72% for multilayer perceptron (MLP) and 70% for decision trees (DTs). The study fills the research gap in in-network placement for real-time metaverse applications, offering insights into efficient rendering task handling.","sentences":["This study addresses the challenge of real-time metaverse applications by proposing an in-network placement and task-offloading solution for delay-constrained computing tasks in next-generation networks.","The metaverse, envisioned as a parallel virtual world, requires seamless real-time experiences across diverse applications.","The study introduces a software-defined networking (SDN)-based architecture and employs graph neural network (GNN) techniques for intelligent and adaptive task allocation in in-network computing (INC).","Considering time constraints and computing capabilities, the proposed model optimally decides whether to offload rendering tasks to INC nodes or edge server.","Extensive experiments demonstrate the superior performance of the proposed GNN model, achieving 97% accuracy compared to 72% for multilayer perceptron (MLP) and 70% for decision trees (DTs).","The study fills the research gap in in-network placement for real-time metaverse applications, offering insights into efficient rendering task handling."],"url":"http://arxiv.org/abs/2403.01780v1","category":"cs.NI"}
{"created":"2024-03-04 07:09:54","title":"Hybrid data-driven and physics-informed regularized learning of cyclic plasticity with Neural Networks","abstract":"An extendable, efficient and explainable Machine Learning approach is proposed to represent cyclic plasticity and replace conventional material models based on the Radial Return Mapping algorithm. High accuracy and stability by means of a limited amount of training data is achieved by implementing physics-informed regularizations and the back stress information. The off-loading of the Neural Network is applied to the maximal extent. The proposed model architecture is simpler and more efficient compared to existing solutions from the literature, while representing a complete three-dimensional material model. The validation of the approach is carried out by means of surrogate data obtained with the Armstrong-Frederick kinematic hardening model. The Mean Squared Error is assumed as the loss function which stipulates several restrictions: deviatoric character of internal variables, compliance with the flow rule, the differentiation of elastic and plastic steps and the associativity of the flow rule. The latter, however, has a minor impact on the accuracy, which implies the generalizability of the model for a broad spectrum of evolution laws for internal variables. Numerical tests simulating several load cases are shown in detail and validated for accuracy and stability.","sentences":["An extendable, efficient and explainable Machine Learning approach is proposed to represent cyclic plasticity and replace conventional material models based on the Radial Return Mapping algorithm.","High accuracy and stability by means of a limited amount of training data is achieved by implementing physics-informed regularizations and the back stress information.","The off-loading of the Neural Network is applied to the maximal extent.","The proposed model architecture is simpler and more efficient compared to existing solutions from the literature, while representing a complete three-dimensional material model.","The validation of the approach is carried out by means of surrogate data obtained with the Armstrong-Frederick kinematic hardening model.","The Mean Squared Error is assumed as the loss function which stipulates several restrictions: deviatoric character of internal variables, compliance with the flow rule, the differentiation of elastic and plastic steps and the associativity of the flow rule.","The latter, however, has a minor impact on the accuracy, which implies the generalizability of the model for a broad spectrum of evolution laws for internal variables.","Numerical tests simulating several load cases are shown in detail and validated for accuracy and stability."],"url":"http://arxiv.org/abs/2403.01776v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-04 06:24:24","title":"AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network","abstract":"Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward. However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The counterfactual reasoning architecture is constructed by adaptive forward and backward transformer generative adversarial network (AFBT GAN), which is specifically designed by network property in FC and inverse patch embedding operation in the transformer. The specific design can make the model focus more on the current network correlation and employ the global insight of the transformer to reconstruct FC, which both help the generation of high-quality target label FC. The validation experiments are conducted on both clinical and public datasets, the generated attention map are both vital correlated to cognitive function and the diagnostic performance is also significant. The code is available at https://github.com/SXR3015/AFBT-GAN.","sentences":["Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward.","However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training.","To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step.","To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC.","The counterfactual reasoning architecture is constructed by adaptive forward and backward transformer generative adversarial network (AFBT GAN), which is specifically designed by network property in FC and inverse patch embedding operation in the transformer.","The specific design can make the model focus more on the current network correlation and employ the global insight of the transformer to reconstruct FC, which both help the generation of high-quality target label FC.","The validation experiments are conducted on both clinical and public datasets, the generated attention map are both vital correlated to cognitive function and the diagnostic performance is also significant.","The code is available at https://github.com/SXR3015/AFBT-GAN."],"url":"http://arxiv.org/abs/2403.01758v1","category":"eess.IV"}
{"created":"2024-03-04 06:20:31","title":"Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models","abstract":"Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings. In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based parameter-efficient tuning and derivative-free optimization methods in few-shot settings.","sentences":["Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters.","However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model.","Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings.","In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately.","Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based parameter-efficient tuning and derivative-free optimization methods in few-shot settings."],"url":"http://arxiv.org/abs/2403.01754v1","category":"cs.CL"}
{"created":"2024-03-04 06:19:27","title":"Training-Free Pretrained Model Merging","abstract":"Recently, model merging techniques have surfaced as a solution to combine multiple single-talent models into a single multi-talent model. However, previous endeavors in this field have either necessitated additional training or fine-tuning processes, or require that the models possess the same pre-trained initialization. In this work, we identify a common drawback in prior works w.r.t. the inconsistency of unit similarity in the weight space and the activation space. To address this inconsistency, we propose an innovative model merging framework, coined as merging under dual-space constraints (MuDSC). Specifically, instead of solely maximizing the objective of a single space, we advocate for the exploration of permutation matrices situated in a region with a unified high similarity in the dual space, achieved through the linear combination of activation and weight similarity matrices. In order to enhance usability, we have also incorporated adaptations for group structure, including Multi-Head Attention and Group Normalization. Comprehensive experimental comparisons demonstrate that MuDSC can significantly boost the performance of merged models with various task combinations and architectures. Furthermore, the visualization of the merged model within the multi-task loss landscape reveals that MuDSC enables the merged model to reside in the overlapping segment, featuring a unified lower loss for each task. Our code is publicly available at https://github.com/zju-vipa/training_free_model_merging.","sentences":["Recently, model merging techniques have surfaced as a solution to combine multiple single-talent models into a single multi-talent model.","However, previous endeavors in this field have either necessitated additional training or fine-tuning processes, or require that the models possess the same pre-trained initialization.","In this work, we identify a common drawback in prior works w.r.t.","the inconsistency of unit similarity in the weight space and the activation space.","To address this inconsistency, we propose an innovative model merging framework, coined as merging under dual-space constraints (MuDSC).","Specifically, instead of solely maximizing the objective of a single space, we advocate for the exploration of permutation matrices situated in a region with a unified high similarity in the dual space, achieved through the linear combination of activation and weight similarity matrices.","In order to enhance usability, we have also incorporated adaptations for group structure, including Multi-Head Attention and Group Normalization.","Comprehensive experimental comparisons demonstrate that MuDSC can significantly boost the performance of merged models with various task combinations and architectures.","Furthermore, the visualization of the merged model within the multi-task loss landscape reveals that MuDSC enables the merged model to reside in the overlapping segment, featuring a unified lower loss for each task.","Our code is publicly available at https://github.com/zju-vipa/training_free_model_merging."],"url":"http://arxiv.org/abs/2403.01753v1","category":"cs.CV"}
{"created":"2024-03-04 05:31:29","title":"ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution","abstract":"Spatiotemporal (ST) learning has become a crucial technique to enable smart cities and sustainable urban development. Current ST learning models capture the heterogeneity via various spatial convolution and temporal evolution blocks. However, rapid urbanization leads to fluctuating distributions in urban data and city structures over short periods, resulting in existing methods suffering generalization and data adaptation issues. Despite efforts, existing methods fail to deal with newly arrived observations and those methods with generalization capacity are limited in repeated training. Motivated by complementary learning in neuroscience, we introduce a prompt-based complementary spatiotemporal learning termed ComS2T, to empower the evolution of models for data adaptation. ComS2T partitions the neural architecture into a stable neocortex for consolidating historical memory and a dynamic hippocampus for new knowledge update. We first disentangle two disjoint structures into stable and dynamic weights, and then train spatial and temporal prompts by characterizing distribution of main observations to enable prompts adaptive to new data. This data-adaptive prompt mechanism, combined with a two-stage training process, facilitates fine-tuning of the neural architecture conditioned on prompts, thereby enabling efficient adaptation during testing. Extensive experiments validate the efficacy of ComS2T in adapting to various spatiotemporal out-of-distribution scenarios while maintaining efficient inference capabilities.","sentences":["Spatiotemporal (ST) learning has become a crucial technique to enable smart cities and sustainable urban development.","Current ST learning models capture the heterogeneity via various spatial convolution and temporal evolution blocks.","However, rapid urbanization leads to fluctuating distributions in urban data and city structures over short periods, resulting in existing methods suffering generalization and data adaptation issues.","Despite efforts, existing methods fail to deal with newly arrived observations and those methods with generalization capacity are limited in repeated training.","Motivated by complementary learning in neuroscience, we introduce a prompt-based complementary spatiotemporal learning termed ComS2T, to empower the evolution of models for data adaptation.","ComS2T partitions the neural architecture into a stable neocortex for consolidating historical memory and a dynamic hippocampus for new knowledge update.","We first disentangle two disjoint structures into stable and dynamic weights, and then train spatial and temporal prompts by characterizing distribution of main observations to enable prompts adaptive to new data.","This data-adaptive prompt mechanism, combined with a two-stage training process, facilitates fine-tuning of the neural architecture conditioned on prompts, thereby enabling efficient adaptation during testing.","Extensive experiments validate the efficacy of ComS2T in adapting to various spatiotemporal out-of-distribution scenarios while maintaining efficient inference capabilities."],"url":"http://arxiv.org/abs/2403.01738v1","category":"cs.LG"}
{"created":"2024-03-04 05:30:43","title":"Deep Horseshoe Gaussian Processes","abstract":"Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated tempered posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regression function and to its structure in terms of compositions. The dependence of the rates in terms of dimension are explicit, allowing in particular for input spaces of dimension increasing with the number of observations.","sentences":["Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures.","Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference.","We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters.","For nonparametric regression with random design, we show that the associated tempered posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way.","The convergence rates are simultaneously adaptive to both the smoothness of the regression function and to its structure in terms of compositions.","The dependence of the rates in terms of dimension are explicit, allowing in particular for input spaces of dimension increasing with the number of observations."],"url":"http://arxiv.org/abs/2403.01737v1","category":"math.ST"}
{"created":"2024-03-04 04:32:28","title":"Statistical Mechanics of Dynamical System Identification","abstract":"Recovering dynamical equations from observed noisy data is the central challenge of system identification. We develop a statistical mechanical approach to analyze sparse equation discovery algorithms, which typically balance data fit and parsimony through a trial-and-error selection of hyperparameters. In this framework, statistical mechanics offers tools to analyze the interplay between complexity and fitness, in analogy to that done between entropy and energy. To establish this analogy, we define the optimization procedure as a two-level Bayesian inference problem that separates variable selection from coefficient values and enables the computation of the posterior parameter distribution in closed form. A key advantage of employing statistical mechanical concepts, such as free energy and the partition function, is in the quantification of uncertainty, especially in in the low-data limit; frequently encountered in real-world applications. As the data volume increases, our approach mirrors the thermodynamic limit, leading to distinct sparsity- and noise-induced phase transitions that delineate correct from incorrect identification. This perspective of sparse equation discovery, is versatile and can be adapted to various other equation discovery algorithms.","sentences":["Recovering dynamical equations from observed noisy data is the central challenge of system identification.","We develop a statistical mechanical approach to analyze sparse equation discovery algorithms, which typically balance data fit and parsimony through a trial-and-error selection of hyperparameters.","In this framework, statistical mechanics offers tools to analyze the interplay between complexity and fitness, in analogy to that done between entropy and energy.","To establish this analogy, we define the optimization procedure as a two-level Bayesian inference problem that separates variable selection from coefficient values and enables the computation of the posterior parameter distribution in closed form.","A key advantage of employing statistical mechanical concepts, such as free energy and the partition function, is in the quantification of uncertainty, especially in in the low-data limit; frequently encountered in real-world applications.","As the data volume increases, our approach mirrors the thermodynamic limit, leading to distinct sparsity- and noise-induced phase transitions that delineate correct from incorrect identification.","This perspective of sparse equation discovery, is versatile and can be adapted to various other equation discovery algorithms."],"url":"http://arxiv.org/abs/2403.01723v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-04 03:09:28","title":"DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling","abstract":"Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the base model. Utilizing this framework, various types and positions of exits can be organized according to predefined configurations, which can be dynamically switched in real-time to accommodate evolving performance-complexity requirements. We also propose techniques for generating optimized configurations based on any desired trade-off between performance and computational complexity. This empowers future researchers to focus on the improvement of individual exits without latent compromise of overall system performance. The efficacy of this approach is demonstrated through image classification tasks with deep CNNs. DyCE significantly reduces the computational complexity by 23.5% of ResNet152 and 25.9% of ConvNextv2-tiny on ImageNet, with accuracy reductions of less than 0.5%. Furthermore, DyCE offers advantages over existing dynamic methods in terms of real-time configuration and fine-grained performance tuning.","sentences":["Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments.","Most existing techniques, such as pruning and quantization are generally static.","On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed.","Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes.","Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models.","This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the base model.","Utilizing this framework, various types and positions of exits can be organized according to predefined configurations, which can be dynamically switched in real-time to accommodate evolving performance-complexity requirements.","We also propose techniques for generating optimized configurations based on any desired trade-off between performance and computational complexity.","This empowers future researchers to focus on the improvement of individual exits without latent compromise of overall system performance.","The efficacy of this approach is demonstrated through image classification tasks with deep CNNs.","DyCE significantly reduces the computational complexity by 23.5% of ResNet152 and 25.9% of ConvNextv2-tiny on ImageNet, with accuracy reductions of less than 0.5%.","Furthermore, DyCE offers advantages over existing dynamic methods in terms of real-time configuration and fine-grained performance tuning."],"url":"http://arxiv.org/abs/2403.01695v1","category":"cs.LG"}
{"created":"2024-03-04 02:25:41","title":"Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection","abstract":"This paper presents Incremental Vision-Language Object Detection (IVLOD), a novel learning task designed to incrementally adapt pre-trained Vision-Language Object Detection Models (VLODMs) to various specialized domains, while simultaneously preserving their zero-shot generalization capabilities for the generalized domain. To address this new challenge, we present the Zero-interference Reparameterizable Adaptation (ZiRa), a novel method that introduces Zero-interference Loss and reparameterization techniques to tackle IVLOD without incurring additional inference costs or a significant increase in memory usage. Comprehensive experiments on COCO and ODinW-13 datasets demonstrate that ZiRa effectively safeguards the zero-shot generalization ability of VLODMs while continuously adapting to new tasks. Specifically, after training on ODinW-13 datasets, ZiRa exhibits superior performance compared to CL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and 8.71 AP, respectively.","sentences":["This paper presents Incremental Vision-Language Object Detection (IVLOD), a novel learning task designed to incrementally adapt pre-trained Vision-Language Object Detection Models (VLODMs) to various specialized domains, while simultaneously preserving their zero-shot generalization capabilities for the generalized domain.","To address this new challenge, we present the Zero-interference Reparameterizable Adaptation (ZiRa), a novel method that introduces Zero-interference Loss and reparameterization techniques to tackle IVLOD without incurring additional inference costs or a significant increase in memory usage.","Comprehensive experiments on COCO and ODinW-13 datasets demonstrate that ZiRa effectively safeguards the zero-shot generalization ability of VLODMs while continuously adapting to new tasks.","Specifically, after training on ODinW-13 datasets, ZiRa exhibits superior performance compared to CL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and 8.71 AP, respectively."],"url":"http://arxiv.org/abs/2403.01680v1","category":"cs.CV"}
{"created":"2024-03-04 01:53:06","title":"ASPIRe: An Informative Trajectory Planner with Mutual Information Approximation for Target Search and Tracking","abstract":"This paper proposes an informative trajectory planning approach, namely, \\textit{adaptive particle filter tree with sigma point-based mutual information reward approximation} (ASPIRe), for mobile target search and tracking (SAT) in cluttered environments with limited sensing field of view. We develop a novel sigma point-based approximation to accurately estimate mutual information (MI) for general, non-Gaussian distributions utilizing particle representation of the belief state, while simultaneously maintaining high computational efficiency. Building upon the MI approximation, we develop the Adaptive Particle Filter Tree (APFT) approach with MI as the reward, which features belief state tree nodes for informative trajectory planning in continuous state and measurement spaces. An adaptive criterion is proposed in APFT to adjust the planning horizon based on the expected information gain. Simulations and physical experiments demonstrate that ASPIRe achieves real-time computation and outperforms benchmark methods in terms of both search efficiency and estimation accuracy.","sentences":["This paper proposes an informative trajectory planning approach, namely, \\textit{adaptive particle filter tree with sigma point-based mutual information reward approximation} (ASPIRe), for mobile target search and tracking (SAT) in cluttered environments with limited sensing field of view.","We develop a novel sigma point-based approximation to accurately estimate mutual information (MI) for general, non-Gaussian distributions utilizing particle representation of the belief state, while simultaneously maintaining high computational efficiency.","Building upon the MI approximation, we develop the Adaptive Particle Filter Tree (APFT) approach with MI as the reward, which features belief state tree nodes for informative trajectory planning in continuous state and measurement spaces.","An adaptive criterion is proposed in APFT to adjust the planning horizon based on the expected information gain.","Simulations and physical experiments demonstrate that ASPIRe achieves real-time computation and outperforms benchmark methods in terms of both search efficiency and estimation accuracy."],"url":"http://arxiv.org/abs/2403.01674v1","category":"cs.RO"}
{"created":"2024-03-04 01:45:19","title":"6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human","abstract":"We aim to perform sound event localization and detection (SELD) using wearable equipment for a moving human, such as a pedestrian. Conventional SELD tasks have dealt only with microphone arrays located in static positions. However, self-motion with three rotational and three translational degrees of freedom (6DoF) shall be considered for wearable microphone arrays. A system trained only with a dataset using microphone arrays in a fixed position would be unable to adapt to the fast relative motion of sound events associated with self-motion, resulting in the degradation of SELD performance. To address this, we designed 6DoF SELD Dataset for wearable systems, the first SELD dataset considering the self-motion of microphones. Furthermore, we proposed a multi-modal SELD system that jointly utilizes audio and motion tracking sensor signals. These sensor signals are expected to help the system find useful acoustic cues for SELD on the basis of the current self-motion state. Experimental results on our dataset show that the proposed method effectively improves SELD performance with a mechanism to extract acoustic features conditioned by sensor signals.","sentences":["We aim to perform sound event localization and detection (SELD) using wearable equipment for a moving human, such as a pedestrian.","Conventional SELD tasks have dealt only with microphone arrays located in static positions.","However, self-motion with three rotational and three translational degrees of freedom (6DoF) shall be considered for wearable microphone arrays.","A system trained only with a dataset using microphone arrays in a fixed position would be unable to adapt to the fast relative motion of sound events associated with self-motion, resulting in the degradation of SELD performance.","To address this, we designed 6DoF SELD Dataset for wearable systems, the first SELD dataset considering the self-motion of microphones.","Furthermore, we proposed a multi-modal SELD system that jointly utilizes audio and motion tracking sensor signals.","These sensor signals are expected to help the system find useful acoustic cues for SELD on the basis of the current self-motion state.","Experimental results on our dataset show that the proposed method effectively improves SELD performance with a mechanism to extract acoustic features conditioned by sensor signals."],"url":"http://arxiv.org/abs/2403.01670v1","category":"eess.AS"}
{"created":"2024-03-04 01:39:10","title":"Reaction-diffusion models of biological invasion: Open source computational tools, key concepts and analysis","abstract":"This review provides open-access computational tools that support a range of mathematical approaches to analyse three related scalar reaction-diffusion models used to study biological invasion. Starting with the classic Fisher-Kolmogorov (Fisher-KPP) model, we illustrate how computational methods can be used to explore time-dependent partial differential equation (PDE) solutions in parallel with phase plane and regular perturbation techniques to explore invading travelling wave solutions moving with dimensionless speed $c \\ge 2$. To overcome the lack of a well-defined sharp front in solutions of the Fisher-KPP model, we also review two alternative modeling approaches. The first is the Porous-Fisher model where the linear diffusion term is replaced with a degenerate nonlinear diffusion term. Using phase plane and regular perturbation methods, we explore the distinction between sharp- and smooth-fronted invading travelling waves that move with dimensionless speed $c \\ge 1/\\sqrt{2}$. The second alternative approach is to reformulate the Fisher-KPP model as a moving boundary problem on $0 < x < L(t)$, leading to the Fisher-Stefan model with sharp-fronted travelling wave solutions arising from a PDE model with a linear diffusion term. Time-dependent PDE solutions and phase plane methods show that travelling wave solutions of the Fisher-Stefan model can describe both biological invasion $(c > 0)$ and biological recession $(c < 0)$. Open source Julia code to replicate all computational results in this review is available on GitHub; we encourage researchers to use this code directly or to adapt the code as required for more complicated models.","sentences":["This review provides open-access computational tools that support a range of mathematical approaches to analyse three related scalar reaction-diffusion models used to study biological invasion.","Starting with the classic Fisher-Kolmogorov (Fisher-KPP) model, we illustrate how computational methods can be used to explore time-dependent partial differential equation (PDE) solutions in parallel with phase plane and regular perturbation techniques to explore invading travelling wave solutions moving with dimensionless speed $c \\ge 2$.","To overcome the lack of a well-defined sharp front in solutions of the Fisher-KPP model, we also review two alternative modeling approaches.","The first is the Porous-Fisher model where the linear diffusion term is replaced with a degenerate nonlinear diffusion term.","Using phase plane and regular perturbation methods, we explore the distinction between sharp- and smooth-fronted invading travelling waves that move with dimensionless speed $c \\ge 1/\\sqrt{2}$.","The second alternative approach is to reformulate the Fisher-KPP model as a moving boundary problem on $0 < x < L(t)$, leading to the Fisher-Stefan model with sharp-fronted travelling wave solutions arising from a PDE model with a linear diffusion term.","Time-dependent PDE solutions and phase plane methods show that travelling wave solutions of the Fisher-Stefan model can describe both biological invasion $(c > 0)$ and biological recession $(c < 0)$. Open source Julia code to replicate all computational results in this review is available on GitHub; we encourage researchers to use this code directly or to adapt the code as required for more complicated models."],"url":"http://arxiv.org/abs/2403.01667v2","category":"nlin.PS"}
{"created":"2024-03-04 00:33:36","title":"Solar Wind Driven from GONG Magnetograms in the Last Solar Cycle","abstract":"In a previous study, Huang et al. (2023) used the Alfven Wave Solar atmosphere Model (AWSoM), one of the widely used solar wind models in the community, driven by ADAPT-GONG magnetograms to simulate the solar wind in the last solar cycle and found that the optimal Poynting flux parameter can be estimated from either the open field area or the average unsigned radial component of the magnetic field in the open field regions. It was also found that the average energy deposition rate (Poynting flux) in the open field regions is approximately constant. In the current study, we expand the previous work by using GONG magnetograms to simulate the solar wind for the same Carrington rotations and determine if the results are similar to the ones obtained with ADAPT-GONG magnetograms. Our results indicate that similar correlations can be obtained from the GONG maps. Moreover, we report that ADAPT-GONG magnetograms can consistently provide better comparisons with 1 AU solar wind observations than GONG magnetograms, based on the best simulations selected by the minimum of the average curve distance for the solar wind speed and density.","sentences":["In a previous study, Huang et al.","(2023) used the Alfven Wave Solar atmosphere Model (AWSoM), one of the widely used solar wind models in the community, driven by ADAPT-GONG magnetograms to simulate the solar wind in the last solar cycle and found that the optimal Poynting flux parameter can be estimated from either the open field area or the average unsigned radial component of the magnetic field in the open field regions.","It was also found that the average energy deposition rate (Poynting flux) in the open field regions is approximately constant.","In the current study, we expand the previous work by using GONG magnetograms to simulate the solar wind for the same Carrington rotations and determine if the results are similar to the ones obtained with ADAPT-GONG magnetograms.","Our results indicate that similar correlations can be obtained from the GONG maps.","Moreover, we report that ADAPT-GONG magnetograms can consistently provide better comparisons with 1 AU solar wind observations than GONG magnetograms, based on the best simulations selected by the minimum of the average curve distance for the solar wind speed and density."],"url":"http://arxiv.org/abs/2403.01656v1","category":"astro-ph.SR"}
{"created":"2024-03-04 00:10:36","title":"A study of Be stars in the time domain. I. Spectral data and polarimetry","abstract":"We present the first part of a spectroscopic and polarimetric study on a sample of 58 Be stars that have been measured since 1998. The aim of the study is to understand the timescales of disk variability, formation and dissipation as a function of the properties (mass, luminosity and rotational velocity) of the underlying B star. In this paper we classified the sample based on the presence of emission or absorption of the H$\\alpha$ line, and the shape of the peak as single or double peak, as well as noting changes between emission and non-emission states. We find a probability of $\\sim 0.75$ percent per year that an object in the sample will undergo such a change. We also present re-derived values of the projected rotational velocities for the sample. When we compare our polarization values with those from the literature, we find that most of the stars do not show a change in the value of the polarization angle, however a small number show significant changes which could be attributed to either disk strength (optical depth) or geometry changes. Finally we show how, by combining the (interstellar corrected) degree of polarization and the projected rotational velocity, we can construct an inclination angle-free parameter that includes the true equatorial velocity. Using this inclination angle-independent parameter we show that the populations of single and double peak stars are indistinguishable, giving further evidence that Be star line profiles are essentially inclination angle driven.","sentences":["We present the first part of a spectroscopic and polarimetric study on a sample of 58 Be stars that have been measured since 1998.","The aim of the study is to understand the timescales of disk variability, formation and dissipation as a function of the properties (mass, luminosity and rotational velocity) of the underlying B star.","In this paper we classified the sample based on the presence of emission or absorption of the H$\\alpha$ line, and the shape of the peak as single or double peak, as well as noting changes between emission and non-emission states.","We find a probability of $\\sim 0.75$ percent per year that an object in the sample will undergo such a change.","We also present re-derived values of the projected rotational velocities for the sample.","When we compare our polarization values with those from the literature, we find that most of the stars do not show a change in the value of the polarization angle, however a small number show significant changes which could be attributed to either disk strength (optical depth) or geometry changes.","Finally we show how, by combining the (interstellar corrected) degree of polarization and the projected rotational velocity, we can construct an inclination angle-free parameter that includes the true equatorial velocity.","Using this inclination angle-independent parameter we show that the populations of single and double peak stars are indistinguishable, giving further evidence that Be star line profiles are essentially inclination angle driven."],"url":"http://arxiv.org/abs/2403.01654v1","category":"astro-ph.SR"}
{"created":"2024-03-04 00:09:07","title":"Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data","abstract":"Regional solar power forecasting, which involves predicting the total power generation from all rooftop photovoltaic systems in a region holds significant importance for various stakeholders in the energy sector. However, the vast amount of solar power generation and weather time series from geographically dispersed locations that need to be considered in the forecasting process makes accurate regional forecasting challenging. Therefore, previous work has limited the focus to either forecasting a single time series (i.e., aggregated time series) which is the addition of all solar generation time series in a region, disregarding the location-specific weather effects or forecasting solar generation time series of each PV site (i.e., individual time series) independently using location-specific weather data, resulting in a large number of forecasting models. In this work, we propose two deep-learning-based regional forecasting methods that can effectively leverage both types of time series (aggregated and individual) with weather data in a region. We propose two hierarchical temporal convolutional neural network architectures (HTCNN) and two strategies to adapt HTCNNs for regional solar power forecasting. At first, we explore generating a regional forecast using a single HTCNN. Next, we divide the region into multiple sub-regions based on weather information and train separate HTCNNs for each sub-region; the forecasts of each sub-region are then added to generate a regional forecast. The proposed work is evaluated using a large dataset collected over a year from 101 locations across Western Australia to provide a day ahead forecast. We compare our approaches with well-known alternative methods and show that the sub-region HTCNN requires fewer individual networks and achieves a forecast skill score of 40.2% reducing a statistically significant error by 6.5% compared to the best counterpart.","sentences":["Regional solar power forecasting, which involves predicting the total power generation from all rooftop photovoltaic systems in a region holds significant importance for various stakeholders in the energy sector.","However, the vast amount of solar power generation and weather time series from geographically dispersed locations that need to be considered in the forecasting process makes accurate regional forecasting challenging.","Therefore, previous work has limited the focus to either forecasting a single time series (i.e., aggregated time series) which is the addition of all solar generation time series in a region, disregarding the location-specific weather effects or forecasting solar generation time series of each PV site (i.e., individual time series) independently using location-specific weather data, resulting in a large number of forecasting models.","In this work, we propose two deep-learning-based regional forecasting methods that can effectively leverage both types of time series (aggregated and individual) with weather data in a region.","We propose two hierarchical temporal convolutional neural network architectures (HTCNN) and two strategies to adapt HTCNNs for regional solar power forecasting.","At first, we explore generating a regional forecast using a single HTCNN.","Next, we divide the region into multiple sub-regions based on weather information and train separate HTCNNs for each sub-region; the forecasts of each sub-region are then added to generate a regional forecast.","The proposed work is evaluated using a large dataset collected over a year from 101 locations across Western Australia to provide a day ahead forecast.","We compare our approaches with well-known alternative methods and show that the sub-region HTCNN requires fewer individual networks and achieves a forecast skill score of 40.2% reducing a statistically significant error by 6.5% compared to the best counterpart."],"url":"http://arxiv.org/abs/2403.01653v1","category":"cs.LG"}
{"created":"2024-03-03 23:35:49","title":"AIO2: Online Correction of Object Labels for Deep Learning with Incomplete Annotation in Remote Sensing Image Segmentation","abstract":"While the volume of remote sensing data is increasing daily, deep learning in Earth Observation faces lack of accurate annotations for supervised optimization. Crowdsourcing projects such as OpenStreetMap distribute the annotation load to their community. However, such annotation inevitably generates noise due to insufficient control of the label quality, lack of annotators, frequent changes of the Earth's surface as a result of natural disasters and urban development, among many other factors. We present Adaptively trIggered Online Object-wise correction (AIO2) to address annotation noise induced by incomplete label sets. AIO2 features an Adaptive Correction Trigger (ACT) module that avoids label correction when the model training under- or overfits, and an Online Object-wise Correction (O2C) methodology that employs spatial information for automated label modification. AIO2 utilizes a mean teacher model to enhance training robustness with noisy labels to both stabilize the training accuracy curve for fitting in ACT and provide pseudo labels for correction in O2C. Moreover, O2C is implemented online without the need to store updated labels every training epoch. We validate our approach on two building footprint segmentation datasets with different spatial resolutions. Experimental results with varying degrees of building label noise demonstrate the robustness of AIO2. Source code will be available at https://github.com/zhu-xlab/AIO2.git.","sentences":["While the volume of remote sensing data is increasing daily, deep learning in Earth Observation faces lack of accurate annotations for supervised optimization.","Crowdsourcing projects such as OpenStreetMap distribute the annotation load to their community.","However, such annotation inevitably generates noise due to insufficient control of the label quality, lack of annotators, frequent changes of the Earth's surface as a result of natural disasters and urban development, among many other factors.","We present Adaptively trIggered Online Object-wise correction (AIO2) to address annotation noise induced by incomplete label sets.","AIO2 features an Adaptive Correction Trigger (ACT) module that avoids label correction when the model training under- or overfits, and an Online Object-wise Correction (O2C) methodology that employs spatial information for automated label modification.","AIO2 utilizes a mean teacher model to enhance training robustness with noisy labels to both stabilize the training accuracy curve for fitting in ACT and provide pseudo labels for correction in O2C. Moreover, O2C is implemented online without the need to store updated labels every training epoch.","We validate our approach on two building footprint segmentation datasets with different spatial resolutions.","Experimental results with varying degrees of building label noise demonstrate the robustness of AIO2.","Source code will be available at https://github.com/zhu-xlab/AIO2.git."],"url":"http://arxiv.org/abs/2403.01641v1","category":"cs.CV"}
{"created":"2024-03-03 23:10:36","title":"Multi-level Product Category Prediction through Text Classification","abstract":"This article investigates applying advanced machine learning models, specifically LSTM and BERT, for text classification to predict multiple categories in the retail sector. The study demonstrates how applying data augmentation techniques and the focal loss function can significantly enhance accuracy in classifying products into multiple categories using a robust Brazilian retail dataset. The LSTM model, enriched with Brazilian word embedding, and BERT, known for its effectiveness in understanding complex contexts, were adapted and optimized for this specific task. The results showed that the BERT model, with an F1 Macro Score of up to $99\\%$ for segments, $96\\%$ for categories and subcategories and $93\\%$ for name products, outperformed LSTM in more detailed categories. However, LSTM also achieved high performance, especially after applying data augmentation and focal loss techniques. These results underscore the effectiveness of NLP techniques in retail and highlight the importance of the careful selection of modelling and preprocessing strategies. This work contributes significantly to the field of NLP in retail, providing valuable insights for future research and practical applications.","sentences":["This article investigates applying advanced machine learning models, specifically LSTM and BERT, for text classification to predict multiple categories in the retail sector.","The study demonstrates how applying data augmentation techniques and the focal loss function can significantly enhance accuracy in classifying products into multiple categories using a robust Brazilian retail dataset.","The LSTM model, enriched with Brazilian word embedding, and BERT, known for its effectiveness in understanding complex contexts, were adapted and optimized for this specific task.","The results showed that the BERT model, with an F1 Macro Score of up to $99\\%$ for segments, $96\\%$ for categories and subcategories and $93\\%$ for name products, outperformed LSTM in more detailed categories.","However, LSTM also achieved high performance, especially after applying data augmentation and focal loss techniques.","These results underscore the effectiveness of NLP techniques in retail and highlight the importance of the careful selection of modelling and preprocessing strategies.","This work contributes significantly to the field of NLP in retail, providing valuable insights for future research and practical applications."],"url":"http://arxiv.org/abs/2403.01638v1","category":"cs.CL"}
{"created":"2024-03-03 22:15:38","title":"Using LLMs for Tabletop Exercises within the Security Domain","abstract":"Tabletop exercises are a crucial component of many company's strategy to test and evaluate its preparedness for security incidents in a realistic way. Traditionally led by external firms specializing in cybersecurity, these exercises can be costly, time-consuming, and may not always align precisely with the client's specific needs. Large Language Models (LLMs) like ChatGPT offer a compelling alternative. They enable faster iteration, provide rich and adaptable simulations, and offer infinite patience in handling feedback and recommendations. This approach can enhances the efficiency and relevance of security preparedness exercises.","sentences":["Tabletop exercises are a crucial component of many company's strategy to test and evaluate its preparedness for security incidents in a realistic way.","Traditionally led by external firms specializing in cybersecurity, these exercises can be costly, time-consuming, and may not always align precisely with the client's specific needs.","Large Language Models (LLMs) like ChatGPT offer a compelling alternative.","They enable faster iteration, provide rich and adaptable simulations, and offer infinite patience in handling feedback and recommendations.","This approach can enhances the efficiency and relevance of security preparedness exercises."],"url":"http://arxiv.org/abs/2403.01626v1","category":"cs.CR"}
{"created":"2024-03-03 22:15:36","title":"Magnonic $\\varphi$ Josephson Junctions and Synchronized Precession","abstract":"There has been a growing interest in non-Hermitian physics. One of its main goals is to engineer dissipation and to explore ensuing functionality. In magnonics, the effect of dissipation due to local damping on magnon transport has been explored. However, the effects of non-local damping on the magnonic analog of the Josephson effect remain missing, despite that non-local damping is inevitable and has been playing a central role in magnonics. Here, we uncover theoretically that a surprisingly rich dynamics can emerge in magnetic junctions due to intrinsic non-local damping, using analytical and numerical methods. In particular, under microwave pumping, we show that coherent spin precession in the right and left insulating ferromagnet (FM) of the junction becomes synchronized by non-local damping and thereby a magnonic analog of the $\\varphi$ Josephson junction emerges, where $\\varphi$ stands here for the relative precession phase of right and left FM in the stationary limit. Remarkably, $\\varphi$ decreases monotonically from $ \\pi$ to $\\pi/2$ as the magnon-magnon interaction, arising from spin anisotropies, increases. Moreover, we also find a magnonic diode effect giving rise to rectification of magnon currents. Our predictions are readily testable with current device and measurement technologies at room temperatures.","sentences":["There has been a growing interest in non-Hermitian physics.","One of its main goals is to engineer dissipation and to explore ensuing functionality.","In magnonics, the effect of dissipation due to local damping on magnon transport has been explored.","However, the effects of non-local damping on the magnonic analog of the Josephson effect remain missing, despite that non-local damping is inevitable and has been playing a central role in magnonics.","Here, we uncover theoretically that a surprisingly rich dynamics can emerge in magnetic junctions due to intrinsic non-local damping, using analytical and numerical methods.","In particular, under microwave pumping, we show that coherent spin precession in the right and left insulating ferromagnet (FM) of the junction becomes synchronized by non-local damping and thereby a magnonic analog of the $\\varphi$ Josephson junction emerges, where $\\varphi$ stands here for the relative precession phase of right and left FM in the stationary limit.","Remarkably, $\\varphi$ decreases monotonically from $ \\pi$ to $\\pi/2$ as the magnon-magnon interaction, arising from spin anisotropies, increases.","Moreover, we also find a magnonic diode effect giving rise to rectification of magnon currents.","Our predictions are readily testable with current device and measurement technologies at room temperatures."],"url":"http://arxiv.org/abs/2403.01625v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-03 20:36:36","title":"Graph-based variant discovery reveals novel dynamics in the human microbiome","abstract":"Sequence differences between the strains of bacteria comprising host-associated and environmental microbiota may play a role in community assembly and influence the resilience of microbial communities to disturbances. Tools for characterizing strain-level variation within microbial communities, however, are limited in scope, focusing on just single nucleotide polymorphisms, or relying on reference-based analyses that miss complex functional and structural variants. Here, we demonstrate the power of assembly graph analysis to detect and characterize structural variants in almost 1,000 metagenomes generated as part of the Human Microbiome Project. We identify over nine million variants comprising insertion/deletion events, repeat copy-number changes, and mobile elements such as plasmids. We highlight some of the potential functional roles of these genomic changes. Our analysis revealed striking differences in the rate of variation across body sites, highlighting niche-specific mechanisms of bacterial adaptation. The structural variants we detect also include potentially novel prophage integration events, highlighting the potential use of graph-based analyses for phage discovery.","sentences":["Sequence differences between the strains of bacteria comprising host-associated and environmental microbiota may play a role in community assembly and influence the resilience of microbial communities to disturbances.","Tools for characterizing strain-level variation within microbial communities, however, are limited in scope, focusing on just single nucleotide polymorphisms, or relying on reference-based analyses that miss complex functional and structural variants.","Here, we demonstrate the power of assembly graph analysis to detect and characterize structural variants in almost 1,000 metagenomes generated as part of the Human Microbiome Project.","We identify over nine million variants comprising insertion/deletion events, repeat copy-number changes, and mobile elements such as plasmids.","We highlight some of the potential functional roles of these genomic changes.","Our analysis revealed striking differences in the rate of variation across body sites, highlighting niche-specific mechanisms of bacterial adaptation.","The structural variants we detect also include potentially novel prophage integration events, highlighting the potential use of graph-based analyses for phage discovery."],"url":"http://arxiv.org/abs/2403.01610v1","category":"q-bio.GN"}
{"created":"2024-03-03 19:27:30","title":"Revisiting Stereotypes: Race and Running","abstract":"The athletic achievements of African athletes in global running championships have long been subject to scientific and sociological inquiry. During the 1990s, a popular narrative emerged, suggesting that West African lineage conferred inherent sprinting advantages, and that North, South and East African's are specialized for longer distances. Part and parcel to this narrative was the enthusiastic belief that it would very soon be substantiated by a genotyping revolution that would enable prognostication of individual athletic potential. We revisit this hypothesis in the post-genomic era. First, we compare the global running records used to generate the racialist hypotheses with performances over the last twenty years (2004- 2023). Focusing on the 100m reveals intriguing trends, including the ascendancy of Jamaica as a sprint powerhouse and the elevation of South African and East Asian sprinters to the global stage, a direct challenge to the racialist paradigm. In line with an in-depth analysis of the influences on elite runners, we build a regression model to predict 100m performance based on environmental and psychological factors. Next, we direct our attention to 1500m, where the last two British champions have been part of a European resurgence that has not been seen in decades. Examining three different time periods, we identify a thirty year national slowdown (1989-2018). Adapting our model to this time period reveals striking evidence that racial perception has greater impact on performance than racial physiology. Synthesizing these findings, we introduce a psychocultural hypothesis, positing that interactions between racial perceptions and social dynamics shape the global distribution of running performance. We contrast this hypothesis with the racialist paradigm and propose extending it beyond sport where it offers insight across many domains.","sentences":["The athletic achievements of African athletes in global running championships have long been subject to scientific and sociological inquiry.","During the 1990s, a popular narrative emerged, suggesting that West African lineage conferred inherent sprinting advantages, and that North, South and East African's are specialized for longer distances.","Part and parcel to this narrative was the enthusiastic belief that it would very soon be substantiated by a genotyping revolution that would enable prognostication of individual athletic potential.","We revisit this hypothesis in the post-genomic era.","First, we compare the global running records used to generate the racialist hypotheses with performances over the last twenty years (2004- 2023).","Focusing on the 100m reveals intriguing trends, including the ascendancy of Jamaica as a sprint powerhouse and the elevation of South African and East Asian sprinters to the global stage, a direct challenge to the racialist paradigm.","In line with an in-depth analysis of the influences on elite runners, we build a regression model to predict 100m performance based on environmental and psychological factors.","Next, we direct our attention to 1500m, where the last two British champions have been part of a European resurgence that has not been seen in decades.","Examining three different time periods, we identify a thirty year national slowdown (1989-2018).","Adapting our model to this time period reveals striking evidence that racial perception has greater impact on performance than racial physiology.","Synthesizing these findings, we introduce a psychocultural hypothesis, positing that interactions between racial perceptions and social dynamics shape the global distribution of running performance.","We contrast this hypothesis with the racialist paradigm and propose extending it beyond sport where it offers insight across many domains."],"url":"http://arxiv.org/abs/2403.02358v1","category":"physics.soc-ph"}
{"created":"2024-03-03 18:22:39","title":"On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation","abstract":"Multi-Source-Free Unsupervised Domain Adaptation (MSFDA) aims to transfer knowledge from multiple well-labeled source domains to an unlabeled target domain, using source models instead of source data. Existing MSFDA methods limited that each source domain provides only a single model, with a uniform structure. This paper introduces a new MSFDA setting: Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (MMDA), allowing diverse source models with varying architectures, without quantitative restrictions. While MMDA holds promising potential, incorporating numerous source models poses a high risk of including undesired models, which highlights the source model selection problem. To address it, we first provide a theoretical analysis of this problem. We reveal two fundamental selection principles: transferability principle and diversity principle, and introduce a selection algorithm to integrate them. Then, considering the measure of transferability is challenging, we propose a novel Source-Free Unsupervised Transferability Estimation (SUTE). This novel formulation enables the assessment and comparison of transferability across multiple source models with different architectures in the context of domain shift, without requiring access to any target labels or source data. Based on the above, we introduce a new framework to address MMDA. Specifically, we first conduct source model selection based on the proposed selection principles. Subsequently, we design two modules to aggregate knowledge from included models and recycle useful knowledge from excluded models. These modules enable us to leverage source knowledge efficiently and effectively, thereby supporting us in learning a discriminative target model via adaptation. We validate the effectiveness of our method through numerous experimental results, and demonstrate that our approach achieves state-of-the-art performance.","sentences":["Multi-Source-Free Unsupervised Domain Adaptation (MSFDA) aims to transfer knowledge from multiple well-labeled source domains to an unlabeled target domain, using source models instead of source data.","Existing MSFDA methods limited that each source domain provides only a single model, with a uniform structure.","This paper introduces a new MSFDA setting: Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (MMDA), allowing diverse source models with varying architectures, without quantitative restrictions.","While MMDA holds promising potential, incorporating numerous source models poses a high risk of including undesired models, which highlights the source model selection problem.","To address it, we first provide a theoretical analysis of this problem.","We reveal two fundamental selection principles: transferability principle and diversity principle, and introduce a selection algorithm to integrate them.","Then, considering the measure of transferability is challenging, we propose a novel Source-Free Unsupervised Transferability Estimation (SUTE).","This novel formulation enables the assessment and comparison of transferability across multiple source models with different architectures in the context of domain shift, without requiring access to any target labels or source data.","Based on the above, we introduce a new framework to address MMDA.","Specifically, we first conduct source model selection based on the proposed selection principles.","Subsequently, we design two modules to aggregate knowledge from included models and recycle useful knowledge from excluded models.","These modules enable us to leverage source knowledge efficiently and effectively, thereby supporting us in learning a discriminative target model via adaptation.","We validate the effectiveness of our method through numerous experimental results, and demonstrate that our approach achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.01582v1","category":"cs.LG"}
{"created":"2024-03-03 17:08:44","title":"Adaptive multiplication of $\\mathcal{H}^2$-matrices with block-relative error control","abstract":"The discretization of non-local operators, e.g., solution operators of partial differential equations or integral operators, leads to large densely populated matrices. $\\mathcal{H}^2$-matrices take advantage of local low-rank structures in these matrices to provide an efficient data-sparse approximation that allows us to handle large matrices efficiently, e.g., to reduce the storage requirements to $\\mathcal{O}(n k)$ for $n$-dimensional matrices with local rank $k$, and to reduce the complexity of the matrix-vector multiplication to $\\mathcal{O}(n k)$ operations.   In order to perform more advanced operations, e.g., to construct efficient preconditioners or evaluate matrix functions, we require algorithms that take $\\mathcal{H}^2$-matrices as input and approximate the result again by $\\mathcal{H}^2$-matrices, ideally with controllable accuracy. In this manuscript, we introduce an algorithm that approximates the product of two $\\mathcal{H}^2$-matrices and guarantees block-relative error estimates for the submatrices of the result. It uses specialized tree structures to represent the exact product in an intermediate step, thereby allowing us to apply mathematically rigorous error control strategies.","sentences":["The discretization of non-local operators, e.g., solution operators of partial differential equations or integral operators, leads to large densely populated matrices.","$\\mathcal{H}^2$-matrices take advantage of local low-rank structures in these matrices to provide an efficient data-sparse approximation that allows us to handle large matrices efficiently, e.g., to reduce the storage requirements to $\\mathcal{O}(n k)$ for $n$-dimensional matrices with local rank $k$, and to reduce the complexity of the matrix-vector multiplication to $\\mathcal{O}(n k)$ operations.   ","In order to perform more advanced operations, e.g., to construct efficient preconditioners or evaluate matrix functions, we require algorithms that take $\\mathcal{H}^2$-matrices as input and approximate the result again by $\\mathcal{H}^2$-matrices, ideally with controllable accuracy.","In this manuscript, we introduce an algorithm that approximates the product of two $\\mathcal{H}^2$-matrices and guarantees block-relative error estimates for the submatrices of the result.","It uses specialized tree structures to represent the exact product in an intermediate step, thereby allowing us to apply mathematically rigorous error control strategies."],"url":"http://arxiv.org/abs/2403.01566v1","category":"math.NA"}
{"created":"2024-03-03 17:00:28","title":"ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates","abstract":"Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the MPC leverages available state information to improve tracking. The central contribution of this work is their reciprocal interaction: DQN's update decisions inform MPC's control strategy, and MPC's outcomes refine DQN's learning, creating a cohesive, adaptive system. Empirical evaluations in simulated and real-world settings demonstrate that ComTraQ-MPC significantly enhances operational efficiency and accuracy, providing a generalizable and approximately optimal solution for trajectory tracking in complex partially observable environments.","sentences":["Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge.","Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance.","This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations.","This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates.","The meta-trained DQN ensures adaptive active localization scheduling, while the MPC leverages available state information to improve tracking.","The central contribution of this work is their reciprocal interaction: DQN's update decisions inform MPC's control strategy, and MPC's outcomes refine DQN's learning, creating a cohesive, adaptive system.","Empirical evaluations in simulated and real-world settings demonstrate that ComTraQ-MPC significantly enhances operational efficiency and accuracy, providing a generalizable and approximately optimal solution for trajectory tracking in complex partially observable environments."],"url":"http://arxiv.org/abs/2403.01564v1","category":"cs.RO"}
{"created":"2024-03-03 16:48:16","title":"Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition","abstract":"Contrastive Language-Image Pretraining (CLIP) has shown remarkable open-vocabulary abilities across various image understanding tasks. Building upon this impressive success, recent pioneer works have proposed to adapt the powerful CLIP to video data, leading to efficient and effective video learners for open-vocabulary action recognition. Inspired by the fact that humans perform actions in diverse environments, our work delves into an intriguing question: Can CLIP-based video learners effectively generalize to video domains they have not encountered during training? To answer this, we establish a CROSS-domain Open-Vocabulary Action recognition benchmark named XOV-Action, and conduct a comprehensive evaluation of five state-of-the-art CLIP-based video learners under various types of domain gaps. Our evaluation demonstrates that previous methods exhibit limited action recognition performance in unseen video domains, revealing potential challenges of the cross-domain open-vocabulary action recognition task. To address this task, our work focuses on a critical challenge, namely scene bias, and we accordingly contribute a novel scene-aware video-text alignment method. Our key idea is to distinguish video representations apart from scene-encoded text representations, aiming to learn scene-agnostic video representations for recognizing actions across domains. Extensive experimental results demonstrate the effectiveness of our method. The benchmark and code will be available at https://github.com/KunyuLin/XOV-Action/.","sentences":["Contrastive Language-Image Pretraining (CLIP) has shown remarkable open-vocabulary abilities across various image understanding tasks.","Building upon this impressive success, recent pioneer works have proposed to adapt the powerful CLIP to video data, leading to efficient and effective video learners for open-vocabulary action recognition.","Inspired by the fact that humans perform actions in diverse environments, our work delves into an intriguing question: Can CLIP-based video learners effectively generalize to video domains they have not encountered during training?","To answer this, we establish a CROSS-domain Open-Vocabulary Action recognition benchmark named XOV-Action, and conduct a comprehensive evaluation of five state-of-the-art CLIP-based video learners under various types of domain gaps.","Our evaluation demonstrates that previous methods exhibit limited action recognition performance in unseen video domains, revealing potential challenges of the cross-domain open-vocabulary action recognition task.","To address this task, our work focuses on a critical challenge, namely scene bias, and we accordingly contribute a novel scene-aware video-text alignment method.","Our key idea is to distinguish video representations apart from scene-encoded text representations, aiming to learn scene-agnostic video representations for recognizing actions across domains.","Extensive experimental results demonstrate the effectiveness of our method.","The benchmark and code will be available at https://github.com/KunyuLin/XOV-Action/."],"url":"http://arxiv.org/abs/2403.01560v1","category":"cs.CV"}
{"created":"2024-03-03 16:45:53","title":"Reconstruction of binary black hole harmonics in LIGO using deep learning","abstract":"Gravitational wave signals from coalescing compact binaries in the LIGO and Virgo interferometers are primarily detected by the template based matched filtering method. While this method is optimal for stationary and Gaussian data scenarios, its sensitivity is often affected by non stationary noise transients in the detectors. Moreover, most of the current searches do not account for the effects of precession of black hole spins and higher order waveform harmonics, focusing solely on the leading order quadrupolar modes. This limitation impacts our search for interesting astrophysical sources, such as intermediate mass black hole binaries and hierarchical mergers. Here we show for the first time that deep learning can be used for accurate waveform reconstruction of precessing binary black hole signals with higher order modes. This approach can also be adapted into a rapid trigger generation algorithm to enhance online searches. Our model, tested on simulated injections in real LIGO noise from the third observing run achieved high-degree of overlap with injected signals. This accuracy was consistent across a wide range of black hole masses and spin configurations chosen for this study. When applied to real gravitational wave events, our reconstructions achieved between 0.85 and 0.98 overlaps with those obtained by Coherent WaveBurst (unmodeled) and LALInference (modeled) analyses. These results suggest that deep learning is a potent tool for analyzing signals from a diverse catalog of compact binaries.","sentences":["Gravitational wave signals from coalescing compact binaries in the LIGO and Virgo interferometers are primarily detected by the template based matched filtering method.","While this method is optimal for stationary and Gaussian data scenarios, its sensitivity is often affected by non stationary noise transients in the detectors.","Moreover, most of the current searches do not account for the effects of precession of black hole spins and higher order waveform harmonics, focusing solely on the leading order quadrupolar modes.","This limitation impacts our search for interesting astrophysical sources, such as intermediate mass black hole binaries and hierarchical mergers.","Here we show for the first time that deep learning can be used for accurate waveform reconstruction of precessing binary black hole signals with higher order modes.","This approach can also be adapted into a rapid trigger generation algorithm to enhance online searches.","Our model, tested on simulated injections in real LIGO noise from the third observing run achieved high-degree of overlap with injected signals.","This accuracy was consistent across a wide range of black hole masses and spin configurations chosen for this study.","When applied to real gravitational wave events, our reconstructions achieved between 0.85 and 0.98 overlaps with those obtained by Coherent WaveBurst (unmodeled) and LALInference (modeled) analyses.","These results suggest that deep learning is a potent tool for analyzing signals from a diverse catalog of compact binaries."],"url":"http://arxiv.org/abs/2403.01559v1","category":"gr-qc"}
{"created":"2024-03-03 16:39:43","title":"Adapt or Wait: Quality Adaptation for Cache-aided Channels","abstract":"This work focuses on quality adaptation as a means to counter the effects of channel degradation in wireless, cache-aided channels. We design a delivery scheme which combines coded caching, superposition coding, and scalable source coding, while keeping the caching scheme oblivious to channel qualities. By properly adjusting the quality at the degraded users we are able to satisfy all demands in a time-efficient manner. In addition, superposition coding allows us to serve high-rate users with high content quality without subjecting them to a delay penalty caused by users with lower rate channels. We design a communication framework that covers all possible channel rate and quality configurations and we further provide algorithms that can optimise the served quality. An interesting outcome of this work is that a modest quality reduction at the degraded users can counter the effects of significant channel degradation. For example, in a 100-user system with normalized cache size 1/10 at each user, if 10 users experience channel degradation of 60% compared to the rate of the non-degraded users, we show that our transmission strategy leads to a 85% quality at the degraded users and perfect quality at the non-degraded users.","sentences":["This work focuses on quality adaptation as a means to counter the effects of channel degradation in wireless, cache-aided channels.","We design a delivery scheme which combines coded caching, superposition coding, and scalable source coding, while keeping the caching scheme oblivious to channel qualities.","By properly adjusting the quality at the degraded users we are able to satisfy all demands in a time-efficient manner.","In addition, superposition coding allows us to serve high-rate users with high content quality without subjecting them to a delay penalty caused by users with lower rate channels.","We design a communication framework that covers all possible channel rate and quality configurations and we further provide algorithms that can optimise the served quality.","An interesting outcome of this work is that a modest quality reduction at the degraded users can counter the effects of significant channel degradation.","For example, in a 100-user system with normalized cache size 1/10 at each user, if 10 users experience channel degradation of 60% compared to the rate of the non-degraded users, we show that our transmission strategy leads to a 85% quality at the degraded users and perfect quality at the non-degraded users."],"url":"http://arxiv.org/abs/2403.01558v1","category":"cs.IT"}
{"created":"2024-03-03 16:12:20","title":"Transformers for Supervised Online Continual Learning","abstract":"Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities. However, their potential for online continual learning remains relatively unexplored. In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the supervised online continual learning setting, where we learn a predictor $x_t \\rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning. Our approach explicitly conditions a transformer on recent observations, while at the same time online training it with stochastic gradient descent, following the procedure introduced with Transformer-XL. We incorporate replay to maintain the benefits of multi-epoch training while adhering to the sequential protocol. We hypothesize that this combination enables fast adaptation through in-context learning and sustained longterm improvement via parametric learning. Our method demonstrates significant improvements over previous state-of-the-art results on CLOC, a challenging large-scale real-world benchmark for image geo-localization.","sentences":["Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification.","Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities.","However, their potential for online continual learning remains relatively unexplored.","In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss.","We focus on the supervised online continual learning setting, where we learn a predictor $x_t","\\rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning.","Our approach explicitly conditions a transformer on recent observations, while at the same time online training it with stochastic gradient descent, following the procedure introduced with Transformer-XL.","We incorporate replay to maintain the benefits of multi-epoch training while adhering to the sequential protocol.","We hypothesize that this combination enables fast adaptation through in-context learning and sustained longterm improvement via parametric learning.","Our method demonstrates significant improvements over previous state-of-the-art results on CLOC, a challenging large-scale real-world benchmark for image geo-localization."],"url":"http://arxiv.org/abs/2403.01554v1","category":"cs.LG"}
{"created":"2024-03-03 15:35:25","title":"A Preliminary Exploration of the Disruption of a Generative AI Systems: Faculty/Staff and Student Perceptions of ChatGPT and its Capability of Completing Undergraduate Engineering Coursework","abstract":"The authors of this study aim to assess the capabilities of the OpenAI ChatGPT tool to understand just how effective such a system might be for students to utilize in their studies as well as deepen understanding of faculty/staff and student perceptions about ChatGPT in general. The purpose of what is learned from the study is to continue the design of a model to facilitate the development of faculty for becoming adept at embracing change, the DANCE model (Designing Adaptations for the Next Changes in Education). This model is used in this study to help faculty with examining the impact that a disruptive new tool, such as ChatGPT, can pose for the learning environment.   The authors analyzed the performance of ChatGPT used to complete course assignments at a variety of levels by novice engineering students working as research assistants. Those completed works have been assessed by the faculty who created those assignments to understand how these completed assignments might compare with the performance of a typical student. A set of surveys conducted by the authors of this work are discussed where students, faculty, and staff respondents in March of 2023 addressed their perceptions of ChatGPT (A follow-up survey is being administered now, February 2024). These survey instruments were analyzed, and the data visualized in this work to bring attention to relevant findings by the researchers. This work reports the findings of the researchers with the purpose of sharing the current state of this work at Texas A&M University with the intention to provide insights to scholars both at our own institution and around the world. This work is not intended to be a finished work but reports these findings with full transparency that this work is currently continuing as the researchers gather new data and develop and validate various measurement instruments.","sentences":["The authors of this study aim to assess the capabilities of the OpenAI ChatGPT tool to understand just how effective such a system might be for students to utilize in their studies as well as deepen understanding of faculty/staff and student perceptions about ChatGPT in general.","The purpose of what is learned from the study is to continue the design of a model to facilitate the development of faculty for becoming adept at embracing change, the DANCE model (Designing Adaptations for the Next Changes in Education).","This model is used in this study to help faculty with examining the impact that a disruptive new tool, such as ChatGPT, can pose for the learning environment.   ","The authors analyzed the performance of ChatGPT used to complete course assignments at a variety of levels by novice engineering students working as research assistants.","Those completed works have been assessed by the faculty who created those assignments to understand how these completed assignments might compare with the performance of a typical student.","A set of surveys conducted by the authors of this work are discussed where students, faculty, and staff respondents in March of 2023 addressed their perceptions of ChatGPT (A follow-up survey is being administered now, February 2024).","These survey instruments were analyzed, and the data visualized in this work to bring attention to relevant findings by the researchers.","This work reports the findings of the researchers with the purpose of sharing the current state of this work at Texas A&M University with the intention to provide insights to scholars both at our own institution and around the world.","This work is not intended to be a finished work but reports these findings with full transparency that this work is currently continuing as the researchers gather new data and develop and validate various measurement instruments."],"url":"http://arxiv.org/abs/2403.01538v1","category":"cs.HC"}
{"created":"2024-03-03 15:09:49","title":"Data-driven local operator finding for reduced-order modelling of plasma systems: II. Application to parametric dynamics","abstract":"Real-world systems often exhibit dynamics influenced by various parameters, either inherent or externally controllable, necessitating models capable of reliably capturing these parametric behaviors. Plasma technologies exemplify such systems. For example, phenomena governing global dynamics in Hall thrusters (a spacecraft propulsion technology) vary with various parameters, such as the \"self-sustained electric field\". In this Part II, following on the introduction of our novel data-driven local operator finding algorithm, Phi Method, in Part I, we showcase the method's effectiveness in learning parametric dynamics to predict system behavior across unseen parameter spaces. We present two adaptations: the \"parametric Phi Method\" and the \"ensemble Phi Method\", which are demonstrated through 2D fluid-flow-past-a-cylinder and 1D Hall-thruster-plasma-discharge problems. Comparative evaluation against parametric OPT-DMD in the fluid case demonstrates superior predictive performance of the parametric Phi Method. Across both test cases, parametric and ensemble Phi Method reliably recover governing parametric PDEs and offer accurate predictions over test parameters. Ensemble ROM analysis underscores Phi Method's robust learning of dominant dynamic coefficients with high confidence.","sentences":["Real-world systems often exhibit dynamics influenced by various parameters, either inherent or externally controllable, necessitating models capable of reliably capturing these parametric behaviors.","Plasma technologies exemplify such systems.","For example, phenomena governing global dynamics in Hall thrusters (a spacecraft propulsion technology) vary with various parameters, such as the \"self-sustained electric field\".","In this Part II, following on the introduction of our novel data-driven local operator finding algorithm, Phi Method, in Part I, we showcase the method's effectiveness in learning parametric dynamics to predict system behavior across unseen parameter spaces.","We present two adaptations: the \"parametric Phi Method\" and the \"ensemble Phi Method\", which are demonstrated through 2D fluid-flow-past-a-cylinder and 1D Hall-thruster-plasma-discharge problems.","Comparative evaluation against parametric OPT-DMD in the fluid case demonstrates superior predictive performance of the parametric Phi Method.","Across both test cases, parametric and ensemble Phi Method reliably recover governing parametric PDEs and offer accurate predictions over test parameters.","Ensemble ROM analysis underscores Phi Method's robust learning of dominant dynamic coefficients with high confidence."],"url":"http://arxiv.org/abs/2403.01532v1","category":"physics.plasm-ph"}
{"created":"2024-03-03 14:03:48","title":"Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models","abstract":"We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context learning and fine tuning blurs: both are methods to condition the model on previously observed tokens.","sentences":["We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation.","While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience.","We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates.","Our empirical study provides insights on when online adaptation is particularly interesting.","We highlight that with online adaptation the conceptual distinction between in-context learning and fine tuning blurs: both are methods to condition the model on previously observed tokens."],"url":"http://arxiv.org/abs/2403.01518v1","category":"cs.CL"}
{"created":"2024-03-03 13:36:07","title":"CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge Detection and Dual-Path SENet Feature Fusion","abstract":"Accurate segmentation of COVID-19 CT images is crucial for reducing the severity and mortality rates associated with COVID-19 infections. In response to blurred boundaries and high variability characteristic of lesion areas in COVID-19 CT images, we introduce CDSE-UNet: a novel UNet-based segmentation model that integrates Canny operator edge detection and a dual-path SENet feature fusion mechanism. This model enhances the standard UNet architecture by employing the Canny operator for edge detection in sample images, paralleling this with a similar network structure for semantic feature extraction. A key innovation is the Double SENet Feature Fusion Block, applied across corresponding network layers to effectively combine features from both image paths. Moreover, we have developed a Multiscale Convolution approach, replacing the standard Convolution in UNet, to adapt to the varied lesion sizes and shapes. This addition not only aids in accurately classifying lesion edge pixels but also significantly improves channel differentiation and expands the capacity of the model. Our evaluations on public datasets demonstrate CDSE-UNet's superior performance over other leading models, particularly in segmenting large and small lesion areas, accurately delineating lesion edges, and effectively suppressing noise","sentences":["Accurate segmentation of COVID-19 CT images is crucial for reducing the severity and mortality rates associated with COVID-19 infections.","In response to blurred boundaries and high variability characteristic of lesion areas in COVID-19 CT images, we introduce CDSE-UNet: a novel UNet-based segmentation model that integrates Canny operator edge detection and a dual-path SENet feature fusion mechanism.","This model enhances the standard UNet architecture by employing the Canny operator for edge detection in sample images, paralleling this with a similar network structure for semantic feature extraction.","A key innovation is the Double SENet Feature Fusion Block, applied across corresponding network layers to effectively combine features from both image paths.","Moreover, we have developed a Multiscale Convolution approach, replacing the standard Convolution in UNet, to adapt to the varied lesion sizes and shapes.","This addition not only aids in accurately classifying lesion edge pixels but also significantly improves channel differentiation and expands the capacity of the model.","Our evaluations on public datasets demonstrate CDSE-UNet's superior performance over other leading models, particularly in segmenting large and small lesion areas, accurately delineating lesion edges, and effectively suppressing noise"],"url":"http://arxiv.org/abs/2403.01513v1","category":"eess.IV"}
{"created":"2024-03-03 12:34:13","title":"Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network","abstract":"Graph Neural Networks (GNNs) have garnered intensive attention for Network Intrusion Detection System (NIDS) due to their suitability for representing the network traffic flows. However, most present GNN-based methods for NIDS are supervised or semi-supervised. Network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making NIDS difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios. The existing GNN-based self-supervised methods focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice. This paper studies the application of GNNs to identify the specific types of network flows in an unsupervised manner. We first design an encoder to obtain graph embedding, that introduces the graph attention mechanism and considers the edge information as the only essential factor. Then, a self-supervised method based on graph contrastive learning is proposed. The method samples center nodes, and for each center node, generates subgraph by it and its direct neighbor nodes, and corresponding contrastive subgraph from the interpolated graph, and finally constructs positive and negative samples from subgraphs. Furthermore, a structured contrastive loss function based on edge features and graph local topology is introduced. To the best of our knowledge, it is the first GNN-based self-supervised method for the multiclass classification of network flows in NIDS. Detailed experiments conducted on four real-world databases (NF-Bot-IoT, NF-Bot-IoT-v2, NF-CSE-CIC-IDS2018, and NF-CSE-CIC-IDS2018-v2) systematically compare our model with the state-of-the-art supervised and self-supervised models, illustrating the considerable potential of our method. Our code is accessible through https://github.com/renj-xu/NEGSC.","sentences":["Graph Neural Networks (GNNs) have garnered intensive attention for Network Intrusion Detection System (NIDS) due to their suitability for representing the network traffic flows.","However, most present GNN-based methods for NIDS are supervised or semi-supervised.","Network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making NIDS difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios.","The existing GNN-based self-supervised methods focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice.","This paper studies the application of GNNs to identify the specific types of network flows in an unsupervised manner.","We first design an encoder to obtain graph embedding, that introduces the graph attention mechanism and considers the edge information as the only essential factor.","Then, a self-supervised method based on graph contrastive learning is proposed.","The method samples center nodes, and for each center node, generates subgraph by it and its direct neighbor nodes, and corresponding contrastive subgraph from the interpolated graph, and finally constructs positive and negative samples from subgraphs.","Furthermore, a structured contrastive loss function based on edge features and graph local topology is introduced.","To the best of our knowledge, it is the first GNN-based self-supervised method for the multiclass classification of network flows in NIDS.","Detailed experiments conducted on four real-world databases (NF-Bot-IoT, NF-Bot-IoT-v2, NF-CSE-CIC-IDS2018, and NF-CSE-CIC-IDS2018-v2) systematically compare our model with the state-of-the-art supervised and self-supervised models, illustrating the considerable potential of our method.","Our code is accessible through https://github.com/renj-xu/NEGSC."],"url":"http://arxiv.org/abs/2403.01501v1","category":"cs.LG"}
{"created":"2024-03-03 12:23:17","title":"Normalising Flow-based Differentiable Particle Filters","abstract":"Recently, there has been a surge of interest in incorporating neural networks into particle filters, e.g. differentiable particle filters, to perform joint sequential state estimation and model learning for non-linear non-Gaussian state-space models in complex environments. Existing differentiable particle filters are mostly constructed with vanilla neural networks that do not allow density estimation. As a result, they are either restricted to a bootstrap particle filtering framework or employ predefined distribution families (e.g. Gaussian distributions), limiting their performance in more complex real-world scenarios. In this paper we present a differentiable particle filtering framework that uses (conditional) normalising flows to build its dynamic model, proposal distribution, and measurement model. This not only enables valid probability densities but also allows the proposed method to adaptively learn these modules in a flexible way, without being restricted to predefined distribution families. We derive the theoretical properties of the proposed filters and evaluate the proposed normalising flow-based differentiable particle filters' performance through a series of numerical experiments.","sentences":["Recently, there has been a surge of interest in incorporating neural networks into particle filters, e.g. differentiable particle filters, to perform joint sequential state estimation and model learning for non-linear non-Gaussian state-space models in complex environments.","Existing differentiable particle filters are mostly constructed with vanilla neural networks that do not allow density estimation.","As a result, they are either restricted to a bootstrap particle filtering framework or employ predefined distribution families (e.g. Gaussian distributions), limiting their performance in more complex real-world scenarios.","In this paper we present a differentiable particle filtering framework that uses (conditional) normalising flows to build its dynamic model, proposal distribution, and measurement model.","This not only enables valid probability densities but also allows the proposed method to adaptively learn these modules in a flexible way, without being restricted to predefined distribution families.","We derive the theoretical properties of the proposed filters and evaluate the proposed normalising flow-based differentiable particle filters' performance through a series of numerical experiments."],"url":"http://arxiv.org/abs/2403.01499v1","category":"cs.LG"}
{"created":"2024-03-03 12:05:49","title":"ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis","abstract":"This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of traditional convolutional networks. Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units. This setting avoids the sparsity semantics associated with raw point-level time steps. Secondly, we design a fully convolutional block by skillfully integrating deepwise and pointwise convolution operations, following the advanced building block style employed in Transformer encoders. This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of Transformer architecture but also inherits the inherent properties of convolution. Furthermore, multi-scale representations of given time series instances can be learned by controlling the kernel size flexibly. Extensive experiments are conducted on both time series forecasting and classification tasks. The results consistently outperformed strong baselines in most situations in terms of effectiveness.The code is publicly available.","sentences":["This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis.","The key design of this network is twofold, designed to overcome the limitations of traditional convolutional networks.","Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units.","This setting avoids the sparsity semantics associated with raw point-level time steps.","Secondly, we design a fully convolutional block by skillfully integrating deepwise and pointwise convolution operations, following the advanced building block style employed in Transformer encoders.","This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of Transformer architecture but also inherits the inherent properties of convolution.","Furthermore, multi-scale representations of given time series instances can be learned by controlling the kernel size flexibly.","Extensive experiments are conducted on both time series forecasting and classification tasks.","The results consistently outperformed strong baselines in most situations in terms of effectiveness.","The code is publicly available."],"url":"http://arxiv.org/abs/2403.01493v1","category":"cs.LG"}
{"created":"2024-03-03 11:55:49","title":"Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models","abstract":"Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image generative models. Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios. (3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing. We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA. We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of text-to-image generative models.","sentences":["Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions.","While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images.","In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models.","Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images.","By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image.","This attribution allows model owners to be held accountable for any misuse of their models.","Note that our approach does not limit the number of candidate text-to-image generative models.","Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios.","(3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing.","We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA.","We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of text-to-image generative models."],"url":"http://arxiv.org/abs/2403.01489v1","category":"cs.CV"}
{"created":"2024-03-03 11:38:58","title":"An RBF partition of unity method for geometry reconstruction and PDE solution in thin structures","abstract":"The main respiratory muscle, the diaphragm, is an example of a thin structure. We aim to perform detailed numerical simulations of the muscle mechanics based on individual patient data. This requires a representation of the diaphragm geometry extracted from medical image data. We design an adaptive reconstruction method based on a least-squares radial basis function partition of unity method. The method is adapted to thin structures by subdividing the structure rather than the surrounding space, and by introducing an anisotropic scaling of local subproblems. The resulting representation is an infinitely smooth level set function, which is stabilized such that there are no spurious zero level sets. We show reconstruction results for 2D cross sections of the diaphragm geometry as well as for the full 3D geometry. We also show solutions to basic PDE test problems in the reconstructed geometries.","sentences":["The main respiratory muscle, the diaphragm, is an example of a thin structure.","We aim to perform detailed numerical simulations of the muscle mechanics based on individual patient data.","This requires a representation of the diaphragm geometry extracted from medical image data.","We design an adaptive reconstruction method based on a least-squares radial basis function partition of unity method.","The method is adapted to thin structures by subdividing the structure rather than the surrounding space, and by introducing an anisotropic scaling of local subproblems.","The resulting representation is an infinitely smooth level set function, which is stabilized such that there are no spurious zero level sets.","We show reconstruction results for 2D cross sections of the diaphragm geometry as well as for the full 3D geometry.","We also show solutions to basic PDE test problems in the reconstructed geometries."],"url":"http://arxiv.org/abs/2403.01486v1","category":"math.NA"}
{"created":"2024-03-03 11:27:34","title":"BronchoCopilot: Towards Autonomous Robotic Bronchoscopy via Multimodal Reinforcement Learning","abstract":"Bronchoscopy plays a significant role in the early diagnosis and treatment of lung diseases. This process demands physicians to maneuver the flexible endoscope for reaching distal lesions, particularly requiring substantial expertise when examining the airways of the upper lung lobe. With the development of artificial intelligence and robotics, reinforcement learning (RL) method has been applied to the manipulation of interventional surgical robots. However, unlike human physicians who utilize multimodal information, most of the current RL methods rely on a single modality, limiting their performance. In this paper, we propose BronchoCopilot, a multimodal RL agent designed to acquire manipulation skills for autonomous bronchoscopy. BronchoCopilot specifically integrates images from the bronchoscope camera and estimated robot poses, aiming for a higher success rate within challenging airway environment. We employ auxiliary reconstruction tasks to compress multimodal data and utilize attention mechanisms to achieve an efficient latent representation of this data, serving as input for the RL module. This framework adopts a stepwise training and fine-tuning approach to mitigate the challenges of training difficulty. Our evaluation in the realistic simulation environment reveals that BronchoCopilot, by effectively harnessing multimodal information, attains a success rate of approximately 90\\% in fifth generation airways with consistent movements. Additionally, it demonstrates a robust capacity to adapt to diverse cases.","sentences":["Bronchoscopy plays a significant role in the early diagnosis and treatment of lung diseases.","This process demands physicians to maneuver the flexible endoscope for reaching distal lesions, particularly requiring substantial expertise when examining the airways of the upper lung lobe.","With the development of artificial intelligence and robotics, reinforcement learning (RL) method has been applied to the manipulation of interventional surgical robots.","However, unlike human physicians who utilize multimodal information, most of the current RL methods rely on a single modality, limiting their performance.","In this paper, we propose BronchoCopilot, a multimodal RL agent designed to acquire manipulation skills for autonomous bronchoscopy.","BronchoCopilot specifically integrates images from the bronchoscope camera and estimated robot poses, aiming for a higher success rate within challenging airway environment.","We employ auxiliary reconstruction tasks to compress multimodal data and utilize attention mechanisms to achieve an efficient latent representation of this data, serving as input for the RL module.","This framework adopts a stepwise training and fine-tuning approach to mitigate the challenges of training difficulty.","Our evaluation in the realistic simulation environment reveals that BronchoCopilot, by effectively harnessing multimodal information, attains a success rate of approximately 90\\% in fifth generation airways with consistent movements.","Additionally, it demonstrates a robust capacity to adapt to diverse cases."],"url":"http://arxiv.org/abs/2403.01483v1","category":"cs.RO"}
{"created":"2024-03-03 11:13:44","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation","abstract":"The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb and WMT-2014 En->De, respectively, compared to Transformer baselines.","sentences":["The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation.","Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model.","However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from.","In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training.","The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem.","Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb and WMT-2014 En->De, respectively, compared to Transformer baselines."],"url":"http://arxiv.org/abs/2403.01479v1","category":"cs.CL"}
{"created":"2024-03-03 10:48:45","title":"Squares of symmetric operators","abstract":"Using the approach proposed in [5] , in an infinite-dimensional separable complex Hilbert space we give abstract constructions of families $\\{{\\mathcal T}_z\\}_{{\\rm Im\\,} z>0}$ of closed densely defined symmetric operators with the properties: (I) the domain of ${\\mathcal T}_z^2$ is a core of ${\\mathcal T}_z$, (II) the domain of ${\\mathcal T}_z^2$ is dense but note a core of ${\\mathcal T}_z$, (III) the domain of ${\\mathcal T}_z^2$ is nontrivial but non-dense. For this purpose a class of maximal dissipative operators is defined and studied. The case ${\\rm dom\\,} {\\mathcal T}_z^2=\\{0\\}$ has been considered in [5].   Given a densely defined closed symmetric operator $S$, in terms of the intersection of the domain of $S$ with ${\\rm ran\\,} (S-\\lambda I)$ and the projection of the domain of the adjoint $S^*$ on ${\\rm ran\\,} (S-\\lambda I)$, $\\lambda\\in{\\mathbb C}\\setminus{\\mathbb R}$, necessary and sufficient conditions for the cases (I)--(III) related to the domain of $S^2$, are obtained.","sentences":["Using the approach proposed in [5] , in an infinite-dimensional separable complex Hilbert space we give abstract constructions of families $\\{{\\mathcal T}_z\\}_{{\\rm Im\\,} z>0}$ of closed densely defined symmetric operators with the properties: (I) the domain of ${\\mathcal T}_z^2$ is a core of ${\\mathcal T}_z$, (II) the domain of ${\\mathcal T}_z^2$ is dense but note a core of ${\\mathcal T}_z$, (III) the domain of ${\\mathcal T}_z^2$ is nontrivial but non-dense.","For this purpose a class of maximal dissipative operators is defined and studied.","The case ${\\rm dom\\,} {\\mathcal T}_z^2=\\{0\\}$ has been considered in [5].   ","Given a densely defined closed symmetric operator $S$, in terms of the intersection of the domain of $S$ with ${\\rm ran\\,} (S-\\lambda I)$ and the projection of the domain of the adjoint $S^*$ on ${\\rm ran\\,} (S-\\lambda I)$, $\\lambda\\in{\\mathbb C}\\setminus{\\mathbb R}$, necessary and sufficient conditions for the cases (I)--(III) related to the domain of $S^2$, are obtained."],"url":"http://arxiv.org/abs/2403.01473v1","category":"math.FA"}
{"created":"2024-03-03 10:23:08","title":"Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation","abstract":"Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive learning; and (3) the updated graph serves as an input to facilitate the subsequent iteration of model adaptation, thereby establishing a collaborative loop between model adaptation and graph adaptation. Comprehensive experiments are conducted on various public datasets. The experimental results demonstrate that our proposed model outperforms recent source-free baselines by large margins.","sentences":["Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph.","However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns.","In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph.","Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive learning; and (3) the updated graph serves as an input to facilitate the subsequent iteration of model adaptation, thereby establishing a collaborative loop between model adaptation and graph adaptation.","Comprehensive experiments are conducted on various public datasets.","The experimental results demonstrate that our proposed model outperforms recent source-free baselines by large margins."],"url":"http://arxiv.org/abs/2403.01467v1","category":"cs.LG"}
{"created":"2024-03-03 10:19:18","title":"Multiview Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks","abstract":"High-dimensional and complex spectral structures make clustering of hy-perspectral images (HSI) a challenging task. Subspace clustering has been shown to be an effective approach for addressing this problem. However, current subspace clustering algorithms are mainly designed for a single view and do not fully exploit spatial or texture feature information in HSI. This study proposed a multiview subspace clustering of HSI based on graph convolutional networks. (1) This paper uses the powerful classification ability of graph convolutional network and the learning ability of topologi-cal relationships between nodes to analyze and express the spatial relation-ship of HSI. (2) Pixel texture and pixel neighbor spatial-spectral infor-mation were sent to construct two graph convolutional subspaces. (3) An attention-based fusion module was used to adaptively construct a more discriminative feature map. The model was evaluated on three popular HSI datasets, including Indian Pines, Pavia University, and Houston. It achieved overall accuracies of 92.38%, 93.43%, and 83.82%, respectively and significantly outperformed the state-of-the-art clustering methods. In conclusion, the proposed model can effectively improve the clustering ac-curacy of HSI.","sentences":["High-dimensional and complex spectral structures make clustering of hy-perspectral images (HSI) a challenging task.","Subspace clustering has been shown to be an effective approach for addressing this problem.","However, current subspace clustering algorithms are mainly designed for a single view and do not fully exploit spatial or texture feature information in HSI.","This study proposed a multiview subspace clustering of HSI based on graph convolutional networks.","(1) This paper uses the powerful classification ability of graph convolutional network and the learning ability of topologi-cal relationships between nodes to analyze and express the spatial relation-ship of HSI.","(2) Pixel texture and pixel neighbor spatial-spectral infor-mation were sent to construct two graph convolutional subspaces.","(3) An attention-based fusion module was used to adaptively construct a more discriminative feature map.","The model was evaluated on three popular HSI datasets, including Indian Pines, Pavia University, and Houston.","It achieved overall accuracies of 92.38%, 93.43%, and 83.82%, respectively and significantly outperformed the state-of-the-art clustering methods.","In conclusion, the proposed model can effectively improve the clustering ac-curacy of HSI."],"url":"http://arxiv.org/abs/2403.01465v1","category":"cs.CV"}
{"created":"2024-03-03 09:18:05","title":"Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment","abstract":"Item difficulty plays a crucial role in adaptive testing. However, few works have focused on generating questions of varying difficulty levels, especially for multiple-choice (MC) cloze tests. We propose training pre-trained language models (PLMs) as surrogate models to enable item response theory (IRT) assessment, avoiding the need for human test subjects. We also propose two strategies to control the difficulty levels of both the gaps and the distractors using ranking rules to reduce invalid distractors. Experimentation on a benchmark dataset demonstrates that our proposed framework and methods can effectively control and evaluate the difficulty levels of MC cloze tests.","sentences":["Item difficulty plays a crucial role in adaptive testing.","However, few works have focused on generating questions of varying difficulty levels, especially for multiple-choice (MC) cloze tests.","We propose training pre-trained language models (PLMs) as surrogate models to enable item response theory (IRT) assessment, avoiding the need for human test subjects.","We also propose two strategies to control the difficulty levels of both the gaps and the distractors using ranking rules to reduce invalid distractors.","Experimentation on a benchmark dataset demonstrates that our proposed framework and methods can effectively control and evaluate the difficulty levels of MC cloze tests."],"url":"http://arxiv.org/abs/2403.01456v1","category":"cs.CL"}
{"created":"2024-03-03 08:42:40","title":"3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos","abstract":"Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods.","sentences":["Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor.","Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering.","To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes.","Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS.","Specifically, we utilize 3D Gaussians (3DGs) to represent the scene.","Instead of the na\\\"ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame.","Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes.","Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.01444v2","category":"cs.CV"}
{"created":"2024-03-03 08:25:04","title":"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis","abstract":"Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal trade-off between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.","sentences":["Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models.","However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space.","In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal trade-off between task performance and parameter efficiency.","To achieve this goal, we freeze the parameters of the default pre-trained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task.","We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction.","Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively.","Code is available at https://github.com/LMD0311/DAPT."],"url":"http://arxiv.org/abs/2403.01439v1","category":"cs.CV"}
{"created":"2024-03-05 18:15:57","title":"Reduction of Cosymplectic Groupoids","abstract":"There is a well-known fact in Poisson geometry that reduction commutes with integration (of the associated integrable Lie algebroid). This is also valid for other types of geometries given by 2-dimensional closed forms. In this manuscript we extend this type of result to the particular case of cosymplectic groupoids, obtaining in particular the reduction of a central extension defined by a 2-IM form and a 1-IM form.","sentences":["There is a well-known fact in Poisson geometry that reduction commutes with integration (of the associated integrable Lie algebroid).","This is also valid for other types of geometries given by 2-dimensional closed forms.","In this manuscript we extend this type of result to the particular case of cosymplectic groupoids, obtaining in particular the reduction of a central extension defined by a 2-IM form and a 1-IM form."],"url":"http://arxiv.org/abs/2403.03178v1","category":"math.SG"}
{"created":"2024-03-05 17:56:10","title":"Statistical modeling of equilibrium phase transition in confined fluids","abstract":"The phase transition of confined fluids in mesoporous materials deviates from that of bulk fluids due to the former's interactions with the surrounding heterogeneous structure. For example, metal-organic frameworks (MOFs) create a strong heterogeneous field, so adsorbed fluids in MOFs have atypical phase characteristics such as capillary condensation and higher-order phase transitions. These characteristics are modeled by decoupling the host-guest and guest-guest interactions as a many-body problem in the presence of an external nonuniform field. To solve the three-dimensional Ising model, we use mean-field theory to approximate the guest-guest interactions and Mayer's (f)-functions to describe the host-guest interactions in a unit cell. Later, using Hill's theory of nanothermodynamics, we define differential and integral thermodynamic functions to describe confined fluids. These integral properties are then used to understand the phase transition in confined fluids. The investigation reveals a distinct behavior where fluids confined in larger pores undergo a discontinuous (first-order) phase transition, whereas those confined in smaller pores undergo a continuous (higher-order) phase transition. Furthermore, the results indicate that the free-energy barrier for phase transitions is lower in confined fluids than in bulk fluids, which helps explain the lower condensation pressure relative to the bulk saturation pressure. Finally, the integral thermodynamic functions are succinctly presented in the form of a phase diagram, marking an initial step toward a more practical approach for understanding the phase behavior of confined fluids.","sentences":["The phase transition of confined fluids in mesoporous materials deviates from that of bulk fluids due to the former's interactions with the surrounding heterogeneous structure.","For example, metal-organic frameworks (MOFs) create a strong heterogeneous field, so adsorbed fluids in MOFs have atypical phase characteristics such as capillary condensation and higher-order phase transitions.","These characteristics are modeled by decoupling the host-guest and guest-guest interactions as a many-body problem in the presence of an external nonuniform field.","To solve the three-dimensional Ising model, we use mean-field theory to approximate the guest-guest interactions and Mayer's (f)-functions to describe the host-guest interactions in a unit cell.","Later, using Hill's theory of nanothermodynamics, we define differential and integral thermodynamic functions to describe confined fluids.","These integral properties are then used to understand the phase transition in confined fluids.","The investigation reveals a distinct behavior where fluids confined in larger pores undergo a discontinuous (first-order) phase transition, whereas those confined in smaller pores undergo a continuous (higher-order) phase transition.","Furthermore, the results indicate that the free-energy barrier for phase transitions is lower in confined fluids than in bulk fluids, which helps explain the lower condensation pressure relative to the bulk saturation pressure.","Finally, the integral thermodynamic functions are succinctly presented in the form of a phase diagram, marking an initial step toward a more practical approach for understanding the phase behavior of confined fluids."],"url":"http://arxiv.org/abs/2403.03162v1","category":"cond-mat.soft"}
{"created":"2024-03-05 17:20:19","title":"Asymptotic expansions with subordinate variables for solutions of the Navier-Stokes equations","abstract":"We study the three-dimensional Navier-Stokes equations in a periodic domain with the force decaying in time. Although the force has a certain coherent decay, as time tends to infinity, it can be too complicated for the previous theory of asymptotic expansions to be applicable. To deal with this issue, we systematically develop a new theory of asymptotic expansions containing the so-called subordinate variables which can be defined recursively. We apply it to obtain an asymptotic expansion for any Leray-Hopf weak solutions. The expansion, in fact, is constructed explicitly and the impact of the subordinate variables can be clearly specified. The complexifications of the Gevrey-Sobolev spaces, and of the Stokes and bilinear operators of the Navier-Stokes equations are utilized to facilitate such a construction.","sentences":["We study the three-dimensional Navier-Stokes equations in a periodic domain with the force decaying in time.","Although the force has a certain coherent decay, as time tends to infinity, it can be too complicated for the previous theory of asymptotic expansions to be applicable.","To deal with this issue, we systematically develop a new theory of asymptotic expansions containing the so-called subordinate variables which can be defined recursively.","We apply it to obtain an asymptotic expansion for any Leray-Hopf weak solutions.","The expansion, in fact, is constructed explicitly and the impact of the subordinate variables can be clearly specified.","The complexifications of the Gevrey-Sobolev spaces, and of the Stokes and bilinear operators of the Navier-Stokes equations are utilized to facilitate such a construction."],"url":"http://arxiv.org/abs/2403.03132v1","category":"math.AP"}
{"created":"2024-03-05 16:56:39","title":"An elliptic problem in dimension N with a varying drift term bounded in $L^N$","abstract":"The present paper is devoted to study the asymptotic behavior of a sequence of linear elliptic equations with a varying drift term, whose coefficients are just bounded in $L^N(\\Omega)$, with $N$ the dimension of the space. It is known that there exists a unique solution for each of these problems in the Sobolev space $H^1_0(\\Omega)$. However, because the operators are not coercive, there is no uniform estimate of the solutions in this space. We use some estimates in \\cite{boc1}, and a regularization obtained by adding a small nonlinear first order term, to pass to the limit in these problems.","sentences":["The present paper is devoted to study the asymptotic behavior of a sequence of linear elliptic equations with a varying drift term, whose coefficients are just bounded in $L^N(\\Omega)$, with $N$ the dimension of the space.","It is known that there exists a unique solution for each of these problems in the Sobolev space $H^1_0(\\Omega)$.","However, because the operators are not coercive, there is no uniform estimate of the solutions in this space.","We use some estimates in \\cite{boc1}, and a regularization obtained by adding a small nonlinear first order term, to pass to the limit in these problems."],"url":"http://arxiv.org/abs/2403.03115v1","category":"math.AP"}
{"created":"2024-03-05 16:03:34","title":"Topological balance of cell distributions in plane monolayers","abstract":"Most of normal proliferative epithelia of plants and metazoans are topologically invariant and characterized by similar cell distributions according to the number of cell neighbors (DCNs). Here we study peculiarities of these distributions and explain why the DCN obtained from the location of intercellular boundaries and that based on the Voronoi tessellation with nodes located on cell nuclei may differ from each other. As we demonstrate, special microdomains where four or more intercellular boundaries converge are topologically charged. Using this fact, we deduce a new equation describing the topological balance of the DCNs. The developed theory is applied for a series of microphotographs of non-tumoral epithelial cells of the human cervix (HCerEpiC) to improve the image processing near the edges of microphotographs and reveal the topological invariance of the examined monolayers. Special contact microdomains may be present in epithelia of various natures, however, considering the well-known vertex model of epithelium, we show that such contacts are absent in the usual solid-like state of the model and appear only in the liquid-like cancer state. Also, we discuss a possible biological role of special contacts in context of proliferative epithelium dynamics and tissue morphogenesis.","sentences":["Most of normal proliferative epithelia of plants and metazoans are topologically invariant and characterized by similar cell distributions according to the number of cell neighbors (DCNs).","Here we study peculiarities of these distributions and explain why the DCN obtained from the location of intercellular boundaries and that based on the Voronoi tessellation with nodes located on cell nuclei may differ from each other.","As we demonstrate, special microdomains where four or more intercellular boundaries converge are topologically charged.","Using this fact, we deduce a new equation describing the topological balance of the DCNs.","The developed theory is applied for a series of microphotographs of non-tumoral epithelial cells of the human cervix (HCerEpiC) to improve the image processing near the edges of microphotographs and reveal the topological invariance of the examined monolayers.","Special contact microdomains may be present in epithelia of various natures, however, considering the well-known vertex model of epithelium, we show that such contacts are absent in the usual solid-like state of the model and appear only in the liquid-like cancer state.","Also, we discuss a possible biological role of special contacts in context of proliferative epithelium dynamics and tissue morphogenesis."],"url":"http://arxiv.org/abs/2403.03079v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:01:28","title":"Fast and robust method for screened Poisson lattice Green's function using asymptotic expansion and Fast Fourier Transform","abstract":"We study the lattice Green's function (LGF) of the screened Poisson equation on a two-dimensional rectangular lattice. This LGF arises in numerical analysis, random walks, solid-state physics, and other fields. Its defining characteristic is the screening term, which defines different regimes. When its coefficient is large, we can accurately approximate the LGF with an exponentially converging asymptotic expansion, and its convergence rate monotonically increases with the coefficient of the screening term. To tabulate the LGF when the coefficient is not large, we derive a one-dimensional integral representation of the LGF. We show that the trapezoidal rule can approximate this integral with exponential convergence, and we propose an efficient algorithm for its evaluation via the Fast Fourier Transform. We discuss applications including computing the LGF of the three-dimensional Poisson equation with one periodic direction and the return probability of a two-dimensional random walk with killing.","sentences":["We study the lattice Green's function (LGF) of the screened Poisson equation on a two-dimensional rectangular lattice.","This LGF arises in numerical analysis, random walks, solid-state physics, and other fields.","Its defining characteristic is the screening term, which defines different regimes.","When its coefficient is large, we can accurately approximate the LGF with an exponentially converging asymptotic expansion, and its convergence rate monotonically increases with the coefficient of the screening term.","To tabulate the LGF when the coefficient is not large, we derive a one-dimensional integral representation of the LGF.","We show that the trapezoidal rule can approximate this integral with exponential convergence, and we propose an efficient algorithm for its evaluation via the Fast Fourier Transform.","We discuss applications including computing the LGF of the three-dimensional Poisson equation with one periodic direction and the return probability of a two-dimensional random walk with killing."],"url":"http://arxiv.org/abs/2403.03076v1","category":"math.NA"}
{"created":"2024-03-05 14:03:56","title":"Assessing the distribution of cancer stem cells in tumorspheres","abstract":"In previous theoretical research, we inferred that cancer stem cells (CSCs), the cells that presumably drive tumor growth and resistance to conventional cancer treatments, are not uniformly distributed in the bulk of a tumorsphere. To confirm this theoretical prediction, we cultivated tumorspheres enriched in CSCs, and performed immunofluorecent detection of the stemness marker SOX2 using a confocal microscope.   In this article, we present a method developed to process the images that reconstruct the amount and location of the CSCs in the tumorspheres. Its advantage is the use of a statistical criterion to classify the cells in stem and differentiated instead of setting an arbitrary threshold. From the analysis of the results of the methods using graph theory and computational modeling, we concluded that the distribution of Cancer Stem Cells in an experimental tumorsphere is non-homogeneous. This method is independent of the tumorsphere assay being useful for analyzing images in which several different kinds of cells are stained with different markers.","sentences":["In previous theoretical research, we inferred that cancer stem cells (CSCs), the cells that presumably drive tumor growth and resistance to conventional cancer treatments, are not uniformly distributed in the bulk of a tumorsphere.","To confirm this theoretical prediction, we cultivated tumorspheres enriched in CSCs, and performed immunofluorecent detection of the stemness marker SOX2 using a confocal microscope.   ","In this article, we present a method developed to process the images that reconstruct the amount and location of the CSCs in the tumorspheres.","Its advantage is the use of a statistical criterion to classify the cells in stem and differentiated instead of setting an arbitrary threshold.","From the analysis of the results of the methods using graph theory and computational modeling, we concluded that the distribution of Cancer Stem Cells in an experimental tumorsphere is non-homogeneous.","This method is independent of the tumorsphere assay being useful for analyzing images in which several different kinds of cells are stained with different markers."],"url":"http://arxiv.org/abs/2403.02984v1","category":"q-bio.QM"}
{"created":"2024-03-05 13:48:55","title":"Periodically activated physics-informed neural networks for assimilation tasks for three-dimensional Rayleigh-Benard convection","abstract":"We apply physics-informed neural networks to three-dimensional Rayleigh-Benard convection in a cubic cell with a Rayleigh number of Ra=10^6 and a Prandtl number of Pr=0.7 to assimilate the velocity vector field from given temperature fields and vice versa. With the respective ground truth data provided by a direct numerical simulation, we are able to evaluate the performance of the different activation functions applied (sine, hyperbolic tangent and exponential linear unit) and different numbers of neuron (32, 64, 128) for each of the five hidden layers of the multi-layer perceptron. The main result is that the use of a periodic activation function (sine) typically benefits the assimilation performance in terms of the analyzed metrics, correlation with the ground truth and mean average error. The higher quality of results from sine-activated physics-informed neural networks is also manifested in the probability density function and power spectra of the inferred velocity or temperature fields. Regarding the two assimilation directions, the assimilation of temperature fields based on velocities appeared to be more challenging in the sense that it exhibited a sharper limit on the number of neurons below which viable assimilation results could not be achieved.","sentences":["We apply physics-informed neural networks to three-dimensional Rayleigh-Benard convection in a cubic cell with a Rayleigh number of Ra=10^6 and a Prandtl number of Pr=0.7 to assimilate the velocity vector field from given temperature fields and vice versa.","With the respective ground truth data provided by a direct numerical simulation, we are able to evaluate the performance of the different activation functions applied (sine, hyperbolic tangent and exponential linear unit) and different numbers of neuron (32, 64, 128) for each of the five hidden layers of the multi-layer perceptron.","The main result is that the use of a periodic activation function (sine) typically benefits the assimilation performance in terms of the analyzed metrics, correlation with the ground truth and mean average error.","The higher quality of results from sine-activated physics-informed neural networks is also manifested in the probability density function and power spectra of the inferred velocity or temperature fields.","Regarding the two assimilation directions, the assimilation of temperature fields based on velocities appeared to be more challenging in the sense that it exhibited a sharper limit on the number of neurons below which viable assimilation results could not be achieved."],"url":"http://arxiv.org/abs/2403.02970v1","category":"physics.flu-dyn"}
{"created":"2024-03-05 12:41:21","title":"Weighted floating functions and weighted functional affine surface areas","abstract":"The purpose of this paper is to introduce the new concept of weighted floating functions associated with log concave or $s$-concave functions. This leads to new notions of weighted functional affine surface areas. Their relation to more traditional versions of functional affine surface areas as well as to the classical affine surface areas for convex bodies is discussed in detail.","sentences":["The purpose of this paper is to introduce the new concept of weighted floating functions associated with log concave or $s$-concave functions.","This leads to new notions of weighted functional affine surface areas.","Their relation to more traditional versions of functional affine surface areas as well as to the classical affine surface areas for convex bodies is discussed in detail."],"url":"http://arxiv.org/abs/2403.02925v1","category":"math.MG"}
{"created":"2024-03-05 12:35:18","title":"Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction","abstract":"In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper. We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot. To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE). To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation. Comparing a signal processing approach, with and without post-filtering, and a convolutional recurrent neural network (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation. These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch.","sentences":["In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper.","We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot.","To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE).","To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation.","Comparing a signal processing approach, with and without post-filtering, and a convolutional recurrent neural network (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation.","These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch."],"url":"http://arxiv.org/abs/2403.02918v1","category":"cs.RO"}
{"created":"2024-03-05 12:13:27","title":"Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects","abstract":"Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.","sentences":["Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts.","Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps.","Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted.","Therefore, researchers working in this area increasingly need support to process this incoming information.","One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest.","Another way is to automate the process with image recognition using convolutional neural networks.","During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis."],"url":"http://arxiv.org/abs/2403.02906v1","category":"cs.HC"}
{"created":"2024-03-05 12:10:11","title":"Two models forsandpile growth in weighted graphs","abstract":"In this paper we study $\\infty$-Laplacian type diffusion equations in weighted graphs obtained as limit as $p\\to \\infty$ to two types of $p$-Laplacian evolution equations in such graphs. We propose these diffusion equations, that are governed by the subdifferential of a convex energy functionals associated to the indicator function of the set $$K^G_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ : \\ \\vert u(y) - u(x) \\vert \\leq 1 \\ \\ \\hbox{if} \\ \\ x \\sim y \\right\\}$$ and the set $$K^w_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ : \\ \\vert u(y) - u(x) \\vert \\leq \\sqrt{w_{xy}} \\ \\ \\hbox{if} \\ \\ x \\sim y \\right\\}$$ as models for sandpile growth in weighted graphs. Moreover, we also analyse the collapse of the initial condition when it does not belong to the stable sets $K^G_{\\infty}$ or $K^w_{\\infty}$ by means of an abstract result given in~\\cite{BEG}. We give an interpretation of the limit problems in terms of Monge-Kantorovich mass transport theory. Finally, we give some explicit solutions of simple examples that illustrate the dynamics of the sandpile growing or collapsing.","sentences":["In this paper we study $\\infty$-Laplacian type diffusion equations in weighted graphs obtained as limit as $p\\to \\infty$ to two types of $p$-Laplacian evolution equations in such graphs.","We propose these diffusion equations, that are governed by the subdifferential of a convex energy functionals associated to the indicator function of the set $$K^G_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ :","\\ \\vert u(y) - u(x)","\\vert \\leq 1 \\ \\ \\hbox{if} \\ \\","x \\sim y \\right\\}$$ and the set $$K^w_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ :","\\ \\vert u(y) - u(x) \\vert \\leq \\sqrt{w_{xy}} \\ \\ \\hbox{if} \\ \\ x \\sim y \\right\\}$$ as models for sandpile growth in weighted graphs.","Moreover, we also analyse the collapse of the initial condition when it does not belong to the stable sets $K^G_{\\infty}$ or $K^w_{\\infty}$ by means of an abstract result given in~\\cite{BEG}.","We give an interpretation of the limit problems in terms of Monge-Kantorovich mass transport theory.","Finally, we give some explicit solutions of simple examples that illustrate the dynamics of the sandpile growing or collapsing."],"url":"http://arxiv.org/abs/2403.02900v1","category":"math.AP"}
{"created":"2024-03-05 11:51:00","title":"Coupling Polyatomic Molecules to Lossy Nanocavities: Lindblad versus Schr\u00f6dinger description","abstract":"The usage of cavities to impact molecular structure and dynamics has become popular. As cavities, in particular plasmonic nanocavities, are lossy and the lifetime of their modes can be very short, their lossy nature must be incorporated into the calculations. The Lindblad master equation is commonly considered as an appropriate tool to describe this lossy nature. This approach requires the dynamics of the density operator and is thus substantially more costly than approaches employing the Schr\\\"odinger equation for the quantum wave function when several or many nuclear degrees of freedom are involved. In this work we compare numerically the Lindblad and Schr\\\"odinger descriptions discussed in the literature for a molecular example where the cavity is pumped by a laser. The laser and cavity properties are varied over a range of parameters. It is found that the Schr\\\"odinger description adequately describes the dynamics of the polaritons and emission signal as long as the laser intensity is moderate and the pump time is not much longer than the lifetime of the cavity mode. Otherwise, it is demonstrated that the Schr\\\"odinger description gradually fails. The results are discussed and analyzed.","sentences":["The usage of cavities to impact molecular structure and dynamics has become popular.","As cavities, in particular plasmonic nanocavities, are lossy and the lifetime of their modes can be very short, their lossy nature must be incorporated into the calculations.","The Lindblad master equation is commonly considered as an appropriate tool to describe this lossy nature.","This approach requires the dynamics of the density operator and is thus substantially more costly than approaches employing the Schr\\\"odinger equation for the quantum wave function when several or many nuclear degrees of freedom are involved.","In this work we compare numerically the Lindblad and Schr\\\"odinger descriptions discussed in the literature for a molecular example where the cavity is pumped by a laser.","The laser and cavity properties are varied over a range of parameters.","It is found that the Schr\\\"odinger description adequately describes the dynamics of the polaritons and emission signal as long as the laser intensity is moderate and the pump time is not much longer than the lifetime of the cavity mode.","Otherwise, it is demonstrated that the Schr\\\"odinger description gradually fails.","The results are discussed and analyzed."],"url":"http://arxiv.org/abs/2403.02890v1","category":"physics.chem-ph"}
{"created":"2024-03-05 11:06:10","title":"On instabilities of perturbations in some homogeneous color-electric and -magnetic backgrounds in SU(2) gauge theory","abstract":"We consider the instabilities of field perturbations around a homogeneous background color-electric and/or -magnetic field in SU(2) pure gauge theory. We investigate a number of distinct cases of background magnetic and electric fields, and compute the dispersion relations in the linearised theory, identifying stable and unstable momentum modes. In the case of a net homogeneous non-abelian B-field, we compute the non-linear (quadratic and cubic) corrections to the equation of motion, and quantify to what extent the instabilities are tempered by these non-linearities.","sentences":["We consider the instabilities of field perturbations around a homogeneous background color-electric and/or -magnetic field in SU(2) pure gauge theory.","We investigate a number of distinct cases of background magnetic and electric fields, and compute the dispersion relations in the linearised theory, identifying stable and unstable momentum modes.","In the case of a net homogeneous non-abelian B-field, we compute the non-linear (quadratic and cubic) corrections to the equation of motion, and quantify to what extent the instabilities are tempered by these non-linearities."],"url":"http://arxiv.org/abs/2403.02859v1","category":"hep-ph"}
{"created":"2024-03-05 11:03:38","title":"Metrically differentiable set-valued functions and their local linear approximants","abstract":"A new notion of metric differentiability of set-valued functions at a point is introduced in terms of right and left limits of special set-valued metric divided differences of first order. A local metric linear approximant of a metrically differentiable set-valued function at a point is defined and studied. This local approximant may be regarded as a special realization of the set-valued Euler approximants of M.~S.~Nikolskii and the directives of Z.~Artstein. Error estimates for the local metric linear approximant are obtained. In particular, second order approximation is derived for a class of ``strongly'' metrically differentiable set-valued maps.","sentences":["A new notion of metric differentiability of set-valued functions at a point is introduced in terms of right and left limits of special set-valued metric divided differences of first order.","A local metric linear approximant of a metrically differentiable set-valued function at a point is defined and studied.","This local approximant may be regarded as a special realization of the set-valued Euler approximants of M.~S.~Nikolskii and the directives of Z.~Artstein.","Error estimates for the local metric linear approximant are obtained.","In particular, second order approximation is derived for a class of ``strongly'' metrically differentiable set-valued maps."],"url":"http://arxiv.org/abs/2403.02858v1","category":"math.CA"}
{"created":"2024-03-05 10:19:37","title":"Trajectory stabilization of nonlocal continuity equations by localized controls","abstract":"We discuss stabilization around trajectories of the continuity equation with nonlocal vector fields, where the control is localized, i.e., it acts on a fixed subset of the configuration space. We first show that the correct definition of stabilization is the following: given an initial error of order $\\varepsilon$, measured in Wasserstein distance, one can improve the final error to an order $\\varepsilon^{1+\\kappa}$ with $\\kappa>0$. We then prove the main result: assuming that the trajectory crosses the subset of control action, stabilization can be achieved. The key problem lies in regularity issues: the reference trajectory needs to be absolutely continuous, while the initial state to be stabilized needs to be realized by a small Lipschitz perturbation or being in a very small neighborhood of it.","sentences":["We discuss stabilization around trajectories of the continuity equation with nonlocal vector fields, where the control is localized, i.e., it acts on a fixed subset of the configuration space.","We first show that the correct definition of stabilization is the following: given an initial error of order $\\varepsilon$, measured in Wasserstein distance, one can improve the final error to an order $\\varepsilon^{1+\\kappa}$ with $\\kappa>0$. We then prove the main result: assuming that the trajectory crosses the subset of control action, stabilization can be achieved.","The key problem lies in regularity issues: the reference trajectory needs to be absolutely continuous, while the initial state to be stabilized needs to be realized by a small Lipschitz perturbation or being in a very small neighborhood of it."],"url":"http://arxiv.org/abs/2403.02837v1","category":"math.OC"}
{"created":"2024-03-05 09:29:24","title":"Translationally invariant shell model calculation of the quasielastic $(p,2p)$ process at intermediate relativistic energies","abstract":"Relativistic beams of heavy ions interacting with various nuclear targets allow to study a broad range of problems starting from nuclear equation of state to the traditional nuclear structure. Some questions which were impossible to answer heretofore -- can be addressed nowadays by using inverse kinematics. These includes the structure of short-lived nuclei and the precision study of exclusive channels with production of residual nuclei in certain quantum states. Theoretical understanding such processes is so far based on factorization models which combine the single-step amplitude of the reaction on a bound nucleon or nuclear cluster with a certain wave function of its relative motion with respect to the residual nucleus. The nuclear structure information is encoded in the spectroscopic amplitude, calculable within nuclear many-body theories. In this work, we use for this purpose the translationally-invariant shell model with configuration mixing and demonstrate that it successfully reproduces the single-differential and integrated cross sections of the quasielastic proton knockout, $^{12}\\mbox{C}(p,2p)^{11}\\mbox{B}$, with outgoing $^{11}$B in the ground state and low-lying excited states measured at GSI at 400 MeV/nucleon.","sentences":["Relativistic beams of heavy ions interacting with various nuclear targets allow to study a broad range of problems starting from nuclear equation of state to the traditional nuclear structure.","Some questions which were impossible to answer heretofore -- can be addressed nowadays by using inverse kinematics.","These includes the structure of short-lived nuclei and the precision study of exclusive channels with production of residual nuclei in certain quantum states.","Theoretical understanding such processes is so far based on factorization models which combine the single-step amplitude of the reaction on a bound nucleon or nuclear cluster with a certain wave function of its relative motion with respect to the residual nucleus.","The nuclear structure information is encoded in the spectroscopic amplitude, calculable within nuclear many-body theories.","In this work, we use for this purpose the translationally-invariant shell model with configuration mixing and demonstrate that it successfully reproduces the single-differential and integrated cross sections of the quasielastic proton knockout, $^{12}\\mbox{C}(p,2p)^{11}\\mbox{B}$, with outgoing $^{11}$B in the ground state and low-lying excited states measured at GSI at 400 MeV/nucleon."],"url":"http://arxiv.org/abs/2403.02812v1","category":"nucl-th"}
{"created":"2024-03-05 09:24:18","title":"Fiducial and differential cross-section measurements of electroweak $W\u03b3jj$ production in $pp$ collisions at $\\sqrt{s} = 13$ TeV with the ATLAS detector","abstract":"The observation of the electroweak production of a $W$ boson and a photon in association with two jets, using $pp$ collision data at the Large Hadron Collider at a centre of mass energy of $\\sqrt{s}=13$~TeV, is reported. The data were recorded by the ATLAS experiment from 2015 to 2018 and correspond to an integrated luminosity of 140 fb$^{-1}$. This process is sensitive to the quartic gauge boson couplings via the vector boson scattering mechanism and provides a stringent test of the electroweak gauge symmetry breaking of the Standard Model. Events are selected if they contain one electron or muon, missing transverse momentum, at least one photon, and two jets. Multivariate techniques are used to distinguish the electroweak $W\\gamma jj$ process from irreducible background processes. The observed significance of the electroweak $W\\gamma jj$ process is well above six standard deviations, compared to an expected significance of 6.3 standard deviations. Fiducial and differential cross sections are measured in a fiducial phase space close to the detector acceptance, which are in reasonable agreement with leading order Standard Model predictions from MadGraph5+Pythia8 and Sherpa. The results are used to constrain new physics effects in the context of an effective field theory.","sentences":["The observation of the electroweak production of a $W$ boson and a photon in association with two jets, using $pp$ collision data at the Large Hadron Collider at a centre of mass energy of $\\sqrt{s}=13$~TeV, is reported.","The data were recorded by the ATLAS experiment from 2015 to 2018 and correspond to an integrated luminosity of 140 fb$^{-1}$. This process is sensitive to the quartic gauge boson couplings via the vector boson scattering mechanism and provides a stringent test of the electroweak gauge symmetry breaking of the Standard Model.","Events are selected if they contain one electron or muon, missing transverse momentum, at least one photon, and two jets.","Multivariate techniques are used to distinguish the electroweak $W\\gamma jj$ process from irreducible background processes.","The observed significance of the electroweak $W\\gamma jj$ process is well above six standard deviations, compared to an expected significance of 6.3 standard deviations.","Fiducial and differential cross sections are measured in a fiducial phase space close to the detector acceptance, which are in reasonable agreement with leading order Standard Model predictions from MadGraph5+Pythia8 and Sherpa.","The results are used to constrain new physics effects in the context of an effective field theory."],"url":"http://arxiv.org/abs/2403.02809v1","category":"hep-ex"}
{"created":"2024-03-05 09:15:26","title":"Nonlinear Thouless Pumping of Solitons Across an Impurity","abstract":"The nonlinear Thouless pumping is an exciting frontier of topological physics. While recent works have revealed the quantized motion of solitons in Thouless pumps, the interplay between the topology, nonlinearity and disorder remains largely unexplored. Here, we investigate the nonlinear Thouless pumping of solitons in the presence of an impurity in the context of a Bose-Einstein condensate. Using both the Gross-Pitaevskii equation and Lagrangian variational approach, we analyze the interaction between a moving soliton and an impurity. Without the pump, the soliton can pass through a light impurity, but gets trapped by the impurity with large mass. In marked contrast, we find the soliton in Thouless pumps can always transit through the impurity, and its motion is topologically quantized. Our result explicitly showcases the robustness of topological soliton pumping against microscopic imperfections, and opens a new perspective in the information processing with solitons.","sentences":["The nonlinear Thouless pumping is an exciting frontier of topological physics.","While recent works have revealed the quantized motion of solitons in Thouless pumps, the interplay between the topology, nonlinearity and disorder remains largely unexplored.","Here, we investigate the nonlinear Thouless pumping of solitons in the presence of an impurity in the context of a Bose-Einstein condensate.","Using both the Gross-Pitaevskii equation and Lagrangian variational approach, we analyze the interaction between a moving soliton and an impurity.","Without the pump, the soliton can pass through a light impurity, but gets trapped by the impurity with large mass.","In marked contrast, we find the soliton in Thouless pumps can always transit through the impurity, and its motion is topologically quantized.","Our result explicitly showcases the robustness of topological soliton pumping against microscopic imperfections, and opens a new perspective in the information processing with solitons."],"url":"http://arxiv.org/abs/2403.02800v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-05 09:10:27","title":"Extended Donnan model for ion partitioning in charged nanopores","abstract":"Membranes consist of pores and the walls of these pores are often charged. In contact with an aqueous solution, the pores fill with water and ions migrate from solution into the pores until chemical equilibrium is reached. The distribution of ions between outside and pore solution is governed by a balance of chemical potential, and the resulting model is called a Donnan theory, or Donnan equation. Including a partitioning coefficient that does not depend on salt concentration results in an extended Donnan equation `of the first kind'. Recently, an electrostatic model was proposed for ions in a pore based on the arrangement of ions around strands of polymer charge, including also ion activity coefficients in solution. That framework leads to an extended Donnan equation `of the second kind', which has extra factors depending on ion concentrations in the pores and salt concentration in solution. In the present work, we set up another Donnan model of the second kind by evaluating the Coulombic interactions of ions in a cylindrical pore, including the interaction of ions with the charged pore walls and between the ions. We assume that counterions are near the pore wall while coions distribute over the center region. Starting from a complete analysis, we arrive at an elegant expression for the chemical potential of ions in such a pore. This expression depends on coion concentration, pore size, and other geometrical factors, but there is no additional dependence on counterion concentration and charge density. This model predicts the Coulombic contribution to the chemical potential in the pore to be small, much smaller than predicted by the electrostatic model from literature. Instead, we predict that up to around 1 M salt concentration, activity effects of ions in solution are more important.","sentences":["Membranes consist of pores and the walls of these pores are often charged.","In contact with an aqueous solution, the pores fill with water and ions migrate from solution into the pores until chemical equilibrium is reached.","The distribution of ions between outside and pore solution is governed by a balance of chemical potential, and the resulting model is called a Donnan theory, or Donnan equation.","Including a partitioning coefficient that does not depend on salt concentration results in an extended Donnan equation `of the first kind'.","Recently, an electrostatic model was proposed for ions in a pore based on the arrangement of ions around strands of polymer charge, including also ion activity coefficients in solution.","That framework leads to an extended Donnan equation `of the second kind', which has extra factors depending on ion concentrations in the pores and salt concentration in solution.","In the present work, we set up another Donnan model of the second kind by evaluating the Coulombic interactions of ions in a cylindrical pore, including the interaction of ions with the charged pore walls and between the ions.","We assume that counterions are near the pore wall while coions distribute over the center region.","Starting from a complete analysis, we arrive at an elegant expression for the chemical potential of ions in such a pore.","This expression depends on coion concentration, pore size, and other geometrical factors, but there is no additional dependence on counterion concentration and charge density.","This model predicts the Coulombic contribution to the chemical potential in the pore to be small, much smaller than predicted by the electrostatic model from literature.","Instead, we predict that up to around 1 M salt concentration, activity effects of ions in solution are more important."],"url":"http://arxiv.org/abs/2403.02796v1","category":"physics.chem-ph"}
{"created":"2024-03-05 08:34:46","title":"Does light slowdown in dielectric media?","abstract":"Observations and theoretical principles indicate that electromagnetic waves, including light, propagate in dielectric media at speeds lower than those in free space, as eloquently expressed in Maxwell's equations featuring material-dependent permittivity and permeability. This study reveals that the observed slower propagation of waves in dielectric media arises from an interference between two types of waves: a forward-moving primary wave and a set of secondary waves induced by the primary wave's interaction with the medium. The forward-moving primary wave and secondary waves travel at the speed of light in vacuum. Notably, the reflected wave caused by the impedance discontinuity on the boundary of a dielectric medium is manifested by the induced secondary waves moving in the opposite direction to the primary wave. From a photonic point of view, photons will be moving back and forth below the slowly moving apparent front of the observed wave with the speed of light in vacuum. Ahead of this front the probability of finding photons is zero due to complete destructive interference of the waves in that region. From a photonic perspective, beneath the gradually advancing facade of the observed wave, photons move at the unyielding speed of light in vacuum. The likelihood of detecting photons ahead of the front diminishes to zero, a consequence of the thorough destructive interference manifesting within that specific region.","sentences":["Observations and theoretical principles indicate that electromagnetic waves, including light, propagate in dielectric media at speeds lower than those in free space, as eloquently expressed in Maxwell's equations featuring material-dependent permittivity and permeability.","This study reveals that the observed slower propagation of waves in dielectric media arises from an interference between two types of waves: a forward-moving primary wave and a set of secondary waves induced by the primary wave's interaction with the medium.","The forward-moving primary wave and secondary waves travel at the speed of light in vacuum.","Notably, the reflected wave caused by the impedance discontinuity on the boundary of a dielectric medium is manifested by the induced secondary waves moving in the opposite direction to the primary wave.","From a photonic point of view, photons will be moving back and forth below the slowly moving apparent front of the observed wave with the speed of light in vacuum.","Ahead of this front the probability of finding photons is zero due to complete destructive interference of the waves in that region.","From a photonic perspective, beneath the gradually advancing facade of the observed wave, photons move at the unyielding speed of light in vacuum.","The likelihood of detecting photons ahead of the front diminishes to zero, a consequence of the thorough destructive interference manifesting within that specific region."],"url":"http://arxiv.org/abs/2403.02766v1","category":"physics.optics"}
{"created":"2024-03-05 08:34:04","title":"G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes","abstract":"G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers. They are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics. After the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active G4s regions \\textit{in vitro} based on the canonical G4 sequence elements, G-\\textit{richness}, and G-\\textit{skewness}, as well as the non-canonical sequence features. Recently, sequencing techniques like G4-seq and G4-ChIP-seq were developed to map the G4s \\textit{in vitro}, and \\textit{in vivo} respectively at a few hundred base resolution. Subsequently, several machine learning approaches were developed for predicting the G4 regions using the existing databases. However, their prediction models were simplistic, and the prediction accuracy was notably poor. In response, here, we propose a novel convolutional neural network with Bi-LSTM and attention layers, named G4-attention, to predict the G4 forming sequences with improved accuracy. G4-attention achieves high accuracy and attains state-of-the-art results in the G4 prediction task. Our model also predicts the G4 regions accurately in the highly class-imbalanced datasets. In addition, the developed model trained on the human genome dataset can be applied to any non-human genome DNA sequences to predict the G4 formation propensities.","sentences":["G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers.","They are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics.","After the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active G4s regions \\textit{in vitro} based on the canonical G4 sequence elements, G-\\textit{richness}, and G-\\textit{skewness}, as well as the non-canonical sequence features.","Recently, sequencing techniques like G4-seq and G4-ChIP-seq were developed to map the G4s \\textit{in vitro}, and \\textit{in vivo} respectively at a few hundred base resolution.","Subsequently, several machine learning approaches were developed for predicting the G4 regions using the existing databases.","However, their prediction models were simplistic, and the prediction accuracy was notably poor.","In response, here, we propose a novel convolutional neural network with Bi-LSTM and attention layers, named G4-attention, to predict the G4 forming sequences with improved accuracy.","G4-attention achieves high accuracy and attains state-of-the-art results in the G4 prediction task.","Our model also predicts the G4 regions accurately in the highly class-imbalanced datasets.","In addition, the developed model trained on the human genome dataset can be applied to any non-human genome DNA sequences to predict the G4 formation propensities."],"url":"http://arxiv.org/abs/2403.02765v1","category":"cs.LG"}
{"created":"2024-03-05 08:10:11","title":"Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps","abstract":"We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision. We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map. We then optimize a B-spline trajectory through this corridor. We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud. The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization. We also incorporate semantics into the GSplat in order to obtain better images for localization. All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction. We demonstrate the safety and robustness of our pipeline in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation.","sentences":["We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision.","We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map.","We then optimize a B-spline trajectory through this corridor.","We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud.","The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization.","We also incorporate semantics into the GSplat in order to obtain better images for localization.","All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction.","We demonstrate the safety and robustness of our pipeline in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation."],"url":"http://arxiv.org/abs/2403.02751v1","category":"cs.RO"}
{"created":"2024-03-05 07:51:38","title":"State-Constrained Zero-Sum Differential Games with One-Sided Information","abstract":"We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies. Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on revealing the mechanism of belief manipulation behaviors resulted from information asymmetry and state constraints. We use a simplified football game to demonstrate the utility of this work, where we reveal player positions and belief states in which the attacker should (or should not) play specific random fake moves to take advantage of information asymmetry, and compute how the defender should respond.","sentences":["We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2).","The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff.","One example of the game is a man-to-man matchup in football.","Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players.","Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies.","Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on revealing the mechanism of belief manipulation behaviors resulted from information asymmetry and state constraints.","We use a simplified football game to demonstrate the utility of this work, where we reveal player positions and belief states in which the attacker should (or should not) play specific random fake moves to take advantage of information asymmetry, and compute how the defender should respond."],"url":"http://arxiv.org/abs/2403.02741v1","category":"cs.GT"}
{"created":"2024-03-05 07:36:36","title":"Relativistic energy density functional from momentum space to coordinate space within a coherent density fluctuation model","abstract":"In this theoretical study, we have derived a simplified analytical expression for the binding energy per nucleon as a function of density and isospin asymmetry within the relativistic mean-field model. We have generated a new parameterization for the density-dependent DD-ME2 parameter set using the Relativistic-Hartree-Bogoliubov approach. Moreover, this work attempts to revisit the prior polynomial fitting in [Phy. Rev. C 103, 024305 (2021)] for the non-linear NL3 force parameter to provide a simplified set of equations for the energy density functional which is used for calculating the surface properties of finite nuclei. The current study improves the existing fitting procedure by effectively proposing a simpler model that provides comparably precise results while lowering the computational expense. To study the surface properties of finite nuclei with these parameterizations, we have adopted the coherent density fluctuation model, which effectively translates the quantities of nuclear matter from momentum space to coordinate space at local density. The isospin properties, such as symmetry energy and its surface and volume components, slope parameter, finite nuclear incompressibility, and surface incompressibility for even-even nuclei, are calculated for different mass regions. Moreover, we have studied the effect of density, weight function, and choice of relativistic force parameters on the surface properties. The consequence of this work will help to determine the properties of nuclei along the nuclear landscape and can facilitate an improved understanding of the island of stability, heavy-ion collision, and nucleosynthesis, among others.","sentences":["In this theoretical study, we have derived a simplified analytical expression for the binding energy per nucleon as a function of density and isospin asymmetry within the relativistic mean-field model.","We have generated a new parameterization for the density-dependent DD-ME2 parameter set using the Relativistic-Hartree-Bogoliubov approach.","Moreover, this work attempts to revisit the prior polynomial fitting in [Phy. Rev. C 103, 024305 (2021)] for the non-linear NL3 force parameter to provide a simplified set of equations for the energy density functional which is used for calculating the surface properties of finite nuclei.","The current study improves the existing fitting procedure by effectively proposing a simpler model that provides comparably precise results while lowering the computational expense.","To study the surface properties of finite nuclei with these parameterizations, we have adopted the coherent density fluctuation model, which effectively translates the quantities of nuclear matter from momentum space to coordinate space at local density.","The isospin properties, such as symmetry energy and its surface and volume components, slope parameter, finite nuclear incompressibility, and surface incompressibility for even-even nuclei, are calculated for different mass regions.","Moreover, we have studied the effect of density, weight function, and choice of relativistic force parameters on the surface properties.","The consequence of this work will help to determine the properties of nuclei along the nuclear landscape and can facilitate an improved understanding of the island of stability, heavy-ion collision, and nucleosynthesis, among others."],"url":"http://arxiv.org/abs/2403.02729v1","category":"nucl-th"}
{"created":"2024-03-05 06:25:19","title":"Noise misleads rotation invariant algorithms on sparse targets","abstract":"It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the \"dimension\" of the problem. This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution). The simplest sparse problem is learning a single feature out of $d$ features. In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen. These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$.   We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples. We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much lower upper bounds on the same problem for simple non-rotation invariant algorithms. Finally we analyze the gradient flow trajectories of many standard optimization algorithms in some simple cases and show how they veer toward or away from the sparse targets.   We believe that our trajectory categorization will be useful in designing algorithms that can exploit sparse targets and our method for proving lower bounds will be crucial for analyzing other families of algorithms that admit different classes of invariances.","sentences":["It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the \"dimension\" of the problem.","This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution).","The simplest sparse problem is learning a single feature out of $d$ features.","In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen.","These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$.   We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples.","We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem.","We then prove much lower upper bounds on the same problem for simple non-rotation invariant algorithms.","Finally we analyze the gradient flow trajectories of many standard optimization algorithms in some simple cases and show how they veer toward or away from the sparse targets.   ","We believe that our trajectory categorization will be useful in designing algorithms that can exploit sparse targets and our method for proving lower bounds will be crucial for analyzing other families of algorithms that admit different classes of invariances."],"url":"http://arxiv.org/abs/2403.02697v1","category":"stat.ML"}
{"created":"2024-03-05 06:10:21","title":"SGD with Partial Hessian for Deep Neural Networks Optimization","abstract":"Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The proposed method, namely SGD with Partial Hessian (SGD-PH), inherits the advantages of both first-order and second-order optimizers. Compared with first-order optimizers, it adopts a certain amount of information from the Hessian matrix to assist optimization, while compared with the existing second-order optimizers, it keeps the good generalization performance of first-order optimizers. Experiments on image classification tasks demonstrate the effectiveness of our proposed optimizer SGD-PH. The code is publicly available at \\url{https://github.com/myingysun/SGDPH}.","sentences":["Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years.","However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization.","Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance.","In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters.","We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods.","The proposed method, namely SGD with Partial Hessian (SGD-PH), inherits the advantages of both first-order and second-order optimizers.","Compared with first-order optimizers, it adopts a certain amount of information from the Hessian matrix to assist optimization, while compared with the existing second-order optimizers, it keeps the good generalization performance of first-order optimizers.","Experiments on image classification tasks demonstrate the effectiveness of our proposed optimizer SGD-PH.","The code is publicly available at \\url{https://github.com/myingysun/SGDPH}."],"url":"http://arxiv.org/abs/2403.02681v1","category":"cs.LG"}
{"created":"2024-03-05 05:14:34","title":"Solutions to q-hypergeometric equations associated with q-middle convolution","abstract":"We investigate the integral representations of solutions to q-hypergeometric equation and its variant obtained through q-middle convolution by using transformation formulas for q-hypergeometric series. We show the correspondence between these integral solutions and solutions obtained through other methods. We also show the linear relationships among the integral solutions. Specifically, we provide an alternative proof for the connection formula for the standard form of q-hypergeometric equation.","sentences":["We investigate the integral representations of solutions to q-hypergeometric equation and its variant obtained through q-middle convolution by using transformation formulas for q-hypergeometric series.","We show the correspondence between these integral solutions and solutions obtained through other methods.","We also show the linear relationships among the integral solutions.","Specifically, we provide an alternative proof for the connection formula for the standard form of q-hypergeometric equation."],"url":"http://arxiv.org/abs/2403.02662v1","category":"math.CA"}
{"created":"2024-03-05 05:03:45","title":"Modified scattering operator for nonlinear Schr\u00f6dinger equations with time-decaying harmonic potentials","abstract":"This paper is concerned with nonlinear Schr\\\"odinger equations with a time-decaying harmonic potential. The nonlinearity is gauge-invariant of the long-range critical order. In [24] and [22], it is proved that the equation admits a nontrivial solution that behaves like a free solution with a logarithmic phase correction in the frameworks of both the final state problem and the initial value problem. Furthermore, a modified scattering operator has been established in the case without the potential in [15]. In this paper, we construct a modified scattering operator for our equation by utilizing a generator of the Galilean transformation. Moreover, we remove a restriction for the coefficient of the potential which is required in [22].","sentences":["This paper is concerned with nonlinear Schr\\\"odinger equations with a time-decaying harmonic potential.","The nonlinearity is gauge-invariant of the long-range critical order.","In [24] and [22], it is proved that the equation admits a nontrivial solution that behaves like a free solution with a logarithmic phase correction in the frameworks of both the final state problem and the initial value problem.","Furthermore, a modified scattering operator has been established in the case without the potential in [15].","In this paper, we construct a modified scattering operator for our equation by utilizing a generator of the Galilean transformation.","Moreover, we remove a restriction for the coefficient of the potential which is required in [22]."],"url":"http://arxiv.org/abs/2403.02657v1","category":"math.AP"}
{"created":"2024-03-05 04:50:28","title":"Sengupta Transformations and Carrollian Relativistic Theory","abstract":"A detailed and systematic formulation of Carrollian relativity is provided. Based on the transformations, first provided by Sengupta [18], we construct a mapping between Lorentz relativistic and Carrollian relativistic vectors. Using this map the Carroll theory is built from the standard Maxwell action. We show that we get self-consistent equations of motion from the action, both in electric and magnetic limits. We introduce Carroll electric and magnetic fields. A new set of maps is derived that connects Carroll electric and magnetic fields with the usual Maxwell ones and yields Carroll equations in terms of fields. Consistency of results with the potential formulation is shown. Carroll version of symmetries like duality, gauge, shift, Noether and boost are treated in details and their implications elaborated. Especially, boost symmetry provides a link to the various maps used in this paper.","sentences":["A detailed and systematic formulation of Carrollian relativity is provided.","Based on the transformations, first provided by Sengupta [18], we construct a mapping between Lorentz relativistic and Carrollian relativistic vectors.","Using this map the Carroll theory is built from the standard Maxwell action.","We show that we get self-consistent equations of motion from the action, both in electric and magnetic limits.","We introduce Carroll electric and magnetic fields.","A new set of maps is derived that connects Carroll electric and magnetic fields with the usual Maxwell ones and yields Carroll equations in terms of fields.","Consistency of results with the potential formulation is shown.","Carroll version of symmetries like duality, gauge, shift, Noether and boost are treated in details and their implications elaborated.","Especially, boost symmetry provides a link to the various maps used in this paper."],"url":"http://arxiv.org/abs/2403.02653v1","category":"hep-th"}
{"created":"2024-03-05 04:29:31","title":"Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain","abstract":"With the evolution of 5G wireless communications, the Synchronization Signal Block (SSB) plays a critical role in the synchronization of devices and accessibility of services. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. By leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in 5G networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block that extracts PSS correlation and energy per null resource elements (EPNRE) characteristics, our method distinguishes between normal and jammed received signals with high precision. Additionally, by incorporation of Discrete Wavelet Transform (DWT), the efficacy of training and detection are optimized. A double threshold double Deep Neural Network (DT-DDNN) is also introduced to the architecture complemented by a deep cascade learning model to increase the sensitivity of the model to variations of signal to jamming noise ratio (SJNR). Results show that the proposed method achieves 96.4% detection rate in extra low jamming power, i.e., SJNR between 15 to 30 dB which outperforms the single threshold DNN design with 86.0% detection rate and unprocessed IQ sample DNN design with 83.2% detection rate. Ultimately, performance of DT-DDNN is validated through the analysis of real 5G signals obtained from a practical testbed, demonstrating a strong alignment with the simulation results.","sentences":["With the evolution of 5G wireless communications, the Synchronization Signal Block (SSB) plays a critical role in the synchronization of devices and accessibility of services.","However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats.","By leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in 5G networks.","Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double threshold deep learning jamming detector by focusing on the SSB.","The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure.","By integrating a preprocessing block that extracts PSS correlation and energy per null resource elements (EPNRE) characteristics, our method distinguishes between normal and jammed received signals with high precision.","Additionally, by incorporation of Discrete Wavelet Transform (DWT), the efficacy of training and detection are optimized.","A double threshold double Deep Neural Network (DT-DDNN) is also introduced to the architecture complemented by a deep cascade learning model to increase the sensitivity of the model to variations of signal to jamming noise ratio (SJNR).","Results show that the proposed method achieves 96.4% detection rate in extra low jamming power, i.e., SJNR between 15 to 30 dB which outperforms the single threshold DNN design with 86.0% detection rate and unprocessed IQ sample DNN design with 83.2% detection rate.","Ultimately, performance of DT-DDNN is validated through the analysis of real 5G signals obtained from a practical testbed, demonstrating a strong alignment with the simulation results."],"url":"http://arxiv.org/abs/2403.02645v1","category":"eess.SP"}
{"created":"2024-03-05 04:26:36","title":"Exact Nonclassical Symmetry Solutions of Lotka-Volterra Type Population Systems","abstract":"New classes of conditionally integrable systems of nonlinear reaction-diffusion equations are introduced. They are obtained by extending a well known nonclassical symmetry of a scalar partial differential equation to a vector equation. New exact solutions of nonlinear predator-prey systems, related to the diffusive Lotka-Volterra system, are constructed. An infinite dimensional class of exact solutions is made available. Unlike in the standard Lotka-Volterra system, in the absence of predators, the prey population has a finite carrying capacity, as in the Fisher equation.","sentences":["New classes of conditionally integrable systems of nonlinear reaction-diffusion equations are introduced.","They are obtained by extending a well known nonclassical symmetry of a scalar partial differential equation to a vector equation.","New exact solutions of nonlinear predator-prey systems, related to the diffusive Lotka-Volterra system, are constructed.","An infinite dimensional class of exact solutions is made available.","Unlike in the standard Lotka-Volterra system, in the absence of predators, the prey population has a finite carrying capacity, as in the Fisher equation."],"url":"http://arxiv.org/abs/2403.02644v1","category":"nlin.SI"}
{"created":"2024-03-05 03:37:28","title":"Interactive Continual Learning: Fast and Slow Thinking","abstract":"Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization. Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods.","sentences":["Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan.","In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL).","Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models.","Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes.","Specifically, we assign the ViT model as System1 and multimodal LLM as System2.","To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA).","Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution.","Meanwhile, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization.","Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods."],"url":"http://arxiv.org/abs/2403.02628v1","category":"cs.CV"}
{"created":"2024-03-05 02:18:11","title":"Learning Stochastic Dynamics from Data","abstract":"We present a noise guided trajectory based system identification method for inferring the dynamical structure from observation generated by stochastic differential equations. Our method can handle various kinds of noise, including the case when the the components of the noise is correlated. Our method can also learn both the noise level and drift term together from trajectory. We present various numerical tests for showcasing the superior performance of our learning algorithm.","sentences":["We present a noise guided trajectory based system identification method for inferring the dynamical structure from observation generated by stochastic differential equations.","Our method can handle various kinds of noise, including the case when the the components of the noise is correlated.","Our method can also learn both the noise level and drift term together from trajectory.","We present various numerical tests for showcasing the superior performance of our learning algorithm."],"url":"http://arxiv.org/abs/2403.02595v1","category":"math.NA"}
{"created":"2024-03-05 02:08:53","title":"On the $p$-adic values of the weight two Eisenstein series for supersingular primes","abstract":"The weight two Eisenstein series may be considered as the first example of a Katz $p$-adic modular form. Classically, its values are defined for the primes of ordinary reduction. We offer a modified definition which applies uniformly to all primes of good reduction, both ordinary and supersingular. We show that, in the case of complex multiplication, these $p$-adic values coincide with the algebraic value of this Eisenstein series.","sentences":["The weight two Eisenstein series may be considered as the first example of a Katz $p$-adic modular form.","Classically, its values are defined for the primes of ordinary reduction.","We offer a modified definition which applies uniformly to all primes of good reduction, both ordinary and supersingular.","We show that, in the case of complex multiplication, these $p$-adic values coincide with the algebraic value of this Eisenstein series."],"url":"http://arxiv.org/abs/2403.02592v1","category":"math.NT"}
{"created":"2024-03-05 01:55:41","title":"Periodic Cylindrical Bilayers Self-Assembled from Diblock Polymers","abstract":"Amphiphilic polymers in aqueous solutions can self-assemble to form bilayer membranes, and their elastic properties can be captured by the well-known Helfrich model involving several elastic constants. In this paper, we employ the self-consistent field model to simulate sinusoidal bilayers self-assembled from diblock copolymers where a proper constraint term is introduced to stabilize periodic bilayers with prescribed amplitudes. Then, we devise several methods to extract the shape of these bilayers and examine the accuracy of the free energy predicted by the Helfrich model. Numerical results show that when the bilayer curvature is small, the Helfrich model predicts the excess free energy more accurately. However, when the curvature is large, the accuracy heavily depends on the method used to determine the shape of the bilayer. In addition, the dependence of free energy on interaction strength, constraint amplitude, and constraint period are systematically studied. Moreover, we obtain certain periodic cylindrical bilayers that are equilibrium states of the self-consistent field model, which agree with the theoretical predictions made by the shape equations.","sentences":["Amphiphilic polymers in aqueous solutions can self-assemble to form bilayer membranes, and their elastic properties can be captured by the well-known Helfrich model involving several elastic constants.","In this paper, we employ the self-consistent field model to simulate sinusoidal bilayers self-assembled from diblock copolymers where a proper constraint term is introduced to stabilize periodic bilayers with prescribed amplitudes.","Then, we devise several methods to extract the shape of these bilayers and examine the accuracy of the free energy predicted by the Helfrich model.","Numerical results show that when the bilayer curvature is small, the Helfrich model predicts the excess free energy more accurately.","However, when the curvature is large, the accuracy heavily depends on the method used to determine the shape of the bilayer.","In addition, the dependence of free energy on interaction strength, constraint amplitude, and constraint period are systematically studied.","Moreover, we obtain certain periodic cylindrical bilayers that are equilibrium states of the self-consistent field model, which agree with the theoretical predictions made by the shape equations."],"url":"http://arxiv.org/abs/2403.02588v1","category":"cond-mat.soft"}
{"created":"2024-03-05 01:30:34","title":"Geometric Dynamics of Signal Propagation Predict Trainability of Transformers","abstract":"We investigate forward signal propagation and gradient back propagation in deep, randomly initialized transformers, yielding simple necessary and sufficient conditions on initialization hyperparameters that ensure trainability of deep transformers. Our approach treats the evolution of the representations of $n$ tokens as they propagate through the transformer layers in terms of a discrete time dynamical system of $n$ interacting particles. We derive simple update equations for the evolving geometry of this particle system, starting from a permutation symmetric simplex. Our update equations show that without MLP layers, this system will collapse to a line, consistent with prior work on rank collapse in transformers. However, unlike prior work, our evolution equations can quantitatively track particle geometry in the additional presence of nonlinear MLP layers, and it reveals an order-chaos phase transition as a function of initialization hyperparameters, like the strength of attentional and MLP residual connections and weight variances. In the ordered phase the particles are attractive and collapse to a line, while in the chaotic phase the particles are repulsive and converge to a regular $n$-simplex. We analytically derive two Lyapunov exponents: an angle exponent that governs departures from the edge of chaos in this particle system, and a gradient exponent that governs the rate of exponential growth or decay of backpropagated gradients. We show through experiments that, remarkably, the final test loss at the end of training is well predicted just by these two exponents at the beginning of training, and that the simultaneous vanishing of these two exponents yields a simple necessary and sufficient condition to achieve minimal test loss.","sentences":["We investigate forward signal propagation and gradient back propagation in deep, randomly initialized transformers, yielding simple necessary and sufficient conditions on initialization hyperparameters that ensure trainability of deep transformers.","Our approach treats the evolution of the representations of $n$ tokens as they propagate through the transformer layers in terms of a discrete time dynamical system of $n$ interacting particles.","We derive simple update equations for the evolving geometry of this particle system, starting from a permutation symmetric simplex.","Our update equations show that without MLP layers, this system will collapse to a line, consistent with prior work on rank collapse in transformers.","However, unlike prior work, our evolution equations can quantitatively track particle geometry in the additional presence of nonlinear MLP layers, and it reveals an order-chaos phase transition as a function of initialization hyperparameters, like the strength of attentional and MLP residual connections and weight variances.","In the ordered phase the particles are attractive and collapse to a line, while in the chaotic phase the particles are repulsive and converge to a regular $n$-simplex.","We analytically derive two Lyapunov exponents: an angle exponent that governs departures from the edge of chaos in this particle system, and a gradient exponent that governs the rate of exponential growth or decay of backpropagated gradients.","We show through experiments that, remarkably, the final test loss at the end of training is well predicted just by these two exponents at the beginning of training, and that the simultaneous vanishing of these two exponents yields a simple necessary and sufficient condition to achieve minimal test loss."],"url":"http://arxiv.org/abs/2403.02579v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-05 00:58:34","title":"DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training","abstract":"Recent developments have underscored the critical role of \\textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML algorithms by enhancing parameter robustness. The fundamental intuition behind this strategy is that models with robust parameters are inherently more resistant to the noise introduced by DP, thereby retaining better performance despite the perturbations. DPAdapter modifies and enhances the sharpness-aware minimization (SAM) technique, utilizing a two-batch strategy to provide a more accurate perturbation estimate and an efficient gradient descent, thereby improving parameter robustness against noise. Notably, DPAdapter can act as a plug-and-play component and be combined with existing DPML algorithms to further improve their performance. Our experiments show that DPAdapter vastly enhances state-of-the-art DPML algorithms, increasing average accuracy from 72.92\\% to 77.09\\% with a privacy budget of $\\epsilon=4$.","sentences":["Recent developments have underscored the critical role of \\textit{differential privacy} (DP) in safeguarding individual data for training machine learning models.","However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field.","To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts.","In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   ","In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML algorithms by enhancing parameter robustness.","The fundamental intuition behind this strategy is that models with robust parameters are inherently more resistant to the noise introduced by DP, thereby retaining better performance despite the perturbations.","DPAdapter modifies and enhances the sharpness-aware minimization (SAM) technique, utilizing a two-batch strategy to provide a more accurate perturbation estimate and an efficient gradient descent, thereby improving parameter robustness against noise.","Notably, DPAdapter can act as a plug-and-play component and be combined with existing DPML algorithms to further improve their performance.","Our experiments show that DPAdapter vastly enhances state-of-the-art DPML algorithms, increasing average accuracy from 72.92\\% to 77.09\\% with a privacy budget of $\\epsilon=4$."],"url":"http://arxiv.org/abs/2403.02571v1","category":"cs.LG"}
{"created":"2024-03-05 00:39:34","title":"Long-time Ricci flow existence and topological rigidity from manifolds with pinched scale-invariant integral curvature","abstract":"We prove long-time existence of the Ricci flow starting from complete manifolds with bounded curvature and scale-invariant integral curvature sufficiently pinched with respect to the inverse of its Sobolev constant. Moreover, if the curvature is sub-critical $L^p$ integrable, this flow converges locally smoothly to a limiting metric $g(\\infty)$ on $M$ with $(M,g(\\infty))$ isometric to the standard flat $\\mathbb{R}^n$, which implies topological rigidity of $M$. This generalizes work of Chen, who proved analogous results for asymptotically flat manifolds. We also prove a long-time Ricci flow existence (and likewise topological rigidity) result for unbounded curvature initial data, assuming the initial data is a locally smooth limit of bounded curvature manifolds as described above.","sentences":["We prove long-time existence of the Ricci flow starting from complete manifolds with bounded curvature and scale-invariant integral curvature sufficiently pinched with respect to the inverse of its Sobolev constant.","Moreover, if the curvature is sub-critical $L^p$ integrable, this flow converges locally smoothly to a limiting metric $g(\\infty)$ on $M$ with $(M,g(\\infty))$ isometric to the standard flat $\\mathbb{R}^n$, which implies topological rigidity of $M$. This generalizes work of Chen, who proved analogous results for asymptotically flat manifolds.","We also prove a long-time Ricci flow existence (and likewise topological rigidity) result for unbounded curvature initial data, assuming the initial data is a locally smooth limit of bounded curvature manifolds as described above."],"url":"http://arxiv.org/abs/2403.02564v1","category":"math.DG"}
{"created":"2024-03-04 23:16:28","title":"Thermal effects on sound velocity peak and conformality in isospin QCD","abstract":"We study thermal effects on equations of state (EOS) in isospin QCD, utilizing a quark-meson model coupled to a Polyakov loop. The quark-meson model is analyzed at one-loop that is the minimal order to include quark substructure constraints on pions which condense at finite isospin density. In the previous study we showed that the quark-meson model at zero temperature produces the sound velocity peak and the negative trace anomaly in the domain between the chiral effective theory regime at low density and the perturbative QCD regime at high density, in reasonable agreement with lattice simulations. We now include thermal effects from quarks in the Polyakov loop background and examine EOS, especially the sound velocity and trace anomaly along isentropic trajectories. At large isospin density, there are three temperature windows; (i) the pion condensed region with almost vanishing Polyakov loops, (ii) the pion condensed region with finite Polyakov loops, and (iii) the quark gas without pion condensates. In the domain (i), the gap associated with the pion condensate strongly quenches thermal excitations. As the system approaches the domain (ii), thermal quarks, which behave as non-relativistic particles, add energy density but little pressure, substantially reducing the sound velocity to the value less than the conformal value while increasing the trace anomaly toward the positive value. Approaching the domain (iii), thermal quarks become more relativistic as pion condensates melt, increasing sound velocity toward the conformal limit. Corrections from thermal pions are also briefly discussed.","sentences":["We study thermal effects on equations of state (EOS) in isospin QCD, utilizing a quark-meson model coupled to a Polyakov loop.","The quark-meson model is analyzed at one-loop that is the minimal order to include quark substructure constraints on pions which condense at finite isospin density.","In the previous study we showed that the quark-meson model at zero temperature produces the sound velocity peak and the negative trace anomaly in the domain between the chiral effective theory regime at low density and the perturbative QCD regime at high density, in reasonable agreement with lattice simulations.","We now include thermal effects from quarks in the Polyakov loop background and examine EOS, especially the sound velocity and trace anomaly along isentropic trajectories.","At large isospin density, there are three temperature windows; (i) the pion condensed region with almost vanishing Polyakov loops, (ii) the pion condensed region with finite Polyakov loops, and (iii) the quark gas without pion condensates.","In the domain (i), the gap associated with the pion condensate strongly quenches thermal excitations.","As the system approaches the domain (ii), thermal quarks, which behave as non-relativistic particles, add energy density but little pressure, substantially reducing the sound velocity to the value less than the conformal value while increasing the trace anomaly toward the positive value.","Approaching the domain (iii), thermal quarks become more relativistic as pion condensates melt, increasing sound velocity toward the conformal limit.","Corrections from thermal pions are also briefly discussed."],"url":"http://arxiv.org/abs/2403.02538v1","category":"hep-ph"}
{"created":"2024-03-04 23:12:42","title":"Demolition and Reinforcement of Memories in Spin-Glass-like Neural Networks","abstract":"Statistical mechanics has made significant contributions to the study of biological neural systems by modeling them as recurrent networks of interconnected units with adjustable interactions. Several algorithms have been proposed to optimize the neural connections to enable network tasks such as information storage (i.e. associative memory) and learning probability distributions from data (i.e. generative modeling). Among these methods, the Unlearning algorithm, aligned with emerging theories of synaptic plasticity, was introduced by John Hopfield and collaborators. The primary objective of this thesis is to understand the effectiveness of Unlearning in both associative memory models and generative models. Initially, we demonstrate that the Unlearning algorithm can be simplified to a linear perceptron model which learns from noisy examples featuring specific internal correlations. The selection of structured training data enables an associative memory model to retrieve concepts as attractors of a neural dynamics with considerable basins of attraction. Subsequently, a novel regularization technique for Boltzmann Machines is presented, proving to outperform previously developed methods in learning hidden probability distributions from data-sets. The Unlearning rule is derived from this new regularized algorithm and is showed to be comparable, in terms of inferential performance, to traditional Boltzmann-Machine learning.","sentences":["Statistical mechanics has made significant contributions to the study of biological neural systems by modeling them as recurrent networks of interconnected units with adjustable interactions.","Several algorithms have been proposed to optimize the neural connections to enable network tasks such as information storage (i.e. associative memory) and learning probability distributions from data (i.e. generative modeling).","Among these methods, the Unlearning algorithm, aligned with emerging theories of synaptic plasticity, was introduced by John Hopfield and collaborators.","The primary objective of this thesis is to understand the effectiveness of Unlearning in both associative memory models and generative models.","Initially, we demonstrate that the Unlearning algorithm can be simplified to a linear perceptron model which learns from noisy examples featuring specific internal correlations.","The selection of structured training data enables an associative memory model to retrieve concepts as attractors of a neural dynamics with considerable basins of attraction.","Subsequently, a novel regularization technique for Boltzmann Machines is presented, proving to outperform previously developed methods in learning hidden probability distributions from data-sets.","The Unlearning rule is derived from this new regularized algorithm and is showed to be comparable, in terms of inferential performance, to traditional Boltzmann-Machine learning."],"url":"http://arxiv.org/abs/2403.02537v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-04 22:51:11","title":"Projected gradient descent accumulates at Bouligand stationary points","abstract":"This paper concerns the projected gradient descent (PGD) algorithm for the problem of minimizing a continuously differentiable function on a nonempty closed subset of a Euclidean vector space. Without further assumptions, this problem is intractable and devoted algorithms are only expected to find a stationary point. PGD is known to generate a sequence whose accumulation points are Mordukhovich stationary. In this paper, these accumulation points are proven to be Bouligand stationary, and even proximally stationary if the gradient is locally Lipschitz continuous. These are the strongest stationarity properties that can be expected for the considered problem.","sentences":["This paper concerns the projected gradient descent (PGD) algorithm for the problem of minimizing a continuously differentiable function on a nonempty closed subset of a Euclidean vector space.","Without further assumptions, this problem is intractable and devoted algorithms are only expected to find a stationary point.","PGD is known to generate a sequence whose accumulation points are Mordukhovich stationary.","In this paper, these accumulation points are proven to be Bouligand stationary, and even proximally stationary if the gradient is locally Lipschitz continuous.","These are the strongest stationarity properties that can be expected for the considered problem."],"url":"http://arxiv.org/abs/2403.02530v1","category":"math.OC"}
{"created":"2024-03-04 22:08:37","title":"MPI Errors Detection using GNN Embedding and Vector Embedding over LLVM IR","abstract":"Identifying errors in parallel MPI programs is a challenging task. Despite the growing number of verification tools, debugging parallel programs remains a significant challenge. This paper is the first to utilize embedding and deep learning graph neural networks (GNNs) to tackle the issue of identifying bugs in MPI programs. Specifically, we have designed and developed two models that can determine, from a code's LLVM Intermediate Representation (IR), whether the code is correct or contains a known MPI error. We tested our models using two dedicated MPI benchmark suites for verification: MBI and MPI-CorrBench. By training and validating our models on the same benchmark suite, we achieved a prediction accuracy of 92% in detecting error types. Additionally, we trained and evaluated our models on distinct benchmark suites (e.g., transitioning from MBI to MPI-CorrBench) and achieved a promising accuracy of over 80%. Finally, we investigated the interaction between different MPI errors and quantified our models' generalization capabilities over new unseen errors. This involved removing error types during training and assessing whether our models could still predict them. The detection accuracy of removed errors varies significantly between 20% to 80%, indicating connected error patterns.","sentences":["Identifying errors in parallel MPI programs is a challenging task.","Despite the growing number of verification tools, debugging parallel programs remains a significant challenge.","This paper is the first to utilize embedding and deep learning graph neural networks (GNNs) to tackle the issue of identifying bugs in MPI programs.","Specifically, we have designed and developed two models that can determine, from a code's LLVM Intermediate Representation (IR), whether the code is correct or contains a known MPI error.","We tested our models using two dedicated MPI benchmark suites for verification: MBI and MPI-CorrBench.","By training and validating our models on the same benchmark suite, we achieved a prediction accuracy of 92% in detecting error types.","Additionally, we trained and evaluated our models on distinct benchmark suites (e.g., transitioning from MBI to MPI-CorrBench) and achieved a promising accuracy of over 80%.","Finally, we investigated the interaction between different MPI errors and quantified our models' generalization capabilities over new unseen errors.","This involved removing error types during training and assessing whether our models could still predict them.","The detection accuracy of removed errors varies significantly between 20% to 80%, indicating connected error patterns."],"url":"http://arxiv.org/abs/2403.02518v1","category":"cs.DC"}
{"created":"2024-03-04 22:07:44","title":"Observation of Seven Astrophysical Tau Neutrino Candidates with IceCube","abstract":"We report on a measurement of astrophysical tau neutrinos with 9.7 years of IceCube data. Using convolutional neural networks trained on images derived from simulated events, seven candidate $\\nu_\\tau$ events were found with visible energies ranging from roughly 20 TeV to 1 PeV and a median expected parent $\\nu_\\tau$ energy of about 200 TeV. Considering backgrounds from astrophysical and atmospheric neutrinos, and muons from $\\pi^\\pm/K^\\pm$ decays in atmospheric air showers, we obtain a total estimated background of about 0.5 events, dominated by non-$\\nu_\\tau$ astrophysical neutrinos. Thus, we rule out the absence of astrophysical $\\nu_\\tau$ at the $5\\sigma$ level. The measured astrophysical $\\nu_\\tau$ flux is consistent with expectations based on previously published IceCube astrophysical neutrino flux measurements and neutrino oscillations.","sentences":["We report on a measurement of astrophysical tau neutrinos with 9.7 years of IceCube data.","Using convolutional neural networks trained on images derived from simulated events, seven candidate $\\nu_\\tau$ events were found with visible energies ranging from roughly 20 TeV to 1 PeV and a median expected parent $\\nu_\\tau$ energy of about 200 TeV. Considering backgrounds from astrophysical and atmospheric neutrinos, and muons from $\\pi^\\pm/K^\\pm$ decays in atmospheric air showers, we obtain a total estimated background of about 0.5 events, dominated by non-$\\nu_\\tau$ astrophysical neutrinos.","Thus, we rule out the absence of astrophysical $\\nu_\\tau$ at the $5\\sigma$ level.","The measured astrophysical $\\nu_\\tau$ flux is consistent with expectations based on previously published IceCube astrophysical neutrino flux measurements and neutrino oscillations."],"url":"http://arxiv.org/abs/2403.02516v1","category":"astro-ph.HE"}
{"created":"2024-03-04 21:53:09","title":"LQ Control of Traffic Flow Models via Variable Speed Limits","abstract":"In this paper, an extension of a linear control design for hyperbolic linear partial differential equations is presented for a first-order traffic flow model. Starting from the Lighthill-Whitham-Richards (LWR) model, variable speed limit control (VSL) is applied through a modification of Greenshield's equilibrium flow model. Then, an optimal linear quadratic (LQ) controller is designed on the linear LWR model. The LQ state feedback function is found via the solution of a Riccati differential equation. Unlike previous studies, the control input is the rate of change of the input, not the input itself. The proposed controller is then verified on both the linear and nonlinear models. In both cases, the controller is able to drive the system to a desired density profile. In the nonlinear application, a higher control gain is needed to achieve similar results as in the linear case.","sentences":["In this paper, an extension of a linear control design for hyperbolic linear partial differential equations is presented for a first-order traffic flow model.","Starting from the Lighthill-Whitham-Richards (LWR) model, variable speed limit control (VSL) is applied through a modification of Greenshield's equilibrium flow model.","Then, an optimal linear quadratic (LQ) controller is designed on the linear LWR model.","The LQ state feedback function is found via the solution of a Riccati differential equation.","Unlike previous studies, the control input is the rate of change of the input, not the input itself.","The proposed controller is then verified on both the linear and nonlinear models.","In both cases, the controller is able to drive the system to a desired density profile.","In the nonlinear application, a higher control gain is needed to achieve similar results as in the linear case."],"url":"http://arxiv.org/abs/2403.02507v1","category":"eess.SY"}
{"created":"2024-03-04 21:48:34","title":"Boundary Behavior of Compact Manifolds With Scalar Curvature Lower Bounds and Static Quasi-Local Mass of Tori","abstract":"A classic result of Shi and Tam states that a 2-sphere of positive Gauss and mean curvature bounding a compact 3-manifold with nonnegative scalar curvature, must have total mean curvature not greater than that of the isometric embedding into Euclidean 3-space, with equality only for domains in this reference manifold. We generalize this result to 2-tori of Guass curvature greater than $-1$, which bound a compact 3-manifold having scalar curvature not less than $-6$ and at least one other boundary component satisfying a 'trapping condition'. The conclusion is that the total weighted mean curvature is not greater than that of an isometric embedding into the Kottler manifold, with equality only for domains in this space. Examples are given to show that the assumption of a secondary boundary component cannot be removed. The result gives a positive mass theorem for the static Brown-York mass of tori, in analogy to the Shi-Tam positivity of the standard Brown-York mass, and represents the first such quasi-local mass positivity result for non-spherical surfaces. Furthermore, we prove a Penrose-type inequality in this setting.","sentences":["A classic result of Shi and Tam states that a 2-sphere of positive Gauss and mean curvature bounding a compact 3-manifold with nonnegative scalar curvature, must have total mean curvature not greater than that of the isometric embedding into Euclidean 3-space, with equality only for domains in this reference manifold.","We generalize this result to 2-tori of Guass curvature greater than $-1$, which bound a compact 3-manifold having scalar curvature not less than $-6$ and at least one other boundary component satisfying a 'trapping condition'.","The conclusion is that the total weighted mean curvature is not greater than that of an isometric embedding into the Kottler manifold, with equality only for domains in this space.","Examples are given to show that the assumption of a secondary boundary component cannot be removed.","The result gives a positive mass theorem for the static Brown-York mass of tori, in analogy to the Shi-Tam positivity of the standard Brown-York mass, and represents the first such quasi-local mass positivity result for non-spherical surfaces.","Furthermore, we prove a Penrose-type inequality in this setting."],"url":"http://arxiv.org/abs/2403.02501v1","category":"math.DG"}
{"created":"2024-03-04 21:48:05","title":"The complexity of computing in continuous time: space complexity is precision","abstract":"Models of computations over the integers are equivalent from a computability and complexity theory point of view by the Church-Turing thesis. It is not possible to unify discrete-time models over the reals. The situation is unclear but simpler for continuous-time models, as there is a unifying mathematical model provided by ordinary differential equations (ODEs). For example, the GPAC model of Shannon is known to correspond to polynomial ODEs. However, the question of a robust complexity theory for such models and its relations to classical (discrete) computation theory is an old problem. There was some recent significant progress: it has been proved that (classical) time complexity corresponds to the length of the involved curves. The question of whether there is a simple and robust way to measure space complexity remains. We argue that space complexity corresponds to precision and conversely. We propose and prove an algebraic characterisation of FPSPACE, using continuous ODEs. Recent papers proposed algebraic characterisations of polynomial-time and -space complexity classes over the reals, but with a discrete-time: those algebras rely on discrete ODE schemes. Here, we use classical (continuous) ODEs, with the classic definition of derivation and hence with the more natural context of continuous-time associated with ODEs. We characterise both the case of polynomial space functions over the integers and the reals. We prove that Turing machines, with a proper representation of real numbers, can be simulated by continuous ODEs and not just discrete ODEs. A major consequence is that the associated space complexity is provably related to the numerical stability of involved schemas and the associated required precision. We obtain that a problem can be solved in polynomial space if and only if it can be simulated by some numerically stable ODE, using a polynomial precision.","sentences":["Models of computations over the integers are equivalent from a computability and complexity theory point of view by the Church-Turing thesis.","It is not possible to unify discrete-time models over the reals.","The situation is unclear but simpler for continuous-time models, as there is a unifying mathematical model provided by ordinary differential equations (ODEs).","For example, the GPAC model of Shannon is known to correspond to polynomial ODEs.","However, the question of a robust complexity theory for such models and its relations to classical (discrete) computation theory is an old problem.","There was some recent significant progress: it has been proved that (classical) time complexity corresponds to the length of the involved curves.","The question of whether there is a simple and robust way to measure space complexity remains.","We argue that space complexity corresponds to precision and conversely.","We propose and prove an algebraic characterisation of FPSPACE, using continuous ODEs.","Recent papers proposed algebraic characterisations of polynomial-time and -space complexity classes over the reals, but with a discrete-time: those algebras rely on discrete ODE schemes.","Here, we use classical (continuous) ODEs, with the classic definition of derivation and hence with the more natural context of continuous-time associated with ODEs.","We characterise both the case of polynomial space functions over the integers and the reals.","We prove that Turing machines, with a proper representation of real numbers, can be simulated by continuous ODEs and not just discrete ODEs.","A major consequence is that the associated space complexity is provably related to the numerical stability of involved schemas and the associated required precision.","We obtain that a problem can be solved in polynomial space if and only if it can be simulated by some numerically stable ODE, using a polynomial precision."],"url":"http://arxiv.org/abs/2403.02499v1","category":"cs.CC"}
{"created":"2024-03-04 20:44:44","title":"Arctic curves of the T-system with Slanted Initial Data","abstract":"We study the T-system of type $A_\\infty$, also known as the octahedron recurrence/equation, viewed as a 2+1-dimensional discrete evolution equation. Generalizing the study of [P. Di Francesco and R. Soto-Garrido. Arctic curves of the octahedron equation. J. Phys. A, 47(28):285204, 34, 2014], we consider initial data along parallel ``slanted\" planes perpendicular to an arbitrary admissible direction $(r,s,t)\\in {\\mathbb Z}_+^3$. The solution of the T-system is interpreted as the partition function of a dimer model on some suitable ``pinecone\" graph introduced in [M. Bousquet-M\\'elou, J. Propp, and J. West. Perfect matchings for the three-term Gale-Robinson sequences. Electron. J. Combin., 16(1):Research Paper 125, 37, 2009]. The T-system formulation and some exact solutions in uniform or periodic cases allow us to explore the thermodynamic limit of the corresponding dimer models and to derive exact arctic curves separating the various phases of the system.","sentences":["We study the T-system of type $A_\\infty$, also known as the octahedron recurrence/equation, viewed as a 2+1-dimensional discrete evolution equation.","Generalizing the study of [P. Di Francesco and R. Soto-Garrido.","Arctic curves of the octahedron equation.","J. Phys.","A, 47(28):285204, 34, 2014], we consider initial data along parallel ``slanted\" planes perpendicular to an arbitrary admissible direction $(r,s,t)\\in {\\mathbb Z}_+^3$. The solution of the T-system is interpreted as the partition function of a dimer model on some suitable ``pinecone\" graph introduced in [M. Bousquet-M\\'elou, J. Propp, and J. West.","Perfect matchings for the three-term Gale-Robinson sequences.","Electron.","J. Combin., 16(1):Research Paper 125, 37, 2009].","The T-system formulation and some exact solutions in uniform or periodic cases allow us to explore the thermodynamic limit of the corresponding dimer models and to derive exact arctic curves separating the various phases of the system."],"url":"http://arxiv.org/abs/2403.02479v1","category":"math-ph"}
{"created":"2024-03-04 20:35:09","title":"When do Convolutional Neural Networks Stop Learning?","abstract":"Convolutional Neural Networks (CNNs) have demonstrated outstanding performance in computer vision tasks such as image classification, detection, segmentation, and medical image analysis. In general, an arbitrary number of epochs is used to train such neural networks. In a single epoch, the entire training data -- divided by batch size -- are fed to the network. In practice, validation error with training loss is used to estimate the neural network's generalization, which indicates the optimal learning capacity of the network. Current practice is to stop training when the training loss decreases and the gap between training and validation error increases (i.e., the generalization gap) to avoid overfitting. However, this is a trial-and-error-based approach which raises a critical question: Is it possible to estimate when neural networks stop learning based on training data? This research work introduces a hypothesis that analyzes the data variation across all the layers of a CNN variant to anticipate its near-optimal learning capacity. In the training phase, we use our hypothesis to anticipate the near-optimal learning capacity of a CNN variant without using any validation data. Our hypothesis can be deployed as a plug-and-play to any existing CNN variant without introducing additional trainable parameters to the network. We test our hypothesis on six different CNN variants and three different general image datasets (CIFAR10, CIFAR100, and SVHN). The result based on these CNN variants and datasets shows that our hypothesis saves 58.49\\% of computational time (on average) in training. We further conduct our hypothesis on ten medical image datasets and compared with the MedMNIST-V2 benchmark. Based on our experimental result, we save $\\approx$ 44.1\\% of computational time without losing accuracy against the MedMNIST-V2 benchmark.","sentences":["Convolutional Neural Networks (CNNs) have demonstrated outstanding performance in computer vision tasks such as image classification, detection, segmentation, and medical image analysis.","In general, an arbitrary number of epochs is used to train such neural networks.","In a single epoch, the entire training data -- divided by batch size -- are fed to the network.","In practice, validation error with training loss is used to estimate the neural network's generalization, which indicates the optimal learning capacity of the network.","Current practice is to stop training when the training loss decreases and the gap between training and validation error increases (i.e., the generalization gap) to avoid overfitting.","However, this is a trial-and-error-based approach which raises a critical question: Is it possible to estimate when neural networks stop learning based on training data?","This research work introduces a hypothesis that analyzes the data variation across all the layers of a CNN variant to anticipate its near-optimal learning capacity.","In the training phase, we use our hypothesis to anticipate the near-optimal learning capacity of a CNN variant without using any validation data.","Our hypothesis can be deployed as a plug-and-play to any existing CNN variant without introducing additional trainable parameters to the network.","We test our hypothesis on six different CNN variants and three different general image datasets (CIFAR10, CIFAR100, and SVHN).","The result based on these CNN variants and datasets shows that our hypothesis saves 58.49\\% of computational time (on average) in training.","We further conduct our hypothesis on ten medical image datasets and compared with the MedMNIST-V2 benchmark.","Based on our experimental result, we save $\\approx$ 44.1\\% of computational time without losing accuracy against the MedMNIST-V2 benchmark."],"url":"http://arxiv.org/abs/2403.02473v1","category":"cs.CV"}
{"created":"2024-03-04 20:29:36","title":"A Primal-dual hybrid gradient method for solving optimal control problems and the corresponding Hamilton-Jacobi PDEs","abstract":"Optimal control problems are crucial in various domains, including path planning, robotics, and humanoid control, demonstrating their broad applicability. The connection between optimal control and Hamilton-Jacobi (HJ) partial differential equations (PDEs) underscores the need for solving HJ PDEs to address these control problems effectively. While numerous numerical methods exist for tackling HJ PDEs across different dimensions, this paper introduces an innovative optimization-based approach that reformulates optimal control problems and HJ PDEs into a saddle point problem using a Lagrange multiplier. Our method, based on the preconditioned primal-dual hybrid gradient (PDHG) method, offers a solution to HJ PDEs with first-order accuracy and numerical unconditional stability, enabling larger time steps and avoiding the limitations of explicit time discretization methods. Our approach has ability to handle a wide variety of Hamiltonian functions, including those that are non-smooth and dependent on time and space, through a simplified saddle point formulation that facilitates easy and parallelizable updates. Furthermore, our framework extends to viscous HJ PDEs and stochastic optimal control problems, showcasing its versatility. Through a series of numerical examples, we demonstrate the method's effectiveness in managing diverse Hamiltonians and achieving efficient parallel computation, highlighting its potential for wide-ranging applications in optimal control and beyond.","sentences":["Optimal control problems are crucial in various domains, including path planning, robotics, and humanoid control, demonstrating their broad applicability.","The connection between optimal control and Hamilton-Jacobi (HJ) partial differential equations (PDEs) underscores the need for solving HJ PDEs to address these control problems effectively.","While numerous numerical methods exist for tackling HJ PDEs across different dimensions, this paper introduces an innovative optimization-based approach that reformulates optimal control problems and HJ PDEs into a saddle point problem using a Lagrange multiplier.","Our method, based on the preconditioned primal-dual hybrid gradient (PDHG) method, offers a solution to HJ PDEs with first-order accuracy and numerical unconditional stability, enabling larger time steps and avoiding the limitations of explicit time discretization methods.","Our approach has ability to handle a wide variety of Hamiltonian functions, including those that are non-smooth and dependent on time and space, through a simplified saddle point formulation that facilitates easy and parallelizable updates.","Furthermore, our framework extends to viscous HJ PDEs and stochastic optimal control problems, showcasing its versatility.","Through a series of numerical examples, we demonstrate the method's effectiveness in managing diverse Hamiltonians and achieving efficient parallel computation, highlighting its potential for wide-ranging applications in optimal control and beyond."],"url":"http://arxiv.org/abs/2403.02468v1","category":"math.OC"}
{"created":"2024-03-04 20:28:28","title":"Applied Causal Inference Powered by ML and AI","abstract":"An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.","sentences":["An introduction to the emerging fusion of machine learning and causal inference.","The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools."],"url":"http://arxiv.org/abs/2403.02467v1","category":"econ.EM"}
{"created":"2024-03-04 20:09:56","title":"Programming the scalable optical learning operator with spatial-spectral optimization","abstract":"Electronic computers have evolved drastically over the past years with an ever-growing demand for improved performance. However, the transfer of information from memory and high energy consumption have emerged as issues that require solutions. Optical techniques are considered promising solutions to these problems with higher speed than their electronic counterparts and with reduced energy consumption. Here, we use the optical reservoir computing framework we have previously described (Scalable Optical Learning Operator or SOLO) to program the spatial-spectral output of the light after nonlinear propagation in a multimode fiber. The novelty in the current paper is that the system is programmed through an output sampling scheme, similar to that used in hyperspectral imaging in astronomy. Linear and nonlinear computations are performed by light in the multimode fiber and the high dimensional spatial-spectral information at the fiber output is optically programmed before it reaches the camera. We then used a digital computer to classify the programmed output of the multi-mode fiber using a simple, single layer network. When combining front-end programming and the proposed spatial-spectral programming, we were able to achieve 89.9% classification accuracy on the dataset consisting of chest X-ray images from COVID-19 patients. At the same time, we obtained a decrease of 99% in the number of tunable parameters compared to an equivalently performing digital neural network. These results show that the performance of programmed SOLO is comparable with cutting-edge electronic computing platforms, albeit with a much-reduced number of electronic operations.","sentences":["Electronic computers have evolved drastically over the past years with an ever-growing demand for improved performance.","However, the transfer of information from memory and high energy consumption have emerged as issues that require solutions.","Optical techniques are considered promising solutions to these problems with higher speed than their electronic counterparts and with reduced energy consumption.","Here, we use the optical reservoir computing framework we have previously described (Scalable Optical Learning Operator or SOLO) to program the spatial-spectral output of the light after nonlinear propagation in a multimode fiber.","The novelty in the current paper is that the system is programmed through an output sampling scheme, similar to that used in hyperspectral imaging in astronomy.","Linear and nonlinear computations are performed by light in the multimode fiber and the high dimensional spatial-spectral information at the fiber output is optically programmed before it reaches the camera.","We then used a digital computer to classify the programmed output of the multi-mode fiber using a simple, single layer network.","When combining front-end programming and the proposed spatial-spectral programming, we were able to achieve 89.9% classification accuracy on the dataset consisting of chest X-ray images from COVID-19 patients.","At the same time, we obtained a decrease of 99% in the number of tunable parameters compared to an equivalently performing digital neural network.","These results show that the performance of programmed SOLO is comparable with cutting-edge electronic computing platforms, albeit with a much-reduced number of electronic operations."],"url":"http://arxiv.org/abs/2403.02452v1","category":"physics.optics"}
{"created":"2024-03-04 20:01:55","title":"Compact Einstein-type manifolds with parallel Ricci tensor","abstract":"In this paper, we deduce a Bochner-type identity for compact gradient Einstein-type manifolds with boundary. As consequence, we are able to show a rigidity result for Einstein-type manifolds assuming the parallel Ricci curvature condition. Moreover, we provide a condition on the norm of the gradient of the potential function in order to classify such structures.","sentences":["In this paper, we deduce a Bochner-type identity for compact gradient Einstein-type manifolds with boundary.","As consequence, we are able to show a rigidity result for Einstein-type manifolds assuming the parallel Ricci curvature condition.","Moreover, we provide a condition on the norm of the gradient of the potential function in order to classify such structures."],"url":"http://arxiv.org/abs/2403.02447v1","category":"math.DG"}
{"created":"2024-03-04 19:53:11","title":"Tuning neural posterior estimation for gravitational wave inference","abstract":"Modern simulation-based inference techniques use neural networks to solve inverse problems efficiently. One notable strategy is neural posterior estimation (NPE), wherein a neural network parameterizes a distribution to approximate the posterior. This approach is particularly advantageous for tackling low-latency or high-volume inverse problems. However, the accuracy of NPE varies significantly within the learned parameter space. This variability is observed even in seemingly straightforward systems like coupled-harmonic oscillators. This paper emphasizes the critical role of prior selection in ensuring the consistency of NPE outcomes. Our findings indicate a clear relationship between NPE performance across the parameter space and the number of similar samples trained on by the model. Thus, the prior should match the sample diversity across the parameter space to promote strong, uniform performance. Furthermore, we introduce a novel procedure, in which amortized and sequential NPE are combined to swiftly refine NPE predictions for individual events. This method substantially improves sample efficiency, on average from nearly 0% to 10-80% within ten minutes. Notably, our research demonstrates its real-world applicability by achieving a significant milestone: accurate and swift inference of posterior distributions for low-mass binary black hole (BBH) events with NPE.","sentences":["Modern simulation-based inference techniques use neural networks to solve inverse problems efficiently.","One notable strategy is neural posterior estimation (NPE), wherein a neural network parameterizes a distribution to approximate the posterior.","This approach is particularly advantageous for tackling low-latency or high-volume inverse problems.","However, the accuracy of NPE varies significantly within the learned parameter space.","This variability is observed even in seemingly straightforward systems like coupled-harmonic oscillators.","This paper emphasizes the critical role of prior selection in ensuring the consistency of NPE outcomes.","Our findings indicate a clear relationship between NPE performance across the parameter space and the number of similar samples trained on by the model.","Thus, the prior should match the sample diversity across the parameter space to promote strong, uniform performance.","Furthermore, we introduce a novel procedure, in which amortized and sequential NPE are combined to swiftly refine NPE predictions for individual events.","This method substantially improves sample efficiency, on average from nearly 0% to 10-80% within ten minutes.","Notably, our research demonstrates its real-world applicability by achieving a significant milestone: accurate and swift inference of posterior distributions for low-mass binary black hole (BBH) events with NPE."],"url":"http://arxiv.org/abs/2403.02443v1","category":"astro-ph.IM"}
{"created":"2024-03-04 19:46:05","title":"Non-Abelian extensions of degree $p^3$ and $p^4$ in characteristic $p>2$","abstract":"This paper describes in terms of Artin-Schreier equations field extensions whose Galois group is isomorphic to any of the four non-cyclic groups of order $p^3$ or the ten non-Abelian groups of order $p^4$, $p$ an odd prime, over a field of characteristic $p$.","sentences":["This paper describes in terms of Artin-Schreier equations field extensions whose Galois group is isomorphic to any of the four non-cyclic groups of order $p^3$ or the ten non-Abelian groups of order $p^4$, $p$ an odd prime, over a field of characteristic $p$."],"url":"http://arxiv.org/abs/2403.02442v1","category":"math.NT"}
{"created":"2024-03-04 19:40:54","title":"Compact stars in Rastall gravity: Hydrostatic equilibrium and radial pulsations","abstract":"Within the context of Rastall gravity, we investigate the hydrostatic equilibrium and dynamical stability against radial pulsations of compact stars, where a free parameter $\\beta$ measures the deviations from General Relativity (GR). We derive both the modified Tolman-Oppenheimer-Volkoff (TOV) equations and the Sturm-Liouville differential equation governing the adiabatic radial oscillations. Such equations are solved numerically in order to obtain the compact-star properties for two realistic equations of state (EoSs). For hadronic matter, the fundamental mode frequency $\\omega_0$ becomes unstable almost at the critical central energy density corresponding to the maximum gravitational mass. However, for quark matter, where larger values of $\\vert\\beta\\vert$ are required to observe appreciable changes in the mass-radius diagram, there exist stable stars after the maximum-mass configuration for negative values of $\\beta$. Using an independent analysis, our results reveal that the emergence of a cusp can be used as a criterion to indicate the onset of instability when the binding energy is plotted as a function of the proper mass. Specifically, we find that the central-density value where the binding energy is a minimum corresponds precisely to $\\omega_0^2 =0$.","sentences":["Within the context of Rastall gravity, we investigate the hydrostatic equilibrium and dynamical stability against radial pulsations of compact stars, where a free parameter $\\beta$ measures the deviations from General Relativity (GR).","We derive both the modified Tolman-Oppenheimer-Volkoff (TOV) equations and the Sturm-Liouville differential equation governing the adiabatic radial oscillations.","Such equations are solved numerically in order to obtain the compact-star properties for two realistic equations of state (EoSs).","For hadronic matter, the fundamental mode frequency $\\omega_0$ becomes unstable almost at the critical central energy density corresponding to the maximum gravitational mass.","However, for quark matter, where larger values of $\\vert\\beta\\vert$ are required to observe appreciable changes in the mass-radius diagram, there exist stable stars after the maximum-mass configuration for negative values of $\\beta$. Using an independent analysis, our results reveal that the emergence of a cusp can be used as a criterion to indicate the onset of instability when the binding energy is plotted as a function of the proper mass.","Specifically, we find that the central-density value where the binding energy is a minimum corresponds precisely to $\\omega_0^2 =0$."],"url":"http://arxiv.org/abs/2403.02440v1","category":"gr-qc"}
{"created":"2024-03-04 19:37:16","title":"Approximation of the Koopman operator via Bernstein polynomials","abstract":"The Koopman operator approach provides a powerful linear description of nonlinear dynamical systems in terms of the evolution of observables. While the operator is typically infinite-dimensional, it is crucial to develop finite-dimensional approximation methods and characterize the related approximation errors with upper bounds, preferably expressed in the uniform norm. In this paper, we depart from the traditional use of orthogonal projection or truncation, and propose a novel method based on Bernstein polynomial approximation. Considering a basis of Bernstein polynomials, we construct a matrix approximation of the Koopman operator in a computationally effective way. Building on results of approximation theory, we characterize the rates of convergence and the upper bounds of the error in various contexts including the cases of univariate and multivariate systems, and continuous and differentiable observables. The obtained bounds are expressed in the uniform norm in terms of the modulus of continuity of the observables. Finally, the method is extended to a data-driven setting through a proper change of coordinates, where it is shown to demonstrate good performance for trajectory prediction.","sentences":["The Koopman operator approach provides a powerful linear description of nonlinear dynamical systems in terms of the evolution of observables.","While the operator is typically infinite-dimensional, it is crucial to develop finite-dimensional approximation methods and characterize the related approximation errors with upper bounds, preferably expressed in the uniform norm.","In this paper, we depart from the traditional use of orthogonal projection or truncation, and propose a novel method based on Bernstein polynomial approximation.","Considering a basis of Bernstein polynomials, we construct a matrix approximation of the Koopman operator in a computationally effective way.","Building on results of approximation theory, we characterize the rates of convergence and the upper bounds of the error in various contexts including the cases of univariate and multivariate systems, and continuous and differentiable observables.","The obtained bounds are expressed in the uniform norm in terms of the modulus of continuity of the observables.","Finally, the method is extended to a data-driven setting through a proper change of coordinates, where it is shown to demonstrate good performance for trajectory prediction."],"url":"http://arxiv.org/abs/2403.02438v1","category":"math.DS"}
{"created":"2024-03-04 19:32:56","title":"Gauging Centrifugal Instabilities in Compressible Free-Shear Layers Via Nonlinear Boundary Region Equations","abstract":"Curved free shear layers emerge in many engineering problems involving complex flow geometries, such as the flow over a backward facing step, flows with wall injection in a boundary layer, the flow inside side-dump combustors, or wakes generated by vertical axis wind turbines, among others. Previous studies involving centrifugal instabilities have mainly focused on wall-flows where Taylor instabilities between two rotating concentric cylinders or G\\\"{o}rtler vortices in boundary layers, resulting from the imbalance between the centrifugal forces and the radial pressure gradients, are generated. Curved free shear layer flows, however, have not received sufficient attention, especially in the nonlinear regime. The present work investigates the development of centrifugal instabilities evolving in a curved free shear layer flow in the nonlinear compressible regime. The compressible Navier-Stokes equations are reduced to the nonlinear boundary region equations (BRE) in a high Reynolds number asymptotic framework wherein the streamwise wavelengths of the disturbances are assumed to be much larger than the spanwise and wall-normal counterparts. We study the effect of the freestream Mach number $\\boldsymbol{M_\\infty}$, the shear layer thickness $\\boldsymbol{\\delta}$, the amplitude of the incoming disturbance $\\boldsymbol{A}$, and the relative velocity difference across the shear layer $\\boldsymbol{\\Delta V}$ on the development of these centrifugal instabilities.","sentences":["Curved free shear layers emerge in many engineering problems involving complex flow geometries, such as the flow over a backward facing step, flows with wall injection in a boundary layer, the flow inside side-dump combustors, or wakes generated by vertical axis wind turbines, among others.","Previous studies involving centrifugal instabilities have mainly focused on wall-flows where Taylor instabilities between two rotating concentric cylinders or G\\\"{o}rtler vortices in boundary layers, resulting from the imbalance between the centrifugal forces and the radial pressure gradients, are generated.","Curved free shear layer flows, however, have not received sufficient attention, especially in the nonlinear regime.","The present work investigates the development of centrifugal instabilities evolving in a curved free shear layer flow in the nonlinear compressible regime.","The compressible Navier-Stokes equations are reduced to the nonlinear boundary region equations (BRE) in a high Reynolds number asymptotic framework wherein the streamwise wavelengths of the disturbances are assumed to be much larger than the spanwise and wall-normal counterparts.","We study the effect of the freestream Mach number $\\boldsymbol{M_\\infty}$, the shear layer thickness $\\boldsymbol{\\delta}$, the amplitude of the incoming disturbance $\\boldsymbol{A}$, and the relative velocity difference across the shear layer $\\boldsymbol{\\Delta V}$ on the development of these centrifugal instabilities."],"url":"http://arxiv.org/abs/2403.02435v1","category":"physics.flu-dyn"}
{"created":"2024-03-04 19:17:51","title":"On supercurves of genus 1 with an underlying odd spin structure","abstract":"We study the standard family of supercurves of genus 1 with an underlying odd spin structures. We give a simple algebraic description of this family and of the compactified family of stable supercurves with one Neveu-Schwarz puncture. We also describe the Gauss-Manin connection on the first de Rham cohomology of this family and compute the superperiods of global differentials.","sentences":["We study the standard family of supercurves of genus 1 with an underlying odd spin structures.","We give a simple algebraic description of this family and of the compactified family of stable supercurves with one Neveu-Schwarz puncture.","We also describe the Gauss-Manin connection on the first de Rham cohomology of this family and compute the superperiods of global differentials."],"url":"http://arxiv.org/abs/2403.02424v1","category":"math.AG"}
{"created":"2024-03-04 19:12:13","title":"From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima","abstract":"We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes. We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end. Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze. The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian. Through both theoretical analysis and numerical experiments, we show that in practical cases, i.e. for finite but even very large $N$, successful optimization via gradient descent in phase retrieval is achieved by falling towards the good minima before reaching the bad ones. This mechanism explains why successful recovery is obtained well before the algorithmic transition corresponding to the high-dimensional limit. Technically, this is associated to strong logarithmic corrections of the algorithmic transition at large $N$ with respect to the one expected in the $N\\to\\infty$ limit. Our analysis sheds light on such a new mechanism that facilitate gradient descent dynamics in finite large dimensions, also highlighting the importance of good initialization of spectral properties for optimization in complex high-dimensional landscapes.","sentences":["We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes.","We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end.","Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze.","The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian.","Through both theoretical analysis and numerical experiments, we show that in practical cases, i.e. for finite but even very large $N$, successful optimization via gradient descent in phase retrieval is achieved by falling towards the good minima before reaching the bad ones.","This mechanism explains why successful recovery is obtained well before the algorithmic transition corresponding to the high-dimensional limit.","Technically, this is associated to strong logarithmic corrections of the algorithmic transition at large $N$ with respect to the one expected in the $N\\to\\infty$ limit.","Our analysis sheds light on such a new mechanism that facilitate gradient descent dynamics in finite large dimensions, also highlighting the importance of good initialization of spectral properties for optimization in complex high-dimensional landscapes."],"url":"http://arxiv.org/abs/2403.02418v1","category":"cs.LG"}
{"created":"2024-03-04 19:04:02","title":"Minimal Surface Equation and Bernstein Property on RCD spaces","abstract":"We show that if $(X,d,m)$ is an RCD(K,N) space and $u \\in W^{1,1}_{loc}(X)$ is a solution of the minimal surface equation, then $u$ is harmonic on its graph (which has a natural metric measure space structure). If K=0 this allows to obtain an Harnack inequality for $u$, which in turn implies the Bernstein property, meaning that any positive solution to the minimal surface equation must be constant. As an application, we obtain oscillation estimates and a Bernstein Theorem for minimal graphs in products $M \\times \\mathbb{R}$, where $M$ is a smooth manifold (possibly weighted and with boundary) with non-negative Ricci curvature","sentences":["We show that if $(X,d,m)$ is an RCD(K,N) space and $u \\in W^{1,1}_{loc}(X)$ is a solution of the minimal surface equation, then $u$ is harmonic on its graph (which has a natural metric measure space structure).","If K=0 this allows to obtain an Harnack inequality for $u$, which in turn implies the Bernstein property, meaning that any positive solution to the minimal surface equation must be constant.","As an application, we obtain oscillation estimates and a Bernstein Theorem for minimal graphs in products $M \\times \\mathbb{R}$, where $M$ is a smooth manifold (possibly weighted and with boundary) with non-negative Ricci curvature"],"url":"http://arxiv.org/abs/2403.02406v1","category":"math.DG"}
{"created":"2024-03-04 19:01:05","title":"Impact of unmodeled eccentricity on the tidal deformability measurement and implications for gravitational wave physics inference","abstract":"With the expected large number of binary neutron star (BNS) observations through gravitational waves (GWs), third-generation GW detectors, Cosmic Explorer (CE) and Einstein Telescope (ET), will be able to constrain the tidal deformability, and hence the equation of state (EoS) of neutron star (NS) with exquisite precision. A subset of the detected BNS systems can retain residual eccentricity in the detector frequency band. We study the systematic errors due to unmodeled eccentricity in the tidal deformability measurement and its implications for NS EoS and redshift measurement via the Love siren method. We find that the systematic errors in the tidal deformability parameter exceed the statistical errors at an eccentricity of $\\sim 10^{-3}$ ($\\sim 3\\times 10^{-4}$) at $10$Hz reference GW frequency for CE (ET). We show that these biases on tidal deformability parameter can significantly bias the NS EoS inference. Furthermore, the error on tidal deformability propagates to the source frame NS mass, which in turn biases the redshift inference. For CE, the redshift inference is significantly biased at an eccentricity of $\\sim 10^{-3}$ (at a reference frequency of $10$Hz). We also study the implications of biased tidal deformability in testing the Kerr nature of black holes. Systematic error on the tidal deformability parameter leads to a non-zero value of tidal deformability for binary black holes, indicating a false deviation from the Kerr nature. Finally, we show that including eccentricity in the waveform model increases the statistical errors in tidal deformability measurement by a factor of $\\lesssim 2$. Our study, therefore, highlights the importance of using accurate eccentric waveform models for GW parameter inference.","sentences":["With the expected large number of binary neutron star (BNS) observations through gravitational waves (GWs), third-generation GW detectors, Cosmic Explorer (CE) and Einstein Telescope (ET), will be able to constrain the tidal deformability, and hence the equation of state (EoS) of neutron star (NS) with exquisite precision.","A subset of the detected BNS systems can retain residual eccentricity in the detector frequency band.","We study the systematic errors due to unmodeled eccentricity in the tidal deformability measurement and its implications for NS EoS and redshift measurement via the Love siren method.","We find that the systematic errors in the tidal deformability parameter exceed the statistical errors at an eccentricity of $\\sim 10^{-3}$ ($\\sim 3\\times 10^{-4}$) at $10$Hz reference GW frequency for CE (ET).","We show that these biases on tidal deformability parameter can significantly bias the NS EoS inference.","Furthermore, the error on tidal deformability propagates to the source frame NS mass, which in turn biases the redshift inference.","For CE, the redshift inference is significantly biased at an eccentricity of $\\sim 10^{-3}$ (at a reference frequency of $10$Hz).","We also study the implications of biased tidal deformability in testing the Kerr nature of black holes.","Systematic error on the tidal deformability parameter leads to a non-zero value of tidal deformability for binary black holes, indicating a false deviation from the Kerr nature.","Finally, we show that including eccentricity in the waveform model increases the statistical errors in tidal deformability measurement by a factor of $\\lesssim 2$.","Our study, therefore, highlights the importance of using accurate eccentric waveform models for GW parameter inference."],"url":"http://arxiv.org/abs/2403.02404v1","category":"astro-ph.HE"}
{"created":"2024-03-04 19:00:01","title":"Bell correlations of a thermal fully-connected spin chain in a vicinity of a quantum critical point","abstract":"Bell correlations are among the most exotic phenomena through which quantum mechanics manifests itself. Their presence signals that the system can violate the postulates of local realism, once believed to be the nonnegotiable property of the physical world. The importance of Bell correlations from this fundamental point of view is even straightened by their applications -- ranging from quantum cryptography through quantum metrology to quantum computing. Hence it is of growing interest to characterize the ``Bell content'' of complex, scalable many-body systems. Here we perform the detailed analysis of the character and strength of many-body Bell correlations in interacting multi-qubit systems with particle-exchange symmetry. Such configuration can be mapped onto an effective Schr\\\"odinger-like equation, which allows for precise analytical predictions. We show that in the vicinity of the quantum critical point, these correlations quickly become so strong that only a fraction of qubits remains uncorrelated. We also identify the threshold temperature, which, once overpassed, empowers thermal fluctuations that destroy Bell correlations in the system. We hope that the approach presented here, due to its universality, could be useful for the upcoming research on genuinely nonclassical Bell-correlated complex systems.","sentences":["Bell correlations are among the most exotic phenomena through which quantum mechanics manifests itself.","Their presence signals that the system can violate the postulates of local realism, once believed to be the nonnegotiable property of the physical world.","The importance of Bell correlations from this fundamental point of view is even straightened by their applications -- ranging from quantum cryptography through quantum metrology to quantum computing.","Hence it is of growing interest to characterize the ``Bell content'' of complex, scalable many-body systems.","Here we perform the detailed analysis of the character and strength of many-body Bell correlations in interacting multi-qubit systems with particle-exchange symmetry.","Such configuration can be mapped onto an effective Schr\\\"odinger-like equation, which allows for precise analytical predictions.","We show that in the vicinity of the quantum critical point, these correlations quickly become so strong that only a fraction of qubits remains uncorrelated.","We also identify the threshold temperature, which, once overpassed, empowers thermal fluctuations that destroy Bell correlations in the system.","We hope that the approach presented here, due to its universality, could be useful for the upcoming research on genuinely nonclassical Bell-correlated complex systems."],"url":"http://arxiv.org/abs/2403.02383v1","category":"quant-ph"}
{"created":"2024-03-04 19:00:00","title":"Relational bulk reconstruction from modular flow","abstract":"The entanglement wedge reconstruction paradigm in AdS/CFT states that for a bulk qudit within the entanglement wedge of a boundary subregion $\\bar{A}$, operators acting on the bulk qudit can be reconstructed as CFT operators on $\\bar{A}$. This naturally fits within the framework of quantum error correction, with the CFT states containing the bulk qudit forming a code protected against the erasure of the boundary subregion $A$. In this paper, we set up and study a framework for relational bulk reconstruction in holography: given two code subspaces both protected against erasure of the boundary region $A$, the goal is to relate the operator reconstructions between the two spaces. To accomplish this, we assume that the two code subspaces are smoothly connected by a one-parameter family of codes all protected against the erasure of $A$, and that the maximally-entangled states on these codes are all full-rank. We argue that such code subspaces can naturally be constructed in holography in a \"measurement-based\" setting. In this setting, we derive a flow equation for the operator reconstruction of a fixed code subspace operator using modular theory which can, in principle, be integrated to relate the reconstructed operators all along the flow. We observe a striking resemblance between our formulas for relational bulk reconstruction and the infinite-time limit of Connes cocycle flow, and take some steps towards making this connection more rigorous. We also provide alternative derivations of our reconstruction formulas in terms of a canonical reconstruction map we call the modular reflection operator.","sentences":["The entanglement wedge reconstruction paradigm in AdS/CFT states that for a bulk qudit within the entanglement wedge of a boundary subregion $\\bar{A}$, operators acting on the bulk qudit can be reconstructed as CFT operators on $\\bar{A}$. This naturally fits within the framework of quantum error correction, with the CFT states containing the bulk qudit forming a code protected against the erasure of the boundary subregion $A$.","In this paper, we set up and study a framework for relational bulk reconstruction in holography: given two code subspaces both protected against erasure of the boundary region $A$, the goal is to relate the operator reconstructions between the two spaces.","To accomplish this, we assume that the two code subspaces are smoothly connected by a one-parameter family of codes all protected against the erasure of $A$, and that the maximally-entangled states on these codes are all full-rank.","We argue that such code subspaces can naturally be constructed in holography in a \"measurement-based\" setting.","In this setting, we derive a flow equation for the operator reconstruction of a fixed code subspace operator using modular theory which can, in principle, be integrated to relate the reconstructed operators all along the flow.","We observe a striking resemblance between our formulas for relational bulk reconstruction and the infinite-time limit of Connes cocycle flow, and take some steps towards making this connection more rigorous.","We also provide alternative derivations of our reconstruction formulas in terms of a canonical reconstruction map we call the modular reflection operator."],"url":"http://arxiv.org/abs/2403.02377v1","category":"hep-th"}
{"created":"2024-03-04 18:59:39","title":"Exploring Well-Posedness and Asymptotic Behavior in an Advection-Diffusion-Reaction (ADR) Model","abstract":"In this paper, the existence, uniqueness, and positivity of solutions, as well as the asymptotic behavior through a finite fractal dimensional global attractor for a general Advection-Diffusion-Reaction (ADR) equation, are investigated. Our findings are innovative, as we employ semigroups and global attractors theories to achieve these results. Also, an analytical solution of a two-dimensional Advection-Diffusion Equation is presented. And finally, two Explicit Finite Difference schemes are used to simulate solutions in the two- and three-dimensional cases. The numerical simulations are conducted with predefined initial and Dirichlet boundary conditions.","sentences":["In this paper, the existence, uniqueness, and positivity of solutions, as well as the asymptotic behavior through a finite fractal dimensional global attractor for a general Advection-Diffusion-Reaction (ADR) equation, are investigated.","Our findings are innovative, as we employ semigroups and global attractors theories to achieve these results.","Also, an analytical solution of a two-dimensional Advection-Diffusion Equation is presented.","And finally, two Explicit Finite Difference schemes are used to simulate solutions in the two- and three-dimensional cases.","The numerical simulations are conducted with predefined initial and Dirichlet boundary conditions."],"url":"http://arxiv.org/abs/2403.02339v1","category":"math.AP"}
{"created":"2024-03-04 18:58:09","title":"Toward Neuromic Computing: Neurons as Autoencoders","abstract":"The computational capabilities of dendrites have become increasingly clear. This letter presents the idea that neural backpropagation is using dendritic processing to enable individual neurons to perform autoencoding. Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored. This is contrasted to the standard layered approach to autoencoding. It is shown that such individualised processing is not detrimental and can improve network learning.","sentences":["The computational capabilities of dendrites have become increasingly clear.","This letter presents the idea that neural backpropagation is using dendritic processing to enable individual neurons to perform autoencoding.","Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored.","This is contrasted to the standard layered approach to autoencoding.","It is shown that such individualised processing is not detrimental and can improve network learning."],"url":"http://arxiv.org/abs/2403.02331v1","category":"cs.NE"}
{"created":"2024-03-04 18:55:42","title":"Approximate Controllability for Nonautonomous Integrodifferential Equations with State-dependent Delay","abstract":"In this work, we study the existence of mild solutions and the approximate controllability for nonautonomous integrodifferential equations with state-dependent delay. We assume the approximate controllability of the linear part, then we use the resolvent operators theory to prove the approximate controllability of the nonlinear case. An example of one-dimensional heat equation with memory is given to illustrate our basic results.","sentences":["In this work, we study the existence of mild solutions and the approximate controllability for nonautonomous integrodifferential equations with state-dependent delay.","We assume the approximate controllability of the linear part, then we use the resolvent operators theory to prove the approximate controllability of the nonlinear case.","An example of one-dimensional heat equation with memory is given to illustrate our basic results."],"url":"http://arxiv.org/abs/2403.02326v1","category":"math.OC"}
{"created":"2024-03-04 18:55:16","title":"Preserving Smart Grid Integrity: A Differential Privacy Framework for Secure Detection of False Data Injection Attacks in the Smart Grid","abstract":"In this paper, we present a framework based on differential privacy (DP) for querying electric power measurements to detect system anomalies or bad data caused by false data injections (FDIs). Our DP approach conceals consumption and system matrix data, while simultaneously enabling an untrusted third party to test hypotheses of anomalies, such as an FDI attack, by releasing a randomized sufficient statistic for hypothesis-testing. We consider a measurement model corrupted by Gaussian noise and a sparse noise vector representing the attack, and we observe that the optimal test statistic is a chi-square random variable. To detect possible attacks, we propose a novel DP chi-square noise mechanism that ensures the test does not reveal private information about power injections or the system matrix. The proposed framework provides a robust solution for detecting FDIs while preserving the privacy of sensitive power system data.","sentences":["In this paper, we present a framework based on differential privacy (DP) for querying electric power measurements to detect system anomalies or bad data caused by false data injections (FDIs).","Our DP approach conceals consumption and system matrix data, while simultaneously enabling an untrusted third party to test hypotheses of anomalies, such as an FDI attack, by releasing a randomized sufficient statistic for hypothesis-testing.","We consider a measurement model corrupted by Gaussian noise and a sparse noise vector representing the attack, and we observe that the optimal test statistic is a chi-square random variable.","To detect possible attacks, we propose a novel DP chi-square noise mechanism that ensures the test does not reveal private information about power injections or the system matrix.","The proposed framework provides a robust solution for detecting FDIs while preserving the privacy of sensitive power system data."],"url":"http://arxiv.org/abs/2403.02324v1","category":"eess.SP"}
{"created":"2024-03-04 18:51:29","title":"Dark Energy Survey Year 3 results: likelihood-free, simulation-based $w$CDM inference with neural compression of weak-lensing map statistics","abstract":"We present simulation-based cosmological $w$CDM inference using Dark Energy Survey Year 3 weak-lensing maps, via neural data compression of weak-lensing map summary statistics: power spectra, peak counts, and direct map-level compression/inference with convolutional neural networks (CNN). Using simulation-based inference, also known as likelihood-free or implicit inference, we use forward-modelled mock data to estimate posterior probability distributions of unknown parameters. This approach allows all statistical assumptions and uncertainties to be propagated through the forward-modelled mock data; these include sky masks, non-Gaussian shape noise, shape measurement bias, source galaxy clustering, photometric redshift uncertainty, intrinsic galaxy alignments, non-Gaussian density fields, neutrinos, and non-linear summary statistics. We include a series of tests to validate our inference results. This paper also describes the Gower Street simulation suite: 791 full-sky PKDGRAV dark matter simulations, with cosmological model parameters sampled with a mixed active-learning strategy, from which we construct over 3000 mock DES lensing data sets. For $w$CDM inference, for which we allow $-1<w<-\\frac{1}{3}$, our most constraining result uses power spectra combined with map-level (CNN) inference. Using gravitational lensing data only, this map-level combination gives $\\Omega_{\\rm m} = 0.283^{+0.020}_{-0.027}$, ${S_8 = 0.804^{+0.025}_{-0.017}}$, and $w < -0.80$ (with a 68 per cent credible interval); compared to the power spectrum inference, this is more than a factor of two improvement in dark energy parameter ($\\Omega_{\\rm DE}, w$) precision.","sentences":["We present simulation-based cosmological $w$CDM inference using Dark Energy Survey Year 3 weak-lensing maps, via neural data compression of weak-lensing map summary statistics: power spectra, peak counts, and direct map-level compression/inference with convolutional neural networks (CNN).","Using simulation-based inference, also known as likelihood-free or implicit inference, we use forward-modelled mock data to estimate posterior probability distributions of unknown parameters.","This approach allows all statistical assumptions and uncertainties to be propagated through the forward-modelled mock data; these include sky masks, non-Gaussian shape noise, shape measurement bias, source galaxy clustering, photometric redshift uncertainty, intrinsic galaxy alignments, non-Gaussian density fields, neutrinos, and non-linear summary statistics.","We include a series of tests to validate our inference results.","This paper also describes the Gower Street simulation suite: 791 full-sky PKDGRAV dark matter simulations, with cosmological model parameters sampled with a mixed active-learning strategy, from which we construct over 3000 mock DES lensing data sets.","For $w$CDM inference, for which we allow $-1<w<-\\frac{1}{3}$, our most constraining result uses power spectra combined with map-level (CNN) inference.","Using gravitational lensing data only, this map-level combination gives $\\Omega_{\\rm m} = 0.283^{+0.020}_{-0.027}$, ${S_8 = 0.804^{+0.025}_{-0.017}}$, and $w < -0.80$ (with a 68 per cent credible interval); compared to the power spectrum inference, this is more than a factor of two improvement in dark energy parameter ($\\Omega_{\\rm DE}, w$) precision."],"url":"http://arxiv.org/abs/2403.02314v1","category":"astro-ph.CO"}
{"created":"2024-03-04 18:47:56","title":"Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications to Cardiac MRI Segmentation","abstract":"Deep learning (DL)-based methods have achieved state-of-the-art performance for a wide range of medical image segmentation tasks. Nevertheless, recent studies show that deep neural networks (DNNs) can be miscalibrated and overconfident, leading to \"silent failures\" that are risky} for clinical applications. Bayesian statistics provide an intuitive approach to DL failure detection, based on posterior probability estimation. However, Bayesian DL, and in particular the posterior estimation, is intractable for large medical image segmentation DNNs. To tackle this challenge, we propose a Bayesian learning framework by Hamiltonian Monte Carlo (HMC), tempered by cold posterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC computation, we further propose a cyclical annealing strategy, which captures both local and global geometries of the posterior distribution, enabling highly efficient Bayesian DNN training with the same computational budget requirements as training a single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along with the segmentation uncertainty. We evaluate the proposed HMC-CP extensively on cardiac magnetic resonance image (MRI) segmentation, using in-domain steady-state free precession (SSFP) cine images as well as out-of-domain datasets of quantitative $T_1$ and $T_2$ mapping.","sentences":["Deep learning (DL)-based methods have achieved state-of-the-art performance for a wide range of medical image segmentation tasks.","Nevertheless, recent studies show that deep neural networks (DNNs) can be miscalibrated and overconfident, leading to \"silent failures\" that are risky} for clinical applications.","Bayesian statistics provide an intuitive approach to DL failure detection, based on posterior probability estimation.","However, Bayesian DL, and in particular the posterior estimation, is intractable for large medical image segmentation DNNs.","To tackle this challenge, we propose a Bayesian learning framework by Hamiltonian Monte Carlo (HMC), tempered by cold posterior (CP) to accommodate medical data augmentation, named HMC-CP.","For HMC computation, we further propose a cyclical annealing strategy, which captures both local and global geometries of the posterior distribution, enabling highly efficient Bayesian DNN training with the same computational budget requirements as training a single DNN.","The resulting Bayesian DNN outputs an ensemble segmentation along with the segmentation uncertainty.","We evaluate the proposed HMC-CP extensively on cardiac magnetic resonance image (MRI) segmentation, using in-domain steady-state free precession (SSFP) cine images as well as out-of-domain datasets of quantitative $T_1$ and $T_2$ mapping."],"url":"http://arxiv.org/abs/2403.02311v1","category":"eess.IV"}
{"created":"2024-03-04 18:41:24","title":"$(2,2p+1)$ minimal string and intersection theory I","abstract":"In view of recent progress in studying matrix model-2D gravity duality, we reexamine some features of $(2,2p+1)$ minimal string. After reviewing both sides of the proposed correspondence in this case, a previously unnoted identification between correlation numbers of tachyon operators in certain domain of parameter space and \"$p$-deformed volumes\", which are certain integral transforms of topological recursion data, is described and clarified. This identification allows us to efficiently study correlation numbers at finite matter central charge. In particular, we obtain an intersection-theoretic formula and the simplest recurrent equations for them, analogous to the ones recently derived for Virasoro minimal string. These formulas might be useful in establishing a more thorough connection between worldsheet and matrix model approaches.","sentences":["In view of recent progress in studying matrix model-2D gravity duality, we reexamine some features of $(2,2p+1)$ minimal string.","After reviewing both sides of the proposed correspondence in this case, a previously unnoted identification between correlation numbers of tachyon operators in certain domain of parameter space and \"$p$-deformed volumes\", which are certain integral transforms of topological recursion data, is described and clarified.","This identification allows us to efficiently study correlation numbers at finite matter central charge.","In particular, we obtain an intersection-theoretic formula and the simplest recurrent equations for them, analogous to the ones recently derived for Virasoro minimal string.","These formulas might be useful in establishing a more thorough connection between worldsheet and matrix model approaches."],"url":"http://arxiv.org/abs/2403.02305v1","category":"hep-th"}
{"created":"2024-03-04 18:30:09","title":"Effective extensional-torsional elasticity and dynamics of helical filaments under distributed loads","abstract":"We study slender, helical elastic rods subject to distributed forces and moments. Focussing on the case when the helix axis remains straight, we employ the method of multiple scales to systematically derive an 'effective-column' theory from the Kirchhoff rod equations: the helical filament is described as a naturally-straight rod (aligned with the helix axis) for which the extensional and torsional deformations are coupled. Importantly, our analysis is asymptotically exact in the limit of a 'highly-coiled' filament (i.e., when the helical wavelength is much smaller than the characteristic lengthscale over which the filament bends due to external loading) and is able to account for large, unsteady displacements. In the small-deformation limit, we exactly recover the coupled wave equations used to describe the free vibrations of helical coil springs, thereby justifying previous effective-column approximations in which linearised stiffness coefficients are assumed to apply locally and dynamically. We then illustrate our theory with two loading scenarios: (I) a heavy helical rod deforming under its own weight; and (II) the dynamics of axial rotation (twirling) in viscous fluid, which may be considered as a simple model for a bacteria flagellar filament. More broadly, our analysis provides a framework to develop reduced models of helical rods in a wide variety of physical and biological settings, and yields analytical insight into their elastic instabilities. In particular, our analysis indicates that tensile instabilities are a generic phenomenon when helical rods are subject to a combination of distributed forces and moments.","sentences":["We study slender, helical elastic rods subject to distributed forces and moments.","Focussing on the case when the helix axis remains straight, we employ the method of multiple scales to systematically derive an 'effective-column' theory from the Kirchhoff rod equations: the helical filament is described as a naturally-straight rod (aligned with the helix axis) for which the extensional and torsional deformations are coupled.","Importantly, our analysis is asymptotically exact in the limit of a 'highly-coiled' filament (i.e., when the helical wavelength is much smaller than the characteristic lengthscale over which the filament bends due to external loading) and is able to account for large, unsteady displacements.","In the small-deformation limit, we exactly recover the coupled wave equations used to describe the free vibrations of helical coil springs, thereby justifying previous effective-column approximations in which linearised stiffness coefficients are assumed to apply locally and dynamically.","We then illustrate our theory with two loading scenarios: (I) a heavy helical rod deforming under its own weight; and (II) the dynamics of axial rotation (twirling) in viscous fluid, which may be considered as a simple model for a bacteria flagellar filament.","More broadly, our analysis provides a framework to develop reduced models of helical rods in a wide variety of physical and biological settings, and yields analytical insight into their elastic instabilities.","In particular, our analysis indicates that tensile instabilities are a generic phenomenon when helical rods are subject to a combination of distributed forces and moments."],"url":"http://arxiv.org/abs/2403.02299v1","category":"cond-mat.soft"}
{"created":"2024-03-04 18:18:52","title":"Physics-Informed Neural Networks with Skip Connections for Modeling and Control of Gas-Lifted Oil Wells","abstract":"Neural networks, while powerful, often lack interpretability. Physics-Informed Neural Networks (PINNs) address this limitation by incorporating physics laws into the loss function, making them applicable to solving Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs). The recently introduced PINC framework extends PINNs to control applications, allowing for open-ended long-range prediction and control of dynamic systems. In this work, we enhance PINC for modeling highly nonlinear systems such as gas-lifted oil wells. By introducing skip connections in the PINC network and refining certain terms in the ODE, we achieve more accurate gradients during training, resulting in an effective modeling process for the oil well system. Our proposed improved PINC demonstrates superior performance, reducing the validation prediction error by an average of 67% in the oil well application and significantly enhancing gradient flow through the network layers, increasing its magnitude by four orders of magnitude compared to the original PINC. Furthermore, experiments showcase the efficacy of Model Predictive Control (MPC) in regulating the bottom-hole pressure of the oil well using the improved PINC model, even in the presence of noisy measurements.","sentences":["Neural networks, while powerful, often lack interpretability.","Physics-Informed Neural Networks (PINNs) address this limitation by incorporating physics laws into the loss function, making them applicable to solving Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs).","The recently introduced PINC framework extends PINNs to control applications, allowing for open-ended long-range prediction and control of dynamic systems.","In this work, we enhance PINC for modeling highly nonlinear systems such as gas-lifted oil wells.","By introducing skip connections in the PINC network and refining certain terms in the ODE, we achieve more accurate gradients during training, resulting in an effective modeling process for the oil well system.","Our proposed improved PINC demonstrates superior performance, reducing the validation prediction error by an average of 67% in the oil well application and significantly enhancing gradient flow through the network layers, increasing its magnitude by four orders of magnitude compared to the original PINC.","Furthermore, experiments showcase the efficacy of Model Predictive Control (MPC) in regulating the bottom-hole pressure of the oil well using the improved PINC model, even in the presence of noisy measurements."],"url":"http://arxiv.org/abs/2403.02289v1","category":"cs.LG"}
{"created":"2024-03-04 18:14:19","title":"Graphical Quadratic Algebra","abstract":"We introduce Graphical Quadratic Algebra (GQA), a string diagrammatic calculus extending the language of Graphical Affine Algebra with a new generator characterised by invariance under rotation matrices. We show that GQA is a sound and complete axiomatisation for three different models: quadratic relations, which are a compositional formalism for least-squares problems, Gaussian stochastic processes, and Gaussian stochastic processes extended with non-determinisms. The equational theory of GQA sheds light on the connections between these perspectives, giving an algebraic interpretation to the interplay of stochastic behaviour, relational behaviour, non-determinism, and conditioning. As applications, we discuss various case studies, including linear regression, probabilistic programming, and electrical circuits with realistic (noisy) components.","sentences":["We introduce Graphical Quadratic Algebra (GQA), a string diagrammatic calculus extending the language of Graphical Affine Algebra with a new generator characterised by invariance under rotation matrices.","We show that GQA is a sound and complete axiomatisation for three different models: quadratic relations, which are a compositional formalism for least-squares problems, Gaussian stochastic processes, and Gaussian stochastic processes extended with non-determinisms.","The equational theory of GQA sheds light on the connections between these perspectives, giving an algebraic interpretation to the interplay of stochastic behaviour, relational behaviour, non-determinism, and conditioning.","As applications, we discuss various case studies, including linear regression, probabilistic programming, and electrical circuits with realistic (noisy) components."],"url":"http://arxiv.org/abs/2403.02284v1","category":"cs.LO"}
{"created":"2024-03-04 18:13:15","title":"Fractional Spins, Unfolding, and Holography: I. Parent field equations for dual higher-spin gravity reductions","abstract":"In this work and in the companion paper arXiv:2403.02301 we initiate an approach to holography based on the AKSZ formalism. As a first step, we refine Vasiliev's holography proposal in arXiv:1203.5554 by obtaining 4D higher-spin gravity (HSG) and 3D coloured conformal higher-spin gravity (CCHSG) -- i.e., coloured conformal matter fields coupled to conformal higher-spin gauge fields and colour gauge fields -- as two distinct and classically consistent reductions of a single parent theory. The latter consists, on-shell, of a flat superconnection valued in a fractional-spin extension of Vasiliev's higher-spin algebra. The 4D HSG and 3D CCHSG reductions are characterized by dual structure groups and two-form cohomology elements, and their embedding in a common parent model provides a rationale for deriving holographic relations from multi-dimensional AKSZ partition functions on cylinders with dual boundary conditions, to appear separately. In this work we i) construct the underlying non-commutative geometry as a metaplectic operator algebra represented in a Hermitian module of a pair of conformal particles; ii) identify a discrete modular group, arising from twisted boundary conditions of the first-quantized system, and connecting different boundary conditions of the second-quantized system; and iii) identify the holonomies, structure groups and two-form cohomology elements that characterize the HSG and CCHSG reductions, and equate the dual second Chern classes.","sentences":["In this work and in the companion paper arXiv:2403.02301 we initiate an approach to holography based on the AKSZ formalism.","As a first step, we refine Vasiliev's holography proposal in arXiv:1203.5554 by obtaining 4D higher-spin gravity (HSG) and 3D coloured conformal higher-spin gravity (CCHSG) -- i.e., coloured conformal matter fields coupled to conformal higher-spin gauge fields and colour gauge fields -- as two distinct and classically consistent reductions of a single parent theory.","The latter consists, on-shell, of a flat superconnection valued in a fractional-spin extension of Vasiliev's higher-spin algebra.","The 4D HSG and 3D CCHSG reductions are characterized by dual structure groups and two-form cohomology elements, and their embedding in a common parent model provides a rationale for deriving holographic relations from multi-dimensional AKSZ partition functions on cylinders with dual boundary conditions, to appear separately.","In this work we i) construct the underlying non-commutative geometry as a metaplectic operator algebra represented in a Hermitian module of a pair of conformal particles; ii) identify a discrete modular group, arising from twisted boundary conditions of the first-quantized system, and connecting different boundary conditions of the second-quantized system; and iii) identify the holonomies, structure groups and two-form cohomology elements that characterize the HSG and CCHSG reductions, and equate the dual second Chern classes."],"url":"http://arxiv.org/abs/2403.02283v2","category":"hep-th"}
{"created":"2024-03-04 18:12:10","title":"Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health","abstract":"We are united in how emotions are central to shaping our experiences; and yet, individuals differ greatly in how we each identify, categorize, and express emotions. In psychology, variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity (determined through self-reports of one's emotions). High emotion granularity has been linked with better mental and physical health; whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes. In this work, we propose computational measures of emotion granularity derived from temporally-ordered speaker utterances in social media (in lieu of self-reports that suffer from various biases). We then investigate the effectiveness of such text-derived measures of emotion granularity in functioning as markers of various mental health conditions (MHCs). We establish baseline measures of emotion granularity derived from textual utterances, and show that, at an aggregate level, emotion granularities are significantly lower for people self-reporting as having an MHC than for the control population. This paves the way towards a better understanding of the MHCs, and specifically the role emotions play in our well-being.","sentences":["We are united in how emotions are central to shaping our experiences; and yet, individuals differ greatly in how we each identify, categorize, and express emotions.","In psychology, variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity (determined through self-reports of one's emotions).","High emotion granularity has been linked with better mental and physical health; whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes.","In this work, we propose computational measures of emotion granularity derived from temporally-ordered speaker utterances in social media (in lieu of self-reports that suffer from various biases).","We then investigate the effectiveness of such text-derived measures of emotion granularity in functioning as markers of various mental health conditions (MHCs).","We establish baseline measures of emotion granularity derived from textual utterances, and show that, at an aggregate level, emotion granularities are significantly lower for people self-reporting as having an MHC than for the control population.","This paves the way towards a better understanding of the MHCs, and specifically the role emotions play in our well-being."],"url":"http://arxiv.org/abs/2403.02281v1","category":"cs.CL"}
{"created":"2024-03-04 18:01:18","title":"Let a Thousand Flowers Bloom: An Algebraic Representation for Edge Graphs","abstract":"Context: Edge graphs are graphs whose edges are labelled with identifiers, and nodes can have multiple edges between them. They are used to model a wide range of systems, including networks with distances or degrees of connection and complex relational data.   Inquiry: Unfortunately, the homogeneity of this graph structure prevents an effective representation in (functional) programs. Either their interface is riddled with partial functions, or the representations are computationally inefficient to process.   Approach: We present a novel data type for edge graphs, based on total and recursive definitions, that prevents usage errors from partial APIs and promotes structurally recursive computations. We follow an algebraic approach and provide a set of primitive constructors and combinators, along with equational laws that identify semantically equivalent constructions.   Knowledge: This algebra translates directly into an implementation using algebraic data types, and its homomorphisms give rise to functions for manipulating and transforming these edge graphs.   Grounding: We exploit the fact that many common graph algorithms are such homomorphisms to implement them in our framework.   Importance: In giving a theoretical grounding for the edge graph data type, we can formalise properties such as soundness and completeness of the representation while also minimising usage errors and maximising re-usability.","sentences":["Context: Edge graphs are graphs whose edges are labelled with identifiers, and nodes can have multiple edges between them.","They are used to model a wide range of systems, including networks with distances or degrees of connection and complex relational data.   ","Inquiry: Unfortunately, the homogeneity of this graph structure prevents an effective representation in (functional) programs.","Either their interface is riddled with partial functions, or the representations are computationally inefficient to process.   ","Approach: We present a novel data type for edge graphs, based on total and recursive definitions, that prevents usage errors from partial APIs and promotes structurally recursive computations.","We follow an algebraic approach and provide a set of primitive constructors and combinators, along with equational laws that identify semantically equivalent constructions.   ","Knowledge:","This algebra translates directly into an implementation using algebraic data types, and its homomorphisms give rise to functions for manipulating and transforming these edge graphs.   ","Grounding: We exploit the fact that many common graph algorithms are such homomorphisms to implement them in our framework.   ","Importance:","In giving a theoretical grounding for the edge graph data type, we can formalise properties such as soundness and completeness of the representation while also minimising usage errors and maximising re-usability."],"url":"http://arxiv.org/abs/2403.02273v1","category":"cs.PL"}
{"created":"2024-03-04 17:54:33","title":"DaReNeRF: Direction-aware Representation for Dynamic Scenes","abstract":"Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance.","sentences":["Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations.","However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions.","In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions.","This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information.","DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes.","Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes.","Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline.","Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance."],"url":"http://arxiv.org/abs/2403.02265v1","category":"cs.CV"}
{"created":"2024-03-04 17:50:48","title":"Dynamics of the collision of two nearly equal solitary waves for the Zakharov-Kuznetsov equation","abstract":"We study the dynamics of the collision of two solitary waves for the Zakharov-Kuznetsov equation in dimension $2$ and $3$. We describe the evolution of the solution behaving as a sum of $2$-solitary waves of nearly equal speeds at time $t=-\\infty$ up to time $t=+\\infty$. We show that this solution behaves as the sum of two modulated solitary waves and an error term which is small in $H^1$ for all time $t \\in \\mathbb R$. Finally, we also prove the stability of this solution for large times around the collision.   The proofs are a non-trivial extension of the ones of Martel and Merle for the quartic generalized Korteweg-de Vries equation to higher dimensions. First, despite the non-explicit nature of the solitary wave, we construct an approximate solution in an intrinsic way by canceling the error to the equation only in the natural directions of scaling and translation. Then, to control the difference between a solution and the approximate solution, we use a modified energy functional and a refined modulation estimate in the transverse variable. Moreover, we rely on the hamiltonian structure of the ODE governing the distance between the waves, which cannot be approximated by explicit solutions, to close the bootstrap estimates on the parameters. We hope that the techniques introduced here are robust and will prove useful in studying the collision phenomena for other focusing non-linear dispersive equations with non-explicit solitary waves.","sentences":["We study the dynamics of the collision of two solitary waves for the Zakharov-Kuznetsov equation in dimension $2$ and $3$. We describe the evolution of the solution behaving as a sum of $2$-solitary waves of nearly equal speeds at time $t=-\\infty$ up to time $t=+\\infty$. We show that this solution behaves as the sum of two modulated solitary waves and an error term which is small in $H^1$ for all time $t \\in \\mathbb R$.","Finally, we also prove the stability of this solution for large times around the collision.   ","The proofs are a non-trivial extension of the ones of Martel and Merle for the quartic generalized Korteweg-de Vries equation to higher dimensions.","First, despite the non-explicit nature of the solitary wave, we construct an approximate solution in an intrinsic way by canceling the error to the equation only in the natural directions of scaling and translation.","Then, to control the difference between a solution and the approximate solution, we use a modified energy functional and a refined modulation estimate in the transverse variable.","Moreover, we rely on the hamiltonian structure of the ODE governing the distance between the waves, which cannot be approximated by explicit solutions, to close the bootstrap estimates on the parameters.","We hope that the techniques introduced here are robust and will prove useful in studying the collision phenomena for other focusing non-linear dispersive equations with non-explicit solitary waves."],"url":"http://arxiv.org/abs/2403.02262v1","category":"math.AP"}
{"created":"2024-03-04 17:50:32","title":"Constraining Cosmological Parameters with Needlet Internal Linear Combination Maps I: Analytic Power Spectrum Formalism","abstract":"The internal linear combination (ILC) method is a popular approach for constructing component-separated maps in cosmic microwave background (CMB) analyses. It optimally combines observed maps at different frequencies to produce an unbiased minimum-variance map of a component. When performed in harmonic space, it is straightforward to analytically compute the contributions of individual sky components to the power spectrum of the resulting ILC map. ILC can also be performed on a basis of needlets, spherical wavelets that have compact support in both pixel and harmonic space, capturing both scale-dependent and spatially varying information. However, an analytic understanding of the power spectra of needlet ILC (NILC) component-separated maps, as needed to enable their use in cosmological parameter inference, has remained an outstanding problem. In this paper, we derive the first analytic expression for the power spectra of NILC maps, as well as an expression for the cross-spectrum of a NILC map with an arbitrary second map, in terms of contributions from individual sky components. We validate our result using simulations, finding that it is exact. These results contain useful insights: we explicitly see how NILC power spectra contain information from contaminant fields beyond the two-point level, and we obtain a formalism with which to parameterize NILC power spectra. However, because this parameter dependence is complicated by correlations and higher-point functions of the component maps and weight maps, we find that it is intractable to perform parameter inference using these analytic expressions. Instead, numerical techniques are needed to estimate parameters using NILC maps -- we explore the use of likelihood-free inference with neural posterior estimation in a companion paper. Our code to produce the results in this paper is available in https://github.com/kmsurrao/NILC-PS-Model.","sentences":["The internal linear combination (ILC) method is a popular approach for constructing component-separated maps in cosmic microwave background (CMB) analyses.","It optimally combines observed maps at different frequencies to produce an unbiased minimum-variance map of a component.","When performed in harmonic space, it is straightforward to analytically compute the contributions of individual sky components to the power spectrum of the resulting ILC map.","ILC can also be performed on a basis of needlets, spherical wavelets that have compact support in both pixel and harmonic space, capturing both scale-dependent and spatially varying information.","However, an analytic understanding of the power spectra of needlet ILC (NILC) component-separated maps, as needed to enable their use in cosmological parameter inference, has remained an outstanding problem.","In this paper, we derive the first analytic expression for the power spectra of NILC maps, as well as an expression for the cross-spectrum of a NILC map with an arbitrary second map, in terms of contributions from individual sky components.","We validate our result using simulations, finding that it is exact.","These results contain useful insights: we explicitly see how NILC power spectra contain information from contaminant fields beyond the two-point level, and we obtain a formalism with which to parameterize NILC power spectra.","However, because this parameter dependence is complicated by correlations and higher-point functions of the component maps and weight maps, we find that it is intractable to perform parameter inference using these analytic expressions.","Instead, numerical techniques are needed to estimate parameters using NILC maps -- we explore the use of likelihood-free inference with neural posterior estimation in a companion paper.","Our code to produce the results in this paper is available in https://github.com/kmsurrao/NILC-PS-Model."],"url":"http://arxiv.org/abs/2403.02261v1","category":"astro-ph.CO"}
{"created":"2024-03-04 17:43:19","title":"Conditionally strong solution for macroscopic polymeric SSS interaction","abstract":"The system under study is a solute-solvent-structure (SSS) interaction problem for the interaction of a dilute three-dimensional Oldroyd-B polymeric fluid with a two-dimensional viscoelastic shell. We show that a unique global strong solution to this system exists under the condition that the classical Ladyzhenskaya--Prodi--Serrin criterion holds for the velocity field and that the shell displacement is essentially bounded in time with values in the space of continuously differentiable functions. No requirement is needed for the polymer number density and the extra stress tensor for the solute component.","sentences":["The system under study is a solute-solvent-structure (SSS) interaction problem for the interaction of a dilute three-dimensional Oldroyd-B polymeric fluid with a two-dimensional viscoelastic shell.","We show that a unique global strong solution to this system exists under the condition that the classical Ladyzhenskaya--Prodi--Serrin criterion holds for the velocity field and that the shell displacement is essentially bounded in time with values in the space of continuously differentiable functions.","No requirement is needed for the polymer number density and the extra stress tensor for the solute component."],"url":"http://arxiv.org/abs/2403.02257v1","category":"math.AP"}
{"created":"2024-03-04 17:35:44","title":"Closed-form solutions for Bernoulli and compound Poisson branching processes in random environments","abstract":"For branching processes, the generating functions for limit distributions of so-called ratios of probabilities of rare events satisfy the Schr\\\"oder-type functional equations. Excepting limited special cases, the corresponding functional equations can not be solved analytically. I found a large class of Poisson-type offspring distributions, for which the Schr\\\"oder-type functional equations can be solved analytically. Moreover, for the asymptotics of limit distributions, the power and constant factor can be written explicitly. As a bonus, Bernoulli branching processes in random environments are treated. The beauty of this example is that the explicit formula for the generating function is unknown. Still, the closed-form expression for the power and constant factor in the asymptotic can be written with the help of some outstanding tricks.","sentences":["For branching processes, the generating functions for limit distributions of so-called ratios of probabilities of rare events satisfy the Schr\\\"oder-type functional equations.","Excepting limited special cases, the corresponding functional equations can not be solved analytically.","I found a large class of Poisson-type offspring distributions, for which the Schr\\\"oder-type functional equations can be solved analytically.","Moreover, for the asymptotics of limit distributions, the power and constant factor can be written explicitly.","As a bonus, Bernoulli branching processes in random environments are treated.","The beauty of this example is that the explicit formula for the generating function is unknown.","Still, the closed-form expression for the power and constant factor in the asymptotic can be written with the help of some outstanding tricks."],"url":"http://arxiv.org/abs/2403.02252v1","category":"math.PR"}
{"created":"2024-03-04 17:35:30","title":"A prediction rigidity formalism for low-cost uncertainties in trained neural networks","abstract":"Regression methods are fundamental for scientific and technological applications. However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications. Based on the solution of a constrained optimization problem, we propose \"prediction rigidities\" as a method to obtain uncertainties of arbitrary pre-trained regressors. We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.","sentences":["Regression methods are fundamental for scientific and technological applications.","However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications.","Based on the solution of a constrained optimization problem, we propose \"prediction rigidities\" as a method to obtain uncertainties of arbitrary pre-trained regressors.","We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks.","This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure.","We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology."],"url":"http://arxiv.org/abs/2403.02251v1","category":"stat.ML"}
{"created":"2024-03-04 17:16:06","title":"Sharp systolic inequalities for invariant tight contact forms on principal S1-bundles over S2","abstract":"The systole $\\text{sys}(\\alpha)$ of a contact form $\\alpha$ is defined as the shortest period of closed Reeb orbits of $\\alpha$. Given a non-trivial $\\mathbb S^1$-principal bundle over $\\mathbb S^2$ with total space $M$, we prove that the inequality $\\text{sys}(\\alpha)^2 \\leq \\text{vol}(M, \\alpha \\wedge \\mathrm{d}\\alpha)$ holds true for any tight contact form $\\alpha$ invariant under the corresponding circle action. Moreover, equality holds exactly for Zoll contact forms, namely, contact forms whose Reeb orbits are all closed and have the same minimal period. This implies a sharp systolic inequality for rotationally symmetric Finsler metrics on $\\mathbb S^2$, and a particular case of a conjecture by Viterbo.","sentences":["The systole $\\text{sys}(\\alpha)$ of a contact form $\\alpha$ is defined as the shortest period of closed Reeb orbits of $\\alpha$. Given a non-trivial $\\mathbb S^1$-principal bundle over $\\mathbb S^2$ with total space $M$, we prove that the inequality $\\text{sys}(\\alpha)^2 \\leq \\text{vol}(M, \\alpha \\wedge \\mathrm{d}\\alpha)$ holds true for any tight contact form $\\alpha$ invariant under the corresponding circle action.","Moreover, equality holds exactly for Zoll contact forms, namely, contact forms whose Reeb orbits are all closed and have the same minimal period.","This implies a sharp systolic inequality for rotationally symmetric Finsler metrics on $\\mathbb S^2$, and a particular case of a conjecture by Viterbo."],"url":"http://arxiv.org/abs/2403.02228v1","category":"math.SG"}
{"created":"2024-03-04 17:13:29","title":"Understanding Latent Timescales in Neural Ordinary Differential Equation Models for Advection-Dominated Dynamical Systems","abstract":"The neural ordinary differential equation (ODE) framework has shown promise in developing accelerated surrogate models for complex systems described by partial differential equations (PDEs). In PDE-based systems, neural ODE strategies use a two-step approach for acceleration: a nonlinear dimensionality reduction via an autoencoder, and a time integration step through a neural-network based model (neural ODE). This study explores the effectiveness of autoencoder-based neural ODE strategies for advection-dominated PDEs. It includes predictive demonstrations and delves into the sources of model acceleration, focusing on how neural ODEs achieve this. The research quantifies the impact of autoencoder and neural ODE components on system time-scales through eigenvalue analysis of dynamical system Jacobians. It examines how various training parameters, like training methods, latent space dimensionality, and training trajectory length, affect model accuracy and latent time-scales. Particularly, it highlights the influence of training trajectory length on neural ODE time-scales, noting that longer trajectories enhance limiting time-scales, with optimal neural ODEs capturing the largest time-scales of the actual system. The study conducts demonstrations on two distinct unsteady fluid dynamics settings influenced by advection: the Kuramoto-Sivashinsky equations and Hydrogen-Air channel detonations, using the compressible reacting Navier--Stokes equations.","sentences":["The neural ordinary differential equation (ODE) framework has shown promise in developing accelerated surrogate models for complex systems described by partial differential equations (PDEs).","In PDE-based systems, neural ODE strategies use a two-step approach for acceleration: a nonlinear dimensionality reduction via an autoencoder, and a time integration step through a neural-network based model (neural ODE).","This study explores the effectiveness of autoencoder-based neural ODE strategies for advection-dominated PDEs.","It includes predictive demonstrations and delves into the sources of model acceleration, focusing on how neural ODEs achieve this.","The research quantifies the impact of autoencoder and neural ODE components on system time-scales through eigenvalue analysis of dynamical system Jacobians.","It examines how various training parameters, like training methods, latent space dimensionality, and training trajectory length, affect model accuracy and latent time-scales.","Particularly, it highlights the influence of training trajectory length on neural ODE time-scales, noting that longer trajectories enhance limiting time-scales, with optimal neural ODEs capturing the largest time-scales of the actual system.","The study conducts demonstrations on two distinct unsteady fluid dynamics settings influenced by advection: the Kuramoto-Sivashinsky equations and Hydrogen-Air channel detonations, using the compressible reacting Navier--Stokes equations."],"url":"http://arxiv.org/abs/2403.02224v1","category":"physics.flu-dyn"}
{"created":"2024-03-04 17:12:33","title":"Neutron star cooling and mass distributions","abstract":"We study the cooling of isolated neutron stars, employing different nuclear equations of state with or without active direct Urca process, and investigate the interplay with the nuclear pairing gaps. We find that a consistent description of all current cooling data requires fast DU cooling and reasonable proton 1S0 gaps, but no neutron 3P2 pairing. We then deduce the neutron star mass distributions compatible with the cooling analysis and compare with current theoretical models. Reduced 1S0 gaps and unimodal mass distributions are preferred by the analysis.","sentences":["We study the cooling of isolated neutron stars, employing different nuclear equations of state with or without active direct Urca process, and investigate the interplay with the nuclear pairing gaps.","We find that a consistent description of all current cooling data requires fast DU cooling and reasonable proton 1S0 gaps, but no neutron 3P2 pairing.","We then deduce the neutron star mass distributions compatible with the cooling analysis and compare with current theoretical models.","Reduced 1S0 gaps and unimodal mass distributions are preferred by the analysis."],"url":"http://arxiv.org/abs/2403.02222v1","category":"astro-ph.HE"}
{"created":"2024-03-04 17:02:23","title":"Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming","abstract":"Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations. Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof of concept underscores the substantial potential of differentiable programming in synergistically combining machine learning with differential equations, thereby enhancing the capabilities of hybrid physics-ML modeling.","sentences":["Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations.","Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers.","In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming.","Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming.","This proof of concept underscores the substantial potential of differentiable programming in synergistically combining machine learning with differential equations, thereby enhancing the capabilities of hybrid physics-ML modeling."],"url":"http://arxiv.org/abs/2403.02215v1","category":"cs.LG"}
{"created":"2024-03-04 16:59:43","title":"Perceptive self-supervised learning network for noisy image watermark removal","abstract":"Popular methods usually use a degradation model in a supervised way to learn a watermark removal model. However, it is true that reference images are difficult to obtain in the real world, as well as collected images by cameras suffer from noise. To overcome these drawbacks, we propose a perceptive self-supervised learning network for noisy image watermark removal (PSLNet) in this paper. PSLNet depends on a parallel network to remove noise and watermarks. The upper network uses task decomposition ideas to remove noise and watermarks in sequence. The lower network utilizes the degradation model idea to simultaneously remove noise and watermarks. Specifically, mentioned paired watermark images are obtained in a self supervised way, and paired noisy images (i.e., noisy and reference images) are obtained in a supervised way. To enhance the clarity of obtained images, interacting two sub-networks and fusing obtained clean images are used to improve the effects of image watermark removal in terms of structural information and pixel enhancement. Taking into texture information account, a mixed loss uses obtained images and features to achieve a robust model of noisy image watermark removal. Comprehensive experiments show that our proposed method is very effective in comparison with popular convolutional neural networks (CNNs) for noisy image watermark removal. Codes can be obtained at https://github.com/hellloxiaotian/PSLNet.","sentences":["Popular methods usually use a degradation model in a supervised way to learn a watermark removal model.","However, it is true that reference images are difficult to obtain in the real world, as well as collected images by cameras suffer from noise.","To overcome these drawbacks, we propose a perceptive self-supervised learning network for noisy image watermark removal (PSLNet) in this paper.","PSLNet depends on a parallel network to remove noise and watermarks.","The upper network uses task decomposition ideas to remove noise and watermarks in sequence.","The lower network utilizes the degradation model idea to simultaneously remove noise and watermarks.","Specifically, mentioned paired watermark images are obtained in a self supervised way, and paired noisy images (i.e., noisy and reference images) are obtained in a supervised way.","To enhance the clarity of obtained images, interacting two sub-networks and fusing obtained clean images are used to improve the effects of image watermark removal in terms of structural information and pixel enhancement.","Taking into texture information account, a mixed loss uses obtained images and features to achieve a robust model of noisy image watermark removal.","Comprehensive experiments show that our proposed method is very effective in comparison with popular convolutional neural networks (CNNs) for noisy image watermark removal.","Codes can be obtained at https://github.com/hellloxiaotian/PSLNet."],"url":"http://arxiv.org/abs/2403.02211v1","category":"cs.CV"}
{"created":"2024-03-04 16:58:01","title":"On the blow-up scenario for some modified Serre-Green-Naghdi equations","abstract":"The present paper deals with a modified Serre-Green-Naghdi (mSGN) system that has been introduced by Clamond et al. to improve the dispersion relation. We present a precise blow-up scenario of the mSGN equations and we prove the existence of a class of solutions that develop singularities in finite time. All the presented results hold also for the Serre-Green-Naghdi system with weak surface tension.","sentences":["The present paper deals with a modified Serre-Green-Naghdi (mSGN) system that has been introduced by Clamond et al. to improve the dispersion relation.","We present a precise blow-up scenario of the mSGN equations and we prove the existence of a class of solutions that develop singularities in finite time.","All the presented results hold also for the Serre-Green-Naghdi system with weak surface tension."],"url":"http://arxiv.org/abs/2403.02208v1","category":"math.AP"}
{"created":"2024-03-04 16:35:03","title":"Exponentially-improved asymptotics for $q$-difference equations: ${}_2\u03c6_0$ and $q{\\rm P}_{\\rm I}$","abstract":"Usually when solving differential or difference equations via series solutions one encounters divergent series in which the coefficients grow like a factorial. Surprisingly, in the $q$-world the $n$th coefficient is often of the size $q^{-\\frac12 n(n-1)}$, in which $q\\in(0,1)$ is fixed. Hence, the divergence is much stronger, and one has to introduce alternative Borel and Laplace transforms to make sense of these formal series. We will discuss exponentially-improved asymptotics for the basic hypergeometric function ${}_2\\phi_0$ and for solutions of the $q$-difference first Painlev\\'e equation $q{\\rm P}_{\\rm I}$. These are optimal truncated expansions, and re-expansions in terms of new $q$-hyperterminant functions. The re-expansions do incorporate the Stokes phenomena.","sentences":["Usually when solving differential or difference equations via series solutions one encounters divergent series in which the coefficients grow like a factorial.","Surprisingly, in the $q$-world the $n$th coefficient is often of the size $q^{-\\frac12 n(n-1)}$, in which $q\\in(0,1)$ is fixed.","Hence, the divergence is much stronger, and one has to introduce alternative Borel and Laplace transforms to make sense of these formal series.","We will discuss exponentially-improved asymptotics for the basic hypergeometric function ${}_2\\phi_0$ and for solutions of the $q$-difference first Painlev\\'e equation $q{\\rm P}_{\\rm I}$.","These are optimal truncated expansions, and re-expansions in terms of new $q$-hyperterminant functions.","The re-expansions do incorporate the Stokes phenomena."],"url":"http://arxiv.org/abs/2403.02196v1","category":"math.CA"}
{"created":"2024-03-04 16:25:57","title":"Classical dynamical $r$-matrices for the Chern-Simons formulation of generalised 3d gravity","abstract":"Classical dynamical $r$-matrices arise naturally in the combinatorial description of the phase space of Chern-Simons theories, either through the inclusion of dynamical sources or through a gauge-fixing procedure involving two punctures. Here we consider classical dynamical $r$-matrices for the family of Lie algebras which arise in the Chern-Simons formulation of 3d gravity, for any value of the cosmological constant. We derive differential equations for classical dynamical $r$-matrices in this case, and show that they can be viewed as generalised complexifications, in a sense which we define, of the equations governing dynamical $r$-matrices for $\\mathfrak{su}(2)$ and $\\mathfrak{sl}(2,\\mathbb{R})$. We obtain explicit families of solutions and relate them, via Weierstrass factorisation, to solutions found by Feher, Gabor, Marshall, Palla and Pusztai in the context of chiral WZWN models.","sentences":["Classical dynamical $r$-matrices arise naturally in the combinatorial description of the phase space of Chern-Simons theories, either through the inclusion of dynamical sources or through a gauge-fixing procedure involving two punctures.","Here we consider classical dynamical $r$-matrices for the family of Lie algebras which arise in the Chern-Simons formulation of 3d gravity, for any value of the cosmological constant.","We derive differential equations for classical dynamical $r$-matrices in this case, and show that they can be viewed as generalised complexifications, in a sense which we define, of the equations governing dynamical $r$-matrices for $\\mathfrak{su}(2)$ and $\\mathfrak{sl}(2,\\mathbb{R})$. We obtain explicit families of solutions and relate them, via Weierstrass factorisation, to solutions found by Feher, Gabor, Marshall, Palla and Pusztai in the context of chiral WZWN models."],"url":"http://arxiv.org/abs/2403.02184v1","category":"hep-th"}
{"created":"2024-03-04 16:24:41","title":"Ice-Tide: Implicit Cryo-ET Imaging and Deformation Estimation","abstract":"We introduce ICE-TIDE, a method for cryogenic electron tomography (cryo-ET) that simultaneously aligns observations and reconstructs a high-resolution volume. The alignment of tilt series in cryo-ET is a major problem limiting the resolution of reconstructions. ICE-TIDE relies on an efficient coordinate-based implicit neural representation of the volume which enables it to directly parameterize deformations and align the projections. Furthermore, the implicit network acts as an effective regularizer, allowing for high-quality reconstruction at low signal-to-noise ratios as well as partially restoring the missing wedge information. We compare the performance of ICE-TIDE to existing approaches on realistic simulated volumes where the significant gains in resolution and accuracy of recovering deformations can be precisely evaluated. Finally, we demonstrate ICE-TIDE's ability to perform on experimental data sets.","sentences":["We introduce ICE-TIDE, a method for cryogenic electron tomography (cryo-ET) that simultaneously aligns observations and reconstructs a high-resolution volume.","The alignment of tilt series in cryo-ET is a major problem limiting the resolution of reconstructions.","ICE-TIDE relies on an efficient coordinate-based implicit neural representation of the volume which enables it to directly parameterize deformations and align the projections.","Furthermore, the implicit network acts as an effective regularizer, allowing for high-quality reconstruction at low signal-to-noise ratios as well as partially restoring the missing wedge information.","We compare the performance of ICE-TIDE to existing approaches on realistic simulated volumes where the significant gains in resolution and accuracy of recovering deformations can be precisely evaluated.","Finally, we demonstrate ICE-TIDE's ability to perform on experimental data sets."],"url":"http://arxiv.org/abs/2403.02182v1","category":"eess.IV"}
{"created":"2024-03-04 16:20:49","title":"On Hilbert's 16th Problem","abstract":"We prove that to each real singularity $f: (\\mathbb{R}^{n}, 0) \\to (\\mathbb{R}^k, 0)$ with $k\\geq 2$ one can associate systems of differential equations $\\mathfrak{g}^{k}_f$ which are pushforwards in the category of $\\mathcal{D}$-modules over $\\mathbb{R}^{k}$ of the sheaf of real analytic functions on the total space of the Milnor fibration. We then use this to study Hilbert's 16th problem on polynomial dynamical systems in the plane.","sentences":["We prove that to each real singularity $f: (\\mathbb{R}^{n}, 0) \\to (\\mathbb{R}^k, 0)$ with $k\\geq 2$ one can associate systems of differential equations $\\mathfrak{g}^{k}_f$ which are pushforwards in the category of $\\mathcal{D}$-modules over $\\mathbb{R}^{k}$ of the sheaf of real analytic functions on the total space of the Milnor fibration.","We then use this to study Hilbert's 16th problem on polynomial dynamical systems in the plane."],"url":"http://arxiv.org/abs/2403.02174v1","category":"math.AG"}
{"created":"2024-03-04 16:05:23","title":"The SPHERE view of the Orion star-forming region","abstract":"We present SPHERE/IRDIS H-band data for a sample of 23 stars in the Orion Star forming region observed within the DESTINYS (Disk Evolution Study Through Imaging of Nearby Young Stars) program. We use polarization differential imaging in order to detect scattered light from circumstellar dust. From the scattered light observations we characterize the disk orientation, radius and contrast. We analyse the disks in context of the stellar parameters and the environment of the Orion star-forming region. We use ancillary X-shooter spectroscopic observations to characterize the central stars in the systems. We furthermore use a combination of new and archival ALMA mm-continuum observations to characterize the dust masses present in the circumstellar disks. Within our sample we detect extended circumstellar disks in 10 of 23 systems. Of these, three are exceptionally extended (V351 Ori, V599 Ori and V1012 Ori) and show scattered light asymmetries which may indicate perturbations by embedded planets or (in the case of V599 Ori) by an outer stellar companion. Our high resolution imaging observations are also sensitive to close (sub)stellar companions and we detect 9 such objects in our sample of which 5 were previously unknown. We find in particular a possible sub-stellar companion (either a very low mass star or a high mass brown dwarf) 137 au from the star RY Ori. We find a strong anti-correlation between disk detection and multiplicity, with only 2 of our 10 disk detections located in stellar multiple systems. We also find a correlation between scattered light contrast and the millimetre flux suggesting that disks that have a high dust content are typically bright in near-infrared scattered light. Conversely we do not find significant correlations between scattered light contrast of the disks and the stellar mass or age.","sentences":["We present SPHERE/IRDIS H-band data for a sample of 23 stars in the Orion Star forming region observed within the DESTINYS (Disk Evolution Study Through Imaging of Nearby Young Stars) program.","We use polarization differential imaging in order to detect scattered light from circumstellar dust.","From the scattered light observations we characterize the disk orientation, radius and contrast.","We analyse the disks in context of the stellar parameters and the environment of the Orion star-forming region.","We use ancillary X-shooter spectroscopic observations to characterize the central stars in the systems.","We furthermore use a combination of new and archival ALMA mm-continuum observations to characterize the dust masses present in the circumstellar disks.","Within our sample we detect extended circumstellar disks in 10 of 23 systems.","Of these, three are exceptionally extended (V351 Ori, V599 Ori and V1012 Ori) and show scattered light asymmetries which may indicate perturbations by embedded planets or (in the case of V599 Ori) by an outer stellar companion.","Our high resolution imaging observations are also sensitive to close (sub)stellar companions and we detect 9 such objects in our sample of which 5 were previously unknown.","We find in particular a possible sub-stellar companion (either a very low mass star or a high mass brown dwarf) 137 au from the star RY Ori.","We find a strong anti-correlation between disk detection and multiplicity, with only 2 of our 10 disk detections located in stellar multiple systems.","We also find a correlation between scattered light contrast and the millimetre flux suggesting that disks that have a high dust content are typically bright in near-infrared scattered light.","Conversely we do not find significant correlations between scattered light contrast of the disks and the stellar mass or age."],"url":"http://arxiv.org/abs/2403.02156v1","category":"astro-ph.SR"}
{"created":"2024-03-04 15:56:40","title":"Reinforcement Learning for Inverse Non-Cooperative Linear-Quadratic Output-feedback Differential Games","abstract":"In this paper, we address the inverse problem for linear-quadratic differential non-cooperative games with output-feedback. Given players' stabilizing feedback laws, the goal is to find cost function parameters that lead to a game for which the observed game dynamics are at a Nash equilibrium. Using the given feedback laws, we introduce a model-based algorithm that generates cost function parameters solving the above inverse problem. We introduce a correction procedure that at each iteration of the algorithm guarantees the existence of the feedback laws, which addresses a key challenge of output-feedback control designs. As an intermediate stage of the algorithm, we have developed a procedure for the initial stabilization of the multiple-input system with output-feedback information structure. We prove convergence and stability of the algorithm, and show the way to generate new games with necessary properties without requiring to run the complete algorithm repeatedly. Then the algorithm is extended to a model-free version that uses data samples generated by unknown dynamics and has the same converging and stabilizing properties as the model-based version. Finally, we show how the inverse problem can be solved in a distributed manner and provide possible extensions. Simulation results validate the effectiveness of the proposed algorithms.","sentences":["In this paper, we address the inverse problem for linear-quadratic differential non-cooperative games with output-feedback.","Given players' stabilizing feedback laws, the goal is to find cost function parameters that lead to a game for which the observed game dynamics are at a Nash equilibrium.","Using the given feedback laws, we introduce a model-based algorithm that generates cost function parameters solving the above inverse problem.","We introduce a correction procedure that at each iteration of the algorithm guarantees the existence of the feedback laws, which addresses a key challenge of output-feedback control designs.","As an intermediate stage of the algorithm, we have developed a procedure for the initial stabilization of the multiple-input system with output-feedback information structure.","We prove convergence and stability of the algorithm, and show the way to generate new games with necessary properties without requiring to run the complete algorithm repeatedly.","Then the algorithm is extended to a model-free version that uses data samples generated by unknown dynamics and has the same converging and stabilizing properties as the model-based version.","Finally, we show how the inverse problem can be solved in a distributed manner and provide possible extensions.","Simulation results validate the effectiveness of the proposed algorithms."],"url":"http://arxiv.org/abs/2403.02146v1","category":"math.OC"}
{"created":"2024-03-04 15:54:33","title":"'SSL?! What on earth is that?': Towards Designing Age-Inclusive Secure Smartphone Browsing","abstract":"Owing to the increase in 'certified' phishing websites, there is a steady increase in the number of phishing cases and general susceptibility to phishing. Trust mechanisms (e.g., HTTPS Lock Indicators, SSL Certificates) that help differentiate genuine and phishing websites should therefore be evaluated for their effectiveness in preventing vulnerable users from accessing phishing websites. In this article, we present a study involving 18 adults (male-6; female-12) and 12 older adults (male-4; female-8) to understand the usability of current trust mechanisms and preferred modalities in a conceptualized mechanism. In the first part of the study, using Chrome browser on Android, we asked the participants to browse a banking website and a government website for digital particulars. We asked them to identify which one of the two was a phishing website, rate the usability of both websites and provide qualitative feedback on the trust mechanisms. In the second part, we conceptualized an alternative trust mechanism, which allows seeking social, community and AI-based support to make website trust-related decisions. Herein, we asked the participants as to which modality (social, community or AI) they prefer to seek support from and why it is preferred. Using the current trust mechanisms, none of the participants were able to identify the phishing website. As the participants rated the current mechanisms poorly in terms of usability, they expressed various difficulties that largely did not differ between adults and older adults. In the conceptualized mechanism, we observed a notable difference in the preferred modalities, in that, older adults primarily preferred social support. In addition to these overall findings, specific observations suggest that future trust mechanisms should not only consider age-specific needs but also incorporate substantial improvement in terms of usability.","sentences":["Owing to the increase in 'certified' phishing websites, there is a steady increase in the number of phishing cases and general susceptibility to phishing.","Trust mechanisms (e.g., HTTPS Lock Indicators, SSL Certificates) that help differentiate genuine and phishing websites should therefore be evaluated for their effectiveness in preventing vulnerable users from accessing phishing websites.","In this article, we present a study involving 18 adults (male-6; female-12) and 12 older adults (male-4; female-8) to understand the usability of current trust mechanisms and preferred modalities in a conceptualized mechanism.","In the first part of the study, using Chrome browser on Android, we asked the participants to browse a banking website and a government website for digital particulars.","We asked them to identify which one of the two was a phishing website, rate the usability of both websites and provide qualitative feedback on the trust mechanisms.","In the second part, we conceptualized an alternative trust mechanism, which allows seeking social, community and AI-based support to make website trust-related decisions.","Herein, we asked the participants as to which modality (social, community or AI) they prefer to seek support from and why it is preferred.","Using the current trust mechanisms, none of the participants were able to identify the phishing website.","As the participants rated the current mechanisms poorly in terms of usability, they expressed various difficulties that largely did not differ between adults and older adults.","In the conceptualized mechanism, we observed a notable difference in the preferred modalities, in that, older adults primarily preferred social support.","In addition to these overall findings, specific observations suggest that future trust mechanisms should not only consider age-specific needs but also incorporate substantial improvement in terms of usability."],"url":"http://arxiv.org/abs/2403.02145v1","category":"cs.HC"}
{"created":"2024-03-04 15:54:00","title":"Stochastic dynamics of the resistively shunted superconducting tunnel junction system under the impact of thermal fluctuations","abstract":"In this work, a Josephson junction consisting of two superconducting layers sandwiching an insulating layer is explored, which is subject to the effects of thermal fluctuations. The precise expressions for the evolution of Josephson phase and the supercurrent through the junction are derived. A clockwise hysteresis cycle in the current-voltage characteristic curve is demonstrated mathematically. Additionally, the bifurcation of a planar limit cycle is established. The numerous stochastic thermodynamic properties of the resistively shunted superconducting tunnel junction system are described, considering the influence of three specific parameters: the conductance, the current bias and the noise intensity. Moreover, the probability density is characterized using the Fokker-Planck equation.","sentences":["In this work, a Josephson junction consisting of two superconducting layers sandwiching an insulating layer is explored, which is subject to the effects of thermal fluctuations.","The precise expressions for the evolution of Josephson phase and the supercurrent through the junction are derived.","A clockwise hysteresis cycle in the current-voltage characteristic curve is demonstrated mathematically.","Additionally, the bifurcation of a planar limit cycle is established.","The numerous stochastic thermodynamic properties of the resistively shunted superconducting tunnel junction system are described, considering the influence of three specific parameters: the conductance, the current bias and the noise intensity.","Moreover, the probability density is characterized using the Fokker-Planck equation."],"url":"http://arxiv.org/abs/2403.02143v1","category":"cond-mat.supr-con"}
{"created":"2024-03-05 18:43:45","title":"Finding Super-spreaders in Network Cascades","abstract":"Suppose that a cascade (e.g., an epidemic) spreads on an unknown graph, and only the infection times of vertices are observed. What can be learned about the graph from the infection times caused by multiple distinct cascades? Most of the literature on this topic focuses on the task of recovering the entire graph, which requires $\\Omega ( \\log n)$ cascades for an $n$-vertex bounded degree graph. Here we ask a different question: can the important parts of the graph be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large?   In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a graph. Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades. Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve. Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $\\log(n) / \\log \\log (n)$ cascades. Surprisingly, this matches (up to $\\log \\log n$ factors) the number of cascades needed to learn the \\emph{entire} graph if it is a tree.","sentences":["Suppose that a cascade (e.g., an epidemic) spreads on an unknown graph, and only the infection times of vertices are observed.","What can be learned about the graph from the infection times caused by multiple distinct cascades?","Most of the literature on this topic focuses on the task of recovering the entire graph, which requires $\\Omega ( \\log n)$ cascades for an $n$-vertex bounded degree graph.","Here we ask a different question: can the important parts of the graph be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large?   ","In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a graph.","Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades.","Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve.","Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $\\log(n) / \\log \\log (n)$ cascades.","Surprisingly, this matches (up to $\\log \\log n$ factors) the number of cascades needed to learn the \\emph{entire} graph if it is a tree."],"url":"http://arxiv.org/abs/2403.03205v1","category":"math.ST"}
{"created":"2024-03-05 18:19:02","title":"Shuffling Momentum Gradient Algorithm for Convex Optimization","abstract":"The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.","sentences":["The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets.","In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants.","However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings.","In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems.","We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs.","Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature."],"url":"http://arxiv.org/abs/2403.03180v1","category":"math.OC"}
{"created":"2024-03-05 16:23:37","title":"Discovering Melting Temperature Prediction Models of Inorganic Solids by Combining Supervised and Unsupervised Learning","abstract":"The melting temperature is important for materials design because of its relationship with thermal stability, synthesis, and processing conditions. Current empirical and computational melting point estimation techniques are limited in scope, computational feasibility, or interpretability. We report the development of a machine learning methodology for predicting melting temperatures of binary ionic solid materials. We evaluated different machine-learning models trained on a data set of the melting points of 476 non-metallic crystalline binary compounds, using materials embeddings constructed from elemental properties and density-functional theory calculations as model inputs. A direct supervised-learning approach yields a mean absolute error of around 180~K but suffers from low interpretability. We find that the fidelity of predictions can further be improved by introducing an additional unsupervised-learning step that first classifies the materials before the melting-point regression. Not only does this two-step model exhibit improved accuracy, but the approach also provides a level of interpretability with insights into feature importance and different types of melting that depend on the specific atomic bonding inside a material. Motivated by this finding, we used a symbolic learning approach to find interpretable physical models for the melting temperature, which recovered the best-performing features from both prior models and provided additional interpretability.","sentences":["The melting temperature is important for materials design because of its relationship with thermal stability, synthesis, and processing conditions.","Current empirical and computational melting point estimation techniques are limited in scope, computational feasibility, or interpretability.","We report the development of a machine learning methodology for predicting melting temperatures of binary ionic solid materials.","We evaluated different machine-learning models trained on a data set of the melting points of 476 non-metallic crystalline binary compounds, using materials embeddings constructed from elemental properties and density-functional theory calculations as model inputs.","A direct supervised-learning approach yields a mean absolute error of around 180~K but suffers from low interpretability.","We find that the fidelity of predictions can further be improved by introducing an additional unsupervised-learning step that first classifies the materials before the melting-point regression.","Not only does this two-step model exhibit improved accuracy, but the approach also provides a level of interpretability with insights into feature importance and different types of melting that depend on the specific atomic bonding inside a material.","Motivated by this finding, we used a symbolic learning approach to find interpretable physical models for the melting temperature, which recovered the best-performing features from both prior models and provided additional interpretability."],"url":"http://arxiv.org/abs/2403.03092v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-05 14:57:28","title":"Toward Improved Deep Learning-based Vulnerability Detection","abstract":"Deep learning (DL) has been a common thread across several recent techniques for vulnerability detection. The rise of large, publicly available datasets of vulnerabilities has fueled the learning process underpinning these techniques. While these datasets help the DL-based vulnerability detectors, they also constrain these detectors' predictive abilities. Vulnerabilities in these datasets have to be represented in a certain way, e.g., code lines, functions, or program slices within which the vulnerabilities exist. We refer to this representation as a base unit. The detectors learn how base units can be vulnerable and then predict whether other base units are vulnerable. We have hypothesized that this focus on individual base units harms the ability of the detectors to properly detect those vulnerabilities that span multiple base units (or MBU vulnerabilities). For vulnerabilities such as these, a correct detection occurs when all comprising base units are detected as vulnerable. Verifying how existing techniques perform in detecting all parts of a vulnerability is important to establish their effectiveness for other downstream tasks. To evaluate our hypothesis, we conducted a study focusing on three prominent DL-based detectors: ReVeal, DeepWukong, and LineVul. Our study shows that all three detectors contain MBU vulnerabilities in their respective datasets. Further, we observed significant accuracy drops when detecting these types of vulnerabilities. We present our study and a framework that can be used to help DL-based detectors toward the proper inclusion of MBU vulnerabilities.","sentences":["Deep learning (DL) has been a common thread across several recent techniques for vulnerability detection.","The rise of large, publicly available datasets of vulnerabilities has fueled the learning process underpinning these techniques.","While these datasets help the DL-based vulnerability detectors, they also constrain these detectors' predictive abilities.","Vulnerabilities in these datasets have to be represented in a certain way, e.g., code lines, functions, or program slices within which the vulnerabilities exist.","We refer to this representation as a base unit.","The detectors learn how base units can be vulnerable and then predict whether other base units are vulnerable.","We have hypothesized that this focus on individual base units harms the ability of the detectors to properly detect those vulnerabilities that span multiple base units (or MBU vulnerabilities).","For vulnerabilities such as these, a correct detection occurs when all comprising base units are detected as vulnerable.","Verifying how existing techniques perform in detecting all parts of a vulnerability is important to establish their effectiveness for other downstream tasks.","To evaluate our hypothesis, we conducted a study focusing on three prominent DL-based detectors: ReVeal, DeepWukong, and LineVul.","Our study shows that all three detectors contain MBU vulnerabilities in their respective datasets.","Further, we observed significant accuracy drops when detecting these types of vulnerabilities.","We present our study and a framework that can be used to help DL-based detectors toward the proper inclusion of MBU vulnerabilities."],"url":"http://arxiv.org/abs/2403.03024v1","category":"cs.SE"}
{"created":"2024-03-05 14:55:14","title":"CRISPR: Ensemble Model","abstract":"Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine. However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability. In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable. Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction. This approach allows us to learn from a wider range of data, which improves the generalizability of our model. We evaluated our method on a benchmark dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability. Our results suggest that our method can be used to design sgRNAs with high sensitivity and specificity, even for new genes or cells. This could have important implications for the clinical use of CRISPR, as it would allow researchers to design more effective and safer treatments for a variety of diseases.","sentences":["Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine.","However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs).","This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability.","In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable.","Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction.","This approach allows us to learn from a wider range of data, which improves the generalizability of our model.","We evaluated our method on a benchmark dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability.","Our results suggest that our method can be used to design sgRNAs with high sensitivity and specificity, even for new genes or cells.","This could have important implications for the clinical use of CRISPR, as it would allow researchers to design more effective and safer treatments for a variety of diseases."],"url":"http://arxiv.org/abs/2403.03018v1","category":"cs.LG"}
{"created":"2024-03-05 14:28:17","title":"Higgs couplings in SMEFT via Zh production at the HL-LHC","abstract":"We study the Higgs couplings present in the $Zh$ associated production mode at the Large Hadron Collider (LHC) in presence of both CP even and CP odd dimension 6 Standard Model Effective Theory (SMEFT) operators. The analysis is performed mainly in context of the HL-LHC (with $\\sqrt{s}=$14 TeV and luminosity 3000 $fb^{-1}$) setup using cut based as well as machine learning techniques. The analysis shows significant betterment in the signal significance by using the machine learning technique. We also do a $\\chi^2$ analysis, which reveals a significant change in the sensitivity of the coupling modifiers due to the presence of effective operators, in particular due to the four point $qqZh$ interaction. The presence of dimension six CP odd four point operators, which contributes at $\\mathcal{O} (\\Lambda^{-4})$ order due to lack of interference with the SM contributions, can only have sensitivity with smaller NP scale at the HL-LHC, after addressing the effective limit and constraints.","sentences":["We study the Higgs couplings present in the $Zh$ associated production mode at the Large Hadron Collider (LHC) in presence of both CP even and CP odd dimension 6 Standard Model Effective Theory (SMEFT) operators.","The analysis is performed mainly in context of the HL-LHC (with $\\sqrt{s}=$14 TeV and luminosity 3000 $fb^{-1}$) setup using cut based as well as machine learning techniques.","The analysis shows significant betterment in the signal significance by using the machine learning technique.","We also do a $\\chi^2$ analysis, which reveals a significant change in the sensitivity of the coupling modifiers due to the presence of effective operators, in particular due to the four point $qqZh$ interaction.","The presence of dimension six CP odd four point operators, which contributes at $\\mathcal{O} (\\Lambda^{-4})$ order due to lack of interference with the SM contributions, can only have sensitivity with smaller NP scale at the HL-LHC, after addressing the effective limit and constraints."],"url":"http://arxiv.org/abs/2403.03001v1","category":"hep-ph"}
{"created":"2024-03-05 12:35:55","title":"Cross-Domain Image Conversion by CycleDM","abstract":"The purpose of this paper is to enable the conversion between machine-printed character images (i.e., font images) and handwritten character images through machine learning. For this purpose, we propose a novel unpaired image-to-image domain conversion method, CycleDM, which incorporates the concept of CycleGAN into the diffusion model. Specifically, CycleDM has two internal conversion models that bridge the denoising processes of two image domains. These conversion models are efficiently trained without explicit correspondence between the domains. By applying machine-printed and handwritten character images to the two modalities, CycleDM realizes the conversion between them. Our experiments for evaluating the converted images quantitatively and qualitatively found that ours performs better than other comparable approaches.","sentences":["The purpose of this paper is to enable the conversion between machine-printed character images (i.e., font images) and handwritten character images through machine learning.","For this purpose, we propose a novel unpaired image-to-image domain conversion method, CycleDM, which incorporates the concept of CycleGAN into the diffusion model.","Specifically, CycleDM has two internal conversion models that bridge the denoising processes of two image domains.","These conversion models are efficiently trained without explicit correspondence between the domains.","By applying machine-printed and handwritten character images to the two modalities, CycleDM realizes the conversion between them.","Our experiments for evaluating the converted images quantitatively and qualitatively found that ours performs better than other comparable approaches."],"url":"http://arxiv.org/abs/2403.02919v1","category":"cs.CV"}
{"created":"2024-03-05 12:11:32","title":"Demonstrating Mutual Reinforcement Effect through Information Flow","abstract":"The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ information flow analysis to observe and substantiate the MRE theory. Our experiments on six MRE hybrid datasets revealed the presence of MRE in the model and its impact. Additionally, we conducted fine-tuning experiments, whose results were consistent with those of the information flow experiments. The convergence of findings from both experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in five out of six datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.","sentences":["The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks.","It posits that the performance of both classification levels can be mutually enhanced.","However, this mechanism has not been adequately demonstrated or explained in prior research.","To address this gap, we employ information flow analysis to observe and substantiate the MRE theory.","Our experiments on six MRE hybrid datasets revealed the presence of MRE in the model and its impact.","Additionally, we conducted fine-tuning experiments, whose results were consistent with those of the information flow experiments.","The convergence of findings from both experiments corroborates the existence of MRE.","Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels.","In our final experiment, the F1-score significantly surpassed the baseline in five out of six datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole."],"url":"http://arxiv.org/abs/2403.02902v1","category":"cs.CL"}
{"created":"2024-03-05 12:06:32","title":"Federated Learning Using Coupled Tensor Train Decomposition","abstract":"Coupled tensor decomposition (CTD) can extract joint features from multimodal data in various applications. It can be employed for federated learning networks with data confidentiality. Federated CTD achieves data privacy protection by sharing common features and keeping individual features. However, traditional CTD schemes based on canonical polyadic decomposition (CPD) may suffer from low computational efficiency and heavy communication costs. Inspired by the efficient tensor train decomposition, we propose a coupled tensor train (CTT) decomposition for federated learning. The distributed coupled multi-way data are decomposed into a series of tensor trains with shared factors. In this way, we can extract common features of coupled modes while maintaining the different features of uncoupled modes. Thus the privacy preservation of information across different network nodes can be ensured. The proposed CTT approach is instantiated for two fundamental network structures, namely master-slave and decentralized networks. Experimental results on synthetic and real datasets demonstrate the superiority of the proposed schemes over existing methods in terms of both computational efficiency and communication rounds. In a classification task, experimental results show that the CTT-based federated learning achieves almost the same accuracy performance as that of the centralized counterpart.","sentences":["Coupled tensor decomposition (CTD) can extract joint features from multimodal data in various applications.","It can be employed for federated learning networks with data confidentiality.","Federated CTD achieves data privacy protection by sharing common features and keeping individual features.","However, traditional CTD schemes based on canonical polyadic decomposition (CPD) may suffer from low computational efficiency and heavy communication costs.","Inspired by the efficient tensor train decomposition, we propose a coupled tensor train (CTT) decomposition for federated learning.","The distributed coupled multi-way data are decomposed into a series of tensor trains with shared factors.","In this way, we can extract common features of coupled modes while maintaining the different features of uncoupled modes.","Thus the privacy preservation of information across different network nodes can be ensured.","The proposed CTT approach is instantiated for two fundamental network structures, namely master-slave and decentralized networks.","Experimental results on synthetic and real datasets demonstrate the superiority of the proposed schemes over existing methods in terms of both computational efficiency and communication rounds.","In a classification task, experimental results show that the CTT-based federated learning achieves almost the same accuracy performance as that of the centralized counterpart."],"url":"http://arxiv.org/abs/2403.02898v1","category":"cs.DC"}
{"created":"2024-03-05 11:50:01","title":"In Search of Truth: An Interrogation Approach to Hallucination Detection","abstract":"Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying on external knowledge.","sentences":["Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons.","One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth.","In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios.","Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them.","Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying on external knowledge."],"url":"http://arxiv.org/abs/2403.02889v1","category":"cs.CL"}
{"created":"2024-03-05 09:48:59","title":"Impact of domain reduction techniques in polynomial optimization: A computational study","abstract":"Domain reduction techniques are at the core of any global optimization solver for NLP or MINLP problems. In this paper, we delve into several of these techniques and assess the impact they may have in the performance of an RLT-based algorithm for polynomial optimization problems. These techniques include i) the use of (nonlinear) conic relaxations for optimality-based bound tightening, ii) the use of Lagrangian dual information to enhance feasibility-based bound tightening, and iii) different strategies for branching point selection. We also explore how a solver equipped with these domain reduction enhancements can further improve its performance by using machine learning to better choose the best domain reduction approach to use on a given instance.","sentences":["Domain reduction techniques are at the core of any global optimization solver for NLP or MINLP problems.","In this paper, we delve into several of these techniques and assess the impact they may have in the performance of an RLT-based algorithm for polynomial optimization problems.","These techniques include i) the use of (nonlinear) conic relaxations for optimality-based bound tightening, ii) the use of Lagrangian dual information to enhance feasibility-based bound tightening, and iii) different strategies for branching point selection.","We also explore how a solver equipped with these domain reduction enhancements can further improve its performance by using machine learning to better choose the best domain reduction approach to use on a given instance."],"url":"http://arxiv.org/abs/2403.02823v1","category":"math.OC"}
{"created":"2024-03-05 09:00:58","title":"Prequestioning Enhances Undergraduate Students Learning in an Environmental Chemistry Course","abstract":"Prequestioning is an instructional strategy that involves taking practice tests on to-be-learned information followed by studying the correct answers. Despite promising results in laboratory studies, it has rarely been examined in authentic educational settings. The current study investigated the pedagogical benefits of prequestioning as a learning intervention in an undergraduate environmental chemistry course. In each of 10 lecture sessions, the course lecturer administered four prequestions targeting concepts that were to be taught in the very next lecture session, then presented the correct answers. On assessments occurring during the next lecture session, there was evidence of a prequestioning effect, that is, better performance on questions targeting prequestioned concepts versus non-prequestioned concepts, in most cases. That benefit of prequestioning, which was relatively large (across all lecture sessions, an overall effect size of Cohens d = 2.04, p < .001), highlights the utility of prequestioning as a promising approach for enhancing learning in undergraduate chemistry and similar courses.","sentences":["Prequestioning is an instructional strategy that involves taking practice tests on to-be-learned information followed by studying the correct answers.","Despite promising results in laboratory studies, it has rarely been examined in authentic educational settings.","The current study investigated the pedagogical benefits of prequestioning as a learning intervention in an undergraduate environmental chemistry course.","In each of 10 lecture sessions, the course lecturer administered four prequestions targeting concepts that were to be taught in the very next lecture session, then presented the correct answers.","On assessments occurring during the next lecture session, there was evidence of a prequestioning effect, that is, better performance on questions targeting prequestioned concepts versus non-prequestioned concepts, in most cases.","That benefit of prequestioning, which was relatively large (across all lecture sessions, an overall effect size of Cohens d = 2.04, p < .001), highlights the utility of prequestioning as a promising approach for enhancing learning in undergraduate chemistry and similar courses."],"url":"http://arxiv.org/abs/2403.02788v1","category":"physics.ed-ph"}
{"created":"2024-03-05 08:52:16","title":"Data Collaboration Analysis Over Matrix Manifolds","abstract":"The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of this data in developing robust ML models. Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC) framework emerges as an innovative approach, potentially resolving the 'data island' issue among institutions through non-iterative communication and robust privacy protections. However, in its current state, the NRI-DC framework faces model performance instability due to theoretical unsteadiness in creating collaboration functions. This study establishes a rigorous theoretical foundation for these collaboration functions and introduces new formulations through optimization problems on matrix manifolds and efficient solutions. Empirical analyses demonstrate that the proposed approach, particularly the formulation over orthogonal matrix manifolds, significantly enhances performance, maintaining consistency and efficiency without compromising communication efficiency or privacy protections.","sentences":["The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets.","Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios.","Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets.","However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure.","Various global legislative frameworks have been established to address these privacy issues.","While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies.","Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of this data in developing robust ML models.","Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC) framework emerges as an innovative approach, potentially resolving the 'data island' issue among institutions through non-iterative communication and robust privacy protections.","However, in its current state, the NRI-DC framework faces model performance instability due to theoretical unsteadiness in creating collaboration functions.","This study establishes a rigorous theoretical foundation for these collaboration functions and introduces new formulations through optimization problems on matrix manifolds and efficient solutions.","Empirical analyses demonstrate that the proposed approach, particularly the formulation over orthogonal matrix manifolds, significantly enhances performance, maintaining consistency and efficiency without compromising communication efficiency or privacy protections."],"url":"http://arxiv.org/abs/2403.02780v1","category":"cs.LG"}
{"created":"2024-03-05 08:35:09","title":"DeconfuseTrack:Dealing with Confusion for Multi-Object Tracking","abstract":"Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.","sentences":["Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT).","However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association.","To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA).","DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues.","Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections.","Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT.","Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers.","Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association."],"url":"http://arxiv.org/abs/2403.02767v1","category":"cs.CV"}
{"created":"2024-03-05 07:16:51","title":"DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation","abstract":"Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data. Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across two datasets.","sentences":["Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data.","Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain.","Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set.","To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition.","This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge.","Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across two datasets."],"url":"http://arxiv.org/abs/2403.02718v1","category":"cs.CL"}
{"created":"2024-03-05 07:15:07","title":"Pre-trained Model-based Actionable Warning Identification: A Feasibility Study","abstract":"Actionable Warning Identification (AWI) plays a pivotal role in improving the usability of static code analyzers. Currently, Machine Learning (ML)-based AWI approaches, which mainly learn an AWI classifier from labeled warnings, are notably common. However, these approaches still face the problem of restricted performance due to the direct reliance on a limited number of labeled warnings to develop a classifier. Very recently, Pre-Trained Models (PTMs), which have been trained through billions of text/code tokens and demonstrated substantial success applications on various code-related tasks, could potentially circumvent the above problem. Nevertheless, the performance of PTMs on AWI has not been systematically investigated, leaving a gap in understanding their pros and cons. In this paper, we are the first to explore the feasibility of applying various PTMs for AWI. By conducting the extensive evaluation on 10K+ SpotBugs warnings from 10 large-scale and open-source projects, we observe that all studied PTMs are consistently 9.85%~21.12% better than the state-of-the-art ML-based AWI approaches. Besides, we investigate the impact of three primary aspects (i.e., data preprocessing, model training, and model prediction) in the typical PTM-based AWI workflow. Further, we identify the reasons for current PTMs' underperformance on AWI. Based on our findings, we provide several practical guidelines to enhance PTM-based AWI in future work.","sentences":["Actionable Warning Identification (AWI) plays a pivotal role in improving the usability of static code analyzers.","Currently, Machine Learning (ML)-based AWI approaches, which mainly learn an AWI classifier from labeled warnings, are notably common.","However, these approaches still face the problem of restricted performance due to the direct reliance on a limited number of labeled warnings to develop a classifier.","Very recently, Pre-Trained Models (PTMs), which have been trained through billions of text/code tokens and demonstrated substantial success applications on various code-related tasks, could potentially circumvent the above problem.","Nevertheless, the performance of PTMs on AWI has not been systematically investigated, leaving a gap in understanding their pros and cons.","In this paper, we are the first to explore the feasibility of applying various PTMs for AWI.","By conducting the extensive evaluation on 10K+ SpotBugs warnings from 10 large-scale and open-source projects, we observe that all studied PTMs are consistently 9.85%~21.12% better than the state-of-the-art ML-based AWI approaches.","Besides, we investigate the impact of three primary aspects (i.e., data preprocessing, model training, and model prediction) in the typical PTM-based AWI workflow.","Further, we identify the reasons for current PTMs' underperformance on AWI.","Based on our findings, we provide several practical guidelines to enhance PTM-based AWI in future work."],"url":"http://arxiv.org/abs/2403.02716v1","category":"cs.SE"}
{"created":"2024-03-05 07:09:35","title":"Android in the Zoo: Chain-of-Action-Thought for GUI Agents","abstract":"Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B.","sentences":["Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API.","Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations.","To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action.","We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling.","To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations.","Experiments show that fine-tuning a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B."],"url":"http://arxiv.org/abs/2403.02713v1","category":"cs.CL"}
{"created":"2024-03-05 07:00:46","title":"RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches","abstract":"Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL). However, natural language can be ambiguous and images can be over-specified. In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning. Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects. We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions. We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches. We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop. Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present. Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings. For supplementary material and videos, please refer to our website: http://rt-sketch.github.io.","sentences":["Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL).","However, natural language can be ambiguous and images can be over-specified.","In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning.","Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects.","We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions.","We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches.","We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop.","Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present.","Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings.","For supplementary material and videos, please refer to our website: http://rt-sketch.github.io."],"url":"http://arxiv.org/abs/2403.02709v1","category":"cs.RO"}
{"created":"2024-03-05 06:21:45","title":"InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents","abstract":"Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.","sentences":["Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites).","However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users.","Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   ","In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks.","InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools.","We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data.","We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time.","Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4.","Our findings raise questions about the widespread deployment of LLM Agents.","Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent."],"url":"http://arxiv.org/abs/2403.02691v1","category":"cs.CL"}
{"created":"2024-03-05 06:17:59","title":"Deep Common Feature Mining for Efficient Video Semantic Segmentation","abstract":"Recent advancements in video semantic segmentation have made substantial progress by exploiting temporal correlations. Nevertheless, persistent challenges, including redundant computation and the reliability of the feature propagation process, underscore the need for further innovation. In response, we present Deep Common Feature Mining (DCFM), a novel approach strategically designed to address these challenges by leveraging the concept of feature sharing. DCFM explicitly decomposes features into two complementary components. The common representation extracted from a key-frame furnishes essential high-level information to neighboring non-key frames, allowing for direct re-utilization without feature propagation. Simultaneously, the independent feature, derived from each video frame, captures rapidly changing information, providing frame-specific clues crucial for segmentation. To achieve such decomposition, we employ a symmetric training strategy tailored for sparsely annotated data, empowering the backbone to learn a robust high-level representation enriched with common information. Additionally, we incorporate a self-supervised loss function to reinforce intra-class feature similarity and enhance temporal consistency. Experimental evaluations on the VSPW and Cityscapes datasets demonstrate the effectiveness of our method, showing a superior balance between accuracy and efficiency.","sentences":["Recent advancements in video semantic segmentation have made substantial progress by exploiting temporal correlations.","Nevertheless, persistent challenges, including redundant computation and the reliability of the feature propagation process, underscore the need for further innovation.","In response, we present Deep Common Feature Mining (DCFM), a novel approach strategically designed to address these challenges by leveraging the concept of feature sharing.","DCFM explicitly decomposes features into two complementary components.","The common representation extracted from a key-frame furnishes essential high-level information to neighboring non-key frames, allowing for direct re-utilization without feature propagation.","Simultaneously, the independent feature, derived from each video frame, captures rapidly changing information, providing frame-specific clues crucial for segmentation.","To achieve such decomposition, we employ a symmetric training strategy tailored for sparsely annotated data, empowering the backbone to learn a robust high-level representation enriched with common information.","Additionally, we incorporate a self-supervised loss function to reinforce intra-class feature similarity and enhance temporal consistency.","Experimental evaluations on the VSPW and Cityscapes datasets demonstrate the effectiveness of our method, showing a superior balance between accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.02689v1","category":"cs.CV"}
{"created":"2024-03-05 06:06:22","title":"Machine Learning Refinements to Metallicity-Dependent Isotopic Abundances","abstract":"The project aims to use machine learning algorithms to fit the free parameters of an isotopic scaling model to elemental observations. The processes considered are massive star nucleosynthesis, Type Ia SNe, the s-process, the r-process, and p-isotope production. The analysis on the successful fits seeks to minimize the reduced chi squared between the model and the data. Based upon the successful refinement of the isotopic parameterized scaling model, a table providing the 287 stable isotopic abundances as a function of metallicity, separated into astrophysical processes, is useful for identifying the chemical history of them. The table provides a complete averaged chemical history for the Galaxy, subject to the underlying model constraints.","sentences":["The project aims to use machine learning algorithms to fit the free parameters of an isotopic scaling model to elemental observations.","The processes considered are massive star nucleosynthesis, Type Ia SNe, the s-process, the r-process, and p-isotope production.","The analysis on the successful fits seeks to minimize the reduced chi squared between the model and the data.","Based upon the successful refinement of the isotopic parameterized scaling model, a table providing the 287 stable isotopic abundances as a function of metallicity, separated into astrophysical processes, is useful for identifying the chemical history of them.","The table provides a complete averaged chemical history for the Galaxy, subject to the underlying model constraints."],"url":"http://arxiv.org/abs/2403.02678v1","category":"astro-ph.IM"}
{"created":"2024-03-05 04:07:54","title":"False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy","abstract":"Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampling and propose a technique that applies the concept of curriculum learning to the sampling strategy that encompasses both false-positive and ground-truth sampling techniques. Our experiments demonstrate that models utilizing false-positive sampling show a reduction in false positives and exhibit improved object detection performance. On the KITTI and Waymo Open datasets, models with false-positive sampling surpass the baseline models by a large margin.","sentences":["Recent studies have focused on enhancing the performance of 3D object detection models.","Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data.","However, an inherent issue with ground-truth sampling is its tendency to increase false positives.","Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling.","False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions.","We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database.","Additionally, we analyze the principles behind the performance enhancement due to false-positive sampling and propose a technique that applies the concept of curriculum learning to the sampling strategy that encompasses both false-positive and ground-truth sampling techniques.","Our experiments demonstrate that models utilizing false-positive sampling show a reduction in false positives and exhibit improved object detection performance.","On the KITTI and Waymo Open datasets, models with false-positive sampling surpass the baseline models by a large margin."],"url":"http://arxiv.org/abs/2403.02639v1","category":"cs.CV"}
{"created":"2024-03-05 03:40:39","title":"FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling","abstract":"In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to decouple domain-exclusive and domain-shared user representations, which are trained by the local-global bi-directional transfer algorithm. In addition, a hypergraph contrastive learning (HCL) module is devised to enhance the learning of domain-shared user relationship information by perturbing the user hypergraph. Extensive experiments conducted on three real-world scenarios demonstrate that FedHCDR outperforms existing baselines significantly.","sentences":["In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance.","However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR).","Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR).","Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning.","In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling.","Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features.","The approach employs high-pass and low-pass hypergraph filters to decouple domain-exclusive and domain-shared user representations, which are trained by the local-global bi-directional transfer algorithm.","In addition, a hypergraph contrastive learning (HCL) module is devised to enhance the learning of domain-shared user relationship information by perturbing the user hypergraph.","Extensive experiments conducted on three real-world scenarios demonstrate that FedHCDR outperforms existing baselines significantly."],"url":"http://arxiv.org/abs/2403.02630v1","category":"cs.LG"}
{"created":"2024-03-05 03:34:11","title":"Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use","abstract":"From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points. Most importantly, our framework eliminates the need for crowd-sourced annotations. Moreover, our framework ultimately produces lightweight classification models that are deployable in cost-sensitive scenarios. Across 15 subjective concepts and across 2 public image classification datasets, our trained models outperform traditional Agile Modeling as well as state-of-the-art zero-shot classification models like ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.","sentences":["From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing.","Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training.","Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier.","Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions.","Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points.","Most importantly, our framework eliminates the need for crowd-sourced annotations.","Moreover, our framework ultimately produces lightweight classification models that are deployable in cost-sensitive scenarios.","Across 15 subjective concepts and across 2 public image classification datasets, our trained models outperform traditional Agile Modeling as well as state-of-the-art zero-shot classification models like ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X."],"url":"http://arxiv.org/abs/2403.02626v1","category":"cs.CV"}
{"created":"2024-03-05 03:17:38","title":"TinyGC-Net: An Extremely Tiny Network for Calibrating MEMS Gyroscopes","abstract":"As the errors of microelectromechanical system (MEMS) gyroscopes are complex and nonlinear, the current calibration methods, which rely on linear models or networks with numerous parameters, are inadequate for low-cost embedded computing platforms to achieve both precision and real-time performance. In this paper, we introduce a extremely tiny network (TGC-Net) that characterizes the measurement model of MEMS gyroscopes. The network has a small number of parameters and can be trained on a central processing unit (CPU) before being deployed on a microcontroller unit (MCU). The TGC-Net leverage the robust data processing capabilities of deep learning to derive a nonlinear measurement model from fragmented gyroscope data. Subsequently, this model is used to regress errors on the gyroscope data. Moreover, we analyze the relationship between the compact network and the traditional linear model for MEMS gyroscopes, and emphasize the significance of the adequate angular motion stimulation for train the network. The experimental results, based on public datasets and real-world scenarios, demonstrate the practicality and effectiveness of the proposed method. These findings suggest that this technique is a viable candidate for applications that require MEMS gyroscopes.","sentences":["As the errors of microelectromechanical system (MEMS) gyroscopes are complex and nonlinear, the current calibration methods, which rely on linear models or networks with numerous parameters, are inadequate for low-cost embedded computing platforms to achieve both precision and real-time performance.","In this paper, we introduce a extremely tiny network (TGC-Net) that characterizes the measurement model of MEMS gyroscopes.","The network has a small number of parameters and can be trained on a central processing unit (CPU) before being deployed on a microcontroller unit (MCU).","The TGC-Net leverage the robust data processing capabilities of deep learning to derive a nonlinear measurement model from fragmented gyroscope data.","Subsequently, this model is used to regress errors on the gyroscope data.","Moreover, we analyze the relationship between the compact network and the traditional linear model for MEMS gyroscopes, and emphasize the significance of the adequate angular motion stimulation for train the network.","The experimental results, based on public datasets and real-world scenarios, demonstrate the practicality and effectiveness of the proposed method.","These findings suggest that this technique is a viable candidate for applications that require MEMS gyroscopes."],"url":"http://arxiv.org/abs/2403.02618v1","category":"cs.RO"}
{"created":"2024-03-05 02:20:33","title":"Pooling Image Datasets With Multiple Covariate Shift and Imbalance","abstract":"Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effectiveness of this approach via extensive experiments on real datasets. Further, we discuss how this style of formulation offers a unified perspective on at least 5+ distinct problem settings, from self-supervised learning to matching problems in 3D reconstruction.","sentences":["Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes.","Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data).","Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models.","Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time.","In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed.","We show the effectiveness of this approach via extensive experiments on real datasets.","Further, we discuss how this style of formulation offers a unified perspective on at least 5+ distinct problem settings, from self-supervised learning to matching problems in 3D reconstruction."],"url":"http://arxiv.org/abs/2403.02598v1","category":"cs.LG"}
{"created":"2024-03-05 01:46:50","title":"Improving Event Definition Following For Zero-Shot Event Detection","abstract":"Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations. In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type. Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection.","sentences":["Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions.","These approaches yield sporadic successes, yet generally fall short of expectations.","In this work, we aim to improve zero-shot event detection by training models to better follow event definitions.","We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types.","To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies.","Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.","Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance.","Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection."],"url":"http://arxiv.org/abs/2403.02586v1","category":"cs.CL"}
{"created":"2024-03-05 18:59:50","title":"How much information can be extracted from galaxy clustering at the field level?","abstract":"We present optimal Bayesian field-level cosmological constraints from nonlinear tracers of the large-scale structure, specifically the amplitude $\\sigma_8$ of linear matter fluctuations inferred from rest-frame simulated dark matter halos in a comoving volume of $8\\,(h^{-1}\\mathrm{Gpc})^3$. Our constraint on $\\sigma_8$ is entirely due to nonlinear information, and obtained by explicitly sampling the initial conditions along with bias and noise parameters via a Lagrangian EFT-based forward model, LEFTfield. The comparison with a simulation-based inference analysis employing the power spectrum and bispectrum likewise using the LEFTfield forward model shows that, when including precisely the same modes of the same data up to $k_{\\mathrm{max}}= 0.10\\,h\\,\\mathrm{Mpc}^{-1}$ ($0.12\\,h\\,\\mathrm{Mpc}^{-1}$), the field-level approach yields a factor of 1.7 (2.6) improvement on the $\\sigma_8$ constraint, from 9.9% to 5.8% (9.4% to 3.6%). This study provides the first direct insights into cosmological information encoded in galaxy clustering beyond low-order $n$-point functions.","sentences":["We present optimal Bayesian field-level cosmological constraints from nonlinear tracers of the large-scale structure, specifically the amplitude $\\sigma_8$ of linear matter fluctuations inferred from rest-frame simulated dark matter halos in a comoving volume of $8\\,(h^{-1}\\mathrm{Gpc})^3$. Our constraint on $\\sigma_8$ is entirely due to nonlinear information, and obtained by explicitly sampling the initial conditions along with bias and noise parameters via a Lagrangian EFT-based forward model, LEFTfield.","The comparison with a simulation-based inference analysis employing the power spectrum and bispectrum likewise using the LEFTfield forward model shows that, when including precisely the same modes of the same data up to $k_{\\mathrm{max}}= 0.10\\,h\\,\\mathrm{Mpc}^{-1}$ ($0.12\\,h\\,\\mathrm{Mpc}^{-1}$), the field-level approach yields a factor of 1.7 (2.6) improvement on the $\\sigma_8$ constraint, from 9.9% to 5.8% (9.4% to 3.6%).","This study provides the first direct insights into cosmological information encoded in galaxy clustering beyond low-order $n$-point functions."],"url":"http://arxiv.org/abs/2403.03220v1","category":"astro-ph.CO"}
{"created":"2024-03-05 17:46:04","title":"Solving non-native combinatorial optimization problems using hybrid quantum-classical algorithms","abstract":"Combinatorial optimization is a challenging problem applicable in a wide range of fields from logistics to finance. Recently, quantum computing has been used to attempt to solve these problems using a range of algorithms, including parameterized quantum circuits, adiabatic protocols, and quantum annealing. These solutions typically have several challenges: 1) there is little to no performance gain over classical methods, 2) not all constraints and objectives may be efficiently encoded in the quantum ansatz, and 3) the solution domain of the objective function may not be the same as the bit strings of measurement outcomes. This work presents \"non-native hybrid algorithms\" (NNHA): a framework to overcome these challenges by integrating quantum and classical resources with a hybrid approach. By designing non-native quantum variational ansatzes that inherit some but not all problem structure, measurement outcomes from the quantum computer can act as a resource to be used by classical routines to indirectly compute optimal solutions, partially overcoming the challenges of contemporary quantum optimization approaches. These methods are demonstrated using a publicly available neutral-atom quantum computer on two simple problems of Max $k$-Cut and maximum independent set. We find improvements in solution quality when comparing the hybrid algorithm to its ``no quantum\" version, a demonstration of a \"comparative advantage\".","sentences":["Combinatorial optimization is a challenging problem applicable in a wide range of fields from logistics to finance.","Recently, quantum computing has been used to attempt to solve these problems using a range of algorithms, including parameterized quantum circuits, adiabatic protocols, and quantum annealing.","These solutions typically have several challenges: 1) there is little to no performance gain over classical methods, 2) not all constraints and objectives may be efficiently encoded in the quantum ansatz, and 3) the solution domain of the objective function may not be the same as the bit strings of measurement outcomes.","This work presents \"non-native hybrid algorithms\" (NNHA): a framework to overcome these challenges by integrating quantum and classical resources with a hybrid approach.","By designing non-native quantum variational ansatzes that inherit some but not all problem structure, measurement outcomes from the quantum computer can act as a resource to be used by classical routines to indirectly compute optimal solutions, partially overcoming the challenges of contemporary quantum optimization approaches.","These methods are demonstrated using a publicly available neutral-atom quantum computer on two simple problems of Max $k$-Cut and maximum independent set.","We find improvements in solution quality when comparing the hybrid algorithm to its ``no quantum\" version, a demonstration of a \"comparative advantage\"."],"url":"http://arxiv.org/abs/2403.03153v1","category":"quant-ph"}
{"created":"2024-03-05 17:23:52","title":"A novel methodological framework for the analysis of health trajectories and survival outcomes in heart failure patients","abstract":"Heart failure (HF) contributes to circa 200,000 annual hospitalizations in France. With the increasing age of HF patients, elucidating the specific causes of inpatient mortality became a public health problematic. We introduce a novel methodological framework designed to identify prevalent health trajectories and investigate their impact on death. The initial step involves applying sequential pattern mining to characterize patients' trajectories, followed by an unsupervised clustering algorithm based on a new metric for measuring the distance between hospitalization diagnoses. Finally, a survival analysis is conducted to assess survival outcomes. The application of this framework to HF patients from a representative sample of the French population demonstrates its methodological significance in enhancing the analysis of healthcare trajectories.","sentences":["Heart failure (HF) contributes to circa 200,000 annual hospitalizations in France.","With the increasing age of HF patients, elucidating the specific causes of inpatient mortality became a public health problematic.","We introduce a novel methodological framework designed to identify prevalent health trajectories and investigate their impact on death.","The initial step involves applying sequential pattern mining to characterize patients' trajectories, followed by an unsupervised clustering algorithm based on a new metric for measuring the distance between hospitalization diagnoses.","Finally, a survival analysis is conducted to assess survival outcomes.","The application of this framework to HF patients from a representative sample of the French population demonstrates its methodological significance in enhancing the analysis of healthcare trajectories."],"url":"http://arxiv.org/abs/2403.03138v1","category":"stat.ME"}
{"created":"2024-03-05 14:07:37","title":"Operational Nonclassicality in Quantum Communication Networks","abstract":"To quantify quantum advantage in communication networks, we apply an operational framework for witnessing quantum nonclassicality. Following previous approaches in the field, this framework first computes linear constraints on the input/output probabilities that arise in classical networks when the amount of communication is bounded. We then apply variational quantum algorithms to optimize these probabilities when quantum communication resources are introduced. Any violation of the classical constraints indicates that extra classical communication is needed to simulate the comparable quantum network, thereby demonstrating an explicit quantum advantage. We demonstrate nonclassicality in many basic networks such as entanglement-assisted point-to-point and multi-point channels. In all examples, we find that equipping classical or quantum channels with entanglement leads to nonclassicality, whereas networks having multiple senders do not require entanglement to achieve nonclassicality. Finally, we discuss how our approaches could be implemented on quantum networking hardware and used to automatically establish certain protocols.","sentences":["To quantify quantum advantage in communication networks, we apply an operational framework for witnessing quantum nonclassicality.","Following previous approaches in the field, this framework first computes linear constraints on the input/output probabilities that arise in classical networks when the amount of communication is bounded.","We then apply variational quantum algorithms to optimize these probabilities when quantum communication resources are introduced.","Any violation of the classical constraints indicates that extra classical communication is needed to simulate the comparable quantum network, thereby demonstrating an explicit quantum advantage.","We demonstrate nonclassicality in many basic networks such as entanglement-assisted point-to-point and multi-point channels.","In all examples, we find that equipping classical or quantum channels with entanglement leads to nonclassicality, whereas networks having multiple senders do not require entanglement to achieve nonclassicality.","Finally, we discuss how our approaches could be implemented on quantum networking hardware and used to automatically establish certain protocols."],"url":"http://arxiv.org/abs/2403.02988v1","category":"quant-ph"}
{"created":"2024-03-05 12:03:10","title":"A\u03b1-spectral radius and path-factor covered graphs","abstract":"Let $\\alpha\\in[0,1)$, and let $G$ be a connected graph of order $n$ with $n\\geq f(\\alpha)$, where $f(\\alpha)=14$ for $\\alpha\\in[0,\\frac{1}{2}]$, $f(\\alpha)=17$ for $\\alpha\\in(\\frac{1}{2},\\frac{2}{3}]$, $f(\\alpha)=20$ for $\\alpha\\in(\\frac{2}{3},\\frac{3}{4}]$ and $f(\\alpha)=\\frac{5}{1-\\alpha}+1$ for $\\alpha\\in(\\frac{3}{4},1)$. A path factor is a spanning subgraph $F$ of $G$ such that every component of $F$ is a path with at least two vertices. Let $k\\geq2$ be an integer. A $P_{\\geq k}$-factor means a path-factor with each component being a path of order at least $k$. A graph $G$ is called a $P_{\\geq k}$-factor covered graph if $G$ has a $P_{\\geq k}$-factor containing $e$ for any $e\\in E(G)$. Let $A_{\\alpha}(G)=\\alpha D(G)+(1-\\alpha)A(G)$, where $D(G)$ denotes the diagonal matrix of vertex degrees of $G$ and $A(G)$ denotes the adjacency matrix of $G$. The largest eigenvalue of $A_{\\alpha}(G)$ is called the $A_{\\alpha}$-spectral radius of $G$, which is denoted by $\\rho_{\\alpha}(G)$. In this paper, it is proved that $G$ is a $P_{\\geq2}$-factor covered graph if $\\rho_{\\alpha}(G)>\\eta(n)$, where $\\eta(n)$ is the largest root of $x^{3}-((\\alpha+1)n+\\alpha-4)x^{2}+(\\alpha n^{2}+(\\alpha^{2}-2\\alpha-1)n-2\\alpha+1)x-\\alpha^{2}n^{2}+(5\\alpha^{2}-3\\alpha+2)n-10\\alpha^{2}+15\\alpha-8=0$. Furthermore, we provide a graph to show that the bound on $A_{\\alpha}$-spectral radius is optimal.","sentences":["Let $\\alpha\\in[0,1)$, and let $G$ be a connected graph of order $n$ with $n\\geq f(\\alpha)$, where $f(\\alpha)=14$ for $\\alpha\\in[0,\\frac{1}{2}]$, $f(\\alpha)=17$ for $\\alpha\\in(\\frac{1}{2},\\frac{2}{3}]$, $f(\\alpha)=20$ for $\\alpha\\in(\\frac{2}{3},\\frac{3}{4}]$ and $f(\\alpha)=\\frac{5}{1-\\alpha}+1$ for $\\alpha\\in(\\frac{3}{4},1)$. A path factor is a spanning subgraph $F$ of $G$ such that every component of $F$ is a path with at least two vertices.","Let $k\\geq2$ be an integer.","A $P_{\\geq k}$-factor means a path-factor with each component being a path of order at least $k$. A graph $G$ is called a $P_{\\geq k}$-factor covered graph if $G$ has a $P_{\\geq k}$-factor containing $e$ for any $e\\in E(G)$. Let $A_{\\alpha}(G)=\\alpha D(G)+(1-\\alpha)A(G)$, where $D(G)$ denotes the diagonal matrix of vertex degrees of $G$ and $A(G)$ denotes the adjacency matrix of $G$. The largest eigenvalue of $A_{\\alpha}(G)$ is called the $A_{\\alpha}$-spectral radius of $G$, which is denoted by $\\rho_{\\alpha}(G)$. In this paper, it is proved that $G$ is a $P_{\\geq2}$-factor covered graph if $\\rho_{\\alpha}(G)>\\eta(n)$, where $\\eta(n)$ is the largest root of $x^{3}-((\\alpha+1)n+\\alpha-4)x^{2}+(\\alpha n^{2}+(\\alpha^{2}-2\\alpha-1)n-2\\alpha+1)x-\\alpha^{2}n^{2}+(5\\alpha^{2}-3\\alpha+2)n-10\\alpha^{2}+15\\alpha-8=0$.","Furthermore, we provide a graph to show that the bound on $A_{\\alpha}$-spectral radius is optimal."],"url":"http://arxiv.org/abs/2403.02896v1","category":"math.CO"}
{"created":"2024-03-05 10:01:18","title":"Targeted optimization in small-scale atomic structure calculations: application to Au I","abstract":"The lack of reliable atomic data can be a severe limitation in astrophysical modelling, in particular of events such as kilonovae that require information on all neutron-capture elements across a wide range of ionization stages. Notably, the presence of non-orthonormalities between electron orbitals representing configurations that are close in energy can introduce significant inaccuracies in computed energies and transition probabilities. Here, we propose an explicit targeted optimization method that can effectively circumvent this concern while retaining an orthonormal orbital basis set. We illustrate this method within the framework of small-scale atomic structure models of Au I, using the GRASP2018 multiconfigurational Dirac-Hartree-Fock atomic structure code. By comparing to conventional optimization schemes we show how a targeted optimization approach improves the energy level positioning and ordering. Targeted optimization also leads to better agreement with experimental data for the strongest E1 transitions. This illustrates how small-scale models can be significantly improved with minor computational costs if orbital non-orthonormalities are considered carefully. These results should prove useful to multi-element atomic structure calculations in, for example, astrophysical opacity applications involving neutron-capture elements.","sentences":["The lack of reliable atomic data can be a severe limitation in astrophysical modelling, in particular of events such as kilonovae that require information on all neutron-capture elements across a wide range of ionization stages.","Notably, the presence of non-orthonormalities between electron orbitals representing configurations that are close in energy can introduce significant inaccuracies in computed energies and transition probabilities.","Here, we propose an explicit targeted optimization method that can effectively circumvent this concern while retaining an orthonormal orbital basis set.","We illustrate this method within the framework of small-scale atomic structure models of Au I, using the GRASP2018 multiconfigurational Dirac-Hartree-Fock atomic structure code.","By comparing to conventional optimization schemes we show how a targeted optimization approach improves the energy level positioning and ordering.","Targeted optimization also leads to better agreement with experimental data for the strongest E1 transitions.","This illustrates how small-scale models can be significantly improved with minor computational costs if orbital non-orthonormalities are considered carefully.","These results should prove useful to multi-element atomic structure calculations in, for example, astrophysical opacity applications involving neutron-capture elements."],"url":"http://arxiv.org/abs/2403.02829v1","category":"physics.atom-ph"}
{"created":"2024-03-05 07:07:23","title":"Reducing computational effort in topology optimization considering the deformation in additive manufacturing","abstract":"Integrating topology optimization and additive manufacturing (AM) technology can facilitate innovative product development. However, laser powder bed fusion, which is the predominant method in metal AM, can lead to issues such as residual stress and deformation. Recently, topology optimization methods considering these stresses and deformations have been proposed; however, they suffer from challenges caused by an increased computational cost. In this study, we propose a method for reducing computational cost in topology optimization considering the deformation in AM. An inherent strain method-based analytical model is presented for simulating the residual stress and deformation in the AM process. Subsequently, a constraint condition to suppress the deformation is formulated, and a method to reduce the computational cost of the adjoint analysis in deriving sensitivity is proposed. The minimum mean compliance problem considering AM deformation and self-support constraints can then be incorporated into the level set-based topology optimization framework. Finally, numerical examples are presented for validating the effectiveness of the proposed topology optimization method.","sentences":["Integrating topology optimization and additive manufacturing (AM) technology can facilitate innovative product development.","However, laser powder bed fusion, which is the predominant method in metal AM, can lead to issues such as residual stress and deformation.","Recently, topology optimization methods considering these stresses and deformations have been proposed; however, they suffer from challenges caused by an increased computational cost.","In this study, we propose a method for reducing computational cost in topology optimization considering the deformation in AM.","An inherent strain method-based analytical model is presented for simulating the residual stress and deformation in the AM process.","Subsequently, a constraint condition to suppress the deformation is formulated, and a method to reduce the computational cost of the adjoint analysis in deriving sensitivity is proposed.","The minimum mean compliance problem considering AM deformation and self-support constraints can then be incorporated into the level set-based topology optimization framework.","Finally, numerical examples are presented for validating the effectiveness of the proposed topology optimization method."],"url":"http://arxiv.org/abs/2403.02711v1","category":"cs.CE"}
{"created":"2024-03-05 05:10:58","title":"A randomized lattice rule without component-by-component construction","abstract":"We study the multivariate integration problem for periodic functions from the weighted Korobov space in the randomized setting. We introduce a new randomized rank-1 lattice rule with a randomly chosen number of points, which avoids the need for component-by-component construction in the search for good generating vectors while still achieving nearly the optimal rate of the randomized error. Our idea is to exploit the fact that at least half of the possible generating vectors yield nearly the optimal rate of the worst-case error in the deterministic setting. By randomly choosing generating vectors $r$ times and comparing their corresponding worst-case errors, one can find one generating vector with a desired worst-case error bound with a very high probability, and the (small) failure probability can be controlled by increasing $r$ logarithmically as a function of the number of points. Numerical experiments are conducted to support our theoretical findings.","sentences":["We study the multivariate integration problem for periodic functions from the weighted Korobov space in the randomized setting.","We introduce a new randomized rank-1 lattice rule with a randomly chosen number of points, which avoids the need for component-by-component construction in the search for good generating vectors while still achieving nearly the optimal rate of the randomized error.","Our idea is to exploit the fact that at least half of the possible generating vectors yield nearly the optimal rate of the worst-case error in the deterministic setting.","By randomly choosing generating vectors $r$ times and comparing their corresponding worst-case errors, one can find one generating vector with a desired worst-case error bound with a very high probability, and the (small) failure probability can be controlled by increasing $r$ logarithmically as a function of the number of points.","Numerical experiments are conducted to support our theoretical findings."],"url":"http://arxiv.org/abs/2403.02660v1","category":"math.NA"}
{"created":"2024-03-05 03:57:54","title":"Greedy receiver for photon-efficient optical communication","abstract":"In optical communication the transmitter encodes information into a set of light states defined by the modulation format, selected to accommodate specific channel conditions and to remain sufficiently distinguishable at the output. Various receiver architectures have been designed to improve the demodulation performance, ultimately limited by quantum theory. In this work I introduce a new receiver based on a locally optimal greedy algorithm and apply it to pulse position modulation. The receiver reduces the error probabilities of previously proposed strategies in all signal strength regimes and achieves results comparable with those obtained by numerical optimization of the detection process. In contrast, however, it is conceptually simple and therefore can be scaled to arbitrarily high modulation orders for which numerical methods become intractable. In the photon-starved regime characteristic of deep space optical communication, the greedy receiver approaches the quantum-optimal Helstrom bound on state discrimination error probability. In the regime of few-photon pulses, the error reduction offered over the other methods grows up to an order of magnitude.","sentences":["In optical communication the transmitter encodes information into a set of light states defined by the modulation format, selected to accommodate specific channel conditions and to remain sufficiently distinguishable at the output.","Various receiver architectures have been designed to improve the demodulation performance, ultimately limited by quantum theory.","In this work I introduce a new receiver based on a locally optimal greedy algorithm and apply it to pulse position modulation.","The receiver reduces the error probabilities of previously proposed strategies in all signal strength regimes and achieves results comparable with those obtained by numerical optimization of the detection process.","In contrast, however, it is conceptually simple and therefore can be scaled to arbitrarily high modulation orders for which numerical methods become intractable.","In the photon-starved regime characteristic of deep space optical communication, the greedy receiver approaches the quantum-optimal Helstrom bound on state discrimination error probability.","In the regime of few-photon pulses, the error reduction offered over the other methods grows up to an order of magnitude."],"url":"http://arxiv.org/abs/2403.02634v1","category":"quant-ph"}
{"created":"2024-03-05 02:21:54","title":"Comparative analysis of diverse methodologies for portfolio optimization leveraging quantum annealing techniques","abstract":"Portfolio optimization (PO) is extensively employed in financial services to assist in achieving investment objectives. By providing an optimal asset allocation, PO effectively balances the risk and returns associated with investments. However, it is important to note that as the number of involved assets and constraints increases, the portfolio optimization problem can become increasingly difficult to solve, falling into the category of NP-hard problems. In such scenarios, classical algorithms, such as the Monte Carlo method, exhibit limitations in addressing this challenge when the number of stocks in the portfolio grows. Quantum annealing algorithm holds promise for solving complex portfolio optimization problems in the NISQ era. Many studies have demonstrated the advantages of various quantum annealing algorithm variations over the standard quantum annealing approach. In this work, we conduct a numerical investigation of randomly generated unconstrained single-period discrete mean-variance portfolio optimization instances. We explore the application of a variety of unconventional quantum annealing algorithms, employing both forward annealing and reverse annealing schedules. By comparing the time-to-solution(TTS) and success probabilities of diverse approaches, we show that certain methods exhibit advantages in enhancing the success probability when utilizing conventional forward annealing schedules. Furthermore, we find that the implementation of reverse annealing schedules can significantly improve the performance of select unconventional quantum annealing algorithms.","sentences":["Portfolio optimization (PO) is extensively employed in financial services to assist in achieving investment objectives.","By providing an optimal asset allocation, PO effectively balances the risk and returns associated with investments.","However, it is important to note that as the number of involved assets and constraints increases, the portfolio optimization problem can become increasingly difficult to solve, falling into the category of NP-hard problems.","In such scenarios, classical algorithms, such as the Monte Carlo method, exhibit limitations in addressing this challenge when the number of stocks in the portfolio grows.","Quantum annealing algorithm holds promise for solving complex portfolio optimization problems in the NISQ era.","Many studies have demonstrated the advantages of various quantum annealing algorithm variations over the standard quantum annealing approach.","In this work, we conduct a numerical investigation of randomly generated unconstrained single-period discrete mean-variance portfolio optimization instances.","We explore the application of a variety of unconventional quantum annealing algorithms, employing both forward annealing and reverse annealing schedules.","By comparing the time-to-solution(TTS) and success probabilities of diverse approaches, we show that certain methods exhibit advantages in enhancing the success probability when utilizing conventional forward annealing schedules.","Furthermore, we find that the implementation of reverse annealing schedules can significantly improve the performance of select unconventional quantum annealing algorithms."],"url":"http://arxiv.org/abs/2403.02599v1","category":"quant-ph"}
{"created":"2024-03-05 01:06:25","title":"Learning-augmented Online Minimization of Age of Information and Transmission Costs","abstract":"We consider a discrete-time system where a resource-constrained source (e.g., a small sensor) transmits its time-sensitive data to a destination over a time-varying wireless channel. Each transmission incurs a fixed transmission cost (e.g., energy cost), and no transmission results in a staleness cost represented by the Age-of-Information. The source must balance the tradeoff between transmission and staleness costs. To address this challenge, we develop a robust online algorithm to minimize the sum of transmission and staleness costs, ensuring a worst-case performance guarantee. While online algorithms are robust, they are usually overly conservative and may have a poor average performance in typical scenarios. In contrast, by leveraging historical data and prediction models, machine learning (ML) algorithms perform well in average cases. However, they typically lack worst-case performance guarantees. To achieve the best of both worlds, we design a learning-augmented online algorithm that exhibits two desired properties: (i) consistency: closely approximating the optimal offline algorithm when the ML prediction is accurate and trusted; (ii) robustness: ensuring worst-case performance guarantee even ML predictions are inaccurate. Finally, we perform extensive simulations to show that our online algorithm performs well empirically and that our learning-augmented algorithm achieves both consistency and robustness.","sentences":["We consider a discrete-time system where a resource-constrained source (e.g., a small sensor) transmits its time-sensitive data to a destination over a time-varying wireless channel.","Each transmission incurs a fixed transmission cost (e.g., energy cost), and no transmission results in a staleness cost represented by the Age-of-Information.","The source must balance the tradeoff between transmission and staleness costs.","To address this challenge, we develop a robust online algorithm to minimize the sum of transmission and staleness costs, ensuring a worst-case performance guarantee.","While online algorithms are robust, they are usually overly conservative and may have a poor average performance in typical scenarios.","In contrast, by leveraging historical data and prediction models, machine learning (ML) algorithms perform well in average cases.","However, they typically lack worst-case performance guarantees.","To achieve the best of both worlds, we design a learning-augmented online algorithm that exhibits two desired properties: (i) consistency: closely approximating the optimal offline algorithm when the ML prediction is accurate and trusted; (ii) robustness: ensuring worst-case performance guarantee even ML predictions are inaccurate.","Finally, we perform extensive simulations to show that our online algorithm performs well empirically and that our learning-augmented algorithm achieves both consistency and robustness."],"url":"http://arxiv.org/abs/2403.02573v1","category":"cs.LG"}
{"created":"2024-03-05 01:04:45","title":"Fill Probabilities in a Limit Order Book with State-Dependent Stochastic Order Flows","abstract":"This paper focuses on computing the fill probabilities for limit orders positioned at various price levels within the limit order book, which play a crucial role in optimizing executions. We adopt a generic stochastic model to capture the dynamics of the order book as a series of queueing systems. This generic model is state-dependent and also incorporates stylized factors. We subsequently derive semi-analytical expressions to compute the relevant probabilities within the context of state-dependent stochastic order flows. These probabilities cover various scenarios, including the probability of a change in the mid-price, the fill probabilities of orders posted at the best quotes, and those posted at a price level deeper than the best quotes in the book, before the opposite best quote moves. These expressions can be further generalized to accommodate orders posted even deeper in the order book, although the associated probabilities are typically very small in such cases. Lastly, we conduct extensive numerical experiments using real order book data from the foreign exchange spot market. Our findings suggest that the model is tractable and possesses the capability to effectively capture the dynamics of the limit order book. Moreover, the derived formulas and numerical methods demonstrate reasonably good accuracy in estimating the fill probabilities.","sentences":["This paper focuses on computing the fill probabilities for limit orders positioned at various price levels within the limit order book, which play a crucial role in optimizing executions.","We adopt a generic stochastic model to capture the dynamics of the order book as a series of queueing systems.","This generic model is state-dependent and also incorporates stylized factors.","We subsequently derive semi-analytical expressions to compute the relevant probabilities within the context of state-dependent stochastic order flows.","These probabilities cover various scenarios, including the probability of a change in the mid-price, the fill probabilities of orders posted at the best quotes, and those posted at a price level deeper than the best quotes in the book, before the opposite best quote moves.","These expressions can be further generalized to accommodate orders posted even deeper in the order book, although the associated probabilities are typically very small in such cases.","Lastly, we conduct extensive numerical experiments using real order book data from the foreign exchange spot market.","Our findings suggest that the model is tractable and possesses the capability to effectively capture the dynamics of the limit order book.","Moreover, the derived formulas and numerical methods demonstrate reasonably good accuracy in estimating the fill probabilities."],"url":"http://arxiv.org/abs/2403.02572v1","category":"q-fin.TR"}
{"created":"2024-03-04 23:30:50","title":"Towards Large-Scale AFQMC Calculations: Large Time Step Auxiliary-Field Quantum Monte Carlo","abstract":"We report modifications of the ph-AFQMC algorithm that allow the use of large time steps and reliable time step extrapolation. Our modified algorithm eliminates size-consistency errors present in the standard algorithm when large time steps are employed. We investigate various methods to approximate the exponential of the one-body operator within the AFQMC framework, distinctly demonstrating the superiority of Krylov methods over the conventional Taylor expansion. We assess various propagators within AFQMC and demonstrate that the Split-2 propagator is the optimal method, exhibiting the smallest time-step errors. For the HEAT set molecules, the time-step extrapolated energies deviate on average by only 0.19 kcal/mol from the accurate small time-step energies. For small water clusters, we obtain accurate complete basis-set binding energies using time-step extrapolation with a mean absolute error of 0.07 kcal/mol compared to CCSD(T). Using large time-step ph-AFQMC for the N$_2$ dimer, we show that accurate bond lengths can be obtained while reducing CPU time by an order of magnitude.","sentences":["We report modifications of the ph-AFQMC algorithm that allow the use of large time steps and reliable time step extrapolation.","Our modified algorithm eliminates size-consistency errors present in the standard algorithm when large time steps are employed.","We investigate various methods to approximate the exponential of the one-body operator within the AFQMC framework, distinctly demonstrating the superiority of Krylov methods over the conventional Taylor expansion.","We assess various propagators within AFQMC and demonstrate that the Split-2 propagator is the optimal method, exhibiting the smallest time-step errors.","For the HEAT set molecules, the time-step extrapolated energies deviate on average by only 0.19 kcal/mol from the accurate small time-step energies.","For small water clusters, we obtain accurate complete basis-set binding energies using time-step extrapolation with a mean absolute error of 0.07 kcal/mol compared to CCSD(T).","Using large time-step ph-AFQMC for the N$_2$ dimer, we show that accurate bond lengths can be obtained while reducing CPU time by an order of magnitude."],"url":"http://arxiv.org/abs/2403.02542v1","category":"physics.chem-ph"}
{"created":"2024-03-04 23:29:09","title":"Shell Effects in Quasi-fission in Reactions Forming 226Th Compound Nucleus","abstract":"Quasi-fission (QF) reactions occur in fully damped heavy-ion collisions without the formation of an equilibrated compound nucleus, leading to the formation of fragments with similar properties as in fission reactions. Similar shell effects are expected to affect fragment formation in both fission and QF. Our purpose is to investigate QF dynamics in different reactions forming the same compound nucleus and search for possible signatures of shell effects in fragment formation. 50Ca+176Yb and 96Zr+130Sn QF reactions are simulated with the time-dependent Hartree-Fock code Sky3D near the Coulomb barrier. Evolutions of the quadrupole (Q20) and octupole (Q30) moments are interpreted in terms of features of the potential energy surface (PES) of the 226Th compound nucleus. Both reactions encounter QF. In 50Ca+176Yb, those only occur at finite angular momenta. In the more symmetric 96Zr+130Sn reaction with stronger Coulomb repulsion in the entrance channel, QF also occurs in central collisions. In agreement with earlier predictions, 50Ca+176Yb encounters partial mass equilibration that is stopped when the heavy fragment reaches Z~54 protons, as in the asymmetric fission mode of 226Th. 96Zr+130Sn encounters an inverse QF also leading to similar fragments as in asymmetric fission. In both systems, QF trajectories in the Q20-Q30 plane follow the asymmetric fission valley of 226Th PES. The observation of an inverse QF is a clear prediction that shell effects have a strong influence in QF. The similarity between fragments formed in asymmetric fission and QF supports the idea that the same shell effects are at play in both mechanisms. Interpreting QF dynamics with PES used in fission is naturally limited by the fact that these PES are usually computed with axial symmetry, no angular momentum and no excitation energy, thus motivating future developments of PES for QF.","sentences":["Quasi-fission (QF) reactions occur in fully damped heavy-ion collisions without the formation of an equilibrated compound nucleus, leading to the formation of fragments with similar properties as in fission reactions.","Similar shell effects are expected to affect fragment formation in both fission and QF.","Our purpose is to investigate QF dynamics in different reactions forming the same compound nucleus and search for possible signatures of shell effects in fragment formation.","50Ca+176Yb and 96Zr+130Sn QF reactions are simulated with the time-dependent Hartree-Fock code Sky3D near the Coulomb barrier.","Evolutions of the quadrupole (Q20) and octupole (Q30) moments are interpreted in terms of features of the potential energy surface (PES) of the 226Th compound nucleus.","Both reactions encounter QF.","In 50Ca+176Yb, those only occur at finite angular momenta.","In the more symmetric 96Zr+130Sn reaction with stronger Coulomb repulsion in the entrance channel, QF also occurs in central collisions.","In agreement with earlier predictions, 50Ca+176Yb encounters partial mass equilibration that is stopped when the heavy fragment reaches Z~54 protons, as in the asymmetric fission mode of 226Th.","96Zr+130Sn encounters an inverse QF also leading to similar fragments as in asymmetric fission.","In both systems, QF trajectories in the Q20-Q30 plane follow the asymmetric fission valley of 226Th PES.","The observation of an inverse QF is a clear prediction that shell effects have a strong influence in QF.","The similarity between fragments formed in asymmetric fission and QF supports the idea that the same shell effects are at play in both mechanisms.","Interpreting QF dynamics with PES used in fission is naturally limited by the fact that these PES are usually computed with axial symmetry, no angular momentum and no excitation energy, thus motivating future developments of PES for QF."],"url":"http://arxiv.org/abs/2403.02541v1","category":"nucl-th"}
{"created":"2024-03-04 22:30:49","title":"An Analysis of Intent-Based Markets","abstract":"Mechanisms for decentralized finance on blockchains suffer from various problems, including suboptimal price execution for users, latency, and a worse user experience compared to their centralized counterparts. Recently, off-chain marketplaces, colloquially called `intent markets,' have been proposed as a solution to these problems. In these markets, agents called \\emph{solvers} compete to satisfy user orders, which may include complicated user-specified conditions. We provide two formal models of solvers' strategic behavior: one probabilistic and another deterministic. In our first model, solvers initially pay upfront costs to enter a Dutch auction to fill the user's order and then exert congestive, costly effort to search for prices for the user. Our results show that the costs incurred by solvers result in restricted entry in the market. Further, in the presence of costly effort and congestion, our results counter-intuitively show that a planner who aims to maximize user welfare may actually prefer to restrict entry, resulting in limited oligopoly. We then introduce an alternative, optimization-based deterministic model which corroborates these results. We conclude with extensions of our model to other auctions within blockchains and non-cryptocurrency applications, such as the US SEC's Proposal 615.","sentences":["Mechanisms for decentralized finance on blockchains suffer from various problems, including suboptimal price execution for users, latency, and a worse user experience compared to their centralized counterparts.","Recently, off-chain marketplaces, colloquially called `intent markets,' have been proposed as a solution to these problems.","In these markets, agents called \\emph{solvers} compete to satisfy user orders, which may include complicated user-specified conditions.","We provide two formal models of solvers' strategic behavior: one probabilistic and another deterministic.","In our first model, solvers initially pay upfront costs to enter a Dutch auction to fill the user's order and then exert congestive, costly effort to search for prices for the user.","Our results show that the costs incurred by solvers result in restricted entry in the market.","Further, in the presence of costly effort and congestion, our results counter-intuitively show that a planner who aims to maximize user welfare may actually prefer to restrict entry, resulting in limited oligopoly.","We then introduce an alternative, optimization-based deterministic model which corroborates these results.","We conclude with extensions of our model to other auctions within blockchains and non-cryptocurrency applications, such as the US SEC's Proposal 615."],"url":"http://arxiv.org/abs/2403.02525v1","category":"cs.GT"}
{"created":"2024-03-04 22:28:20","title":"Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces","abstract":"This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and capturing them outside the original function space. Through the theory of rigged Hilbert space, our study provides a principled methodology to analyze the estimated spectrum and eigenfunctions of Koopman operators, and enables eigendecomposition within a rigged RKHS. We also propose a new effective method for reconstructing the dynamical system from temporally-sampled trajectory data of the dynamical system with solid theoretical guarantee. We conduct several numerical simulations using the van der Pol oscillator, the Duffing oscillator, the H\\'enon map, and the Lorenz attractor, and illustrate the performance of JetDMD with clear numerical computations of eigenvalues and accurate predictions of the dynamical systems.","sentences":["This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra.","We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator.","This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues.","This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance.","We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space.","This notion leads to a deeper understanding of estimated Koopman eigenfunctions and capturing them outside the original function space.","Through the theory of rigged Hilbert space, our study provides a principled methodology to analyze the estimated spectrum and eigenfunctions of Koopman operators, and enables eigendecomposition within a rigged RKHS.","We also propose a new effective method for reconstructing the dynamical system from temporally-sampled trajectory data of the dynamical system with solid theoretical guarantee.","We conduct several numerical simulations using the van der Pol oscillator, the Duffing oscillator, the H\\'enon map, and the Lorenz attractor, and illustrate the performance of JetDMD with clear numerical computations of eigenvalues and accurate predictions of the dynamical systems."],"url":"http://arxiv.org/abs/2403.02524v1","category":"math.DS"}
{"created":"2024-03-04 21:48:32","title":"RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder for Stock Returns Prediction","abstract":"In recent years, the dynamic factor model has emerged as a dominant tool in economics and finance, particularly for investment strategies. This model offers improved handling of complex, nonlinear, and noisy market conditions compared to traditional static factor models. The advancement of machine learning, especially in dealing with nonlinear data, has further enhanced asset pricing methodologies. This paper introduces a groundbreaking dynamic factor model named RVRAE. This model is a probabilistic approach that addresses the temporal dependencies and noise in market data. RVRAE ingeniously combines the principles of dynamic factor modeling with the variational recurrent autoencoder (VRAE) from deep learning. A key feature of RVRAE is its use of a prior-posterior learning method. This method fine-tunes the model's learning process by seeking an optimal posterior factor model informed by future data. Notably, RVRAE is adept at risk modeling in volatile stock markets, estimating variances from latent space distributions while also predicting returns. Our empirical tests with real stock market data underscore RVRAE's superior performance compared to various established baseline methods.","sentences":["In recent years, the dynamic factor model has emerged as a dominant tool in economics and finance, particularly for investment strategies.","This model offers improved handling of complex, nonlinear, and noisy market conditions compared to traditional static factor models.","The advancement of machine learning, especially in dealing with nonlinear data, has further enhanced asset pricing methodologies.","This paper introduces a groundbreaking dynamic factor model named RVRAE.","This model is a probabilistic approach that addresses the temporal dependencies and noise in market data.","RVRAE ingeniously combines the principles of dynamic factor modeling with the variational recurrent autoencoder (VRAE) from deep learning.","A key feature of RVRAE is its use of a prior-posterior learning method.","This method fine-tunes the model's learning process by seeking an optimal posterior factor model informed by future data.","Notably, RVRAE is adept at risk modeling in volatile stock markets, estimating variances from latent space distributions while also predicting returns.","Our empirical tests with real stock market data underscore RVRAE's superior performance compared to various established baseline methods."],"url":"http://arxiv.org/abs/2403.02500v1","category":"q-fin.PM"}
{"created":"2024-03-04 21:22:05","title":"Ivie: Lightweight Anchored Explanations of Just-Generated Code","abstract":"Programming assistants have reshaped the experience of programming into one where programmers spend less time writing and more time critically examining code. In this paper, we explore how programming assistants can be extended to accelerate the inspection of generated code. We introduce an extension to the programming assistant called Ivie, or instantly visible in-situ explanations. When using Ivie, a programmer's generated code is instantly accompanied by explanations positioned just adjacent to the code. Our design was optimized for extremely low-cost invocation and dismissal. Explanations are compact and informative. They describe meaningful expressions, from individual variables to entire blocks of code. We present an implementation of Ivie that forks VS Code, applying a modern LLM for timely segmentation and explanation of generated code. In a lab study, we compared Ivie to a contemporary baseline tool for code understanding. Ivie improved understanding of generated code, and was received by programmers as a highly useful, low distraction, desirable complement to the programming assistant.","sentences":["Programming assistants have reshaped the experience of programming into one where programmers spend less time writing and more time critically examining code.","In this paper, we explore how programming assistants can be extended to accelerate the inspection of generated code.","We introduce an extension to the programming assistant called Ivie, or instantly visible in-situ explanations.","When using Ivie, a programmer's generated code is instantly accompanied by explanations positioned just adjacent to the code.","Our design was optimized for extremely low-cost invocation and dismissal.","Explanations are compact and informative.","They describe meaningful expressions, from individual variables to entire blocks of code.","We present an implementation of Ivie that forks VS Code, applying a modern LLM for timely segmentation and explanation of generated code.","In a lab study, we compared Ivie to a contemporary baseline tool for code understanding.","Ivie improved understanding of generated code, and was received by programmers as a highly useful, low distraction, desirable complement to the programming assistant."],"url":"http://arxiv.org/abs/2403.02491v1","category":"cs.HC"}
{"created":"2024-03-04 20:39:21","title":"The Emotion Dynamics of Literary Novels","abstract":"Stories are rich in the emotions they exhibit in their narratives and evoke in the readers. The emotional journeys of the various characters within a story are central to their appeal. Computational analysis of the emotions of novels, however, has rarely examined the variation in the emotional trajectories of the different characters within them, instead considering the entire novel to represent a single story arc. In this work, we use character dialogue to distinguish between the emotion arcs of the narration and the various characters. We analyze the emotion arcs of the various characters in a dataset of English literary novels using the framework of Utterance Emotion Dynamics. Our findings show that the narration and the dialogue largely express disparate emotions through the course of a novel, and that the commonalities or differences in the emotional arcs of stories are more accurately captured by those associated with individual characters.","sentences":["Stories are rich in the emotions they exhibit in their narratives and evoke in the readers.","The emotional journeys of the various characters within a story are central to their appeal.","Computational analysis of the emotions of novels, however, has rarely examined the variation in the emotional trajectories of the different characters within them, instead considering the entire novel to represent a single story arc.","In this work, we use character dialogue to distinguish between the emotion arcs of the narration and the various characters.","We analyze the emotion arcs of the various characters in a dataset of English literary novels using the framework of Utterance Emotion Dynamics.","Our findings show that the narration and the dialogue largely express disparate emotions through the course of a novel, and that the commonalities or differences in the emotional arcs of stories are more accurately captured by those associated with individual characters."],"url":"http://arxiv.org/abs/2403.02474v1","category":"cs.CL"}
{"created":"2024-03-04 20:06:17","title":"Exposure-Conscious Path Planning for Equal-Exposure Corridors","abstract":"While maximizing line-of-sight coverage of specific regions or agents in the environment is a well explored path planning objective, the converse problem of minimizing exposure to the entire environment during navigation is especially interesting in the context of minimizing detection risk. This work demonstrates that minimizing line-of-sight exposure to the environment is non-Markovian, which cannot be efficiently solved optimally with traditional path planning. The optimality gap of the graph-search algorithm A* and the trade-offs in optimality vs. computation time of several approximating heuristics is explored. Finally, the concept of equal-exposure corridors, which afford polynomial time determination of all paths that do not increase exposure, is presented.","sentences":["While maximizing line-of-sight coverage of specific regions or agents in the environment is a well explored path planning objective, the converse problem of minimizing exposure to the entire environment during navigation is especially interesting in the context of minimizing detection risk.","This work demonstrates that minimizing line-of-sight exposure to the environment is non-Markovian, which cannot be efficiently solved optimally with traditional path planning.","The optimality gap of the graph-search algorithm A* and the trade-offs in optimality vs. computation time of several approximating heuristics is explored.","Finally, the concept of equal-exposure corridors, which afford polynomial time determination of all paths that do not increase exposure, is presented."],"url":"http://arxiv.org/abs/2403.02450v1","category":"cs.RO"}
{"created":"2024-03-04 20:05:28","title":"Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging","abstract":"High dynamic range (HDR) imaging involves capturing a series of frames of the same scene, each with different exposure settings, to broaden the dynamic range of light. This can be achieved through burst capturing or using staggered HDR sensors that capture long and short exposures simultaneously in the camera image signal processor (ISP). Within camera ISP pipeline, illuminant estimation is a crucial step aiming to estimate the color of the global illuminant in the scene. This estimation is used in camera ISP white-balance module to remove undesirable color cast in the final image. Despite the multiple frames captured in the HDR pipeline, conventional illuminant estimation methods often rely only on a single frame of the scene. In this paper, we explore leveraging information from frames captured with different exposure times. Specifically, we introduce a simple feature extracted from dual-exposure images to guide illuminant estimators, referred to as the dual-exposure feature (DEF). To validate the efficiency of DEF, we employed two illuminant estimators using the proposed DEF: 1) a multilayer perceptron network (MLP), referred to as exposure-based MLP (EMLP), and 2) a modified version of the convolutional color constancy (CCC) to integrate our DEF, that we call ECCC. Both EMLP and ECCC achieve promising results, in some cases surpassing prior methods that require hundreds of thousands or millions of parameters, with only a few hundred parameters for EMLP and a few thousand parameters for ECCC.","sentences":["High dynamic range (HDR) imaging involves capturing a series of frames of the same scene, each with different exposure settings, to broaden the dynamic range of light.","This can be achieved through burst capturing or using staggered HDR sensors that capture long and short exposures simultaneously in the camera image signal processor (ISP).","Within camera ISP pipeline, illuminant estimation is a crucial step aiming to estimate the color of the global illuminant in the scene.","This estimation is used in camera ISP white-balance module to remove undesirable color cast in the final image.","Despite the multiple frames captured in the HDR pipeline, conventional illuminant estimation methods often rely only on a single frame of the scene.","In this paper, we explore leveraging information from frames captured with different exposure times.","Specifically, we introduce a simple feature extracted from dual-exposure images to guide illuminant estimators, referred to as the dual-exposure feature (DEF).","To validate the efficiency of DEF, we employed two illuminant estimators using the proposed DEF: 1) a multilayer perceptron network (MLP), referred to as exposure-based MLP (EMLP), and 2) a modified version of the convolutional color constancy (CCC) to integrate our DEF, that we call ECCC.","Both EMLP and ECCC achieve promising results, in some cases surpassing prior methods that require hundreds of thousands or millions of parameters, with only a few hundred parameters for EMLP and a few thousand parameters for ECCC."],"url":"http://arxiv.org/abs/2403.02449v1","category":"cs.CV"}
{"created":"2024-03-04 19:28:24","title":"Connections between Bressan's Mixing Conjecture, the Branched Optimal Transport and Combinatorial Optimization","abstract":"We investigate the 1D version of the notable Bressan's mixing conjecture, and introduce various formulation in the classical optimal transport setting, the branched optimal transport setting and a combinatorial optimization. In the discrete case of the combinatorial problem, we prove the number of admissible solutions is on the Catalan number. Our investigation sheds light on the intricate relationship between mixing problem in the fluid dynamics and many other popular fields, leaving many interesting open questions in both theoretical and practical applications across disciplines.","sentences":["We investigate the 1D version of the notable Bressan's mixing conjecture, and introduce various formulation in the classical optimal transport setting, the branched optimal transport setting and a combinatorial optimization.","In the discrete case of the combinatorial problem, we prove the number of admissible solutions is on the Catalan number.","Our investigation sheds light on the intricate relationship between mixing problem in the fluid dynamics and many other popular fields, leaving many interesting open questions in both theoretical and practical applications across disciplines."],"url":"http://arxiv.org/abs/2403.02433v1","category":"math.OC"}
{"created":"2024-03-04 19:08:20","title":"NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function","abstract":"The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an alternative to the standard ViT block that reduces the compute burdens by replacing the normal Attention layers with a Network in Network structure that enhances the static approach of the MLP Mixer with a dynamic system of learning an element-wise gating function by a token mixing process. Extensive experimentation shows that the proposed design provides better performance than the baseline architectures on multiple datasets applied in the image classification task of the vision domain.","sentences":["The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks.","The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation.","While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization.","To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements.","Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more.","This paper introduces a new computational block as an alternative to the standard ViT block that reduces the compute burdens by replacing the normal Attention layers with a Network in Network structure that enhances the static approach of the MLP Mixer with a dynamic system of learning an element-wise gating function by a token mixing process.","Extensive experimentation shows that the proposed design provides better performance than the baseline architectures on multiple datasets applied in the image classification task of the vision domain."],"url":"http://arxiv.org/abs/2403.02411v1","category":"cs.CV"}
{"created":"2024-03-04 18:59:45","title":"nimCSO: A Nim package for Compositional Space Optimization","abstract":"nimCSO is a high-performance tool implementing several methods for selecting components (data dimensions) in compositional datasets, which optimize the data availability and density for applications such as machine learning. Making said choice is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present. Such spaces are encountered, for instance, in materials science, where datasets on Compositionally Complex Materials (CCMs) often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions.   At its core, nimCSO leverages the metaprogramming ability of the Nim language (nim-lang.org) to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file. As demonstrated in this paper, nimCSO reaches the physical limits of the hardware (L1 cache latency) and can outperform an efficient native Python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming NumPy implementation 35 and 17 times, respectively, when checking a candidate solution.   It is designed to be both (1) a user-ready tool, implementing two efficient brute-force approaches (for handling up to 25 dimensions), a custom search algorithm (for up to 40 dimensions), and a genetic algorithm (for any dimensionality), and (2) a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability. All configuration is done with a simple human-readable YAML config file and plain text data files, making it easy to modify the search method and its parameters with no knowledge of programming and only basic command line skills.","sentences":["nimCSO is a high-performance tool implementing several methods for selecting components (data dimensions) in compositional datasets, which optimize the data availability and density for applications such as machine learning.","Making said choice is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present.","Such spaces are encountered, for instance, in materials science, where datasets on Compositionally Complex Materials (CCMs) often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions.   ","At its core, nimCSO leverages the metaprogramming ability of the Nim language (nim-lang.org) to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file.","As demonstrated in this paper, nimCSO reaches the physical limits of the hardware (L1 cache latency) and can outperform an efficient native Python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming NumPy implementation 35 and 17 times, respectively, when checking a candidate solution.   ","It is designed to be both (1) a user-ready tool, implementing two efficient brute-force approaches (for handling up to 25 dimensions), a custom search algorithm (for up to 40 dimensions), and a genetic algorithm (for any dimensionality), and (2) a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability.","All configuration is done with a simple human-readable YAML config file and plain text data files, making it easy to modify the search method and its parameters with no knowledge of programming and only basic command line skills."],"url":"http://arxiv.org/abs/2403.02340v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-04 18:59:07","title":"First Constraints on the Epoch of Reionization Using the non-Gaussianity of the Kinematic Sunyaev-Zel{'}dovich Effect from the South Pole Telescope and {\\it Herschel}-SPIRE Observations","abstract":"We report results from an analysis aimed at detecting the trispectrum of the kinematic Sunyaev-Zel{'}dovich (kSZ) effect by combining data from the South Pole Telescope (SPT) and {\\it Herschel}-SPIRE experiments over a 100 ${\\rm deg}^{2}$ field. The SPT observations combine data from the previous and current surveys, namely SPTpol and SPT-3G, to achieve depths of 4.5, 3, and 16 $\\mu {\\rm K-arcmin}$ in bands centered at 95, 150, and 220 GHz. For SPIRE, we include data from the 600 and 857 GHz bands. We reconstruct the velocity-induced large-scale correlation of the small-scale kSZ signal with a quadratic estimator that uses two cosmic microwave background (CMB) temperature maps, constructed by optimally combining data from all the frequency bands. We reject the null hypothesis of a zero trispectrum at $10.3\\sigma$ level. However, the measured trispectrum contains contributions from both the kSZ and other undesired components, such as CMB lensing and astrophysical foregrounds, with kSZ being sub-dominant. We use the \\textsc{Agora} simulations to estimate the expected signal from CMB lensing and astrophysical foregrounds. After accounting for the contributions from CMB lensing and foreground signals, we do not detect an excess kSZ-only trispectrum and use this non-detection to set constraints on reionization. By applying a prior based on observations of the Gunn-Peterson trough, we obtain an upper limit on the duration of reionization of $\\Delta z_{\\rm re, 50} < 4.5$ (95\\% C.L). We find these constraints are fairly robust to foregrounds assumptions. This trispectrum measurement is independent of, but consistent with, {\\it Planck}'s optical depth measurement. This result is the first constraint on the epoch of reionization using the non-Gaussian nature of the kSZ signal.","sentences":["We report results from an analysis aimed at detecting the trispectrum of the kinematic Sunyaev-Zel{'}dovich (kSZ) effect by combining data from the South Pole Telescope (SPT) and {\\it Herschel}-SPIRE experiments over a 100 ${\\rm deg}^{2}$ field.","The SPT observations combine data from the previous and current surveys, namely SPTpol and SPT-3G, to achieve depths of 4.5, 3, and 16 $\\mu {\\rm K-arcmin}$ in bands centered at 95, 150, and 220 GHz.","For SPIRE, we include data from the 600 and 857 GHz bands.","We reconstruct the velocity-induced large-scale correlation of the small-scale kSZ signal with a quadratic estimator that uses two cosmic microwave background (CMB) temperature maps, constructed by optimally combining data from all the frequency bands.","We reject the null hypothesis of a zero trispectrum at $10.3\\sigma$ level.","However, the measured trispectrum contains contributions from both the kSZ and other undesired components, such as CMB lensing and astrophysical foregrounds, with kSZ being sub-dominant.","We use the \\textsc{Agora} simulations to estimate the expected signal from CMB lensing and astrophysical foregrounds.","After accounting for the contributions from CMB lensing and foreground signals, we do not detect an excess kSZ-only trispectrum and use this non-detection to set constraints on reionization.","By applying a prior based on observations of the Gunn-Peterson trough, we obtain an upper limit on the duration of reionization of $\\Delta z_{\\rm re, 50} < 4.5$ (95\\% C.L).","We find these constraints are fairly robust to foregrounds assumptions.","This trispectrum measurement is independent of, but consistent with, {\\it Planck}'s optical depth measurement.","This result is the first constraint on the epoch of reionization using the non-Gaussian nature of the kSZ signal."],"url":"http://arxiv.org/abs/2403.02337v1","category":"astro-ph.CO"}
{"created":"2024-03-04 18:47:08","title":"Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve","abstract":"Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi to create stall-free schedules that can add new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Our evaluation shows that Sarathi-Serve improves serving throughput within desired latency SLOs of Mistral-7B by up to 2.6x on a single A100 GPU and up to 6.9x for Falcon-180B on 8 A100 GPUs over Orca and vLLM.","sentences":["Each LLM serving request goes through two phases.","The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time.","Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt.","In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request.","This makes batching highly effective for decodes and consequently for overall throughput.","However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   ","We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi.","Sarathi-Serve leverages chunked-prefills from Sarathi to create stall-free schedules that can add new requests in a batch without pausing ongoing decodes.","Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency.","Our evaluation shows that Sarathi-Serve improves serving throughput within desired latency SLOs of Mistral-7B by up to 2.6x on a single A100 GPU and up to 6.9x for Falcon-180B on 8 A100 GPUs over Orca and vLLM."],"url":"http://arxiv.org/abs/2403.02310v1","category":"cs.LG"}
{"created":"2024-03-04 18:17:18","title":"Stage: Query Execution Time Prediction in Amazon Redshift","abstract":"Query performance (e.g., execution time) prediction is a critical component of modern DBMSes. As a pioneering cloud data warehouse, Amazon Redshift relies on an accurate execution time prediction for many downstream tasks, ranging from high-level optimizations, such as automatically creating materialized views, to low-level tasks on the critical path of query execution, such as admission, scheduling, and execution resource control. Unfortunately, many existing execution time prediction techniques, including those used in Redshift, suffer from cold start issues, inaccurate estimation, and are not robust against workload/data changes.   In this paper, we propose a novel hierarchical execution time predictor: the Stage predictor. The Stage predictor is designed to leverage the unique characteristics and challenges faced by Redshift. The Stage predictor consists of three model states: an execution time cache, a lightweight local model optimized for a specific DB instance with uncertainty measurement, and a complex global model that is transferable across all instances in Redshift. We design a systematic approach to use these models that best leverages optimality (cache), instance-optimization (local model), and transferable knowledge about Redshift (global model). Experimentally, we show that the Stage predictor makes more accurate and robust predictions while maintaining a practical inference latency and memory overhead. Overall, the Stage predictor can improve the average query execution latency by $20\\%$ on these instances compared to the prior query performance predictor in Redshift.","sentences":["Query performance (e.g., execution time) prediction is a critical component of modern DBMSes.","As a pioneering cloud data warehouse, Amazon Redshift relies on an accurate execution time prediction for many downstream tasks, ranging from high-level optimizations, such as automatically creating materialized views, to low-level tasks on the critical path of query execution, such as admission, scheduling, and execution resource control.","Unfortunately, many existing execution time prediction techniques, including those used in Redshift, suffer from cold start issues, inaccurate estimation, and are not robust against workload/data changes.   ","In this paper, we propose a novel hierarchical execution time predictor: the Stage predictor.","The Stage predictor is designed to leverage the unique characteristics and challenges faced by Redshift.","The Stage predictor consists of three model states: an execution time cache, a lightweight local model optimized for a specific DB instance with uncertainty measurement, and a complex global model that is transferable across all instances in Redshift.","We design a systematic approach to use these models that best leverages optimality (cache), instance-optimization (local model), and transferable knowledge about Redshift (global model).","Experimentally, we show that the Stage predictor makes more accurate and robust predictions while maintaining a practical inference latency and memory overhead.","Overall, the Stage predictor can improve the average query execution latency by $20\\%$ on these instances compared to the prior query performance predictor in Redshift."],"url":"http://arxiv.org/abs/2403.02286v1","category":"cs.DB"}
{"created":"2024-03-04 18:05:59","title":"Diametric problem for permutations with the Ulam metric (optimal anticodes)","abstract":"We study the diametric problem (i.e., optimal anticodes) in the space of permutations under the Ulam distance. That is, let $S_n$ denote the set of permutations on $n$ symbols, and for each $\\sigma, \\tau \\in S_n$, define their Ulam distance as the number of distinct symbols that must be deleted from each until they are equal. We obtain a near-optimal upper bound on the size of the intersection of two balls in this space, and as a corollary, we prove that a set of diameter at most $k$ has size at most $2^{k + C k^{2/3}} n! / (n-k)!$, compared to the best known construction of size $n!/(n-k)!$. We also prove that sets of diameter $1$ have at most $n$ elements.","sentences":["We study the diametric problem (i.e., optimal anticodes) in the space of permutations under the Ulam distance.","That is, let $S_n$ denote the set of permutations on $n$ symbols, and for each $\\sigma, \\tau \\in S_n$, define their Ulam distance as the number of distinct symbols that must be deleted from each until they are equal.","We obtain a near-optimal upper bound on the size of the intersection of two balls in this space, and as a corollary, we prove that a set of diameter at most $k$ has size at most $2^{k + C k^{2/3}} n! /","(n-k)!$, compared to the best known construction of size $n!/(n-k)!$. We also prove that sets of diameter $1$ have at most $n$ elements."],"url":"http://arxiv.org/abs/2403.02276v1","category":"math.CO"}
{"created":"2024-03-04 17:58:09","title":"RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models","abstract":"Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.","sentences":["Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks.","Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA).","In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods.","To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective.","Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone."],"url":"http://arxiv.org/abs/2403.02271v1","category":"cs.CL"}
{"created":"2024-03-04 17:53:01","title":"Influence of catastrophes and hidden dynamical symmetries on ultrafast backscattered photoelectrons","abstract":"We discuss the effect of using potentials with a Coulomb tail and different degrees of softening in the photoelectron momentum distributions (PMDs) using the recently implemented hybrid forward-boundary CQSFA (H-CQSFA). We show that introducing a softening in the Coulomb interaction influences the ridges observed in the PMDs associated with backscattered electron trajectories. In the limit of a hard-core Coulomb interaction, the re-scattering ridges close along the polarization axis, while for a soft-core potential, they are interrupted at ridge-specific angles. We analyze the momentum mapping of the different orbits leading to the ridges. For the hard-core potential, there exist two types of saddle-point solutions that coalesce at the ridge. By increasing the softening, we show that two additional solutions emerge as the result of breaking a hidden dynamical symmetry associated exclusively with the Coulomb potential. Further signatures of this symmetry breaking are encountered in subsets of momentum-space trajectories. Finally, we use scattering theory to show how the softening affects the maximal scattering angle and provide estimates that agree with our observations from the CQSFA. This implies that, in the presence of residual binding potentials in the electron's continuum propagation, the distinction between purely kinematic and dynamic caustics becomes blurred.","sentences":["We discuss the effect of using potentials with a Coulomb tail and different degrees of softening in the photoelectron momentum distributions (PMDs) using the recently implemented hybrid forward-boundary CQSFA (H-CQSFA).","We show that introducing a softening in the Coulomb interaction influences the ridges observed in the PMDs associated with backscattered electron trajectories.","In the limit of a hard-core Coulomb interaction, the re-scattering ridges close along the polarization axis, while for a soft-core potential, they are interrupted at ridge-specific angles.","We analyze the momentum mapping of the different orbits leading to the ridges.","For the hard-core potential, there exist two types of saddle-point solutions that coalesce at the ridge.","By increasing the softening, we show that two additional solutions emerge as the result of breaking a hidden dynamical symmetry associated exclusively with the Coulomb potential.","Further signatures of this symmetry breaking are encountered in subsets of momentum-space trajectories.","Finally, we use scattering theory to show how the softening affects the maximal scattering angle and provide estimates that agree with our observations from the CQSFA.","This implies that, in the presence of residual binding potentials in the electron's continuum propagation, the distinction between purely kinematic and dynamic caustics becomes blurred."],"url":"http://arxiv.org/abs/2403.02264v1","category":"physics.atom-ph"}
{"created":"2024-03-04 17:52:54","title":"The $U$-Matrix geometrical model for multi-particle production in high-energy hadronic collisions","abstract":"Inspired by the picture portraying the KNO scaling violation as an extension of the geometrical scaling violation, the current study proposes a phenomenological model for multi-particle production in hadron collisions based on the geometrical approach and using the $U$-Matrix unitarization scheme of the scattering amplitude. The model has been fine-tuned and all parameters have been derived from optimal fits to various hadronic multiplicity distributions data in $p + p(\\bar{p})$ collisions across a broad range of energies. The results have revealed that our model furnishes a reasonable description of diverse multiplicity distributions at various energies. Besides, they have demonstrated a pronounced violation of the geometrical scaling, which eventually resulted in a significant violation of the KNO scaling. The study has also analyzed the higher-order moments of the multiplicity distribution. We have observed an unexpected overestimation of the fluctuations and correlations between final state particles with increasing energy, particularly above LHC energy. It is claimed that this overestimation is due to statistical fluctuations embedded in the $U$-matrix scheme. The findings of this study have shed light on the key role of the $U$-matrix scheme in the impact of collision geometry on multi-particle production processes at high energy.","sentences":["Inspired by the picture portraying the KNO scaling violation as an extension of the geometrical scaling violation, the current study proposes a phenomenological model for multi-particle production in hadron collisions based on the geometrical approach and using the $U$-Matrix unitarization scheme of the scattering amplitude.","The model has been fine-tuned and all parameters have been derived from optimal fits to various hadronic multiplicity distributions data in $p + p(\\bar{p})$ collisions across a broad range of energies.","The results have revealed that our model furnishes a reasonable description of diverse multiplicity distributions at various energies.","Besides, they have demonstrated a pronounced violation of the geometrical scaling, which eventually resulted in a significant violation of the KNO scaling.","The study has also analyzed the higher-order moments of the multiplicity distribution.","We have observed an unexpected overestimation of the fluctuations and correlations between final state particles with increasing energy, particularly above LHC energy.","It is claimed that this overestimation is due to statistical fluctuations embedded in the $U$-matrix scheme.","The findings of this study have shed light on the key role of the $U$-matrix scheme in the impact of collision geometry on multi-particle production processes at high energy."],"url":"http://arxiv.org/abs/2403.02263v1","category":"hep-ph"}
{"created":"2024-03-04 17:45:52","title":"Human-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy","abstract":"AI approaches are progressing besting humans at game-related tasks (e.g. chess). The next stage is expected to be Human-AI collaboration; however, the research on this subject has been mixed and is in need of additional data points. We add to this nascent literature by studying Human-AI collaboration on a common administrative educational task. Education is a special domain in its relation to AI and has been slow to adopt AI approaches in practice, concerned with the educational enterprise losing its humanistic touch and because standard of quality is demanded because of the impact on a person's career and developmental trajectory. In this study (N = 22), we design an experiment to explore the effect of Human-AI collaboration on the task of tagging educational content with skills from the US common core taxonomy. Our results show that the experiment group (with AI recommendations) saved around 50% time (p < 0.01) in the execution of their tagging task but at the sacrifice of 7.7% recall (p = 0.267) and 35% accuracy (p= 0.1170) compared with the non-AI involved control group, placing the AI+human group in between the AI alone (lowest performance) and the human alone (highest performance). We further analyze log data from this AI collaboration experiment to explore under what circumstances humans still exercised their discernment when receiving recommendations. Finally, we outline how this study can assist in implementing AI tools, like ChatGPT, in education.","sentences":["AI approaches are progressing besting humans at game-related tasks (e.g. chess).","The next stage is expected to be Human-AI collaboration; however, the research on this subject has been mixed and is in need of additional data points.","We add to this nascent literature by studying Human-AI collaboration on a common administrative educational task.","Education is a special domain in its relation to AI and has been slow to adopt AI approaches in practice, concerned with the educational enterprise losing its humanistic touch and because standard of quality is demanded because of the impact on a person's career and developmental trajectory.","In this study (N = 22), we design an experiment to explore the effect of Human-AI collaboration on the task of tagging educational content with skills from the US common core taxonomy.","Our results show that the experiment group (with AI recommendations) saved around 50% time (p < 0.01) in the execution of their tagging task but at the sacrifice of 7.7% recall (p = 0.267) and 35% accuracy (p= 0.1170) compared with the non-AI involved control group, placing the AI+human group in between the AI alone (lowest performance) and the human alone (highest performance).","We further analyze log data from this AI collaboration experiment to explore under what circumstances humans still exercised their discernment when receiving recommendations.","Finally, we outline how this study can assist in implementing AI tools, like ChatGPT, in education."],"url":"http://arxiv.org/abs/2403.02259v1","category":"cs.HC"}
{"created":"2024-03-04 17:34:32","title":"Dynamic programming principle in cost-efficient sequential design: application to switching measurements","abstract":"We study sequential cost-efficient design in a situation where each update of covariates involves a fixed time cost typically considerable compared to a single measurement time. The problem arises from parameter estimation in switching measurements on superconducting Josephson junctions which are components needed in quantum computers and other superconducting electronics. In switching measurements, a sequence of current pulses is applied to the junction and a binary voltage response is observed. The measurement requires a very low temperature that can be kept stable only for a relatively short time, and therefore it is essential to use an efficient design. We use the dynamic programming principle from the mathematical theory of optimal control to solve the optimal update times. Our simulations demonstrate the cost-efficiency compared to the previously used methods.","sentences":["We study sequential cost-efficient design in a situation where each update of covariates involves a fixed time cost typically considerable compared to a single measurement time.","The problem arises from parameter estimation in switching measurements on superconducting Josephson junctions which are components needed in quantum computers and other superconducting electronics.","In switching measurements, a sequence of current pulses is applied to the junction and a binary voltage response is observed.","The measurement requires a very low temperature that can be kept stable only for a relatively short time, and therefore it is essential to use an efficient design.","We use the dynamic programming principle from the mathematical theory of optimal control to solve the optimal update times.","Our simulations demonstrate the cost-efficiency compared to the previously used methods."],"url":"http://arxiv.org/abs/2403.02245v1","category":"stat.ME"}
{"created":"2024-03-04 17:26:28","title":"3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors","abstract":"We present a two-stage text-to-3D generation system, namely 3DTopia, which generates high-quality general 3D assets within 5 minutes using hybrid diffusion priors. The first stage samples from a 3D diffusion prior directly learned from 3D data. Specifically, it is powered by a text-conditioned tri-plane latent diffusion model, which quickly generates coarse 3D samples for fast prototyping. The second stage utilizes 2D diffusion priors to further refine the texture of coarse 3D models from the first stage. The refinement consists of both latent and pixel space optimization for high-quality texture generation. To facilitate the training of the proposed system, we clean and caption the largest open-source 3D dataset, Objaverse, by combining the power of vision language models and large language models. Experiment results are reported qualitatively and quantitatively to show the performance of the proposed system. Our codes and models are available at https://github.com/3DTopia/3DTopia","sentences":["We present a two-stage text-to-3D generation system, namely 3DTopia, which generates high-quality general 3D assets within 5 minutes using hybrid diffusion priors.","The first stage samples from a 3D diffusion prior directly learned from 3D data.","Specifically, it is powered by a text-conditioned tri-plane latent diffusion model, which quickly generates coarse 3D samples for fast prototyping.","The second stage utilizes 2D diffusion priors to further refine the texture of coarse 3D models from the first stage.","The refinement consists of both latent and pixel space optimization for high-quality texture generation.","To facilitate the training of the proposed system, we clean and caption the largest open-source 3D dataset, Objaverse, by combining the power of vision language models and large language models.","Experiment results are reported qualitatively and quantitatively to show the performance of the proposed system.","Our codes and models are available at https://github.com/3DTopia/3DTopia"],"url":"http://arxiv.org/abs/2403.02234v1","category":"cs.CV"}
{"created":"2024-03-04 17:24:03","title":"Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling","abstract":"Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the interplay between the attention of feature-position and position-wise correlations.","sentences":["Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining.","However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers.","In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining.","On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations.","On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the interplay between the attention of feature-position and position-wise correlations."],"url":"http://arxiv.org/abs/2403.02233v1","category":"cs.LG"}
{"created":"2024-03-04 17:02:52","title":"Optimizing the Energetics of the Finite-time Driving of Field Theories","abstract":"The phase transitions for many-body systems have been understood using field theories. A few canonical physical model classes encapsulate the underlying physical properties of a large number of systems. The finite-time driving of such systems and associated optimal energetic costs have not been investigated yet. We consider two universality classes Model A and Model B, that describe the dynamics for the non-conserved and conserved scalar order parameters respectively. Here, using the recent developments in stochastic thermodynamics and optimal transport theory, we analytically compute the optimal driving protocols by minimizing the mean stochastic work required for finite-time driving. Further, we numerically optimize the mean and variance of the stochastic work simultaneously. Such a multi-objective optimization is called a Pareto optimization problem and its optimal solution is a Pareto front. We discover a first-order Pareto phase transition in the Pareto front. Physically, it corresponds to the coexistence of two classes of optimal driving protocols analogous to the liquid-gas coexistence for the equilibrium phase transition. Our framework sheds light on the finite-time optimal driving of the fields and the trade-off between the mean and fluctuations of the optimal work.","sentences":["The phase transitions for many-body systems have been understood using field theories.","A few canonical physical model classes encapsulate the underlying physical properties of a large number of systems.","The finite-time driving of such systems and associated optimal energetic costs have not been investigated yet.","We consider two universality classes Model A and Model B, that describe the dynamics for the non-conserved and conserved scalar order parameters respectively.","Here, using the recent developments in stochastic thermodynamics and optimal transport theory, we analytically compute the optimal driving protocols by minimizing the mean stochastic work required for finite-time driving.","Further, we numerically optimize the mean and variance of the stochastic work simultaneously.","Such a multi-objective optimization is called a Pareto optimization problem and its optimal solution is a Pareto front.","We discover a first-order Pareto phase transition in the Pareto front.","Physically, it corresponds to the coexistence of two classes of optimal driving protocols analogous to the liquid-gas coexistence for the equilibrium phase transition.","Our framework sheds light on the finite-time optimal driving of the fields and the trade-off between the mean and fluctuations of the optimal work."],"url":"http://arxiv.org/abs/2403.02216v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-04 16:44:48","title":"Scheduling Garbage Collection for Energy Efficiency on Asymmetric Multicore Processors","abstract":"The growing concern for energy efficiency in the Information and Communication Technology (ICT) sector has prompted the exploration of resource management techniques. While hardware architectures, such as single-ISA asymmetric multicore processors (AMP), offer potential energy savings, there is still untapped potential for software optimizations. This paper aims to bridge this gap by investigating the scheduling of garbage collection (GC) activities on a heterogeneous architecture with both performance cores (\"p-cores\") and energy cores (\"e-cores\") to achieve energy savings.   Our study focuses on the concurrent ZGC collector in the context of Java Virtual Machines (JVM), as the energy aspect is not well studied in the context of latency-sensitive Java workloads. By comparing the energy efficiency, performance, latency, and memory utilization of executing GC on p-cores versus e-cores, we present compelling findings.   We demonstrate that scheduling GC work on e-cores overall leads to approximately 3% energy savings without performance and mean latency degradation while requiring no additional effort from developers. Overall energy reduction can increase to 5.3$\\pm$0.0225% by tuning the number of e-cores (still not changing the program!).   Our findings highlight the practicality and benefits of scheduling GC on e-cores, showcasing the potential for energy savings in heterogeneous architectures running Java workloads while meeting critical latency requirements. Our research contributes to the ongoing efforts toward achieving a more sustainable and efficient ICT sector.","sentences":["The growing concern for energy efficiency in the Information and Communication Technology (ICT) sector has prompted the exploration of resource management techniques.","While hardware architectures, such as single-ISA asymmetric multicore processors (AMP), offer potential energy savings, there is still untapped potential for software optimizations.","This paper aims to bridge this gap by investigating the scheduling of garbage collection (GC) activities on a heterogeneous architecture with both performance cores (\"p-cores\") and energy cores (\"e-cores\") to achieve energy savings.   ","Our study focuses on the concurrent ZGC collector in the context of Java Virtual Machines (JVM), as the energy aspect is not well studied in the context of latency-sensitive Java workloads.","By comparing the energy efficiency, performance, latency, and memory utilization of executing GC on p-cores versus e-cores, we present compelling findings.   ","We demonstrate that scheduling GC work on e-cores overall leads to approximately 3% energy savings without performance and mean latency degradation while requiring no additional effort from developers.","Overall energy reduction can increase to 5.3$\\pm$0.0225% by tuning the number of e-cores (still not changing the program!).   ","Our findings highlight the practicality and benefits of scheduling GC on e-cores, showcasing the potential for energy savings in heterogeneous architectures running Java workloads while meeting critical latency requirements.","Our research contributes to the ongoing efforts toward achieving a more sustainable and efficient ICT sector."],"url":"http://arxiv.org/abs/2403.02200v1","category":"cs.PL"}
{"created":"2024-03-04 16:29:51","title":"Smart abstraction based on iterative cover and non-uniform cells","abstract":"We propose a multi-scale approach for computing abstractions of dynamical systems, that incorporates both local and global optimal control to construct a goal-specific abstraction. For a local optimal control problem, we not only design the controller ensuring the transition between every two subsets (cells) of the state space but also incorporate the volume and shape of these cells into the optimization process. This integrated approach enables the design of non-uniform cells, effectively reducing the complexity of the abstraction. These local optimal controllers are then combined into a digraph, which is globally optimized to obtain the entire trajectory. The global optimizer attempts to lazily build the abstraction along the optimal trajectory, which is less affected by an increase in the number of dimensions. Since the optimal trajectory is generally unknown in practice, we propose a methodology based on the RRT* algorithm to determine it incrementally. We illustrate the approach on a planar nonlinear dynamical system.","sentences":["We propose a multi-scale approach for computing abstractions of dynamical systems, that incorporates both local and global optimal control to construct a goal-specific abstraction.","For a local optimal control problem, we not only design the controller ensuring the transition between every two subsets (cells) of the state space but also incorporate the volume and shape of these cells into the optimization process.","This integrated approach enables the design of non-uniform cells, effectively reducing the complexity of the abstraction.","These local optimal controllers are then combined into a digraph, which is globally optimized to obtain the entire trajectory.","The global optimizer attempts to lazily build the abstraction along the optimal trajectory, which is less affected by an increase in the number of dimensions.","Since the optimal trajectory is generally unknown in practice, we propose a methodology based on the RRT* algorithm to determine it incrementally.","We illustrate the approach on a planar nonlinear dynamical system."],"url":"http://arxiv.org/abs/2403.02190v1","category":"math.DS"}
{"created":"2024-03-04 16:17:39","title":"NeuroVoz: a Castillian Spanish corpus of parkinsonian speech","abstract":"The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, and the first in Castillian Spanish.   NeuroVoz is composed by 2,903 audio recordings averaging $26.88 \\pm 3.35$ recordings per participant, offering a substantial resource for the scientific exploration of PD's impact on speech. This dataset has already underpinned several studies, achieving a benchmark accuracy of 89% in PD speech pattern identification, indicating marked speech alterations attributable to PD. Despite these advances, the broader challenge of conducting a language-agnostic, cross-corpora analysis of Parkinsonian speech patterns remains an open area for future research. This contribution not only fills a critical void in PD speech analysis resources but also sets a new standard for the global research community in leveraging speech as a diagnostic tool for neurodegenerative diseases.","sentences":["The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   ","In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state.","This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues.","The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, and the first in Castillian Spanish.   ","NeuroVoz is composed by 2,903 audio recordings averaging $26.88 \\pm 3.35$ recordings per participant, offering a substantial resource for the scientific exploration of PD's impact on speech.","This dataset has already underpinned several studies, achieving a benchmark accuracy of 89% in PD speech pattern identification, indicating marked speech alterations attributable to PD.","Despite these advances, the broader challenge of conducting a language-agnostic, cross-corpora analysis of Parkinsonian speech patterns remains an open area for future research.","This contribution not only fills a critical void in PD speech analysis resources but also sets a new standard for the global research community in leveraging speech as a diagnostic tool for neurodegenerative diseases."],"url":"http://arxiv.org/abs/2403.02371v1","category":"eess.AS"}
{"created":"2024-03-04 16:10:34","title":"A particle model that conserves the measure in the phase space, but does not conserve the kinetic energy","abstract":"We consider a particular model of hard spheres that collide inelastically, losing a fixed amount of kinetic energy at each collision. We show that the transport associated to this hard sphere dynamics preserves locally the measure in the phase space. We prove the analog of Alexander's theorem for our model, providing the global well-posedness of the trajectories, for almost every initial datum.","sentences":["We consider a particular model of hard spheres that collide inelastically, losing a fixed amount of kinetic energy at each collision.","We show that the transport associated to this hard sphere dynamics preserves locally the measure in the phase space.","We prove the analog of Alexander's theorem for our model, providing the global well-posedness of the trajectories, for almost every initial datum."],"url":"http://arxiv.org/abs/2403.02162v1","category":"math-ph"}
{"created":"2024-03-04 15:54:19","title":"Improved Tests for Mediation","abstract":"Testing for a mediation effect is important in many disciplines, but is made difficult - even asymptotically - by the influence of nuisance parameters. Classical tests such as likelihood ratio (LR) and Wald (Sobel) tests have very poor power properties in parts of the parameter space, and many attempts have been made to produce improved tests, with limited success. In this paper we show that augmenting the critical region of the LR test can produce a test with much improved behavior everywhere. In fact, we first show that there exists a test of this type that is (asymptotically) exact for certain test levels $\\alpha $, including the common choices $\\alpha =.01,.05,.10.$ The critical region of this exact test has some undesirable properties. We go on to show that there is a very simple class of augmented LR critical regions which provides tests that are nearly exact, and avoid the issues inherent in the exact test. We suggest an optimal and coherent member of this class, provide the table needed to implement the test and to report p-values if desired. Simulation confirms validity with non-Gaussian disturbances, under heteroskedasticity, and in a nonlinear (logit) model. A short application of the method to an entrepreneurial attitudes study is included for illustration.","sentences":["Testing for a mediation effect is important in many disciplines, but is made difficult - even asymptotically - by the influence of nuisance parameters.","Classical tests such as likelihood ratio (LR) and Wald (Sobel) tests have very poor power properties in parts of the parameter space, and many attempts have been made to produce improved tests, with limited success.","In this paper we show that augmenting the critical region of the LR test can produce a test with much improved behavior everywhere.","In fact, we first show that there exists a test of this type that is (asymptotically) exact for certain test levels $\\alpha $, including the common choices $\\alpha =.01,.05,.10.$","The critical region of this exact test has some undesirable properties.","We go on to show that there is a very simple class of augmented LR critical regions which provides tests that are nearly exact, and avoid the issues inherent in the exact test.","We suggest an optimal and coherent member of this class, provide the table needed to implement the test and to report p-values if desired.","Simulation confirms validity with non-Gaussian disturbances, under heteroskedasticity, and in a nonlinear (logit) model.","A short application of the method to an entrepreneurial attitudes study is included for illustration."],"url":"http://arxiv.org/abs/2403.02144v1","category":"econ.EM"}
{"created":"2024-03-04 15:50:17","title":"Matching Algorithms in the Sparse Stochastic Block Model","abstract":"The stochastic block model (SBM) is a generalization of the Erd\\H{o}s--R\\'enyi model of random graphs that describes the interaction of a finite number of distinct communities. In sparse Erd\\H{o}s--R\\'enyi graphs, it is known that a linear-time algorithm of Karp and Sipser achieves near-optimal matching sizes asymptotically almost surely, giving a law-of-large numbers for the matching sizes of such graphs in terms of solutions to an ODE. We provide an extension of this analysis, identifying broad ranges of stochastic block model parameters for which the Karp--Sipser algorithm achieves near-optimal matching sizes, but demonstrating that it cannot perform optimally on general SBM instances.   We also consider the problem of constructing a matching online, in which the vertices of one half of a bipartite stochastic block model arrive one-at-a-time, and must be matched as they arrive. We show that the competitive ratio lower bound of 0.837 found by Mastin and Jaillet for the Erd\\H{o}s--R\\'enyi case is tight whenever the expected degrees in all communities are equal. We propose several linear-time algorithms for online matching in the general stochastic block model, but prove that despite very good experimental performance, none of these achieve online asymptotic optimality.","sentences":["The stochastic block model (SBM) is a generalization of the Erd\\H{o}s--R\\'enyi model of random graphs that describes the interaction of a finite number of distinct communities.","In sparse Erd\\H{o}s--R\\'enyi graphs, it is known that a linear-time algorithm of Karp and Sipser achieves near-optimal matching sizes asymptotically almost surely, giving a law-of-large numbers for the matching sizes of such graphs in terms of solutions to an ODE.","We provide an extension of this analysis, identifying broad ranges of stochastic block model parameters for which the Karp--Sipser algorithm achieves near-optimal matching sizes, but demonstrating that it cannot perform optimally on general SBM instances.   ","We also consider the problem of constructing a matching online, in which the vertices of one half of a bipartite stochastic block model arrive one-at-a-time, and must be matched as they arrive.","We show that the competitive ratio lower bound of 0.837 found by Mastin and Jaillet for the Erd\\H{o}s--R\\'enyi case is tight whenever the expected degrees in all communities are equal.","We propose several linear-time algorithms for online matching in the general stochastic block model, but prove that despite very good experimental performance, none of these achieve online asymptotic optimality."],"url":"http://arxiv.org/abs/2403.02140v1","category":"cs.DS"}
{"created":"2024-03-04 15:40:28","title":"Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution","abstract":"Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorithmic features. Meanwhile, we employ a sophisticated deep neural network model to infer the optimal action, ensuring informed algorithm selections. Additionally, an algorithm context restoration mechanism is embedded to facilitate smooth switching among different algorithms. These mechanisms together enable our framework to seamlessly select and switch algorithms in a dynamic online fashion. Notably, the proposed framework is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms. As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms. The experimental results showcase the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also demonstrating favorable generalization ability across different problem classes.","sentences":["Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges.","However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration.","This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems.","We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task.","Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process.","To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorithmic features.","Meanwhile, we employ a sophisticated deep neural network model to infer the optimal action, ensuring informed algorithm selections.","Additionally, an algorithm context restoration mechanism is embedded to facilitate smooth switching among different algorithms.","These mechanisms together enable our framework to seamlessly select and switch algorithms in a dynamic online fashion.","Notably, the proposed framework is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms.","As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms.","The experimental results showcase the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also demonstrating favorable generalization ability across different problem classes."],"url":"http://arxiv.org/abs/2403.02131v1","category":"cs.NE"}
{"created":"2024-03-04 15:37:55","title":"Demeter: Resource-Efficient Distributed Stream Processing under Dynamic Loads with Multi-Configuration Optimization","abstract":"Distributed Stream Processing (DSP) focuses on the near real-time processing of large streams of unbounded data. To increase processing capacities, DSP systems are able to dynamically scale across a cluster of commodity nodes, ensuring a good Quality of Service despite variable workloads. However, selecting scaleout configurations which maximize resource utilization remains a challenge. This is especially true in environments where workloads change over time and node failures are all but inevitable. Furthermore, configuration parameters such as memory allocation and checkpointing intervals impact performance and resource usage as well. Sub-optimal configurations easily lead to high operational costs, poor performance, or unacceptable loss of service.   In this paper, we present Demeter, a method for dynamically optimizing key DSP system configuration parameters for resource efficiency. Demeter uses Time Series Forecasting to predict future workloads and Multi-Objective Bayesian Optimization to model runtime behaviors in relation to parameter settings and workload rates. Together, these techniques allow us to determine whether or not enough is known about the predicted workload rate to proactively initiate short-lived parallel profiling runs for data gathering. Once trained, the models guide the adjustment of multiple, potentially dependent system configuration parameters ensuring optimized performance and resource usage in response to changing workload rates. Our experiments on a commodity cluster using Apache Flink demonstrate that Demeter significantly improves the operational efficiency of long-running benchmark jobs.","sentences":["Distributed Stream Processing (DSP) focuses on the near real-time processing of large streams of unbounded data.","To increase processing capacities, DSP systems are able to dynamically scale across a cluster of commodity nodes, ensuring a good Quality of Service despite variable workloads.","However, selecting scaleout configurations which maximize resource utilization remains a challenge.","This is especially true in environments where workloads change over time and node failures are all but inevitable.","Furthermore, configuration parameters such as memory allocation and checkpointing intervals impact performance and resource usage as well.","Sub-optimal configurations easily lead to high operational costs, poor performance, or unacceptable loss of service.   ","In this paper, we present Demeter, a method for dynamically optimizing key DSP system configuration parameters for resource efficiency.","Demeter uses Time Series Forecasting to predict future workloads and Multi-Objective Bayesian Optimization to model runtime behaviors in relation to parameter settings and workload rates.","Together, these techniques allow us to determine whether or not enough is known about the predicted workload rate to proactively initiate short-lived parallel profiling runs for data gathering.","Once trained, the models guide the adjustment of multiple, potentially dependent system configuration parameters ensuring optimized performance and resource usage in response to changing workload rates.","Our experiments on a commodity cluster using Apache Flink demonstrate that Demeter significantly improves the operational efficiency of long-running benchmark jobs."],"url":"http://arxiv.org/abs/2403.02129v1","category":"cs.DC"}
{"created":"2024-03-04 15:05:10","title":"Shape optimization in the space of piecewise-smooth shapes for the Bingham flow variational inequality","abstract":"This paper sets up an approach for shape optimization problems constrained by variational inequalities (VI) in an appropriate shape space. In contrast to classical VI, where no explicit dependence on the domain is given, VI constrained shape optimization problems are in particular highly challenging because of two main reasons: Firstly, one needs to operate in inherently non-linear, non-convex and infinite-dimensional shape spaces. Secondly, the problem cannot be solved directly without any regularization techniques in general because, e.g., one cannot expect the existence of the shape derivative for an arbitrary shape functional depending on solutions to VI. This paper introduces a specific shape manifold and presents an optimization technique to handle the non-differentiabilities on this shape manifold. In particular, we formulate an optimization system based on G\\^ateaux semiderivatives and Eulerian derivatives for a shape optimization problem constrained by the Bingham flow variational inequality. Numerical results show the applicability and efficiency of the proposed approach.","sentences":["This paper sets up an approach for shape optimization problems constrained by variational inequalities (VI) in an appropriate shape space.","In contrast to classical VI, where no explicit dependence on the domain is given, VI constrained shape optimization problems are in particular highly challenging because of two main reasons: Firstly, one needs to operate in inherently non-linear, non-convex and infinite-dimensional shape spaces.","Secondly, the problem cannot be solved directly without any regularization techniques in general because, e.g., one cannot expect the existence of the shape derivative for an arbitrary shape functional depending on solutions to VI.","This paper introduces a specific shape manifold and presents an optimization technique to handle the non-differentiabilities on this shape manifold.","In particular, we formulate an optimization system based on G\\^ateaux semiderivatives and Eulerian derivatives for a shape optimization problem constrained by the Bingham flow variational inequality.","Numerical results show the applicability and efficiency of the proposed approach."],"url":"http://arxiv.org/abs/2403.02106v1","category":"math.OC"}
{"created":"2024-03-04 14:56:45","title":"Some optimality conditions of set-valued optimization problems in locally convex topological vector spaces","abstract":"In this article, we work with set-valued optimization problems in locally convex topological vector spaces. We prove the equivalencies of some definitions of generalized convex maps introduced by Jeyakumar, Yang, Yang & Yang & Chen, as well as Zeng. And then, we discuss the conditions of weakly efficient solutions, proper efficient solutions, and optimal solutions of set-valued optimization problems.","sentences":["In this article, we work with set-valued optimization problems in locally convex topological vector spaces.","We prove the equivalencies of some definitions of generalized convex maps introduced by Jeyakumar, Yang, Yang & Yang & Chen, as well as Zeng.","And then, we discuss the conditions of weakly efficient solutions, proper efficient solutions, and optimal solutions of set-valued optimization problems."],"url":"http://arxiv.org/abs/2403.02101v1","category":"math.OC"}
{"created":"2024-03-04 14:56:44","title":"A generalized hybrid method for surfactant dynamics","abstract":"In this paper, we develop a generalized hybrid method for both two-dimensional (2-D) and three-dimensional (3-D) surfactant dynamics. While the Navier-Stokes equations are solved by the Eulerian method, the surfactant transport is tracked by a Lagrangian particle method, in which the remeshing technique is employed to prevent particle clustering. For the mass redistribution during remeshing, the redistribution weight is selected with weighted least squares, which shares the theoretical basis of the moving least squares method (MLS) and enables the present hybrid method to work in both 2-D and 3-D cases. This optimized mass redistribution effectively strengthens the robustness of the present hybrid method, as validated by 2-D topological changes of the dumbbell. The conservation, accuracy, and convergence of the present hybrid method have also been validated with both 2-D and 3-D test cases, including a translation circle/sphere, a deformed circle/sphere in the shear flow, and droplet deformation.","sentences":["In this paper, we develop a generalized hybrid method for both two-dimensional (2-D) and three-dimensional (3-D) surfactant dynamics.","While the Navier-Stokes equations are solved by the Eulerian method, the surfactant transport is tracked by a Lagrangian particle method, in which the remeshing technique is employed to prevent particle clustering.","For the mass redistribution during remeshing, the redistribution weight is selected with weighted least squares, which shares the theoretical basis of the moving least squares method (MLS) and enables the present hybrid method to work in both 2-D and 3-D cases.","This optimized mass redistribution effectively strengthens the robustness of the present hybrid method, as validated by 2-D topological changes of the dumbbell.","The conservation, accuracy, and convergence of the present hybrid method have also been validated with both 2-D and 3-D test cases, including a translation circle/sphere, a deformed circle/sphere in the shear flow, and droplet deformation."],"url":"http://arxiv.org/abs/2403.02100v1","category":"physics.comp-ph"}
{"created":"2024-03-04 14:55:05","title":"Homotopy Methods for Convex Optimization","abstract":"Convex optimization encompasses a wide range of optimization problems, containing many efficiently solvable subclasses. Interior point methods are currently the state-of-the-art approach for solving such problems, particularly effective for classes like semidefinite programming, quadratic programming, and geometric programming. However, their success hinges on the construction of self-concordant barrier functions for the feasible sets. In this work, we introduce an alternative method for tackling convex optimization problems, employing a homotopy. With this technique, the feasible set of a trivial optimization problem is continuously transformed into the target one, while tracking the solutions. We conduct an analysis of this approach, focusing on its application to semidefinite programs, hyperbolic programs, and convex optimization problems with a single convexity constraint. Moreover, we demonstrate that our approach numerically outperforms state-of-the-art methods in several interesting cases.","sentences":["Convex optimization encompasses a wide range of optimization problems, containing many efficiently solvable subclasses.","Interior point methods are currently the state-of-the-art approach for solving such problems, particularly effective for classes like semidefinite programming, quadratic programming, and geometric programming.","However, their success hinges on the construction of self-concordant barrier functions for the feasible sets.","In this work, we introduce an alternative method for tackling convex optimization problems, employing a homotopy.","With this technique, the feasible set of a trivial optimization problem is continuously transformed into the target one, while tracking the solutions.","We conduct an analysis of this approach, focusing on its application to semidefinite programs, hyperbolic programs, and convex optimization problems with a single convexity constraint.","Moreover, we demonstrate that our approach numerically outperforms state-of-the-art methods in several interesting cases."],"url":"http://arxiv.org/abs/2403.02095v1","category":"math.OC"}
{"created":"2024-03-04 14:47:27","title":"On the Need for Extensible Quantum Compilers with Verification","abstract":"In this position paper, we posit that a major Department of Energy (DOE)-funded open-source quantum compilation platform is needed to facilitate: (a) resource optimization at the fault-tolerant layer of the quantum computing software stack, and (b) co-design of that layer of the stack with other layers, and that this platform needs to be extensible and include verification.","sentences":["In this position paper, we posit that a major Department of Energy (DOE)-funded open-source quantum compilation platform is needed to facilitate: (a) resource optimization at the fault-tolerant layer of the quantum computing software stack, and (b) co-design of that layer of the stack with other layers, and that this platform needs to be extensible and include verification."],"url":"http://arxiv.org/abs/2403.02091v1","category":"quant-ph"}
{"created":"2024-03-04 14:45:33","title":"Particle detectors in superposition in de Sitter spacetime","abstract":"Cosmological particle creation is the phenomenon by which the expansion of spacetime results in the production of particles of a given quantum field in that spacetime. In this paper, we study this phenomenon by considering a multi-level quantum particle detector in de Sitter spacetime coupled to a massless real quantum scalar field. Rather than considering a fixed classical trajectory for the detector, following recent novel approaches we consider a quantum superposition of trajectories, in particular of static trajectories which keep a fixed distance from one another. The main novel result is that, due to the quantum nature of the superposition of trajectories, the state of the detector after interaction with the field is not only a mixture of the thermal states that would be expected from each individual static trajectory but rather exhibits additional coherences due to interferences between the different trajectories. We study these in detail and associate them with the properties of the particle absorbed by the detector from the thermal bath.","sentences":["Cosmological particle creation is the phenomenon by which the expansion of spacetime results in the production of particles of a given quantum field in that spacetime.","In this paper, we study this phenomenon by considering a multi-level quantum particle detector in de Sitter spacetime coupled to a massless real quantum scalar field.","Rather than considering a fixed classical trajectory for the detector, following recent novel approaches we consider a quantum superposition of trajectories, in particular of static trajectories which keep a fixed distance from one another.","The main novel result is that, due to the quantum nature of the superposition of trajectories, the state of the detector after interaction with the field is not only a mixture of the thermal states that would be expected from each individual static trajectory but rather exhibits additional coherences due to interferences between the different trajectories.","We study these in detail and associate them with the properties of the particle absorbed by the detector from the thermal bath."],"url":"http://arxiv.org/abs/2403.02087v1","category":"gr-qc"}
{"created":"2024-03-04 14:28:04","title":"Recovering quantum coherence of a cavity qubit through environment monitoring and active feedback","abstract":"Decoherence in qubits, caused by their interaction with a noisy environment, poses a significant challenge to developing reliable quantum processors. Monitoring the qubit's environment enables not only to flag decoherence events but also to reverse these errors, thereby restoring the qubit coherence. This approach is particularly beneficial for superconducting cavity qubits, whose unavoidable interaction with auxiliary transmons impacts their coherence. In this work, we uncover the intricate dynamics of cavity qubit decoherence by tracking the noisy trajectory of a transmon acting as the cavity's environment. Using real-time feedback, we successfully recover the lost coherence of the cavity qubit, achieving a fivefold increase in its dephasing time. Alternatively, by detecting transmon errors and converting them into erasures, we improve the cavity phase coherence by more than an order of magnitude. These advances are essential for using cavity qubits with low photon loss rates as long-lived quantum memories with high-fidelity gates and can enable more efficient bosonic quantum error correction codes.","sentences":["Decoherence in qubits, caused by their interaction with a noisy environment, poses a significant challenge to developing reliable quantum processors.","Monitoring the qubit's environment enables not only to flag decoherence events but also to reverse these errors, thereby restoring the qubit coherence.","This approach is particularly beneficial for superconducting cavity qubits, whose unavoidable interaction with auxiliary transmons impacts their coherence.","In this work, we uncover the intricate dynamics of cavity qubit decoherence by tracking the noisy trajectory of a transmon acting as the cavity's environment.","Using real-time feedback, we successfully recover the lost coherence of the cavity qubit, achieving a fivefold increase in its dephasing time.","Alternatively, by detecting transmon errors and converting them into erasures, we improve the cavity phase coherence by more than an order of magnitude.","These advances are essential for using cavity qubits with low photon loss rates as long-lived quantum memories with high-fidelity gates and can enable more efficient bosonic quantum error correction codes."],"url":"http://arxiv.org/abs/2403.02081v1","category":"quant-ph"}
{"created":"2024-03-04 14:26:22","title":"The ultimate upper bound on the injectivity radius of the Stiefel manifold","abstract":"We exhibit conjugate points on the Stiefel manifold endowed with any member of the family of Riemannian metrics introduced by H\\\"uper et al. (2021). This family contains the well-known canonical and Euclidean metrics. An upper bound on the injectivity radius of the Stiefel manifold in the considered metric is then obtained as the minimum between the length of the geodesic along which the points are conjugate and the length of certain geodesic loops. Numerical experiments support the conjecture that the obtained upper bound is in fact equal to the injectivity radius.","sentences":["We exhibit conjugate points on the Stiefel manifold endowed with any member of the family of Riemannian metrics introduced by H\\\"uper et al. (2021).","This family contains the well-known canonical and Euclidean metrics.","An upper bound on the injectivity radius of the Stiefel manifold in the considered metric is then obtained as the minimum between the length of the geodesic along which the points are conjugate and the length of certain geodesic loops.","Numerical experiments support the conjecture that the obtained upper bound is in fact equal to the injectivity radius."],"url":"http://arxiv.org/abs/2403.02079v1","category":"math.DG"}
{"created":"2024-03-04 14:21:51","title":"DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction","abstract":"In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) with Kalman Filter motion prediction works well in pedestrian-dominant scenarios but falls short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much less sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with 63.4 and 76.2 in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.","sentences":["In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes.","Tacking-by-detection (TBD) with Kalman Filter motion prediction works well in pedestrian-dominant scenarios but falls short in complex situations when multiple objects perform non-linear and diverse motion simultaneously.","To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT.","Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D MP).","It models the entire distribution of various motion presented by the data as a whole.","It also predicts an individual object's motion conditioning on an individual's historical motion information.","Furthermore, it optimizes the diffusion process with much less sampling steps.","As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with 63.4 and 76.2 in HOTA metrics, respectively.","To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction."],"url":"http://arxiv.org/abs/2403.02075v1","category":"cs.CV"}
{"created":"2024-03-04 14:21:24","title":"Variational Monte Carlo study of stripes as a function of doping in the $t-t'$ Hubbard model","abstract":"We perform variational Monte Carlo simulations of the single-band Hubbard model on the square lattice with both nearest ($t$) and next-nearest ($t'$) neighbor hoppings. Our work investigates the consequences of increasing hole doping on the instauration of stripes and the behavior of the superconducting order parameter, with a discussion on how the two phenomena affect each other. We consider two different values of the next-nearest neighbor hopping parameter, that are appropriate for describing cuprate superconductors. We observe that stripes are the optimal state in a wide doping range; the stripe wavelength reduces at increasing doping, until stripes melt into a uniform state for large values of doping. Superconducting pair-pair correlations, indicating the presence of superconductivity, are always suppressed in the presence of stripes. Our results suggest that the phase diagram for the single-band Hubbard model is dominated by stripes, with superconductivity being possible only in a narrow doping range between striped states and a nonsuperconducting metal.","sentences":["We perform variational Monte Carlo simulations of the single-band Hubbard model on the square lattice with both nearest ($t$) and next-nearest ($t'$) neighbor hoppings.","Our work investigates the consequences of increasing hole doping on the instauration of stripes and the behavior of the superconducting order parameter, with a discussion on how the two phenomena affect each other.","We consider two different values of the next-nearest neighbor hopping parameter, that are appropriate for describing cuprate superconductors.","We observe that stripes are the optimal state in a wide doping range; the stripe wavelength reduces at increasing doping, until stripes melt into a uniform state for large values of doping.","Superconducting pair-pair correlations, indicating the presence of superconductivity, are always suppressed in the presence of stripes.","Our results suggest that the phase diagram for the single-band Hubbard model is dominated by stripes, with superconductivity being possible only in a narrow doping range between striped states and a nonsuperconducting metal."],"url":"http://arxiv.org/abs/2403.02073v1","category":"cond-mat.str-el"}
{"created":"2024-03-04 14:19:55","title":"On Efficient Approximation of the Maximum Distance to A Point Over an Intersection of Balls","abstract":"In this paper we study the NP-Hard problem of maximizing the distance over an intersection of balls to a given point. We expand the results found in \\cite{funcos1}, where the authors characterize the farthest in an intersection of balls $\\mathcal{Q}$ to the given point $C_0$ by constructing some intersection of halfspaces. In this paper, by slightly modifying the technique found in literature, we characterize the farthest in an intersection of balls $\\mathcal{Q}$ with another intersection of balls $\\mathcal{Q}_1$. As such, going backwards, we are naturally able to find the given intersection of balls $\\mathcal{Q}$ as the max indicator intersection of balls of another one $\\mathcal{Q}_{-1}$. By repeating the process, we find a sequence of intersection of balls $(\\mathcal{Q}_{i})_{i \\in \\mathbb{Z}}$, which has $\\mathcal{Q}$ as an element, namely $\\mathcal{Q}_{0}$ and show that $\\mathcal{Q}_{-\\infty} = \\mathcal{B}(C_0,R_0)$ where $R_0$ is the maximum distance from $C_0$ to a point in $\\mathcal{Q}$. As a final application of the proposed theory we give a polynomial algorithm for computing the maximum distance under an oracle which returns the volume of an intersection of balls, showing that the later is NP-Hard. Finally, we present a randomized method %of polynomial complexity which allows an approximation of the maximum distance.","sentences":["In this paper we study the NP-Hard problem of maximizing the distance over an intersection of balls to a given point.","We expand the results found in \\cite{funcos1}, where the authors characterize the farthest in an intersection of balls $\\mathcal{Q}$ to the given point $C_0$ by constructing some intersection of halfspaces.","In this paper, by slightly modifying the technique found in literature, we characterize the farthest in an intersection of balls $\\mathcal{Q}$ with another intersection of balls $\\mathcal{Q}_1$. As such, going backwards, we are naturally able to find the given intersection of balls $\\mathcal{Q}$ as the max indicator intersection of balls of another one $\\mathcal{Q}_{-1}$. By repeating the process, we find a sequence of intersection of balls $(\\mathcal{Q}_{i})_{i \\in \\mathbb{Z}}$, which has $\\mathcal{Q}$ as an element, namely $\\mathcal{Q}_{0}$ and show that $\\mathcal{Q}_{-\\infty} = \\mathcal{B}(C_0,R_0)$ where $R_0$ is the maximum distance from $C_0$ to a point in $\\mathcal{Q}$. As a final application of the proposed theory we give a polynomial algorithm for computing the maximum distance under an oracle which returns the volume of an intersection of balls, showing that the later is NP-Hard.","Finally, we present a randomized method %of polynomial complexity which allows an approximation of the maximum distance."],"url":"http://arxiv.org/abs/2403.02071v1","category":"cs.CG"}
{"created":"2024-03-04 14:17:30","title":"HyperPredict: Estimating Hyperparameter Effects for Instance-Specific Regularization in Deformable Image Registration","abstract":"Methods for medical image registration infer geometric transformations that align pairs/groups of images by maximising an image similarity metric. This problem is ill-posed as several solutions may have equivalent likelihoods, also optimising purely for image similarity can yield implausible transformations. For these reasons regularization terms are essential to obtain meaningful registration results. However, this requires the introduction of at least one hyperparameter often termed {\\lambda}, that serves as a tradeoff between loss terms. In some situations, the quality of the estimated transformation greatly depends on hyperparameter choice, and different choices may be required depending on the characteristics of the data. Analyzing the effect of these hyperparameters requires labelled data, which is not commonly available at test-time. In this paper, we propose a method for evaluating the influence of hyperparameters and subsequently selecting an optimal value for given image pairs. Our approach which we call HyperPredict, implements a Multi-Layer Perceptron that learns the effect of selecting particular hyperparameters for registering an image pair by predicting the resulting segmentation overlap and measure of deformation smoothness. This approach enables us to select optimal hyperparameters at test time without requiring labelled data, removing the need for a one-size-fits-all cross-validation approach. Furthermore, the criteria used to define optimal hyperparameter is flexible post-training, allowing us to efficiently choose specific properties. We evaluate our proposed method on the OASIS brain MR dataset using a recent deep learning approach(cLapIRN) and an algorithmic method(Niftyreg). Our results demonstrate good performance in predicting the effects of regularization hyperparameters and highlight the benefits of our image-pair specific approach to hyperparameter selection.","sentences":["Methods for medical image registration infer geometric transformations that align pairs/groups of images by maximising an image similarity metric.","This problem is ill-posed as several solutions may have equivalent likelihoods, also optimising purely for image similarity can yield implausible transformations.","For these reasons regularization terms are essential to obtain meaningful registration results.","However, this requires the introduction of at least one hyperparameter often termed {\\lambda}, that serves as a tradeoff between loss terms.","In some situations, the quality of the estimated transformation greatly depends on hyperparameter choice, and different choices may be required depending on the characteristics of the data.","Analyzing the effect of these hyperparameters requires labelled data, which is not commonly available at test-time.","In this paper, we propose a method for evaluating the influence of hyperparameters and subsequently selecting an optimal value for given image pairs.","Our approach which we call HyperPredict, implements a Multi-Layer Perceptron that learns the effect of selecting particular hyperparameters for registering an image pair by predicting the resulting segmentation overlap and measure of deformation smoothness.","This approach enables us to select optimal hyperparameters at test time without requiring labelled data, removing the need for a one-size-fits-all cross-validation approach.","Furthermore, the criteria used to define optimal hyperparameter is flexible post-training, allowing us to efficiently choose specific properties.","We evaluate our proposed method on the OASIS brain MR dataset using a recent deep learning approach(cLapIRN) and an algorithmic method(Niftyreg).","Our results demonstrate good performance in predicting the effects of regularization hyperparameters and highlight the benefits of our image-pair specific approach to hyperparameter selection."],"url":"http://arxiv.org/abs/2403.02069v1","category":"cs.CV"}
{"created":"2024-03-04 14:00:38","title":"Utility-based optimization of Fujikawa's basket trial design -- Pre-specified protocol of a comparison study","abstract":"Basket trial designs are a type of master protocol in which the same therapy is tested in several strata of the patient cohort. Many basket trial designs implement borrowing mechanisms. These allow sharing information between similar strata with the goal of increasing power in responsive strata while at the same time constraining type-I error inflation to a bearable threshold. These borrowing mechanisms can be tuned using numerical tuning parameters. The optimal choice of these tuning parameters is subject to research. In a comparison study using simulations and numerical calculations, we are planning to investigate the use of utility functions for quantifying the compromise between power and type-I error inflation and the use of numerical optimization algorithms for optimizing these functions. The present document is the protocol of this comparison study, defining each step of the study in accordance with the ADEMP scheme for pre-specification of simulation studies.","sentences":["Basket trial designs are a type of master protocol in which the same therapy is tested in several strata of the patient cohort.","Many basket trial designs implement borrowing mechanisms.","These allow sharing information between similar strata with the goal of increasing power in responsive strata while at the same time constraining type-I error inflation to a bearable threshold.","These borrowing mechanisms can be tuned using numerical tuning parameters.","The optimal choice of these tuning parameters is subject to research.","In a comparison study using simulations and numerical calculations, we are planning to investigate the use of utility functions for quantifying the compromise between power and type-I error inflation and the use of numerical optimization algorithms for optimizing these functions.","The present document is the protocol of this comparison study, defining each step of the study in accordance with the ADEMP scheme for pre-specification of simulation studies."],"url":"http://arxiv.org/abs/2403.02058v1","category":"stat.ME"}
{"created":"2024-03-04 13:57:37","title":"Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism","abstract":"Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers. This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using LLMs called Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.","sentences":["Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers.","This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems.","We introduce a novel population-based method for numerical optimization using LLMs called Language-Model-Based Evolutionary Optimizer (LEO).","Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization.","We compare our method to several gradient-based and gradient-free optimization approaches.","While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling.","We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions."],"url":"http://arxiv.org/abs/2403.02054v1","category":"cs.AI"}
{"created":"2024-03-05 18:15:20","title":"Searching for globular clusters in the inner halo of the Circinus galaxy","abstract":"In this study, we search for Globular Clusters (GCs) in the inner halo of the Circinus galaxy using a combination of observational data. Our dataset includes observations from the VISTA Variables in the V\\'ia L\\'actea Extended Survey (VVVX), optical data from Gaia Release 3 (DR3), and observations from the Dark Energy Camera (DECam). These multiple data sources provide a comprehensive basis for our analysis. Our search was concentrated within a 50 kpc radius from the centre, leading to the identification of 93 sources that met our established criteria. To ensure the reliability of our findings, we conducted multiple examinations for sample contamination. These examinations incorporated tests based on Gaia Astrometric Excess Noise (AEN), the Blue Photometer (BP)/Red Photometer (RP) Excess Factor (BRexcess), as well as comparisons with stellar population models.   This analysis confidently classified 41 sources as genuine GCs, as they successfully passed both the 3$\\sigma$ Gaia AEN and BRexcess tests. We used the ISHAPE program to determine the structural parameters (half-light radii) of the GC candidates, with a peak effective radius of 4$\\pm$ 0.5 pc. The catalogue mainly consists of bright GCs. Relationships between colour, size, and distance were found in the GC candidates, alongside confirmation of bi-modality in colour distributions.","sentences":["In this study, we search for Globular Clusters (GCs) in the inner halo of the Circinus galaxy using a combination of observational data.","Our dataset includes observations from the VISTA Variables in the V\\'ia L\\'actea Extended Survey (VVVX), optical data from Gaia Release 3 (DR3), and observations from the Dark Energy Camera (DECam).","These multiple data sources provide a comprehensive basis for our analysis.","Our search was concentrated within a 50 kpc radius from the centre, leading to the identification of 93 sources that met our established criteria.","To ensure the reliability of our findings, we conducted multiple examinations for sample contamination.","These examinations incorporated tests based on Gaia Astrometric Excess Noise (AEN), the Blue Photometer (BP)/Red Photometer (RP) Excess Factor (BRexcess), as well as comparisons with stellar population models.   ","This analysis confidently classified 41 sources as genuine GCs, as they successfully passed both the 3$\\sigma$ Gaia AEN and BRexcess tests.","We used the ISHAPE program to determine the structural parameters (half-light radii) of the GC candidates, with a peak effective radius of 4$\\pm$ 0.5 pc.","The catalogue mainly consists of bright GCs.","Relationships between colour, size, and distance were found in the GC candidates, alongside confirmation of bi-modality in colour distributions."],"url":"http://arxiv.org/abs/2403.03177v1","category":"astro-ph.GA"}
{"created":"2024-03-05 17:49:06","title":"The Fast Stochastic Matching Pursuit for Neutrino and Dark Matter Experiments","abstract":"Photomultiplier tubes (PMT) are widely deployed at neutrino and dark matter experiments for photon counting. When multiple photons hit a PMT consecutively, their photo-electron (PE) pulses pile up to hinder the precise measurements of the count and timings. We introduce Fast Stochastic Matching Pursuit (FSMP) to analyze the PMT signal waveforms into individual PEs with the strategy of reversible-jump Markov-chain Monte Carlo. We demonstrate that FSMP improves the energy and time resolution of PMT-based experiments, gains acceleration on GPUs and is extensible to microchannel-plate (MCP) PMTs with jumbo-charge outputs. In the condition of our laboratory characterization of 8-inch MCP-PMTs, FSMP improves the energy resolution by up to 12% from the long-serving method of waveform integration.","sentences":["Photomultiplier tubes (PMT) are widely deployed at neutrino and dark matter experiments for photon counting.","When multiple photons hit a PMT consecutively, their photo-electron (PE) pulses pile up to hinder the precise measurements of the count and timings.","We introduce Fast Stochastic Matching Pursuit (FSMP) to analyze the PMT signal waveforms into individual PEs with the strategy of reversible-jump Markov-chain Monte Carlo.","We demonstrate that FSMP improves the energy and time resolution of PMT-based experiments, gains acceleration on GPUs and is extensible to microchannel-plate (MCP) PMTs with jumbo-charge outputs.","In the condition of our laboratory characterization of 8-inch MCP-PMTs, FSMP improves the energy resolution by up to 12% from the long-serving method of waveform integration."],"url":"http://arxiv.org/abs/2403.03156v1","category":"hep-ex"}
{"created":"2024-03-05 17:43:57","title":"The Density of Relic Neutrinos Near the Surface of Earth","abstract":"It has been claimed that matter effects cause an asymmetry in the density of relic neutrinos versus antineutrinos near the surface of Earth, of order $O(G_F^{1/2})\\sim 10^{-4}$, with the vertical extent $\\sim 10$m. We argue that the effect is of order $O(G_F)\\sim 10^{-8}$, with the vertical extent $\\sim 1$mm.","sentences":["It has been claimed that matter effects cause an asymmetry in the density of relic neutrinos versus antineutrinos near the surface of Earth, of order $O(G_F^{1/2})\\sim 10^{-4}$, with the vertical extent $\\sim 10$m.","We argue that the effect is of order $O(G_F)\\sim 10^{-8}$, with the vertical extent $\\sim 1$mm."],"url":"http://arxiv.org/abs/2403.03152v1","category":"hep-ph"}
{"created":"2024-03-05 17:23:44","title":"Enhanced beam-beam modeling to include longitudinal variation during weak-strong simulation","abstract":"Beam-beam interactions pose substantial challenges in the design and operation of circular colliders, significantly affecting their performance. In particular, the weak-strong simulation approach is pivotal for investigating single-particle dynamics during the collider design phase. This paper evaluates the limitations of existing models in weak-strong simulations, noting that while they accurately account for energy changes due to slingshot effects, they fail to incorporate longitudinal coordinate changes ($z$-variation). To address this gap, we introduce two novel transformations that enhance Hirata's original framework by including both $z$-variation and slingshot effect-induced energy changes. Through rigorous mathematical analysis and extensive weak-strong simulation studies, we validate the efficacy of these enhancements in achieving a more precise simulation of beam-beam interactions. Our results reveal that although $z$-variation constitutes a higher-order effect and does not substantially affect the emittance growth rate within the specific design parameters of the Electron-Ion Collider (EIC), the refined model offers improved accuracy, particularly in scenarios involving the interaction between beam-beam effects and other random diffusion processes, as well as in simulations incorporating realistic lattice models.","sentences":["Beam-beam interactions pose substantial challenges in the design and operation of circular colliders, significantly affecting their performance.","In particular, the weak-strong simulation approach is pivotal for investigating single-particle dynamics during the collider design phase.","This paper evaluates the limitations of existing models in weak-strong simulations, noting that while they accurately account for energy changes due to slingshot effects, they fail to incorporate longitudinal coordinate changes ($z$-variation).","To address this gap, we introduce two novel transformations that enhance Hirata's original framework by including both $z$-variation and slingshot effect-induced energy changes.","Through rigorous mathematical analysis and extensive weak-strong simulation studies, we validate the efficacy of these enhancements in achieving a more precise simulation of beam-beam interactions.","Our results reveal that although $z$-variation constitutes a higher-order effect and does not substantially affect the emittance growth rate within the specific design parameters of the Electron-Ion Collider (EIC), the refined model offers improved accuracy, particularly in scenarios involving the interaction between beam-beam effects and other random diffusion processes, as well as in simulations incorporating realistic lattice models."],"url":"http://arxiv.org/abs/2403.03137v1","category":"physics.acc-ph"}
{"created":"2024-03-05 17:22:59","title":"Entanglement Entropy of a Scalar Field in a Squeezed State","abstract":"We study the entanglement entropy within a spherical region for a free scalar field in a squeezed state in 3+1 dimensions. We show that, even for small squeezing, a volume term appears, whose coefficient is essentially independent of the field mass. This is in line with Page's argument that the entanglement entropy in an arbitrary quantum state is proportional to the number of degrees of freedom of the smaller subsystem. It follows that squeezed states can be considered as arbitrary quantum states, in contrast to the ground or coherent states that give rise to entanglement entropy that is dominated by a term proportional to the area of the entangling surface.","sentences":["We study the entanglement entropy within a spherical region for a free scalar field in a squeezed state in 3+1 dimensions.","We show that, even for small squeezing, a volume term appears, whose coefficient is essentially independent of the field mass.","This is in line with Page's argument that the entanglement entropy in an arbitrary quantum state is proportional to the number of degrees of freedom of the smaller subsystem.","It follows that squeezed states can be considered as arbitrary quantum states, in contrast to the ground or coherent states that give rise to entanglement entropy that is dominated by a term proportional to the area of the entangling surface."],"url":"http://arxiv.org/abs/2403.03136v1","category":"hep-th"}
{"created":"2024-03-05 17:14:25","title":"Probing Recoil Signatures of Inelastic Dark Matter","abstract":"Different dark matter (DM) candidates could have different types of DM-lepton and/or DM-quark interactions. For direct detection experiments, this leads to diversity in the recoil spectra, where both DM-electron and DM-nucleus scatterings may contribute. Furthermore, kinematic effects such as those of the inelastic scattering can also play an important role in shaping the recoil spectra. In this work, we systematically study signatures of the light exothermic inelastic DM from the recoil spectra including both the DM-electron scattering and Migdal effect. Such inelastic DM has mass around (sub-)GeV scale and the DM mass-splitting ranges from 1keV to 30keV. We analyze the direct detection sensitivities to such light inelastic DM. For different inelastic DM masses and mass-splittings, we find that the DM-electron recoil and Migdal effect can contribute significantly and differently to the direct detection signatures. Hence, it is important to perform combined analysis to include both the DM-electron recoil and Migdal effect. We further demonstrate that this analysis has strong impacts on the cosmological and laboratory bounds for the inelastic DM.","sentences":["Different dark matter (DM) candidates could have different types of DM-lepton and/or DM-quark interactions.","For direct detection experiments, this leads to diversity in the recoil spectra, where both DM-electron and DM-nucleus scatterings may contribute.","Furthermore, kinematic effects such as those of the inelastic scattering can also play an important role in shaping the recoil spectra.","In this work, we systematically study signatures of the light exothermic inelastic DM from the recoil spectra including both the DM-electron scattering and Migdal effect.","Such inelastic DM has mass around (sub-)GeV scale and the DM mass-splitting ranges from 1keV to 30keV. We analyze the direct detection sensitivities to such light inelastic DM.","For different inelastic DM masses and mass-splittings, we find that the DM-electron recoil and Migdal effect can contribute significantly and differently to the direct detection signatures.","Hence, it is important to perform combined analysis to include both the DM-electron recoil and Migdal effect.","We further demonstrate that this analysis has strong impacts on the cosmological and laboratory bounds for the inelastic DM."],"url":"http://arxiv.org/abs/2403.03128v1","category":"hep-ph"}
{"created":"2024-03-05 16:57:28","title":"Suppressed localization effects in topological insulator -- antiferromagnetic insulator thin film bilayers of (BiSb)$_2$Te$_3$-MnF$_2$","abstract":"Thin films of co-doped (BiSb)$_2$Te$_3$ (0001) are grown via molecular beam epitaxy on substrates of Al$_2$O$_3$ (0001), MgF$_2$ (110) and a thin film of the antiferromagnetic insulator MnF$_2$ (110). Magnetoconductivity measurements of these (BiSb)$_2$Te$_3$ films show weak antilocalization at low temperature that is suppressed in (BiSb)$_2$Te$_3$ films grown on a MnF$_2$ layer, an effect attributed either to enhanced magnetic scattering at the interface or a proximity induced energy gap at the Dirac point. The (BiSb)$_2$Te$_3$ films are fit to a model describing the magnetoconductivity corrections in the 2D Dirac surface state of the topological insulator, from which the approximate value of the Fermi velocity near the Dirac point is derived. The (BiSb)$_2$Te$_3$ - MnF$_2$ bilayer samples are fit to a model describing the crossover from weak antilocalization to weak localization due to magnetic doping, suggesting the opening of an energy gap at the Dirac point in the (BiSb)$_2$Te$_3$ due to proximity with the antiferromagnetic MnF$_2$.","sentences":["Thin films of co-doped (BiSb)$_2$Te$_3$ (0001) are grown via molecular beam epitaxy on substrates of Al$_2$O$_3$ (0001), MgF$_2$ (110) and a thin film of the antiferromagnetic insulator MnF$_2$ (110).","Magnetoconductivity measurements of these (BiSb)$_2$Te$_3$ films show weak antilocalization at low temperature that is suppressed in (BiSb)$_2$Te$_3$ films grown on a MnF$_2$ layer, an effect attributed either to enhanced magnetic scattering at the interface or a proximity induced energy gap at the Dirac point.","The (BiSb)$_2$Te$_3$ films are fit to a model describing the magnetoconductivity corrections in the 2D Dirac surface state of the topological insulator, from which the approximate value of the Fermi velocity near the Dirac point is derived.","The (BiSb)$_2$Te$_3$ - MnF$_2$ bilayer samples are fit to a model describing the crossover from weak antilocalization to weak localization due to magnetic doping, suggesting the opening of an energy gap at the Dirac point in the (BiSb)$_2$Te$_3$ due to proximity with the antiferromagnetic MnF$_2$."],"url":"http://arxiv.org/abs/2403.03116v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 16:06:00","title":"Resolving chiral transitions in Rydberg arrays with quantum Kibble-Zurek mechanism and finite-time scaling","abstract":"The experimental realization of the quantum Kibble-Zurek mechanism in arrays of trapped Rydberg atoms has brought the problem of commensurate-incommensurate transition back into the focus of active research. Relying on equilibrium simulations of finite intervals, direct chiral transitions at the boundary of the period-3 and period-4 phases have been predicted. Here, we study how these chiral transitions can be diagnosed experimentally with critical dynamics. We demonstrate that chiral transitions can be distinguished from the floating phases by comparing Kibble-Zurek dynamics on arrays with different numbers of atoms. Furthermore, by sweeping in the opposite direction and keeping track of the order parameter, we identify the location of conformal points. Finally, combining forward and backward sweeps, we extract all critical exponents characterizing the transition.","sentences":["The experimental realization of the quantum Kibble-Zurek mechanism in arrays of trapped Rydberg atoms has brought the problem of commensurate-incommensurate transition back into the focus of active research.","Relying on equilibrium simulations of finite intervals, direct chiral transitions at the boundary of the period-3 and period-4 phases have been predicted.","Here, we study how these chiral transitions can be diagnosed experimentally with critical dynamics.","We demonstrate that chiral transitions can be distinguished from the floating phases by comparing Kibble-Zurek dynamics on arrays with different numbers of atoms.","Furthermore, by sweeping in the opposite direction and keeping track of the order parameter, we identify the location of conformal points.","Finally, combining forward and backward sweeps, we extract all critical exponents characterizing the transition."],"url":"http://arxiv.org/abs/2403.03081v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 16:02:58","title":"$N$-jettiness soft function at next-to-next-to-leading order in perturbative QCD","abstract":"We derive a compact representation of the renormalized $N$-jettiness soft function that is free of infrared and collinear divergences through next-to-next-to-leading order in perturbative QCD. The number of hard partons $N$ is a parameter in the formula for the finite remainder. Cancellation of all infrared and collinear singularities between the bare soft function and its renormalization matrix in color space is demonstrated analytically.","sentences":["We derive a compact representation of the renormalized $N$-jettiness soft function that is free of infrared and collinear divergences through next-to-next-to-leading order in perturbative QCD.","The number of hard partons $N$ is a parameter in the formula for the finite remainder.","Cancellation of all infrared and collinear singularities between the bare soft function and its renormalization matrix in color space is demonstrated analytically."],"url":"http://arxiv.org/abs/2403.03078v1","category":"hep-ph"}
{"created":"2024-03-05 16:00:58","title":"Physical Properties and Kinematics of Dense Cores Associated with Regions of Massive Star Formation from the Southern Sky","abstract":"The results of spectral observations in the $\\sim 84-92$ GHz frequency range of six objects in the southern sky containing dense cores and associated with regions of massive stars and star clusters formation are presented. The observations were carried out with the MOPRA-22m radio telescope. Within the framework of the local thermodynamic equilibrium (LTE) approximation, the column densities and abundances of the H$^{13}$CN, H$^{13}$CO$^+$, HN$^{13}$C, HC$_3$N, c-C$_3$H$_2$, SiO, CH$_3$C$_2$H and CH$_3$CN molecules are calculated. Estimates of kinetic temperatures ($\\sim 30-50$ K), sizes of emission regions ($\\sim 0.2-3.1$ pc) and virial masses ($\\sim 70-4600~M_{\\odot}$) are obtained. The line widths in the three cores decrease with increasing distance from the center. In four cores, asymmetry in the profiles of the optically thick lines HCO$^+$(1-0) and HCN(1-0) is observed, indicating the presence of systematic motions along the line of sight. In two cases, the asymmetry can be caused by contraction of gas. The model spectral maps of HCO$^+$(1-0) and H$^{13}$CO$^+$(1-0), obtained within the framework of the non-LTE spherically symmetric model, are fitted into the observed ones. The radial profiles of density ($\\propto r^{-1.6}$), turbulent velocity ($\\propto r^{-0.2}$), and contraction velocity ($\\propto r^{0.5}$) in the G268.42--0.85 core have been calculated. The contraction velocity profile differs from that expected both in the case of free fall of gas onto a protostar ($\\propto r^{-0.5}$), and in the case of global core collapse (contraction velocity does not depend on distance). A discussion of the obtained results is provided.","sentences":["The results of spectral observations in the $\\sim 84-92$ GHz frequency range of six objects in the southern sky containing dense cores and associated with regions of massive stars and star clusters formation are presented.","The observations were carried out with the MOPRA-22m radio telescope.","Within the framework of the local thermodynamic equilibrium (LTE) approximation, the column densities and abundances of the H$^{13}$CN, H$^{13}$CO$^+$, HN$^{13}$C, HC$_3$N, c-C$_3$H$_2$, SiO, CH$_3$C$_2$H and CH$_3$CN molecules are calculated.","Estimates of kinetic temperatures ($\\sim 30-50$ K), sizes of emission regions ($\\sim 0.2-3.1$ pc) and virial masses ($\\sim 70-4600~M_{\\odot}$) are obtained.","The line widths in the three cores decrease with increasing distance from the center.","In four cores, asymmetry in the profiles of the optically thick lines HCO$^+$(1-0) and HCN(1-0) is observed, indicating the presence of systematic motions along the line of sight.","In two cases, the asymmetry can be caused by contraction of gas.","The model spectral maps of HCO$^+$(1-0) and H$^{13}$CO$^+$(1-0), obtained within the framework of the non-LTE spherically symmetric model, are fitted into the observed ones.","The radial profiles of density ($\\propto r^{-1.6}$), turbulent velocity ($\\propto r^{-0.2}$), and contraction velocity ($\\propto r^{0.5}$) in the G268.42--0.85 core have been calculated.","The contraction velocity profile differs from that expected both in the case of free fall of gas onto a protostar ($\\propto r^{-0.5}$), and in the case of global core collapse (contraction velocity does not depend on distance).","A discussion of the obtained results is provided."],"url":"http://arxiv.org/abs/2403.03074v1","category":"astro-ph.GA"}
{"created":"2024-03-05 15:55:44","title":"Tight stellar binaries favour active longitudes at sub- and anti-stellar points","abstract":"Stellar binaries are ubiquitous in the galaxy and a laboratory for astrophysical effects. We use TESS to study photometric modulations in the lightcurves of 162 unequal mass eclipsing binaries from the EBLM (Eclipsing Binary Low Mass) survey, comprising F/G/K primaries and M-dwarf secondaries. We detect modulations on 81 eclipsing binaries. We catalog the rotation rates of the primary star in 69 binaries and discover 17 ellipsoidal variables. In a large portion (at least $\\sim 51\\%$) of our sample, we detect photometric modulations consistent with two over-densities of spots on the primary star that are roughly $180^{\\circ}$ apart. We show that these so-called active longitudes are preferentially at the sub- and anti-stellar points on the primary star. Physically, this means that the spots on the primary star preferentially face directly towards and away from the secondary star.","sentences":["Stellar binaries are ubiquitous in the galaxy and a laboratory for astrophysical effects.","We use TESS to study photometric modulations in the lightcurves of 162 unequal mass eclipsing binaries from the EBLM (Eclipsing Binary Low Mass) survey, comprising F/G/K primaries and M-dwarf secondaries.","We detect modulations on 81 eclipsing binaries.","We catalog the rotation rates of the primary star in 69 binaries and discover 17 ellipsoidal variables.","In a large portion (at least $\\sim 51\\%$) of our sample, we detect photometric modulations consistent with two over-densities of spots on the primary star that are roughly $180^{\\circ}$ apart.","We show that these so-called active longitudes are preferentially at the sub- and anti-stellar points on the primary star.","Physically, this means that the spots on the primary star preferentially face directly towards and away from the secondary star."],"url":"http://arxiv.org/abs/2403.03065v1","category":"astro-ph.SR"}
{"created":"2024-03-05 15:26:18","title":"Internal consistency of multi-tier $GW$+EDMFT","abstract":"The multi-tier $GW$+EDMFT scheme is an ab-initio method for calculating the electronic structure of correlated materials. While the approach is free from ad-hoc parameters, it requires a selection of appropriate energy windows for describing low-energy and strongly correlated physics. In this study, we test the consistency of the multi-tier description by considering different low-energy windows for a series of cubic SrXO$_3$ (X=V,Cr,Mn) perovskites. Specifically, we compare the 3-orbital $t_{2g}$ model, the 5-orbital $t_{2g}+e_g$ model, the 12-orbital $t_{2g}+O_p$ model, and (in the case of SrVO$_3$) the 14-orbital $t_{2g}+e_g+O_p$ model and compare the results to available photoemission and X-ray absorption measurements. The multi-tier method yields consistent results for the $t_{2g}$ and $t_{2g}+e_g$ low-energy windows, while the models with $O_p$ states produce stronger correlation effects and mostly agree well with experiment, especially in the unoccupied part of the spectrum.   We also discuss the consistency between the fermionic and bosonic spectral functions and the physical origin of satellite features, and present momentum-resolved charge susceptibilities.","sentences":["The multi-tier $GW$+EDMFT scheme is an ab-initio method for calculating the electronic structure of correlated materials.","While the approach is free from ad-hoc parameters, it requires a selection of appropriate energy windows for describing low-energy and strongly correlated physics.","In this study, we test the consistency of the multi-tier description by considering different low-energy windows for a series of cubic SrXO$_3$ (X=V,Cr,Mn) perovskites.","Specifically, we compare the 3-orbital $t_{2g}$ model, the 5-orbital $t_{2g}+e_g$ model, the 12-orbital $t_{2g}+O_p$ model, and (in the case of SrVO$_3$) the 14-orbital $t_{2g}+e_g+O_p$ model and compare the results to available photoemission and X-ray absorption measurements.","The multi-tier method yields consistent results for the $t_{2g}$ and $t_{2g}+e_g$ low-energy windows, while the models with $O_p$ states produce stronger correlation effects and mostly agree well with experiment, especially in the unoccupied part of the spectrum.   ","We also discuss the consistency between the fermionic and bosonic spectral functions and the physical origin of satellite features, and present momentum-resolved charge susceptibilities."],"url":"http://arxiv.org/abs/2403.03044v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 15:25:12","title":"Orbital torque switching in perpendicularly magnetized materials","abstract":"The orbital Hall effect in light materials has attracted considerable attention for developing novel orbitronic devices. Here we investigate the orbital torque efficiency and demonstrate the switching of the perpendicularly magnetized materials through the orbital Hall material (OHM), i.e., Zirconium (Zr). The orbital torque efficiency of approximately 0.78 is achieved in the Zr OHM with the perpendicularly magnetized [Co/Pt]3 sample, which significantly surpasses that of the perpendicularly magnetized CoFeB/Gd/CoFeB sample (approximately 0.04). Such notable difference is attributed to the different spin-orbit correlation strength between the [Co/Pt]3 sample and the CoFeB/Gd/CoFeB sample, which has been confirmed through the theoretical calculations. Furthermore, the full magnetization switching of the [Co/Pt]3 sample with a switching current density of approximately 2.6x106 A/cm2 has been realized through Zr, which even outperforms that of the W spin Hall material. Our finding provides a guideline to understand orbital torque efficiency and paves the way to develop energy-efficient orbitronic devices.","sentences":["The orbital Hall effect in light materials has attracted considerable attention for developing novel orbitronic devices.","Here we investigate the orbital torque efficiency and demonstrate the switching of the perpendicularly magnetized materials through the orbital Hall material (OHM), i.e., Zirconium (Zr).","The orbital torque efficiency of approximately 0.78 is achieved in the Zr OHM with the perpendicularly magnetized [Co/Pt]3 sample, which significantly surpasses that of the perpendicularly magnetized CoFeB/Gd/CoFeB sample (approximately 0.04).","Such notable difference is attributed to the different spin-orbit correlation strength between the [Co/Pt]3 sample and the CoFeB/Gd/CoFeB sample, which has been confirmed through the theoretical calculations.","Furthermore, the full magnetization switching of the [Co/Pt]3 sample with a switching current density of approximately 2.6x106 A/cm2 has been realized through Zr, which even outperforms that of the W spin Hall material.","Our finding provides a guideline to understand orbital torque efficiency and paves the way to develop energy-efficient orbitronic devices."],"url":"http://arxiv.org/abs/2403.03043v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-05 15:20:50","title":"The height gap of planar Brownian motion is $\\frac{5}\u03c0$","abstract":"We show that the occupation measure of planar Brownian motion exhibits a constant height gap of $5/\\pi$ across its outer boundary. This property bears similarities with the celebrated results of Schramm--Sheffield [18] and Miller--Sheffield [12] concerning the height gap of the Gaussian free field across SLE$_4$/CLE$_4$ curves. Heuristically, our result can also be thought of as the $\\theta \\to 0^+$ limit of the height gap property of a field built out of a Brownian loop soup with subcritical intensity $\\theta>0$, proved in our recent paper [3]. To obtain the explicit value of the height gap, we rely on the computation by Garban and Trujillo Ferreras [1] of the expected area of the domain delimited by the outer boundary of a Brownian bridge.","sentences":["We show that the occupation measure of planar Brownian motion exhibits a constant height gap of $5/\\pi$ across its outer boundary.","This property bears similarities with the celebrated results of Schramm--","Sheffield","[18] and Miller--Sheffield","[12] concerning the height gap of the Gaussian free field across SLE$_4$/CLE$_4$ curves.","Heuristically, our result can also be thought of as the $\\theta \\to 0^+$ limit of the height gap property of a field built out of a Brownian loop soup with subcritical intensity $\\theta>0$, proved in our recent paper [3].","To obtain the explicit value of the height gap, we rely on the computation by Garban and Trujillo Ferreras","[1] of the expected area of the domain delimited by the outer boundary of a Brownian bridge."],"url":"http://arxiv.org/abs/2403.03040v1","category":"math.PR"}
{"created":"2024-03-05 14:47:02","title":"Interannual Magneto-Coriolis modes and their sensitivity on the magnetic field within the Earth's core","abstract":"Linear modes for which the Coriolis acceleration is almost entirely in balance with the Lorentz force are called Magneto-Coriolis (MC) modes. These MC modes are thought to exist in Earth's liquid outer core and could therefore contribute to the variations observed in Earth's magnetic field. The background state on which these waves ride is assumed here to be static and defined by a prescribed magnetic field and zero flow. We introduce a new computational tool to efficiently compute solutions to the related eigenvalue problem, and study the affect of a range of both axisymmetric and non-axisymmetric background magnetic fields on the MC modes. We focus on a hierarchy of conditions that sequentially partition the numerous computed modes into those which are in principle observable, those which match a proxy for interannual geomagnetic signal over 1999-2023, and those which align with core-flows based on recent geomagnetic data. We found that the background field plays a crucial role in modal structure. In particular, we found no examples of axisymmetric background fields that support modes consistent with recent geomagnetic changes, but that some non-axisymmetric background fields do support consistent modes.","sentences":["Linear modes for which the Coriolis acceleration is almost entirely in balance with the Lorentz force are called Magneto-Coriolis (MC) modes.","These MC modes are thought to exist in Earth's liquid outer core and could therefore contribute to the variations observed in Earth's magnetic field.","The background state on which these waves ride is assumed here to be static and defined by a prescribed magnetic field and zero flow.","We introduce a new computational tool to efficiently compute solutions to the related eigenvalue problem, and study the affect of a range of both axisymmetric and non-axisymmetric background magnetic fields on the MC modes.","We focus on a hierarchy of conditions that sequentially partition the numerous computed modes into those which are in principle observable, those which match a proxy for interannual geomagnetic signal over 1999-2023, and those which align with core-flows based on recent geomagnetic data.","We found that the background field plays a crucial role in modal structure.","In particular, we found no examples of axisymmetric background fields that support modes consistent with recent geomagnetic changes, but that some non-axisymmetric background fields do support consistent modes."],"url":"http://arxiv.org/abs/2403.03011v1","category":"physics.geo-ph"}
{"created":"2024-03-05 14:26:43","title":"Radio outburst from a massive (proto)star. III. Unveiling the bipolarity of the radio jet from S255IR NIRS3","abstract":"We report new Very Large Array high-resolution observations of the radio jet from the outbursting high-mass star S255IR~NIRS3. The images at 6, 10, and 22.2 GHz confirm the existence of a new lobe emerging to the SW and expanding at a mean speed of ~285 km/s, about half as fast as the NE lobe. The new data allow us to reproduce both the morphology and the continuum spectrum of the two lobes with the model already adopted in our previous studies. We conclude that in all likelihood both lobes are powered by the same accretion outburst. We also find that the jet is currently fading down, recollimating, and recombining.","sentences":["We report new Very Large Array high-resolution observations of the radio jet from the outbursting high-mass star S255IR~NIRS3.","The images at 6, 10, and 22.2 GHz confirm the existence of a new lobe emerging to the SW and expanding at a mean speed of ~285 km/s, about half as fast as the NE lobe.","The new data allow us to reproduce both the morphology and the continuum spectrum of the two lobes with the model already adopted in our previous studies.","We conclude that in all likelihood both lobes are powered by the same accretion outburst.","We also find that the jet is currently fading down, recollimating, and recombining."],"url":"http://arxiv.org/abs/2403.02999v1","category":"astro-ph.SR"}
{"created":"2024-03-05 13:59:21","title":"Doubly Abductive Counterfactual Inference for Text-based Image Editing","abstract":"We study text-based image editing (TBIE) of a single image by counterfactual inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one. Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image fine-tuning. To this end, we propose a Doubly Abductive Counterfactual inference framework (DAC). We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details. Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction. Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit back to post-edit, thereby accomplishing the edit. Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity. Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, style transfer, and facial change, which are extensively validated in both qualitative and quantitative evaluations. Codes are in https://github.com/xuesong39/DAC.","sentences":["We study text-based image editing (TBIE) of a single image by counterfactual inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one.","Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image fine-tuning.","To this end, we propose a Doubly Abductive Counterfactual inference framework (DAC).","We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details.","Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction.","Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit back to post-edit, thereby accomplishing the edit.","Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity.","Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, style transfer, and facial change, which are extensively validated in both qualitative and quantitative evaluations.","Codes are in https://github.com/xuesong39/DAC."],"url":"http://arxiv.org/abs/2403.02981v1","category":"cs.CV"}
{"created":"2024-03-05 13:55:43","title":"Ensemble Averages of $\\mathbb{Z}_2$ Orbifold Classes of Narain CFTs","abstract":"In this work we study families of $\\mathbb{Z}_2$ orbifolds of toroidal conformal field theories based on both factorizable and non-factorizable target space tori. For these classes of theories, we analyze their moduli spaces, and compute their partition functions. Building on previous work, we express the calculated partition functions in terms of suitable Siegel-Narain theta functions that allow us to determine their ensemble averages. We express the derived averaged partition functions of the studied families of conformal field theories in a manifest modular invariant finite sum of products of real analytic Eisenstein series. We speculate on a tentative holographic three-dimensional dual bulk interpretations for the considered $\\mathbb{Z}_2$ orbifold classes of ensembles of conformal field theories.","sentences":["In this work we study families of $\\mathbb{Z}_2$ orbifolds of toroidal conformal field theories based on both factorizable and non-factorizable target space tori.","For these classes of theories, we analyze their moduli spaces, and compute their partition functions.","Building on previous work, we express the calculated partition functions in terms of suitable Siegel-Narain theta functions that allow us to determine their ensemble averages.","We express the derived averaged partition functions of the studied families of conformal field theories in a manifest modular invariant finite sum of products of real analytic Eisenstein series.","We speculate on a tentative holographic three-dimensional dual bulk interpretations for the considered $\\mathbb{Z}_2$ orbifold classes of ensembles of conformal field theories."],"url":"http://arxiv.org/abs/2403.02976v1","category":"hep-th"}
{"created":"2024-03-05 13:20:52","title":"Front-end electronics development of large-area SiPM arrays for high-precision single-photon time measurement","abstract":"TRopIcal DEep-sea Neutrino Telescope (TRIDENT) plans to incorporate silicon photomultipliers (SiPMs) with superior time resolution in addition to photomultiplier tubes (PMTs) into its detection units, namely hybrid Optical Digital Modules (hDOMs), to improve its angular resolution.   However, the time resolution significantly degrades for large-area SiPMs due to the increased parasitic capacitance, posing significant challenges for the readout electronics of SiPMs in hDOM.   We designed the front-end readout electronics for large-area SiPM arrays dedicated to high-precision time measurements, which consists of a high-speed pre-amplifier based on transformers (MABA-007159) and radio frequency (RF) amplifiers (BGA2803), a series-parallel combination SiPM array with reduced capacitance, and an analog multi-channel summing circuit.   We measured the single photon time resolution (SPTR) of a $4\\times4$ SiPM (Hamamatsu S13360-3050PE) array ($12\\times12~\\mathrm{mm}^2$) of approximately 300 ps FWHM with a power consumption of less than 100 mW.   This front-end readout design enables the large-area SiPM array to achieve high-precision SPTR with low power consumption.","sentences":["TRopIcal DEep-sea Neutrino Telescope (TRIDENT) plans to incorporate silicon photomultipliers (SiPMs) with superior time resolution in addition to photomultiplier tubes (PMTs) into its detection units, namely hybrid Optical Digital Modules (hDOMs), to improve its angular resolution.   ","However, the time resolution significantly degrades for large-area SiPMs due to the increased parasitic capacitance, posing significant challenges for the readout electronics of SiPMs in hDOM.   ","We designed the front-end readout electronics for large-area SiPM arrays dedicated to high-precision time measurements, which consists of a high-speed pre-amplifier based on transformers (MABA-007159) and radio frequency (RF) amplifiers (BGA2803), a series-parallel combination SiPM array with reduced capacitance, and an analog multi-channel summing circuit.   ","We measured the single photon time resolution (SPTR) of a $4\\times4$ SiPM (Hamamatsu S13360-3050PE) array ($12\\times12~\\mathrm{mm}^2$) of approximately 300 ps FWHM with a power consumption of less than 100 mW.   This front-end readout design enables the large-area SiPM array to achieve high-precision SPTR with low power consumption."],"url":"http://arxiv.org/abs/2403.02948v1","category":"physics.ins-det"}
{"created":"2024-03-05 13:13:57","title":"Transcription factor clusters as information transfer agents","abstract":"Deciphering how genes interpret information from the concentration of transcription factors (TFs) within the cell nucleus remains a fundamental question in gene regulation. Recent advancements have unveiled the heterogeneous distribution of TF molecules in the nucleus, posing challenges to the precise decoding of concentration signals. To explore this phenomenon, we employ high-resolution single-cell imaging of a fluorescently tagged TF protein, Bicoid, in living fly embryos. We show that accumulation of Bicoid in submicron clusters preserves the spatial information of the maternal Bicoid gradient, and that cluster intensity, size, and frequency offer remarkably precise spatial cues. We further discover that various known gene targets of Bicoid activation colocalize with clusters and that for the target gene Hunchback, this colocalization is dependent on its enhancer binding affinity. Modeling information transfer through these clusters suggests that clustering offers a more rapid sensing mechanism for global nuclear concentrations than freely diffusing TF molecules detected by simple enhancers.","sentences":["Deciphering how genes interpret information from the concentration of transcription factors (TFs) within the cell nucleus remains a fundamental question in gene regulation.","Recent advancements have unveiled the heterogeneous distribution of TF molecules in the nucleus, posing challenges to the precise decoding of concentration signals.","To explore this phenomenon, we employ high-resolution single-cell imaging of a fluorescently tagged TF protein, Bicoid, in living fly embryos.","We show that accumulation of Bicoid in submicron clusters preserves the spatial information of the maternal Bicoid gradient, and that cluster intensity, size, and frequency offer remarkably precise spatial cues.","We further discover that various known gene targets of Bicoid activation colocalize with clusters and that for the target gene Hunchback, this colocalization is dependent on its enhancer binding affinity.","Modeling information transfer through these clusters suggests that clustering offers a more rapid sensing mechanism for global nuclear concentrations than freely diffusing TF molecules detected by simple enhancers."],"url":"http://arxiv.org/abs/2403.02943v1","category":"physics.bio-ph"}
{"created":"2024-03-05 12:12:01","title":"Chemical abundances and deviations from the solar S/O ratio in the gas-phase ISM of galaxies based on infrared emission lines","abstract":"The infrared (IR) range is extremely useful in the context of chemical abundance studies of the gas-phase interstellar medium (ISM) due to the large variety of ionic species traced in this regime, the negligible effects from dust attenuation or temperature stratification, and the amount of data that has been and will be released in the coming years. Taking advantage of available IR emission lines, we analysed the chemical content of the gas-phase ISM in a sample of 131 Star-Forming Galaxies (SFGs) and 73 Active Galactic Nuclei (AGNs). Particularly, we derived the chemical content via their total oxygen abundance in combination with nitrogen and sulfur abundances, and with the ionisation parameter. We used a new version of the code HII-CHI-Mistry-IR v3.1 which allows us to estimate log(N/O), 12+log(O/H), log(U), and, for the first time, 12+log(S/H) from IR emission lines, which can be applied to both SFGs and AGNs. We tested that the estimations from this new version, that only considers sulfur lines for the derivation of sulfur abundances, are compatible with previous studies. While most of the SFGs and AGNs show solar log(N/O) abundances, we found a large spread in the log(S/O) relative abundances. Specifically, we found extremely low log(S/O) values (1/10th solar) in some SFGs and AGNs with solar-like oxygen abundances. This result warns against the use of optical and IR sulfur emission lines to estimate oxygen abundances when no prior estimation of log(S/O) is provided.","sentences":["The infrared (IR) range is extremely useful in the context of chemical abundance studies of the gas-phase interstellar medium (ISM) due to the large variety of ionic species traced in this regime, the negligible effects from dust attenuation or temperature stratification, and the amount of data that has been and will be released in the coming years.","Taking advantage of available IR emission lines, we analysed the chemical content of the gas-phase ISM in a sample of 131 Star-Forming Galaxies (SFGs) and 73 Active Galactic Nuclei (AGNs).","Particularly, we derived the chemical content via their total oxygen abundance in combination with nitrogen and sulfur abundances, and with the ionisation parameter.","We used a new version of the code HII-CHI-Mistry-IR v3.1 which allows us to estimate log(N/O), 12+log(O/H), log(U), and, for the first time, 12+log(S/H) from IR emission lines, which can be applied to both SFGs and AGNs.","We tested that the estimations from this new version, that only considers sulfur lines for the derivation of sulfur abundances, are compatible with previous studies.","While most of the SFGs and AGNs show solar log(N/O) abundances, we found a large spread in the log(S/O) relative abundances.","Specifically, we found extremely low log(S/O) values (1/10th solar) in some SFGs and AGNs with solar-like oxygen abundances.","This result warns against the use of optical and IR sulfur emission lines to estimate oxygen abundances when no prior estimation of log(S/O) is provided."],"url":"http://arxiv.org/abs/2403.02903v1","category":"astro-ph.GA"}
{"created":"2024-03-05 11:43:18","title":"Numerical studies of triangulated vesicles with anisotropic membrane inclusions","abstract":"In this study, we implement the deviatoric curvature model to examine dynamically triangulated surfaces with anisotropic membrane inclusions. The Monte-Carlo numerical scheme is devised to not only minimize the total bending energy of the membrane but also the in-plane nematic order of the inclusions by considering the mismatch between the curvature of the membrane and the intrinsic curvature of the inclusion. Neighboring inclusions can either attract with nearest-neighbor interaction or with a nematic interaction derived from liquid crystal theory. Orientational order determines whether vesicles fully covered with inclusions result in bulbs connected by necks or long tubes. Remarkably, when inclusions on vesicles with no vacancies interact non-nematically, a spontaneous local order can lead to a bulb transition which may have implications in cell or organelle division. Furthermore we find that average nematic order is inversely proportional to the number of thin necks formed in the vesicles. Our method shows good convergence and is suitable for further upgrades, for example to vesicles constrained by volume.","sentences":["In this study, we implement the deviatoric curvature model to examine dynamically triangulated surfaces with anisotropic membrane inclusions.","The Monte-Carlo numerical scheme is devised to not only minimize the total bending energy of the membrane but also the in-plane nematic order of the inclusions by considering the mismatch between the curvature of the membrane and the intrinsic curvature of the inclusion.","Neighboring inclusions can either attract with nearest-neighbor interaction or with a nematic interaction derived from liquid crystal theory.","Orientational order determines whether vesicles fully covered with inclusions result in bulbs connected by necks or long tubes.","Remarkably, when inclusions on vesicles with no vacancies interact non-nematically, a spontaneous local order can lead to a bulb transition which may have implications in cell or organelle division.","Furthermore we find that average nematic order is inversely proportional to the number of thin necks formed in the vesicles.","Our method shows good convergence and is suitable for further upgrades, for example to vesicles constrained by volume."],"url":"http://arxiv.org/abs/2403.02885v1","category":"cond-mat.soft"}
{"created":"2024-03-05 11:39:19","title":"Algebraic aspects of holomorphic quantum modular forms","abstract":"Matrix-valued holomorphic quantum modular forms are intricate objects that arise in successive refinements of the Volume Conjecture of knots and involve three holomorphic, asymptotic and arithmetic objects. It is expected that the algebraic properties of these objects can be deduced from the algebraic properties of descendant state integrals, and we illustrate this for the case of the $(-2,3,7)$-pretzel knot.","sentences":["Matrix-valued holomorphic quantum modular forms are intricate objects that arise in successive refinements of the Volume Conjecture of knots and involve three holomorphic, asymptotic and arithmetic objects.","It is expected that the algebraic properties of these objects can be deduced from the algebraic properties of descendant state integrals, and we illustrate this for the case of the $(-2,3,7)$-pretzel knot."],"url":"http://arxiv.org/abs/2403.02880v1","category":"math.GT"}
{"created":"2024-03-05 11:38:29","title":"The bright black hole X-ray binary 4U 1543-47 during 2021 outburst. A clear state transition from super-Eddington to sub-Eddington accretion revealed by Insight-HXMT","abstract":"We present a detailed analysis of the observations with the Hard X-ray Modulation Telescope of the black hole X-ray transient 4U~1543-47 during its outburst in 2021. We find a clear state transition during the outburst decay of the source. Using previous measurements of the black-hole mass and distance to the source, the source luminosity during this transition is close to the Eddington limit. The light curves before and after the transition can be fitted by two exponential functions with short ($\\sim 16$ days) and long ($\\sim 130$ days) decay time scales, respectively. We detect strong reflection features in all observations that can be described with either the RelxillNS or Reflionx_bb reflection models, both of which have a black-body incident spectrum. In the super-Eddington state, we observe a Comptonized component characterized by a low electron temperature of approximately 2.0 keV. We suggest that this component appears exclusively within the inner radiation-pressure dominated region of the supercritical disk as a part of the intrinsic spectrum of the accretion disk itself. This feature vanishes as the source transitions into the sub-Eddington state. The emissivity index of the accretion disk in the reflection component is significantly different before and after the transition, $\\sim3.0$-$5.0$ and $\\sim7.0$-$9.0$ in the super- and sub-Eddington states, respectively. Based on the reflection geometry of returning disk radiation, the geometrically thicker the accretion disk, the smaller the emissivity index. Therefore, we propose that the transition is primarily driven by the change of the accretion flow from a supercritical to a thin disk configuration.","sentences":["We present a detailed analysis of the observations with the Hard X-ray Modulation Telescope of the black hole X-ray transient 4U~1543-47 during its outburst in 2021.","We find a clear state transition during the outburst decay of the source.","Using previous measurements of the black-hole mass and distance to the source, the source luminosity during this transition is close to the Eddington limit.","The light curves before and after the transition can be fitted by two exponential functions with short ($\\sim 16$ days) and long ($\\sim 130$ days) decay time scales, respectively.","We detect strong reflection features in all observations that can be described with either the RelxillNS or Reflionx_bb reflection models, both of which have a black-body incident spectrum.","In the super-Eddington state, we observe a Comptonized component characterized by a low electron temperature of approximately 2.0 keV.","We suggest that this component appears exclusively within the inner radiation-pressure dominated region of the supercritical disk as a part of the intrinsic spectrum of the accretion disk itself.","This feature vanishes as the source transitions into the sub-Eddington state.","The emissivity index of the accretion disk in the reflection component is significantly different before and after the transition, $\\sim3.0$-$5.0$ and $\\sim7.0$-$9.0$ in the super- and sub-Eddington states, respectively.","Based on the reflection geometry of returning disk radiation, the geometrically thicker the accretion disk, the smaller the emissivity index.","Therefore, we propose that the transition is primarily driven by the change of the accretion flow from a supercritical to a thin disk configuration."],"url":"http://arxiv.org/abs/2403.02874v1","category":"astro-ph.HE"}
{"created":"2024-03-05 09:22:56","title":"Cold Filaments Formed in Hot Wake Flows Uplifted by AGN Bubbles in Galaxy Clusters","abstract":"Multi-wavelength observations indicate that the intracluster medium in some galaxy clusters contains cold filaments, while their formation mechanism remains debated. Using hydrodynamic simulations, we show that cold filaments could naturally condense out of hot gaseous wake flows uplifted by the jet-inflated active galactic nucleus (AGN) bubbles. Consistent with observations, the simulated filaments extend to tens of kpc from the cluster center, with a representative mass of $\\rm 10^{8}- 10^{9}\\ M_{\\odot}$ for a typical AGN outburst energy of $10^{60}~ \\rm erg$. They show smooth velocity gradients, stretching typically from inner inflows to outer outflows with velocity dispersions of several hundred $\\rm km\\ s^{-1}$. The properties of cold filaments are affected substantially by jet properties. Compared to kinetic energy-dominated jets, thermal energy-dominated jets tend to produce longer cold filaments with higher masses. With the same jet energy, AGN jets with an earlier turn-on time, a lower jet base, or a higher power heat the cluster center more effectively, and produce shorter filaments at a much later epoch.","sentences":["Multi-wavelength observations indicate that the intracluster medium in some galaxy clusters contains cold filaments, while their formation mechanism remains debated.","Using hydrodynamic simulations, we show that cold filaments could naturally condense out of hot gaseous wake flows uplifted by the jet-inflated active galactic nucleus (AGN) bubbles.","Consistent with observations, the simulated filaments extend to tens of kpc from the cluster center, with a representative mass of $\\rm 10^{8}- 10^{9}\\ M_{\\odot}$ for a typical AGN outburst energy of $10^{60}~ \\rm erg$.","They show smooth velocity gradients, stretching typically from inner inflows to outer outflows with velocity dispersions of several hundred $\\rm km\\ s^{-1}$.","The properties of cold filaments are affected substantially by jet properties.","Compared to kinetic energy-dominated jets, thermal energy-dominated jets tend to produce longer cold filaments with higher masses.","With the same jet energy, AGN jets with an earlier turn-on time, a lower jet base, or a higher power heat the cluster center more effectively, and produce shorter filaments at a much later epoch."],"url":"http://arxiv.org/abs/2403.02807v1","category":"astro-ph.GA"}
{"created":"2024-03-05 09:17:01","title":"On Cepheid distances in the $H_0$ measurement","abstract":"Classical Cepheids were the first stellar standard candles and have played a crucial role for astronomical distance measurements ever since the discovery of the Leavitt law (period-luminosity relation). Enormous improvements in distance accuracy have been achieved since Hertzsprung's first application of Leavitt's law to measure the distance to the Small Magellanic Cloud in 1913, notably in very recent years thanks to a large data set of highly accurate space astrometry from the ESA mission Gaia. Complemented by homogeneous space photometry, Cepheids enable the most accurate distance estimates to galaxies hosting type-Ia supernovae up to approximately 70 Mpc distant. Here, I review the history of Cepheid distance measurements, open questions on the side of stellar astrophysics, and recent studies seeking to quantify and mitigate systematics with a view to further improve the accuracy on the Hubble constant. For example, the recently launched James Webb Space Telescope will enhance precision due to 4x lower sensitivity to source blending in crowded regions and greater sensitivity in dust-insensitive infrared bands. Future 30m-class telescopes could in principle further improve Cepheid distance measurements towards the Hubble flow, if technical challenges related to a continuously evolving instrument can be overcome.","sentences":["Classical Cepheids were the first stellar standard candles and have played a crucial role for astronomical distance measurements ever since the discovery of the Leavitt law (period-luminosity relation).","Enormous improvements in distance accuracy have been achieved since Hertzsprung's first application of Leavitt's law to measure the distance to the Small Magellanic Cloud in 1913, notably in very recent years thanks to a large data set of highly accurate space astrometry from the ESA mission Gaia.","Complemented by homogeneous space photometry, Cepheids enable the most accurate distance estimates to galaxies hosting type-Ia supernovae up to approximately 70 Mpc distant.","Here, I review the history of Cepheid distance measurements, open questions on the side of stellar astrophysics, and recent studies seeking to quantify and mitigate systematics with a view to further improve the accuracy on the Hubble constant.","For example, the recently launched James Webb Space Telescope will enhance precision due to 4x lower sensitivity to source blending in crowded regions and greater sensitivity in dust-insensitive infrared bands.","Future 30m-class telescopes could in principle further improve Cepheid distance measurements towards the Hubble flow, if technical challenges related to a continuously evolving instrument can be overcome."],"url":"http://arxiv.org/abs/2403.02801v1","category":"astro-ph.SR"}
{"created":"2024-03-05 08:07:10","title":"Tsirelson bounds for quantum correlations with indefinite causal order","abstract":"Quantum theory is in principle compatible with processes that violate causal inequalities, an analogue of Bell inequalities that constrain the correlations observed by a set of parties operating in a definite order. Since the introduction of causal inequalities, determining their maximum quantum violation, analogue to Tsirelson's bound, has remained an open problem. Here we provide a general method for bounding the violation of causal inequalities by arbitrary quantum processes with indefinite causal order. We prove that the maximum violation is generally smaller than the algebraic maximum, and determine a Tsirelson-like bound for the paradigmatic example of the Oreshkov-Brukner-Costa causal inequality. Surprisingly, we find that the algebraic maximum of arbitrary causal inequalities can be achieved by a new type of processes that allow for information to flow in an indefinite direction within the parties' laboratories. In the classification of the possible correlations, these processes play a similar role as the no-signalling processes in Bell scenarios.","sentences":["Quantum theory is in principle compatible with processes that violate causal inequalities, an analogue of Bell inequalities that constrain the correlations observed by a set of parties operating in a definite order.","Since the introduction of causal inequalities, determining their maximum quantum violation, analogue to Tsirelson's bound, has remained an open problem.","Here we provide a general method for bounding the violation of causal inequalities by arbitrary quantum processes with indefinite causal order.","We prove that the maximum violation is generally smaller than the algebraic maximum, and determine a Tsirelson-like bound for the paradigmatic example of the Oreshkov-Brukner-Costa causal inequality.","Surprisingly, we find that the algebraic maximum of arbitrary causal inequalities can be achieved by a new type of processes that allow for information to flow in an indefinite direction within the parties' laboratories.","In the classification of the possible correlations, these processes play a similar role as the no-signalling processes in Bell scenarios."],"url":"http://arxiv.org/abs/2403.02749v1","category":"quant-ph"}
{"created":"2024-03-05 07:48:49","title":"Implementation of the microscopic nuclear potential in the coupled channels calculations to study the fusion dynamics of Oxygen based reactions","abstract":"In the present work, we have incorporated the microscopic relativistic nuclear potential obtained from recently developed relativistic R3Y NN potential in the coupled channels code CCFULL to study the fusion dynamics. The R3Y NN-potential and the densities of interacting nuclei are obtained for the relativistic mean-field approach for the NL3$^*$ parameter set. It is to be noted that the R3Y NN potential can be expressed in terms of masses of the mesons and their couplings by considering the meson degrees of freedom within the relativistic mean field, which has a form similar to the widely used M3Y potential. We focused on the fusion cross-sections for $Oxygen$-based reactions with targets from different mass regions of the periodic table i.e. $^{16}$O + $^{24}$Mg, $^{18}$O + $^{24}$Mg, $^{16}$O + $^{148}$Sm, $^{16}$O + $^{176}$Hf, $^{16}$O + $^{176}$Yb, $^{16}$O + $^{182}$W, and $^{16}$O + $^{186}$W. A comparison is also made with the ones calculated using the nuclear potential obtained from the traditional Woods-Saxon potential and the widely used M3Y NN potential within CCFULL. The coupled channel calculations are performed with shape and rotational degrees of freedom to examine the fusion enhancement at below-barrier energies. It is observed from the calculations that the fusion cross-sections obtained using R3Y NN potential with rotational degrees of freedom are found to be more consistent with the experimental data than those for the M3Y and Woods-Saxon potentials mainly at below barrier energies.","sentences":["In the present work, we have incorporated the microscopic relativistic nuclear potential obtained from recently developed relativistic R3Y NN potential in the coupled channels code CCFULL to study the fusion dynamics.","The R3Y NN-potential and the densities of interacting nuclei are obtained for the relativistic mean-field approach for the NL3$^*$ parameter set.","It is to be noted that the R3Y NN potential can be expressed in terms of masses of the mesons and their couplings by considering the meson degrees of freedom within the relativistic mean field, which has a form similar to the widely used M3Y potential.","We focused on the fusion cross-sections for $Oxygen$-based reactions with targets from different mass regions of the periodic table i.e. $^{16}$O + $^{24}$Mg, $^{18}$O + $^{24}$Mg, $^{16}$O + $^{148}$Sm, $^{16}$O + $^{176}$Hf, $^{16}$O + $^{176}$Yb, $^{16}$O + $^{182}$W, and $^{16}$O + $^{186}$W. A comparison is also made with the ones calculated using the nuclear potential obtained from the traditional Woods-Saxon potential and the widely used M3Y NN potential within CCFULL.","The coupled channel calculations are performed with shape and rotational degrees of freedom to examine the fusion enhancement at below-barrier energies.","It is observed from the calculations that the fusion cross-sections obtained using R3Y NN potential with rotational degrees of freedom are found to be more consistent with the experimental data than those for the M3Y and Woods-Saxon potentials mainly at below barrier energies."],"url":"http://arxiv.org/abs/2403.02739v1","category":"nucl-th"}
{"created":"2024-03-05 07:38:04","title":"Attempt Constructing a Model of Grand Gauge-Higgs Unification with Family Unification","abstract":"We discuss a possibility whether a model of grand gauge-Higgs unification incorporating family unification in higher dimensions can be constructed. We first extend a five dimensional $SU(6)$ grand gauge-Higgs unification model to a five dimensional $SU(7)$ grand gauge-Higgs unification model compactified on an orbifold $S^1/Z_2$ to obtain three generations of quarks and leptons after symmetrybreaking of the larger family unified gauge group. A prescription of constructing a six dimensional $SU(N)$ grand gauge-Higgs unification model including a five dimensional $SU(7)$ grand gauge-Higgs unification after compactifying the sixth dimension on an orbifold $S^1/Z_2$ is given. We find a six dimensional $SU(14)$ grand gauge-Higgs unification model with a set of representations containing three generations of quarks and leptons.","sentences":["We discuss a possibility whether a model of grand gauge-Higgs unification incorporating family unification in higher dimensions can be constructed.","We first extend a five dimensional $SU(6)$ grand gauge-Higgs unification model to a five dimensional $SU(7)$ grand gauge-Higgs unification model compactified on an orbifold $S^1/Z_2$ to obtain three generations of quarks and leptons after symmetrybreaking of the larger family unified gauge group.","A prescription of constructing a six dimensional $SU(N)$ grand gauge-Higgs unification model including a five dimensional $SU(7)$ grand gauge-Higgs unification after compactifying the sixth dimension on an orbifold $S^1/Z_2$ is given.","We find a six dimensional $SU(14)$ grand gauge-Higgs unification model with a set of representations containing three generations of quarks and leptons."],"url":"http://arxiv.org/abs/2403.02731v1","category":"hep-ph"}
{"created":"2024-03-05 07:34:56","title":"Nuclear medium effects on the properties of $\u039b(1405)$","abstract":"We study the $\\Lambda(1405)$ resonance with $I(J^{P})=0(1/2^{-}) $ in the context of the pentaquark hypothesis in the nuclear medium. For the investigation of the influence of the nuclear medium on the physical parameters of $\\Lambda(1405)$, we propose a molecular-type structure involving $K^{-}p$ and $\\bar{K}^{0}n$ admixtures, correlated with the nuclear matter density. Our analysis reveals a substantial shift in both mass and residue, approximately $20\\%$ and $38\\%$, respectively. These findings have significant implications for experimental researchers aimed at identifying in-medium characteristics of hyperon resonances.","sentences":["We study the $\\Lambda(1405)$ resonance with $I(J^{P})=0(1/2^{-}) $ in the context of the pentaquark hypothesis in the nuclear medium.","For the investigation of the influence of the nuclear medium on the physical parameters of $\\Lambda(1405)$, we propose a molecular-type structure involving $K^{-}p$ and $\\bar{K}^{0}n$ admixtures, correlated with the nuclear matter density.","Our analysis reveals a substantial shift in both mass and residue, approximately $20\\%$ and $38\\%$, respectively.","These findings have significant implications for experimental researchers aimed at identifying in-medium characteristics of hyperon resonances."],"url":"http://arxiv.org/abs/2403.02728v1","category":"nucl-th"}
{"created":"2024-03-05 07:01:53","title":"FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View","abstract":"In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D object detection and bird's-eye view (BEV) semantic segmentation. Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance. However, the inference speed, crucial for running on an autonomous vehicle, is neglected. To this end, a new method, dubbed FastOcc, is proposed. By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy. Targeted at improving this component, the time-consuming 3D convolution network is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV convolution network and compensated by integrating the 3D voxel features interpolated from the original image features. Experiments on the Occ3D-nuScenes benchmark demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed.","sentences":["In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D object detection and bird's-eye view (BEV) semantic segmentation.","Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance.","However, the inference speed, crucial for running on an autonomous vehicle, is neglected.","To this end, a new method, dubbed FastOcc, is proposed.","By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy.","Targeted at improving this component, the time-consuming 3D convolution network is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV convolution network and compensated by integrating the 3D voxel features interpolated from the original image features.","Experiments on the Occ3D-nuScenes benchmark demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed."],"url":"http://arxiv.org/abs/2403.02710v1","category":"cs.CV"}
{"created":"2024-03-05 06:55:34","title":"Impact of (magneto-)thermoelectric effect on diffusion of conserved charges in hot and dense hadronic matter","abstract":"We investigate the thermoelectric effect, which describes the generation of an electric field induced by temperature and conserved charge chemical potential gradients, in the hot and dense hadronic matter created in heavy-ion collisions. Utilizing the Boltzmann kinetic theory within the repulsive mean-field hadron resonance gas model, we evaluate both the diffusion thermopower matrix and diffusion coefficient matrix for the baryon number ($B$), electric charge ($Q$), and strangeness ($S$).The Landau-Lifshitz frame is enforced in the derivation. We find that the thermoelectric effect hinders the diffusion processes of multiple conserved charges, particularly reducing the coupling between electric charge and baryon number (strangeness) in baryon (strangeness) diffusion. Based on the fact that the repulsive mean-field interactions between hadrons have a significant effect on the diffusion thermopower matrix and diffusion coefficient matrix in the baryon-rich region, we extend the investigation to include the impact of the magnetic field, analyzing the magneto-thermoelectric effect on both the diffusion coefficient matrix and the Hall-like diffusion coefficient matrix. The sensitivity of the magnetic field-dependent diffusion thermopower matrix and magneto-thermoelectric modified diffusion coefficient matrix to the choices of various transverse conditions is also studied.","sentences":["We investigate the thermoelectric effect, which describes the generation of an electric field induced by temperature and conserved charge chemical potential gradients, in the hot and dense hadronic matter created in heavy-ion collisions.","Utilizing the Boltzmann kinetic theory within the repulsive mean-field hadron resonance gas model, we evaluate both the diffusion thermopower matrix and diffusion coefficient matrix for the baryon number ($B$), electric charge ($Q$), and strangeness ($S$).The Landau-Lifshitz frame is enforced in the derivation.","We find that the thermoelectric effect hinders the diffusion processes of multiple conserved charges, particularly reducing the coupling between electric charge and baryon number (strangeness) in baryon (strangeness) diffusion.","Based on the fact that the repulsive mean-field interactions between hadrons have a significant effect on the diffusion thermopower matrix and diffusion coefficient matrix in the baryon-rich region, we extend the investigation to include the impact of the magnetic field, analyzing the magneto-thermoelectric effect on both the diffusion coefficient matrix and the Hall-like diffusion coefficient matrix.","The sensitivity of the magnetic field-dependent diffusion thermopower matrix and magneto-thermoelectric modified diffusion coefficient matrix to the choices of various transverse conditions is also studied."],"url":"http://arxiv.org/abs/2403.02705v1","category":"nucl-th"}
{"created":"2024-03-05 06:53:01","title":"Common neighborhood (signless) Laplacian spectrum and energy of CCC-graph","abstract":"In this paper, we consider commuting conjugacy class graph (abbreviated as CCC-graph) of a finite group $G$ which is a graph with vertex set $\\{x^G : x \\in G \\setminus Z(G)\\}$ (where $x^G$ denotes the conjugacy class containing $x$) and two distinct vertices $x^G$ and $y^G$ are joined by an edge if there exist some elements $x'\\in x^G$ and $y'\\in y^G$ such that they commute. We compute common neighborhood (signless) Laplacian spectrum and energy of CCC-graph of finite non-abelian groups whose central quotient is isomorphic to either $\\mathbb{Z}_p \\times \\mathbb{Z}_p$ (where $p$ is any prime) or the dihedral group $D_{2n}$ ($n \\geq 3$); and determine whether CCC-graphs of these groups are common neighborhood (signless) Laplacian hyperenergetic/borderenergetic. As a consequence, we characterize certain finite non-abelian groups viz. $D_{2n}$, $T_{4n}$, $U_{6n}$, $U_{(n, m)}$, $SD_{8n}$ and $V_{8n}$ such that their CCC-graphs are common neighborhood (signless) Laplacian hyperenergetic/borderenergetic.","sentences":["In this paper, we consider commuting conjugacy class graph (abbreviated as CCC-graph) of a finite group $G$ which is a graph with vertex set $\\{x^G : x \\in G \\setminus Z(G)\\}$ (where $x^G$ denotes the conjugacy class containing $x$) and two distinct vertices $x^G$ and $y^G$ are joined by an edge if there exist some elements $x'\\in x^G$ and $y'\\in y^G$ such that they commute.","We compute common neighborhood (signless) Laplacian spectrum and energy of CCC-graph of finite non-abelian groups whose central quotient is isomorphic to either $\\mathbb{Z}_p \\times \\mathbb{Z}_p$ (where $p$ is any prime) or the dihedral group $D_{2n}$ ($n \\geq 3$); and determine whether CCC-graphs of these groups are common neighborhood (signless) Laplacian hyperenergetic/borderenergetic.","As a consequence, we characterize certain finite non-abelian groups viz.","$D_{2n}$, $T_{4n}$, $U_{6n}$, $U_{(n, m)}$, $SD_{8n}$ and $V_{8n}$ such that their CCC-graphs are common neighborhood (signless) Laplacian hyperenergetic/borderenergetic."],"url":"http://arxiv.org/abs/2403.02703v1","category":"math.GR"}
{"created":"2024-03-05 06:38:31","title":"Aspects of superconformal symmetry","abstract":"In this thesis we study classical aspects of superconformal field theory via symmetry principles. Specifically, by employing the powerful setup of conformal superspace, we obtain a plethora of new results in the fields of geometric and higher symmetries, (super)conformal higher-spin theory and conformal supergravity. These findings open up numerous novel research pathways.","sentences":["In this thesis we study classical aspects of superconformal field theory via symmetry principles.","Specifically, by employing the powerful setup of conformal superspace, we obtain a plethora of new results in the fields of geometric and higher symmetries, (super)conformal higher-spin theory and conformal supergravity.","These findings open up numerous novel research pathways."],"url":"http://arxiv.org/abs/2403.02700v1","category":"hep-th"}
{"created":"2024-03-05 06:28:02","title":"Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment","abstract":"Conventional multi-hop fact verification models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets. Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or counterfactual reasoning. However, existing causal inference-based debiasing methods, which mainly formulate fact verification as a single-hop reasoning task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence. To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop fact verification from a causal perspective with front-door adjustment. Specifically, in the structural causal model, the reasoning path between the treatment (the input claim-evidence graph) and the outcome (the veracity label) is introduced as the mediator to block the confounder. With the front-door adjustment, the causal effect between the treatment and the outcome is decomposed into the causal effect between the treatment and the mediator, which is estimated by applying the idea of random walk, and the causal effect between the mediator and the outcome, which is estimated with normalized weighted geometric mean approximation. To investigate the effectiveness of the proposed method, an adversarial multi-hop fact verification dataset and a symmetric multi-hop fact verification dataset are proposed with the help of the large language model. Experimental results show that Causal Walk outperforms some previous debiasing methods on both existing datasets and the newly constructed datasets. Code and data will be released at https://github.com/zcccccz/CausalWalk.","sentences":["Conventional multi-hop fact verification models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets.","Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or counterfactual reasoning.","However, existing causal inference-based debiasing methods, which mainly formulate fact verification as a single-hop reasoning task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence.","To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop fact verification from a causal perspective with front-door adjustment.","Specifically, in the structural causal model, the reasoning path between the treatment (the input claim-evidence graph) and the outcome (the veracity label) is introduced as the mediator to block the confounder.","With the front-door adjustment, the causal effect between the treatment and the outcome is decomposed into the causal effect between the treatment and the mediator, which is estimated by applying the idea of random walk, and the causal effect between the mediator and the outcome, which is estimated with normalized weighted geometric mean approximation.","To investigate the effectiveness of the proposed method, an adversarial multi-hop fact verification dataset and a symmetric multi-hop fact verification dataset are proposed with the help of the large language model.","Experimental results show that Causal Walk outperforms some previous debiasing methods on both existing datasets and the newly constructed datasets.","Code and data will be released at https://github.com/zcccccz/CausalWalk."],"url":"http://arxiv.org/abs/2403.02698v1","category":"cs.CL"}
{"created":"2024-03-05 06:15:02","title":"Limits on scalar dark matter interactions from radiative corrections","abstract":"There is limited information about interaction strength of scalar dark matter candidate with hadrons and leptons for scalar particle mass exceeding $10^{-3}$ eV while its interaction with photon is well-studied. The scalar-photon coupling constant receives quantum corrections from one-loop Feynman diagrams which involve the scalar-lepton, scalar-quark and scalar-W boson vertices. We calculate these one-loop quantum corrections and find new limits on the scalar particle interactions with electron, muon, tau, quarks, nucleons, gluons and W boson for $m_{\\phi}<15$ MeV by re-purposing the results of experiments measuring the scalar-photon interaction. Limits on interactions with heavy leptons and quarks have been obtained for the first time, limits on other interactions in certain mass intervals are 2 to 15 orders of magnitude stronger than those presented in previous publications.","sentences":["There is limited information about interaction strength of scalar dark matter candidate with hadrons and leptons for scalar particle mass exceeding $10^{-3}$ eV while its interaction with photon is well-studied.","The scalar-photon coupling constant receives quantum corrections from one-loop Feynman diagrams which involve the scalar-lepton, scalar-quark and scalar-W boson vertices.","We calculate these one-loop quantum corrections and find new limits on the scalar particle interactions with electron, muon, tau, quarks, nucleons, gluons and W boson for $m_{\\phi}<15$ MeV by re-purposing the results of experiments measuring the scalar-photon interaction.","Limits on interactions with heavy leptons and quarks have been obtained for the first time, limits on other interactions in certain mass intervals are 2 to 15 orders of magnitude stronger than those presented in previous publications."],"url":"http://arxiv.org/abs/2403.02685v1","category":"hep-ph"}
{"created":"2024-03-05 05:45:39","title":"An Experimentally Benchmarked Geant4 SiPM-Based Scintillator Detector Simulation Platform for Gamma Ray Detection","abstract":"Radiation detection is vital for naturally occurring radioactive material detection, port and border monitoring, and homeland security operations. SiPM-based single-volume scintillator detectors offer a cost-effective and robust solution that enables timely radioactive site surveys, while retaining accuracy and sensitivity sufficient for isotope identification. Enhanced site surveying can be achieved by optimising scintillator detector design. In this work, a detailed GAGG:Ce, CLLBC:Ce, BGO, NaI:Tl, and CsI:Tl SiPM-based scintillator detector simulation platform was developed with the Monte Carlo radiation transport toolkit, Geant4, and experimentally benchmarked. This simulation platform successfully predicted the spectral features for selected gamma ray emitting isotopes with energies between 30 keV to 2 MeV. The full width half maximum (FWHM) and normalised cross-correlation coefficient (NCCC) between simulated and experimental energy spectra were also compared. The majority of simulated FWHM values reproduced the experimental results within 2% and the NCCC values demonstrated agreement between the simulated and experimental energy spectra. Discrepancies in these figures of merit can be attributed to detector signal processing electronics modelling, geometry approximations, and multiple Compton scattering within the detector and surrounding environment.","sentences":["Radiation detection is vital for naturally occurring radioactive material detection, port and border monitoring, and homeland security operations.","SiPM-based single-volume scintillator detectors offer a cost-effective and robust solution that enables timely radioactive site surveys, while retaining accuracy and sensitivity sufficient for isotope identification.","Enhanced site surveying can be achieved by optimising scintillator detector design.","In this work, a detailed GAGG:Ce, CLLBC:Ce, BGO, NaI:Tl, and CsI:Tl SiPM-based scintillator detector simulation platform was developed with the Monte Carlo radiation transport toolkit, Geant4, and experimentally benchmarked.","This simulation platform successfully predicted the spectral features for selected gamma ray emitting isotopes with energies between 30 keV to 2 MeV.","The full width half maximum (FWHM) and normalised cross-correlation coefficient (NCCC) between simulated and experimental energy spectra were also compared.","The majority of simulated FWHM values reproduced the experimental results within 2% and the NCCC values demonstrated agreement between the simulated and experimental energy spectra.","Discrepancies in these figures of merit can be attributed to detector signal processing electronics modelling, geometry approximations, and multiple Compton scattering within the detector and surrounding environment."],"url":"http://arxiv.org/abs/2403.02668v1","category":"physics.ins-det"}
{"created":"2024-03-05 04:57:10","title":"The Mass Gap of the Space-time and its Shape","abstract":"Snyder's quantum space-time which is Lorentz invariant is investigated. It is found that the quanta of space-time have a positive mass that is interpreted as a positive real mass gap of space-time. This mass gap is related to the minimal length of measurement which is provided by Snyder's algebra. Several reasons to consider the space-time quanta as a 24-cell are discussed. Geometric reasons include its self-duality property and its 24 vertices that may represent the standard model of elementary particles. The 24-cell symmetry group is the Weyl/Coxeter group of the $F_4$ group which was found recently to generate the gauge group of the standard model. It is found that 24-cell may provide a geometric interpretation of the mass generation, Avogadro number, color confinement, and the flatness of the observable universe. The phenomenology and consistency with measurements is discussed.","sentences":["Snyder's quantum space-time which is Lorentz invariant is investigated.","It is found that the quanta of space-time have a positive mass that is interpreted as a positive real mass gap of space-time.","This mass gap is related to the minimal length of measurement which is provided by Snyder's algebra.","Several reasons to consider the space-time quanta as a 24-cell are discussed.","Geometric reasons include its self-duality property and its 24 vertices that may represent the standard model of elementary particles.","The 24-cell symmetry group is the Weyl/Coxeter group of the $F_4$ group which was found recently to generate the gauge group of the standard model.","It is found that 24-cell may provide a geometric interpretation of the mass generation, Avogadro number, color confinement, and the flatness of the observable universe.","The phenomenology and consistency with measurements is discussed."],"url":"http://arxiv.org/abs/2403.02655v1","category":"hep-th"}
{"created":"2024-03-05 04:30:47","title":"The GMRT High-Resolution Southern Sky Survey for pulsars and transients -- VII: Timing of Spider MSP J1242-4712, A Bridge Between Redback and Black Widow Pulsars","abstract":"We present the timing solution for the 5.31-ms spider millisecond pulsar (MSP) J1242-4712, discovered with the GMRT. PSR J1242-4712 orbits a companion of minimum mass 0.08 M$_{\\odot}$ with an orbital period of 7.7 hrs and occupies a relatively unexplored region in the orbital period versus companion mass space. We did not detect gamma-ray pulsations for this MSP, and also could not identify the optical counterpart for PSR J1242-4712 in available optical/near-infrared data. The profile of J1242-4712 evolves with frequency showing a clear single component at lower frequencies and a three-component profile at 650 MHz. PSR J1242-4712 eclipses for a very short duration near superior conjunction (orbital phase ~ 0.23-0.25) below 360 MHz. Moreover, significant DM delays and errors in pulse times of arrivals are observed near inferior conjunction (orbital phase ~ 0.7), along with an observed eclipse in one epoch at 650 MHz. Observed eclipses and significant orbital period variability suggest that PSR J1242-4712 is possibly not a He-WD binary, but has a semi or non-degenerate companion, indicating that this is a ``spider\" MSP lying in a region between typical black widows and redbacks. This system may represent a distinct category of spider MSPs, displaying characteristics that bridge the gap between known black widow and redback MSPs.","sentences":["We present the timing solution for the 5.31-ms spider millisecond pulsar (MSP) J1242-4712, discovered with the GMRT.","PSR J1242-4712 orbits a companion of minimum mass 0.08 M$_{\\odot}$ with an orbital period of 7.7 hrs and occupies a relatively unexplored region in the orbital period versus companion mass space.","We did not detect gamma-ray pulsations for this MSP, and also could not identify the optical counterpart for PSR J1242-4712 in available optical/near-infrared data.","The profile of J1242-4712 evolves with frequency showing a clear single component at lower frequencies and a three-component profile at 650 MHz.","PSR J1242-4712 eclipses for a very short duration near superior conjunction (orbital phase ~ 0.23-0.25) below 360 MHz.","Moreover, significant DM delays and errors in pulse times of arrivals are observed near inferior conjunction (orbital phase ~ 0.7), along with an observed eclipse in one epoch at 650 MHz.","Observed eclipses and significant orbital period variability suggest that PSR J1242-4712 is possibly not a He-WD binary, but has a semi or non-degenerate companion, indicating that this is a ``spider\" MSP lying in a region between typical black widows and redbacks.","This system may represent a distinct category of spider MSPs, displaying characteristics that bridge the gap between known black widow and redback MSPs."],"url":"http://arxiv.org/abs/2403.02646v1","category":"astro-ph.HE"}
{"created":"2024-03-05 04:02:49","title":"Real-time portable muography with Hankuk Atmospheric-muon Wide Landscaping : HAWL","abstract":"Cosmic ray muons prove valuable across various fields, from particle physics experiments to non-invasive tomography, thanks to their high flux and exceptional penetrating capability. Utilizing a scintillator detector, one can effectively study the topography of mountains situated above tunnels and underground spaces. The Hankuk Atmospheric-muon Wide Landscaping (HAWL) project successfully charts the mountainous region of eastern Korea by measuring cosmic ray muons with a detector in motion. The real-time muon flux measurement shows a tunnel length accuracy of 6.5 %, with a detectable overburden range spanning from 8 to 400 meter-water-equivalent depth. This is the first real-time portable muon tomography.","sentences":["Cosmic ray muons prove valuable across various fields, from particle physics experiments to non-invasive tomography, thanks to their high flux and exceptional penetrating capability.","Utilizing a scintillator detector, one can effectively study the topography of mountains situated above tunnels and underground spaces.","The Hankuk Atmospheric-muon Wide Landscaping (HAWL) project successfully charts the mountainous region of eastern Korea by measuring cosmic ray muons with a detector in motion.","The real-time muon flux measurement shows a tunnel length accuracy of 6.5 %, with a detectable overburden range spanning from 8 to 400 meter-water-equivalent depth.","This is the first real-time portable muon tomography."],"url":"http://arxiv.org/abs/2403.02638v1","category":"hep-ex"}
{"created":"2024-03-05 03:55:55","title":"Spatially Non-Stationary XL-MIMO Channel Estimation: A Three-Layer Generalized Approximate Message Passing Method","abstract":"In this paper, channel estimation problem for extremely large-scale multi-input multi-output (XL-MIMO) systems is investigated with the considerations of the spherical wavefront effect and the spatially non-stationary (SnS) property. Due to the diversities of SnS characteristics among different propagation paths, the concurrent channel estimation of multiple paths becomes intractable. To address this challenge, we propose a two-phase channel estimation scheme. In the first phase, the angles of departure (AoDs) on the user side are estimated, and a carefully designed pilot transmission scheme enables the decomposition of the received signal from different paths. In the second phase, the subchannel estimation corresponding to different paths is formulated as a three-layer Bayesian inference problem. Specifically, the first layer captures block sparsity in the angular domain, the second layer promotes SnS property in the antenna domain, and the third layer decouples the subchannels from the observed signals. To efficiently facilitate Bayesian inference, we propose a novel three-layer generalized approximate message passing (TL-GAMP) algorithm based on structured variational massage passing and belief propagation rules. Simulation results validate the convergence and effectiveness of the proposed algorithm, showcasing its robustness to different channel scenarios.","sentences":["In this paper, channel estimation problem for extremely large-scale multi-input multi-output (XL-MIMO) systems is investigated with the considerations of the spherical wavefront effect and the spatially non-stationary (SnS) property.","Due to the diversities of SnS characteristics among different propagation paths, the concurrent channel estimation of multiple paths becomes intractable.","To address this challenge, we propose a two-phase channel estimation scheme.","In the first phase, the angles of departure (AoDs) on the user side are estimated, and a carefully designed pilot transmission scheme enables the decomposition of the received signal from different paths.","In the second phase, the subchannel estimation corresponding to different paths is formulated as a three-layer Bayesian inference problem.","Specifically, the first layer captures block sparsity in the angular domain, the second layer promotes SnS property in the antenna domain, and the third layer decouples the subchannels from the observed signals.","To efficiently facilitate Bayesian inference, we propose a novel three-layer generalized approximate message passing (TL-GAMP) algorithm based on structured variational massage passing and belief propagation rules.","Simulation results validate the convergence and effectiveness of the proposed algorithm, showcasing its robustness to different channel scenarios."],"url":"http://arxiv.org/abs/2403.02633v1","category":"cs.IT"}
{"created":"2024-03-05 03:30:18","title":"Stability investigations of de Sitter inflationary solutions in power-law extensions of the Starobinsky model","abstract":"In this paper, we would like to examine whether stable de Sitter inflationary solutions appear within power-law extensions of the Starobinsky model. In particular, we will address general constraints for the existence along with the stability of de Sitter inflationary solutions in a general case involving not only the Starobinsky $R^2$ term but also an additional power-law $R^n$ one. According to the obtained results, we will be able to identify which extension is more suitable for an early inflationary phase rather than a late-time cosmic acceleration phase. To be more specific, we will consider several values of $n$ to see whether the corresponding de Sitter inflationary solutions are stable or not.","sentences":["In this paper, we would like to examine whether stable de Sitter inflationary solutions appear within power-law extensions of the Starobinsky model.","In particular, we will address general constraints for the existence along with the stability of de Sitter inflationary solutions in a general case involving not only the Starobinsky $R^2$ term but also an additional power-law $R^n$ one.","According to the obtained results, we will be able to identify which extension is more suitable for an early inflationary phase rather than a late-time cosmic acceleration phase.","To be more specific, we will consider several values of $n$ to see whether the corresponding de Sitter inflationary solutions are stable or not."],"url":"http://arxiv.org/abs/2403.02623v1","category":"gr-qc"}
{"created":"2024-03-05 02:39:16","title":"Double Deeply Virtual Compton Scattering at Jefferson Lab Hall A","abstract":"This paper presents our project and perspectives to measure for the first time beam spin asymmetries from Double Deeply Virtual Compton Scattering in the $eP\\to e'P' \\mu^+\\mu^-$ reaction at Jefferson Lab. Our goal is to constrain the so-called Generalized Parton Distribution (GPDs) in a kinematic region that isn't accessible from other reactions, such as Deeply Virtual Compton Scattering, to allow for their extrapolation to \"zero skewness\", i.e. at a specific kinematic point enabling for tomographic interpretations of the nucleon's partonic structure. We are discussing DDVCS phenomenology and our approach, as well as our experimental project aimed at complementing the SoLID experiment at JLab Hall A with a new muon detector.","sentences":["This paper presents our project and perspectives to measure for the first time beam spin asymmetries from Double Deeply Virtual Compton Scattering in the $eP\\to e'P' \\mu^+\\mu^-$ reaction at Jefferson Lab.","Our goal is to constrain the so-called Generalized Parton Distribution (GPDs) in a kinematic region that isn't accessible from other reactions, such as Deeply Virtual Compton Scattering, to allow for their extrapolation to \"zero skewness\", i.e. at a specific kinematic point enabling for tomographic interpretations of the nucleon's partonic structure.","We are discussing DDVCS phenomenology and our approach, as well as our experimental project aimed at complementing the SoLID experiment at JLab Hall A with a new muon detector."],"url":"http://arxiv.org/abs/2403.02605v1","category":"nucl-ex"}
{"created":"2024-03-05 02:34:14","title":"Macroscopic neutrinoless double beta decay: long range quantum coherence","abstract":"We introduce the concept of ``macroscopic neutrinoless double beta decay\" (MDBD) for Majorana neutrinos. In this process an antineutrino produced by a nucleus undergoing beta decay, $X \\to Y + e^- + \\bar \\nu_e$, is absorbed as a neutrino by another identical $X$ nucleus via the inverse beta decay reaction, $\\nu_e + X \\to e^-+Y$. The distinct signature of MDBD is that the total kinetic energy of the two electrons equals twice the endpoint energy of single beta decay. The amplitude for MDBD, a coherent sum over the contribution of different mass states of the intermediate neutrinos, reflects quantum coherence over macroscopic distances, and is a new macroscopic quantum effect. We evaluate the rate of MDBD for a macroscopic sample of ``$X$\" material, e.g., tritium, acting both as the source and the target. The accidental background for MDBD originating from two separate single beta decays, which contains two final state neutrinos, can be readily rejected by measuring the energy of the detected two electrons. We discuss the similarities and differences between the MDBD and conventional neutrinoless double beta decay.","sentences":["We introduce the concept of ``macroscopic neutrinoless double beta decay\" (MDBD) for Majorana neutrinos.","In this process an antineutrino produced by a nucleus undergoing beta decay, $X \\to Y + e^- + \\bar \\nu_e$, is absorbed as a neutrino by another identical $X$ nucleus via the inverse beta decay reaction, $\\nu_e + X \\to e^-+Y$.","The distinct signature of MDBD is that the total kinetic energy of the two electrons equals twice the endpoint energy of single beta decay.","The amplitude for MDBD, a coherent sum over the contribution of different mass states of the intermediate neutrinos, reflects quantum coherence over macroscopic distances, and is a new macroscopic quantum effect.","We evaluate the rate of MDBD for a macroscopic sample of ``$X$\" material, e.g., tritium, acting both as the source and the target.","The accidental background for MDBD originating from two separate single beta decays, which contains two final state neutrinos, can be readily rejected by measuring the energy of the detected two electrons.","We discuss the similarities and differences between the MDBD and conventional neutrinoless double beta decay."],"url":"http://arxiv.org/abs/2403.02602v1","category":"nucl-th"}
