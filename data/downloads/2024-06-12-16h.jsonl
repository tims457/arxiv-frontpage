{"created":"2024-06-10 17:59:59","title":"IllumiNeRF: 3D Relighting without Inverse Rendering","abstract":"Existing methods for relightable view synthesis -- using a set of images of an object under unknown lighting to recover a 3D representation that can be rendered from novel viewpoints under a target illumination -- are based on inverse rendering, and attempt to disentangle the object geometry, materials, and lighting that explain the input images. Furthermore, this typically involves optimization through differentiable Monte Carlo rendering, which is brittle and computationally-expensive. In this work, we propose a simpler approach: we first relight each input image using an image diffusion model conditioned on lighting and then reconstruct a Neural Radiance Field (NeRF) with these relit images, from which we render novel views under the target lighting. We demonstrate that this strategy is surprisingly competitive and achieves state-of-the-art results on multiple relighting benchmarks. Please see our project page at https://illuminerf.github.io/.","sentences":["Existing methods for relightable view synthesis -- using a set of images of an object under unknown lighting to recover a 3D representation that can be rendered from novel viewpoints under a target illumination -- are based on inverse rendering, and attempt to disentangle the object geometry, materials, and lighting that explain the input images.","Furthermore, this typically involves optimization through differentiable Monte Carlo rendering, which is brittle and computationally-expensive.","In this work, we propose a simpler approach: we first relight each input image using an image diffusion model conditioned on lighting and then reconstruct a Neural Radiance Field (NeRF) with these relit images, from which we render novel views under the target lighting.","We demonstrate that this strategy is surprisingly competitive and achieves state-of-the-art results on multiple relighting benchmarks.","Please see our project page at https://illuminerf.github.io/."],"url":"http://arxiv.org/abs/2406.06527v1","category":"cs.CV"}
{"created":"2024-06-10 17:58:48","title":"Decentralized Personalized Federated Learning","abstract":"This work tackles the challenges of data heterogeneity and communication limitations in decentralized federated learning. We focus on creating a collaboration graph that guides each client in selecting suitable collaborators for training personalized models that leverage their local data effectively. Our approach addresses these issues through a novel, communication-efficient strategy that enhances resource efficiency. Unlike traditional methods, our formulation identifies collaborators at a granular level by considering combinatorial relations of clients, enhancing personalization while minimizing communication overhead. We achieve this through a bi-level optimization framework that employs a constrained greedy algorithm, resulting in a resource-efficient collaboration graph for personalized learning. Extensive evaluation against various baselines across diverse datasets demonstrates the superiority of our method, named DPFL. DPFL consistently outperforms other approaches, showcasing its effectiveness in handling real-world data heterogeneity, minimizing communication overhead, enhancing resource efficiency, and building personalized models in decentralized federated learning scenarios.","sentences":["This work tackles the challenges of data heterogeneity and communication limitations in decentralized federated learning.","We focus on creating a collaboration graph that guides each client in selecting suitable collaborators for training personalized models that leverage their local data effectively.","Our approach addresses these issues through a novel, communication-efficient strategy that enhances resource efficiency.","Unlike traditional methods, our formulation identifies collaborators at a granular level by considering combinatorial relations of clients, enhancing personalization while minimizing communication overhead.","We achieve this through a bi-level optimization framework that employs a constrained greedy algorithm, resulting in a resource-efficient collaboration graph for personalized learning.","Extensive evaluation against various baselines across diverse datasets demonstrates the superiority of our method, named DPFL.","DPFL consistently outperforms other approaches, showcasing its effectiveness in handling real-world data heterogeneity, minimizing communication overhead, enhancing resource efficiency, and building personalized models in decentralized federated learning scenarios."],"url":"http://arxiv.org/abs/2406.06520v1","category":"cs.LG"}
{"created":"2024-06-10 17:53:01","title":"Merlin: A Vision Language Foundation Model for 3D Computed Tomography","abstract":"Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs). However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens). We evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU.","sentences":["Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen.","Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies.","Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs).","However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision.","We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens).","We evaluate Merlin on 6 task types and 752 individual tasks.","The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs).","We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator).","Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines.","We derive data scaling laws to empirically assess training data needs for requisite downstream task performance.","Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU."],"url":"http://arxiv.org/abs/2406.06512v1","category":"cs.CV"}
{"created":"2024-06-10 17:47:14","title":"Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer","abstract":"Given the remarkable results of motion synthesis with diffusion models, a natural question arises: how can we effectively leverage these models for motion editing? Existing diffusion-based motion editing methods overlook the profound potential of the prior embedded within the weights of pre-trained models, which enables manipulating the latent feature space; hence, they primarily center on handling the motion space. In this work, we explore the attention mechanism of pre-trained motion diffusion models. We uncover the roles and interactions of attention elements in capturing and representing intricate human motion patterns, and carefully integrate these elements to transfer a leader motion to a follower one while maintaining the nuanced characteristics of the follower, resulting in zero-shot motion transfer. Editing features associated with selected motions allows us to confront a challenge observed in prior motion diffusion approaches, which use general directives (e.g., text, music) for editing, ultimately failing to convey subtle nuances effectively. Our work is inspired by how a monkey closely imitates what it sees while maintaining its unique motion patterns; hence we call it Monkey See, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing tasks such as synthesizing out-of-distribution motions, style transfer, and spatial editing. Furthermore, diffusion inversion is seldom employed for motions; as a result, editing efforts focus on generated motions, limiting the editability of real ones. MoMo harnesses motion inversion, extending its application to both real and generated motions. Experimental results show the advantage of our approach over the current art. In particular, unlike methods tailored for specific applications through training, our approach is applied at inference time, requiring no training. Our webpage is at https://monkeyseedocg.github.io.","sentences":["Given the remarkable results of motion synthesis with diffusion models, a natural question arises: how can we effectively leverage these models for motion editing?","Existing diffusion-based motion editing methods overlook the profound potential of the prior embedded within the weights of pre-trained models, which enables manipulating the latent feature space; hence, they primarily center on handling the motion space.","In this work, we explore the attention mechanism of pre-trained motion diffusion models.","We uncover the roles and interactions of attention elements in capturing and representing intricate human motion patterns, and carefully integrate these elements to transfer a leader motion to a follower one while maintaining the nuanced characteristics of the follower, resulting in zero-shot motion transfer.","Editing features associated with selected motions allows us to confront a challenge observed in prior motion diffusion approaches, which use general directives (e.g., text, music) for editing, ultimately failing to convey subtle nuances effectively.","Our work is inspired by how a monkey closely imitates what it sees while maintaining its unique motion patterns; hence we call it Monkey See, Monkey Do, and dub it MoMo.","Employing our technique enables accomplishing tasks such as synthesizing out-of-distribution motions, style transfer, and spatial editing.","Furthermore, diffusion inversion is seldom employed for motions; as a result, editing efforts focus on generated motions, limiting the editability of real ones.","MoMo harnesses motion inversion, extending its application to both real and generated motions.","Experimental results show the advantage of our approach over the current art.","In particular, unlike methods tailored for specific applications through training, our approach is applied at inference time, requiring no training.","Our webpage is at https://monkeyseedocg.github.io."],"url":"http://arxiv.org/abs/2406.06508v1","category":"cs.CV"}
{"created":"2024-06-10 17:34:44","title":"Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation","abstract":"In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments. While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance. Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies. These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice. Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies. OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank. Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments. Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights.","sentences":["In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments.","While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance.","Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies.","These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice.","Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies.","OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank.","Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments.","Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights."],"url":"http://arxiv.org/abs/2406.06500v1","category":"cs.AI"}
{"created":"2024-06-10 17:30:17","title":"Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits","abstract":"Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.   So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs.","sentences":["Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs).","PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs.","They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.   ","So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale.","In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training.","In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs."],"url":"http://arxiv.org/abs/2406.06494v1","category":"cs.LG"}
{"created":"2024-06-10 17:26:39","title":"When is Multicalibration Post-Processing Necessary?","abstract":"Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts.","sentences":["Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates.","Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income).","We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs.","Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly.","More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts."],"url":"http://arxiv.org/abs/2406.06487v1","category":"cs.LG"}
{"created":"2024-06-10 17:24:44","title":"Can Language Models Serve as Text-Based World Simulators?","abstract":"Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.","sentences":["Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand.","Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding?","Our goal is to answer this question in the context of text-based simulators.","Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks.","We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators.","We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations.","This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear."],"url":"http://arxiv.org/abs/2406.06485v1","category":"cs.CL"}
{"created":"2024-06-10 17:22:09","title":"Quantum Equilibrium Propagation for efficient training of quantum systems based on Onsager reciprocity","abstract":"The widespread adoption of machine learning and artificial intelligence in all branches of science and technology has created a need for energy-efficient, alternative hardware platforms. While such neuromorphic approaches have been proposed and realised for a wide range of platforms, physically extracting the gradients required for training remains challenging as generic approaches only exist in certain cases. Equilibrium propagation (EP) is such a procedure that has been introduced and applied to classical energy-based models which relax to an equilibrium. Here, we show a direct connection between EP and Onsager reciprocity and exploit this to derive a quantum version of EP. This can be used to optimize loss functions that depend on the expectation values of observables of an arbitrary quantum system. Specifically, we illustrate this new concept with supervised and unsupervised learning examples in which the input or the solvable task is of quantum mechanical nature, e.g., the recognition of quantum many-body ground states, quantum phase exploration, sensing and phase boundary exploration. We propose that in the future quantum EP may be used to solve tasks such as quantum phase discovery with a quantum simulator even for Hamiltonians which are numerically hard to simulate or even partially unknown. Our scheme is relevant for a variety of quantum simulation platforms such as ion chains, superconducting qubit arrays, neutral atom Rydberg tweezer arrays and strongly interacting atoms in optical lattices.","sentences":["The widespread adoption of machine learning and artificial intelligence in all branches of science and technology has created a need for energy-efficient, alternative hardware platforms.","While such neuromorphic approaches have been proposed and realised for a wide range of platforms, physically extracting the gradients required for training remains challenging as generic approaches only exist in certain cases.","Equilibrium propagation (EP) is such a procedure that has been introduced and applied to classical energy-based models which relax to an equilibrium.","Here, we show a direct connection between EP and Onsager reciprocity and exploit this to derive a quantum version of EP.","This can be used to optimize loss functions that depend on the expectation values of observables of an arbitrary quantum system.","Specifically, we illustrate this new concept with supervised and unsupervised learning examples in which the input or the solvable task is of quantum mechanical nature, e.g., the recognition of quantum many-body ground states, quantum phase exploration, sensing and phase boundary exploration.","We propose that in the future quantum EP may be used to solve tasks such as quantum phase discovery with a quantum simulator even for Hamiltonians which are numerically hard to simulate or even partially unknown.","Our scheme is relevant for a variety of quantum simulation platforms such as ion chains, superconducting qubit arrays, neutral atom Rydberg tweezer arrays and strongly interacting atoms in optical lattices."],"url":"http://arxiv.org/abs/2406.06482v1","category":"quant-ph"}
{"created":"2024-06-10 17:16:59","title":"Survey for Landing Generative AI in Social and E-commerce Recsys -- the Industry Perspectives","abstract":"Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys). Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication. Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys. As pioneering work in this domain, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research.","sentences":["Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys).","Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication.","Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys.","As pioneering work in this domain, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research."],"url":"http://arxiv.org/abs/2406.06475v1","category":"cs.IR"}
{"created":"2024-06-10 17:16:49","title":"Towards a Personal Health Large Language Model","abstract":"In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data. We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes. For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness. Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on fitness, exceeding average scores from a sample of human experts. Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models. Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM.","sentences":["In health, most large language model (LLM) research has focused on clinical tasks.","However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring.","Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data.","We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes.","For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness.","Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights.","We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations.","PH-LLM achieved 79% on sleep and 88% on fitness, exceeding average scores from a sample of human experts.","Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models.","Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM."],"url":"http://arxiv.org/abs/2406.06474v1","category":"cs.AI"}
{"created":"2024-06-10 17:09:38","title":"GKAN: Graph Kolmogorov-Arnold Networks","abstract":"We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural network architecture that extends the principles of the recently proposed Kolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the unique characteristics of KANs, notably the use of learnable univariate functions instead of fixed linear weights, we develop a powerful model for graph-based learning tasks. Unlike traditional Graph Convolutional Networks (GCNs) that rely on a fixed convolutional architecture, GKANs implement learnable spline-based functions between layers, transforming the way information is processed across the graph structure. We present two different ways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable functions are applied to input features after aggregation and architecture 2 -- where the learnable functions are applied to input features before aggregation. We evaluate GKAN empirically using a semi-supervised graph learning task on a real-world dataset (Cora). We find that architecture generally performs better. We find that GKANs achieve higher accuracy in semi-supervised learning tasks on graphs compared to the traditional GCN model. For example, when considering 100 features, GCN provides an accuracy of 53.5 while a GKAN with a comparable number of parameters gives an accuracy of 61.76; with 200 features, GCN provides an accuracy of 61.24 while a GKAN with a comparable number of parameters gives an accuracy of 67.66. We also present results on the impact of various parameters such as the number of hidden nodes, grid-size, and the polynomial-degree of the spline on the performance of GKAN.","sentences":["We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural network architecture that extends the principles of the recently proposed Kolmogorov-Arnold Networks (KAN) to graph-structured data.","By adopting the unique characteristics of KANs, notably the use of learnable univariate functions instead of fixed linear weights, we develop a powerful model for graph-based learning tasks.","Unlike traditional Graph Convolutional Networks (GCNs) that rely on a fixed convolutional architecture, GKANs implement learnable spline-based functions between layers, transforming the way information is processed across the graph structure.","We present two different ways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable functions are applied to input features after aggregation and architecture 2 -- where the learnable functions are applied to input features before aggregation.","We evaluate GKAN empirically using a semi-supervised graph learning task on a real-world dataset (Cora).","We find that architecture generally performs better.","We find that GKANs achieve higher accuracy in semi-supervised learning tasks on graphs compared to the traditional GCN model.","For example, when considering 100 features, GCN provides an accuracy of 53.5 while a GKAN with a comparable number of parameters gives an accuracy of 61.76; with 200 features, GCN provides an accuracy of 61.24 while a GKAN with a comparable number of parameters gives an accuracy of 67.66.","We also present results on the impact of various parameters such as the number of hidden nodes, grid-size, and the polynomial-degree of the spline on the performance of GKAN."],"url":"http://arxiv.org/abs/2406.06470v1","category":"cs.LG"}
{"created":"2024-06-10 17:07:25","title":"Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning","abstract":"Language agents perform complex tasks by using tools to execute each step precisely. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering. We introduce Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. We identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. Our experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, we introduce HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems. Our code and models are available at https://github.com/agent-husky/Husky-v1.","sentences":["Language agents perform complex tasks by using tools to execute each step precisely.","However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering.","We introduce Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning.","Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state.","We identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions.","Our experiments show that Husky outperforms prior language agents across 14 evaluation datasets.","Moreover, we introduce HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning.","Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems.","Our code and models are available at https://github.com/agent-husky/Husky-v1."],"url":"http://arxiv.org/abs/2406.06469v1","category":"cs.AI"}
{"created":"2024-06-10 17:05:12","title":"How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad","abstract":"Can Transformers predict new syllogisms by composing established ones? More generally, what type of targets can be learned by such models from scratch? Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective. This paper puts forward the notion of 'distribution locality' to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target. As shown experimentally and theoretically under additional assumptions, distributions with high locality cannot be learned efficiently. In particular, syllogisms cannot be composed on long chains. Furthermore, we show that (i) an agnostic scratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the locality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the out-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks.","sentences":["Can Transformers predict new syllogisms by composing established ones?","More generally, what type of targets can be learned by such models from scratch?","Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective.","This paper puts forward the notion of 'distribution locality' to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target.","As shown experimentally and theoretically under additional assumptions, distributions with high locality cannot be learned efficiently.","In particular, syllogisms cannot be composed on long chains.","Furthermore, we show that (i) an agnostic scratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the locality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the out-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks."],"url":"http://arxiv.org/abs/2406.06467v1","category":"cs.LG"}
{"created":"2024-06-10 17:59:55","title":"GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation","abstract":"3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation. However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2. In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass. Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation. 2) Spatial-aware Gaussian Attribute Decoder: We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points. Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS).","sentences":["3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient.","Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation.","However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial.","Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2.","In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass.","Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation.","2) Spatial-aware Gaussian Attribute Decoder:","We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points.","Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation.","Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS)."],"url":"http://arxiv.org/abs/2406.06526v1","category":"cs.CV"}
{"created":"2024-06-10 17:59:46","title":"NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing","abstract":"We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video. Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics. By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times. Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences. See our project page for video results at https://koi953215.github.io/NaRCan_page/.","sentences":["We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video.","Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics.","By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods.","Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times.","Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences.","See our project page for video results at https://koi953215.github.io/NaRCan_page/."],"url":"http://arxiv.org/abs/2406.06523v1","category":"cs.CV"}
{"created":"2024-06-10 17:58:02","title":"Data Augmentation for Multivariate Time Series Classification: An Experimental Study","abstract":"Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive. Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models. This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision. Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification. Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models. Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy. This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability.","sentences":["Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive.","Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models.","This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision.","Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification.","Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models.","Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy.","This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability."],"url":"http://arxiv.org/abs/2406.06518v1","category":"cs.LG"}
{"created":"2024-06-10 17:55:43","title":"Distribution-Free Predictive Inference under Unknown Temporal Drift","abstract":"Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models. Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time. In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets. The window is selected by optimizing an estimated bias-variance tradeoff. We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and real data.","sentences":["Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models.","Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time.","In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets.","The window is selected by optimizing an estimated bias-variance tradeoff.","We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift.","We also illustrate its efficacy through numerical experiments on synthetic and real data."],"url":"http://arxiv.org/abs/2406.06516v1","category":"stat.ME"}
{"created":"2024-06-10 17:31:07","title":"Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity","abstract":"For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings. Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences. This enables RL agents to adapt their behavior based on human desires. However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task. It becomes essential that agents learn to focus on the subset of task-relevant environment features. Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features. This can result in algorithms that may not effectively transfer to a more noisy real-world setting. To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features. We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions. In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments.","sentences":["For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings.","Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences.","This enables RL agents to adapt their behavior based on human desires.","However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task.","It becomes essential that agents learn to focus on the subset of task-relevant environment features.","Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features.","This can result in algorithms that may not effectively transfer to a more noisy real-world setting.","To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features.","We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions.","In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments."],"url":"http://arxiv.org/abs/2406.06495v1","category":"cs.LG"}
{"created":"2024-06-10 17:02:08","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction","abstract":"Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation. Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task. However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets. We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control. Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task. To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions. More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction. Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs. Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains. More examples can be found at our website https://chenhsing.github.io/AID.","sentences":["Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation.","Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task.","However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets.","We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control.","Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task.","To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions.","More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction.","Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs.","Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101.","Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains.","More examples can be found at our website https://chenhsing.github.io/AID."],"url":"http://arxiv.org/abs/2406.06465v1","category":"cs.CV"}
{"created":"2024-06-10 16:24:35","title":"DISCO: An End-to-End Bandit Framework for Personalised Discount Allocation","abstract":"Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce. Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment. Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS. DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control. Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy. Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\"). These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network. These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base. These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity. Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint. Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems.","sentences":["Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce.","Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment.","Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS.","DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control.","Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy.","Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\").","These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network.","These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base.","These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity.","Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint.","Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems."],"url":"http://arxiv.org/abs/2406.06433v2","category":"cs.LG"}
{"created":"2024-06-10 16:15:14","title":"Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome","abstract":"Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials. With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial. While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context. Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects. In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption. This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment. We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation. We present a numerical example of re-designing an oncology trial. An extensive simulation study is conducted to evaluate the performance of the proposed design.","sentences":["Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials.","With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial.","While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context.","Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects.","In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption.","This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment.","We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation.","We present a numerical example of re-designing an oncology trial.","An extensive simulation study is conducted to evaluate the performance of the proposed design."],"url":"http://arxiv.org/abs/2406.06426v1","category":"stat.ME"}
{"created":"2024-06-10 16:02:48","title":"Differentially Private Best-Arm Identification","abstract":"Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP. Our lower bounds suggest the existence of two privacy regimes. In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation. In the low-privacy regime, the lower bounds reduce to the non-private lower bounds. We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively. For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response. For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.","sentences":["Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies.","Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP).","First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP.","Our lower bounds suggest the existence of two privacy regimes.","In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation.","In the low-privacy regime, the lower bounds reduce to the non-private lower bounds.","We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively.","For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response.","For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off.","By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants."],"url":"http://arxiv.org/abs/2406.06408v1","category":"stat.ML"}
{"created":"2024-06-10 15:57:33","title":"A LoRa-based Energy-efficient Sensing System for Urban Data Collection","abstract":"Nowadays, cities provide much more than shopping opportunities or working spaces. Individual locations such as parks and squares are used as meeting points and local recreation areas by many people. To ensure that they remain attractive in the future, the design of such squares must be regularly adapted to the needs of the public. These utilization trends can be derived using public data collection. The more diverse and rich the data sets are, the easier it is to optimize public space design through data analysis. Traditional data collection methods such as questionnaires, observations, or videos are either labor intensive or cannot guarantee to preserve the individual's privacy. This work presents a privacy-preserving, low-power, and low-cost smart sensing system that is capable of anonymously collecting data about public space utilization by analyzing the occupancy distribution of public seating. To support future urban planning the sensor nodes are capable of monitoring environmental noise, chair utilization, and their position, temperature, and humidity and provide them over a city-wide Long Range Wide Area Network (LoRaWAN). The final sensing system's robust operation is proven in a trial run at two public squares in a city with 16 sensor nodes over a duration of two months. By consuming 33.65 mWh per day with all subsystems enabled, including sitting detection based on a continuous acceleration measurement operating on a robust and simple threshold algorithm, the custom-designed sensor node achieves continuous monitoring during the 2-month trial run. The evaluation of the experimental results clearly shows how the two locations are used, which confirms the practicability of the proposed solution. All data collected during the field trial is publicly available as open data.","sentences":["Nowadays, cities provide much more than shopping opportunities or working spaces.","Individual locations such as parks and squares are used as meeting points and local recreation areas by many people.","To ensure that they remain attractive in the future, the design of such squares must be regularly adapted to the needs of the public.","These utilization trends can be derived using public data collection.","The more diverse and rich the data sets are, the easier it is to optimize public space design through data analysis.","Traditional data collection methods such as questionnaires, observations, or videos are either labor intensive or cannot guarantee to preserve the individual's privacy.","This work presents a privacy-preserving, low-power, and low-cost smart sensing system that is capable of anonymously collecting data about public space utilization by analyzing the occupancy distribution of public seating.","To support future urban planning the sensor nodes are capable of monitoring environmental noise, chair utilization, and their position, temperature, and humidity and provide them over a city-wide Long Range Wide Area Network (LoRaWAN).","The final sensing system's robust operation is proven in a trial run at two public squares in a city with 16 sensor nodes over a duration of two months.","By consuming 33.65 mWh per day with all subsystems enabled, including sitting detection based on a continuous acceleration measurement operating on a robust and simple threshold algorithm, the custom-designed sensor node achieves continuous monitoring during the 2-month trial run.","The evaluation of the experimental results clearly shows how the two locations are used, which confirms the practicability of the proposed solution.","All data collected during the field trial is publicly available as open data."],"url":"http://arxiv.org/abs/2406.06404v1","category":"eess.SY"}
{"created":"2024-06-10 15:52:49","title":"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue","abstract":"We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.","sentences":["We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue.","Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain).","However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics.","In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types.","We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.","We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type.","We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge.","We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols.","Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue.","Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics."],"url":"http://arxiv.org/abs/2406.06399v1","category":"cs.CL"}
{"created":"2024-06-10 15:52:27","title":"Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction","abstract":"We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting covers many interesting examples including H\\\"older smooth problems and various inexact computations of the stochastic gradient. Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants. We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results. First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded. We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer. Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms. In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds. As a direct corollary of our results, we obtain universal stochastic gradient methods for H\\\"older smooth problems which can be used in all situations.","sentences":["We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle.","This setting covers many interesting examples including H\\\"older smooth problems and various inexact computations of the stochastic gradient.","Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants.","We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results.","First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded.","We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer.","Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms.","In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds.","As a direct corollary of our results, we obtain universal stochastic gradient methods for H\\\"older smooth problems which can be used in all situations."],"url":"http://arxiv.org/abs/2406.06398v1","category":"math.OC"}
{"created":"2024-06-10 15:46:25","title":"Towards Lifelong Learning of Large Language Models: A Survey","abstract":"As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information. Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge. Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios. External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters. The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era. Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications.","sentences":["As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial.","Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information.","Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting.","This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge.","Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios.","External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters.","The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era.","Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications."],"url":"http://arxiv.org/abs/2406.06391v1","category":"cs.LG"}
{"created":"2024-06-10 15:44:41","title":"FPN-IAIA-BL: A Multi-Scale Interpretable Deep Learning Model for Classification of Mass Margins in Digital Mammography","abstract":"Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis. In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models. Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography. However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant. This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification. Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes.","sentences":["Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis.","In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models.","Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography.","However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant.","This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification.","Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes."],"url":"http://arxiv.org/abs/2406.06386v1","category":"cs.CV"}
{"created":"2024-06-10 15:44:22","title":"Low-Rank Quantization-Aware Training for LLMs","abstract":"Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage. Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory.","sentences":["Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands.","Quantization is one of the most effective ways to make them more compute and memory efficient.","Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs.","Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs.","LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing.","Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques.","We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks.","Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage.","Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory."],"url":"http://arxiv.org/abs/2406.06385v1","category":"cs.LG"}
{"created":"2024-06-10 15:13:57","title":"A quantitative investigation for deployment of mobile collaborative robots in high-value manufacturing","abstract":"Component inspection is often the bottleneck in high-value manufacturing, driving industries like aerospace toward automated inspection technologies. Current systems often employ fixed arm robots, but they lack the flexibility in adapting to new components or orientations Advanced mobile robotic platforms with updated sensor technologies and algorithms have improved localization and path planning capabilities, making them ideal for bringing inspection processes directly to parts. However, mobile platforms introduce challenges in localization and maneuverability, leading to potential errors. Their positional uncertainty is higher than fixed systems due to the lack of a fixed calibrated location, posing challenges for position-sensitive inspection sensors. Therefore, it's essential to assess the positional accuracy and repeatability of mobile manipulator platforms. The KUKA KMR iiwa was chosen for its collaborative features, robust build, and scalability within the KUKA product range. The accuracy and repeatability of the mobile platform were evaluated through a series of tests to evaluate the performance of its integrated feature mapping, the effect of various speeds on positional accuracy, and the efficiency of the omnidirectional wheels for a range of translation orientations. Experimental evaluation revealed that enabling feature mapping substantially improves the KUKA KMR iiwa's performance, with accuracy gains and error reductions exceeding 90%. Repeatability errors were under 7 mm with mapping activated and around 2.5 mm in practical scenarios, demonstrating that mobile manipulators, incorporating both the manipulator and platform, can fulfil the precise requirements of industries with high precision needs. Providing a highly diverse alternative to traditional fixed-base industrial manipulators.","sentences":["Component inspection is often the bottleneck in high-value manufacturing, driving industries like aerospace toward automated inspection technologies.","Current systems often employ fixed arm robots, but they lack the flexibility in adapting to new components or orientations Advanced mobile robotic platforms with updated sensor technologies and algorithms have improved localization and path planning capabilities, making them ideal for bringing inspection processes directly to parts.","However, mobile platforms introduce challenges in localization and maneuverability, leading to potential errors.","Their positional uncertainty is higher than fixed systems due to the lack of a fixed calibrated location, posing challenges for position-sensitive inspection sensors.","Therefore, it's essential to assess the positional accuracy and repeatability of mobile manipulator platforms.","The KUKA KMR iiwa was chosen for its collaborative features, robust build, and scalability within the KUKA product range.","The accuracy and repeatability of the mobile platform were evaluated through a series of tests to evaluate the performance of its integrated feature mapping, the effect of various speeds on positional accuracy, and the efficiency of the omnidirectional wheels for a range of translation orientations.","Experimental evaluation revealed that enabling feature mapping substantially improves the KUKA KMR iiwa's performance, with accuracy gains and error reductions exceeding 90%.","Repeatability errors were under 7 mm with mapping activated and around 2.5 mm in practical scenarios, demonstrating that mobile manipulators, incorporating both the manipulator and platform, can fulfil the precise requirements of industries with high precision needs.","Providing a highly diverse alternative to traditional fixed-base industrial manipulators."],"url":"http://arxiv.org/abs/2406.06353v1","category":"cs.RO"}
{"created":"2024-06-10 15:13:51","title":"Latent Directions: A Simple Pathway to Bias Mitigation in Generative AI","abstract":"Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society. The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions. Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process. Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases. Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments. Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected. Thus, we provide a tool to empower developers to select their desired concepts to mitigate. The project page with code is available online.","sentences":["Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society.","The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions.","Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process.","Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases.","Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments.","Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected.","Thus, we provide a tool to empower developers to select their desired concepts to mitigate.","The project page with code is available online."],"url":"http://arxiv.org/abs/2406.06352v1","category":"cs.CV"}
{"created":"2024-06-10 14:46:07","title":"A Parameter-efficient Language Extension Framework for Multilingual ASR","abstract":"Covering all languages with a multilingual speech recognition model (MASR) is very difficult. Performing language extension on top of an existing MASR is a desirable choice. In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems. Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE. PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language. Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA. Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes. The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting. Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter.","sentences":["Covering all languages with a multilingual speech recognition model (MASR) is very difficult.","Performing language extension on top of an existing MASR is a desirable choice.","In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems.","Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE.","PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language.","Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA.","Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes.","The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting.","Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter."],"url":"http://arxiv.org/abs/2406.06329v1","category":"cs.CL"}
{"created":"2024-06-10 14:33:59","title":"Should my Blockchain Learn to Drive? A Study of Hyperledger Fabric","abstract":"Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions. However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters. This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention. We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains. We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved. Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings. Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future.","sentences":["Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions.","However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters.","This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention.","We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains.","We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved.","Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings.","Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future."],"url":"http://arxiv.org/abs/2406.06318v1","category":"cs.DC"}
{"created":"2024-06-10 14:11:27","title":"Identifying Bottlenecks of NISQ-friendly HHL algorithms","abstract":"Quantum computing promises enabling solving large problem instances, e.g. large linear equation systems with HHL algorithm, once the hardware stack matures. For the foreseeable future quantum computing will remain in the so-called NISQ era, in which the algorithms need to account for the flaws of the hardware such as noise. In this work, we perform an empirical study to test scaling properties and directly related noise resilience of the the most resources-intense component of the HHL algorithm, namely QPE and its NISQ-adaptation Iterative QPE. We explore the effectiveness of noise mitigation techniques for these algorithms and investigate whether we can keep the gate number low by enforcing sparsity constraints on the input or using circuit optimization techniques provided by Qiskit package. Our results indicate that currently available noise mitigation techniques, such as Qiskit readout and Mthree readout packages, are insufficient for enabling results recovery even in the small instances tested here. Moreover, our results indicate that the scaling of these algorithms with increase in precision seems to be the most substantial obstacle. These insights allowed us to deduce an approximate bottleneck for algorithms that consider a similar time evolution as QPE. Such observations provide evidence of weaknesses of such algorithms on NISQ devices and help us formulate meaningful future research directions.","sentences":["Quantum computing promises enabling solving large problem instances, e.g. large linear equation systems with HHL algorithm, once the hardware stack matures.","For the foreseeable future quantum computing will remain in the so-called NISQ era, in which the algorithms need to account for the flaws of the hardware such as noise.","In this work, we perform an empirical study to test scaling properties and directly related noise resilience of the the most resources-intense component of the HHL algorithm, namely QPE and its NISQ-adaptation Iterative QPE.","We explore the effectiveness of noise mitigation techniques for these algorithms and investigate whether we can keep the gate number low by enforcing sparsity constraints on the input or using circuit optimization techniques provided by Qiskit package.","Our results indicate that currently available noise mitigation techniques, such as Qiskit readout and Mthree readout packages, are insufficient for enabling results recovery even in the small instances tested here.","Moreover, our results indicate that the scaling of these algorithms with increase in precision seems to be the most substantial obstacle.","These insights allowed us to deduce an approximate bottleneck for algorithms that consider a similar time evolution as QPE.","Such observations provide evidence of weaknesses of such algorithms on NISQ devices and help us formulate meaningful future research directions."],"url":"http://arxiv.org/abs/2406.06288v1","category":"quant-ph"}
{"created":"2024-06-10 14:01:21","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone","abstract":"This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","sentences":["This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity.","The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations.","Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference.","Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations.","The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks.","Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone.","For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM.","For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2."],"url":"http://arxiv.org/abs/2406.06282v1","category":"cs.LG"}
{"created":"2024-06-10 13:59:28","title":"Optimal sensing policy with interference-model uncertainty","abstract":"Assume that an interferer behaves according to a parametric model but one does not know the value of the model parameters. Sensing enables to improve the model knowledge and therefore perform a better link adaptation. However, we consider a half-duplex scenario where, at each time slot, the communication system should decide between sensing and communication. We thus propose to investigate the optimal policy to maximize the expected sum rate given a finite-time communication. % the following question therefore arises: At a given time slot, should one sense or communicate? We first show that this problem can be modelled in the Markov decision process (MDP) framework. We then demonstrate that the optimal open-loop and closed-loop policies can be found significantly faster than the standard backward-induction algorithm.","sentences":["Assume that an interferer behaves according to a parametric model but one does not know the value of the model parameters.","Sensing enables to improve the model knowledge and therefore perform a better link adaptation.","However, we consider a half-duplex scenario where, at each time slot, the communication system should decide between sensing and communication.","We thus propose to investigate the optimal policy to maximize the expected sum rate given a finite-time communication.","% the following question therefore arises: At a given time slot, should one sense or communicate?","We first show that this problem can be modelled in the Markov decision process (MDP) framework.","We then demonstrate that the optimal open-loop and closed-loop policies can be found significantly faster than the standard backward-induction algorithm."],"url":"http://arxiv.org/abs/2406.06280v1","category":"cs.IT"}
{"created":"2024-06-10 13:58:46","title":"Multi-Prompting Decoder Helps Better Language Understanding","abstract":"Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting.","sentences":["Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting.","To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM.","Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt.","In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation.","The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding.","Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs.","Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores.","Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting."],"url":"http://arxiv.org/abs/2406.06279v1","category":"cs.CL"}
{"created":"2024-06-10 13:53:31","title":"Global-in-time energy stability analysis for the exponential time differencing Runge-Kutta scheme for the phase field crystal equation","abstract":"The global-in-time energy estimate is derived for the second-order accurate exponential time differencing Runge-Kutta (ETDRK2) numerical scheme to the phase field crystal (PFC) equation, a sixth-order parabolic equation modeling crystal evolution. To recover the value of stabilization constant, some local-in-time convergence analysis has been reported, and the energy stability becomes available over a fixed final time. In this work, we develop a global-in-time energy estimate for the ETDRK2 numerical scheme to the PFC equation by showing the energy dissipation property for any final time. An a priori assumption at the previous time step, combined with a single-step $H^2$ estimate of the numerical solution, is the key point in the analysis. Such an $H^2$ estimate recovers the maximum norm bound of the numerical solution at the next time step, and then the value of the stabilization parameter can be theoretically justified. This justification ensures the energy dissipation at the next time step, so that the mathematical induction can be effectively applied, by then the global-in-time energy estimate is accomplished. This paper represents the first effort to theoretically establish a global-in-time energy stability analysis for a second-order stabilized numerical scheme in terms of the original free energy functional. The presented methodology is expected to be available for many other Runge-Kutta numerical schemes to the gradient flow equations.","sentences":["The global-in-time energy estimate is derived for the second-order accurate exponential time differencing Runge-Kutta (ETDRK2) numerical scheme to the phase field crystal (PFC) equation, a sixth-order parabolic equation modeling crystal evolution.","To recover the value of stabilization constant, some local-in-time convergence analysis has been reported, and the energy stability becomes available over a fixed final time.","In this work, we develop a global-in-time energy estimate for the ETDRK2 numerical scheme to the PFC equation by showing the energy dissipation property for any final time.","An a priori assumption at the previous time step, combined with a single-step $H^2$ estimate of the numerical solution, is the key point in the analysis.","Such an $H^2$ estimate recovers the maximum norm bound of the numerical solution at the next time step, and then the value of the stabilization parameter can be theoretically justified.","This justification ensures the energy dissipation at the next time step, so that the mathematical induction can be effectively applied, by then the global-in-time energy estimate is accomplished.","This paper represents the first effort to theoretically establish a global-in-time energy stability analysis for a second-order stabilized numerical scheme in terms of the original free energy functional.","The presented methodology is expected to be available for many other Runge-Kutta numerical schemes to the gradient flow equations."],"url":"http://arxiv.org/abs/2406.06272v1","category":"math.NA"}
{"created":"2024-06-10 13:41:10","title":"Tuning-Free Visual Customization via View Iterative Self-Attention Control","abstract":"Fine-Tuning Diffusion Models enable a wide range of personalized generation and editing applications on diverse visual modalities. While Low-Rank Adaptation (LoRA) accelerates the fine-tuning process, it still requires multiple reference images and time-consuming training, which constrains its scalability for large-scale and real-time applications. In this paper, we propose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this challenge. Specifically, VisCtrl is a training-free method that injects the appearance and structure of a user-specified subject into another subject in the target image, unlike previous approaches that require fine-tuning the model. Initially, we obtain the initial noise for both the reference and target images through DDIM inversion. Then, during the denoising phase, features from the reference image are injected into the target image via the self-attention mechanism. Notably, by iteratively performing this feature injection process, we ensure that the reference image features are gradually integrated into the target image. This approach results in consistent and harmonious editing with only one reference image in a few denoising steps. Moreover, benefiting from our plug-and-play architecture design and the proposed Feature Gradual Sampling strategy for multi-view editing, our method can be easily extended to edit in complex visual domains. Extensive experiments show the efficacy of VisCtrl across a spectrum of tasks, including personalized editing of images, videos, and 3D scenes.","sentences":["Fine-Tuning Diffusion Models enable a wide range of personalized generation and editing applications on diverse visual modalities.","While Low-Rank Adaptation (LoRA) accelerates the fine-tuning process, it still requires multiple reference images and time-consuming training, which constrains its scalability for large-scale and real-time applications.","In this paper, we propose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this challenge.","Specifically, VisCtrl is a training-free method that injects the appearance and structure of a user-specified subject into another subject in the target image, unlike previous approaches that require fine-tuning the model.","Initially, we obtain the initial noise for both the reference and target images through DDIM inversion.","Then, during the denoising phase, features from the reference image are injected into the target image via the self-attention mechanism.","Notably, by iteratively performing this feature injection process, we ensure that the reference image features are gradually integrated into the target image.","This approach results in consistent and harmonious editing with only one reference image in a few denoising steps.","Moreover, benefiting from our plug-and-play architecture design and the proposed Feature Gradual Sampling strategy for multi-view editing, our method can be easily extended to edit in complex visual domains.","Extensive experiments show the efficacy of VisCtrl across a spectrum of tasks, including personalized editing of images, videos, and 3D scenes."],"url":"http://arxiv.org/abs/2406.06258v2","category":"cs.CV"}
{"created":"2024-06-10 13:34:43","title":"Stabilized Adaptive Steering for 3D Sonar Microphone Arrays with IMU Sensor Fusion","abstract":"This paper presents a novel software-based approach to stabilizing the acoustic images for in-air 3D sonars. Due to uneven terrain, traditional static beamforming techniques can be misaligned, causing inaccurate measurements and imaging artifacts. Furthermore, mechanical stabilization can be more costly and prone to failure. We propose using an adaptive conventional beamforming approach by fusing it with real-time IMU data to adjust the sonar array's steering matrix dynamically based on the elevation tilt angle caused by the uneven ground. Additionally, we propose gaining compensation to offset emission energy loss due to the transducer's directivity pattern and validate our approach through various experiments, which show significant improvements in temporal consistency in the acoustic images. We implemented a GPU-accelerated software system that operates in real-time with an average execution time of 210ms, meeting autonomous navigation requirements.","sentences":["This paper presents a novel software-based approach to stabilizing the acoustic images for in-air 3D sonars.","Due to uneven terrain, traditional static beamforming techniques can be misaligned, causing inaccurate measurements and imaging artifacts.","Furthermore, mechanical stabilization can be more costly and prone to failure.","We propose using an adaptive conventional beamforming approach by fusing it with real-time IMU data to adjust the sonar array's steering matrix dynamically based on the elevation tilt angle caused by the uneven ground.","Additionally, we propose gaining compensation to offset emission energy loss due to the transducer's directivity pattern and validate our approach through various experiments, which show significant improvements in temporal consistency in the acoustic images.","We implemented a GPU-accelerated software system that operates in real-time with an average execution time of 210ms, meeting autonomous navigation requirements."],"url":"http://arxiv.org/abs/2406.06255v1","category":"cs.RO"}
{"created":"2024-06-10 13:34:23","title":"Understanding Students' Acceptance of ChatGPT as a Translation Tool: A UTAUT Model Analysis","abstract":"The potential of ChatGPT to transform the education landscape is drawing increasing attention. With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training. The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation. Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships. An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students. Respondents were divided into two groups based on their ChatGPT use experience. Data were analyzed using structural equation modeling. A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups. Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users. Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors. These results are discussed with the different primary purposes of translation and non-translation students' translation practices. The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs.","sentences":["The potential of ChatGPT to transform the education landscape is drawing increasing attention.","With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training.","The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation.","Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships.","An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students.","Respondents were divided into two groups based on their ChatGPT use experience.","Data were analyzed using structural equation modeling.","A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups.","Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users.","Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors.","These results are discussed with the different primary purposes of translation and non-translation students' translation practices.","The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs."],"url":"http://arxiv.org/abs/2406.06254v1","category":"cs.HC"}
{"created":"2024-06-10 13:31:18","title":"Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning","abstract":"As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.","sentences":["As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations.","In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module.","To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches.","Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality.","Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter.","Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups."],"url":"http://arxiv.org/abs/2406.06251v1","category":"eess.AS"}
{"created":"2024-06-10 13:24:18","title":"Image Compression with Isotropic and Anisotropic Shepard Inpainting","abstract":"Inpainting-based codecs store sparse selected pixel data and decode by reconstructing the discarded image parts by inpainting. Successful codecs (coders and decoders) traditionally use inpainting operators that solve partial differential equations. This requires some numerical expertise if efficient implementations are necessary. Our goal is to investigate variants of Shepard inpainting as simple alternatives for inpainting-based compression. They can be implemented efficiently when we localise their weighting function. To turn them into viable codecs, we have to introduce novel extensions of classical Shepard interpolation that adapt successful ideas from previous codecs: Anisotropy allows direction-dependent inpainting, which improves reconstruction quality. Additionally, we incorporate data selection by subdivision as an efficient way to tailor the stored information to the image structure. On the encoding side, we introduce the novel concept of joint inpainting and prediction for isotropic Shepard codecs, where storage cost can be reduced based on intermediate inpainting results. In an ablation study, we show the usefulness of these individual contributions and demonstrate that they offer synergies which elevate the performance of Shepard inpainting to surprising levels. Our resulting approaches offer a more favourable trade-off between simplicity and quality than traditional inpainting-based codecs. Experiments show that they can outperform JPEG and JPEG2000 at high compression ratios.","sentences":["Inpainting-based codecs store sparse selected pixel data and decode by reconstructing the discarded image parts by inpainting.","Successful codecs (coders and decoders) traditionally use inpainting operators that solve partial differential equations.","This requires some numerical expertise if efficient implementations are necessary.","Our goal is to investigate variants of Shepard inpainting as simple alternatives for inpainting-based compression.","They can be implemented efficiently when we localise their weighting function.","To turn them into viable codecs, we have to introduce novel extensions of classical Shepard interpolation that adapt successful ideas from previous codecs: Anisotropy allows direction-dependent inpainting, which improves reconstruction quality.","Additionally, we incorporate data selection by subdivision as an efficient way to tailor the stored information to the image structure.","On the encoding side, we introduce the novel concept of joint inpainting and prediction for isotropic Shepard codecs, where storage cost can be reduced based on intermediate inpainting results.","In an ablation study, we show the usefulness of these individual contributions and demonstrate that they offer synergies which elevate the performance of Shepard inpainting to surprising levels.","Our resulting approaches offer a more favourable trade-off between simplicity and quality than traditional inpainting-based codecs.","Experiments show that they can outperform JPEG and JPEG2000 at high compression ratios."],"url":"http://arxiv.org/abs/2406.06247v1","category":"eess.IV"}
{"created":"2024-06-10 13:23:00","title":"Data-Efficient Learning with Neural Programs","abstract":"Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.","sentences":["Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM.","We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite.","When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components.","We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components.","For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature.","Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks.","For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner."],"url":"http://arxiv.org/abs/2406.06246v1","category":"cs.LG"}
{"created":"2024-06-10 13:20:46","title":"Lower eigenvalue bounds with hybrid high-order methods","abstract":"This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds. These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms. The involved constants arise from local embeddings and are available for all polynomial degrees. Applications include the linear elasticity and Steklov eigenvalue problem.","sentences":["This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds.","These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms.","The involved constants arise from local embeddings and are available for all polynomial degrees.","Applications include the linear elasticity and Steklov eigenvalue problem."],"url":"http://arxiv.org/abs/2406.06244v1","category":"math.NA"}
{"created":"2024-06-10 13:08:31","title":"I-MPN: Inductive Message Passing Network for Effective and Efficient Human-in-the-Loop Annotation of Mobile Eye Tracking Data","abstract":"Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design. Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights. However, manual analysis of these recordings is time-intensive. In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings. Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions. This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment. Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback. Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation.","sentences":["Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design.","Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights.","However, manual analysis of these recordings is time-intensive.","In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings.","Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions.","This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment.","Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback.","Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation."],"url":"http://arxiv.org/abs/2406.06239v1","category":"cs.CV"}
{"created":"2024-06-10 13:06:13","title":"Adaptive combinations of tail-risk forecasts","abstract":"In order to meet the increasingly stringent global standards of banking management and regulation, several methods have been proposed in the literature for forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected Shortfall (ES). However, regardless of the approach used, there are several sources of uncertainty, including model specifications, data-related issues and the estimation procedure, which can significantly affect the accuracy of VaR and ES measures. Aiming to mitigate the influence of these sources of uncertainty and improve the predictive performance of individual models, we propose novel forecast combination strategies based on the Model Confidence Set (MCS). In particular, consistent joint VaR and ES loss functions within the MCS framework are used to adaptively combine forecasts generated by a wide range of parametric, semi-parametric, and non-parametric models. Our results reveal that the proposed combined predictors provide a suitable alternative for forecasting risk measures, passing the usual backtests, entering the set of superior models of the MCS, and usually exhibiting lower standard deviations than other model specifications.","sentences":["In order to meet the increasingly stringent global standards of banking management and regulation, several methods have been proposed in the literature for forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected Shortfall (ES).","However, regardless of the approach used, there are several sources of uncertainty, including model specifications, data-related issues and the estimation procedure, which can significantly affect the accuracy of VaR and ES measures.","Aiming to mitigate the influence of these sources of uncertainty and improve the predictive performance of individual models, we propose novel forecast combination strategies based on the Model Confidence Set (MCS).","In particular, consistent joint VaR and ES loss functions within the MCS framework are used to adaptively combine forecasts generated by a wide range of parametric, semi-parametric, and non-parametric models.","Our results reveal that the proposed combined predictors provide a suitable alternative for forecasting risk measures, passing the usual backtests, entering the set of superior models of the MCS, and usually exhibiting lower standard deviations than other model specifications."],"url":"http://arxiv.org/abs/2406.06235v1","category":"q-fin.RM"}
{"created":"2024-06-10 12:47:49","title":"Siren -- Advancing Cybersecurity through Deception and Adaptive Analysis","abstract":"Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis. Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments. The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats. The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement. Data protection within the honeypot is fortified with probabilistic encryption. Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement. Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries. The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats.","sentences":["Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis.","Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments.","The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats.","The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement.","Data protection within the honeypot is fortified with probabilistic encryption.","Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement.","Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries.","The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats."],"url":"http://arxiv.org/abs/2406.06225v1","category":"cs.CR"}
{"created":"2024-06-10 12:14:05","title":"Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning","abstract":"Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data. However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data. To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data. While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications. In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale of two cities. On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models. Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks. On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security. We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process. Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models. \\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms. Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges.","sentences":["Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data.","However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data.","To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data.","While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications.","In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks.","Our analysis showcases a tale of two cities.","On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models.","Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks.","On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security.","We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems.","\\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process.","Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models.","\\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms.","Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges."],"url":"http://arxiv.org/abs/2406.06207v1","category":"cs.LG"}
{"created":"2024-06-10 11:50:38","title":"Learning effective Hamiltonians for adaptive time-evolution quantum algorithms","abstract":"Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization. Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions. However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled. Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration. Our key observation is that deviations from the target generator remain bounded on all simulation times. This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system. Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms.","sentences":["Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization.","Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions.","However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled.","Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration.","Our key observation is that deviations from the target generator remain bounded on all simulation times.","This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system.","Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms."],"url":"http://arxiv.org/abs/2406.06198v1","category":"quant-ph"}
{"created":"2024-06-10 11:09:11","title":"Non-equilibrium cotunneling in interacting Josephson junctions","abstract":"We investigate non-equilibrium transport through interacting superconducting nanojunctions using a Liouville space approach. The formalism allows us to study finite gap effects, and to account for both quasiparticle and Cooper pair tunneling. With focus on the weak tunneling limit, we study the stationary dc and ac current up to second order (cotunneling) in the hybridization energy. We identify the characteristic virtual processes that yield the Andreev and Josephson current and obtain the dependence on the gate and bias voltage for the dc current, the critical current and the phase-dependent dissipative current. In particular, the critical current is characterized by regions in the stability diagram in which its sign changes from positive to negative, resulting in a multitude of 0-\\pi transitions. The latter signal the interplay between strong interactions and tunneling at finite bias.","sentences":["We investigate non-equilibrium transport through interacting superconducting nanojunctions using a Liouville space approach.","The formalism allows us to study finite gap effects, and to account for both quasiparticle and Cooper pair tunneling.","With focus on the weak tunneling limit, we study the stationary dc and ac current up to second order (cotunneling) in the hybridization energy.","We identify the characteristic virtual processes that yield the Andreev and Josephson current and obtain the dependence on the gate and bias voltage for the dc current, the critical current and the phase-dependent dissipative current.","In particular, the critical current is characterized by regions in the stability diagram in which its sign changes from positive to negative, resulting in a multitude of 0-\\pi transitions.","The latter signal the interplay between strong interactions and tunneling at finite bias."],"url":"http://arxiv.org/abs/2406.06170v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 10:53:23","title":"Extending Segment Anything Model into Auditory and Temporal Dimensions for Audio-Visual Segmentation","abstract":"Audio-visual segmentation (AVS) aims to segment sound sources in the video sequence, requiring a pixel-level understanding of audio-visual correspondence. As the Segment Anything Model (SAM) has strongly impacted extensive fields of dense prediction problems, prior works have investigated the introduction of SAM into AVS with audio as a new modality of the prompt. Nevertheless, constrained by SAM's single-frame segmentation scheme, the temporal context across multiple frames of audio-visual data remains insufficiently utilized. To this end, we study the extension of SAM's capabilities to the sequence of audio-visual scenes by analyzing contextual cross-modal relationships across the frames. To achieve this, we propose a Spatio-Temporal, Bidirectional Audio-Visual Attention (ST-BAVA) module integrated into the middle of SAM's image encoder and mask decoder. It adaptively updates the audio-visual features to convey the spatio-temporal correspondence between the video frames and audio streams. Extensive experiments demonstrate that our proposed model outperforms the state-of-the-art methods on AVS benchmarks, especially with an 8.3% mIoU gain on a challenging multi-sources subset.","sentences":["Audio-visual segmentation (AVS) aims to segment sound sources in the video sequence, requiring a pixel-level understanding of audio-visual correspondence.","As the Segment Anything Model (SAM) has strongly impacted extensive fields of dense prediction problems, prior works have investigated the introduction of SAM into AVS with audio as a new modality of the prompt.","Nevertheless, constrained by SAM's single-frame segmentation scheme, the temporal context across multiple frames of audio-visual data remains insufficiently utilized.","To this end, we study the extension of SAM's capabilities to the sequence of audio-visual scenes by analyzing contextual cross-modal relationships across the frames.","To achieve this, we propose a Spatio-Temporal, Bidirectional Audio-Visual Attention (ST-BAVA) module integrated into the middle of SAM's image encoder and mask decoder.","It adaptively updates the audio-visual features to convey the spatio-temporal correspondence between the video frames and audio streams.","Extensive experiments demonstrate that our proposed model outperforms the state-of-the-art methods on AVS benchmarks, especially with an 8.3% mIoU gain on a challenging multi-sources subset."],"url":"http://arxiv.org/abs/2406.06163v1","category":"cs.CV"}
{"created":"2024-06-10 10:37:38","title":"Asymptotic Approximation of Fading Mode in Neurooscillator Dynamics","abstract":"We consider a system consisting of two delay differential equations with a large parameter, modeling the association of a pair of neurooscillators. The unknown functions describe the changes in the normalized membrane potentials of neurons over time, with the large parameter characterizing the speed of electrical processes. The first equation is separated from the system and represents a generalized Hutchinson equation. This equation, as known, possesses periodic solutions with high peaks over the period. The second equation is also based on the generalized Hutchinson equation, but with an additional term, linking it to an oscillator satisfying the first equation. For the second equation, it is possible to asymptotically construct the so-called fading neuron mode, which is as follows: for any natural number $n$, one can adjust the parameters of the problem in such a way that the solution is asymptotically close to a periodic function with high peaks over $n$ periods, and then, after a transient process represented by decreasing peaks, becomes asymptotically small.","sentences":["We consider a system consisting of two delay differential equations with a large parameter, modeling the association of a pair of neurooscillators.","The unknown functions describe the changes in the normalized membrane potentials of neurons over time, with the large parameter characterizing the speed of electrical processes.","The first equation is separated from the system and represents a generalized Hutchinson equation.","This equation, as known, possesses periodic solutions with high peaks over the period.","The second equation is also based on the generalized Hutchinson equation, but with an additional term, linking it to an oscillator satisfying the first equation.","For the second equation, it is possible to asymptotically construct the so-called fading neuron mode, which is as follows: for any natural number $n$, one can adjust the parameters of the problem in such a way that the solution is asymptotically close to a periodic function with high peaks over $n$ periods, and then, after a transient process represented by decreasing peaks, becomes asymptotically small."],"url":"http://arxiv.org/abs/2406.06155v1","category":"nlin.SI"}
{"created":"2024-06-10 10:04:21","title":"Mastering truss structure optimization with tree search","abstract":"This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures. Our approach accommodates intermediate construction stages characteristic of progressive construction settings. We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning. These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs. We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for identifying the global optimum. Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality. While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications.","sentences":["This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures.","Our approach accommodates intermediate construction stages characteristic of progressive construction settings.","We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning.","These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs.","We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for identifying the global optimum.","Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality.","While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications."],"url":"http://arxiv.org/abs/2406.06145v1","category":"cs.CE"}
{"created":"2024-06-10 08:36:55","title":"Adaptive Control in Assistive Application -- A Study Evaluating Shared Control by Users with Limited Upper Limb Mobility","abstract":"Shared control in assistive robotics blends human autonomy with computer assistance, thus simplifying complex tasks for individuals with physical impairments. This study assesses an adaptive Degrees of Freedom control method specifically tailored for individuals with upper limb impairments. It employs a between-subjects analysis with 24 participants, conducting 81 trials across three distinct input devices in a realistic everyday-task setting. Given the diverse capabilities of the vulnerable target demographic and the known challenges in statistical comparisons due to individual differences, the study focuses primarily on subjective qualitative data. The results reveal consistently high success rates in trial completions, irrespective of the input device used. Participants appreciated their involvement in the research process, displayed a positive outlook, and quick adaptability to the control system. Notably, each participant effectively managed the given task within a short time frame.","sentences":["Shared control in assistive robotics blends human autonomy with computer assistance, thus simplifying complex tasks for individuals with physical impairments.","This study assesses an adaptive Degrees of Freedom control method specifically tailored for individuals with upper limb impairments.","It employs a between-subjects analysis with 24 participants, conducting 81 trials across three distinct input devices in a realistic everyday-task setting.","Given the diverse capabilities of the vulnerable target demographic and the known challenges in statistical comparisons due to individual differences, the study focuses primarily on subjective qualitative data.","The results reveal consistently high success rates in trial completions, irrespective of the input device used.","Participants appreciated their involvement in the research process, displayed a positive outlook, and quick adaptability to the control system.","Notably, each participant effectively managed the given task within a short time frame."],"url":"http://arxiv.org/abs/2406.06103v1","category":"cs.HC"}
{"created":"2024-06-10 08:23:16","title":"Sim-To-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery","abstract":"Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making. Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost. A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap.   In this work, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation. We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery. After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images. Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images. This work introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics.","sentences":["Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making.","Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost.","A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap.   ","In this work, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation.","We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery.","After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images.","Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images.","This work introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics."],"url":"http://arxiv.org/abs/2406.06092v1","category":"cs.RO"}
{"created":"2024-06-10 07:36:55","title":"Efficient k-Nearest-Neighbor Machine Translation with Dynamic Retrieval","abstract":"To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine Translation ($k$NN-MT) constructs an external datastore to store domain-specific translation knowledge, which derives a $k$NN distribution to interpolate the prediction distribution of the NMT model via a linear interpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at each timestep leads to substantial time overhead. To address this issue, dominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR), which dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is less than a fixed threshold. Unfortunately, $k$NN-MT-AR does not yield satisfactory results. In this paper, we first conduct a preliminary study to reveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to inaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping, and 2) using a fixed threshold fails to accommodate the dynamic demands for $k$NN retrieval at different timesteps. To mitigate these limitations, we then propose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly extends vanilla $k$NN-MT in two aspects. Firstly, we equip $k$NN-MT with a MLP-based classifier for determining whether to skip $k$NN retrieval at each timestep. Particularly, we explore several carefully-designed scalar features to fully exert the potential of the classifier. Secondly, we propose a timestep-aware threshold adjustment method to dynamically generate the threshold, which further improves the efficiency of our model. Experimental results on the widely-used datasets demonstrate the effectiveness and generality of our model.\\footnote{Our code is available at \\url{https://github.com/DeepLearnXMU/knn-mt-dr}.","sentences":["To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine Translation ($k$NN-MT) constructs an external datastore to store domain-specific translation knowledge, which derives a $k$NN distribution to interpolate the prediction distribution of the NMT model via a linear interpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at each timestep leads to substantial time overhead.","To address this issue, dominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR), which dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is less than a fixed threshold.","Unfortunately, $k$NN-MT-AR does not yield satisfactory results.","In this paper, we first conduct a preliminary study to reveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to inaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping, and 2) using a fixed threshold fails to accommodate the dynamic demands for $k$NN retrieval at different timesteps.","To mitigate these limitations, we then propose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly extends vanilla $k$NN-MT in two aspects.","Firstly, we equip $k$NN-MT with a MLP-based classifier for determining whether to skip $k$NN retrieval at each timestep.","Particularly, we explore several carefully-designed scalar features to fully exert the potential of the classifier.","Secondly, we propose a timestep-aware threshold adjustment method to dynamically generate the threshold, which further improves the efficiency of our model.","Experimental results on the widely-used datasets demonstrate the effectiveness and generality of our model.\\footnote{Our code is available at \\url{https://github.com/DeepLearnXMU/knn-mt-dr}."],"url":"http://arxiv.org/abs/2406.06073v1","category":"cs.CL"}
{"created":"2024-06-10 07:36:24","title":"Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control","abstract":"Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias. However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases. Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control. We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases.","sentences":["Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias.","However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases.","Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer.","To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control.","We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases."],"url":"http://arxiv.org/abs/2406.06072v1","category":"cs.CV"}
{"created":"2024-06-10 07:21:23","title":"Enabling Large-Scale and High-Precision Fluid Simulations on Near-Term Quantum Computers","abstract":"Quantum computational fluid dynamics (QCFD) offers a promising alternative to classical computational fluid dynamics (CFD) by leveraging quantum algorithms for higher efficiency. This paper introduces a comprehensive QCFD method implemented on a superconducting quantum computer, demonstrating successful simulations of steady Poiseuille flow and unsteady acoustic wave propagation. The Poiseuille flow simulation achieved a relative error of less than $0.2\\%$, and the unsteady acoustic wave simulation solved a 5043-dimension matrix, marking the largest fluid simulation on a quantum computer to date. Our approach bridges quantum and classical computing, adapting to quantum hardware constraints and offering scalable solutions for large-scale CFD problems, which paves the way for practical applications of near-term quantum computers in computational science.","sentences":["Quantum computational fluid dynamics (QCFD) offers a promising alternative to classical computational fluid dynamics (CFD) by leveraging quantum algorithms for higher efficiency.","This paper introduces a comprehensive QCFD method implemented on a superconducting quantum computer, demonstrating successful simulations of steady Poiseuille flow and unsteady acoustic wave propagation.","The Poiseuille flow simulation achieved a relative error of less than $0.2\\%$, and the unsteady acoustic wave simulation solved a 5043-dimension matrix, marking the largest fluid simulation on a quantum computer to date.","Our approach bridges quantum and classical computing, adapting to quantum hardware constraints and offering scalable solutions for large-scale CFD problems, which paves the way for practical applications of near-term quantum computers in computational science."],"url":"http://arxiv.org/abs/2406.06063v2","category":"physics.comp-ph"}
{"created":"2024-06-10 06:38:11","title":"Generalizable Human Gaussians from Single-View Image","abstract":"In this work, we tackle the task of learning generalizable 3D human Gaussians from a single image. The main challenge for this task is to recover detailed geometry and appearance, especially for the unobserved regions. To this end, we propose single-view generalizable Human Gaussian model (HGM), a diffusion-guided framework for 3D human modeling from a single image. We design a diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted to refine novel-view images rendered from a coarse human Gaussian model. The refined images are then used together with the input image to learn a refined human Gaussian model. Although effective in hallucinating the unobserved views, the approach may generate unrealistic human pose and shapes due to the lack of supervision. We circumvent this problem by further encoding the geometric priors from SMPL model. Specifically, we propagate geometric features from SMPL volume to the predicted Gaussians via sparse convolution and attention mechanism. We validate our approach on publicly available datasets and demonstrate that it significantly surpasses state-of-the-art methods in terms of PSNR and SSIM. Additionally, our method exhibits strong generalization for in-the-wild images.","sentences":["In this work, we tackle the task of learning generalizable 3D human Gaussians from a single image.","The main challenge for this task is to recover detailed geometry and appearance, especially for the unobserved regions.","To this end, we propose single-view generalizable Human Gaussian model (HGM), a diffusion-guided framework for 3D human modeling from a single image.","We design a diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted to refine novel-view images rendered from a coarse human Gaussian model.","The refined images are then used together with the input image to learn a refined human Gaussian model.","Although effective in hallucinating the unobserved views, the approach may generate unrealistic human pose and shapes due to the lack of supervision.","We circumvent this problem by further encoding the geometric priors from SMPL model.","Specifically, we propagate geometric features from SMPL volume to the predicted Gaussians via sparse convolution and attention mechanism.","We validate our approach on publicly available datasets and demonstrate that it significantly surpasses state-of-the-art methods in terms of PSNR and SSIM.","Additionally, our method exhibits strong generalization for in-the-wild images."],"url":"http://arxiv.org/abs/2406.06050v1","category":"cs.CV"}
{"created":"2024-06-10 06:27:42","title":"MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models","abstract":"Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we fine-tune a small data influence model to approximate oracle data preference signals collected by locally probing the pretraining model and to select data accordingly for the next pretraining stage. Experiments on Pythia and the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks in both zero- and few-shot settings. It doubles the gains achieved by recent data selection approaches that leverage larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analysis validates the ever-changing data preferences of pretraining models and the effectiveness of our data influence models to capture them. Our code is open-sourced at https://github.com/cxcscmu/MATES.","sentences":["Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora.","Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining.","In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress.","Specifically, we fine-tune a small data influence model to approximate oracle data preference signals collected by locally probing the pretraining model and to select data accordingly for the next pretraining stage.","Experiments on Pythia and the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks in both zero-","and few-shot settings.","It doubles the gains achieved by recent data selection approaches that leverage larger reference models and reduces the total FLOPs required to reach certain performances by half.","Further analysis validates the ever-changing data preferences of pretraining models and the effectiveness of our data influence models to capture them.","Our code is open-sourced at https://github.com/cxcscmu/MATES."],"url":"http://arxiv.org/abs/2406.06046v1","category":"cs.CL"}
{"created":"2024-06-10 06:24:19","title":"FRAG: Frequency Adapting Group for Diffusion Video Editing","abstract":"In video editing, the hallmark of a quality edit lies in its consistent and unobtrusive adjustment. Modification, when integrated, must be smooth and subtle, preserving the natural flow and aligning seamlessly with the original vision. Therefore, our primary focus is on overcoming the current challenges in high quality edit to ensure that each edit enhances the final product without disrupting its intended essence. However, quality deterioration such as blurring and flickering is routinely observed in recent diffusion video editing systems. We confirm that this deterioration often stems from high-frequency leak: the diffusion model fails to accurately synthesize high-frequency components during denoising process. To this end, we devise Frequency Adapting Group (FRAG) which enhances the video quality in terms of consistency and fidelity by introducing a novel receptive field branch to preserve high-frequency components during the denoising process. FRAG is performed in a model-agnostic manner without additional training and validates the effectiveness on video editing benchmarks (i.e., TGVE, DAVIS).","sentences":["In video editing, the hallmark of a quality edit lies in its consistent and unobtrusive adjustment.","Modification, when integrated, must be smooth and subtle, preserving the natural flow and aligning seamlessly with the original vision.","Therefore, our primary focus is on overcoming the current challenges in high quality edit to ensure that each edit enhances the final product without disrupting its intended essence.","However, quality deterioration such as blurring and flickering is routinely observed in recent diffusion video editing systems.","We confirm that this deterioration often stems from high-frequency leak: the diffusion model fails to accurately synthesize high-frequency components during denoising process.","To this end, we devise Frequency Adapting Group (FRAG) which enhances the video quality in terms of consistency and fidelity by introducing a novel receptive field branch to preserve high-frequency components during the denoising process.","FRAG is performed in a model-agnostic manner without additional training and validates the effectiveness on video editing benchmarks (i.e., TGVE, DAVIS)."],"url":"http://arxiv.org/abs/2406.06044v1","category":"cs.CV"}
{"created":"2024-06-10 06:17:33","title":"Diving into Underwater: Segment Anything Model Guided Underwater Salient Instance Segmentation and A Large-scale Dataset","abstract":"With the breakthrough of large models, Segment Anything Model (SAM) and its extensions have been attempted to apply in diverse tasks of computer vision. Underwater salient instance segmentation is a foundational and vital step for various underwater vision tasks, which often suffer from low segmentation accuracy due to the complex underwater circumstances and the adaptive ability of models. Moreover, the lack of large-scale datasets with pixel-level salient instance annotations has impeded the development of machine learning techniques in this field. To address these issues, we construct the first large-scale underwater salient instance segmentation dataset (USIS10K), which contains 10,632 underwater images with pixel-level annotations in 7 categories from various underwater scenes. Then, we propose an Underwater Salient Instance Segmentation architecture based on Segment Anything Model (USIS-SAM) specifically for the underwater domain. We devise an Underwater Adaptive Visual Transformer (UA-ViT) encoder to incorporate underwater domain visual prompts into the segmentation network. We further design an out-of-the-box underwater Salient Feature Prompter Generator (SFPG) to automatically generate salient prompters instead of explicitly providing foreground points or boxes as prompts in SAM. Comprehensive experimental results show that our USIS-SAM method can achieve superior performance on USIS10K datasets compared to the state-of-the-art methods. Datasets and codes are released on https://github.com/LiamLian0727/USIS10K.","sentences":["With the breakthrough of large models, Segment Anything Model (SAM) and its extensions have been attempted to apply in diverse tasks of computer vision.","Underwater salient instance segmentation is a foundational and vital step for various underwater vision tasks, which often suffer from low segmentation accuracy due to the complex underwater circumstances and the adaptive ability of models.","Moreover, the lack of large-scale datasets with pixel-level salient instance annotations has impeded the development of machine learning techniques in this field.","To address these issues, we construct the first large-scale underwater salient instance segmentation dataset (USIS10K), which contains 10,632 underwater images with pixel-level annotations in 7 categories from various underwater scenes.","Then, we propose an Underwater Salient Instance Segmentation architecture based on Segment Anything Model (USIS-SAM) specifically for the underwater domain.","We devise an Underwater Adaptive Visual Transformer (UA-ViT) encoder to incorporate underwater domain visual prompts into the segmentation network.","We further design an out-of-the-box underwater Salient Feature Prompter Generator (SFPG) to automatically generate salient prompters instead of explicitly providing foreground points or boxes as prompts in SAM.","Comprehensive experimental results show that our USIS-SAM method can achieve superior performance on USIS10K datasets compared to the state-of-the-art methods.","Datasets and codes are released on https://github.com/LiamLian0727/USIS10K."],"url":"http://arxiv.org/abs/2406.06039v1","category":"cs.CV"}
{"created":"2024-06-10 04:41:48","title":"Quantifying dissipation in stochastic complex oscillations","abstract":"Fluctuations-driven complex oscillations are experimentally observed in cellular systems such as hepatocytes, cardiac cells, neuronal cells, etc. These systems are generally operating in regimes far from thermodynamic equilibrium. To study nonequilibrium thermodynamic properties such as energy dissipation in stochastic complex oscillations, we consider stochastic modeling of two nonlinear biological oscillators, namely, the intracellular calcium (Ca$^{2+}$) oscillation model and the Hindmarsh-Rose model for neuronal dynamics. These models exhibit various types of complex oscillations like bursting and quasi-periodic oscillations for various system parameter values. In this work, we formulate open chemical reaction schemes for the two model systems driving the systems far from thermodynamic equilibrium. We then analyze the steady-state total entropy production rate (EPR) in the various types of stochastic complex oscillations. Our results show higher values of steady-state total EPR in stochastic complex oscillations than simple periodic oscillations. Moreover, in the Hindmarsh-Rose neuronal model, we observe an order-to-disorder transition from periodic (organized) bursts of spikes to chaotic (unorganized) oscillations with distinct behaviors of steady-state total EPR. Our results reveal that stochastic complex oscillations are produced at the cost of higher energy consumption and that it requires a higher thermodynamic cost to maintain the periodic bursts than chaotic oscillations. Our findings indicate that complex cellular regulatory or signaling processes by Ca$^{2+}$ that help perform complex tasks of the nervous system or rich information coding by neurons involve a higher thermodynamic cost. The results deepen our understanding of energy dissipation in nonlinear, nonequilibrium biological systems with stochastic complex oscillatory dynamics.","sentences":["Fluctuations-driven complex oscillations are experimentally observed in cellular systems such as hepatocytes, cardiac cells, neuronal cells, etc.","These systems are generally operating in regimes far from thermodynamic equilibrium.","To study nonequilibrium thermodynamic properties such as energy dissipation in stochastic complex oscillations, we consider stochastic modeling of two nonlinear biological oscillators, namely, the intracellular calcium (Ca$^{2+}$) oscillation model and the Hindmarsh-Rose model for neuronal dynamics.","These models exhibit various types of complex oscillations like bursting and quasi-periodic oscillations for various system parameter values.","In this work, we formulate open chemical reaction schemes for the two model systems driving the systems far from thermodynamic equilibrium.","We then analyze the steady-state total entropy production rate (EPR) in the various types of stochastic complex oscillations.","Our results show higher values of steady-state total EPR in stochastic complex oscillations than simple periodic oscillations.","Moreover, in the Hindmarsh-Rose neuronal model, we observe an order-to-disorder transition from periodic (organized) bursts of spikes to chaotic (unorganized) oscillations with distinct behaviors of steady-state total EPR.","Our results reveal that stochastic complex oscillations are produced at the cost of higher energy consumption and that it requires a higher thermodynamic cost to maintain the periodic bursts than chaotic oscillations.","Our findings indicate that complex cellular regulatory or signaling processes by Ca$^{2+}$ that help perform complex tasks of the nervous system or rich information coding by neurons involve a higher thermodynamic cost.","The results deepen our understanding of energy dissipation in nonlinear, nonequilibrium biological systems with stochastic complex oscillatory dynamics."],"url":"http://arxiv.org/abs/2406.06019v1","category":"physics.chem-ph"}
{"created":"2024-06-10 03:38:35","title":"fSEAD: a Composable FPGA-based Streaming Ensemble Anomaly Detection Library","abstract":"Machine learning ensembles combine multiple base models to produce a more accurate output. They can be applied to a range of machine learning problems, including anomaly detection. In this paper, we investigate how to maximize the composability and scalability of an FPGA-based streaming ensemble anomaly detector (fSEAD). To achieve this, we propose a flexible computing architecture consisting of multiple partially reconfigurable regions, pblocks, which each implement anomaly detectors. Our proof-of-concept design supports three state-of-the-art anomaly detection algorithms: Loda, RS-Hash and xStream. Each algorithm is scalable, meaning multiple instances can be placed within a pblock to improve performance. Moreover, fSEAD is implemented using High-level synthesis (HLS), meaning further custom anomaly detectors can be supported. Pblocks are interconnected via an AXI-switch, enabling them to be composed in an arbitrary fashion before combining and merging results at run-time to create an ensemble that maximizes the use of FPGA resources and accuracy. Through utilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be modified at run-time to adapt to changing environmental conditions. We compare fSEAD to an equivalent central processing unit (CPU) implementation using four standard datasets, with speed-ups ranging from $3\\times$ to $8\\times$.","sentences":["Machine learning ensembles combine multiple base models to produce a more accurate output.","They can be applied to a range of machine learning problems, including anomaly detection.","In this paper, we investigate how to maximize the composability and scalability of an FPGA-based streaming ensemble anomaly detector (fSEAD).","To achieve this, we propose a flexible computing architecture consisting of multiple partially reconfigurable regions, pblocks, which each implement anomaly detectors.","Our proof-of-concept design supports three state-of-the-art anomaly detection algorithms: Loda, RS-Hash and xStream.","Each algorithm is scalable, meaning multiple instances can be placed within a pblock to improve performance.","Moreover, fSEAD is implemented using High-level synthesis (HLS), meaning further custom anomaly detectors can be supported.","Pblocks are interconnected via an AXI-switch, enabling them to be composed in an arbitrary fashion before combining and merging results at run-time to create an ensemble that maximizes the use of FPGA resources and accuracy.","Through utilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be modified at run-time to adapt to changing environmental conditions.","We compare fSEAD to an equivalent central processing unit (CPU) implementation using four standard datasets, with speed-ups ranging from $3\\times$ to $8\\times$."],"url":"http://arxiv.org/abs/2406.05999v1","category":"cs.AR"}
{"created":"2024-06-10 03:25:49","title":"Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning","abstract":"Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment. Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies. In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL. We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance. Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL.","sentences":["Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment.","Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies.","In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL.","We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance.","Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL."],"url":"http://arxiv.org/abs/2406.05993v1","category":"cs.LG"}
{"created":"2024-06-10 03:06:09","title":"Data-Driven Real-time Coupon Allocation in the Online Platform","abstract":"Traditionally, firms have offered coupons to customer groups at predetermined discount rates. However, advancements in machine learning and the availability of abundant customer data now enable platforms to provide real-time customized coupons to individuals. In this study, we partner with Meituan, a leading shopping platform, to develop a real-time, end-to-end coupon allocation system that is fast and effective in stimulating demand while adhering to marketing budgets when faced with uncertain traffic from a diverse customer base. Leveraging comprehensive customer and product features, we estimate Conversion Rates (CVR) under various coupon values and employ isotonic regression to ensure the monotonicity of predicted CVRs with respect to coupon value. Using calibrated CVR predictions as input, we propose a Lagrangian Dual-based algorithm that efficiently determines optimal coupon values for each arriving customer within 50 milliseconds. We theoretically and numerically investigate the model performance under parameter misspecifications and apply a control loop to adapt to real-time updated information, thereby better adhering to the marketing budget. Finally, we demonstrate through large-scale field experiments and observational data that our proposed coupon allocation algorithm outperforms traditional approaches in terms of both higher conversion rates and increased revenue. As of May 2024, Meituan has implemented our framework to distribute coupons to over 100 million users across more than 110 major cities in China, resulting in an additional CNY 8 million in annual profit. We demonstrate how to integrate a machine learning prediction model for estimating customer CVR, a Lagrangian Dual-based coupon value optimizer, and a control system to achieve real-time coupon delivery while dynamically adapting to random customer arrival patterns.","sentences":["Traditionally, firms have offered coupons to customer groups at predetermined discount rates.","However, advancements in machine learning and the availability of abundant customer data now enable platforms to provide real-time customized coupons to individuals.","In this study, we partner with Meituan, a leading shopping platform, to develop a real-time, end-to-end coupon allocation system that is fast and effective in stimulating demand while adhering to marketing budgets when faced with uncertain traffic from a diverse customer base.","Leveraging comprehensive customer and product features, we estimate Conversion Rates (CVR) under various coupon values and employ isotonic regression to ensure the monotonicity of predicted CVRs with respect to coupon value.","Using calibrated CVR predictions as input, we propose a Lagrangian Dual-based algorithm that efficiently determines optimal coupon values for each arriving customer within 50 milliseconds.","We theoretically and numerically investigate the model performance under parameter misspecifications and apply a control loop to adapt to real-time updated information, thereby better adhering to the marketing budget.","Finally, we demonstrate through large-scale field experiments and observational data that our proposed coupon allocation algorithm outperforms traditional approaches in terms of both higher conversion rates and increased revenue.","As of May 2024, Meituan has implemented our framework to distribute coupons to over 100 million users across more than 110 major cities in China, resulting in an additional CNY 8 million in annual profit.","We demonstrate how to integrate a machine learning prediction model for estimating customer CVR, a Lagrangian Dual-based coupon value optimizer, and a control system to achieve real-time coupon delivery while dynamically adapting to random customer arrival patterns."],"url":"http://arxiv.org/abs/2406.05987v1","category":"econ.EM"}
{"created":"2024-06-10 01:35:42","title":"Photoinduced non-reciprocal magnetism","abstract":"Out of equilibrium, the action-reaction symmetry of the interactions is often broken, leading to the emergence of various collective phenomena with no equilibrium counterparts. Although ubiquitous in classical active systems, implementing such non-reciprocal interactions in solid-state systems has remained challenging, as the known quantum schemes require precise control over the system on a single-site level. Here, we propose a novel dissipation-engineering protocol to induce non-reciprocal interactions in solid-state platforms with light, which we expect to be achievable with state-of-the-art experimental techniques. Focusing on magnetic metals for concreteness, we show microscopically that a light injection that introduces the decay channel to a virtually excited state gives rise to non-reciprocal interactions between localized spins. One can even realize a situation where spin A tries to align with spin B but the B tries the opposite, resulting in a chase-and-runaway dynamics. Applying our scheme to layered ferromagnets, we show that a non-reciprocal phase transition from a static to a many-body time-dependent chiral phase emerges. Our work paves the way to bring solid-state systems to the realm of non-reciprocal science, providing yet another possibility to control quantum matter with light.","sentences":["Out of equilibrium, the action-reaction symmetry of the interactions is often broken, leading to the emergence of various collective phenomena with no equilibrium counterparts.","Although ubiquitous in classical active systems, implementing such non-reciprocal interactions in solid-state systems has remained challenging, as the known quantum schemes require precise control over the system on a single-site level.","Here, we propose a novel dissipation-engineering protocol to induce non-reciprocal interactions in solid-state platforms with light, which we expect to be achievable with state-of-the-art experimental techniques.","Focusing on magnetic metals for concreteness, we show microscopically that a light injection that introduces the decay channel to a virtually excited state gives rise to non-reciprocal interactions between localized spins.","One can even realize a situation where spin A tries to align with spin B but the B tries the opposite, resulting in a chase-and-runaway dynamics.","Applying our scheme to layered ferromagnets, we show that a non-reciprocal phase transition from a static to a many-body time-dependent chiral phase emerges.","Our work paves the way to bring solid-state systems to the realm of non-reciprocal science, providing yet another possibility to control quantum matter with light."],"url":"http://arxiv.org/abs/2406.05957v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 00:35:23","title":"Safety Alignment Should Be Made More Than Just a Few Tokens Deep","abstract":"The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.","sentences":["The safety alignment of current Large Language Models (LLMs) is vulnerable.","Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models.","We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens.","We refer to this issue as shallow safety alignment.","In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue.","We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks.","Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities.","For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits.","Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens.","Overall, we advocate that future safety alignment should be made more than just a few tokens deep."],"url":"http://arxiv.org/abs/2406.05946v1","category":"cs.CR"}
{"created":"2024-06-09 23:57:47","title":"Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs","abstract":"Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective precondition.   Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. Despite notable empirical observations, theoretical foundations are still lacking. In this work, we investigate the expressive or representative power of GNNs, a crucial aspect of neural network theory, specifically in the context of QP tasks, with both continuous and mixed-integer settings. We prove the existence of message-passing GNNs that can reliably represent key properties of quadratic programs, including feasibility, optimal objective value, and optimal solution. Our theory is validated by numerical results.","sentences":["Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming.","Many applications require real-time/fast solutions, though not necessarily with high precision.","Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method.","For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective precondition.   ","Recently, graph neural networks (GNNs) opened new possibilities for QP.","Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution.","Despite notable empirical observations, theoretical foundations are still lacking.","In this work, we investigate the expressive or representative power of GNNs, a crucial aspect of neural network theory, specifically in the context of QP tasks, with both continuous and mixed-integer settings.","We prove the existence of message-passing GNNs that can reliably represent key properties of quadratic programs, including feasibility, optimal objective value, and optimal solution.","Our theory is validated by numerical results."],"url":"http://arxiv.org/abs/2406.05938v1","category":"cs.LG"}
{"created":"2024-06-09 20:42:02","title":"A primer on the analogue black hole bomb with capillary-gravity waves","abstract":"Draining vortices with a free surface are frequently employed as rotating black hole simulators, both in theory and experiments. However, most theoretical work is restricted to the idealised regime, where wave dispersion and dissipation are neglected. We investigate the role of these effects on the analogue black hole bomb, an instability resulting from rotational superradiant amplification in confined systems. We reveal that the dispersion of deep water capillary-gravity waves significantly modifies the unstable mode eigenfrequencies, whereas viscosity only affects those with high frequencies. Furthermore, if the circulation is less than an order 1 multiple of the drain rate, superradiance does not occur and the vortex is stable. The instability is maximised in small systems with high flow velocities, provided there is sufficient space between the vortex and the outer boundary for the first excited state to lie inside the superradiant bandwidth. Implications for experiments on analogue black holes and free surface vortices are discussed.","sentences":["Draining vortices with a free surface are frequently employed as rotating black hole simulators, both in theory and experiments.","However, most theoretical work is restricted to the idealised regime, where wave dispersion and dissipation are neglected.","We investigate the role of these effects on the analogue black hole bomb, an instability resulting from rotational superradiant amplification in confined systems.","We reveal that the dispersion of deep water capillary-gravity waves significantly modifies the unstable mode eigenfrequencies, whereas viscosity only affects those with high frequencies.","Furthermore, if the circulation is less than an order 1 multiple of the drain rate, superradiance does not occur and the vortex is stable.","The instability is maximised in small systems with high flow velocities, provided there is sufficient space between the vortex and the outer boundary for the first excited state to lie inside the superradiant bandwidth.","Implications for experiments on analogue black holes and free surface vortices are discussed."],"url":"http://arxiv.org/abs/2406.05910v1","category":"gr-qc"}
{"created":"2024-06-09 19:35:20","title":"Async Learned User Embeddings for Ads Delivery Optimization","abstract":"User representation is crucial for recommendation systems as it helps to deliver personalized recommendations by capturing user preferences and behaviors in low-dimensional vectors. High-quality user embeddings can capture subtle preferences, enable precise similarity calculations, and adapt to changing preferences over time to maintain relevance. The effectiveness of recommendation systems depends significantly on the quality of user embedding. We propose to asynchronously learn high fidelity user embeddings for billions of users each day from sequence based multimodal user activities in Meta platforms through a Transformer-like large scale feature learning module. The async learned user representations embeddings (ALURE) are further converted to user similarity graphs through graph learning and then combined with user realtime activities to retrieval highly related ads candidates for the entire ads delivery system. Our method shows significant gains in both offline and online experiments.","sentences":["User representation is crucial for recommendation systems as it helps to deliver personalized recommendations by capturing user preferences and behaviors in low-dimensional vectors.","High-quality user embeddings can capture subtle preferences, enable precise similarity calculations, and adapt to changing preferences over time to maintain relevance.","The effectiveness of recommendation systems depends significantly on the quality of user embedding.","We propose to asynchronously learn high fidelity user embeddings for billions of users each day from sequence based multimodal user activities in Meta platforms through a Transformer-like large scale feature learning module.","The async learned user representations embeddings (ALURE) are further converted to user similarity graphs through graph learning and then combined with user realtime activities to retrieval highly related ads candidates for the entire ads delivery system.","Our method shows significant gains in both offline and online experiments."],"url":"http://arxiv.org/abs/2406.05898v1","category":"cs.IR"}
{"created":"2024-06-09 19:14:44","title":"Structure and energy transfer in homogeneous turbulence below a free surface","abstract":"We investigate the turbulence below a quasi-flat free surface, focusing on the energy transport in space and across scales. We leverage a large zero-mean-flow tank where homogeneous turbulence is generated by randomly actuated jets. A wide range of Reynolds number is spanned, reaching sufficient scale separation for the emergence of an inertial sub-range. Unlike previous studies, the forcing extends through the source layer, though the surface deformation remains millimetric. Particle image velocimetry along a surface-normal plane resolves from the dissipative to the integral scales. Both vertical and horizontal components of the turbulent kinetic energy approach the prediction based on rapid distortion theory as the Reynolds number is increased, indicating that discrepancies among previous studies are likely due to differences in the forcing. At odds with the theory, however, the integral scale of the horizontal fluctuations grows as the surface is approached. This is rooted in the profound influence exerted by the surface on the inter-scale energy transfer: along horizontal separations, the direct cascade of horizontal energy is hindered, while an inverse cascade of vertical energy is established. This is connected to the structure of upwellings and downwellings. The former, characterized by somewhat larger spatial extent and stronger intensity, are associated to extensional surface-parallel motions. They thus transfer energy to the larger horizontal scales, prevailing over downwellings which favour the compression (and concurrent vertical stretching) of the eddies. Both types of structures extend to depths between the integral and Taylor microscales.","sentences":["We investigate the turbulence below a quasi-flat free surface, focusing on the energy transport in space and across scales.","We leverage a large zero-mean-flow tank where homogeneous turbulence is generated by randomly actuated jets.","A wide range of Reynolds number is spanned, reaching sufficient scale separation for the emergence of an inertial sub-range.","Unlike previous studies, the forcing extends through the source layer, though the surface deformation remains millimetric.","Particle image velocimetry along a surface-normal plane resolves from the dissipative to the integral scales.","Both vertical and horizontal components of the turbulent kinetic energy approach the prediction based on rapid distortion theory as the Reynolds number is increased, indicating that discrepancies among previous studies are likely due to differences in the forcing.","At odds with the theory, however, the integral scale of the horizontal fluctuations grows as the surface is approached.","This is rooted in the profound influence exerted by the surface on the inter-scale energy transfer: along horizontal separations, the direct cascade of horizontal energy is hindered, while an inverse cascade of vertical energy is established.","This is connected to the structure of upwellings and downwellings.","The former, characterized by somewhat larger spatial extent and stronger intensity, are associated to extensional surface-parallel motions.","They thus transfer energy to the larger horizontal scales, prevailing over downwellings which favour the compression (and concurrent vertical stretching) of the eddies.","Both types of structures extend to depths between the integral and Taylor microscales."],"url":"http://arxiv.org/abs/2406.05889v1","category":"physics.flu-dyn"}
{"created":"2024-06-09 18:59:08","title":"Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach","abstract":"Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.","sentences":["Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible.","This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning.","Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples.","In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network.","The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers.","Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric.","Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models."],"url":"http://arxiv.org/abs/2406.05887v1","category":"cs.LG"}
{"created":"2024-06-09 18:42:27","title":"Revisiting institutional punishment in the $N$-person prisoner's dilemma","abstract":"The conflict between individual and collective interests makes fostering cooperation in human societies a challenging task, requiring drastic measures such as the establishment of sanctioning institutions. These institutions are costly because they have to be maintained regardless of the presence or absence of offenders. Here, we propose realistic improvements to the standard $N$-person prisoner's dilemma formulation with institutional punishment by eliminating overpunishment, requiring a minimum number of contributors to establish the sanctioning institution, and sharing the cost among them once this minimum number is reached. In addition, we focus on large groups or communities for which sanctioning institutions are ubiquitous. Using the replicator equation framework for an infinite population, we find that by sufficiently fining players who fail to contribute either to the public good or to the sanctioning institution, a population of contributors immune to invasion by these free riders can be established, provided that the contributors are sufficiently numerous. In a finite population, we use finite-size scaling to show that, for some parameter settings, demographic noise helps to fixate the strategy that contributes to the public good but not to the sanctioning institution even for infinitely large populations when, somewhat counterintuitively, its proportion in the initial population vanishes with a small power of the population size.","sentences":["The conflict between individual and collective interests makes fostering cooperation in human societies a challenging task, requiring drastic measures such as the establishment of sanctioning institutions.","These institutions are costly because they have to be maintained regardless of the presence or absence of offenders.","Here, we propose realistic improvements to the standard $N$-person prisoner's dilemma formulation with institutional punishment by eliminating overpunishment, requiring a minimum number of contributors to establish the sanctioning institution, and sharing the cost among them once this minimum number is reached.","In addition, we focus on large groups or communities for which sanctioning institutions are ubiquitous.","Using the replicator equation framework for an infinite population, we find that by sufficiently fining players who fail to contribute either to the public good or to the sanctioning institution, a population of contributors immune to invasion by these free riders can be established, provided that the contributors are sufficiently numerous.","In a finite population, we use finite-size scaling to show that, for some parameter settings, demographic noise helps to fixate the strategy that contributes to the public good but not to the sanctioning institution even for infinitely large populations when, somewhat counterintuitively, its proportion in the initial population vanishes with a small power of the population size."],"url":"http://arxiv.org/abs/2406.05884v1","category":"physics.soc-ph"}
{"created":"2024-06-09 17:27:20","title":"Source -Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels","abstract":"Domain adaptation is often hampered by exceedingly small target datasets and inaccessible source data. These conditions are prevalent in speech verification, where privacy policies and/or languages with scarce speech resources limit the availability of sufficient data. This paper explored techniques of sourcefree domain adaptation unto a limited target speech dataset for speaker verificationin data-scarce languages. Both language and channel mis-match between source and target were investigated. Fine-tuning methods were evaluated and compared across different sizes of labeled target data. A novel iterative cluster-learn algorithm was studied for unlabeled target datasets.","sentences":["Domain adaptation is often hampered by exceedingly small target datasets and inaccessible source data.","These conditions are prevalent in speech verification, where privacy policies and/or languages with scarce speech resources limit the availability of sufficient data.","This paper explored techniques of sourcefree domain adaptation unto a limited target speech dataset for speaker verificationin data-scarce languages.","Both language and channel mis-match between source and target were investigated.","Fine-tuning methods were evaluated and compared across different sizes of labeled target data.","A novel iterative cluster-learn algorithm was studied for unlabeled target datasets."],"url":"http://arxiv.org/abs/2406.05863v1","category":"cs.SD"}
{"created":"2024-06-09 16:53:48","title":"Non-uniqueness of weak solutions for a logarithmically supercritical hyperdissipative Navier-Stokes system","abstract":"The existence of non-unique solutions of finite kinetic energy for the three dimensional Navier-Stokes equations is proved in the slightly supercritical hyper-dissipative setting introduced by Tao. The result is based on the convex integration techniques of Buckmaster and Vicol and extends Luo and Titi result in the slightly supercritical setting. To be able to be closer to the threshold identified by Tao, we introduce the impulsed Beltrami flows, a variant of the intermittent Beltrami flows of Buckmaster and Vicol.","sentences":["The existence of non-unique solutions of finite kinetic energy for the three dimensional Navier-Stokes equations is proved in the slightly supercritical hyper-dissipative setting introduced by Tao.","The result is based on the convex integration techniques of Buckmaster and Vicol and extends Luo and Titi result in the slightly supercritical setting.","To be able to be closer to the threshold identified by Tao, we introduce the impulsed Beltrami flows, a variant of the intermittent Beltrami flows of Buckmaster and Vicol."],"url":"http://arxiv.org/abs/2406.05853v1","category":"math.AP"}
{"created":"2024-06-09 16:48:27","title":"MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps","abstract":"Creating 3D semantic reconstructions of environments is fundamental to many applications, especially when related to autonomous agent operation (e.g., goal-oriented navigation or object interaction and manipulation). Commonly, 3D semantic reconstruction systems capture the entire scene in the same level of detail. However, certain tasks (e.g., object interaction) require a fine-grained and high-resolution map, particularly if the objects to interact are of small size or intricate geometry. In recent practice, this leads to the entire map being in the same high-quality resolution, which results in increased computational and storage costs. To address this challenge, we propose MAP-ADAPT, a real-time method for quality-adaptive semantic 3D reconstruction using RGBD frames. MAP-ADAPT is the first adaptive semantic 3D mapping algorithm that, unlike prior work, generates directly a single map with regions of different quality based on both the semantic information and the geometric complexity of the scene. Leveraging a semantic SLAM pipeline for pose and semantic estimation, we achieve comparable or superior results to state-of-the-art methods on synthetic and real-world data, while significantly reducing storage and computation requirements.","sentences":["Creating 3D semantic reconstructions of environments is fundamental to many applications, especially when related to autonomous agent operation (e.g., goal-oriented navigation or object interaction and manipulation).","Commonly, 3D semantic reconstruction systems capture the entire scene in the same level of detail.","However, certain tasks (e.g., object interaction) require a fine-grained and high-resolution map, particularly if the objects to interact are of small size or intricate geometry.","In recent practice, this leads to the entire map being in the same high-quality resolution, which results in increased computational and storage costs.","To address this challenge, we propose MAP-ADAPT, a real-time method for quality-adaptive semantic 3D reconstruction using RGBD frames.","MAP-ADAPT is the first adaptive semantic 3D mapping algorithm that, unlike prior work, generates directly a single map with regions of different quality based on both the semantic information and the geometric complexity of the scene.","Leveraging a semantic SLAM pipeline for pose and semantic estimation, we achieve comparable or superior results to state-of-the-art methods on synthetic and real-world data, while significantly reducing storage and computation requirements."],"url":"http://arxiv.org/abs/2406.05849v1","category":"cs.RO"}
{"created":"2024-06-10 17:56:21","title":"Genomics-guided Representation Learning for Pathologic Pan-cancer Tumor Microenvironment Subtype Prediction","abstract":"The characterization of Tumor MicroEnvironment (TME) is challenging due to its complexity and heterogeneity. Relatively consistent TME characteristics embedded within highly specific tissue features, render them difficult to predict. The capability to accurately classify TME subtypes is of critical significance for clinical tumor diagnosis and precision medicine. Based on the observation that tumors with different origins share similar microenvironment patterns, we propose PathoTME, a genomics-guided Siamese representation learning framework employing Whole Slide Image (WSI) for pan-cancer TME subtypes prediction. Specifically, we utilize Siamese network to leverage genomic information as a regularization factor to assist WSI embeddings learning during the training phase. Additionally, we employ Domain Adversarial Neural Network (DANN) to mitigate the impact of tissue type variations. To eliminate domain bias, a dynamic WSI prompt is designed to further unleash the model's capabilities. Our model achieves better performance than other state-of-the-art methods across 23 cancer types on TCGA dataset. Our code is available at https://github.com/Mengflz/PathoTME.","sentences":["The characterization of Tumor MicroEnvironment (TME) is challenging due to its complexity and heterogeneity.","Relatively consistent TME characteristics embedded within highly specific tissue features, render them difficult to predict.","The capability to accurately classify TME subtypes is of critical significance for clinical tumor diagnosis and precision medicine.","Based on the observation that tumors with different origins share similar microenvironment patterns, we propose PathoTME, a genomics-guided Siamese representation learning framework employing Whole Slide Image (WSI) for pan-cancer TME subtypes prediction.","Specifically, we utilize Siamese network to leverage genomic information as a regularization factor to assist WSI embeddings learning during the training phase.","Additionally, we employ Domain Adversarial Neural Network (DANN) to mitigate the impact of tissue type variations.","To eliminate domain bias, a dynamic WSI prompt is designed to further unleash the model's capabilities.","Our model achieves better performance than other state-of-the-art methods across 23 cancer types on TCGA dataset.","Our code is available at https://github.com/Mengflz/PathoTME."],"url":"http://arxiv.org/abs/2406.06517v1","category":"cs.CV"}
{"created":"2024-06-10 17:43:39","title":"Phragm\u00e8n-Lindel\u00f6f type theorems for elliptic equations on infinite graphs","abstract":"We investigate the validity of the Phragm\\`en-Lindel\\\"of principle for a class of elliptic equations with a potential, posed on infinite graphs. Consequently, we get uniqueness, in the class of solutions satisfying a suitable growth condition at infinity. We suppose that the {\\it outer degree (or outer curvature)} of the graph is bounded from above, and we allow the potential to go to zero at infinity in a controlled way. Finally, we discuss the optimality of the condition on the potential on symmetric trees and on the integer lattice.","sentences":["We investigate the validity of the Phragm\\`en-Lindel\\\"of principle for a class of elliptic equations with a potential, posed on infinite graphs.","Consequently, we get uniqueness, in the class of solutions satisfying a suitable growth condition at infinity.","We suppose that the {\\it outer degree (or outer curvature)} of the graph is bounded from above, and we allow the potential to go to zero at infinity in a controlled way.","Finally, we discuss the optimality of the condition on the potential on symmetric trees and on the integer lattice."],"url":"http://arxiv.org/abs/2406.06505v1","category":"math.AP"}
{"created":"2024-06-10 17:43:13","title":"Equivariant Neural Tangent Kernels","abstract":"Equivariant neural networks have in recent years become an important technique for guiding architecture selection for neural networks with many applications in domains ranging from medical image analysis to quantum chemistry. In particular, as the most general linear equivariant layers with respect to the regular representation, group convolutions have been highly impactful in numerous applications. Although equivariant architectures have been studied extensively, much less is known about the training dynamics of equivariant neural networks. Concurrently, neural tangent kernels (NTKs) have emerged as a powerful tool to analytically understand the training dynamics of wide neural networks. In this work, we combine these two fields for the first time by giving explicit expressions for NTKs of group convolutional neural networks. In numerical experiments, we demonstrate superior performance for equivariant NTKs over non-equivariant NTKs on a classification task for medical images.","sentences":["Equivariant neural networks have in recent years become an important technique for guiding architecture selection for neural networks with many applications in domains ranging from medical image analysis to quantum chemistry.","In particular, as the most general linear equivariant layers with respect to the regular representation, group convolutions have been highly impactful in numerous applications.","Although equivariant architectures have been studied extensively, much less is known about the training dynamics of equivariant neural networks.","Concurrently, neural tangent kernels (NTKs) have emerged as a powerful tool to analytically understand the training dynamics of wide neural networks.","In this work, we combine these two fields for the first time by giving explicit expressions for NTKs of group convolutional neural networks.","In numerical experiments, we demonstrate superior performance for equivariant NTKs over non-equivariant NTKs on a classification task for medical images."],"url":"http://arxiv.org/abs/2406.06504v1","category":"cs.LG"}
{"created":"2024-06-10 17:37:56","title":"Viscous shock fluctuations in KPZ","abstract":"We study ``V-shaped'' solutions to the KPZ equation, those having opposite asymptotic slopes $\\theta$ and $-\\theta$, with $\\theta>0$, at positive and negative infinity, respectively. Answering a question of Janjigian, Rassoul-Agha, and Sepp\\\"al\\\"ainen, we show that the spatial increments of V-shaped solutions cannot be statistically stationary in time. This completes the classification of statistically time-stationary spatial increments for the KPZ equation by ruling out the last case left by those authors.   To show that these V-shaped time-stationary measures do not exist, we study the location of the corresponding ``viscous shock,'' which, roughly speaking, is the location of the bottom of the V. We describe the limiting rescaled fluctuations, and in particular show that the fluctuations of the shock location are not tight, for both stationary and flat initial data. We also show that if the KPZ equation is started with V-shaped initial data, then the long-time limits of the time-averaged laws of the spatial increments of the solution are mixtures of the laws of the spatial increments of $x\\mapsto B(x)+\\theta x$ and $x\\mapsto B(x)-\\theta x$, where $B$ is a standard two-sided Brownian motion.","sentences":["We study ``V-shaped'' solutions to the KPZ equation, those having opposite asymptotic slopes $\\theta$ and $-\\theta$, with $\\theta>0$, at positive and negative infinity, respectively.","Answering a question of Janjigian, Rassoul-Agha, and Sepp\\\"al\\\"ainen, we show that the spatial increments of V-shaped solutions cannot be statistically stationary in time.","This completes the classification of statistically time-stationary spatial increments for the KPZ equation by ruling out the last case left by those authors.   ","To show that these V-shaped time-stationary measures do not exist, we study the location of the corresponding ``viscous shock,'' which, roughly speaking, is the location of the bottom of the V. We describe the limiting rescaled fluctuations, and in particular show that the fluctuations of the shock location are not tight, for both stationary and flat initial data.","We also show that if the KPZ equation is started with V-shaped initial data, then the long-time limits of the time-averaged laws of the spatial increments of the solution are mixtures of the laws of the spatial increments of $x\\mapsto B(x)+\\theta x$ and $x\\mapsto B(x)-\\theta x$, where $B$ is a standard two-sided Brownian motion."],"url":"http://arxiv.org/abs/2406.06502v1","category":"math.PR"}
{"created":"2024-06-10 17:29:03","title":"Input Driven Synchronization of Chaotic Neural Networks with Analyticaly Determined Conditional Lyapunov Exponents","abstract":"Recurrent neural networks (RNNs) with random, but sufficiently strong and balanced coupling display a well known high-dimensional chaotic dynamics. Here, we investigate if externally applied inputs to these RNNs can stabilize globally synchronous, input-dependent solutions, in spite of the strong chaos-inducing coupling. We find that when the balance between excitation and inhibition is exact, that is when the row-sum of the weights is constant and 0, a globally applied input can readily synchronize all neurons onto a synchronous solution. The stability of the synchronous solution is analytically explored in this work with a master stability function. For any synchronous solution to the network dynamics, the conditional Lyapunov spectrum can be readily determined, with the stability of the synchronous solution critically dependent on the largest real eigenvalue component of the RNN weight matrix. We find that the smaller the maximum real component of the weight matrix eigenvalues, the more readily the network synchronizes. Further, the conditional Lyapunov exponents are easily computed numerically for any synchronization signal without simulating the RNN. Finally, for certain oscillatory synchronization signals, the conditional Lyapunov exponents can be determined analytically.","sentences":["Recurrent neural networks (RNNs) with random, but sufficiently strong and balanced coupling display a well known high-dimensional chaotic dynamics.","Here, we investigate if externally applied inputs to these RNNs can stabilize globally synchronous, input-dependent solutions, in spite of the strong chaos-inducing coupling.","We find that when the balance between excitation and inhibition is exact, that is when the row-sum of the weights is constant and 0, a globally applied input can readily synchronize all neurons onto a synchronous solution.","The stability of the synchronous solution is analytically explored in this work with a master stability function.","For any synchronous solution to the network dynamics, the conditional Lyapunov spectrum can be readily determined, with the stability of the synchronous solution critically dependent on the largest real eigenvalue component of the RNN weight matrix.","We find that the smaller the maximum real component of the weight matrix eigenvalues, the more readily the network synchronizes.","Further, the conditional Lyapunov exponents are easily computed numerically for any synchronization signal without simulating the RNN.","Finally, for certain oscillatory synchronization signals, the conditional Lyapunov exponents can be determined analytically."],"url":"http://arxiv.org/abs/2406.06491v1","category":"nlin.CD"}
{"created":"2024-06-10 17:27:12","title":"Probing out-of-distribution generalization in machine learning for materials","abstract":"Scientific machine learning (ML) endeavors to develop generalizable models with broad applicability. However, the assessment of generalizability is often based on heuristics. Here, we demonstrate in the materials science setting that heuristics based evaluations lead to substantially biased conclusions of ML generalizability and benefits of neural scaling. We evaluate generalization performance in over 700 out-of-distribution tasks that features new chemistry or structural symmetry not present in the training data. Surprisingly, good performance is found in most tasks and across various ML models including simple boosted trees. Analysis of the materials representation space reveals that most tasks contain test data that lie in regions well covered by training data, while poorly-performing tasks contain mainly test data outside the training domain. For the latter case, increasing training set size or training time has marginal or even adverse effects on the generalization performance, contrary to what the neural scaling paradigm assumes. Our findings show that most heuristically-defined out-of-distribution tests are not genuinely difficult and evaluate only the ability to interpolate. Evaluating on such tasks rather than the truly challenging ones can lead to an overestimation of generalizability and benefits of scaling.","sentences":["Scientific machine learning (ML) endeavors to develop generalizable models with broad applicability.","However, the assessment of generalizability is often based on heuristics.","Here, we demonstrate in the materials science setting that heuristics based evaluations lead to substantially biased conclusions of ML generalizability and benefits of neural scaling.","We evaluate generalization performance in over 700 out-of-distribution tasks that features new chemistry or structural symmetry not present in the training data.","Surprisingly, good performance is found in most tasks and across various ML models including simple boosted trees.","Analysis of the materials representation space reveals that most tasks contain test data that lie in regions well covered by training data, while poorly-performing tasks contain mainly test data outside the training domain.","For the latter case, increasing training set size or training time has marginal or even adverse effects on the generalization performance, contrary to what the neural scaling paradigm assumes.","Our findings show that most heuristically-defined out-of-distribution tests are not genuinely difficult and evaluate only the ability to interpolate.","Evaluating on such tasks rather than the truly challenging ones can lead to an overestimation of generalizability and benefits of scaling."],"url":"http://arxiv.org/abs/2406.06489v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 17:25:46","title":"Continuum Attention for Neural Operators","abstract":"Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.","sentences":["Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning.","Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems.","Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators.","Motivated by this, we study transformers in the function space setting.","We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator.","The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result.","The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures.","For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators.","Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators."],"url":"http://arxiv.org/abs/2406.06486v1","category":"cs.LG"}
{"created":"2024-06-10 17:14:53","title":"DiffAudit: Auditing Privacy Practices of Online Services for Children and Adolescents","abstract":"Children's and adolescents' online data privacy are regulated by laws such as the Children's Online Privacy Protection Act (COPPA) and the California Consumer Privacy Act (CCPA). Online services that are directed towards general audiences (i.e., including children, adolescents, and adults) must comply with these laws. In this paper, first, we present DiffAudit, a platform-agnostic privacy auditing methodology for general audience services. DiffAudit performs differential analysis of network traffic data flows to compare data processing practices (i) between child, adolescent, and adult users and (ii) before and after consent is given and user age is disclosed. We also present a data type classification method that utilizes GPT-4 and our data type ontology based on COPPA and CCPA, allowing us to identify considerably more data types than prior work. Second, we apply DiffAudit to a set of popular general audience mobile and web services and observe a rich set of behaviors extracted from over 440K outgoing requests, containing 3,968 unique data types we extracted and classified. We reveal problematic data processing practices prior to consent and age disclosure, lack of differentiation between age-specific data flows, inconsistent privacy policy disclosures, and sharing of linkable data with third parties, including advertising and tracking services.","sentences":["Children's and adolescents' online data privacy are regulated by laws such as the Children's Online Privacy Protection Act (COPPA) and the California Consumer Privacy Act (CCPA).","Online services that are directed towards general audiences (i.e., including children, adolescents, and adults) must comply with these laws.","In this paper, first, we present DiffAudit, a platform-agnostic privacy auditing methodology for general audience services.","DiffAudit performs differential analysis of network traffic data flows to compare data processing practices (i) between child, adolescent, and adult users and (ii) before and after consent is given and user age is disclosed.","We also present a data type classification method that utilizes GPT-4 and our data type ontology based on COPPA and CCPA, allowing us to identify considerably more data types than prior work.","Second, we apply DiffAudit to a set of popular general audience mobile and web services and observe a rich set of behaviors extracted from over 440K outgoing requests, containing 3,968 unique data types we extracted and classified.","We reveal problematic data processing practices prior to consent and age disclosure, lack of differentiation between age-specific data flows, inconsistent privacy policy disclosures, and sharing of linkable data with third parties, including advertising and tracking services."],"url":"http://arxiv.org/abs/2406.06473v1","category":"cs.CR"}
{"created":"2024-06-10 17:10:41","title":"It\u00f4's Formula for the Rearranged Stochastic Heat Equation","abstract":"The purpose of this short note is to prove a convenient version of It\\^o's formula for the Rearranged Stochastic Heat Equation (RSHE) introduced by the two authors in a previous contribution. This equation is a penalised version of the standard Stochastic Heat Equation (SHE) on the circle subject to a coloured noise, whose solution is constrained to stay within the set of symmetric quantile functions by means of a reflection term. Here, we identity the generator of the solution when it is acting on functions defined on the space ${\\mathcal P}_2({\\mathbb R})$ (of one-dimensional probability measures with a finite second moment) that are assumed to be smooth in Lions' sense. In particular, we prove that the reflection term in the RSHE is orthogonal to the Lions (or Wasserstein) derivative of smooth functions defined on ${\\mathcal P}_2({\\mathbb R})$. The proof relies on non-trivial bounds for the gradient of the solution to the RSHE.","sentences":["The purpose of this short note is to prove a convenient version of It\\^o's formula for the Rearranged Stochastic Heat Equation (RSHE) introduced by the two authors in a previous contribution.","This equation is a penalised version of the standard Stochastic Heat Equation (SHE) on the circle subject to a coloured noise, whose solution is constrained to stay within the set of symmetric quantile functions by means of a reflection term.","Here, we identity the generator of the solution when it is acting on functions defined on the space ${\\mathcal P}_2({\\mathbb R})$ (of one-dimensional probability measures with a finite second moment) that are assumed to be smooth in Lions' sense.","In particular, we prove that the reflection term in the RSHE is orthogonal to the Lions (or Wasserstein) derivative of smooth functions defined on ${\\mathcal P}_2({\\mathbb R})$.","The proof relies on non-trivial bounds for the gradient of the solution to the RSHE."],"url":"http://arxiv.org/abs/2406.06471v1","category":"math.PR"}
{"created":"2024-06-10 16:42:44","title":"Time Series Analysis: yesterday, today, tomorrow","abstract":"Forecasts of various processes have always been a sophisticated problem for statistics and data science. Over the past decades the solution procedures were updated by deep learning and kernel methods. According to many specialists, these approaches are much more precise, stable, and suitable compared to the classical statistical linear time series methods. Here we investigate how true this point of view is.","sentences":["Forecasts of various processes have always been a sophisticated problem for statistics and data science.","Over the past decades the solution procedures were updated by deep learning and kernel methods.","According to many specialists, these approaches are much more precise, stable, and suitable compared to the classical statistical linear time series methods.","Here we investigate how true this point of view is."],"url":"http://arxiv.org/abs/2406.06453v1","category":"cs.CY"}
{"created":"2024-06-10 16:34:30","title":"Interpretability of Language Models via Task Spaces","abstract":"The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.","sentences":["The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes.","In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities.","To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena.","Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'.","To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD).","We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure.","Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks.","The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."],"url":"http://arxiv.org/abs/2406.06441v1","category":"cs.CL"}
{"created":"2024-06-10 16:24:46","title":"Spatiotemporal Graph Neural Network Modelling Perfusion MRI","abstract":"Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and promises to predict tumor genotypes, thus benefiting prognosis for glioma patients, yet effective models tailored to 4D pMRI are still lacking. This study presents the first attempt to model 4D pMRI using a GNN-based spatiotemporal model PerfGAT, integrating spatial information and temporal kinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma patients. Specifically, we propose a graph structure learning approach based on edge attention and negative graphs to optimize temporal correlations modeling. Moreover, we design a dual-attention feature fusion module to integrate spatiotemporal features while addressing tumor-related brain regions. Further, we develop a class-balanced augmentation methods tailored to spatiotemporal data, which could mitigate the common label imbalance issue in clinical datasets. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art approaches, promising to model pMRI effectively for patient characterization.","sentences":["Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and promises to predict tumor genotypes, thus benefiting prognosis for glioma patients, yet effective models tailored to 4D pMRI are still lacking.","This study presents the first attempt to model 4D pMRI using a GNN-based spatiotemporal model PerfGAT, integrating spatial information and temporal kinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma patients.","Specifically, we propose a graph structure learning approach based on edge attention and negative graphs to optimize temporal correlations modeling.","Moreover, we design a dual-attention feature fusion module to integrate spatiotemporal features while addressing tumor-related brain regions.","Further, we develop a class-balanced augmentation methods tailored to spatiotemporal data, which could mitigate the common label imbalance issue in clinical datasets.","Our experimental results demonstrate that the proposed method outperforms other state-of-the-art approaches, promising to model pMRI effectively for patient characterization."],"url":"http://arxiv.org/abs/2406.06434v1","category":"eess.IV"}
{"created":"2024-06-10 16:12:00","title":"Foundation Inference Models for Markov Jump Processes","abstract":"Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observation process. Second, a neural network model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that one and the same (pretrained) model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are finetuned to the target datasets.","sentences":["Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces.","These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial.","In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components.","First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observation process.","Second, a neural network model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way.","We empirically demonstrate that one and the same (pretrained) model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities.","Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models.","What is more, we show that our model performs on par with state-of-the-art models which are finetuned to the target datasets."],"url":"http://arxiv.org/abs/2406.06419v1","category":"cs.LG"}
{"created":"2024-06-10 16:09:16","title":"Explainable Graph Neural Networks Under Fire","abstract":"Predictions made by graph neural networks (GNNs) usually lack interpretability due to their complex computational behavior and the abstract nature of graphs. In an attempt to tackle this, many GNN explanation methods have emerged. Their goal is to explain a model's predictions and thereby obtain trust when GNN models are deployed in decision critical applications. Most GNN explanation methods work in a post-hoc manner and provide explanations in the form of a small subset of important edges and/or nodes. In this paper we demonstrate that these explanations can unfortunately not be trusted, as common GNN explanation methods turn out to be highly susceptible to adversarial perturbations. That is, even small perturbations of the original graph structure that preserve the model's predictions may yield drastically different explanations. This calls into question the trustworthiness and practical utility of post-hoc explanation methods for GNNs. To be able to attack GNN explanation models, we devise a novel attack method dubbed \\textit{GXAttack}, the first \\textit{optimization-based} adversarial attack method for post-hoc GNN explanations under such settings. Due to the devastating effectiveness of our attack, we call for an adversarial evaluation of future GNN explainers to demonstrate their robustness.","sentences":["Predictions made by graph neural networks (GNNs) usually lack interpretability due to their complex computational behavior and the abstract nature of graphs.","In an attempt to tackle this, many GNN explanation methods have emerged.","Their goal is to explain a model's predictions and thereby obtain trust when GNN models are deployed in decision critical applications.","Most GNN explanation methods work in a post-hoc manner and provide explanations in the form of a small subset of important edges and/or nodes.","In this paper we demonstrate that these explanations can unfortunately not be trusted, as common GNN explanation methods turn out to be highly susceptible to adversarial perturbations.","That is, even small perturbations of the original graph structure that preserve the model's predictions may yield drastically different explanations.","This calls into question the trustworthiness and practical utility of post-hoc explanation methods for GNNs.","To be able to attack GNN explanation models, we devise a novel attack method dubbed \\textit{GXAttack}, the first \\textit{optimization-based} adversarial attack method for post-hoc GNN explanations under such settings.","Due to the devastating effectiveness of our attack, we call for an adversarial evaluation of future GNN explainers to demonstrate their robustness."],"url":"http://arxiv.org/abs/2406.06417v1","category":"cs.LG"}
{"created":"2024-06-10 16:04:58","title":"Bose-Einstein condensates of microwave-shielded polar molecules","abstract":"We investigate the ground-state properties of the ultracold gases of bosonic microwave-shielded polar molecules. To account for the large shielding core of the inter-molecular potential, we adopt a variational ansatz incorporating the Jastrow correlation factor. We show that the system is always stable and supports a self-bound gas phase and an expanding gas phase. We also calculate the condensate fraction which is significantly reduced when the size of the shielding core of the two-body potential becomes comparable to the inter-molecular distance. Our studies distinguish the molecular condensates from the atomic ones and invalidate the application of the Gross-Pitaevskii equation to the microwave-shielded molecular gases. Our work paves the way for studying the Bose-Einstein condensations of ultracold gases of microwave-shielded polar molecules.","sentences":["We investigate the ground-state properties of the ultracold gases of bosonic microwave-shielded polar molecules.","To account for the large shielding core of the inter-molecular potential, we adopt a variational ansatz incorporating the Jastrow correlation factor.","We show that the system is always stable and supports a self-bound gas phase and an expanding gas phase.","We also calculate the condensate fraction which is significantly reduced when the size of the shielding core of the two-body potential becomes comparable to the inter-molecular distance.","Our studies distinguish the molecular condensates from the atomic ones and invalidate the application of the Gross-Pitaevskii equation to the microwave-shielded molecular gases.","Our work paves the way for studying the Bose-Einstein condensations of ultracold gases of microwave-shielded polar molecules."],"url":"http://arxiv.org/abs/2406.06412v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-10 16:02:59","title":"On the structure of the value function of optimal exit time problems","abstract":"In this paper, we study an optimal exit time problem with general running and terminal costs and a target $\\mathcal{S}\\subset\\mathbb{R}^d$ having an inner ball property for a nonlinear control system that satisfies mild controllability assumptions. In particular, Petrov's condition at the boundary of $\\mathcal{S}$ is not required and the value function $V$ may fail to be locally Lipschitz. In such a weakened set-up, we first establish a representation formula for proximal (horizontal) supergradients of $V$ by using transported proximal normal vectors. This allows us to obtain an external sphere condition for the hypograph of $V$ which yields several regularity properties. In particular, $V$ is almost everywhere twice differentiable and the Hausdorff dimension of its singularities is not greater than $d-1/2$. Furthermore, besides optimality conditions for trajectories of the optimal control problem, we extend the analysis to propagation of singularities and differentiability properties of the value function. An upper bound for the Hausdorff measure of the singular set is also studied, which implies that $V$ is a function of special bounded variation.","sentences":["In this paper, we study an optimal exit time problem with general running and terminal costs and a target $\\mathcal{S}\\subset\\mathbb{R}^d$ having an inner ball property for a nonlinear control system that satisfies mild controllability assumptions.","In particular, Petrov's condition at the boundary of $\\mathcal{S}$ is not required and the value function $V$ may fail to be locally Lipschitz.","In such a weakened set-up, we first establish a representation formula for proximal (horizontal) supergradients of $V$ by using transported proximal normal vectors.","This allows us to obtain an external sphere condition for the hypograph of $V$ which yields several regularity properties.","In particular, $V$ is almost everywhere twice differentiable and the Hausdorff dimension of its singularities is not greater than $d-1/2$. Furthermore, besides optimality conditions for trajectories of the optimal control problem, we extend the analysis to propagation of singularities and differentiability properties of the value function.","An upper bound for the Hausdorff measure of the singular set is also studied, which implies that $V$ is a function of special bounded variation."],"url":"http://arxiv.org/abs/2406.06409v1","category":"math.OC"}
{"created":"2024-06-10 15:44:49","title":"Time-tronics: from temporal printed circuit board to quantum computer","abstract":"Time crystalline structures can be created in periodically driven systems. They are temporal lattices which can reveal different condensed matter behaviours ranging from Anderson localization in time to temporal analogues of many-body localization or topological insulators. However, the potential practical applications of time crystalline structures have yet to be explored. Here, we pave the way for time-tronics where temporal lattices are like printed circuit boards for realization of a broad range of quantum devices. The elements of these devices can correspond to structures of dimensions higher than three and can be arbitrarily connected and reconfigured at any moment. Moreover, our approach allows for the construction of a quantum computer, enabling quantum gate operations for all possible pairs of qubits. Our findings indicate that the limitations faced in building devices using conventional spatial crystals can be overcome by adopting crystalline structures in time.","sentences":["Time crystalline structures can be created in periodically driven systems.","They are temporal lattices which can reveal different condensed matter behaviours ranging from Anderson localization in time to temporal analogues of many-body localization or topological insulators.","However, the potential practical applications of time crystalline structures have yet to be explored.","Here, we pave the way for time-tronics where temporal lattices are like printed circuit boards for realization of a broad range of quantum devices.","The elements of these devices can correspond to structures of dimensions higher than three and can be arbitrarily connected and reconfigured at any moment.","Moreover, our approach allows for the construction of a quantum computer, enabling quantum gate operations for all possible pairs of qubits.","Our findings indicate that the limitations faced in building devices using conventional spatial crystals can be overcome by adopting crystalline structures in time."],"url":"http://arxiv.org/abs/2406.06387v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-10 15:22:10","title":"On inverse scattering for the two-dimensional nonlinear Klein-Gordon equation","abstract":"The inverse scattering problem for the two-dimensional nonlinear Klein-Gordon equation $u_{tt}-\\Delta u + u = \\mathcal{N}(u)$ is studied. We assume that the unknown nonlinearity $\\mathcal{N}$ of the equation satisfies $\\mathcal{N}\\in C^\\infty(\\mathbb{R};\\mathbb{R})$, $\\mathcal{N}^{(k)}(y)=O(|y|^{\\max\\{ 3-k,0 \\}})$ ($y \\to 0$) and $\\mathcal{N}^{(k)}(y)=O(e^{c y^2})$ ($|y| \\to \\infty$) for any $k=0,1,2,\\cdots$. Here, $c$ is a positive constant. We establish a reconstraction formula of $\\mathcal{N}^{(k)}(0)$ ($k=3,4,5,\\cdots$) by the knowledge of the scattering operator for the equation. As an application, we also give an expression for higher order G\\^{a}teaux differentials of the scattering operator at 0.","sentences":["The inverse scattering problem for the two-dimensional nonlinear Klein-Gordon equation $u_{tt}-\\Delta u + u = \\mathcal{N}(u)$ is studied.","We assume that the unknown nonlinearity $\\mathcal{N}$ of the equation satisfies $\\mathcal{N}\\in C^\\infty(\\mathbb{R};\\mathbb{R})$, $\\mathcal{N}^{(k)}(y)=O(|y|^{\\max\\{ 3-k,0 \\}})$ ($y \\to 0$) and $\\mathcal{N}^{(k)}(y)=O(e^{c y^2})$ ($|y| \\to \\infty$) for any $k=0,1,2,\\cdots$. Here, $c$ is a positive constant.","We establish a reconstraction formula of $\\mathcal{N}^{(k)}(0)$ ($k=3,4,5,\\cdots$) by the knowledge of the scattering operator for the equation.","As an application, we also give an expression for higher order G\\^{a}teaux differentials of the scattering operator at 0."],"url":"http://arxiv.org/abs/2406.06362v1","category":"math.AP"}
{"created":"2024-06-10 15:21:30","title":"Challenges with Differentiable Quantum Dynamics","abstract":"Differentiable quantum dynamics require automatic differentiation of a complex-valued initial value problem, which numerically integrates a system of ordinary differential equations from a specified initial condition, as well as the eigendecomposition of a matrix. We explored several automatic differentiation frameworks for these tasks, finding that no framework natively supports our application requirements. We therefore demonstrate a need for broader support of complex-valued, differentiable numerical integration in scientific computing libraries.","sentences":["Differentiable quantum dynamics require automatic differentiation of a complex-valued initial value problem, which numerically integrates a system of ordinary differential equations from a specified initial condition, as well as the eigendecomposition of a matrix.","We explored several automatic differentiation frameworks for these tasks, finding that no framework natively supports our application requirements.","We therefore demonstrate a need for broader support of complex-valued, differentiable numerical integration in scientific computing libraries."],"url":"http://arxiv.org/abs/2406.06361v1","category":"quant-ph"}
{"created":"2024-06-10 15:19:49","title":"A Field-Theoretic Example for Hodge Theory in 3D","abstract":"We focus on the continuous symmetry transformations for the three ($2 + 1$)-dimensional (3D) system of a combination of the free Abelian 1-form and 2-form gauge theories within the framework of Becchi-Rouet-Stora-Tyutin (BRST) formalism. We establish that this combined system is a tractable field-theoretic model of Hodge theory. The symmetry operators of our present theory provide the physical realizations of the de Rham cohomological operators of differential geometry at the algebraic level. Our present investigation is important in the sense that, for the first time, we are able to establish an odd dimensional (i.e. $D = 3$) field-theoretic system to be an example for Hodge theory (besides earlier works on a few interesting ($0 + 1$)-dimensional toy models as well as a set of well-known ${\\mathcal N} = 2$ SUSY quantum mechanical systems of physical interest).","sentences":["We focus on the continuous symmetry transformations for the three ($2 + 1$)-dimensional (3D) system of a combination of the free Abelian 1-form and 2-form gauge theories within the framework of Becchi-Rouet-Stora-Tyutin (BRST) formalism.","We establish that this combined system is a tractable field-theoretic model of Hodge theory.","The symmetry operators of our present theory provide the physical realizations of the de Rham cohomological operators of differential geometry at the algebraic level.","Our present investigation is important in the sense that, for the first time, we are able to establish an odd dimensional (i.e. $D = 3$) field-theoretic system to be an example for Hodge theory (besides earlier works on a few interesting ($0 + 1$)-dimensional toy models as well as a set of well-known ${\\mathcal N} = 2$ SUSY quantum mechanical systems of physical interest)."],"url":"http://arxiv.org/abs/2406.06358v1","category":"hep-th"}
{"created":"2024-06-10 15:12:53","title":"Error Analysis and Numerical Algorithm for PDE Approximation with Hidden-Layer Concatenated Physics Informed Neural Networks","abstract":"We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs). We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations). We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons. The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees. This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\\tanh$ activation function. Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation. Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses.","sentences":["We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs).","We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations).","We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons.","The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees.","This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\\tanh$ activation function.","Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation.","Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses."],"url":"http://arxiv.org/abs/2406.06350v1","category":"math.NA"}
{"created":"2024-06-10 15:06:02","title":"Dynamical Mean-Field Theory of Complex Systems on Sparse Directed Networks","abstract":"Although real-world complex systems typically interact through sparse and heterogeneous networks, analytic solutions of their dynamics are limited to models with all-to-all interactions. Here, we solve the dynamics of a broad range of nonlinear models of complex systems on sparse directed networks with a random structure. By generalizing dynamical mean-field theory to sparse systems, we derive an exact equation for the path-probability describing the effective dynamics of a single degree of freedom. Our general solution applies to key models in the study of neural networks, ecosystems, epidemic spreading, and synchronization. Using the population dynamics algorithm, we solve the path-probability equation to determine the phase diagram of a seminal neural network model in the sparse regime, showing that this model undergoes a transition from a fixed-point phase to chaos as a function of the network topology.","sentences":["Although real-world complex systems typically interact through sparse and heterogeneous networks, analytic solutions of their dynamics are limited to models with all-to-all interactions.","Here, we solve the dynamics of a broad range of nonlinear models of complex systems on sparse directed networks with a random structure.","By generalizing dynamical mean-field theory to sparse systems, we derive an exact equation for the path-probability describing the effective dynamics of a single degree of freedom.","Our general solution applies to key models in the study of neural networks, ecosystems, epidemic spreading, and synchronization.","Using the population dynamics algorithm, we solve the path-probability equation to determine the phase diagram of a seminal neural network model in the sparse regime, showing that this model undergoes a transition from a fixed-point phase to chaos as a function of the network topology."],"url":"http://arxiv.org/abs/2406.06346v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-10 14:59:01","title":"Audio-based Step-count Estimation for Running -- Windowing and Neural Network Baselines","abstract":"In recent decades, running has become an increasingly popular pastime activity due to its accessibility, ease of practice, and anticipated health benefits. However, the risk of running-related injuries is substantial for runners of different experience levels. Several common forms of injuries result from overuse -- extending beyond the recommended running time and intensity. Recently, audio-based tracking has emerged as yet another modality for monitoring running behaviour and performance, with previous studies largely concentrating on predicting runner fatigue. In this work, we investigate audio-based step count estimation during outdoor running, achieving a mean absolute error of 1.098 in window-based step-count differences and a Pearson correlation coefficient of 0.479 when predicting the number of steps in a 5-second window of audio. Our work thus showcases the feasibility of audio-based monitoring for estimating important physiological variables and lays the foundations for further utilising audio sensors for a more thorough characterisation of runner behaviour.","sentences":["In recent decades, running has become an increasingly popular pastime activity due to its accessibility, ease of practice, and anticipated health benefits.","However, the risk of running-related injuries is substantial for runners of different experience levels.","Several common forms of injuries result from overuse -- extending beyond the recommended running time and intensity.","Recently, audio-based tracking has emerged as yet another modality for monitoring running behaviour and performance, with previous studies largely concentrating on predicting runner fatigue.","In this work, we investigate audio-based step count estimation during outdoor running, achieving a mean absolute error of 1.098 in window-based step-count differences and a Pearson correlation coefficient of 0.479 when predicting the number of steps in a 5-second window of audio.","Our work thus showcases the feasibility of audio-based monitoring for estimating important physiological variables and lays the foundations for further utilising audio sensors for a more thorough characterisation of runner behaviour."],"url":"http://arxiv.org/abs/2406.06339v1","category":"cs.SD"}
{"created":"2024-06-10 17:59:52","title":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation","abstract":"We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.","sentences":["We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain.","It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly.","We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality.","The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark.","(2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment.","(4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup.","We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models."],"url":"http://arxiv.org/abs/2406.06525v1","category":"cs.CV"}
{"created":"2024-06-10 17:54:57","title":"Random Features Approximation for Control-Affine Systems","abstract":"Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback. Many nonlinear dynamical systems of interest are control affine. We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence. Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost. We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce. We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF). Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods.","sentences":["Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback.","Many nonlinear dynamical systems of interest are control affine.","We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence.","Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost.","We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce.","We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF).","Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods."],"url":"http://arxiv.org/abs/2406.06514v2","category":"cs.LG"}
{"created":"2024-06-10 17:48:36","title":"Robust Distribution Learning with Local and Global Adversarial Corruptions","abstract":"We consider learning in an adversarial environment, where an $\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily modified (*global* corruptions) and the remaining perturbations have average magnitude bounded by $\\rho$ (*local* corruptions). Given access to $n$ such corrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$ that minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact, we attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n, \\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$, with performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to account simultaneously for mean estimation ($k=1$), distribution estimation ($k=d$), as well as the settings interpolating between these two extremes. We characterize the optimal population-limit risk for this task and then develop an efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon k} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order $2+\\delta$, for constant $\\delta > 0$. For data distributions with bounded covariance, our finite-sample bounds match the minimax population-level optimum for large sample sizes. Our efficient procedure relies on a novel trace norm approximation of an ideal yet intractable 2-Wasserstein projection estimator. We apply this algorithm to robust stochastic optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in Wasserstein distributionally robust optimization.","sentences":["We consider learning in an adversarial environment, where an $\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily modified (*global* corruptions) and the remaining perturbations have average magnitude bounded by $\\rho$ (*local* corruptions).","Given access to $n$ such corrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$ that minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact, we attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n, \\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$, with performance scaling with $\\mathrm{rank}(\\Pi)","= k$.","This allows us to account simultaneously for mean estimation ($k=1$), distribution estimation ($k=d$), as well as the settings interpolating between these two extremes.","We characterize the optimal population-limit risk for this task and then develop an efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon k} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order $2+\\delta$, for constant $\\delta > 0$.","For data distributions with bounded covariance, our finite-sample bounds match the minimax population-level optimum for large sample sizes.","Our efficient procedure relies on a novel trace norm approximation of an ideal yet intractable 2-Wasserstein projection estimator.","We apply this algorithm to robust stochastic optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in Wasserstein distributionally robust optimization."],"url":"http://arxiv.org/abs/2406.06509v1","category":"cs.LG"}
{"created":"2024-06-10 17:44:11","title":"Online Newton Method for Bandit Convex Optimisation","abstract":"We introduce a computationally efficient algorithm for zeroth-order bandit convex optimisation and prove that in the adversarial setting its regret is at most $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$ is the dimension and $n$ is the time horizon. In the stochastic setting the bound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in [d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the constraint set and the desired computational properties.","sentences":["We introduce a computationally efficient algorithm for zeroth-order bandit convex optimisation and prove that in the adversarial setting its regret is at most $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$ is the dimension and $n$ is the time horizon.","In the stochastic setting the bound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in [d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the constraint set and the desired computational properties."],"url":"http://arxiv.org/abs/2406.06506v1","category":"math.OC"}
{"created":"2024-06-10 17:31:36","title":"Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation","abstract":"Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation. Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics. Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy.","sentences":["Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm.","Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations.","We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation.","Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics.","Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy."],"url":"http://arxiv.org/abs/2406.06496v1","category":"cs.LG"}
{"created":"2024-06-10 17:14:19","title":"Multi-Amplifier Sensing Charge-coupled Devices for Next Generation Spectroscopy","abstract":"We present characterization results and performance of a prototype Multiple-Amplifier Sensing (MAS) silicon charge-coupled device (CCD) sensor with 16 channels potentially suitable for faint object astronomical spectroscopy and low-signal, photon-limited imaging. The MAS CCD is designed to reach sub-electron readout noise by repeatedly measuring charge through a line of amplifiers during the serial transfer shifts. Using synchronized readout electronics based on the DESI CCD controller, we report a read noise of 1.03 e- rms/pix at a speed of 26 $\\mu$s/pix with a single-sample readout scheme where charge in a pixel is measured only once for each output stage. At these operating parameters, we find the amplifier-to-amplifier charge transfer efficiency (ACTE) to be $>0.9995$ at low counts for all amplifiers but one for which the ACTE is 0.997. This charge transfer efficiency falls above 50,000 electrons for the read-noise optimized voltage configuration we chose for the serial clocks and gates. The amplifier linearity across a broad dynamic range from $\\sim$300--35,000 e- was also measured to be $\\pm 2.5\\%$. We describe key operating parameters to optimize on these characteristics and describe the specific applications for which the MAS CCD may be a suitable detector candidate.","sentences":["We present characterization results and performance of a prototype Multiple-Amplifier Sensing (MAS) silicon charge-coupled device (CCD) sensor with 16 channels potentially suitable for faint object astronomical spectroscopy and low-signal, photon-limited imaging.","The MAS CCD is designed to reach sub-electron readout noise by repeatedly measuring charge through a line of amplifiers during the serial transfer shifts.","Using synchronized readout electronics based on the DESI CCD controller, we report a read noise of 1.03 e-","rms/pix at a speed of 26 $\\mu$s/pix with a single-sample readout scheme where charge in a pixel is measured only once for each output stage.","At these operating parameters, we find the amplifier-to-amplifier charge transfer efficiency (ACTE) to be $>0.9995$ at low counts for all amplifiers but one for which the ACTE is 0.997.","This charge transfer efficiency falls above 50,000 electrons for the read-noise optimized voltage configuration we chose for the serial clocks and gates.","The amplifier linearity across a broad dynamic range from $\\sim$300--35,000 e- was also measured to be $\\pm 2.5\\%$.","We describe key operating parameters to optimize on these characteristics and describe the specific applications for which the MAS CCD may be a suitable detector candidate."],"url":"http://arxiv.org/abs/2406.06472v1","category":"astro-ph.IM"}
{"created":"2024-06-10 16:54:51","title":"Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots","abstract":"In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions. Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error. Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success. For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback. Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location. We assessed our approach through a series of experiments in both simulated and real-world environments. The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp.","sentences":["In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions.","Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error.","Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success.","For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback.","Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location.","We assessed our approach through a series of experiments in both simulated and real-world environments.","The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp."],"url":"http://arxiv.org/abs/2406.06460v1","category":"cs.RO"}
{"created":"2024-06-10 16:53:58","title":"How Useful is Intermittent, Asynchronous Expert Feedback for Bayesian Optimization?","abstract":"Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking. However, scientists often have strong intuition, and thus human feedback is still useful. Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs. In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign. To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process. The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process. Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO. This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly.","sentences":["Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking.","However, scientists often have strong intuition, and thus human feedback is still useful.","Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs.","In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign.","To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process.","The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process.","Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO.","This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly."],"url":"http://arxiv.org/abs/2406.06459v1","category":"cs.LG"}
{"created":"2024-06-10 16:46:13","title":"Improved convergence rates for the multiobjective Frank-Wolfe method","abstract":"This paper analyzes the convergence rates of the {\\it Frank-Wolfe } method for solving convex constrained multiobjective optimization. We establish improved convergence rates under different assumptions on the objective function, the feasible set, and the localization of the limit point of the sequence generated by the method. In terms of the objective function values, we firstly show that if the objective function is strongly convex and the limit point of the sequence generated by the method lies in the relative interior of the feasible set, then the algorithm achieves a linear convergence rate. Next, we focus on a special class of problems where the feasible constraint set is $(\\alpha,q)$-uniformly convex for some $\\alpha >0$ and $q \\geq 2$, including, in particular, \\(\\ell_p\\)-balls for all $p>1$. In this context, we prove that the method attains: (i) a rate of $\\mathcal{O}(1/k^\\frac{q}{q-1})$ when the objective function is strongly convex; and (ii) a linear rate (if $q=2$) or a rate of $\\mathcal{O}(1/k^{\\frac{q}{q-2}})$ (if $q>2$) under an additional assumption, which always holds if the feasible set does not contain an unconstrained weak Pareto point. We also discuss enhanced convergence rates for the algorithm in terms of an optimality measure. Finally, we provide some simple examples to illustrate the convergence rates and the set of assumptions.","sentences":["This paper analyzes the convergence rates of the {\\it Frank-Wolfe } method for solving convex constrained multiobjective optimization.","We establish improved convergence rates under different assumptions on the objective function, the feasible set, and the localization of the limit point of the sequence generated by the method.","In terms of the objective function values, we firstly show that if the objective function is strongly convex and the limit point of the sequence generated by the method lies in the relative interior of the feasible set, then the algorithm achieves a linear convergence rate.","Next, we focus on a special class of problems where the feasible constraint set is $(\\alpha,q)$-uniformly convex for some $\\alpha >0$ and $q \\geq 2$, including, in particular, \\(\\ell_p\\)-balls for all $p>1$. In this context, we prove that the method attains: (i) a rate of $\\mathcal{O}(1/k^\\frac{q}{q-1})$ when the objective function is strongly convex; and (ii) a linear rate (if $q=2$) or a rate of $\\mathcal{O}(1/k^{\\frac{q}{q-2}})$ (if $q>2$) under an additional assumption, which always holds if the feasible set does not contain an unconstrained weak Pareto point.","We also discuss enhanced convergence rates for the algorithm in terms of an optimality measure.","Finally, we provide some simple examples to illustrate the convergence rates and the set of assumptions."],"url":"http://arxiv.org/abs/2406.06457v1","category":"math.OC"}
{"created":"2024-06-10 16:35:52","title":"Parallel Quantum Local Search via Evolutionary Mechanism","abstract":"We propose an innovative Parallel Quantum Local Search (PQLS) methodology that leverages the capabilities of small-scale quantum computers to efficiently address complex combinatorial optimization problems. Traditional Quantum Local Search (QLS) methods face limitations due to the sequential nature of solving sub-problems, which arises from dependencies between their solutions. Our approach transcends this constraint by simultaneously executing multiple QLS pathways and aggregating their most effective outcomes at certain intervals to establish a ``generation''. Each subsequent generation commences with the optimal solution from its predecessor, thereby significantly accelerating the convergence towards an optimal solution. Our findings demonstrate the profound impact of parallel quantum computing in enhancing the resolution of Ising problems, which are synonymous with combinatorial optimization challenges.","sentences":["We propose an innovative Parallel Quantum Local Search (PQLS) methodology that leverages the capabilities of small-scale quantum computers to efficiently address complex combinatorial optimization problems.","Traditional Quantum Local Search (QLS) methods face limitations due to the sequential nature of solving sub-problems, which arises from dependencies between their solutions.","Our approach transcends this constraint by simultaneously executing multiple QLS pathways and aggregating their most effective outcomes at certain intervals to establish a ``generation''.","Each subsequent generation commences with the optimal solution from its predecessor, thereby significantly accelerating the convergence towards an optimal solution.","Our findings demonstrate the profound impact of parallel quantum computing in enhancing the resolution of Ising problems, which are synonymous with combinatorial optimization challenges."],"url":"http://arxiv.org/abs/2406.06445v1","category":"quant-ph"}
{"created":"2024-06-10 16:14:50","title":"Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking","abstract":"Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models.","sentences":["Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes.","While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes.","By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost.","Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic.","Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm.","We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics.","Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models."],"url":"http://arxiv.org/abs/2406.06425v1","category":"stat.ML"}
{"created":"2024-06-10 16:14:45","title":"Margin-aware Preference Optimization for Aligning Diffusion Models without Reference","abstract":"Modern alignment techniques based on human preferences, such as RLHF and DPO, typically employ divergence regularization relative to the reference model to ensure training stability. However, this often limits the flexibility of models during alignment, especially when there is a clear distributional discrepancy between the preference data and the reference model. In this paper, we focus on the alignment of recent text-to-image diffusion models, such as Stable Diffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a significant problem in aligning these models due to the unstructured nature of visual modalities: e.g., a preference for a particular stylistic aspect can easily induce such a discrepancy. Motivated by this observation, we propose a novel and memory-friendly preference alignment method for diffusion models that does not depend on any reference model, coined margin-aware preference optimization (MaPO). MaPO jointly maximizes the likelihood margin between the preferred and dispreferred image sets and the likelihood of the preferred sets, simultaneously learning general stylistic features and preferences. For evaluation, we introduce two new pairwise preference datasets, which comprise self-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating diverse scenarios of reference mismatch. Our experiments validate that MaPO can significantly improve alignment on Pick-Style and Pick-Safety and general preference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and other existing methods. Our code, models, and datasets are publicly available via https://mapo-t2i.github.io","sentences":["Modern alignment techniques based on human preferences, such as RLHF and DPO, typically employ divergence regularization relative to the reference model to ensure training stability.","However, this often limits the flexibility of models during alignment, especially when there is a clear distributional discrepancy between the preference data and the reference model.","In this paper, we focus on the alignment of recent text-to-image diffusion models, such as Stable Diffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a significant problem in aligning these models due to the unstructured nature of visual modalities: e.g., a preference for a particular stylistic aspect can easily induce such a discrepancy.","Motivated by this observation, we propose a novel and memory-friendly preference alignment method for diffusion models that does not depend on any reference model, coined margin-aware preference optimization (MaPO).","MaPO jointly maximizes the likelihood margin between the preferred and dispreferred image sets and the likelihood of the preferred sets, simultaneously learning general stylistic features and preferences.","For evaluation, we introduce two new pairwise preference datasets, which comprise self-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating diverse scenarios of reference mismatch.","Our experiments validate that MaPO can significantly improve alignment on Pick-Style and Pick-Safety and general preference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and other existing methods.","Our code, models, and datasets are publicly available via https://mapo-t2i.github.io"],"url":"http://arxiv.org/abs/2406.06424v1","category":"cs.CV"}
{"created":"2024-06-10 16:13:11","title":"Notes on Various Errors and Jacobian Derivations for SLAM","abstract":"This paper delves into critical concepts and meticulous calculations pertinent to Simultaneous Localization and Mapping (SLAM), with a focus on error analysis and Jacobian matrices. We introduce various types of errors commonly encountered in SLAM, including reprojection error, photometric error, relative pose error, and line reprojection error, alongside their mathematical formulations. The fundamental role of error as the discrepancy between observed and predicted values in SLAM optimization is examined, emphasizing non-linear least squares methods for optimization.   We provide a detailed analysis of: - Reprojection Error: Including Jacobian calculations for camera poses and map points, highlighting both theoretical underpinnings and practical consequences. - Photometric Error: Addressing errors from image intensity variations, essential for direct method-based SLAM. - Relative Pose Error: Discussing its significance in pose graph optimization, especially in loop closure scenarios. The paper also presents extensive derivations of Jacobian matrices for various SLAM components such as camera poses, map points, and motion parameters. We explore the application of Lie theory to optimize rotation representations and transformations, improving computational efficiency. Specific software implementations are referenced, offering practical insights into the real-world application of these theories in SLAM systems.   Additionally, advanced topics such as line reprojection errors and IMU measurement errors are explored, discussing their impact on SLAM accuracy and performance. This comprehensive examination aims to enhance understanding and implementation of error analysis and Jacobian derivation in SLAM, contributing to more accurate and efficient state estimation in complex environments.","sentences":["This paper delves into critical concepts and meticulous calculations pertinent to Simultaneous Localization and Mapping (SLAM), with a focus on error analysis and Jacobian matrices.","We introduce various types of errors commonly encountered in SLAM, including reprojection error, photometric error, relative pose error, and line reprojection error, alongside their mathematical formulations.","The fundamental role of error as the discrepancy between observed and predicted values in SLAM optimization is examined, emphasizing non-linear least squares methods for optimization.   ","We provide a detailed analysis of: - Reprojection Error: Including Jacobian calculations for camera poses and map points, highlighting both theoretical underpinnings and practical consequences.","- Photometric Error:","Addressing errors from image intensity variations, essential for direct method-based SLAM.","- Relative Pose Error: Discussing its significance in pose graph optimization, especially in loop closure scenarios.","The paper also presents extensive derivations of Jacobian matrices for various SLAM components such as camera poses, map points, and motion parameters.","We explore the application of Lie theory to optimize rotation representations and transformations, improving computational efficiency.","Specific software implementations are referenced, offering practical insights into the real-world application of these theories in SLAM systems.   ","Additionally, advanced topics such as line reprojection errors and IMU measurement errors are explored, discussing their impact on SLAM accuracy and performance.","This comprehensive examination aims to enhance understanding and implementation of error analysis and Jacobian derivation in SLAM, contributing to more accurate and efficient state estimation in complex environments."],"url":"http://arxiv.org/abs/2406.06422v1","category":"cs.RO"}
{"created":"2024-06-10 15:56:14","title":"Early Acceptance Matching Game for User-Centric Clustering in Scalable Cell-free MIMO Networks","abstract":"The canonical setup is the primary approach adopted in cell-free multiple-input multiple-output (MIMO) networks, in which all access points (APs) jointly serve every user equipment (UE). This approach is not scalable in terms of computational complexity and fronthaul signaling becoming impractical in large networks. This work adopts a user-centric approach, a scalable alternative in which only a set of preferred APs jointly serve a UE. Forming the optimal cluster of APs for each UE is a challenging task, especially, when it needs to be dynamically adjusted to meet the quality of service (QoS) requirements of the UE. This complexity is even exacerbated when considering the constrained fronthaul capacity of the UE and the AP. We solve this problem with a novel many-to-many matching game. More specifically, we devise an early acceptance matching algorithm, which immediately admits or rejects UEs based on their requests and available radio resources. The proposed solution significantly reduces the fronthaul signaling while satisfying the maximum of UEs in terms of requested QoS compared to state-of-the-art approaches.","sentences":["The canonical setup is the primary approach adopted in cell-free multiple-input multiple-output (MIMO) networks, in which all access points (APs) jointly serve every user equipment (UE).","This approach is not scalable in terms of computational complexity and fronthaul signaling becoming impractical in large networks.","This work adopts a user-centric approach, a scalable alternative in which only a set of preferred APs jointly serve a UE.","Forming the optimal cluster of APs for each UE is a challenging task, especially, when it needs to be dynamically adjusted to meet the quality of service (QoS) requirements of the UE.","This complexity is even exacerbated when considering the constrained fronthaul capacity of the UE and the AP.","We solve this problem with a novel many-to-many matching game.","More specifically, we devise an early acceptance matching algorithm, which immediately admits or rejects UEs based on their requests and available radio resources.","The proposed solution significantly reduces the fronthaul signaling while satisfying the maximum of UEs in terms of requested QoS compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2406.06402v1","category":"eess.SP"}
{"created":"2024-06-10 15:48:08","title":"A Gigabit, DMA-enhanced Open-Source Ethernet Controller for Mixed-Criticality Systems","abstract":"The ongoing revolution in application domains targeting autonomous navigation, first and foremost automotive \"zonalization\", has increased the importance of certain off-chip communication interfaces, particularly Ethernet. The latter will play an essential role in next-generation vehicle architectures as the backbone connecting simultaneously and instantaneously the zonal/domain controllers. There is thereby an incumbent need to introduce a performant Ethernet controller in the open-source HW community, to be used as a proxy for architectural explorations and prototyping of mixed-criticality systems (MCSs). Driven by this trend, in this work, we propose a fully open-source, DMA-enhanced, technology-agnostic Gigabit Ethernet architecture that overcomes the limitations of existing open-source architectures, such as Lowrisc's Ethernet, often tied to FPGA implementation, performance-bound by sub-optimal design choices such as large memory buffers, and in general not mature enough to bridge the gap between academia and industry. Besides the area advantage, the proposed design increases packet transmission speed up to almost 3x compared to Lowrisc's and is validated through implementation and FPGA prototyping into two open-source, heterogeneous MCSs.","sentences":["The ongoing revolution in application domains targeting autonomous navigation, first and foremost automotive \"zonalization\", has increased the importance of certain off-chip communication interfaces, particularly Ethernet.","The latter will play an essential role in next-generation vehicle architectures as the backbone connecting simultaneously and instantaneously the zonal/domain controllers.","There is thereby an incumbent need to introduce a performant Ethernet controller in the open-source HW community, to be used as a proxy for architectural explorations and prototyping of mixed-criticality systems (MCSs).","Driven by this trend, in this work, we propose a fully open-source, DMA-enhanced, technology-agnostic Gigabit Ethernet architecture that overcomes the limitations of existing open-source architectures, such as Lowrisc's Ethernet, often tied to FPGA implementation, performance-bound by sub-optimal design choices such as large memory buffers, and in general not mature enough to bridge the gap between academia and industry.","Besides the area advantage, the proposed design increases packet transmission speed up to almost 3x compared to Lowrisc's and is validated through implementation and FPGA prototyping into two open-source, heterogeneous MCSs."],"url":"http://arxiv.org/abs/2406.06394v1","category":"cs.AR"}
{"created":"2024-06-10 15:42:03","title":"Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization","abstract":"Aligning large language models with human preferences has emerged as a critical focus in language modeling research. Yet, integrating preference learning into Text-to-Image (T2I) generative models is still relatively uncharted territory. The Diffusion-DPO technique made initial strides by employing pairwise preference learning in diffusion models tailored for specific text prompts. We introduce Diffusion-RPO, a new method designed to align diffusion-based T2I models with human preferences more effectively. This approach leverages both prompt-image pairs with identical prompts and those with semantically related content across various modalities. Furthermore, we have developed a new evaluation metric, style alignment, aimed at overcoming the challenges of high costs, low reproducibility, and limited interpretability prevalent in current evaluations of human preference alignment. Our findings demonstrate that Diffusion-RPO outperforms established methods such as Supervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions 1.5 and XL-1.0, achieving superior results in both automated evaluations of human preferences and style alignment. Our code is available at https://github.com/yigu1008/Diffusion-RPO","sentences":["Aligning large language models with human preferences has emerged as a critical focus in language modeling research.","Yet, integrating preference learning into Text-to-Image (T2I) generative models is still relatively uncharted territory.","The Diffusion-DPO technique made initial strides by employing pairwise preference learning in diffusion models tailored for specific text prompts.","We introduce Diffusion-RPO, a new method designed to align diffusion-based T2I models with human preferences more effectively.","This approach leverages both prompt-image pairs with identical prompts and those with semantically related content across various modalities.","Furthermore, we have developed a new evaluation metric, style alignment, aimed at overcoming the challenges of high costs, low reproducibility, and limited interpretability prevalent in current evaluations of human preference alignment.","Our findings demonstrate that Diffusion-RPO outperforms established methods such as Supervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions 1.5 and XL-1.0, achieving superior results in both automated evaluations of human preferences and style alignment.","Our code is available at https://github.com/yigu1008/Diffusion-RPO"],"url":"http://arxiv.org/abs/2406.06382v1","category":"cs.CV"}
{"created":"2024-06-10 15:40:23","title":"FinVerse: An Autonomous Agent System for Versatile Financial Analysis","abstract":"With the significant advancements in cognitive intelligence driven by LLMs, autonomous agent systems have attracted extensive attention. Despite this growing interest, the development of stable and efficient agent systems poses substantial practical challenges. In this paper, we introduce FinVerse, a meticulously crafted agent system designed for a broad range of financial topics. FinVerse integrates over 600 financial APIs, enabling access to more accurate and extensive financial information compared to generalist agents. To enhance financial information processing capabilities, FinVerse is equipped with an embedded code interpreter, enabling the execution of complex data analysis tasks with precision and efficiency. Our work includes an empirical comparison of several LLMs in driving FinVerse. Specifically, we propose our own scheme for training LLMs using SFT to optimize LLM performance within FinVerse. Recognizing the scarcity of specialized datasets to build LLMs for agents, we have constructed a dataset and plan to make it open-source, providing a valuable resource for peer application developers. The demo video has been released on YouTube at https://www.youtube.com/watch?v=sk8L9_Wv7J4","sentences":["With the significant advancements in cognitive intelligence driven by LLMs, autonomous agent systems have attracted extensive attention.","Despite this growing interest, the development of stable and efficient agent systems poses substantial practical challenges.","In this paper, we introduce FinVerse, a meticulously crafted agent system designed for a broad range of financial topics.","FinVerse integrates over 600 financial APIs, enabling access to more accurate and extensive financial information compared to generalist agents.","To enhance financial information processing capabilities, FinVerse is equipped with an embedded code interpreter, enabling the execution of complex data analysis tasks with precision and efficiency.","Our work includes an empirical comparison of several LLMs in driving FinVerse.","Specifically, we propose our own scheme for training LLMs using SFT to optimize LLM performance within FinVerse.","Recognizing the scarcity of specialized datasets to build LLMs for agents, we have constructed a dataset and plan to make it open-source, providing a valuable resource for peer application developers.","The demo video has been released on YouTube at https://www.youtube.com/watch?v=sk8L9_Wv7J4"],"url":"http://arxiv.org/abs/2406.06379v1","category":"cs.CE"}
{"created":"2024-06-10 15:22:41","title":"Automating Food Drop: The Power of Two Choices for Dynamic and Fair Food Allocation","abstract":"Food waste and food insecurity are two closely related pressing global issues. Food rescue organizations worldwide run programs aimed at addressing the two problems. In this paper, we partner with a non-profit organization in the state of Indiana that leads \\emph{Food Drop}, a program that is designed to redirect rejected truckloads of food away from landfills and into food banks. The truckload to food bank matching decisions are currently made by an employee of our partner organization. In addition to this being a very time-consuming task, as perhaps expected from human-based matching decisions, the allocations are often skewed: a small percentage of the possible recipients receives the majority of donations. Our goal in this partnership is to completely automate Food Drop. In doing so, we need a matching algorithm for making real-time decisions that strikes a balance between ensuring fairness for the food banks that receive the food and optimizing efficiency for the truck drivers. In this paper, we describe the theoretical guarantees and experiments that dictated our choice of algorithm in the platform we built and deployed for our partner organization. Our work also makes contributions to the literature on load balancing and balls-into-bins games, that might be of independent interest. Specifically, we study the allocation of $m$ weighted balls into $n$ weighted bins, where each ball has two non-uniformly sampled random bin choices, and prove upper bounds, that hold with high probability, on the maximum load of any bin.","sentences":["Food waste and food insecurity are two closely related pressing global issues.","Food rescue organizations worldwide run programs aimed at addressing the two problems.","In this paper, we partner with a non-profit organization in the state of Indiana that leads \\emph{Food Drop}, a program that is designed to redirect rejected truckloads of food away from landfills and into food banks.","The truckload to food bank matching decisions are currently made by an employee of our partner organization.","In addition to this being a very time-consuming task, as perhaps expected from human-based matching decisions, the allocations are often skewed: a small percentage of the possible recipients receives the majority of donations.","Our goal in this partnership is to completely automate Food Drop.","In doing so, we need a matching algorithm for making real-time decisions that strikes a balance between ensuring fairness for the food banks that receive the food and optimizing efficiency for the truck drivers.","In this paper, we describe the theoretical guarantees and experiments that dictated our choice of algorithm in the platform we built and deployed for our partner organization.","Our work also makes contributions to the literature on load balancing and balls-into-bins games, that might be of independent interest.","Specifically, we study the allocation of $m$ weighted balls into $n$ weighted bins, where each ball has two non-uniformly sampled random bin choices, and prove upper bounds, that hold with high probability, on the maximum load of any bin."],"url":"http://arxiv.org/abs/2406.06363v1","category":"cs.GT"}
